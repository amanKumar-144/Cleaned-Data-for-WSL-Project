question_id,answer_id,comment_id,comment_text,comment_owner
117508,507134.0,,nan,
117508,507207.0,,nan,
117508,507587.0,,nan,
117508,507911.0,117788.0,Any cur off coefficient if we say big positive. Because if i take all positive best features comes are near to 100 ..,312019.0
117508,507911.0,117784.0,"Clear for fetching about features. how to can we find r2,aic,bic,cp. we are using ridge/alsso.",312019.0
117508,508003.0,118199.0,Should we do this assignment for both Ridge and LAsso methods? and compare?,308437.0
119445,,,nan,
117938,508577.0,,nan,
117938,508692.0,117976.0,did the same.,318756.0
118156,509430.0,,nan,
118156,509858.0,,nan,
117836,508110.0,,nan,
117836,508120.0,,nan,
117836,509013.0,,nan,
117836,509514.0,,nan,
118417,510725.0,,nan,
118417,510731.0,,nan,
118417,510968.0,,nan,
118417,510992.0,,nan,
118417,511816.0,,nan,
117880,508256.0,,nan,
117518,507125.0,,nan,
117518,507204.0,,nan,
117605,507310.0,,nan,
117605,508047.0,,nan,
117605,507324.0,117674.0,"NA here doesn't mean ""No garage"". This is applicable for the categorical variables. In this case it just means that the data of the year is missing. Try imputing with mode.",310974.0
117605,507324.0,117683.0,"NA here means no garage, as if you check all rows having this column as null is same as the ones which dont have any garage.",317689.0
117605,507324.0,117697.0,"So, what approach shall we take ?",317689.0
117605,507324.0,117686.0,I'm sorry you're right,310974.0
117605,508104.0,,nan,
117605,507836.0,,nan,
117605,507937.0,,nan,
117605,507943.0,,nan,
117605,507944.0,,nan,
117605,508165.0,,nan,
117605,508736.0,,nan,
117605,510261.0,,nan,
117950,508648.0,,nan,
117950,508685.0,,nan,
118441,510518.0,,nan,
118441,510521.0,,nan,
118441,510546.0,,nan,
117468,506919.0,,nan,
117468,506955.0,,nan,
117468,507049.0,,nan,
117468,507044.0,,nan,
117267,505708.0,117475.0,Where are these L1 and L2 regularization techniques mentioned? Are they covered in lectures?,308437.0
117267,505708.0,117481.0,OK thanks 😊,308437.0
117267,505708.0,117478.0,Its not covered in the lectures. Please refer below link. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/,310467.0
117267,505786.0,,nan,
118448,510569.0,118234.0,some specific ones not all.,302738.0
118448,510569.0,118229.0,"Hi Naseem, How many rows have you retained after the outlier treatment?",311160.0
118448,510569.0,118232.0,"Hi Harsha, 1253 i.e. approximately 86% of the original data",302738.0
118448,510569.0,118233.0,"Thank you, Have you treated all the Numerical variables or specific ones ?",311160.0
118448,510569.0,118244.0,Thanks Naseem...I missed out on this somehow... Now the models are performing good...,318479.0
118448,510569.0,118252.0,you are welcome :),302738.0
118448,510728.0,118246.0,"Thanks Ram, outliers were the main culprit...",318479.0
117276,505920.0,,nan,
117276,505834.0,,nan,
117276,507459.0,,nan,
117276,505767.0,,nan,
117276,506086.0,,nan,
117276,506109.0,,nan,
118433,510484.0,,nan,
118086,509885.0,,nan,
118367,510237.0,118169.0,i did rfe on that and checked vif using the statsmodel. Moreover the age feature is having negative coeff with Lasso but positive coeff with Ridge. The swinging of signs means multicollinearity is present.,304319.0
118367,510236.0,,nan,
118100,509018.0,,nan,
118650,511814.0,118375.0,Hi naseem.. There is no TA verified answer for that query....I still hv a doubt,308437.0
118650,511901.0,,nan,
118102,509309.0,,nan,
118102,509864.0,118129.0,"I know how to generate dummies, my question is regarding is this okay to have 25 dummie columns for single feature like Neighborhood.",301118.0
118102,509864.0,118250.0,Yes Vender,344894.0
118103,509021.0,,nan,
117317,,,nan,
117322,506080.0,,nan,
117322,506951.0,,nan,
117745,507839.0,117738.0,"If we have two linear models, having different beta0 and beta1 . what is the criteria to select best model, based on constant or slope(independent variable co-efficient). In general whats the criteria to be checked.",312019.0
117745,507898.0,,nan,
118074,509039.0,,nan,
118074,509109.0,,nan,
118044,508795.0,,nan,
117336,506124.0,,nan,
117760,508374.0,,nan,
117760,507896.0,117747.0,How do we find out AIC and BIC?,304319.0
117760,508129.0,117812.0,Ok thnk you. I thought some inbuilt API is there to calculate these parameters.,304319.0
117760,508129.0,117841.0,LassoLarsIC(criterion='aic') from : https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html,318438.0
117760,508131.0,,nan,
117760,508193.0,,nan,
117760,508372.0,,nan,
117760,509856.0,,nan,
118134,509409.0,,nan,
118134,509179.0,,nan,
118134,509660.0,,nan,
118134,509999.0,,nan,
118082,509884.0,,nan,
117774,507892.0,,nan,
117774,508126.0,,nan,
117774,508155.0,,nan,
117359,509946.0,,nan,
117359,506890.0,,nan,
117359,506957.0,,nan,
118161,509303.0,,nan,
118492,510963.0,,nan,
118492,510952.0,118261.0,"yes, I did outlier treatment only for large numerical values, not for categorical.",317073.0
118172,509682.0,,nan,
117277,505918.0,117515.0,when we are importing CSV file it is treating as Nan. How can we progress ?,320103.0
117277,505918.0,117520.0,Please try to replace/impute them based on significance.,307495.0
117277,505769.0,,nan,
117277,506123.0,117546.0,"The trick here is both nulls and NAs are imported as nulls, There is no way to tell which is NA and which is an actual null. Any thoughts?",310974.0
117277,506123.0,117558.0,this is the case for only two features. handling them manually,311686.0
117277,506142.0,118198.0,same question,308437.0
117816,508124.0,,nan,
117816,508097.0,,nan,
117816,508753.0,,nan,
117816,508744.0,,nan,
117816,508789.0,,nan,
118785,512709.0,118546.0,why only less than 200 considered and on what basis?..also can u explain what is happening in this code am not able to understand... cv_results_ -----> what variable is this?,308437.0
118785,512836.0,,nan,
118785,512858.0,118602.0,"Got it. But on what basis 200,100 etc is selected for filtering?",308437.0
117695,507767.0,,nan,
117695,507773.0,117722.0,nopes.,311686.0
117695,507835.0,,nan,
117695,507940.0,,nan,
118214,509420.0,,nan,
118214,509669.0,118097.0,1. It is basically relative to know what is high enough to be in the model. Lasso would have zeroed them if it is not worth to be in the model. Hence you can leave it at that. 2. Not necessary,301121.0
118214,509669.0,118096.0,Thank you very much. So basically I have just 2 more outstanding doubts left. 1) What should be the approx coefficient we should consider as a high enough value? 2) Is it necessary to further perform ridge regression after lasso regression?,310472.0
118236,509627.0,,nan,
118236,509672.0,,nan,
118011,508695.0,117914.0,"Ohh no it is not negative mean absolute error, it is r2 just I forgot to change the title of the plot",301655.0
118011,508695.0,117921.0,What alpha you have taken? For ridge it seems to be 5 and for lasso it seems to be 75 per your graph. If you take more than that then you might run into over fitting,304814.0
118011,508695.0,117927.0,For ridge I am getting a stable value of test and train scores from an alpha value of 100 onwards and for lasso above 470 or 480,301655.0
118243,509625.0,,nan,
118243,509817.0,,nan,
118332,510124.0,,nan,
118332,510150.0,118158.0,"1) If there are completely no outliers present in the data or if all of the errors have the same magnitude, then RMSE=MAE. Otherwise, in all cases RMSE will always be larger than MAE. 2) From business perpective I may not be able to provide any concrete examples, but, if you want to penalize the model more when there is atleast one huge error (i.e. variation in errors is high) then pick RMSE, otherwise MAE.",318730.0
118332,510205.0,,nan,
118299,510905.0,,nan,
118462,510724.0,118294.0,"Any idea does best linear model requires lambda ? Lambda comes in place if we are using ridge/lasso ,",309212.0
118462,510807.0,,nan,
118249,510223.0,,nan,
118249,509657.0,118085.0,full columns,318335.0
118249,509657.0,118084.0,Doing the same since earlier..but still the view is not expanding..not able to see columns,318335.0
118249,509749.0,118091.0,Thanks this worked.,318335.0
118249,510510.0,,nan,
118080,508921.0,,nan,
118258,509803.0,118259.0,In that case how size of Hall or Kitchen are correlated to the response variable,344894.0
118258,509803.0,118159.0,"Say we have asked what are the two key dimension that you are looking while selecting House Option1: Response: Overall Size: 40%, Hall Size: 50%, Kitchen Size:0%, Others:10% Option2: Response: Overall Size: 40%, Hall Size: 0%, Kitchen Size:50%, Others:10% Assuming Sale Price is directly related with the choices. What shall the model select? The priority should be going to Overall Size compared to the size of Hall or Kitchen",317514.0
118273,510820.0,118256.0,Thanks Paras,318329.0
118570,511581.0,,nan,
118277,509828.0,118122.0,"Rajesh, what do you mean by not scale but transform?",304319.0
118277,509828.0,118123.0,"You can apply any sort of transformation, square, cube, log etc (depending on how your data looks). Just remember to apply the same transformation to the test data as well.",318438.0
118277,509828.0,118124.0,"If we do not scale the Target variable, the coeff of the predictors in the regression equation would be quite large. We scaled Target variable in our prev Models. Any reason for not scaling in Lasso/Ridge?",304319.0
118277,509828.0,118222.0,"My apologies, I checked and tend to agree with your above comments. If you scale only dependent and not target then the coefficients value might not come right. TA, please confirm",317514.0
118277,510062.0,,nan,
118277,510258.0,118171.0,"Infact the coeff change drastically. With scaling of output variable, coeff are ranging from -1 to 1, whereas without scaling there are in the order of ten thousands. Yes, the accuracy does not change.",304319.0
118277,510258.0,118173.0,Yes true. The coeffs are taking care of the difference in the scales of dependent variable and the independent variables. Not scaling the dependent variable will make the interpretation better.,304319.0
118277,510258.0,118172.0,"I misspoke, I meant the importance of coefficients i.e. the important features remain same whether we scale or not. The one issue I see is that if we have to predict based on this model, will we not have to 'rescale' the predicted Sale price? The model would generate a scaled value, which would not make sense.",318438.0
117882,508267.0,117825.0,"So RFE is not required, we can use Lasso for feature selection and then Ridge regression on the feature selected by Lasso ? Is my understanding correct ?",317991.0
117882,508267.0,117838.0,Thanks.,317991.0
117882,508267.0,117837.0,RFE is not required. Your understanding of that is correct. Use Lasso to do feature selection and then use Ridge to find the optimal value of lambda. And then compare the two. You can do cross-validation to ignore overfitting,304281.0
117882,508267.0,118074.0,"Vikas, Since Lasso is selecting more than 90 variables, is it ok to build a model so complex? Earlier we were taught that the model should be simple and have less variables. Moreover the variables selected by lasso are showing multicollinearity. Please guide.",304319.0
117882,509652.0,,nan,
117882,508793.0,118486.0,exactly..u nailed it... ridge and lasso gives u 2 different models...u cant feed input of 1 model to another one,308437.0
117882,508793.0,118541.0,"at first after reading the question, this was my understanding - build 2 separate models lasso and ridge and compare...but after seeing TA response i got confused...",308437.0
117882,508793.0,118509.0,"As you can see in the comments above If we go through the TA's response, in first place we get impression that TA is verifying that we have to do lasso and then output of lasso is to be given to ridge. But this is not the case. Once we go through my ques and TA's response carefully, he verified that RFE is not required, but he didn't verify my statement of ""we can use Lasso for feature selection and then Ridge regression on the feature selected by Lasso ?"". In fact he clearly states that use Lasso for feature selection and Ridge for finding optimal alpha means we need to build two separate mode using Lasso and Ridge and then comparison needs to be done. I hope this clarify your doubt.",317991.0
117899,508305.0,,nan,
118644,511905.0,,nan,
118292,509993.0,118134.0,Ok. Thanks Ashish,318335.0
118175,509361.0,,nan,
118175,509302.0,,nan,
117849,508337.0,117898.0,Need more inputs.,310467.0
117849,508611.0,,nan,
117849,508927.0,,nan,
118302,510002.0,118167.0,Something's not right. R2 values cannot be more than 1. It is generally between 0 and 1 (although can be negative). How are you calculating it? I hope you are using the API's provided by sklearn to do so.,318438.0
118302,510002.0,118125.0,Yes they are insanely high thinking is it because of overfitting R2-780180881757.3029 AIC-27036.005200142346 BIC-27113.98916875267,314197.0
118302,510014.0,118132.0,I did scaling even before Splitting the data into train and test,314197.0
118302,510477.0,,nan,
118302,510720.0,,nan,
117853,508274.0,,nan,
117853,508339.0,,nan,
117854,,,nan,
120162,520976.0,,nan,
120162,520983.0,,nan,
120162,521336.0,,nan,
120162,521581.0,,nan,
117489,507012.0,117587.0,"i want the columns/column names which has values only yes and no. So i can map all the columns with yes to 1 and no to 1. If there are too many variables, difficult to go through each column.",312019.0
117489,507012.0,117828.0,You can use data dictionary to find out,312758.0
118820,512863.0,118607.0,That's right. Alpha for the least negative score.,318438.0
118820,512863.0,118604.0,"Thanks Ashish for the explanation. I understood almost. When u say test score is minimum , y-axis is entirely on negative scale. So, alpha at least negative value is to be chosen right?",308437.0
118820,512862.0,,nan,
117868,508500.0,,nan,
117868,508917.0,,nan,
117868,509173.0,118209.0,outlier treatments ?,318780.0
117868,509173.0,118280.0,outlier treatment is necessary.,301114.0
117185,505495.0,,nan,
117185,505171.0,,nan,
117185,506097.0,,nan,
117185,506959.0,,nan,
117185,507047.0,,nan,
117495,507136.0,,nan,
117495,507210.0,,nan,
117495,507784.0,,nan,
118154,510810.0,,nan,
118154,510798.0,,nan,
118271,509835.0,,nan,
118271,509930.0,,nan,
118291,510006.0,,nan,
118291,509915.0,,nan,
118291,509928.0,,nan,
118333,510139.0,118148.0,"That I calculated. My question is AIC, BIC and Adjusted r_square needs to be calculated ?",317991.0
118333,510196.0,118221.0,r2 and mean sqr error shd be sufficient,317514.0
118333,510196.0,118155.0,"That I calculated. My question is AIC, BIC and Adjusted r_square needs to be calculated or r2 is sufficient?",317991.0
118333,510214.0,118163.0,Thanks Ashish. This is what I was looking for.,317991.0
118361,510210.0,,nan,
117878,508259.0,117824.0,"Thanks Vikas, so can we use OLS from stats model such as sm.OLS(y_train, lasso_lm).fit() to see AIC, BIC and Adjusted R2 values ?",309211.0
117878,508259.0,117956.0,"OLS itself is a different model without Regularization. You can use that to calculate AIC, BIC of Regularized models.",310974.0
117881,508263.0,,nan,
117512,507131.0,,nan,
117512,507598.0,,nan,
118028,508745.0,,nan,
117891,508285.0,,nan,
117891,508289.0,,nan,
117891,510448.0,118241.0,We have many columns with Nan....should nt we replace with appropriate value,308638.0
117891,510448.0,118240.0,please drop all the rows with zeros probably should help .,318723.0
117891,510448.0,118242.0,Are u suggesting to drop all the rows having Nan,308638.0
117891,510448.0,118281.0,you could drop or replace with some average values...,318723.0
117892,508287.0,,nan,
118219,509425.0,118108.0,Thanks for the input.,306243.0
118219,509809.0,118106.0,Thanks for the input,306243.0
118219,509912.0,,nan,
118219,510021.0,,nan,
118360,510204.0,,nan,
117895,508301.0,117832.0,"Thanks TA, saved my columns.",300721.0
117896,508302.0,,nan,
118643,512094.0,118471.0,So the point at which the test curve starts increasing is the approximate alpha value right?,308437.0
118643,512095.0,118445.0,"Yes, that way too many combinations.. remember system will try to train your model 10^6 (~1 million) times over 5 folds!! Start with alpha = [0.001, 0.01, 0.1, 1, 10, 50, 100, 150] or something similar If your model suggests 50 (let's say), you can add 40 and 60 and re-test. This way you can get a feel of what the alpha value might be and then fine tune it. Hope that helps",318438.0
118643,512095.0,118440.0,Hi ashish...I tried restarting kernel and didn't help me...I am using 5 folds and alpha from 0.0001 to 1000...u mean reduce this range and retry??,308437.0
117900,508307.0,,nan,
117900,508584.0,117920.0,"thanks, already answered and verified by TA",309211.0
117918,508446.0,,nan,
119240,515250.0,,nan,
119240,515133.0,,nan,
119240,515523.0,,nan,
119240,516289.0,,nan,
117603,508089.0,,nan,
117603,507250.0,,nan,
117603,507328.0,,nan,
117603,507580.0,,nan,
117603,507901.0,,nan,
117603,508135.0,118001.0,How to map the coefficient values to feature names?,307495.0
118031,509879.0,,nan,
118131,509114.0,117979.0,That I am aware of. But the main concern is data-set is small. We did linear regression on Car Price data -set and it had 205 data points. Their we did not perform outlier treatment. This time it has more data points but still it comes under category of small data-set.,317991.0
118131,509115.0,117978.0,That is the main concern. We did linear regression on Car Price data -set and it had 205 data points. Their we did not perform outlier treatment. This time it has more data points but still it comes under category of small data-set. Generally we should do outlier treatment. But for this assignment don't know.,317991.0
118131,509367.0,,nan,
118131,509499.0,,nan,
118131,509647.0,,nan,
118410,510315.0,118189.0,thank you,320606.0
118410,510347.0,,nan,
118410,510726.0,,nan,
117888,508283.0,,nan,
117888,508281.0,117827.0,"I was looking forward to know what's the ""case"" and the ""preferences"", it just cannot be something random; and there is definitely a logical reason to select the same. Am not sure if it is present in any of lecture notes, unable to recollect from the lectures. found my answer here : https://datascience.stackexchange.com/questions/43972/when-should-i-use-standardscaler-and-when-minmaxscaler",309211.0
118473,510902.0,118287.0,Totally 52 categorical variables,308437.0
118473,510902.0,118286.0,Yes I did convert some numeric to categorical,308437.0
118473,510902.0,118298.0,I had 50 categorical variables.,318730.0
118473,511109.0,118283.0,"Hi ram....I converted around 7 numeric to categorical like year, month....because they can't be numeric as no math can b performed on these variables...also there were some variables which are basically ratings/scale, these have to be categorical...",308437.0
118473,511109.0,118323.0,"Hi Madhu, Please convert categorical strings into a representative number (As Like LabelEncoder). It will reduce the number of features. Please do not use these many features Thanks",344894.0
118293,509989.0,118127.0,Imputing drops the score on test data by 5%. Now also would you impute ?,317991.0
118293,510003.0,,nan,
118293,510030.0,,nan,
118293,510015.0,,nan,
118293,510221.0,,nan,
118293,513943.0,,nan,
117731,507941.0,,nan,
117731,508132.0,,nan,
117731,508380.0,,nan,
117731,508258.0,,nan,
118354,510345.0,118208.0,use select_dtypes(). columns.values and verify before scaling,318438.0
118354,510273.0,118177.0,I only selected int32 and float ...still getting error,310509.0
118354,510273.0,118181.0,try to post the code snippet with error..,318438.0
118354,510273.0,118260.0,"column = house.select_dtypes(include=['float64','int64']).columns",344894.0
119355,516017.0,,nan,
119355,516165.0,,nan,
90603,,,nan,
90026,374853.0,,nan,
90026,374833.0,,nan,
90026,374834.0,,nan,
90026,374836.0,,nan,
92342,388966.0,,nan,
92342,389194.0,,nan,
93232,392434.0,,nan,
93232,392400.0,,nan,
92390,389121.0,,nan,
92390,389547.0,,nan,
92390,389326.0,,nan,
92390,389127.0,97029.0,I am getting the same error.,306729.0
92390,389131.0,97031.0,I have ran the whole query in a new sheet . But getting that error .,306729.0
92390,389104.0,97028.0,I have changed the salary column to pay as a part of my practice . When I copy the quest from the learning portal and change the salary to pay it works but when I type the query myself it shows an error .,306729.0
92390,389126.0,,nan,
92390,389218.0,,nan,
92390,389256.0,,nan,
92390,389859.0,,nan,
91846,384819.0,96450.0,Seems like some issue with the platform.. Not able to upload the screenshot,311741.0
91846,384819.0,96445.0,Any specific settings screenshot or error screenshot you want ?,311741.0
91846,384819.0,96447.0,Workbench screenshot,313826.0
91846,384819.0,96465.0,"error message shows issue is near the ""OVER ( order by e.salary ) as 'row_number' from employee as e"". Tried all but still no luck",311741.0
91846,384819.0,96452.0,"Ohk. Then some general checks: 1. Make sure you have selected the right database. Use command ""use companydb;"" if you are doing query on the companydb database. 2. Make sure that if there are any queries before this query, then they have been terminated with semi-colon . 3. Try to type in the entire query manually. Sometimes copying and pasting from a different source does not work. 4. If nothing works, then restart :) .",313826.0
91846,384852.0,,nan,
91846,384859.0,,nan,
91846,384868.0,,nan,
91846,385103.0,,nan,
91846,385129.0,96488.0,FYI: I am using ubuntu. Not windows OS,311741.0
91846,385129.0,96487.0,No i get the same error,311741.0
91846,385390.0,96495.0,I tried this also. But no luck,311741.0
91846,386643.0,,nan,
91846,387048.0,,nan,
91470,382703.0,,nan,
91470,382761.0,,nan,
92080,386796.0,96721.0,not working !!,318429.0
92080,386854.0,96664.0,Its not telling error is with the table column to which i am inserting. Its telling error is in the window function field.,311741.0
92080,387061.0,,nan,
92080,386901.0,96720.0,not working,318429.0
92080,386901.0,96753.0,"Close_Price, 20_Day_MA, 50_Day_MA change for these all, It will work for sure",317811.0
92080,386901.0,96952.0,"CREATE TABLE A( 20_Day_MA FLOAT(25,6) ); INSERT INTO A (20_Day_MA) SELECT TRUNCATE(avg(Close_Price) over(ORDER BY Date ROWS 19 PRECEDING),2) as 20_DMA from assignment.bajajauto_1; I still have issue with insert",311741.0
92080,386901.0,96766.0,thanks it worked.,318429.0
91541,383097.0,96182.0,welcome :),317998.0
91541,383097.0,96181.0,Got. It. Thanks for an elaborated explanation,319006.0
91541,383172.0,,nan,
91541,383186.0,,nan,
91541,383238.0,,nan,
91541,383332.0,,nan,
91541,383368.0,,nan,
91541,389515.0,,nan,
89830,373409.0,,nan,
89830,380947.0,96446.0,"Hi veena, can you please elaborate more on this based on the query. Thanks.",315028.0
89830,380947.0,96484.0,"In the above query, the main idea is to demonstrate how the query works in a frame . By defining a frame to be all rows from the partition start to the current row (rows unbounded preceding), it computes running first_value and n_value for each row. The ssn, emp_name, dno, salary , first_val, second_val will be given out for the defined frame.",300717.0
91567,383327.0,,nan,
91567,383326.0,,nan,
91567,383393.0,,nan,
91567,383579.0,,nan,
94059,396390.0,,nan,
94059,396399.0,,nan,
94059,396480.0,,nan,
91693,384044.0,,nan,
91693,389328.0,,nan,
92111,387228.0,,nan,
92111,387430.0,,nan,
91246,381338.0,,nan,
91246,381372.0,95876.0,thanks i didnt read the query properly my bad,304692.0
91246,381372.0,95875.0,when you're pulling out specific columns from a table there is no need to put comma after the last column (here dept_salary).. i guess SQL results in error doing that! :),316349.0
91246,381384.0,95883.0,no ... definitely not see why not in the above post where Jetendra suggested the same thing,300694.0
91246,381384.0,95922.0,ok got it is the last column.,317689.0
91246,381390.0,,nan,
91246,381780.0,,nan,
91246,383241.0,,nan,
91246,384495.0,,nan,
91246,384496.0,,nan,
91879,388588.0,,nan,
92777,391998.0,,nan,
91978,385921.0,,nan,
91978,385916.0,,nan,
91986,389427.0,,nan,
91248,381370.0,,nan,
91881,385251.0,,nan,
91881,385267.0,,nan,
92340,389001.0,97353.0,syntax is correct but windowing is not supported by 32-bit,318804.0
92340,389196.0,,nan,
92340,390112.0,97354.0,not working,318804.0
92340,390248.0,,nan,
92067,386924.0,,nan,
92067,386767.0,,nan,
92067,386825.0,,nan,
92594,390226.0,,nan,
92594,390056.0,,nan,
92594,390091.0,,nan,
92615,391307.0,,nan,
93286,392627.0,99512.0,Thanks Vipul,314629.0
93197,392282.0,98033.0,what's the process now. what all I have to uninstall and them install. can you please give me detailed steps so that I can quickly follow,303227.0
93358,392882.0,98344.0,another queries are working fine,320687.0
93358,392882.0,98462.0,Please respond me,320687.0
93358,392910.0,,nan,
93358,394270.0,98748.0,I have already done.,320687.0
93358,394270.0,98678.0,I checked sever is running,320687.0
93358,394270.0,98679.0,Please help me I'm facing this problem yesterday,320687.0
93358,394270.0,98732.0,"Please check if th output area is visible. View > Panels > and looking whether it says Hide Output Area or Show Output Area. If it is set to hide, click on show. And then check.",304281.0
93358,395431.0,98988.0,I'm using Manjaro Budgie which is Arch linux based. It uses mariadb.,320687.0
93358,395431.0,98983.0,"Hey, for MySQL versions older than 8.0, the windows functions do not work. So, make sure that your version is 8.0 or newer.",319721.0
93358,395431.0,98980.0,I'm able to run all queries except window function queries,320687.0
93358,395431.0,98981.0,"select ssn, salary, row_number() over (order by salary) as 'row_number', rank() over (order by salary) as 'rank', dense_rank() over (order by salary) as 'dense_rank' from employee; The above query is also giving query interrupt error.",320687.0
93358,395438.0,,nan,
94708,399989.0,,nan,
91690,383923.0,96328.0,Oh! Okay. Thank you :),312507.0
91690,384222.0,,nan,
91690,385064.0,,nan,
91690,385355.0,,nan,
91690,390265.0,,nan,
91899,385243.0,,nan,
91899,385257.0,,nan,
91899,386313.0,,nan,
90061,374975.0,,nan,
90061,374979.0,,nan,
90061,375233.0,,nan,
90061,378328.0,,nan,
91428,382288.0,,nan,
91428,382340.0,,nan,
91428,382587.0,,nan,
91428,383350.0,96312.0,for a udf the deterministic return type value is fixed so output type can not be changed but value can be changed,301108.0
91450,,,nan,
91507,382903.0,96186.0,welcome bro :),305129.0
91507,382903.0,96144.0,Thank you,303674.0
91507,382968.0,,nan,
91526,383245.0,,nan,
91526,382996.0,,nan,
91526,382993.0,,nan,
93005,391718.0,97891.0,DML standlone gives correct result. The isse is with UDf only I guess.,308673.0
93005,391717.0,97890.0,Still the same issue with Buy and Sell. I don't think providing the code would be advisable. Would be breaching.,308673.0
93005,391717.0,97906.0,"true.. try playing with your UDF, this is not that difficult.. :)",316349.0
93005,391717.0,97908.0,great! :),316349.0
93005,391717.0,97907.0,Yes . It worked !! Silly mistake,308673.0
93005,391863.0,,nan,
91549,383128.0,,nan,
91549,403586.0,,nan,
91549,383144.0,,nan,
91565,383268.0,,nan,
91565,383250.0,,nan,
91565,383328.0,,nan,
92105,387157.0,,nan,
92105,387313.0,,nan,
92105,387526.0,,nan,
92105,388611.0,,nan,
92105,389114.0,,nan,
91667,383791.0,,nan,
91667,383933.0,,nan,
91667,383831.0,,nan,
91667,387888.0,,nan,
91666,384058.0,,nan,
91975,385922.0,,nan,
91975,385936.0,,nan,
91745,384069.0,,nan,
91747,384173.0,,nan,
91747,384181.0,,nan,
92226,388467.0,,nan,
92226,388512.0,,nan,
92226,388509.0,,nan,
92226,388696.0,,nan,
92226,389566.0,,nan,
92226,389727.0,,nan,
91317,381616.0,,nan,
91317,381615.0,,nan,
91317,381636.0,,nan,
91317,382211.0,,nan,
91317,382580.0,,nan,
91822,384711.0,,nan,
91822,384622.0,,nan,
91822,384650.0,,nan,
91822,389715.0,,nan,
92065,386784.0,,nan,
92065,386931.0,,nan,
92065,389713.0,,nan,
93103,393566.0,,nan,
92609,390194.0,,nan,
92609,390210.0,97703.0,"As opposed to inderministic functions where outputs are not same for the same inputs, an example of an inderministic function would be a random function which generates random numebrs.",319721.0
92609,390327.0,,nan,
92609,391024.0,,nan,
92570,389941.0,,nan,
92570,389958.0,,nan,
92570,390022.0,,nan,
92570,390063.0,,nan,
92570,391276.0,,nan,
92570,391554.0,,nan,
92780,390741.0,,nan,
92780,390785.0,,nan,
92780,390791.0,,nan,
92975,391568.0,,nan,
92975,391565.0,,nan,
92975,391657.0,,nan,
91648,383926.0,,nan,
91648,383949.0,,nan,
91648,385368.0,,nan,
93338,392795.0,,nan,
93338,392766.0,98410.0,"hello,Chandan, I want to create function which will fill the baja2 table",306008.0
93338,392766.0,98647.0,"True,I have done that only,I tried to do in another process also;it has thrown error",306008.0
93338,392766.0,98423.0,why do you want that? you create Bajaj2 table directly in the form : create table Bajaj2 as select ............. from bajaj1,311686.0
93338,392771.0,98406.0,"hello Sushmitha, TA told us in Discussion forum already to use CASE;I am trying with a different method,Which is throwing error and I want to understand what I have done mistake",306008.0
93340,392767.0,98304.0,can give me WhatsApp no. then i send you,320687.0
93340,392767.0,98303.0,I'm not able to upload the snapshot on discussion forum,320687.0
93340,392770.0,98305.0,How to share?,320687.0
93340,392773.0,98309.0,i already seen this link,320687.0
93340,392783.0,98311.0,while I'm uploading but it doesn't work,320687.0
93340,392783.0,98316.0,Please accept my request on LinkedIn then i send you,320687.0
93340,392783.0,98314.0,"take a screen snap and SAVE IT , and then share it here. you cant share in discussion forum unless its saved.",306735.0
93407,393431.0,,nan,
93407,393130.0,,nan,
93407,393131.0,,nan,
93407,393212.0,,nan,
93354,392884.0,,nan,
91974,386823.0,,nan,
91974,390256.0,,nan,
93512,393704.0,98551.0,Glad that it worked for you :),317811.0
93512,393704.0,98550.0,You got me. Thank you.,318372.0
93512,393911.0,,nan,
93512,393706.0,,nan,
92329,388803.0,,nan,
92329,388810.0,,nan,
92329,388858.0,,nan,
92329,388911.0,,nan,
92329,389118.0,,nan,
92329,389154.0,,nan,
92329,393046.0,,nan,
90766,378213.0,95331.0,What are the statements you are talking about?firstly data(CSV file)would be imported in table.,300690.0
90766,381218.0,,nan,
91086,380455.0,95738.0,Nice Article. Thanks Indranil.,305845.0
91086,380585.0,,nan,
91086,381223.0,,nan,
94000,396099.0,,nan,
94000,396135.0,,nan,
94000,396105.0,,nan,
90767,388914.0,,nan,
90767,378297.0,95349.0,Even after setting it as default schema same error persists.,300690.0
90767,379088.0,,nan,
90767,386136.0,,nan,
91892,385145.0,,nan,
91892,385264.0,,nan,
91892,385303.0,,nan,
91892,392129.0,,nan,
92554,389902.0,97258.0,But video said the opposite. First inner select would be executed,320074.0
92554,390852.0,,nan,
92554,390859.0,,nan,
92554,391273.0,,nan,
92620,390499.0,97767.0,The reason is given below,318576.0
92620,390345.0,,nan,
92620,390239.0,,nan,
92620,390867.0,,nan,
92620,391286.0,97766.0,The reason is given below,318576.0
92620,391512.0,,nan,
92620,391695.0,,nan,
92620,391531.0,,nan,
92435,389391.0,,nan,
91418,382279.0,,nan,
91418,382302.0,,nan,
92355,,,nan,
92358,388923.0,,nan,
92358,389120.0,,nan,
92358,389520.0,,nan,
92358,389662.0,,nan,
91025,379931.0,95656.0,But values in employee.super_ssn are not matching with employee.ssn right?,308437.0
91025,379928.0,,nan,
91025,380072.0,95691.0,Superssn is the foreign key which references ssn in employee table..right? Then what is this fksuperssn??,308437.0
91025,380133.0,95693.0,Thanks almost clear...maybe have to go through few examples to grasp completely 😊,308437.0
91025,380592.0,,nan,
91025,383023.0,96287.0,Add constraint can be used for adding any constraint not just foreign key.... Here foreign key is the constraint type,308437.0
91576,383736.0,96355.0,"As you can see on screen shot, I have already invoked command use 'Companydb' which is selecting the database.",318335.0
91576,383323.0,96225.0,Still gives the same error,318335.0
91576,383405.0,96263.0,Error Code: 3664. Failed to set SDI 'CompanyDB.dependent' in tablespace 'companydb/dependent'. This is the error that is coming up,318335.0
91576,383405.0,96264.0,Have you reloaded the companyDB from scratch? I am trying it on earlier loaded companyDB.,318335.0
91576,383405.0,96265.0,no have you restarted the workbench and i have not reloaded the companyDB..,305129.0
91576,383405.0,96268.0,Restarted workbench. Rerun the script. Still getting the same error,318335.0
91576,383405.0,96266.0,try refereshing the db or importing it again and you can save your worksheet.. so data will be there..,305129.0
91576,385392.0,,nan,
92075,388609.0,,nan,
92075,386975.0,96761.0,"Hi, Thanks for the reply, but i don't want to use SUBSTRING_INDEX twice",317073.0
92075,386813.0,96772.0,"yeah, that's fine you can put in a case stmt and pop out the second last word else do something else. INSTR() would work that way",306735.0
92075,386813.0,96760.0,"Hi, Thanks for the answer. Actually i want to know the second last word, and some times i may not know the second last word. So INSTR won't help me",317073.0
91495,385437.0,,nan,
91495,382803.0,,nan,
91314,381623.0,,nan,
91314,381629.0,,nan,
91314,381627.0,,nan,
91525,383246.0,,nan,
91525,382983.0,,nan,
91525,382986.0,,nan,
91525,383014.0,,nan,
91525,384032.0,,nan,
91536,383132.0,,nan,
91536,383056.0,96161.0,"it is giving null ,not working",317982.0
91536,383051.0,96159.0,i AM WONDERING IF WE NEED TO CONVERT MONTH TO MONTH N UMBER MANUALLY AND THEN USE CASE? AS IF WE USE CAST IT IS NOT WORKING,317982.0
91536,383051.0,96162.0,yes it is not covered,317982.0
91536,383051.0,96160.0,i am not sure what CAST means? dont think it was covered in any lecture,310509.0
91536,383064.0,96166.0,the values are read as string(char) but my value is in text,317982.0
91536,383064.0,96170.0,"are you giving double quotes("") ? .",306735.0
91536,383064.0,96185.0,"nope, it should , in the above query select str_to_date('2018-october-18', ' %Y-%M-%d) '2018-october-18' is text.",306735.0
91536,383064.0,96184.0,"my date column is text , so i am doing str_to_date (date_mine,format) and it returns null",317982.0
91536,383053.0,,nan,
91536,383121.0,96188.0,@Abishek use %M instead of %m. %M is for abbreviated month name. pls refer http://www.mysqltutorial.org/mysql-date_format/,306735.0
91536,383125.0,96191.0,i did that also but still not getting it...,305129.0
91536,383125.0,96190.0,"I didn't notice you should %d not %D. select str_to_date('31-July-2018','%d-%M-%Y'); use this.",306735.0
91536,383322.0,,nan,
91536,383242.0,,nan,
91536,384037.0,,nan,
91536,384375.0,,nan,
93016,391764.0,,nan,
93016,391814.0,,nan,
91535,383244.0,,nan,
91535,383061.0,,nan,
91575,383733.0,,nan,
91575,383336.0,,nan,
91575,383416.0,,nan,
91575,383524.0,,nan,
92858,391031.0,97601.0,"restart the workbench,, might work.. :)",305129.0
92858,391031.0,97600.0,its not showing any output,317558.0
92858,391034.0,97603.0,yes it means as a table with name emp already exists- Try with emp1 to check its not the error in the code.. if emp1 runs successfully means emp already exists.. to check also you can refresh the schema and check for tables and you will see the name emp is already there..,305129.0
92858,391034.0,97607.0,yes but why isnt the new table being displayed in output?,317558.0
92858,391034.0,97613.0,"to display the table you have to use- select * from emp1; or select * from emp; as create only creates the table, in order to display the results you have to use above commands. Please let me know if it works..",305129.0
92858,391034.0,97617.0,"so should the code look like: create table emp select fname, minit, lname, ssn, bdate from employee select * from emp;",317558.0
92858,391034.0,97618.0,"noo, after you created the table just run-- select * from emp;",305129.0
92858,391034.0,97620.0,welcome Josyula :) Happy Learning,305129.0
92858,391034.0,97619.0,yes it works thanx a lot,317558.0
92858,391039.0,97615.0,thanx,317558.0
92858,391047.0,97614.0,yess got it thanx,317558.0
92858,391047.0,97621.0,"yes. If you want to display data, you hv to write select * from emp.",310511.0
92858,391047.0,97616.0,will the table not be displayed automatically on writing this code?? do i have to write desc emp; to view table always??,317558.0
92858,391055.0,,nan,
92858,391109.0,,nan,
92858,391141.0,,nan,
92858,391189.0,,nan,
92858,391672.0,,nan,
92860,391049.0,97611.0,yes thanx,317558.0
92860,391054.0,,nan,
92860,391052.0,,nan,
92860,391037.0,97610.0,Check this out:- https://stackoverflow.com/questions/7979912/difference-between-float2-2-and-float-in-mysql,301652.0
92860,391037.0,97612.0,thanx,317558.0
92860,391037.0,97609.0,in ur example wat does numbers within bracket signify after the float,317558.0
92401,389145.0,,nan,
92401,389170.0,,nan,
92401,389189.0,,nan,
92401,389249.0,,nan,
92401,389517.0,,nan,
92401,389665.0,,nan,
91697,384043.0,96367.0,we are changing column name for primary key.so i thought we can do foreign key constraints,306996.0
91697,384166.0,,nan,
91956,385653.0,,nan,
91956,385746.0,,nan,
91956,385672.0,,nan,
91335,381736.0,,nan,
91335,381752.0,,nan,
91335,381755.0,,nan,
91335,382252.0,,nan,
91335,382292.0,,nan,
91335,381777.0,,nan,
91335,382505.0,,nan,
91335,382583.0,,nan,
92123,387408.0,,nan,
92123,387385.0,,nan,
92123,389522.0,,nan,
92131,387491.0,,nan,
92131,387503.0,,nan,
92131,387458.0,96768.0,yes.. :),316349.0
92131,387458.0,96756.0,thanks is this the same way you did it for creating the Tables and importing the CSV files?,316036.0
92131,387510.0,,nan,
92131,388627.0,,nan,
92131,389409.0,,nan,
92131,391702.0,,nan,
91786,384396.0,,nan,
91786,384451.0,,nan,
91786,384918.0,96456.0,"Sorry I got the answer , I was using sql keyword ""desc"" so due to which i got compilation error.",301108.0
91786,384454.0,,nan,
91240,381430.0,95894.0,What will be the output of the following set of queries Alter table employee Add col varchar(15) Default ‘upgrad’ ; Select * from employee Where col = ‘upgrad’ ;,308782.0
91240,381430.0,95897.0,well you are creating a new column called 'col' then you are telling the database engine to set the value of this column to 'upgrad' for all existing rows in the table you are also saying that if any new rows are created then set that row's 'col' column to 'upgrad' UNLESS the user has provided a value for 'col' column if user provides value of col=null then that is considered that the user provided a value and the default of 'upgrad' will not be stored so to repeat again - in the above query you are adding a NEW Column 'col' and setting the value to 'upgrad' for all the existing rows and for all future rows where the user does not explicitly provide a value for 'col' column you can see this behavior if you type 'desc employee' you will see for column 'col' it will say default 'upgrad',300694.0
91240,381583.0,95969.0,"then why does it show me null values , i will paste the code and output in a while",308782.0
91123,381604.0,95970.0,"The year is sorted in desc order which means youngest is on top ! I made a mistake in thinking that small year number means smaller age, which is obviously wrong !!",300717.0
91123,380678.0,,nan,
91123,380707.0,,nan,
91123,380716.0,95764.0,Can you tell me which module has this quizz ?,317991.0
91123,380716.0,95769.0,"Advanced SQL, Date Manipulation quiz question",300717.0
91123,380716.0,95772.0,sorry haven't reached there yet,317149.0
91123,380716.0,96247.0,Yes you can do you have to just change the command by asc not asce which is you used it wouldn't execute the command and its throws an error. I tried both the command it works; select * from employee order by extract(year from bdate) desc; select * from employee order by extract (year from bdate) asc;,308639.0
91123,380861.0,,nan,
91123,380890.0,,nan,
91123,380963.0,,nan,
91123,381180.0,95868.0,This is a quiz question and the answer is wrong. Advanced SQL DAte Manipulation session.,300717.0
91123,389524.0,,nan,
91019,379901.0,95646.0,yes,308437.0
91019,379926.0,95650.0,Thanks worked,308437.0
91019,379926.0,95652.0,But in the beginning of script file there is already a command which says safe is set to 0... Still we have to disable in preferences why?,308437.0
91019,379926.0,95767.0,feel free to upvote my answer then ;-) I prefer to do it in preference as it is via the UI and feels more global,300694.0
91019,380012.0,95683.0,"No Madhusudhan, I did not make any changes to the settings or preferences.",300717.0
91019,380012.0,95680.0,U have to execute this and also disable this option in preferences right??,308437.0
91019,380409.0,,nan,
91019,380591.0,,nan,
91019,389137.0,,nan,
91019,382263.0,,nan,
91855,384999.0,,nan,
91855,385098.0,,nan,
91321,381609.0,95934.0,thank you for the reply.. ssn is added back..but the pno is not added with the above code,305335.0
91321,381609.0,95944.0,ok..thank you,305335.0
91321,381609.0,95939.0,"It worked for me with the exact code. Click on the ""works_on"" table on the ""Schemas"" panel and click on ""Refresh All"". Check if it appears after this.",313826.0
91321,381624.0,95935.0,thank you for the reply..so how can we check if the foreign key is a primary key for some column in table?..like in this case how can we add back pno?,305335.0
91321,381655.0,95942.0,ok.. thank you..,305335.0
92178,388381.0,96859.0,Thanks got it. Where is this file link provided in the module/session?,311857.0
91254,381356.0,95890.0,Gr8,318770.0
91254,381356.0,97500.0,"I am also getting the same error, (Error code 1064-- Error in SQL syntax.). How did you fixed it?",306012.0
91254,381422.0,,nan,
91254,381436.0,,nan,
92051,386534.0,,nan,
92051,386549.0,,nan,
92051,386865.0,,nan,
92051,388958.0,,nan,
91242,381280.0,,nan,
91242,381282.0,,nan,
91895,385302.0,,nan,
91895,385213.0,,nan,
91568,383737.0,,nan,
91568,383272.0,,nan,
91568,383333.0,,nan,
92743,390650.0,97559.0,Thanks Ram for replying.. can you help me understand better as to where we use fk_super_ssn?,318079.0
92516,389714.0,,nan,
92516,389848.0,97243.0,thanks it worked,310504.0
92516,389712.0,97240.0,"Thanks, it worked",310504.0
92516,389718.0,97241.0,thanks it worked,310504.0
92516,389776.0,97242.0,thanks it worked,310504.0
92516,389893.0,,nan,
92516,389901.0,,nan,
92516,390045.0,,nan,
93162,392262.0,,nan,
92670,390416.0,,nan,
92670,390750.0,97704.0,That probably is the case. Great that you could solve it by yourself.,319721.0
92670,391700.0,,nan,
93006,391688.0,97827.0,Can you please share the screen shot of error,303673.0
93006,391688.0,97825.0,"i have tried with , as well still not getting output its showing error",317558.0
93006,391715.0,,nan,
93006,391809.0,97871.0,"remove , after day and try .. This time it should get executed",303673.0
93006,391809.0,97872.0,"remove, from the last column (day) in the selection",319056.0
93006,391809.0,97937.0,"yes if I remove the comma after day then its getting executed.. but why is it so?? and why do i have to put , after every line in this code?",317558.0
93006,391809.0,98435.0,"The commas are a part of the syntax, we need to use them/",319721.0
91853,384946.0,,nan,
91853,385105.0,,nan,
91853,384957.0,,nan,
91431,382331.0,96057.0,yes,300690.0
91431,382329.0,96109.0,No.there is no column as `dno`,300690.0
93401,393209.0,,nan,
93570,394132.0,,nan,
93570,394122.0,98638.0,nope. Data at the column is getting truncated.,303085.0
93570,394122.0,98649.0,thank you,303085.0
93570,394122.0,98645.0,"There are null values at rows and also data is too long. LOAD DATA infile 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Assignment/xxxxxx.csv' INTO TABLE `table name` FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '' LINES TERMINATED BY '\r\n' IGNORE 1 LINES;",313200.0
93570,394151.0,,nan,
102148,,,nan,
102142,437117.0,,nan,
102142,437071.0,,nan,
102145,437109.0,,nan,
123479,538391.0,,nan,
92448,389374.0,,nan,
92448,389382.0,,nan,
92448,389414.0,,nan,
92448,389725.0,,nan,
92448,389810.0,,nan,
92448,390039.0,,nan,
92448,392840.0,,nan,
90160,375573.0,94840.0,"I'm trying to wrap my head around this concept. When we write server-side scripts for a website, say to collect data from a form, are we dealing with transactional databases? If so, this means that the data from in those databases are archived and stored in a data lake to make way for new data, right?",306733.0
90160,375573.0,94846.0,"Yes, they are archived and store in a data lake which is called as data warehouse.",319696.0
90160,375616.0,94848.0,"No transactional databases are also used by internal members of an organisation . But the work they do with this data is different from the work they do with data warehouse. for example : Transactional data is also called OLTP (Online Transaction processing) An example of OLTP system is ATM center. Assume that a couple has a joint account with a bank. One day both simultaneously reach different ATM centers at precisely the same time and want to withdraw total amount present in their bank account. However, the person that completes authentication process first will be able to get money. In this case, OLTP system makes sure that withdrawn amount will be never more than the amount present in the bank. The key to note here is that OLTP systems are optimized for transactional superiority instead data analysis. Other examples of OLTP system are: Online banking Online airline ticket booking Sending a text message Order entry Add a book to shopping cart",319696.0
90160,375616.0,94847.0,"I've got a rough idea of transactional databases. But, what I don't get is how where they show up in a business. For example, from the content in the videos I've come to understand that data warehouses are typically things only internal members of an organization, especially analysts interact with. But, who interacts with the transactional database? Is it another group of people from the organization or is it customers? And, who creates these transactional databases?",306733.0
90160,376101.0,95886.0,"DeNormalization is done in Data warehousing inorder to avoid lot of joins between tables. Joins when dealing with large amount of data and aggregation is a performance overhead. Normalization is better suited for Transactional Dbs where typically there is an update to one table. but it takes a toll on the DB system when you join all the different tables to get an aggregated value. But when you denormalize, all the data is captured in a single table. Aggregation part will be simpler as you query a single table and improves performance. Again Denormalization has its rules around it. you dont usually denormalize everything. this is where we really goof up in many DW designs.",300708.0
90160,377612.0,,nan,
90160,382379.0,,nan,
90160,382413.0,,nan,
91382,388985.0,,nan,
91382,382081.0,,nan,
91382,382084.0,,nan,
91382,382108.0,,nan,
91382,382163.0,,nan,
91382,382197.0,,nan,
91382,382227.0,96042.0,absolutely no,315679.0
91382,382267.0,,nan,
91382,382290.0,,nan,
91382,382304.0,,nan,
91382,382058.0,96052.0,"Date is a conformed Dimension, a dimension table that could be used across multiple dimensions and facts",301555.0
91382,382058.0,96041.0,"Date is a dimension and can be used for analyzing the fact. For example if you have sales figure as fact and date as dimension then you can analyse sales figure daily, monthly, quarterly or yearly. Generally we create date dimension as role playing dimension and store multiple date ids in the fact tables.",315679.0
91382,382596.0,,nan,
91382,382423.0,,nan,
91382,383000.0,,nan,
91382,383177.0,,nan,
91382,386788.0,,nan,
92452,389411.0,,nan,
91436,382335.0,96526.0,It worked. Thanx..,311117.0
91436,382336.0,96525.0,It worked. Thanx..,311117.0
91467,382998.0,,nan,
91467,390734.0,,nan,
91467,383259.0,,nan,
91501,382828.0,,nan,
91501,383004.0,,nan,
91517,382978.0,,nan,
91517,382966.0,,nan,
91517,382995.0,,nan,
91517,392189.0,,nan,
92533,389812.0,97265.0,okay but the question need summary so we can write either way ...,319969.0
92533,390504.0,,nan,
92533,392898.0,,nan,
92534,391258.0,,nan,
92534,391083.0,,nan,
90756,378198.0,,nan,
90756,378205.0,,nan,
90756,378285.0,,nan,
90756,378432.0,,nan,
90756,381419.0,,nan,
91504,383277.0,,nan,
91504,383348.0,,nan,
91504,384820.0,,nan,
90346,376638.0,,nan,
90346,376642.0,,nan,
90346,376654.0,,nan,
90346,376652.0,,nan,
90346,380485.0,,nan,
91594,383459.0,96332.0,"No, its always dimensions that are decided first. The facts",301555.0
91594,383459.0,96334.0,Oh is it. Thanks for updating. I’ll read it a bit more.,318328.0
91594,383459.0,96333.0,And then the facts,301555.0
91594,383474.0,96331.0,or even you could have a surrogate key in fact table instead of composite keys. But the concept remains the same,301555.0
91594,383577.0,,nan,
91594,383598.0,,nan,
91594,383728.0,,nan,
91594,383973.0,,nan,
91594,384829.0,,nan,
91638,388989.0,,nan,
91638,383686.0,,nan,
91638,383730.0,,nan,
91638,388483.0,,nan,
91638,389474.0,,nan,
91625,383626.0,,nan,
91625,383984.0,,nan,
91625,389478.0,,nan,
90508,377548.0,,nan,
90508,380840.0,,nan,
90508,377638.0,,nan,
91217,381107.0,,nan,
91217,381110.0,,nan,
91217,381121.0,,nan,
91217,381185.0,,nan,
91217,381413.0,,nan,
91847,384874.0,,nan,
91847,386899.0,,nan,
91847,385418.0,,nan,
91126,380944.0,,nan,
91126,380757.0,,nan,
91126,380681.0,95761.0,Had the same question. thanks,317149.0
91126,380696.0,,nan,
91126,380740.0,,nan,
91126,381026.0,,nan,
91126,381402.0,,nan,
91126,381274.0,,nan,
91126,383633.0,,nan,
91858,385097.0,,nan,
91249,381431.0,,nan,
91249,381382.0,,nan,
91361,,,nan,
91046,380104.0,,nan,
91046,380126.0,,nan,
91046,380497.0,,nan,
91046,380502.0,,nan,
91046,380704.0,,nan,
91046,380738.0,,nan,
91046,380760.0,,nan,
91046,380793.0,,nan,
91046,381019.0,,nan,
91046,381175.0,,nan,
91046,381407.0,,nan,
92156,387820.0,96797.0,"Ok, I tried with quotes. Thanks for the correction.",311502.0
92156,387820.0,96793.0,I think those are not quotes. those are back ticks. that should work fine.,312746.0
92156,387814.0,,nan,
92156,387896.0,,nan,
92156,388067.0,,nan,
93288,392702.0,,nan,
93288,392618.0,98246.0,I have both installed in my laptop. But still i'm facing this issue,315831.0
93288,392703.0,,nan,
93237,392432.0,,nan,
93237,392447.0,,nan,
91508,382875.0,,nan,
91508,382879.0,,nan,
91508,382976.0,,nan,
91508,382988.0,,nan,
91508,383349.0,,nan,
91425,382277.0,96047.0,how many entries do you have in address column of employee table.? I just have 8 which sums up to 13017.,304696.0
91425,382277.0,96048.0,Got my answer.Thanks.!,304696.0
91425,382277.0,97632.0,how did u get your answer ....,300684.0
91425,382278.0,,nan,
91425,382356.0,,nan,
91425,382522.0,,nan,
91425,382661.0,,nan,
91425,382743.0,,nan,
91425,386134.0,,nan,
91425,389015.0,,nan,
89461,371188.0,96037.0,"Just above the graded question, there is this instruction ""Please use the following schema to answer the questions that follow in this section."" Below that there is 'Graded Questions : datasheet"". We need to download that and create the tables from this in order to answer the graded questions.",300717.0
89461,371188.0,93919.0,Thank you for the information.,311254.0
92477,389530.0,,nan,
92477,389541.0,97166.0,**Not sure how to solve the problem with foreign key constraints. I removed the schema and added it again.,312376.0
92477,389579.0,97167.0,Thanx. I removed the the schema and added it back. It worked,312376.0
89520,371635.0,94012.0,"There are specific queries to be run in the quiz itself, for the same entities the attributes are different in the quiz. For eg, for a given employee SSN birthdates are different.",318381.0
89520,371635.0,94398.0,"Please note that the tables being referred to in this segment and the subsequent segments are the same as those in the comapnydb. Please refresh your schema here. Looking at the address field in the employee table, you would notice that all the employees reside in ""Fondren, Houston, TX"". Consider the integer in the address field as house number. This is what is given before the questions but there is only one entry with address containing Fondren, Houston, TX. If I consider integers in all the addresses for SUM, the answer is not among the given options. Am in missing something?",318329.0
89520,371635.0,94400.0,Notice that it's been mentioned to refresh company db schema for all below questions Please note that the tables being referred to in this segment and the subsequent segments are the same as those in the comapnydb. Please refresh your schema here.,318329.0
89520,371635.0,94538.0,I posted this after clearing my database and re-initializing.,318381.0
89520,371635.0,94540.0,"Nope, i feel there's some issue too. I'd raised a support ticket for it.",318381.0
89520,371635.0,94539.0,"@Avneesh, are you able to get them correctly now? I still feel there is some problem.",318329.0
89520,377090.0,,nan,
92195,388336.0,,nan,
92195,388360.0,,nan,
92195,388857.0,,nan,
92195,389201.0,,nan,
91494,382788.0,96398.0,"The question is still confusing. it says age as of 30-sep-2018. That statement doesnt exactly tell me that i have to consider the year alone. If that was the case the question shouldnt have mentioned age as of 30 Sep 2018. In real business context, age as of Date means of as on Date what is the age. OR am I missing something here? Consider an alternate context, Let us say that we have a person born on 31-12-2017. Should I consider baby to be 1 year old as of 1-Jan-2018? Your explaination suggests we should consider baby's age to be one year.",300708.0
91494,382788.0,96400.0,"I agree that the question should have been worded better and a lot of confusion could have been avoided if they had not mentioned ""as of 30 September 2018."" in the question. For me, ""To find the age of any person, you subtract the current year from the year of birth."" was the hint that prompted me to use just the year of birth for age calculation.",313826.0
91494,382815.0,,nan,
91494,382837.0,96431.0,the relevance is that it is a marker. So if it said as of 30-Sep-2017 then the solution would be different accordingly,300694.0
91494,382837.0,96397.0,What is the relevance of 30-Sep-2018 in computing Age? The question says Age as of 30-Sep-2018,300708.0
91494,388230.0,97115.0,what is your output in age column? Also did u check what year(bdate) is retrieving?,300708.0
91494,388230.0,97121.0,it's was giving me null values as age,308639.0
91494,388230.0,97262.0,Try by giving where condition which satisfies all the rows & Columns... I've faced similar issue and it is been rectified after giving where clause,303673.0
91494,388230.0,97263.0,"Use - where dno in (1,4,5) at the end it will display the values",303673.0
91494,388230.0,97998.0,"Nakul your code is correct , just use: SET SQL_SAFE_UPDATES = 0; before updating the column",315560.0
91494,388230.0,98375.0,You are using safe update mode .To disable safe mode. begin with :SET SQL_SAFE_UPDATES = 0; will solve your case,300684.0
91494,384477.0,,nan,
91494,385445.0,,nan,
89627,372002.0,94128.0,Sorry have not reached that far.,318358.0
89627,372002.0,94127.0,"Advanced SQL , Graded Question-II",310419.0
89627,373300.0,,nan,
91531,383022.0,,nan,
91531,383074.0,,nan,
91531,383833.0,,nan,
93689,394876.0,,nan,
93689,395444.0,,nan,
93689,395039.0,,nan,
92538,389828.0,97233.0,"No its not correct, i have tried also its include alphabet part and we have to consider only integer part.",305847.0
92538,389828.0,97493.0,Use substring_index instead of substring.,308434.0
92538,389828.0,97949.0,yes Substring_index is the right way to use,311803.0
92538,389843.0,,nan,
91546,383115.0,,nan,
91546,383599.0,,nan,
91546,383707.0,,nan,
91929,388287.0,,nan,
91929,385481.0,,nan,
91929,385429.0,,nan,
91929,385439.0,96500.0,Did you also drop the table?,318335.0
91929,385439.0,96508.0,"i created db with different name, so this problem was not there for me :), you can do the same..",305129.0
91929,385439.0,96515.0,"I am trying the same, but in that too it says that column employee already exists. Can you guide me in this?",318335.0
91929,385439.0,96516.0,"yes, how are you trying to create new? They have provided a dataset-- download it.. then open workbench..and click on file-- then run sql script.... then select the dataset--and before clicking on run in ""Default schema name""- manually enter the new db name for example companydb2 and then click run.... after all this process.. refresh the schemas.. or restart the workbench..",305129.0
91929,385439.0,96521.0,"Did exactly as you told, this time that duplication of employee table didn't arise. But its taking a lot of time in fetching data for tables and other columns. Was that the case for you too?",318335.0
91929,385439.0,96533.0,Welcome bro.. :) Happy learning ;),305129.0
91929,385439.0,96531.0,I got it. Thanks Abhishek!,318335.0
91929,385663.0,,nan,
91929,386112.0,,nan,
91929,391282.0,,nan,
93045,391804.0,,nan,
91007,379934.0,,nan,
91007,380063.0,95681.0,How have you done it?csv files would be imported in table?,300690.0
91007,380063.0,95684.0,"I have opened the sql file, ran it so that all the schema and tables are created and data is populated. The same sql file has data as well.",318329.0
91007,380063.0,95713.0,yes,318329.0
91007,380063.0,95708.0,importing and populating data are same thing?,300690.0
91007,381206.0,95846.0,after doing it each column of employee table is showing null.,300690.0
91007,381206.0,95848.0,ok.I got it.,300690.0
91577,383487.0,,nan,
91577,383347.0,96227.0,First time i interpreted as Jan 1 should come before Jan 2 . when I query the tables I interpreted as Vice versa. TA should clarify this. Can't loose marks for such a confusion questions. this can be put up in a clearer way.,306735.0
91577,383401.0,,nan,
91577,384142.0,,nan,
91577,383743.0,,nan,
91614,383539.0,96318.0,"I have replaced ( ) with "" "" and removed backtick of column name.stillit is showing error as I am working in safe update mode.To disable safe update mode I need to toggle preference option<- SQL Editor and reconnect.Not understanding.",300690.0
91614,383539.0,96325.0,after doing that hno column is remaining entirely null.but I already set a value to hno column.,300690.0
91614,383539.0,96324.0,checkout the below link for similar issue and resolutuon https://learn.upgrad.com/v/course/208/question/91673,313826.0
91614,383593.0,96319.0,"I have replaced ( ) with "" "" and removed backtick of column name.stillit is showing error as I am working in safe update mode.To disable safe update mode I need to toggle preference option<- SQL Editor and reconnect.Not understanding.",300690.0
91614,383677.0,96317.0,after doing so it is showing error as I am working in safe update mode.To disable safe update mode I need to toggle preference option<- SQL Editor and reconnect.Not understanding.,300690.0
91614,383746.0,,nan,
91614,384366.0,,nan,
91963,386375.0,,nan,
91963,386793.0,,nan,
91963,386506.0,,nan,
91963,385749.0,,nan,
91963,389757.0,,nan,
91701,383969.0,96337.0,Yes... Makes sense now.. Thank you:),305845.0
91700,383972.0,,nan,
91700,383981.0,,nan,
91700,389012.0,,nan,
92154,389754.0,,nan,
92154,387731.0,,nan,
92154,387764.0,96770.0,Thank you,316416.0
92154,387901.0,,nan,
92154,389010.0,,nan,
92154,389103.0,,nan,
92154,391278.0,,nan,
92154,392008.0,,nan,
91751,384170.0,,nan,
91751,384217.0,,nan,
91751,384098.0,,nan,
91994,385952.0,96575.0,It was so easy.. I thought whole lot of advanced concept. but finally got the easiest and very basic way. I spend 1 hr on this ... btw thanks for your answer..,312746.0
91994,386065.0,,nan,
91673,383856.0,,nan,
91673,383934.0,,nan,
92009,386050.0,96639.0,Even after doing as u said it is showing the same error,300690.0
92009,386050.0,96674.0,"There is a space between ',' and the word Houston. Similarly. A space between ',' and TX1. Can you adjust the query from that perspective.",311502.0
92009,386440.0,96773.0,Please recheck..,312746.0
92009,386440.0,96637.0,No.They are seperated by a dot . as given.,300690.0
92009,386844.0,96755.0,"The code I have used is as follows update employee set hno=substring_index(address,' ',1) where address like '%Fondren.Houston.TX%'; select * from employee;",300690.0
92009,389942.0,,nan,
91348,381919.0,96006.0,I actually took care of both these points.,316202.0
91348,381837.0,,nan,
91348,383211.0,,nan,
91228,381398.0,98172.0,"Hi Pulkit, I wrote query and retrieved result. I used abs for calculating distance and then average. But my answer doesnt matches any of the option already available. Not sure what to do.",318362.0
91228,381546.0,,nan,
91235,381418.0,,nan,
91235,381547.0,,nan,
91235,381603.0,,nan,
94291,397162.0,99413.0,"Yes, the entire column gets updated, use, str_to_date(DATE, '%d-%M-%Y')",319721.0
94291,397133.0,,nan,
93329,392800.0,,nan,
93329,392774.0,,nan,
92518,389726.0,97206.0,"In somem sql version where on PK is required, you can put a where which satisfies all rows in the table. Also null at the last just signifies there is no row, that is the end, it does not that there is a null. hope it helps!",304813.0
92518,389726.0,97202.0,"select * from employee; alter table employee add name varchar(50); update employee set name = concat(fname, ' ',minit, ' ',lname);",303673.0
92518,389726.0,97203.0,is the update query successful? or are you getting any error saying to add a where clause?,304813.0
92518,389726.0,97205.0,the query is succesful.. there is no error when i am trying to update one row with where clause it is working but it is not the case for all rows it is displaying null values also i am unable to figure it out why the last row values are null in the table,303673.0
92518,389726.0,97209.0,No the issue still prevails.... Given the where condition as Salary > 20000 but it is displaying still the null values,303673.0
92518,389726.0,97225.0,you could also use ssn > '1',304813.0
92518,389726.0,97217.0,"Got it when given where condition as where dno in (1,4,5)",303673.0
92518,389847.0,,nan,
92518,389787.0,97606.0,This explains the null values in the last row well.,319721.0
92518,391044.0,,nan,
93453,393480.0,,nan,
93453,393614.0,,nan,
93453,393551.0,,nan,
93423,393311.0,,nan,
93423,393312.0,,nan,
93423,393567.0,,nan,
92413,389211.0,97070.0,i don't think you need window function here.. this is simple straight forward absolute difference between hno and the hno of interest which can be brought up through select clause..,316349.0
92413,389211.0,97069.0,"hi yeah I understand but, w.rto the question we need cumulative difference b/w two rows. In this case above method will not work i believe.",317410.0
92413,389211.0,97171.0,hi absolute difference would work as hemant suggested select abs(numberA - numberB) as abs_diff from your_table,319056.0
92413,389257.0,,nan,
92574,389954.0,,nan,
92574,390018.0,,nan,
92574,390010.0,,nan,
92574,390357.0,,nan,
92574,390023.0,,nan,
92412,389206.0,,nan,
92412,389208.0,,nan,
92412,389205.0,,nan,
92412,389454.0,,nan,
92412,389456.0,,nan,
92412,389752.0,,nan,
92906,391185.0,,nan,
92906,391223.0,97671.0,Thanks it worked!,311868.0
92906,391300.0,,nan,
92908,391252.0,,nan,
92908,391210.0,,nan,
92908,392094.0,,nan,
92750,390646.0,,nan,
92750,390706.0,,nan,
92750,390992.0,,nan,
92809,390864.0,,nan,
92809,390874.0,,nan,
92809,390903.0,97635.0,This worked. Thanks,312608.0
92809,390922.0,,nan,
92809,391058.0,,nan,
92809,391419.0,97929.0,"and what if it is still showing an error even after doing both mentioned things, then what can be done to rectify it?",316545.0
92809,391808.0,,nan,
93189,392317.0,,nan,
93189,392252.0,,nan,
93189,392453.0,,nan,
93368,392948.0,,nan,
93368,392954.0,,nan,
93415,393191.0,,nan,
93415,394240.0,,nan,
93413,393196.0,,nan,
93413,393314.0,,nan,
93434,393306.0,,nan,
93434,393337.0,98632.0,no error,302735.0
93434,393348.0,,nan,
93434,393565.0,98633.0,its a logical error,302735.0
93434,393680.0,,nan,
93434,393726.0,98634.0,"Yes Ankit, I figured it out later. Thanks a lot",302735.0
93577,394174.0,,nan,
93577,394281.0,,nan,
83382,342423.0,,nan,
83382,342414.0,88372.0,thanks!,310585.0
82117,335500.0,87059.0,"In this again , more positive values in dark green less positive values in light green Similarly for losses - more negative in dark red and lesser negative in light red",308437.0
82117,335500.0,89349.0,"Were you able to do this? I can only get one shade for ex: all positive in green and all negative in red, using hightlight cell rules. I tried doing gradation using color scale but didn't work as expected.",311857.0
82117,335500.0,90696.0,Sreenath - I believe that is not what Madhusudhan was asking,300694.0
82117,335417.0,87037.0,In the same column am not able to do this simultaneously.....,308437.0
82117,335417.0,87040.0,how you determining the profit and loss for a particular cell ?,301648.0
82117,335417.0,87093.0,"Go to Conditional Formatting --> select Manage Rules When the Conditional Formatting Rules Manager window appears, click on the ""New Rule"" button to enter the first condition. select Format only cells that contain as the rule type Then select Cell Value in the first drop down, greater than in the second drop down, and enter 0 in the final box. Then you have to set the colour so use this -->When the Format Cells window appears, select the formatting conditions that you wish to apply. Then click on the OK button. Accordingly you can set for positive Number (Profit).",301648.0
82117,335456.0,87109.0,really...is this is an assignment..?,300693.0
82117,335527.0,,nan,
82117,336036.0,87307.0,No this is a different issue,308437.0
82117,336389.0,,nan,
82117,336393.0,,nan,
82117,336995.0,,nan,
82117,354388.0,,nan,
82117,358013.0,91673.0,"Hi Somnath, your different values are not in the gradient of Red but it actually shades of Red.. Once you choose a conditional formatting of gradient, the other rule will override. If you try to use two color scales color will be from green to red. But not solely green gradient or red gradient.",315028.0
82117,358013.0,91733.0,"HI Rahul , This answer was in reference to Madhusudhan question ""Multiple conditional formatting in one column ? Like all profits with shades of green and losses with shades of red "".Actually there was no mention of gradient in his question. Regards Somnath",314617.0
82117,359903.0,,nan,
82152,335718.0,,nan,
82152,341457.0,,nan,
86347,355794.0,,nan,
86347,358517.0,,nan,
86347,359858.0,,nan,
83410,342591.0,,nan,
82535,338638.0,87880.0,You are right even i am not happy with this solution ...rather,300687.0
82535,338638.0,87790.0,"Hi Maya, Thank you for your reply. In this case, the color Green doesn't get Darker with the value. Correct me if I am wrong!",311160.0
82535,338815.0,,nan,
82535,338823.0,,nan,
82535,338921.0,,nan,
82535,339138.0,,nan,
82535,338856.0,87804.0,Thanks a lot :),311160.0
82535,338856.0,87881.0,"Hi Alok,when i try to color scale after the last two condition all the color from my screen gets disappeared may i know where am i going wrong .i have cleared all rules before doing .i have attached the screen shot below thanks",300687.0
82535,338856.0,87883.0,"@Maya, Please follow my screen shot. Order matters while applying the rules. Please paste your screen-shot of rules if you face any issue.",312746.0
82535,338856.0,88299.0,Hi Alok.i have attached the screen shot in the below can you please advise thanks,300687.0
82535,338856.0,89053.0,Is there any other way to do this problem. As for the mid percentile terms we get a mixture of red and green i.e orange.,304319.0
82535,342078.0,88739.0,You have give the minimum value as -1. So only values between -1 and 0 will be shown as red.,304319.0
82535,342078.0,89076.0,but i have given whether the cell value<0 and greater than or equal to zero .pls refer the attached above screen shot maybe you are referring the old one,300687.0
82535,348079.0,,nan,
83381,342385.0,,nan,
83381,342427.0,,nan,
83381,342664.0,88578.0,it is general only,310585.0
83381,342664.0,88893.0,"Are you able to edit Dolllar sign manually? If yes, please use Find and Replace. If its just appearing in the cell then it is because of formatting. In that case you need to update the format of numbers.",301559.0
83381,346658.0,,nan,
83381,344295.0,,nan,
86041,352525.0,,nan,
86041,352544.0,,nan,
86041,352859.0,,nan,
86041,352992.0,,nan,
87471,361561.0,,nan,
87471,361594.0,,nan,
87471,364084.0,,nan,
85740,352405.0,,nan,
85740,352148.0,,nan,
85742,352523.0,,nan,
86201,354456.0,90708.0,that's Ok but I want to understand if any formula exist to to count cells highlighted by specific colors. Counting by filtering is tedious when you have multiple colours in database.,313767.0
86201,354456.0,90801.0,"Hi, As per my understanding there is no built-in functionality on count the cell by color, apart from doing filter by using filter by color which helps you to display only columns/rows in color. Or you can write module.Below is the link please refer https://www.ablebits.com/office-addins-blog/2013/12/12/count-sum-by-color-excel/",318077.0
86201,354456.0,90788.0,"Hi, to my understanding, there is no in-built formula to count the cells by color. If you want to automate this, you can create a VBA Script or macro for the same. Refer to this link https://trumpexcel.com/count-colored-cells-in-excel/ for the details.",314730.0
86201,354087.0,,nan,
86201,356108.0,,nan,
86300,354937.0,,nan,
86300,356534.0,,nan,
87782,364705.0,,nan,
88142,365118.0,,nan,
88142,365212.0,,nan,
88142,365907.0,,nan,
88142,366080.0,,nan,
95872,409164.0,,nan,
82270,337771.0,87495.0,Ya sometimes it’s in audible sometimes it’s ok... no consistency,308437.0
87109,358787.0,,nan,
80126,325207.0,,nan,
80126,326921.0,,nan,
80126,327867.0,,nan,
76876,307058.0,,nan,
76876,357940.0,,nan,
87234,359631.0,91661.0,True. handling Excel becomes very difficult when the data volumes are huge.,319898.0
87234,359631.0,91660.0,I think it is possible to import data in more than one sheet changng the starting row whle importing again the the same data. but doing like this analysis can become very cumbersome.,318344.0
83571,343674.0,,nan,
83571,343675.0,,nan,
83571,343458.0,,nan,
83571,343433.0,,nan,
88278,365267.0,,nan,
83172,341536.0,,nan,
85619,351454.0,90467.0,"Alt - helps you navigate through tabs. H - indicated the home tab. Alt+H - takes you to the home tab and using the shortcuts further enables you to perform different actions. After, Alt+H if you press I, it opens the insert menu for you where you can insert rows (you press the letter R), columns ( the letter C), sheets (the letter S) or cells (letter I). But to insert stuff, you need to be in the home tab. And hence you need Alt+H first. You can explore different shortcuts by exploring different options afrer Alt.",319721.0
85619,351454.0,90460.0,I believe this is still time consuming given that we need to press 4 keys to perform the action. I just found easier approach to insert rows and columns using shortcut keys (3 keys instead of 4). Insert a row: Alt + I + R Insert a column: Alt + I + C There seems to be no fewer keys approach for deletion of rows and columns.,314730.0
85619,357937.0,,nan,
85619,351834.0,,nan,
86130,353145.0,,nan,
86130,353464.0,,nan,
80603,357943.0,,nan,
80603,328518.0,,nan,
82175,337029.0,87263.0,The question was to use raw data. Ofcourse pivot table can be used.,310974.0
82175,337029.0,87333.0,No raw data solution can be a better solution than pivot tables as they are meant to handle such scenarios.,301276.0
82175,340047.0,,nan,
82175,350815.0,,nan,
82175,350838.0,,nan,
82637,338799.0,87789.0,Checked not available,308437.0
82637,339268.0,87894.0,This is interesting Thanks ll try ☺️,308437.0
82723,339964.0,88562.0,Thanks jitendra I will check and let you know if this is resolving my query or not,307843.0
83469,342931.0,92004.0,"Hi Ram, What is column q in your averageif function.",318010.0
83469,350839.0,,nan,
86283,356243.0,91226.0,How do we derive chairs and tables are brought together or any two sub categories are brought together from this data?,305842.0
86283,357809.0,,nan,
86283,367655.0,,nan,
120683,524554.0,,nan,
119216,,,nan,
120782,524176.0,,nan,
120782,524545.0,,nan,
120681,,,nan,
119669,519514.0,119441.0,So do we have to check accuracy for the linear and non-linear model or do we have to check only for non-linear?,311466.0
119669,519514.0,119614.0,You can check for linear first and then tune hyperparameter adding non-linearity and see the accuracy again after building the model.,315028.0
119669,519513.0,119428.0,Majorly C and Gamma parameters to be used for tunning.,300721.0
119669,521024.0,,nan,
120668,523594.0,,nan,
120668,523461.0,,nan,
120650,523074.0,,nan,
120650,524882.0,,nan,
120650,523097.0,,nan,
120650,523462.0,120040.0,Hi Vinay Got cleared with confusions...Thanks a lot,308638.0
120650,523462.0,119965.0,Thanks for the Help!,315423.0
120650,523064.0,,nan,
120930,524982.0,,nan,
120930,525443.0,,nan,
120519,523521.0,120270.0,I am sure of that too. But it would still be nice to know. Not sure why TA verified this answer :-/,313515.0
120519,523521.0,120320.0,"Hi Deepak, Grader will not check, what is your execution time. If you pass your hyperparameter in the smarter way(Like C = {0.001,1,100}) then it will reduce your time. Please comment here if have any other question on this",344894.0
120519,523027.0,,nan,
119739,519480.0,,nan,
119739,518559.0,119326.0,have anyone tried in GCP?,301648.0
119739,519872.0,119445.0,"That's a neat trick. Thanks, Harsh.",319302.0
119739,519872.0,119455.0,"thanks Anand, glad it works",309211.0
119739,519872.0,119955.0,Its more than half an hour its still executing. waiting for some more time else need to kill it.,301114.0
119739,519872.0,120024.0,that's a neat explanation thank u will try it,301114.0
119739,519872.0,120015.0,"Hi Jayashree , Say 3 alpha and 3 Gamma parameter with 5 folds, the code will train the model 3*3*5 = 45 passes ; if you are not using parallel option, this would take sequentially 3.2 minutes for a single pass and about 147 minutes [ 2.45 hours] ; however if I use n_jobs=-1, since I have octa-core, the single pass takes about 3.2/8 = 0.4 minutes i.e for 45 passes it takes roughly 18 minutes. If my machine was a quad core (instead of octa core), it would take 3.2/4 = 0.8 minutes i.e for 45 passes it takes 36 minutes. Now instead of 5 folds I change it to 10 folds, the total computation would be for 3*3*10 = 90 passes and it would roughly take 36 minutes for Octa core and 72 minutes for quad core So in short, number of passes you choose depends on number of hyper params and the folds you choose and you can roughly arrive at total time taken by just choosing 1 hyperparameter each and not select any folds (default is 3 folds ), so you can estimate the time taken for a single hyper parameters (1 gamma and 1 C value) in 3*1*1 = 3 passes and extrapolate the time taken on your machine using gridsearchCV for the same single hyperparamater, use the n_jobs=1 and you will certainly see that job processes the passes in parallel Hope the estimation technique I mentioned above helps. You do the math for your scenario and can precisely estimate total time that's taken by your Gridsearch. Also please use this on 20% of the subset. [even with single hyperparameter gamma / alpha, if you try to run this on entire dataset, the computation will take really longer and you will additionally need to factor that in as well; the above time that I indicated was on 20% subset and not the entire dataset ] Thank you, Harsha",309211.0
121000,525523.0,,nan,
121000,525530.0,,nan,
121000,525518.0,,nan,
121000,525617.0,,nan,
121000,525890.0,,nan,
121000,525995.0,,nan,
121000,525964.0,,nan,
120001,520047.0,,nan,
120004,520052.0,,nan,
120004,520074.0,119482.0,"Thanks for the information, pretty good. I am worried, hope it won't upload automatically to google cloud, otherwise it will be a problem for assignment submission.",312019.0
119218,515190.0,119016.0,"I understand that Kaggle/Colab would help here, but I want to understand from TA's perspective when he/she is testing/running submission code on their environment.",306248.0
119218,515190.0,119210.0,"I understand that Kaggle/Colab would help here, but I want to understand from TA's perspective when he/she is testing/running submission code on their environment.",306248.0
119218,515190.0,119309.0,"Hi Arpit, If you have 8GB RAM then it will trained within 30 Min.",344894.0
119218,515190.0,119472.0,Can we do the parallel computation for different C and gamma values together in python with jupyter note book ?,312019.0
119218,525894.0,,nan,
120125,520748.0,119687.0,How do you run on colab..so many ppl are talking about colab..can you throw some light on this plz?,307710.0
120125,520748.0,119688.0,You can import the Assignment notebook and run in colab. colab run in google server so it takes less amount of time.,318780.0
120125,520748.0,119689.0,you mean input the ipynb file to Google colab and it will run over there? is that it ?,307710.0
120125,520748.0,119829.0,yes. Or you can create your own ipynb file there and import the csv files in your drive,318780.0
120125,520858.0,119609.0,"Yes.. but it is taking time with k = 5 as well.. so, should i understand that we should use k = 4 or something? or how about choice of the gamma values?",305335.0
120125,520858.0,119611.0,Less no of hyperparameter? We have to take the 2 hyperparater c and gamma. Are you talking about running with less no of values for these parameter? With even 3 folds and 2 values it is taking more than hour to run. TA please clarify.,315028.0
120125,520858.0,119735.0,"Please use rbf as kernel, and take 3 values for each hyperparameter. I am running on 8GB sytem. It tooks only 20 min.",344894.0
120125,520858.0,119658.0,I have considered that statemet. At the end I am training with 14% of the data. Still it is taking time,318780.0
120125,524617.0,,nan,
120125,521340.0,,nan,
118497,510989.0,118535.0,Did you get a chance to build the model. It is computationally very heavy. I am not getting any output from gridsearchCV method. Even after working on 8K training data.,310511.0
118497,510989.0,118998.0,not yet rajarshi. I'll probably do it today,318329.0
118497,512011.0,,nan,
118497,512516.0,,nan,
118497,525891.0,,nan,
119768,519407.0,,nan,
120435,522011.0,,nan,
120435,522208.0,,nan,
120732,523670.0,119979.0,reduce too 100 in C ? Will that be fine?,315560.0
120732,523670.0,119986.0,"What i meant is that the no of values you are passing in the C and Gamma....Try to reduce that number For example, instead of passing 5 values you can use 4 or 3 and check the performance",311254.0
120732,525320.0,,nan,
119336,516168.0,,nan,
119336,516486.0,,nan,
119336,519873.0,,nan,
121050,525838.0,,nan,
121050,526205.0,,nan,
119809,,,nan,
119197,515202.0,118931.0,"hi Ashish, My question is not about the technical difference between GridSearchCV and finding the optimal hyperparmeters through hit and trial. my question is whether hit and trial approach is acceptable in this assignment? and there is a significant time difference in hit and trial (only 3 values for each hyperparameter) and using GridSearchCV. I could complete the former but latter still going on on Colab.",311686.0
119197,515276.0,118936.0,ok. noted. what are the values of hyperparmeters that you are passing in your 3-fold validation? what values for gamma and C?,311686.0
119197,516826.0,,nan,
119197,519870.0,,nan,
120449,522021.0,,nan,
120449,521998.0,119862.0,Same problem for kaggle submit ? But as per assignment code e should not submit public sites ? Could you please clarify ?,312019.0
120780,524023.0,120067.0,"Yes for Kaggle only i am asking, how to proceed with test.csv?",318448.0
120780,524023.0,120143.0,"For Kaggle, export the index and the predicted value and upload on Kaggle. You'll get to know the accuracy of your model.",318078.0
120780,524181.0,,nan,
120780,524547.0,120113.0,"Yup got it, thanks",318448.0
120062,520403.0,,nan,
120062,520419.0,119588.0,"Hi Ram I am not getting correct confusion matrix why? array([[787, 0, 3, 1, 1, 1, 7, 3, 3, 0], [ 0, 927, 3, 1, 1, 1, 1, 2, 0, 1], [ 2, 2, 781, 8, 8, 1, 3, 13, 5, 1], [ 0, 3, 9, 851, 0, 7, 0, 13, 11, 4], [ 0, 3, 6, 0, 767, 1, 1, 3, 1, 20], [ 1, 1, 2, 23, 0, 704, 5, 2, 10, 3], [ 6, 0, 5, 1, 3, 8, 785, 7, 1, 0], [ 1, 2, 3, 3, 4, 1, 0, 877, 0, 12], [ 1, 5, 5, 6, 1, 5, 2, 5, 742, 1], [ 5, 2, 7, 10, 9, 2, 0, 15, 4, 836]], dtype=int64)",308639.0
120062,520475.0,,nan,
120062,521554.0,,nan,
120062,521616.0,,nan,
120062,530650.0,,nan,
119836,519394.0,,nan,
120249,521374.0,,nan,
120461,522816.0,,nan,
120258,522035.0,119806.0,"Mine is 4 GB RAM. I did use discrete values like .1,1,10,100",308437.0
120259,521802.0,,nan,
120259,521682.0,,nan,
120259,523078.0,,nan,
119860,519599.0,119977.0,yes i have also done the same approach as chandan mentioned. took 20% for training and then rest 80% as validation for testing.,318756.0
119860,519599.0,119485.0,I thought we need to test on test.csv.,304319.0
119860,519599.0,119490.0,You will predict on test.csv using your final model.,310974.0
119860,519599.0,119586.0,can't we use the 20% entirely for training through cross validation and when best hyperparmeters are found test the accuracy on remaining 80%?,311686.0
119860,519640.0,,nan,
119860,519703.0,,nan,
120581,523595.0,,nan,
120581,524557.0,,nan,
120581,522630.0,119864.0,But wouldn't this clash with what is required from us in the problem statement? That we have to use train.csv to train and test.csv to test..,310472.0
120244,521461.0,,nan,
120244,522753.0,,nan,
119870,519631.0,,nan,
120270,522070.0,119746.0,---------------------------------------- Failed building wheel for mahotas Running setup.py clean for mahotas Failed to build mahotas Getting this error,310508.0
120270,522070.0,119747.0,After running the following >pip install mahotas Collecting mahotas,310508.0
120657,523817.0,,nan,
121113,526191.0,,nan,
121113,526378.0,,nan,
121113,531611.0,,nan,
119658,522577.0,,nan,
119658,518275.0,,nan,
119658,518327.0,119655.0,the dataset (train.csv) which u take for splitting into test and train again should have only 20% of values right?,308437.0
119658,518398.0,,nan,
119658,521280.0,120114.0,you predict the values by applying the model to the test data set,310629.0
119658,521280.0,119936.0,How are you using test.csv for predicting the model built on train since it doesn't have output variable along with it.,304696.0
121125,526322.0,,nan,
121125,526198.0,,nan,
121125,526491.0,,nan,
121125,526735.0,,nan,
121125,527035.0,,nan,
121125,527097.0,,nan,
121125,527684.0,,nan,
118977,514026.0,,nan,
118977,519561.0,,nan,
118977,519714.0,119582.0,Do we needs to do prediction on complete test.csv . Because it has 28000 observations. Here also we should take 10% data and do test prediction ?,312019.0
119885,519862.0,,nan,
119885,524619.0,,nan,
119886,519722.0,,nan,
119886,519943.0,,nan,
119886,520420.0,,nan,
119886,520450.0,,nan,
120909,524903.0,120155.0,Try plotting the heatmap of correlation between the features,344353.0
120910,524907.0,,nan,
120910,525597.0,120302.0,"i did pca to reduce load on data and accuracy i got was around 95% for 20 % data ,accuracy increases with 40 % data",318005.0
120910,525597.0,120301.0,"i deleted columns having high corr >0.7 ,but accuracy drops very much (below 0.5) by chosing threshold as 0.95 ,accuracy was maintained above 90 % but dont seem to improve also",318005.0
119525,517343.0,,nan,
119525,517332.0,,nan,
119525,517573.0,119128.0,"hi, you can use df.sample commad",311686.0
119525,517573.0,119183.0,Thanks Anshul,310974.0
119525,517573.0,119174.0,"I used test_train_split to divide the original df into two different df. Just did not use Y. something like a,b=test_train_split(train,test_size = 0.2, random_state = 101) here b contains the 20% of original data set.",304814.0
119525,519330.0,,nan,
118686,513175.0,,nan,
118686,513192.0,118618.0,It will take time. Grid search CV is a very heavy computational task and the time complexity is expected. There is no way to reduce the time but to reduce the hyperparameters you tune.,301619.0
118686,513469.0,,nan,
118686,513486.0,,nan,
118686,519874.0,119453.0,"it's on 20% of the training dataset (8400 records), 3 alpha, 3 gamma",309211.0
118686,519874.0,119447.0,what volume of data did you train with?,318438.0
118686,519901.0,119456.0,"the processing time in minutes for 20% of the training dataset (8400 records), 3 alpha, 3 gamma took me ""18 minutes using 8 way parallel processing using the flag n_jobs=-1.",309211.0
118686,519901.0,119568.0,thanks,308437.0
118686,519901.0,119788.0,thanks a lot harshendra. I tried this method. I got output in less than 15 mins on my 4GB RAM Didnt try on Colab,308437.0
118686,519901.0,119861.0,"you're welcome , glad it worked out fine. I tried Google TPU and GPU, my laptop speed was better for this assignment with Octacore.",309211.0
120913,525408.0,,nan,
120913,524912.0,,nan,
120913,524918.0,120159.0,We will certainly look into it.,344353.0
120184,521206.0,,nan,
120184,521145.0,119665.0,I am also doing something similar.. For step 1 I plan to use train_test_split or sample,313826.0
120184,521145.0,119660.0,"Step 1: you subsample 42000 to 8400. Step 2: you divide 8400 into train and test Step 3: train set to train model and test set for final prediction I hope this is correct. In step 1, how to subsample from 42000 to 8400?? I am not able to understand. I directly extracted top 8400 rows from dataset. Is this fine?",308437.0
120184,521145.0,119666.0,ok. I am using .sample function and setting frac=0.2. And this frac extraction seems to be random as per my running the code several times,308437.0
120184,521145.0,119708.0,thanks,314678.0
120184,521145.0,119734.0,"@Madhu, you can explore the parameter random_state with the sample() method. That would produce the same set of random samples and provide stability to the results provided by your model.",313826.0
120184,521145.0,119863.0,Same problem for kaggle submit ? But as per assignment code e should not submit public sites ? Could you please clarify ?,312019.0
120594,522745.0,,nan,
120594,522802.0,119978.0,"But we are not using the pixel information to draw an image. We are only classifying the pixels on the basis of their variance. If the pixel value has only one unique value, means that there is no variance and hence that pixel is not useful for classification and can be dropped, TA, please explain.",304319.0
120594,522802.0,119994.0,"Say for eg, in spam-ham problem if a word 'xyz' appears equal no. of times in all the mails, that word is not helpful in determining whether the mail is spam or ham and hence can be dropped.",304319.0
120594,522802.0,119982.0,"These are hand written digits that have been converted into 28x28 pixels. Imagine a 28x28 square box where you have to write digits between 0-9. Most of us will write in the center of the box, however, there could be many who would probably write on the margins (hope you can visualize this). Dropping pixels could be detrimental to your model. Here 0 is not the same as the 0 we encounter in other data sets (like house price etc). It is the gray scale value which ranges from 0-255",318438.0
120594,522802.0,119985.0,"But since the pixel value is 0 for all data points, means that all have written at the centre. And these columns will actually not figure in the model. If we keep them , it will unnecessary increase our computation time.",304319.0
120594,522802.0,120241.0,"I think they can be safely dropped, it'll be akin to cropping the edges of the image before analysis",304022.0
120594,522802.0,120108.0,I completely agree with @Ruchita to drop off the columns with 0 variance for the training our model. If it comes to creating images out of the pixels then we should not drop. Dropping these columns won't give any accuracy improvement but yes the computation time would improve.,318355.0
120594,523098.0,,nan,
120594,523129.0,,nan,
120594,524594.0,,nan,
120594,524555.0,,nan,
120535,522398.0,,nan,
120922,524945.0,120166.0,"I am asking for assignment , this is the csv file i have made? what sould i do next?",315560.0
120539,522463.0,,nan,
120539,522634.0,,nan,
120539,522738.0,,nan,
120539,523103.0,,nan,
120539,523139.0,,nan,
120539,523351.0,,nan,
120539,524561.0,,nan,
120539,526346.0,,nan,
120925,524962.0,,nan,
120925,525629.0,,nan,
120263,521524.0,119675.0,Okay prefect. Thanks Chandan.,307710.0
120263,521524.0,119674.0,"Yes. But they got this 20% using train_test_split only which you mentioned about. Or you can use df.sample command. Once you get this 20%, you can treat this 20% as your entire data set and do the normal processes which might again include a train_test_split.",311686.0
120263,521539.0,,nan,
120263,521518.0,,nan,
120263,522255.0,119859.0,"My understanding is same. TA , Could you pls confirm",312019.0
120263,522255.0,120027.0,Yes we need to split train data in 20-80 ration and then in use 20% data for m0delling,317156.0
120522,522400.0,119907.0,Got it ! Thanks,310467.0
120522,522479.0,119812.0,These steps are already done in the SVM module example - Letter recognition. Right? Or is there something additional which we need to do?,308437.0
120522,522479.0,119841.0,Yes.. We can follow steps similar to the letter recognition example,313826.0
119987,519959.0,119467.0,True. Thankyou,312019.0
120764,523978.0,,nan,
120764,524186.0,,nan,
120764,523903.0,,nan,
120940,525264.0,,nan,
118629,511722.0,,nan,
118805,513122.0,,nan,
118805,518117.0,,nan,
118805,518113.0,,nan,
119956,519894.0,,nan,
119956,519941.0,,nan,
119656,525635.0,,nan,
119656,518252.0,,nan,
119656,519475.0,119353.0,Ok... Thanks.,310505.0
120949,,,nan,
120954,525272.0,,nan,
120954,525368.0,,nan,
120954,525441.0,,nan,
120956,525265.0,120215.0,you're welcome Rashmi :),302738.0
120956,525265.0,120214.0,"Yeah , i missed it Thanks Naseem",308495.0
120956,525439.0,,nan,
120956,525606.0,,nan,
120338,521788.0,119700.0,"ok thanks, that helps",309211.0
120338,524436.0,,nan,
120338,524635.0,120117.0,"Thanks Avinash, got that part from Vinay and you're correct. I don't see a ""training.csv"", the file name from Kaggle is ""train.csv"", I hope you meant ""train.csv""",309211.0
120139,520977.0,,nan,
120139,520991.0,119621.0,How to deal with this,308639.0
120139,520991.0,119652.0,What made you think it is bad?,311160.0
120139,524616.0,,nan,
120928,524977.0,,nan,
120959,525297.0,,nan,
120959,525324.0,,nan,
120959,525440.0,,nan,
119626,519321.0,,nan,
119626,518097.0,119427.0,"Just for precaution, we should not entertain 80 of the training data for cleaning, right? TA can verify this well.",300721.0
119626,518179.0,119247.0,what is this random state do and what number is optimal for use here ?,310501.0
119626,518179.0,119325.0,yoo can check the below stackoverflow link for more detailed discussion https://stackoverflow.com/questions/42191717/python-random-state-in-splitting-dataset/42197534,301648.0
119626,518341.0,119501.0,thank you,305335.0
119626,519226.0,119500.0,thank you,305335.0
119626,519266.0,119499.0,Thank you,305335.0
119626,519918.0,119498.0,Thank you..just one doubt..how did you choose random state?,305335.0
119626,519918.0,119557.0,ok thank you..,305335.0
119626,519918.0,119504.0,"As I know we can give any integer number for random_state to return the fixed set of records every time from the underline data file. It could be 1, 2, 42, etc. So If we are splitting the data with random_state = 2 then we will get specific fixed set of records every time which could be different from random_state=99. I am also not that much clear about the random_state. Anyone who has a clear idea about this please help to fix the understanding.",304812.0
120600,522722.0,119866.0,Thanks Vinay,312019.0
120600,523099.0,,nan,
119665,518345.0,119215.0,Thanks Ashish for the quick reply. Your answer is clear too.,307494.0
119665,519334.0,119338.0,Thanks Girish..,307494.0
119665,519334.0,119458.0,"Considering 20%, 8400 rows to be taken randomly ? problem says you can sub-sample the data for training (10-20% of the data should be enough to achieve decent accuracy) 10% also good to take ?",312019.0
119665,519796.0,119484.0,No. As the test data is not so huge.,304319.0
119665,519796.0,119507.0,"Jay, Not needed coz test data doesn't contain label. Prediction will be done on test data from the split as per my understanding.",302742.0
119665,519796.0,119509.0,"You have to predict on test,csv and record the label in the sample_submission file",304319.0
119665,519796.0,119511.0,Okay. So accuracy we will check on test data from the split & prediction will be done on test.csv! Right?,302742.0
119665,519796.0,119512.0,right,304319.0
119665,519796.0,119513.0,Thanks,302742.0
119665,519796.0,119618.0,Thanks Ruchiya & Rashmi,300721.0
122032,533139.0,,nan,
122032,530504.0,,nan,
122032,530683.0,,nan,
120809,524244.0,,nan,
120809,524163.0,,nan,
120809,524269.0,120088.0,"On which columns am I supposed to plot the Bar plot or Count plot, as there are 785 columns in the dataset. It will become a trivia to choose which of the column(s) for plotting the graph.",301655.0
120809,524544.0,,nan,
120809,524556.0,,nan,
120809,524665.0,,nan,
120430,521869.0,119712.0,"Thank you. first data frame as 0, to n index. second was having x to n. second one i did reset_index and concat. works fine. Thanks for the clarification. second one is partial data taken from full df. so indexes different. Got the clue.",312019.0
120262,522072.0,119807.0,Oh...yes thanks a lot for the clarification,308437.0
120262,522484.0,,nan,
120278,521794.0,119709.0,Agree. I also think it is not required. But waiting for TAs for confirmation just to make sure that I perform all the steps that are required.,317991.0
120278,521794.0,119745.0,"Since TA has verified the answer I am assuming whether we write the code for image processing (deskewing the image, re-center of image etc) for this assignment or not it will not be included for evaluation (or marks calculation) and no marks will be deducted if we don't include it.",317991.0
120278,522063.0,,nan,
121051,525839.0,,nan,
121051,525840.0,,nan,
121051,525871.0,,nan,
121051,526208.0,120364.0,why random_state = 42 taken.? why not 100 or 101?,320606.0
121051,526208.0,120411.0,"You can take 100 or 101 , it will not affect",344894.0
119824,519331.0,119334.0,but to calculate accuracy you need the actual value.,317073.0
119824,519400.0,119508.0,But how will the accuracy be measured. For measuring accuracy we need both predicted data and actual data for comparison?,311729.0
119824,520283.0,,nan,
119824,521687.0,,nan,
120050,520352.0,,nan,
120050,520324.0,,nan,
119869,519617.0,119384.0,Are these any faster than files.upload?,310467.0
119869,519617.0,119400.0,I worked on some personal project where I had 3-4 GB's of data on my github repo. It used to clone within a minute on colab.,318329.0
119869,519617.0,119396.0,Thanks!,310467.0
119869,519674.0,119395.0,Thanks!,310467.0
119869,519717.0,,nan,
120247,521367.0,119805.0,I used sample function. Hope this is the right approach Thanks,308437.0
120247,521370.0,119678.0,ok got it thanks,308437.0
120247,521370.0,119669.0,"You will have to randomly sample 20% of original data i.e., take random 8400 data points out of 42000. For that you can use train test split or any other function as well. Once you get this 20%, further split into train and test taking 70:30 ratio.",310505.0
120247,522258.0,,nan,
120921,524938.0,,nan,
120693,523472.0,119984.0,thanks,311004.0
120693,523402.0,,nan,
120693,523756.0,120001.0,If test.csv is to be used then how will you check accuracy..? Because label is not present in test.csv.,317991.0
120693,523967.0,120049.0,"I didn't find Label variable in test.csv, now my question is how to evaluate result??",311004.0
120693,523967.0,120115.0,We don't have to use test.csv dataset for our assignment but still if you are interested to use it ! Test.csv doesn't have the label variable because we have to predict it . So use test.csv save it into a dataframe then scale it then pass it into the final model which have made using best optimal C and gamma. save it into dataframe like predicted df,311466.0
120689,523593.0,,nan,
120689,523387.0,,nan,
120689,523705.0,,nan,
120689,524192.0,,nan,
120689,524693.0,120213.0,too good harsha thank for explaining it,308495.0
120926,524971.0,,nan,
120995,525485.0,,nan,
120995,525498.0,,nan,
120995,526457.0,,nan,
120995,526697.0,,nan,
121041,526160.0,,nan,
121041,525872.0,120296.0,Make sense. I followed you.,317991.0
121041,526031.0,,nan,
100197,430938.0,,nan,
90919,379089.0,,nan,
90919,379802.0,,nan,
90922,379083.0,,nan,
90922,379396.0,,nan,
90922,379652.0,,nan,
90920,,,nan,
90926,379069.0,95524.0,"tried copying the name then it got filtered,thanks",314678.0
90926,379069.0,95504.0,able to filter for other two actors but i am not getting any movies but i can see movies in the df,314678.0
90926,379069.0,95508.0,code is same for Leonardo too.. Check the spelling you are using.,309451.0
90926,379069.0,95512.0,used the name provided.,314678.0
90926,379076.0,,nan,
90926,379093.0,95509.0,What error you are getting?,311686.0
90926,379093.0,95510.0,just that nothing gets filtered,314678.0
90926,379093.0,95528.0,this is strange. same code you are using for other actors?,311686.0
90926,379093.0,95531.0,"tried copying the name from the instructions and it worked,thanks",314678.0
90926,379093.0,95533.0,great.,311686.0
90926,379091.0,,nan,
90926,379279.0,,nan,
90926,379423.0,,nan,
90926,379794.0,,nan,
90926,380285.0,,nan,
92440,389361.0,,nan,
92440,389370.0,,nan,
92440,389389.0,,nan,
92440,389412.0,97175.0,"Yes, it really does not matter because the real intent of this starts after we have have some meaningful values of MA20 and MA50.",300717.0
92440,389412.0,97124.0,You can keep it as NULL or HOLD as per your preference. But we need to ignore it so better to keep null.,317689.0
92440,389412.0,97518.0,"It is preferable to drop those rows. However, you can fill them with Hold or Null and the grading would not be affected.",319721.0
92440,389627.0,,nan,
92440,389696.0,,nan,
92440,390064.0,,nan,
92440,390286.0,,nan,
90159,375571.0,94835.0,"You got the maximum count for ""genre_1 + genre_2""as ""Family+Sci Fi""?",306250.0
90159,375571.0,94836.0,In fact I am looking at the CSV file....can find only 12 records where genre_1 is Family,306250.0
90159,375571.0,94837.0,"Question is to get top genre by gross, not by count.",318368.0
90159,375571.0,94868.0,Yep...got it! Thanks for your patience. Got the same result now!!,306250.0
90159,375838.0,,nan,
90159,376024.0,,nan,
90159,376125.0,,nan,
90159,376538.0,,nan,
90159,376727.0,95064.0,There shouldnt be any changes to the datasets unless you dropped some rows incorrectly. Please check the row deletion section of assignment.,300708.0
90159,377631.0,,nan,
90390,376889.0,,nan,
90390,376914.0,95232.0,"Here you were asked to file, 250 top movies. so, the first 250 number of movies are required. That means, you can just rank them.",319721.0
90390,376914.0,95077.0,"Vinay in that case we will not have rank =250 for any of the row, when i tried ranking based on imdb score last row were having rank =206 not 250, please clarify.",320073.0
90390,376914.0,95085.0,"Correct. No row will have a rank of 250 in that case. BTW, please note that I got a different rank of the last row so checking my analysis again :(. I think this question is one for the TAs. We can raise this in tomorrow's live discussion forum session.",313826.0
90390,376914.0,95097.0,Thank you Vinay and let me know if i have to check my analysis if you are right about last 2 rows ranks,320073.0
90390,376954.0,,nan,
90167,375612.0,,nan,
90167,377103.0,,nan,
90929,379217.0,,nan,
90929,379275.0,,nan,
90168,375687.0,94942.0,Thanks got it now.,300718.0
90168,375687.0,94940.0,As per Task we should delete those rows. Now I can see we have 3-4 columns where we have more than 5% of null values. Does this mean we need to delete all those rows where we have null values in these 3-4 columns?,300718.0
90168,375645.0,,nan,
90168,375630.0,,nan,
90168,375648.0,,nan,
90935,379249.0,,nan,
90935,379799.0,,nan,
90935,379261.0,,nan,
90935,380410.0,,nan,
90171,376547.0,,nan,
90171,375680.0,94869.0,"i have submitted the assignment, i cant change it now right? so for next time, to avoid this where do we need to save the ipynb file",300733.0
90171,375680.0,94875.0,I don't think so. Requesting TAs to verify.,311686.0
90171,375680.0,94870.0,"yes. it can't be changed. your ipynb file can be anywhere but better if it is in the working directory to avoid any running issue. important is that the files being accessed by the ipynb file should be at the same location in terms of folder, directory etc. while writing assignments.",311686.0
90171,375680.0,94872.0,will there be any marks deduction for that?,300733.0
90171,375680.0,94890.0,this might help: https://stackoverflow.com/questions/35664972/how-to-change-working-directory-in-jupyter-notebook however pls note that it is not mandatory to keep your accessed files there. it's just that by having them at the same place will make commands like read_csv simpler,311686.0
90171,375680.0,94876.0,"ok. thanks. can u pls tell me where to save the csv and .ipynb file exactly. you said working directory, how to know that? can u tell me the steps, so that i will keep all my files in that directory.",300733.0
90171,375659.0,94858.0,"General means like ""...../filename.csv""?",300733.0
90171,375659.0,94871.0,yes.,318429.0
90171,375659.0,94860.0,"I have submitted file already, so i cant change it now. is it ok?",300733.0
90171,377025.0,,nan,
90171,377105.0,,nan,
90943,379213.0,,nan,
90943,379787.0,95735.0,no issues.. i did not see your earlier reponse. my bad,317811.0
90943,379787.0,95722.0,"yeah... that was the issue, i figure it out as soon as i was done with posting this question on forum",306005.0
90943,379215.0,96763.0,I am facing the similar issue. how did you solve it,315121.0
90943,379215.0,96789.0,"it faced issue because while reading data from csv to data frame, i used argument header = none, which means there would not be any header from csv data. since the attribute which i was trying to delete was not my header in dataframe hence was not able to delete or perform any action on the same",306005.0
90943,389138.0,,nan,
90329,376543.0,,nan,
90329,376471.0,94996.0,haha I am trying since yesterday.,301643.0
90329,376471.0,94997.0,Good luck :),310974.0
90329,376471.0,95208.0,"Even I couldn't find any method to do it faster. But, I can tell you that there won't be too many columns you'd need to drop in this step.",319721.0
90329,376500.0,,nan,
90329,376524.0,,nan,
90329,376664.0,,nan,
90324,376433.0,,nan,
90324,376446.0,,nan,
90324,376556.0,,nan,
90324,376670.0,,nan,
90324,376672.0,,nan,
90324,376921.0,,nan,
90324,377339.0,95245.0,IMDb_Top_250 is a subset of Movies df purpose of cleaning the data: is done on master df i.e Movies df,301115.0
90324,378831.0,,nan,
90324,378433.0,,nan,
90165,375603.0,,nan,
90165,375606.0,,nan,
90165,375628.0,94856.0,yeah understood,308495.0
90165,375934.0,,nan,
90165,376028.0,,nan,
90165,379245.0,,nan,
90877,378930.0,,nan,
90877,378793.0,95429.0,Thanks,304319.0
90877,378801.0,,nan,
90877,378989.0,,nan,
90877,378815.0,,nan,
90877,379657.0,,nan,
90877,379785.0,,nan,
90646,378063.0,,nan,
90646,378086.0,,nan,
90638,378065.0,,nan,
90638,377955.0,,nan,
90638,377981.0,,nan,
90142,375484.0,94820.0,"You can simply use append function df, which will append rows for df. df.append(df2, ignore_index=True).append(df, ignore_index=True)",318368.0
90142,375484.0,94814.0,row wise and length means total no of rows,310419.0
90142,375489.0,,nan,
90142,375485.0,,nan,
90142,375519.0,,nan,
90403,376988.0,,nan,
90403,377238.0,,nan,
90854,378691.0,,nan,
90854,378701.0,,nan,
90854,378733.0,95484.0,Got it...Thanks,301644.0
90854,378733.0,95601.0,"infact rank funtion has an option to select the order for ranking. we can select first instance,last instance etc to be given the preference for the rank. once we specify the same the rank will assign unique values for each rank. you can check the rank documentation for more details about syntax.",319302.0
90854,378850.0,95483.0,Got it...Thanks,301644.0
90056,374948.0,,nan,
90056,374944.0,,nan,
90056,375046.0,,nan,
90056,374949.0,,nan,
90056,375252.0,,nan,
90056,375419.0,,nan,
90056,375334.0,,nan,
90056,375424.0,,nan,
90056,375625.0,,nan,
90056,375980.0,,nan,
90663,378115.0,,nan,
90663,378035.0,,nan,
90621,377959.0,,nan,
90621,378001.0,95265.0,Excellent. Amit thank Amit,317269.0
90655,378037.0,,nan,
90655,378099.0,,nan,
90655,378113.0,,nan,
90655,388737.0,,nan,
90656,378052.0,,nan,
90120,375322.0,,nan,
90120,375338.0,,nan,
90120,375365.0,,nan,
90218,376424.0,,nan,
90218,377022.0,,nan,
90218,375836.0,,nan,
90670,378015.0,,nan,
90670,378046.0,,nan,
90670,378132.0,,nan,
90625,377996.0,,nan,
90626,378002.0,,nan,
90581,377872.0,,nan,
90581,377936.0,,nan,
90639,378042.0,,nan,
90741,378164.0,95488.0,Thanks,304693.0
90741,378201.0,95489.0,Thanks,304693.0
90741,378222.0,95491.0,Thanks,304693.0
90741,378479.0,,nan,
90741,379000.0,95492.0,Thanks a lot for your response!,304693.0
90741,380080.0,,nan,
90224,375834.0,94900.0,Okay thanks,311727.0
90224,375850.0,,nan,
90224,376025.0,,nan,
90224,376089.0,,nan,
90224,377021.0,,nan,
91020,379895.0,,nan,
91020,379896.0,,nan,
91020,380228.0,,nan,
91020,379925.0,95924.0,I did shut down and did do fresh start,315856.0
91020,379925.0,95927.0,ohh okk..fine...thank you for information. It is useful:),315856.0
91020,379925.0,95926.0,"if token has expired then, in my case, no matter how many times I restarted it made no difference i had to copy-past the new token into my browser then everything worked",300694.0
91024,379922.0,,nan,
91024,379962.0,,nan,
91024,380166.0,,nan,
91024,380010.0,,nan,
91024,380226.0,,nan,
91024,380883.0,,nan,
91024,381241.0,,nan,
90717,378071.0,95279.0,That's correct.,319721.0
90717,378098.0,,nan,
90717,378627.0,,nan,
90628,377965.0,,nan,
90628,378007.0,,nan,
90629,377943.0,,nan,
90629,377952.0,95250.0,save the initial rows count in any variable and use that.,320073.0
90629,378848.0,,nan,
90629,377989.0,,nan,
90629,378488.0,,nan,
90630,377933.0,,nan,
90630,377957.0,,nan,
90062,374977.0,,nan,
90062,374973.0,,nan,
90062,375041.0,,nan,
90062,376027.0,,nan,
90062,376563.0,,nan,
90062,377755.0,,nan,
90636,377950.0,,nan,
90636,378054.0,,nan,
90636,378161.0,,nan,
90616,377961.0,,nan,
90653,377971.0,,nan,
90653,377977.0,,nan,
91041,380061.0,,nan,
91041,380056.0,,nan,
91041,380053.0,,nan,
91041,380043.0,95670.0,that was helpful. Thank you :),310472.0
91041,380043.0,95729.0,You can use .div . If required you can pass fill_values to avoid null errors,311803.0
91041,380157.0,,nan,
90588,377885.0,,nan,
90588,377928.0,,nan,
90588,377946.0,95259.0,The 1st FAQ clearly explains the difference between merge()and append() with an example. Can you please extend the same example for concat()?,304319.0
90442,377172.0,,nan,
90442,377170.0,,nan,
90447,,,nan,
91039,380112.0,,nan,
91039,380045.0,,nan,
91039,380163.0,,nan,
91039,380220.0,,nan,
90221,375829.0,94901.0,I have switched to Jupyter NB and directly uploaded the file manually so that I need not change WD and then read it using pandas command. Is that fine,311727.0
90221,376562.0,,nan,
89903,374050.0,,nan,
89903,374097.0,94602.0,Language of Veer Zaara is mentioned as Hindi. It is not English and thus a foreign film. The criteria is language only. anything other than English is foreign film.,311686.0
89903,374097.0,95182.0,"Avinash, I am also facing the same problem, if I extract the top ""top foreign language films from 'IMDb_Top_250'"", I do not see ""Veer-Zara"", am I doing something wrong here? or do we need to extract foreign films from movies data frame not top250 imdb df?",313676.0
89903,374142.0,,nan,
89903,374241.0,,nan,
89903,375311.0,,nan,
90659,378036.0,,nan,
90659,378029.0,,nan,
90659,378053.0,,nan,
90557,377744.0,,nan,
90557,377748.0,95218.0,"we have to upload both the file .ipynb and Excel, so compressed these file in zip or rar? Can I upload compressed rar file?",310419.0
90557,377748.0,95220.0,okay,310419.0
90557,377748.0,95219.0,No. We need to upload only .ipynb file as I believe they would be having the csv dataset,313826.0
90557,377767.0,,nan,
90557,378133.0,,nan,
90557,378534.0,,nan,
90557,378652.0,,nan,
90593,377908.0,,nan,
90593,377929.0,,nan,
90594,377951.0,,nan,
90595,377956.0,95308.0,Yes use of all dataframe and vectorized operation. But the use of different functions or different approach will not lead to any deduction right?,315028.0
90560,377738.0,,nan,
90560,377743.0,,nan,
90560,377745.0,,nan,
90560,377800.0,,nan,
90560,377803.0,,nan,
90560,377861.0,,nan,
90560,377915.0,,nan,
90560,377925.0,,nan,
90560,378532.0,,nan,
90666,378045.0,,nan,
90666,378025.0,,nan,
90666,378058.0,,nan,
90666,378119.0,,nan,
90666,378097.0,,nan,
90666,378068.0,,nan,
90666,377994.0,,nan,
90666,378004.0,,nan,
90607,377982.0,95266.0,"but i understand that i will have to work on cleaned data and proceed step by step,thanku for explanation",318005.0
90607,377982.0,95263.0,"for example, if we run subtask 2.1 (column wise null count)after running subsequent tasks after that,it will give different output",318005.0
90611,377911.0,,nan,
90611,377923.0,,nan,
90611,377927.0,95270.0,"Thanks, will try to resolve this later",301115.0
90612,377916.0,95243.0,"@Bhanu, there is a mention of jupyter workbook but whether or not to include data set is not clearly elaborated; hence asked. Also, I have already submitted it. My question is will it be accepted? only TA can answer",309211.0
90612,377954.0,,nan,
91074,380321.0,,nan,
91074,380456.0,,nan,
91074,380510.0,,nan,
91074,380721.0,,nan,
91074,380767.0,,nan,
91074,380770.0,,nan,
90615,377953.0,,nan,
90722,378106.0,,nan,
90722,378219.0,,nan,
89938,374460.0,,nan,
90723,378111.0,,nan,
90723,378110.0,,nan,
90721,378102.0,95289.0,"Yeah, just divide those elemnts with 1000000",319721.0
90721,378112.0,,nan,
90721,379254.0,,nan,
90724,378116.0,,nan,
90724,379402.0,,nan,
90725,378114.0,,nan,
90725,378217.0,95388.0,No.. It should be on separate column.,311004.0
90726,378224.0,,nan,
90726,378117.0,,nan,
90727,378128.0,,nan,
90727,378257.0,,nan,
90727,378227.0,,nan,
90822,378522.0,,nan,
90822,378523.0,,nan,
89934,374404.0,94639.0,Correct. But print statement is just one scenario! Is there any other precautions needs to be taken ?,311741.0
89934,374408.0,,nan,
89934,374825.0,,nan,
89934,376029.0,,nan,
89934,377002.0,,nan,
90956,379833.0,,nan,
90956,379363.0,,nan,
90956,379394.0,,nan,
90956,379420.0,,nan,
90956,379485.0,,nan,
90633,377932.0,95244.0,"That is what i thought, but just want confirmation from TAs to be sure.",320073.0
90633,378033.0,,nan,
90598,377970.0,,nan,
90598,377902.0,,nan,
89022,369204.0,,nan,
89022,369393.0,93447.0,ipynb would be opened in rodeo?,300690.0
89022,369393.0,93427.0,Sorry. It is 'Markdown' or Comments.,311686.0
89022,377756.0,,nan,
90250,376026.0,,nan,
90250,375983.0,,nan,
90250,376051.0,,nan,
90250,376018.0,,nan,
90250,377014.0,,nan,
90608,377983.0,95293.0,"No no, you need to overwrite the budget column initially. Then you use the updated df with overwriten budget in the further analysis.",319721.0
90608,377983.0,95291.0,"actually, If I rewrite then every run it will overwrite the same column and the subsequent results are generated wrongly for budget and jupyter notebook",311115.0
89611,372039.0,94148.0,Thank you,301655.0
90103,375201.0,,nan,
90816,378495.0,,nan,
90816,378530.0,,nan,
90816,378511.0,,nan,
90816,378970.0,,nan,
90816,379109.0,95518.0,"Done, please ignore my question.",311006.0
90816,379109.0,95543.0,How did you do this question?,308639.0
90816,379109.0,95694.0,"Use movies dataset you will get answer, dont use IMDb_Top_250",318851.0
90816,378504.0,,nan,
90966,379578.0,,nan,
90966,379468.0,,nan,
90966,379461.0,,nan,
90966,379473.0,,nan,
90966,379684.0,,nan,
90966,379933.0,,nan,
90830,378580.0,,nan,
90830,378540.0,,nan,
90830,379247.0,,nan,
90830,379044.0,,nan,
90830,378942.0,95563.0,"Please recheck your code, this should not happen. You may want to first try printing the duplicate rows just to see if the code for the way you have identified the duplicate rows is correct. Then you can drop those.",300717.0
90830,378942.0,95496.0,Same here even i am facing the same issue,300687.0
90830,378972.0,,nan,
90830,379046.0,,nan,
90830,379642.0,,nan,
90830,378568.0,,nan,
90973,379573.0,,nan,
90973,379559.0,,nan,
90973,379575.0,95581.0,Would not be able to mention the query here,300729.0
90973,379575.0,95597.0,got it. there was an issue with the file itself,300729.0
90973,379775.0,,nan,
90973,379776.0,,nan,
90973,380857.0,,nan,
90973,379795.0,,nan,
90976,,,nan,
90627,378410.0,,nan,
90627,377945.0,,nan,
90627,378023.0,95276.0,You still have null values because you only assigned 'English' to the rows at positions where the 'language' column had null values. There might be null values in other columns at positions where 'language' column wasn't null.,301652.0
90627,377948.0,,nan,
90627,378408.0,,nan,
90622,377986.0,,nan,
90845,378634.0,95408.0,How can i do that.That is where i was struck.,309452.0
90845,378634.0,95410.0,"How you have extracted the genre_1, apply the same for genre_2. As it is a Assignment im not allowed provide you with Syntax or the functions",318804.0
90845,378643.0,,nan,
90845,379025.0,,nan,
90847,378649.0,,nan,
90847,378640.0,,nan,
90847,378669.0,,nan,
90960,379389.0,,nan,
90960,379415.0,,nan,
90960,379453.0,,nan,
90960,379492.0,,nan,
90960,379471.0,95698.0,not correct solution,305847.0
90960,379471.0,95699.0,This is the correct solution and successfully completed assignment. There are ways to achieve task and this is 100% correct. please dont misguide other if you are not able to get it.,317811.0
90748,,,nan,
90963,379412.0,,nan,
90963,379475.0,,nan,
90963,379583.0,,nan,
90963,380450.0,,nan,
90962,379414.0,,nan,
90962,379478.0,,nan,
90962,379586.0,,nan,
90751,378175.0,,nan,
90751,378228.0,,nan,
90751,378288.0,,nan,
90751,378307.0,,nan,
90751,378995.0,,nan,
90753,,,nan,
89640,372301.0,94284.0,The first checkpoint is way later and after all the cleansing operation the percentage of the rows retained in my case is matching with the checkpoint value.,318756.0
89640,372318.0,,nan,
89640,373446.0,,nan,
90632,377937.0,,nan,
90647,378050.0,,nan,
89841,374346.0,94955.0,Nothing wrong. It is expected and you can now proceed to next step. Cheers.,300718.0
89841,373457.0,,nan,
89841,373483.0,94494.0,yup same here..just now cleared that,311004.0
89841,377447.0,,nan,
90776,378280.0,,nan,
90776,378276.0,,nan,
90776,378501.0,,nan,
90776,378624.0,,nan,
90776,378983.0,,nan,
90654,378094.0,,nan,
92490,389599.0,97168.0,"ok. when i open the jupyter notebook , why im unable to load the file when i say pd.read_csv(""movies"") its not loading",320251.0
92490,389599.0,97320.0,thanks praneeth,320251.0
92490,389599.0,97188.0,"check the directory where 'movies' has been saved and give the location of the file in read_csv command. or u can also just give 'movies' if u can change ur working directory to path where ur files saved. pls find the below link to change it. https://stackoverflow.com/questions/1810743/how-to-set-the-current-working-directory/1810760 so , if u change to current working directory to whatever path u want to, copy all the files like csv and .ipynb file etc to that same directory.",300733.0
92490,390945.0,,nan,
90794,378425.0,,nan,
90794,378370.0,,nan,
90794,379033.0,,nan,
90792,378360.0,,nan,
90792,378364.0,95353.0,You're welcome :),310974.0
90792,378364.0,95354.0,"please check other post in assignments, it's answered by upgrad team",301115.0
90792,378388.0,,nan,
90792,379792.0,,nan,
90798,378394.0,,nan,
90798,378500.0,,nan,
90798,378420.0,,nan,
90357,376671.0,,nan,
90357,376692.0,,nan,
90357,376722.0,,nan,
90357,376794.0,,nan,
90357,376867.0,,nan,
90357,377807.0,95285.0,That means same imdb_score ---we ahve to give different values (diff Series num should to be followed)?,318846.0
90357,377807.0,95557.0,"I understand that, but what should be the criteria for assigning rank to same IMDb score. Do we need to consider no. of user rating or randomly assign incremental rank to same score movies.",306725.0
90357,377807.0,95675.0,"Continuing Pulkit's question, random assigning may result in different results for different ppl. Also, assigning same rank to movies with same rating would not result in more than 250 movies having rank 1-250. ex: rank . rating 1 9 1 9 3 8.9",317995.0
90775,378278.0,,nan,
90775,378283.0,,nan,
90775,378620.0,,nan,
90775,379016.0,,nan,
90805,378419.0,95364.0,"I expected the same. So, I started doing it already. Thank you.",305843.0
90805,378418.0,,nan,
90805,378609.0,,nan,
90805,378978.0,,nan,
90805,378574.0,,nan,
89881,373940.0,,nan,
89881,373877.0,,nan,
89411,370910.0,93817.0,Thank You,306038.0
90035,374947.0,,nan,
89418,370969.0,,nan,
89418,370977.0,,nan,
90030,374867.0,,nan,
90030,375047.0,,nan,
90030,375269.0,,nan,
90289,376476.0,,nan,
90289,376299.0,94956.0,"Thanks Nishan, i have done sorting with IMDB_SCORE, but i am not able add RANK column",300726.0
90289,376305.0,,nan,
90289,376677.0,,nan,
90289,376701.0,,nan,
90289,377019.0,,nan,
89885,373919.0,94543.0,I am checking if the column has more than 5% Null values. here I am getting type error. I guess there is some issue in my approach. we need to check for column with >5% Null and then ignore the corresponding row. How do we do that?,314547.0
89885,373919.0,94546.0,Yes but what is the exact type error you are seeing? What error did Jupyter throw is what i'm asking,310974.0
89885,373919.0,94551.0,I got it.. silly mistake Thanks by the way bro.,314547.0
89885,373919.0,94547.0,"raise ValueError(""Can only compare identically-labeled "" 1188 ""Series objects"") 1189 ValueError: Can only compare identically-labeled Series objects",314547.0
89885,374351.0,,nan,
89885,373967.0,,nan,
90421,377061.0,,nan,
90421,377072.0,95109.0,"yes,,, that was the silly mistake I was doing, from past 2 days.. I was struck here :( thanks man",313676.0
90421,377086.0,95499.0,so many have explained about inplace but still i am confused and it plays a big role while using the functions.thanks .i was using drop function faicng smae problem as the values were not getting deleted.but now i realised the issue,300687.0
90421,377988.0,,nan,
90421,378126.0,,nan,
90652,378060.0,,nan,
90231,376139.0,,nan,
90231,375878.0,,nan,
90231,376099.0,,nan,
90231,376120.0,,nan,
90231,376146.0,95215.0,"is it ? i checked it was ok, action/comedy was action comedy action/ was action action",308495.0
90231,376146.0,95200.0,"Hi Rashmi, the above commands will duplicate all values col2 with col1(not oly NaN other also). please check once.",300726.0
90419,377033.0,,nan,
90419,377036.0,,nan,
90419,377031.0,95256.0,You can use sorted data and select head(10),319869.0
90419,377027.0,,nan,
90419,377073.0,,nan,
90419,377265.0,,nan,
90419,377449.0,,nan,
90419,377894.0,,nan,
90419,377661.0,,nan,
90419,377888.0,95239.0,but this is not working. getting error if I use nlargest. Are you getting the result?,318429.0
90419,377888.0,95240.0,yes,317269.0
90419,377888.0,95283.0,"Works perfect Sample: df = DataFrame({'a': [1, 10, 8, 11, -1], ... 'b': list('abdce'), ... 'c': [1.0, 2.0, np.nan, 3.0, 4.0]}) >>> df.nlargest(3, 'a') a b c 3 11 c 3 1 10 b 2 2 8 d NaN",317514.0
90419,377888.0,95280.0,Effective way to retrieve the largest selections compared to sorting and then getting the first 10. But found that if there are say 12 rows that meet the top 10 criteria...(a)sorting on ascending = False and retrieving top 10 versus (b) retrieving nlargest provides different answer. You cannot say one is right or other as wrong. Not sure why the result varies ...the implementation should be same.,317514.0
90419,377964.0,,nan,
90495,,,nan,
91987,385903.0,96588.0,Oh Okay! But I have not used inplace=True anywhere in my code.,316255.0
91987,385903.0,96902.0,"even if you have not used inplace = True, you can update your function by assigning it to your data variable like, movies = *the operation you want to perform*",319721.0
91987,386164.0,,nan,
91987,386604.0,,nan,
90565,,,nan,
91993,385974.0,,nan,
91993,385964.0,,nan,
91993,385958.0,,nan,
91993,387935.0,,nan,
90591,377893.0,,nan,
90591,377914.0,95241.0,Can they be used interchangeably?,304319.0
90600,378405.0,,nan,
90600,378261.0,,nan,
90600,377899.0,,nan,
90600,377912.0,,nan,
90600,377917.0,95242.0,Thanks Bro.,318319.0
90600,377939.0,,nan,
90600,377975.0,95269.0,Good question. Will be helpful in future for quick review,317269.0
90599,377906.0,,nan,
90599,377935.0,95252.0,"Hey, I hope this helps. If there are further questions, post them again.",319721.0
90599,377935.0,95307.0,"I am aware of the difference , can you give certain example where use of one will be better than the other in terms of any parameter.",315028.0
90599,380092.0,,nan,
90609,377998.0,95281.0,"Yes I do, but then 2.4 becomes redundant",319866.0
90609,383503.0,,nan,
90620,377979.0,,nan,
90620,378476.0,,nan,
90620,378510.0,,nan,
90650,378090.0,,nan,
90649,378087.0,,nan,
90645,377995.0,,nan,
90645,378076.0,,nan,
90651,378051.0,,nan,
90580,377873.0,,nan,
90580,377863.0,,nan,
90580,377930.0,,nan,
90580,377934.0,,nan,
90580,377962.0,,nan,
90658,378044.0,,nan,
90658,378077.0,95278.0,Used the same but indices mismatch restricting to get desired result,319869.0
90658,378082.0,,nan,
90658,378104.0,,nan,
90658,378456.0,,nan,
90658,378515.0,95399.0,good to see someone trying other than rank..,319869.0
90658,378528.0,,nan,
90658,378904.0,,nan,
90658,379315.0,,nan,
90610,378003.0,95272.0,this error I have already searched.,318429.0
90610,378003.0,95271.0,"my code was working properly previously, but now that part is not executing, otherwise I'm executing that part is executing on another ipynb file",318429.0
90617,377968.0,95268.0,thanks for explanation,318005.0
90662,378109.0,,nan,
90662,378233.0,,nan,
90605,377940.0,95248.0,"It means we have to pass the original row count say 5042 explicitly.. Since, we don't have the original data frame anymore?",300727.0
90605,377940.0,95254.0,"Yeah, that's correct Siddhart. You need to use the number of rows in original dataframe.",319721.0
90605,377940.0,95258.0,okay alright Sushmitha.. Thank you,300727.0
90605,377940.0,95255.0,My understanding we shouldn't have it hard coded.. So wanna have a check..,300727.0
90669,378021.0,,nan,
90669,378127.0,,nan,
90669,378089.0,,nan,
90669,378012.0,,nan,
90669,378006.0,,nan,
90669,378014.0,,nan,
90669,378019.0,,nan,
90669,378034.0,95275.0,Also from my limited experience and research regarding this kind of problem. People suggest to create a copy of main file and work on that. Just in case. So that original file/ questions/ comments do not get distorted.,317269.0
90669,378056.0,,nan,
90669,378238.0,,nan,
90669,379580.0,,nan,
90669,379530.0,,nan,
90669,379779.0,,nan,
90641,378074.0,,nan,
90644,378081.0,,nan,
92008,386118.0,,nan,
92008,388690.0,,nan,
92008,388830.0,,nan,
90637,377967.0,,nan,
90637,378059.0,95284.0,This we did not do. We did rows if more than five col values are missing. Those rows removal has done. But not Drop unecessary rows using columns with high Null percentages. facing same issue. I am literally stuck here. until % missing column wise is ok. Not able to find removal of rows based on % col missing.,312019.0
90637,377958.0,,nan,
90637,378000.0,,nan,
90637,378150.0,,nan,
90637,379174.0,,nan,
92000,386026.0,96581.0,"I Agree to your point. Please check the number of rows before and after executing the Task 3.3. If there is a difference in the number of rows after executing the Task 3.3, then you can use the same data frame. In my case, after running the task 3.3, movies dataframe rowcount remains the same. So, I have created a new dataframe which has details without any duplicates.",311502.0
92000,386026.0,96578.0,We have removed duplicates from movies in task 3.3..Then there wont be any duplicates. What do you think?,320603.0
92000,387572.0,,nan,
92000,386290.0,96600.0,thanks:),320603.0
92000,388688.0,,nan,
90631,378026.0,,nan,
90596,377913.0,95238.0,Thanks,301652.0
90596,377913.0,95249.0,"Thanks Vipul, that's correct",319721.0
90596,377913.0,95262.0,Please go through this link https://docs.python.org/3/library/warnings.html,317991.0
90596,377913.0,95260.0,Please answer my 3rd query as well.,301652.0
90640,378070.0,,nan,
91018,379892.0,,nan,
91018,379927.0,,nan,
90545,377672.0,,nan,
90545,377681.0,,nan,
90648,377973.0,,nan,
90648,378030.0,95303.0,using split over '|',318005.0
90643,377984.0,,nan,
90643,378075.0,,nan,
90643,378084.0,,nan,
90643,379747.0,,nan,
89670,372269.0,94201.0,"true, actually i asked this question, because upgrad will run the code in our notebook on their datasets, what if they haven't kept data and notebook in same folder",318433.0
89670,372355.0,,nan,
89670,372737.0,,nan,
91014,379899.0,,nan,
91014,380003.0,,nan,
90322,376416.0,95205.0,"So when you group your data frames by genres, you get a different dataframe with only two genres and you can store this in another variable. Is that what you're trying to understand? Can you please reframe your question Amit?",319721.0
90322,376416.0,95010.0,"Thanks Nithesh still not able understand, please see below group the dataframe using these two values with genre_1 being primary and genre_2 being secondary- grouping sort of creates a subset of the data; so here are some examples grouping(genre_1+genre_2) # You mentioned this but not able what is grouping grouping(genre_1+genre_2)",307843.0
90322,376559.0,95029.0,"Hi Bhanu, my apologies. They haven't. I was looking at my test file. My code was interfering when i kept genres. So i dropped it. Since they have not asked to drop genres., one should keep genres, genre_1 and genre_2.",317269.0
90322,376559.0,95028.0,"Hi Amit , in the question they have not asked us to drop the genres column . They asked us to create 2 new column form genres column. do we have to drop the column? or is it ok to keep that as well",320073.0
90322,376676.0,,nan,
91236,381242.0,95857.0,"Yes, the zip file can be uploaded but make sure that it the assignment jupyter notebook in .ipynb format for sure. As this has been specifically mentioned in the submission section.",317460.0
91236,381245.0,,nan,
91236,381347.0,,nan,
91236,382082.0,,nan,
91236,381414.0,,nan,
90618,377974.0,,nan,
90618,377987.0,,nan,
90894,378885.0,95456.0,Thanks. Forgot that groupby can accept two columns if entered as a list,316416.0
90894,378927.0,,nan,
90894,378984.0,,nan,
90949,379268.0,,nan,
90949,379286.0,,nan,
90949,379291.0,95710.0,Open jupyter notebook and browse your file from there,318426.0
90949,379291.0,95766.0,"You can’t access it directly, you need to save that file in either documents/downloads/ desktop and then browse its location from jupyter notebook",314936.0
90949,379291.0,95865.0,Did you install anaconda3 ?? You can launch jupyter notebook from anaconda,316323.0
90949,379331.0,,nan,
90949,379332.0,,nan,
90949,379505.0,,nan,
90949,379494.0,96774.0,Your wc Srinithi....Glad to hear :),317811.0
90949,379494.0,96757.0,Thanks Ankit. It worked :),315831.0
90904,378922.0,,nan,
90904,378925.0,95474.0,"ya i understood what u are trying to do, but i can tell u may have drooped that row, while removing NAN values. because that row contains one NAN values. you can check the movie DF excel.",300726.0
90904,378925.0,95465.0,"If the movie exist in Veer-Zaara is in IMDb_Top_250 dataframe, it should also exist in movies dataframe. I am just trying to extract row related to ""movie_title=Veer-Zaara' from movies dataframe.",317991.0
90904,378936.0,95478.0,Yes...I agree it is present in original dataset. And also it should come in Top_Foreign_Lang_Film dataframe. So I was backtracking why the movie='Veer-Zaara' is not coming in my solution.,317991.0
90904,378936.0,95479.0,Your code is working fine. Is there any mistake in my code attached in screenshot ?,317991.0
90904,378936.0,95480.0,"Earlier I also tried the same way, but got suspicious and then tried using regex type checks to search for the text patterns. Looks like there might be some control characters / special character as string delimiters because of which the equality conditions are failing.",313826.0
90904,378936.0,95486.0,After getting top_250 movie dataframe when I tried to get hindi movies 'Veer-Zaara' and some 3 hindi movie also missing. Don't know why this is happening. Any hints how to proceed ?,317991.0
90904,378936.0,95497.0,"Tasks 2.3 , 2.4 and 3.3 are the tasks where in you are removing rows from the dataframe. Inspect you dataframe after each of these tasks.",313826.0
90904,378936.0,95502.0,Actually I tried on newly created dataframe. Do we supposed to do all subtask with single dataframe created at top ?,317991.0
90904,378936.0,95660.0,"I was able to find the Veer-Zaara in the dataframe, there was issue with the condition being asked to apply on num_voted_users, which i corrected and got the answer.",310210.0
90904,378936.0,95539.0,I inspected dataframe at each point and don't see Veer Zaara in the list for foreign languages. Also in IMDb_Top_250 Veer Zaara does not come. Can someone please confirm.,310210.0
90904,379010.0,95493.0,typo mistake,304319.0
90904,379010.0,95501.0,"I already tried......its not working. And its not 'A' at the end of Zaara, it is the character which is there in original dataset i.e excel file.",317991.0
90904,379148.0,95721.0,"same here , i have done exactly correct but unable to find veer Zaara infact it is not there in IMDB_250 as well",311386.0
90904,379301.0,,nan,
90904,379771.0,95636.0,It will give you the row containing Veer-Zaara.,304319.0
90904,379771.0,95617.0,Or you could covert it to str and then check.,318329.0
90904,379771.0,95620.0,"in the actor subtask, I am directly comparing with ""Meryl Streep'. There was no need to add the extra character.",304319.0
90904,379771.0,95622.0,"actor_1_name didn't have any extra characters. That's the reason we didn't need to. But, inspect the movie_tile column.",318329.0
90904,379771.0,95623.0,Also I think you need to use movies.loc[movies.title=='Veer Zaara'],304319.0
90904,379771.0,95625.0,The column name is movie_tile .,318329.0
90904,379771.0,95626.0,No. It is movie_title,304319.0
90904,379771.0,95627.0,"Yes, type. It's movie_title.",318329.0
90904,379771.0,95628.0,"So, it would be movies.loc[movies.movie_title=='Veer-Zaara/xa0'] or movies.loc[movies.movie_title.str.contains('Veer-Zaara')]",318329.0
90904,379771.0,95632.0,mv=movies.loc[movies.movie_title=='Veer-Zaara'] print (mv),304319.0
90904,379771.0,95633.0,There is no extra character in my dataframe.,304319.0
90904,379771.0,95635.0,What's the output,318329.0
90904,379771.0,95643.0,Can you please try and confirm,318329.0
90904,379771.0,95653.0,contains ('Veer-Zaara') better choice and working..,315679.0
90904,379771.0,95644.0,Sorry guys was held up in doing assignment. All errors sorted out. Thank you everyone for your inputs.,317991.0
90904,379803.0,,nan,
90904,379909.0,,nan,
90904,379982.0,,nan,
90904,380019.0,,nan,
90904,380038.0,,nan,
90904,380044.0,,nan,
90904,380136.0,,nan,
90905,378943.0,,nan,
90905,378954.0,,nan,
90905,379648.0,,nan,
90905,380142.0,,nan,
90951,379323.0,,nan,
90951,379321.0,,nan,
90951,379322.0,95545.0,Thank you Amani !,306735.0
90951,379328.0,,nan,
90951,379353.0,,nan,
90951,379486.0,,nan,
90927,379172.0,,nan,
90927,379097.0,95511.0,Got it. Assigned num_voted to different dataframe and sorted the top 250 of that df into mentioned data frame.,305843.0
91259,381435.0,,nan,
91259,381425.0,95891.0,oh yes! that's correct 👍,316349.0
91259,381425.0,95888.0,"but you can still submit it until next Sunday and receive a 30% penalty, so it is not too late",300694.0
90664,378047.0,95544.0,"hi check below code and correct me and how to find highest imbd rating IMD_Top_250 = movies.sort_values(by='imdb_score',ascending= False).head(250).nlargest('imdb_score') print(IMD_Top_250)",308639.0
90664,378048.0,,nan,
90664,378118.0,,nan,
90664,378024.0,,nan,
90664,378079.0,,nan,
90671,378011.0,,nan,
90671,378013.0,95267.0,Thanks. Got the answer,320103.0
90671,378073.0,,nan,
90671,378055.0,,nan,
90671,378135.0,,nan,
90671,378131.0,,nan,
90675,378066.0,,nan,
90675,378062.0,,nan,
90675,378067.0,,nan,
90675,378072.0,,nan,
92474,389509.0,,nan,
92474,389523.0,,nan,
91088,380480.0,,nan,
91088,380586.0,,nan,
91088,380505.0,,nan,
92522,389818.0,97920.0,i put my code at the the bottom check and tell me where it went wrong,318461.0
92522,389818.0,99678.0,what i did is correct. i got full marks,318461.0
92522,389846.0,,nan,
92522,391205.0,99677.0,thank you,318461.0
92522,391205.0,97947.0,"That's correct, please give your assignment for reval.",319721.0
92522,391033.0,97919.0,i put my code at the the bottom check and tell me where it went wrong,318461.0
92522,391201.0,,nan,
92945,391435.0,98460.0,"ok. thanks for the help,:)",302738.0
92945,391499.0,98459.0,"thanks for the help,:)",302738.0
92945,391499.0,98511.0,then feel free to upvote my answer ;-),300694.0
92945,392553.0,98458.0,"thanks for the help,:)",302738.0
134042,583323.0,,nan,
133169,580655.0,,nan,
133312,,,nan,
132441,578611.0,,nan,
132441,578225.0,,nan,
132441,578250.0,,nan,
132441,578295.0,,nan,
132441,578312.0,,nan,
132441,578309.0,,nan,
132558,578528.0,129812.0,"Thanks a lot, this helps me a lot.",315423.0
132558,578528.0,130173.0,Thanks a lot Shubham..Well explained,308638.0
132558,578607.0,,nan,
132558,579554.0,,nan,
138142,597410.0,,nan,
138142,595845.0,,nan,
134184,584426.0,,nan,
134184,583465.0,,nan,
134184,583466.0,,nan,
133074,580323.0,,nan,
134229,583585.0,,nan,
133076,580311.0,,nan,
133076,580708.0,,nan,
133045,580184.0,,nan,
133045,580203.0,,nan,
132963,579819.0,,nan,
136381,589969.0,,nan,
136381,589827.0,,nan,
134860,585749.0,,nan,
134860,585748.0,,nan,
134860,585750.0,,nan,
134860,585727.0,,nan,
132035,577034.0,,nan,
132131,577313.0,,nan,
133119,,,nan,
133120,580756.0,,nan,
133120,580622.0,,nan,
132782,579172.0,129733.0,"from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(""Spark ML"") \ .getOrCreate() I added the above code and got the same error",319759.0
132782,579172.0,129739.0,"This is not the way to create a View. Please refer to the notebook provided. It will have code something like df.createOrReplaceTempView(""table-name"")",428646.0
132782,579172.0,129738.0,"from pyspark.sql import SparkSession spark = SparkSession.builder.appName(""reviews"").getOrCreate() review_length= spark.sql('SELECT helpful, overall, reviewText, reviewTime, summary, asin, LENGTH(reviewText) AS reviewLength FROM review') This also gives the same error",319759.0
132782,579172.0,129970.0,"After adding the SparkSession reference, create a dataframe with the path mentioned /common_folder/amazon_reviews_graded/reviews_Home_and_Kitchen_5.json. then df.createOrReplaceTempView(""tablename"") then take some variable say dfnew dfnew=spark.sql(""query"") dfnew.show() Refer https://learn.upgrad.com/v/course/208/session/37076/segment/200988",301114.0
132789,579201.0,,nan,
132796,579237.0,,nan,
132769,579117.0,129712.0,Try out my new answer.,428646.0
132769,579117.0,129709.0,Tried that as well. Same issue,305653.0
132769,579118.0,,nan,
132769,579122.0,129714.0,Cool.,428646.0
132769,579122.0,129713.0,Thanks. This works,305653.0
133024,580207.0,129975.0,Did that. Still not working. The spark session object command is right now getting hung up when I run it.,310511.0
133024,580207.0,129978.0,Tried all that. Are you able to create the session object now?,310511.0
133024,580207.0,129977.0,Then check if there are multiple session of notebook running. If that's the case then shutdown all notebook and again restart.,317991.0
133024,580207.0,129984.0,Yes.....I tried now also....its working.,317991.0
133024,580315.0,130017.0,No. What about u?,310511.0
133024,580315.0,130034.0,Yes. Restarted the laptop. The issue as per the logs was lack of physical memory space (RAM).,310511.0
133024,580321.0,130015.0,where do I write about the issue?,310511.0
133024,580321.0,130085.0,Please get in touch with you mentor. He will let you know about the email address that belongs to Corestack.,428646.0
133024,580321.0,130016.0,where do I write about the issue?,310511.0
133024,580407.0,130032.0,Yes. Restarted the laptop. The issue as per the logs was lack of physical memory space (RAM).,310511.0
133024,580407.0,130096.0,OK. Same for me,318335.0
133024,580093.0,129955.0,Thanks Vinay. I have been trying to re-establish the session for the past 30 mins without any luck. Will you be able to check if you are facing the same issue too at the moment?,310511.0
133024,580093.0,129959.0,Having same issue when I tried creating a session just now.,313826.0
133024,580093.0,130033.0,Issue was resolved by restarting the laptop. The issue as per the logs was lack of physical memory space (RAM).,310511.0
132823,579565.0,,nan,
132823,579936.0,,nan,
132823,580190.0,,nan,
132823,579313.0,,nan,
132823,580597.0,130079.0,"refer the discussion - Helpfulness Score: Question 2- ""check the fourth row where reviewLength = 29087""",301114.0
132823,580597.0,130077.0,I have done order by reviewlength in descending order on the select statement which will arrange the reviewtext from descending order. the topmost one is the reviewtext with maxlength.again no need to perform maxlength..,301114.0
132827,579421.0,,nan,
132854,579469.0,129813.0,ok understood,318005.0
132863,580705.0,,nan,
132863,579847.0,,nan,
132863,584456.0,,nan,
132863,579495.0,129827.0,"Still not able to execute it even though only one notebook is running.I should write to Corestack ,right away. THanks for the help",318386.0
132863,579495.0,129836.0,I separated the import command and spark comman where we are getting a spark session object.The problem was not in import command but in the spark = SparkSession \ ... command.Now this is taking long to execute,318386.0
132863,579495.0,129848.0,Did you get a resolution? Facing the same issue,305653.0
132863,579495.0,129884.0,"I couldn't find any particular resoution .I did get a response from Corestack same as the above. Additional to that ""spark.stop()"" use this command before switching to another notebook at the end. And also was suggested to use only one notebook to be safe and max two Sparksession objs. All these methods may or may not work.If nothing works just log off and use it after 15-20 mins",318386.0
132863,579495.0,129904.0,Just gave up last night and logged off. It started working in the morning. Thanks for your reply anyway!,305653.0
132863,579495.0,129969.0,Please find the walk around .It really helps when ever you are facing with the issue.,318386.0
132863,579467.0,129826.0,"It didn't execute even after waiting for long and restarting the corestack.thanks for the quick reply,though",318386.0
132863,579576.0,,nan,
132885,579562.0,,nan,
132885,580713.0,,nan,
132885,579620.0,,nan,
132285,577633.0,,nan,
132658,580514.0,130138.0,Have same doubt. How to split TARGET and feature variables. Did you find the anser?,307495.0
132658,580514.0,130607.0,Thank you.,307495.0
132658,580514.0,130256.0,"Yes. When you make new Feature Column using VectorAssemebler and set the input coils to TV, Radio and Newspaper and the target column as Sales. In the creation of LinearRegression instance, you can set the feature col as features and labelcol as Sales.",311115.0
132658,578745.0,129645.0,"Great . Perfect material. I have a doubt in this material finding the best features. After finding the dt_model.featureImportances what is the base to find best feature, min of that SparshVector ? Please clarify",312019.0
133101,580700.0,,nan,
133101,580440.0,,nan,
133101,580437.0,,nan,
133294,580760.0,,nan,
133299,580777.0,,nan,
133299,580774.0,,nan,
132811,579299.0,129776.0,Its still now working! Details posted as answer,318355.0
132811,579299.0,129917.0,Can you raise an issue with corestack? It's working fine on my end.,428646.0
132811,579310.0,,nan,
133493,581443.0,,nan,
133493,581333.0,130192.0,"Yes...i did that.. since this is from graded question, couldn't upload the whole pic..",305335.0
133493,581199.0,,nan,
132418,578092.0,129860.0,not working woth this sol.can you please chk again,315560.0
132418,578113.0,129641.0,"sorry it was syntax mistake above. df = spark.read.csv(""/common_folder/Advertising.csv"",header=True,sep="","");",312019.0
132418,578113.0,129637.0,"File "" "", line 1 df1=spark.read.format(""csv"").load(""""/common_folder/Advertising.csv"") ^ SyntaxError: EOL while scanning string literal",312019.0
132383,578010.0,,nan,
132421,578106.0,,nan,
132421,578111.0,,nan,
132421,578109.0,,nan,
132421,578134.0,,nan,
132421,578107.0,,nan,
134584,584662.0,,nan,
134584,584664.0,,nan,
134584,585453.0,,nan,
134584,584754.0,,nan,
132960,579884.0,,nan,
132960,580117.0,,nan,
132434,578228.0,129477.0,Thanks..tried the same and got this : AttributeError: 'DataFrame' object has no attribute 'age' There is a column by name age - not sure what went wrong.,319759.0
132434,578228.0,129486.0,"Is column 'age' present in your dataframe. If yes, Can you please post the full stack trace of the error that you're getting?",318355.0
132434,578228.0,129496.0,Output of df.show(5) +---+---+---------+---+----------+---+-----------+--------------+---------------+-------------+--------+--------------------+----+-------------+----+ |_c0|_c1| _c2|_c3| _c4|_c5| _c6| _c7| _c8| _c9| _c10| _c11|_c12| _c13|_c14| +---+---+---------+---+----------+---+-----------+--------------+---------------+-------------+--------+--------------------+----+-------------+----+ |age|sex|pain type| BP|cholestrol|fbs|resting ecg|max heart rate|exercise angina|ST depression|ST slope|flouroscopy coloured|thal|heart disease|null| | 70| 1| 4|130| 322| 0| 2| 109| 0| 2.4| 2| 3| 3| 2|null| | 67| 0| 3|115| 564| 0| 2| 160| 0| 1.6| 2| 0| 7| 1|null| | 57| 1| 2|124| 261| 0| 0| 141| 0| 0.3| 1| 0| 7| 2|null| | 64| 1| 4|128| 263| 0| 0| 105| 1| 0.2| 2| 1| 7| 1|null|,319759.0
132434,578228.0,129497.0,"Full error message : AttributeError Traceback (most recent call last) in () 1 from pyspark.sql.functions import min, max ----> 2 df.agg(min(df.age), max(df.age)).show() /usr/local/spark2.4/python/pyspark/sql/dataframe.py in __getattr__(self, name) 1298 if name not in self.columns: 1299 raise AttributeError( -> 1300 ""'%s' object has no attribute '%s'"" % (self.__class__.__name__, name)) 1301 jc = self._jdf.apply(name) 1302 return Column(jc) AttributeError: 'DataFrame' object has no attribute 'age'",319759.0
132434,578228.0,129499.0,Answered below.,428646.0
132434,578228.0,129503.0,sorry..where?,319759.0
132434,578228.0,129558.0,@sumit shukla - let me know where is the answer for this..thanks,319759.0
132434,578228.0,129574.0,"Check the verified answer. Basically, you have not used the header = True due to which column names are c1, c2 and so on.",428646.0
132434,578306.0,,nan,
132434,578567.0,,nan,
132973,579814.0,,nan,
132879,579497.0,,nan,
132984,579891.0,,nan,
132497,578333.0,,nan,
132675,578806.0,,nan,
132675,578803.0,,nan,
132675,578807.0,129703.0,"IllegalArgumentException: 'Data type string of column TV is not supported.\nData type string of column Radio is not supported.\nData type string of column Newspaper is not supported.\nData type string of column Sales is not supported.' I get this error while running the below code: from pyspark.ml.feature import VectorAssembler vectorAssembler = VectorAssembler(inputCols = ['TV' , 'Radio' , 'Newspaper' , 'Sales'], outputCol = 'features') vhouse_df = vectorAssembler.transform(df) vhouse_df = vhouse_df.select(['features', 'MV']) vhouse_df.show(3) Please guide",319759.0
132675,578807.0,129694.0,use below link as one of the TA mentioned. https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a Only thing is columns to be converted as proper type before usage Otherwise perfect,312019.0
132675,578807.0,129710.0,"before this we needs to convert to datatypes proper, in this pyspark from pyspark.sql.types import * df = df.withColumn(""TV"", df[""TV""].cast(FloatType())) .withColumn(""Radio"", df[""Radio""].cast(FloatType())) .withColumn(""Newspaper"",df[""Newspaper""].cast(FloatType())) .withColumn(""Sales"", df[""Sales""].cast(FloatType())) in your code change MV to output variable required",312019.0
132675,578856.0,,nan,
132675,580507.0,,nan,
132675,579614.0,,nan,
133689,581963.0,130372.0,"Thank you! In simple terms, what leads to an output is an action and what leads to more data storage/mutation is a transformation.",308637.0
132633,578660.0,,nan,
132992,580188.0,,nan,
132992,579929.0,,nan,
132640,578686.0,,nan,
133393,580986.0,130109.0,"Spark session was created with the name ""Basic RDD openration"" Please refer: from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(""Basic RDD openration"") \ .getOrCreate() But a Sparkcontext was created first with name ""rdd"". Does it mean the name for spark session will also be the same?",310533.0
133393,580986.0,130131.0,So why is the discrepancy in the session name?,310533.0
133393,580986.0,130130.0,"Here you are creating a Spark Session but you are referring to Spark Context, both are different.",428646.0
132577,584458.0,,nan,
132577,578577.0,129570.0,"Thanks Jyotishri. But , I could nt find the path through CoreStack login. Can you help here?",307494.0
132577,578577.0,129573.0,"You can't access it in CoreStack, as far as I know, You can write a mail to them to check this.",428646.0
132577,578577.0,129588.0,"Ok, Thanks.",307494.0
132577,578577.0,129842.0,Thanks!,311004.0
133304,580846.0,,nan,
133304,580808.0,,nan,
100939,433647.0,,nan,
100939,433665.0,,nan,
100947,433619.0,,nan,
100949,,,nan,
100953,433658.0,105296.0,Thank you,301655.0
100953,434173.0,,nan,
100957,433678.0,,nan,
100957,433931.0,,nan,
101029,434038.0,105369.0,ratio is 4:1 (Satisfactory : Not satiasfactory),301648.0
101029,434038.0,105342.0,What it means by 4 times more likely that a drug is able to produce a satisfactory result than not ?,318780.0
101029,434148.0,,nan,
101029,434313.0,,nan,
101029,434361.0,,nan,
101029,435415.0,,nan,
101034,434032.0,,nan,
101034,434054.0,,nan,
101034,434146.0,,nan,
101034,438355.0,,nan,
100132,429112.0,104696.0,we have to take example of hypothetical scenarios,317990.0
100132,429112.0,104692.0,what are the two scenarios?,310419.0
100132,431467.0,,nan,
99559,425888.0,,nan,
99559,425890.0,104299.0,ok. Thanx chandan..,311117.0
99559,425890.0,104295.0,can you explain it with a mathematical example please?,305845.0
99559,425890.0,104297.0,"You mean to say, if i consider ""no satisfactory result"" as 1, so ""satisfactory result"" would be 1/4?",311117.0
99559,425890.0,104298.0,"no. if we consider probability of 'no satisfactory result' as X then probability of 'satisfactory result' will be 4X. And as there are only these two events that can take place, so total probability of these two is 1. so X+4X = 1. with this you can find the probability of success. I hope this also answers Parul's question.",311686.0
99559,425890.0,104302.0,yeah.. understood this now... thanks Chandan :),305845.0
99559,426446.0,,nan,
100425,431935.0,,nan,
100344,431551.0,,nan,
100344,431795.0,,nan,
100344,431903.0,,nan,
100344,432149.0,,nan,
100956,434138.0,105364.0,Yes.,313517.0
100956,433674.0,105343.0,Still I don't know how to use beta for testing ! In the examples in the videos we have used only alpha to compare !,311466.0
100956,433674.0,105346.0,"Go through the video, practice questions and graded questions on types of errors once again. They'll definitely help you in thinking in the right track in context to this problem as well.",313517.0
100956,433674.0,105353.0,does null and alternate hypothesis remain same in both the cases?,314621.0
100956,433674.0,105365.0,"Yes, Sachita",311466.0
100956,433674.0,105481.0,Do we have to explain situation considering the same Null and Alternate hypothesis defined in Q3 a ? Or we only have to give the generalized situation that can occur in Drug manufacturing ?,317991.0
100956,433674.0,105482.0,The null and alternate hypothesis would remain the same.,313517.0
100956,433918.0,,nan,
100956,434171.0,,nan,
100956,434709.0,105427.0,"No, you have to explain the situations as mentioned above. You are on the right track, just think a bit more on the preference part.",313517.0
100956,436188.0,,nan,
101113,434657.0,,nan,
101113,434671.0,,nan,
101113,434885.0,,nan,
101113,438352.0,,nan,
101131,,,nan,
101137,434944.0,,nan,
101137,435483.0,,nan,
101137,438351.0,,nan,
101147,435660.0,,nan,
100194,430089.0,,nan,
100194,431417.0,,nan,
100194,432854.0,,nan,
100218,430481.0,104794.0,So does it mean probability of drug passing the test is 4 times more than failing ?,311741.0
100218,430481.0,104968.0,yes,304814.0
100218,431212.0,,nan,
100218,431460.0,,nan,
100218,431476.0,104877.0,The ratio is 4:1 only. you don't have to add one more,313517.0
100218,431476.0,104931.0,could you please explain why don't need to add one more?,318398.0
100218,431476.0,104934.0,"I understand that there is some confusion here. But for this question specifically, you don't have to add one more. Take the ratio as 4:1 only instead of 5:1 and then proceed to solve the question.",313517.0
100218,431477.0,104879.0,"So, p = probability that the event will occur = 0.4?",310508.0
100218,431477.0,104883.0,got it thanks,311741.0
100218,432187.0,,nan,
100262,431063.0,,nan,
100788,432978.0,,nan,
100788,432941.0,,nan,
100788,433073.0,,nan,
100788,433220.0,,nan,
100788,433405.0,,nan,
100788,433423.0,,nan,
100788,434149.0,,nan,
100788,438359.0,,nan,
100871,433232.0,,nan,
100871,433235.0,,nan,
100871,433239.0,,nan,
100871,433644.0,,nan,
100871,434145.0,,nan,
100871,433921.0,,nan,
99627,431924.0,,nan,
99627,426634.0,,nan,
99627,426449.0,,nan,
99627,427250.0,,nan,
99627,427728.0,105309.0,"A different sample (mean, standard deviation and sample size) is used, which will result α and β as 0.15 respectively.",302750.0
99628,426606.0,,nan,
99628,426475.0,,nan,
99628,426476.0,,nan,
99628,426495.0,,nan,
99628,426535.0,104397.0,Which method. Two hypothesis testing method or two scenarios with different alpha and beta values. TA please explain.,319319.0
99628,426621.0,104892.0,"Hi Mahima, so basically you are saying that the treating the cure in 200 seconds ( as mentioned in the question ) is not a mandatory condition. Just based on some assumptions, we need to make a statement why to chose alpha = 0.05 and beta = 0.45 over alpha = 0.15 and beta = 0.15 and vice versa.... Is that correct?",304814.0
99628,426621.0,104897.0,No the hypothesis test that you are conducting would be the same in both the cases. Only the values of alpha and beta are different. You need to give situations where choosing alpha = 0.05 and beta = 0.45 over alpha = 0.15 and beta = 0.15 is better and vice-versa.,313517.0
99628,431162.0,104875.0,No. Beta is not 1-alpha. There is an inverse relation between the two but not this.,304319.0
99628,432103.0,,nan,
99628,432436.0,,nan,
99628,432764.0,,nan,
99628,432950.0,,nan,
99584,426229.0,104427.0,Thanks Ram..,307494.0
99584,426229.0,104769.0,Any inputs on how I get the p value here? I mean the probability of success rate in each trail ?,300727.0
99584,426229.0,104840.0,"Chetan, yes ideally the p value is provided as some percentage.. but we don't have this % provided in the problem example.. so any inputs on how I get this p value ? bit confused on how I get the p value.. Is it something No of possible successful outcomes/no of trails ?",300727.0
99584,426229.0,104838.0,you dont need a p-value here as you are testing any hypothesis...you are just trying to calculate the probability for the appropriate distribution,310509.0
99584,426674.0,104426.0,Thanks Kapil,307494.0
99584,432653.0,,nan,
100536,432159.0,,nan,
100536,432775.0,,nan,
100536,433411.0,,nan,
100271,431062.0,,nan,
100271,431415.0,,nan,
100273,431769.0,,nan,
100273,431074.0,,nan,
100273,431061.0,,nan,
100604,432558.0,105072.0,Please refer to below. https://learn.upgrad.com/v/course/208/question/100218,314197.0
100604,432558.0,105068.0,"What does ""4 times more likely that a drug is able to produce a satisfactory result than not."" mean here?",301641.0
100604,432607.0,,nan,
100604,432660.0,,nan,
100410,431720.0,,nan,
100410,431721.0,,nan,
100410,431792.0,,nan,
100410,431869.0,,nan,
100341,431723.0,,nan,
100341,432102.0,,nan,
100341,432865.0,,nan,
100575,432632.0,105294.0,No need to do it Excel. Just explain the steps,313517.0
100575,432349.0,,nan,
100575,433302.0,,nan,
100488,432070.0,,nan,
100488,432069.0,105095.0,"Does conditions refers to sample size, mean, and standard deviation as mentioned in the question?",311254.0
100488,432069.0,105318.0,No. This is specific to type 1 and type 2 error,317514.0
100488,435882.0,,nan,
100591,432439.0,105091.0,"No ! it totally depends upon your alternate hypothesis ! If alternative Hypothesis has >, then it is on right side , then you have to find one side. If alternative Hypothesis has <, then it is on left side , then you have to find one side. If alternative Hypothesis has !=, then it is on both side , then you have to find both two side. P.S.: If I'm wrong guys please correct me ! I would appreciated that ! Thank you",311466.0
100591,432423.0,,nan,
100591,432540.0,,nan,
100591,433409.0,,nan,
100618,432622.0,,nan,
100618,432665.0,,nan,
100594,432490.0,,nan,
100594,432557.0,,nan,
100594,432630.0,,nan,
100594,432763.0,,nan,
100757,432768.0,,nan,
100757,432772.0,,nan,
100757,432856.0,,nan,
100757,433224.0,,nan,
100633,432677.0,,nan,
100633,432664.0,,nan,
100633,432704.0,105103.0,but there has given condition atmost 200,306996.0
100633,432704.0,105111.0,Thanks,306996.0
100633,432704.0,105104.0,Yes.. based on this you will have to prepare the null hypothesis...,305129.0
100633,432773.0,,nan,
100633,432938.0,,nan,
100633,432867.0,,nan,
100795,433000.0,,nan,
100795,433088.0,,nan,
100795,433105.0,,nan,
141282,610701.0,,nan,
141282,610574.0,,nan,
91362,381865.0,,nan,
91362,381874.0,,nan,
91362,382203.0,,nan,
88607,366822.0,,nan,
88607,366793.0,,nan,
88607,366979.0,,nan,
88607,367054.0,,nan,
88607,367356.0,,nan,
88607,403904.0,,nan,
87455,361687.0,,nan,
87455,363933.0,,nan,
87455,363995.0,,nan,
85717,352120.0,90668.0,Happy to help :),318495.0
85717,352009.0,,nan,
85717,352023.0,,nan,
85717,352486.0,90559.0,Thanks for update.,320008.0
85741,352478.0,,nan,
85741,352529.0,,nan,
85741,365535.0,,nan,
95957,406945.0,,nan,
95957,406987.0,,nan,
95600,404553.0,,nan,
95980,407289.0,101661.0,Thanks Ranjith,317412.0
95980,407371.0,,nan,
88354,366515.0,,nan,
80289,325442.0,84944.0,"Perfect, thank you.",300748.0
80289,345214.0,89251.0,"Answer is House maid, Entrepreneur.",304812.0
84691,346956.0,,nan,
84691,346957.0,,nan,
84691,347235.0,,nan,
84691,347541.0,,nan,
84691,347547.0,89443.0,"wao it worked, thanks",305129.0
88499,366328.0,,nan,
88499,406483.0,,nan,
86351,356240.0,,nan,
86351,358292.0,,nan,
86351,359961.0,,nan,
86351,361860.0,,nan,
85326,350093.0,,nan,
85326,351018.0,,nan,
85326,351037.0,,nan,
85326,351063.0,90527.0,thank you..,307494.0
87537,364194.0,,nan,
84096,346003.0,,nan,
84096,346029.0,,nan,
84096,346007.0,,nan,
84096,346327.0,,nan,
84096,359955.0,,nan,
84096,355153.0,,nan,
84095,346008.0,,nan,
84095,346332.0,,nan,
84095,346005.0,,nan,
87477,361855.0,,nan,
87477,404501.0,101281.0,"Hi, i got the same issue. make sure u select the box-and-whisker plots from the show me drop down",315383.0
87477,363516.0,,nan,
87570,364213.0,92467.0,"The answer was really helpful but it did not resolve my query. I am saying that if shapes overlap it is of no use, Is there a way that we can fix the shape placement . I am representing different minerals with different shapes all i want to do is to represent if one state contains more than one mineral the shapes overlap.",318344.0
87570,364213.0,92497.0,"Hey Ashish, there's no built in way to change the placement of the shapes. But there's something called jittering, https://community.tableau.com/people/sarah.battersby.0/blog/2017/07/19/jitter-those-points here's a blog similar to your issue. I hope this helps.",319721.0
87687,364110.0,,nan,
87687,364284.0,92794.0,Sorry. Refreshing page solved the issue.,320074.0
87687,364284.0,92793.0,Box Plot II video doesn't show tableau screen,320074.0
87688,364288.0,,nan,
88235,365214.0,,nan,
88242,365476.0,92599.0,"Hi Shriroop, thanks for your time to answer my doubt. I am also observing same values that u mentioned as example for response yes and no in my box plot also. But the thing is for response=NO, Entrepreneur -1495 is closer to median value(1424) right than Housemaid-1354. so, i am bit confused there, could you please explain . Thanks in advance",300733.0
88242,365476.0,92603.0,"ohh yess, i have done simple subtractions incorrectly. Thanks",300733.0
88242,365476.0,92600.0,"Hi Praneeth, For House Maid I can see difference of 70. While for Entrepreneur, its 71. So House Maid is a tad bit closer.",315471.0
95587,404564.0,,nan,
96147,409148.0,,nan,
96147,409166.0,,nan,
87268,359668.0,91648.0,"No, there is no delete option in right click menu.",314730.0
87268,361229.0,,nan,
87120,358767.0,,nan,
87120,359802.0,,nan,
87120,361216.0,,nan,
87120,363823.0,,nan,
88594,366859.0,,nan,
84696,346991.0,,nan,
96355,,,nan,
88484,366332.0,92864.0,https://www.tableau.com/support/releases/desktop/10.3.1 try this version. all the latest version of the tableau software will be of 64 bit environment. read this link for more information https://www.tableau.com/support/releases/desktop/10.3.1,301649.0
88484,366332.0,92863.0,it is 64 bit. not 32.,318009.0
88484,366857.0,,nan,
81490,331971.0,,nan,
81490,361223.0,,nan,
81490,366363.0,,nan,
77909,312100.0,,nan,
77909,312094.0,,nan,
77909,312389.0,,nan,
77909,313620.0,,nan,
84927,347950.0,,nan,
84927,347989.0,,nan,
88689,367236.0,,nan,
88689,367306.0,,nan,
88689,367440.0,,nan,
88689,367733.0,,nan,
88689,367888.0,,nan,
88689,368092.0,,nan,
82446,339792.0,88010.0,Thanks sagar. I checked on tableau website i got 8.1 version for 32bit let me see if i can get 10.4,312623.0
82446,338670.0,87802.0,I am using 8.1 deaktop version which support 32bit. I do not see any other desktop version >8.1 which supports 32bit windows 7machine... my concern will I able to complete all the assignment on this version or do i need to uograde my machine to 64bit?,312623.0
88586,367062.0,92952.0,"yes.thankyou for the heads up..though I also completed it ,I felt It could have been applicable to a question from the sheet",300684.0
87752,364298.0,92281.0,"If you are referring to the fatal error you got while installation, check response to this quesion, https://learn.upgrad.com/v/course/208/question/87761",319721.0
87752,364298.0,92277.0,it is not opening.about tabaleu software to run and showing of log file,306996.0
95421,,,nan,
91068,380488.0,,nan,
91068,380492.0,,nan,
86060,355826.0,,nan,
86060,352952.0,,nan,
86060,353371.0,,nan,
86060,355829.0,,nan,
86060,355937.0,91348.0,It worked.. Thanx for support,311117.0
86243,354517.0,90719.0,Thanks 👍,306243.0
86977,358214.0,,nan,
86977,358881.0,,nan,
86977,358389.0,91489.0,well the question states that there IS a common variable across the multiple data sources (check box 3) - hence there is only one right answer which is the first option in the Multiple choice,300694.0
86977,358389.0,91545.0,yes.. but just having a common variable wont work in merging when your data belongs to DIFFERENT data Sources.,305845.0
86917,357770.0,,nan,
86917,358334.0,,nan,
86917,358421.0,,nan,
87388,363727.0,,nan,
87489,362005.0,,nan,
87489,363869.0,,nan,
87563,362870.0,,nan,
87502,362000.0,91999.0,"ok , thanks",312479.0
87700,364068.0,92187.0,Thank you for the detailed response.,315471.0
87700,365032.0,,nan,
87441,361688.0,,nan,
87732,364180.0,,nan,
87732,364181.0,,nan,
87760,,,nan,
86969,358073.0,,nan,
89352,370519.0,,nan,
89352,370521.0,,nan,
89352,372306.0,,nan,
88308,365466.0,,nan,
88308,366170.0,92815.0,yes the upgrad videos allows 1.5x speed. I wanted to check if the official tableau videos can be watched @1.5x,315383.0
88308,366170.0,92859.0,Oh I'm sorry I miss understood this. I thought you were talking about Tableau videos on the UpGrad platform.,319721.0
94164,396709.0,,nan,
95527,405263.0,101093.0,Thanks that helps.,318458.0
96426,411186.0,,nan,
95617,404709.0,,nan,
95617,404741.0,,nan,
95617,405525.0,,nan,
95617,405748.0,,nan,
94614,399532.0,100288.0,"In tableau, I have created 2 columns using split (Category list). I want to use one of these columns for reference. (Primary category). It is showing sheet reference as ""calculation"" and can not be used while giving the left join. My question was, can I use the custom split column directly for union. When searched at internet, it is said using columns created by custom split is not possible and you have to export data to excel , and re-import then use. Is that the only option?",311859.0
95279,402920.0,100602.0,I assume there will be many records for salary in July month,315679.0
95279,402968.0,100601.0,Agree then how one answer for percentage difference.,315679.0
95279,403163.0,100890.0,Cool,310974.0
95279,403163.0,100886.0,BTW i reported a mistake and they accepted that there was problem in framing the question. And they have corrected the question. Because difference of AVG SALARY and SALARY can't be a single answer.,315679.0
95283,402991.0,,nan,
95283,403003.0,,nan,
95283,403086.0,,nan,
95289,403054.0,100630.0,"I'm sure it is, in industry. But, is Tableau Desktop mandatory for the completion of the course?",306733.0
95289,403054.0,100724.0,Ok. Thank you :),306733.0
95289,403054.0,100632.0,i think during assignments it will be required to create PPT's but if Tableau online can suffice the neeed then it's not required.,300735.0
95289,403155.0,100685.0,"VM's tend to take up a lot of ram. And, both the case studies state that visualizations must be created using Python and that Tableau is optional. So, is it really needed?",306733.0
95289,403432.0,,nan,
95289,403961.0,,nan,
95289,406190.0,,nan,
95289,406245.0,,nan,
95729,405420.0,,nan,
95729,405506.0,,nan,
95729,406267.0,,nan,
95743,405548.0,,nan,
95743,405546.0,,nan,
95743,405656.0,,nan,
95760,405531.0,,nan,
95760,405541.0,,nan,
95760,405713.0,,nan,
95760,407208.0,,nan,
98674,420158.0,,nan,
98674,420205.0,,nan,
88560,366527.0,,nan,
88560,366620.0,,nan,
88560,366766.0,,nan,
88560,366823.0,,nan,
88560,366902.0,92925.0,Thank you Rohit.,301649.0
88560,366860.0,92977.0,"Just wanted to add an example here -- when we go to tailor for cutting a coat, he notes down: length/27"", shoulder/17"", sleeves/23"" and so on. Looks like K/V pairs and may we call it D/M pairs :) where length, shoulder and sleeves are dimensions and 27"", 17"", 23"" are measures",301116.0
88560,366978.0,,nan,
88560,367120.0,,nan,
88560,367286.0,,nan,
88560,367324.0,,nan,
88560,367304.0,,nan,
88560,367436.0,,nan,
88560,367529.0,,nan,
88560,367648.0,,nan,
88560,367662.0,,nan,
88560,367786.0,,nan,
88560,367709.0,,nan,
88560,368064.0,,nan,
88560,368591.0,,nan,
88560,368830.0,,nan,
87766,364356.0,92287.0,"Thank you for the help, I need to change the values in csv file for response attribute.",306010.0
87766,364356.0,92469.0,Here did you unchecked the Aggregate measures option under Analysis tab.? I din't receive the plot as I have selected the respective columns in questions,318846.0
87766,364356.0,92896.0,No I did not unchecked it.,306010.0
88336,365708.0,,nan,
88336,366062.0,,nan,
88428,365953.0,,nan,
88428,365901.0,92734.0,"Hi, May i know how to apply aggregated functions only on one measure(Balance) but not on the other measure (Salary)?",318328.0
88428,365901.0,92849.0,Hi Rajasekhar Fist drag the salary filed to columns then right click on salary on select it as dimension,319444.0
88428,365901.0,93025.0,"A line chart was coming after this, but changing the mark to ""shapes"" gave this scatter plot.",318436.0
88428,365892.0,,nan,
88428,366139.0,,nan,
88428,366149.0,92778.0,Yes it is working. Thank you.,320195.0
88428,366149.0,92775.0,Changing the automatic option below Marks dropdown to circle should work.,319302.0
88428,366149.0,93274.0,You can see the answer clearly in this plot but target is to do scatter plot :),313228.0
88428,366149.0,100613.0,"True, We can get the answer but we need to get the answer by scatter plot and not line chart",301124.0
88428,366185.0,92784.0,Thanks Anand,310502.0
88428,366380.0,92838.0,You can go for more option to select csv file.,320195.0
88428,366787.0,,nan,
88428,368116.0,,nan,
88428,403522.0,,nan,
88438,365984.0,92723.0,Thanks Shri,318335.0
88477,366085.0,,nan,
88477,368384.0,,nan,
88477,400496.0,,nan,
93748,395098.0,,nan,
93748,395611.0,,nan,
88527,366382.0,,nan,
88527,378311.0,,nan,
88527,367294.0,,nan,
87501,,,nan,
82562,338518.0,92207.0,Thanks man! This was helpful,308962.0
82562,338678.0,,nan,
82562,343996.0,89645.0,I think this is a right answer,306011.0
82562,343996.0,92566.0,How did you get avg balance in the scatter plot?Its not happening with me.I had the same rows and columns applied and i am not getting the option of scatter plot available.i got the right answer through bar chart.Any changes you made in the Analysis tab?,305655.0
82562,345099.0,,nan,
82562,365492.0,100519.0,"i used the same ,,",318005.0
87558,362862.0,,nan,
87558,363612.0,,nan,
87558,366458.0,,nan,
87558,369470.0,,nan,
83563,343745.0,,nan,
83563,343749.0,88667.0,thanks but the problem is i am selecting text option then csv file bank marketing and its not splitting the data automatically any idea ?,300687.0
83563,343861.0,89078.0,unfortunately it is not working .even i tried reinstalling,300687.0
83563,355891.0,91727.0,i have been using the first option saving as excel and it does work but when i try the text file and select the csv i face the above issue so if you can advise on it but now my tableau access is expired but still your assistance is appreciated,300687.0
83563,355891.0,91729.0,"Maya when you have saved it as Excel Workbook, then export in Tableau as Excel Worksheet data source. Don't Use CSV then , it will work. And for Tableu use a different ID and re register it. It will give you another 15 days. Let me know if you still face issue.",315028.0
83563,355891.0,91732.0,pls refer my screen shot it would help you in understanding my query,300687.0
83563,355891.0,91731.0,its works when i save and import as excel but my query is what could be the issue why the data doesnt get seperated when i import the csv file because in tableau it has to be seperated wherein in ms excel we do is manually for csv files,300687.0
83596,344048.0,,nan,
86443,356140.0,90903.0,Got it. Thanks a lot for the detailed clarification.,314361.0
80782,,,nan,
88816,368411.0,,nan,
82801,339565.0,88091.0,Still my query is not resolved.,300690.0
82801,339979.0,,nan,
82801,340156.0,,nan,
82801,340475.0,88096.0,"If you want, you can make the graph look like a Scatter Plot. Look at the Marks card on the left shelf. Change Automatic to Shape.",301652.0
82801,359919.0,,nan,
82801,356010.0,,nan,
87467,361562.0,,nan,
87467,364804.0,,nan,
80903,329303.0,,nan,
80903,331845.0,86152.0,Thanks...Very Good Answer,301652.0
80903,331845.0,90872.0,i am not getting the same plot,308634.0
80903,331845.0,90875.0,"If you're getting a line graph (as shown in the question) by following the same steps as Pulkit, you will need to change Automatic to Shape on the left pane. Check other answers on this same question. Might help you.",301652.0
80903,331845.0,91854.0,Thanks Pulkit. Nice answer,318397.0
80903,331845.0,92549.0,"see my comment below - this is a circle graph .. not a scatter plot. Scatter plot is 'disabled'. But probably this is what answer Upgrad wanted. Even Prof RC, towards the end in his video for scatter plot says not to use scatter plots for measuring two quantitative attributes and a categorical attribute - to use bar graph instead..",300694.0
80903,365491.0,,nan,
80903,365299.0,,nan,
86522,356532.0,90963.0,Thank you !,312259.0
86522,356532.0,91238.0,You need to complete course1 first then go for course 2. Tableau module available in course 2. Thank you.,320195.0
86522,357071.0,,nan,
86522,357108.0,,nan,
86522,357766.0,,nan,
86522,358177.0,,nan,
86522,358267.0,,nan,
88881,368688.0,,nan,
88881,368410.0,,nan,
89104,369441.0,,nan,
89104,369436.0,,nan,
89104,369449.0,,nan,
87699,364689.0,92405.0,But after calculating the avg salary getting different answer with given answer.,305847.0
87699,364689.0,92406.0,Can you please describe what steps you followed?,319721.0
87699,364689.0,92465.0,Thanx got my mistake actually I were selecting the data from only 1st page in tableau and got the different answer. When I have selected all the rows till end got my answer exactly.,305847.0
87608,363609.0,,nan,
87608,363581.0,,nan,
87719,364224.0,92272.0,https://learn.upgrad.com/v/course/208/question/86443 Here's the link for a similar question.,319721.0
87764,365204.0,,nan,
89344,370505.0,,nan,
89344,370908.0,,nan,
89344,370541.0,,nan,
88239,365210.0,,nan,
88239,365707.0,,nan,
88314,365838.0,,nan,
88314,366171.0,92852.0,The file which got downloaded was bargraphs.twb and i'm not able to open it! note : not talking about excel or .csv files,311032.0
88314,366488.0,,nan,
88314,366519.0,,nan,
88296,365362.0,,nan,
88296,365524.0,,nan,
95514,405232.0,,nan,
88787,367828.0,,nan,
88787,368779.0,,nan,
88787,367881.0,,nan,
95801,405907.0,,nan,
95801,405908.0,101269.0,Thank you,316926.0
95801,406126.0,,nan,
95542,404131.0,,nan,
95916,406667.0,101423.0,Thanks Vinay,314629.0
94486,398258.0,,nan,
94486,398182.0,,nan,
95631,404989.0,101041.0,"The license key given is told to be usable only once. If Tableau is re-installed, then we will not be able to use the license key provided. May be it is better to check with mentor before trying re-installation. Start with cleaning up some space if HDD is a concern.",318007.0
95631,404989.0,101228.0,"please contact your student mentor, Tez.",319721.0
95631,404989.0,101241.0,"before uninstalling Tableau, got to license and you can deactivate and then proceed for installation",318429.0
96429,410780.0,,nan,
96429,411188.0,,nan,
95977,407201.0,,nan,
95977,407374.0,,nan,
95261,,,nan,
95678,405080.0,,nan,
95754,,,nan,
95759,405543.0,,nan,
95759,405555.0,,nan,
95759,413015.0,,nan,
96733,412873.0,,nan,
102759,439944.0,106735.0,"I feel otherwise. Beginners should start with seaborn because it's easier than matplotlib. It is a fact that seaborn is built on top of matplotlib. But, using the base provides way more flexibility. It's like using C vs using Python. C is tougher, but has so much more flexibility than Python.",306733.0
102759,439478.0,,nan,
102759,447469.0,,nan,
97326,416552.0,,nan,
95705,405326.0,101080.0,refer my Frist image I gave link second image tried with last part of link as per instruction,315455.0
95705,405450.0,,nan,
97385,416824.0,,nan,
94973,401150.0,,nan,
94973,402005.0,,nan,
94973,401442.0,,nan,
94973,402738.0,,nan,
94993,401348.0,,nan,
94993,401519.0,,nan,
96197,409280.0,101889.0,Thanks :),318481.0
98926,421136.0,,nan,
98926,420920.0,,nan,
98926,421038.0,,nan,
95771,405628.0,,nan,
95771,405761.0,,nan,
102467,438348.0,106258.0,I am trying through Laptop,318329.0
102467,438348.0,106363.0,"Same with me, please drop mail to coaching@upgrad.com keeping your mentor in cc.",314730.0
102467,438585.0,106292.0,Nothing yet,318329.0
127346,567504.0,,nan,
88605,366769.0,,nan,
88605,384553.0,,nan,
88605,368387.0,,nan,
88722,367465.0,,nan,
88722,367438.0,,nan,
88722,367540.0,,nan,
88722,367614.0,,nan,
88722,367439.0,,nan,
88568,366583.0,,nan,
88568,366566.0,92871.0,"select Name, population from country where population >(select max(population) from country); This is the correct query..Please ignore above!",318495.0
88568,366818.0,,nan,
88568,366870.0,,nan,
88568,366959.0,,nan,
88568,367417.0,,nan,
88568,367339.0,,nan,
90522,377639.0,95189.0,"do i need to add each column name to another , is there any way i write a command that selects all column",308495.0
90522,377639.0,95211.0,"To my knowledge you have to do it separately for each column in SQL. Else, integrate MySQL with python and just use ""df.isnull().sum()"" to find it for all columns at once.",318328.0
90522,378600.0,,nan,
92779,390951.0,,nan,
92779,391159.0,,nan,
92779,391091.0,,nan,
93258,392495.0,98188.0,"Think like this, you have 1 to 10 rows. Now using self join you want to join row number 2 of 1st table with row number 1 of another table (though same)",304814.0
93258,392494.0,,nan,
92287,388617.0,96891.0,I read somewhere that you don't need password for connecting to server with windows 7 . Is this true,300706.0
92287,388617.0,96888.0,by all the components you mean the connectors also,300706.0
92287,388617.0,96889.0,yes. ODBC connectors and routers as well which are part of MySQL Workbench package.,311686.0
92287,388617.0,96898.0,yes,300706.0
92287,388617.0,96892.0,I am not sure about that. You mean connecting to MySQL server?,311686.0
92287,388677.0,,nan,
92287,388695.0,,nan,
92287,389252.0,,nan,
92287,389407.0,97180.0,Now it's working fine.. thanks,300706.0
92136,387512.0,97289.0,"Hey Vishal, like Hemanth mentioned, the procedure is shown in the session.",319721.0
92136,387512.0,98656.0,what i actually meant was where can i find the DDL part of the world database or where can i see the tables as it was there in the company database!,319846.0
92136,387538.0,96905.0,what I was trying to ask is when we saw the company database there was a DDL code run and we had a Microsoft PowerPoint for it as well known as the populated database state for company in which all the tables where present on a piece of paper. Where can I find the same for world??,319846.0
92136,388713.0,97238.0,I don't want the solution to the questions. I want to first clear out my doubt,319846.0
92136,393351.0,,nan,
92782,390759.0,,nan,
92782,391094.0,,nan,
93455,393478.0,,nan,
93455,393436.0,,nan,
93141,392113.0,,nan,
92816,390918.0,,nan,
92816,390939.0,,nan,
92816,391007.0,,nan,
84651,346807.0,,nan,
84651,346924.0,,nan,
84651,346895.0,,nan,
92444,389381.0,97107.0,"So i should be able to make it 0 for empty values,correct?",317149.0
92444,389381.0,97293.0,I am a little confused. I am able to insert null values into the field. The warning comes when I am trying to get a decimal value to a double field. Can you please clarify how this works?,317149.0
92444,389381.0,97288.0,There will be an error if there is a null value assigned to a double variable. 0 will not be assigned isntead of null. Double is considered as a primitive datatype and cannot take null values.,319721.0
92444,389388.0,,nan,
87266,359641.0,92035.0,thanks,311119.0
87266,359637.0,92034.0,thanks,311119.0
91374,382016.0,96148.0,This helps.,312953.0
91374,382029.0,,nan,
91374,382048.0,,nan,
92388,389105.0,,nan,
92388,389219.0,,nan,
92388,389172.0,,nan,
88346,365697.0,,nan,
88346,365618.0,,nan,
84650,346808.0,,nan,
84650,346934.0,89387.0,I did not get this Any example can u give what happens if I join only 2 tables?,308437.0
84205,346376.0,,nan,
84205,346615.0,,nan,
84205,346760.0,,nan,
84205,348565.0,,nan,
84205,346942.0,,nan,
84206,346374.0,89258.0,Thank u for the explanation sivaprasad,308437.0
84206,346447.0,89228.0,"when the query executes, the complier did not know from which table the salary column should be considered. So, avg(salary) throws erros like not a scaler value. In case of ""Salary >= SELECT avg(salary) FROM employee"", here it knows that, table is employee and selects Salary column and perform aggregate function - AVG on Salary column and returns a scaler/fixed value to compare. Later, salary columns compares with that value. Hope, this clarified your doubt.",311502.0
84206,346447.0,89219.0,OK my question is in the above queries what happens if we use Salary >= avg(salary) Instead of using select avg(salary) from employee It didn't return anything when executed Y is it so??,308437.0
84244,346485.0,,nan,
84249,346484.0,,nan,
88434,365891.0,92813.0,Thank you,305841.0
88434,366161.0,,nan,
88476,366132.0,,nan,
88476,366159.0,,nan,
88476,366513.0,,nan,
88476,366927.0,,nan,
88645,367056.0,,nan,
88645,367327.0,,nan,
88645,367090.0,,nan,
88490,366136.0,92816.0,"Thanjs Vinay, Result Grid is showing up now",314730.0
88490,366516.0,,nan,
84819,347560.0,,nan,
84819,347539.0,,nan,
84819,347540.0,,nan,
84819,347738.0,,nan,
84819,350035.0,,nan,
84819,361529.0,,nan,
85009,348269.0,89632.0,thanks!!,316368.0
85009,350042.0,,nan,
85009,350032.0,,nan,
85009,348386.0,,nan,
85009,349432.0,,nan,
85009,349557.0,,nan,
85327,350098.0,,nan,
85327,350100.0,90101.0,"I just wanted to clear my concept of ""HAVING"" clause. Thanks for the help!!!",304815.0
85327,351144.0,,nan,
93257,392490.0,,nan,
93257,392491.0,,nan,
85328,350089.0,,nan,
85328,350106.0,,nan,
85328,351193.0,,nan,
85328,356307.0,,nan,
85350,350287.0,90125.0,Introduction to Data Management > Module 3 > Session 2 Advanced SQL > Order by Clause There is only one video in this lecture.,311061.0
85350,356293.0,,nan,
86441,356367.0,91445.0,Well explained,316066.0
86441,356367.0,92448.0,I have the same question as Chetan,318335.0
86441,356367.0,91695.0,thank you very helpful. One follow up question: 1) in the subquery - what is the need to create an employee table with alias e2 as both e1 and e2 essentially refer to the same employee table? Can e1 not be used directly?,310509.0
86441,356078.0,,nan,
86441,366163.0,,nan,
85345,350203.0,,nan,
85345,350316.0,,nan,
85345,350548.0,,nan,
85345,351194.0,90348.0,"Why would u write this query? In terms of performance, honestly this query is really bad! You could have just used from employee however, you chose to twist it to another sub query and then another sub query. Two subquery to get a result where only 1 was sufficient.. not sure whether you know this or not. Subqueries utilise a lot of resource.. its advisable not to use subqueries unless you dont have any alternative.",307176.0
85345,359061.0,,nan,
85345,352029.0,,nan,
85345,366166.0,,nan,
83295,342015.0,88292.0,and d.dept_no = 5,307843.0
83295,366169.0,,nan,
83295,342163.0,,nan,
81635,332756.0,,nan,
81635,339438.0,,nan,
81635,333339.0,,nan,
83312,342156.0,89437.0,"I was wondering if we need to take ""sum()"" of Hours grouped by Essn, as a single employee may be working in more than one project. select fname from employee where ssn in ( select essn from (select essn,sum(hours) sum_hours from works_on group by essn having sum_hours > 20) temptbl ) ;",313826.0
83312,349313.0,,nan,
83312,366168.0,,nan,
91448,382940.0,,nan,
91448,383732.0,,nan,
91448,382463.0,96076.0,did not help the error...,305845.0
91448,382425.0,96068.0,"Tried that a million times now :( complete error: Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '() as total_salary, sum(salary) over (partition by dn' at line 6 0.000 sec",305845.0
91448,382425.0,96069.0,Can you try without using over(). Just to see if rest of it is fine.,318084.0
91448,382425.0,96070.0,yes everything else is fine. Just facing the issue with over(),305845.0
91448,382425.0,96072.0,"Just shutdown and reopen mysql and try executing the SQL statement. Also for this specific statement, the result should be the same irrespective of whether over() is used or not as the window refers to the entire table.",318084.0
91448,382425.0,96075.0,Tried shutting down... yes true.. but about the other statements that are executed on partitions?,305845.0
91448,382425.0,96077.0,"What I was saying was, the line where you are calculating sum(salary) over() as total_salary is the same without over() also. For the other statements there is a partition defined and you said it is working fine.... right?",318084.0
91448,382425.0,96093.0,https://stackoverflow.com/questions/23515347/how-can-i-fix-mysql-error-1064 check the above link to see if any of the solutions provided there is helping you.,318084.0
91448,382494.0,96098.0,Yes Suja,305845.0
91448,382525.0,96099.0,"I've gone through this article.. but did not help.. are you able to run the query? if yes, then what version of sql you are using?",305845.0
91448,382525.0,96107.0,okay.. i am using 6.3.. i guess that is the issue,305845.0
91448,382525.0,96101.0,I’m able to run. I’m using v8.0.12,318328.0
91448,382526.0,96122.0,tried... did not work,305845.0
91448,382540.0,96123.0,yes that is a possibility but not here.. ; is not the problem definitely.. Thank you,305845.0
91448,382751.0,96124.0,Thanks rishi.. I tried but of no help,305845.0
92362,388982.0,,nan,
92362,389111.0,,nan,
92362,388993.0,,nan,
92362,389220.0,,nan,
92523,390076.0,,nan,
92523,390068.0,,nan,
92523,389822.0,,nan,
92523,389814.0,97419.0,i went through this but didn't get what to do..,300735.0
92523,389800.0,,nan,
92523,390309.0,,nan,
92523,390460.0,,nan,
92523,390479.0,,nan,
86976,358074.0,,nan,
86976,358247.0,,nan,
86976,358284.0,,nan,
86976,358908.0,91518.0,"Thanks for this answer but a small correction Small correction in your query, select * from employee e where e.salary >= (select avg(salary) from employee s where e.dno = s.dno); table name is ""employee"" and in nested query we need to use avg(salary)",318328.0
86976,358908.0,91524.0,"thanks , but don't know I am not able to correct .....edit option is not showing",320685.0
86976,359013.0,,nan,
86976,364680.0,,nan,
86727,357825.0,,nan,
86727,357958.0,,nan,
86727,358052.0,,nan,
90980,379655.0,,nan,
90980,379817.0,95654.0,yes that (;) is necessary since that ends the sentence and now the code has to be delimited using $$,316349.0
90980,379817.0,95639.0,"thanks for writing have a query are we supposed to use ; in procedure, as its starts and ends with $$",308495.0
87693,364033.0,,nan,
87693,364004.0,,nan,
87693,364003.0,,nan,
87721,364345.0,92377.0,"Try this... select dno, count(*) as Count of Employees from employee where sex = 'M' group by dno;",317418.0
87721,364345.0,92336.0,It is showing error code 1064 ‘you have an error in your SQL syntax’,318335.0
87721,364290.0,92337.0,Attach a screenshot of your Workbench.,313826.0
87721,364290.0,92335.0,It is showing error code 1064 ‘you have an error in your SQL syntax’,318335.0
87721,364290.0,92267.0,Thanks Vinay. I have not written that comment on actual screen. I just mentioned here to explain the problem. However I am facing problem with the code. Is there any error in it?,318335.0
87721,364290.0,92268.0,I could execute the same query successfully.,313826.0
87721,364290.0,92338.0,Just executed it. Deleted the whole thing and entered again. Success. Phew.,318335.0
87710,364222.0,,nan,
87710,364108.0,,nan,
92289,388716.0,,nan,
92289,388719.0,,nan,
92289,388770.0,,nan,
92289,388912.0,,nan,
92289,389125.0,,nan,
92289,389332.0,,nan,
92289,389373.0,,nan,
92289,389343.0,,nan,
91002,380448.0,,nan,
91002,379829.0,,nan,
91002,380689.0,,nan,
91002,380508.0,,nan,
91580,385411.0,,nan,
91580,383378.0,,nan,
91580,383532.0,96261.0,Try this - https://www.w3schools.com/sql/func_mysql_str_to_date.asp,318085.0
91580,383532.0,96260.0,How this conversion can be done ?what is the code for that?,300690.0
91587,383403.0,,nan,
91587,383406.0,,nan,
91587,383415.0,,nan,
87647,363798.0,,nan,
87647,366366.0,,nan,
88138,364924.0,,nan,
91597,383929.0,,nan,
91597,383966.0,,nan,
88256,365178.0,,nan,
88256,365339.0,,nan,
88256,365784.0,,nan,
92798,390820.0,,nan,
92798,390807.0,,nan,
92798,391097.0,,nan,
90498,377518.0,95301.0,It is showing that Table 'company_db.bank_marketing' does not exist.,300690.0
90568,378748.0,95564.0,"ohkk! got it update is the key ,,",308495.0
90568,377805.0,,nan,
90568,377918.0,95302.0,when am doing this it add Address column Null values. It doesnt give the address as given in employee,308495.0
90568,378172.0,,nan,
92117,388640.0,,nan,
92117,388667.0,96899.0,but why not ??,319846.0
92117,388667.0,96910.0,"Because Dnumber is not unique for a row. There are values repeated in it right? If there are three 5s in both the rows, how will we know what 5 in one table to be joined with the one in the othertable? That's why we need a relation which has usique values and completes the joining of two table without any ambiguity.",319721.0
92117,388667.0,97239.0,"Okay, I got it now.",319846.0
91196,380946.0,,nan,
91196,380950.0,95811.0,Thanks for the confidence buddy,307176.0
91196,381049.0,,nan,
91196,381115.0,,nan,
91196,382210.0,,nan,
91196,381386.0,95882.0,this list is WRONG. e.g. user defined functions and stored procedures ARE available in MySQL and we are learning that and using that as part of the Upgrad course,300694.0
91196,381386.0,95908.0,"• Built-in functions and user-defined functions An error occurs if you try to create a UDF with the same name as a built-in function. • Built-in functions and stored functions It is possible to create a stored function with the same name as a built-in function, but to invoke the stored function it is necessary to qualify it with a schema name. For example, if you create a stored function named PI in the test schema, invoke it as test.PI() because the server resolves PI() without a qualifier as a reference to the built-in function. The server generates a warning if the stored function name collides with a built-in function name. The warning can be displayed with SHOW WARNINGS. • User-defined functions and stored functions User-defined functions and stored functions share the same namespace, so you cannot create a UDF and a stored function with the same name. The preceding function name resolution rules have implications for upgrading to versions of MySQL that implement new built-in functions: • If you have already created a user-defined function with a given name and upgrade MySQL to a version that implements a new built-in function with the same name, the UDF becomes inaccessible. To correct this, use DROP FUNCTION to drop the UDF and CREATE FUNCTION to re-create the UDF with a different nonconflicting name. Then modify any affected code to use the new name. • If a new version of MySQL implements a built-in function with the same name as an existing stored function, you have two choices: Rename the stored function to use a nonconflicting name, or change calls to the function so that they use a schema qualifier (that is, use schema_name.func_name() syntax). In either case, modify any affected code accordingly.",306242.0
92015,386110.0,,nan,
92015,387569.0,,nan,
92015,387380.0,,nan,
92015,389928.0,,nan,
91117,380637.0,95753.0,"yes. Since you are not writing any where clause to restrict rows, it will update all the rows.",310511.0
91117,380637.0,95752.0,I have one more doubt in this set command that it will update all the rows with corresponding value right?,320073.0
91117,380666.0,,nan,
91131,380717.0,,nan,
91131,380734.0,,nan,
91131,380866.0,,nan,
91131,382597.0,,nan,
91835,384760.0,,nan,
91835,384722.0,,nan,
91837,384728.0,,nan,
91842,384773.0,,nan,
91842,384792.0,,nan,
91842,384750.0,,nan,
91842,389931.0,,nan,
91870,385206.0,,nan,
91871,385091.0,96523.0,"Thanks also help do need use LOAD DATA INFILE 'c:/tmp/discounts.csv' INTO TABLE discounts FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS;",307843.0
91871,385091.0,96595.0,Can you please explain in detail. I am not getting what you are saying.,317689.0
91871,385205.0,96524.0,Thanks,307843.0
91871,390827.0,,nan,
91320,381647.0,,nan,
91320,381610.0,95931.0,I am getting very close to 13000.,318585.0
91320,381610.0,95932.0,Thank you! I will check it again.,318585.0
91320,381630.0,,nan,
91320,381745.0,,nan,
91295,381576.0,,nan,
91311,381553.0,,nan,
91311,381570.0,,nan,
93082,391889.0,,nan,
93082,391918.0,98190.0,what if I am getting multiple rows as an output?,302739.0
93082,392181.0,,nan,
92868,391110.0,,nan,
92868,391133.0,,nan,
92769,391089.0,97630.0,"Thanks, issue was sorted out with correct field length",320648.0
92769,390724.0,,nan,
92769,390686.0,97623.0,"Thanks all , proper field length should be given. For Data Type Text it may not accept PK. Need to change this to VARCHAR",320648.0
92769,391514.0,,nan,
93422,393307.0,,nan,
93422,393313.0,,nan,
93422,393339.0,,nan,
93422,393730.0,,nan,
93422,393760.0,,nan,
93488,393595.0,,nan,
92622,,,nan,
92531,389820.0,,nan,
92531,389845.0,,nan,
92531,389837.0,,nan,
92531,390511.0,,nan,
92745,390609.0,,nan,
92745,390687.0,,nan,
92745,391025.0,,nan,
92749,390647.0,,nan,
92749,390995.0,,nan,
92749,391379.0,,nan,
92723,390558.0,,nan,
92723,390555.0,,nan,
92616,390798.0,,nan,
92616,390229.0,,nan,
93222,392411.0,,nan,
93224,392379.0,,nan,
91874,384997.0,96569.0,Thank you :),305845.0
91874,384997.0,96570.0,The summary also right?,305845.0
91874,385031.0,96571.0,The summary also?,305845.0
91874,385031.0,96597.0,"No, summary should include all. Since we will be creating buy and sell signals for all stocks.",301643.0
91874,385031.0,96886.0,okay.. Thank you,305845.0
94707,,,nan,
95375,403533.0,,nan,
95375,403970.0,,nan,
95375,403749.0,,nan,
93251,392474.0,98180.0,"I have calculated using nested subquery, which also works in same manner.",318362.0
93251,392524.0,98216.0,"Hi Vikas, I am not sure how to raise a issue ticket. Please let me know and i will raise one with detailed description.",318362.0
103619,444889.0,,nan,
103619,444511.0,,nan,
103619,444598.0,,nan,
103619,444853.0,,nan,
103619,445029.0,107444.0,"Thanks, that was quite helpful",318344.0
103619,446350.0,,nan,
91375,382020.0,,nan,
91375,382022.0,,nan,
91375,382015.0,,nan,
91375,382059.0,,nan,
78633,318394.0,,nan,
78633,317815.0,,nan,
78633,350743.0,,nan,
78633,332616.0,,nan,
87131,358856.0,91556.0,thanks,310385.0
84701,347137.0,,nan,
80584,327241.0,,nan,
80584,327269.0,85179.0,Thank you,301655.0
80584,328098.0,,nan,
82297,341873.0,,nan,
82297,337187.0,,nan,
82297,337202.0,,nan,
81437,,,nan,
78954,319027.0,83711.0,Thanks ramya,305838.0
78954,319248.0,83784.0,"already gone through this the issue was resolved way earlier, anyhow thanks for your assistance appreciated",305838.0
78954,376171.0,,nan,
77554,310144.0,81937.0,Welcome Shalina! :),300688.0
77554,310144.0,81932.0,Thanks Prakash!!,300686.0
77554,311947.0,,nan,
77554,316736.0,,nan,
77554,366172.0,,nan,
77554,336958.0,,nan,
81607,332410.0,,nan,
81607,332973.0,,nan,
81607,332428.0,,nan,
81607,333018.0,,nan,
81607,335798.0,,nan,
81607,333101.0,,nan,
87202,359151.0,,nan,
87202,359680.0,,nan,
87202,364802.0,,nan,
79199,321989.0,84331.0,"Anshul that is for ubuntu I am using windows , executing the statement in cmd or mysql server client?",305838.0
79199,321989.0,86037.0,"i actually reinstalled it since there were some errors during installation. After that when i tried it , it worked.",301114.0
79199,321989.0,86036.0,Can you check if this works? https://www.youtube.com/watch?v=fZ0P71gGjeE,301114.0
79199,329058.0,97232.0,"Hi Anshul, I followed every step as in the video and the following error is coming. mysqld : The term 'mysqld' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. At line:1 char:1 + mysqld --defaults-file=""C:\\ProgramData\\MySQL\\MySQL Server 8.0\\my. ... + ~~~~~~ + CategoryInfo : ObjectNotFound: (mysqld:String) [], CommandNotFoundException + FullyQualifiedErrorId : CommandNotFoundException",320603.0
79199,388631.0,,nan,
81690,332837.0,,nan,
81690,333052.0,,nan,
84133,346420.0,,nan,
84133,346133.0,,nan,
87213,359263.0,91591.0,Thank you for the quick response.,320195.0
87213,359839.0,,nan,
87213,364793.0,,nan,
84931,347948.0,,nan,
84931,347997.0,,nan,
84931,351229.0,,nan,
84931,349946.0,,nan,
88555,366509.0,,nan,
88555,366533.0,,nan,
88575,366981.0,,nan,
88575,366593.0,,nan,
88575,366587.0,,nan,
88575,366814.0,,nan,
88575,367296.0,,nan,
82028,334902.0,,nan,
82028,335780.0,89271.0,yes,304319.0
82028,335780.0,89233.0,did you use the developer version?,311857.0
82028,336181.0,89232.0,why is it necessary to install Visual C++?,311857.0
80253,325198.0,,nan,
80253,325275.0,84890.0,"Yea, I get that. But, my question was if I can use MySQL in the place of PostgreSQL. And I get that we can't.",306733.0
80253,325303.0,,nan,
80253,353537.0,,nan,
82229,336624.0,87189.0,"I just downloaded it and after clicking on it, there is a dialog box and in that i clicked on RUN. Just after this only that error is coming up....",300719.0
82229,336624.0,87190.0,have u installed the current version of my sql check in your control panel and uninstall the my sql if any install then retry installtion as per sussested in previous post,307843.0
82229,336624.0,87194.0,"no i have not installed anything....its just after downloading the file when you click on it...its says ""Do you want to run this file?"" and when i click on run, then that error appears.",300719.0
82229,336624.0,87195.0,what is the error can you provide the screenshot.,307843.0
82229,336624.0,87197.0,Please see the screenshot.,300719.0
82229,336624.0,87207.0,its in different comment under my question.,300719.0
82229,336624.0,87199.0,I do not find any screenshot pls resend :),307843.0
82229,336629.0,87196.0,Windows 7,300719.0
82229,336641.0,,nan,
82229,337200.0,,nan,
91898,385194.0,96815.0,https://learn.upgrad.com/v/course/208/session/15786/segment/79810,318791.0
91898,388681.0,,nan,
90210,375775.0,,nan,
90210,375764.0,,nan,
90210,375882.0,95027.0,I need to install on Macbook,317990.0
90210,376643.0,,nan,
87420,,,nan,
83416,342574.0,,nan,
83416,342686.0,,nan,
83416,342865.0,,nan,
93261,392496.0,,nan,
93261,392511.0,,nan,
93261,392515.0,,nan,
91216,381113.0,,nan,
91216,381111.0,,nan,
91216,381103.0,95828.0,"once installation is complete, i have not got the options of 1)authentication method 2)accounts and roles 3)windows service4)apply config 5)product config 6) router config etc. What can be done",314678.0
91216,381124.0,95834.0,Did all the steps got completed prior to this ?,317991.0
91216,381124.0,95831.0,"once installation is complete, i have not got the options of 1)authentication method 2)accounts and roles 3)windows service4)apply config 5)product config 6) router config etc. What can be done",314678.0
91216,381124.0,95863.0,yes,314678.0
91216,381767.0,,nan,
91216,381765.0,96029.0,not resolved asking for .net 4.5.2,314678.0
91216,381765.0,96014.0,https://dev.mysql.com/resources/wb52_prerequisites.html Here are the .NET pre-requesites that are to be installed. Please make sure you have these,319721.0
91216,381811.0,95983.0,have attached few screen shots too,314678.0
91216,381811.0,95981.0,Have downloaded but not able to complete my installation,314678.0
86413,356221.0,,nan,
85269,349681.0,,nan,
81165,329785.0,,nan,
81165,349612.0,,nan,
81165,356569.0,,nan,
81165,386135.0,96598.0,"for me it happned only once, after that it is working smoothly, I restarted my system after closing mysql, you can try it too... if dosent work, i would suggest to uninstall and do the installation again, as per guidlines given in beginning module.",306012.0
85312,349969.0,97313.0,I have tried with above code. But all SSN are displayed.Am not getting why it s so?,308638.0
85312,350034.0,,nan,
85312,350082.0,90074.0,ok,300690.0
85312,350082.0,90063.0,"it will check whole address in ""Houston"". Which will not give desired result.",312746.0
85312,350145.0,90096.0,thanks 😊. will try this as well,317982.0
85312,350137.0,90095.0,https://www.w3schools.com/sql/sql_wildcards.asp might be useful,317982.0
85312,350311.0,,nan,
85312,352386.0,,nan,
91250,381374.0,,nan,
91250,381343.0,,nan,
91250,381365.0,,nan,
80974,329175.0,86340.0,"Hi Vijay, As instructed in SQL installation guide - windows, i downloaded the ''mysql-installer-community-8.0.12.0'' from following link - https://dev.mysql.com/downloads/installer/ & the file selected was - '' Windows (x86, 32-bit), MSI Installer 8.0.12 273.4M (mysql-installer-community-8.0.12.0.msi).'' During installation it i s giving error as- 'no compatible server were found and you will need to cancel this wizard and install one'. i am useing WINDOWS 7 64 Bit. Thanks, Amarjit",306245.0
80974,335804.0,,nan,
80974,333097.0,,nan,
88798,367916.0,,nan,
88798,367959.0,,nan,
88798,367970.0,,nan,
88798,368022.0,,nan,
88798,373490.0,,nan,
83632,343673.0,88659.0,I have uninstalled all products while trying again. There isn't.,302742.0
83632,343673.0,88702.0,Is it because my OS is 32 bit? Coz when I installed it in 64 bit OS it's working absolutely fine.,302742.0
83632,345940.0,89134.0,Thank you,302742.0
83632,369447.0,93443.0,In that case please ensure that Visual C++ and other softwares are X32 version if your computer is 32 bit version because yes you need a 64bit processor to upgrade your OS to 64bit.,302742.0
83632,344226.0,88723.0,"I guess it's because my OS is 32 bit coz when I installed it in another system which is 64 bit, it's working fine.",302742.0
83632,344226.0,88735.0,"Hi Rashmi, most softwares nowadays are based on x64. Also please ensure that Visual C++ and other softwares are X32 version if your computer is 32 bit version.",301618.0
83632,344226.0,91473.0,"facing same issue as Rashmi. Tried all the methods given above, no luck. Please help.",308967.0
83632,344226.0,91585.0,"Hey Shilpa, I upgraded my OS from 32bit to 64bit, installed .net framework , Visual c++ and then installed MySQL. Its working fine now.",302742.0
83632,344226.0,91726.0,Thanks a lot Rashmi. I will try doing the same.,308967.0
83129,341310.0,,nan,
83129,341401.0,,nan,
86478,356311.0,,nan,
86478,356407.0,,nan,
86478,357042.0,,nan,
83282,341921.0,,nan,
83282,342572.0,,nan,
83282,341937.0,,nan,
83282,342123.0,88366.0,please create that table and check,301648.0
83282,342123.0,88365.0,Table exists as bank-marketing in the schema.while typing bank_marketing gives error showing such table does not exist.,300690.0
83282,342350.0,,nan,
83282,342404.0,88385.0,In the schema itself delete the bank-marketing table and recreate bank_marketing. Something wrong in the schema,304319.0
83282,342404.0,88386.0,How editing would be done in schema SQL text file?,300690.0
83282,342404.0,88388.0,Its is a script file. Open it in workbench and make the changes.,304319.0
83282,342404.0,88393.0,Thanks a lot.got it.,300690.0
83282,342404.0,88389.0,dont forget the delete the previous created tables in the workbench.,304319.0
83282,342442.0,,nan,
81083,329258.0,,nan,
81083,330045.0,,nan,
81083,332285.0,,nan,
85472,350808.0,,nan,
85472,350800.0,90205.0,Thanks Alok. Third option has worked and i have ran the script But i am not able to see the tables after clicking on drill down TABLES under my schema. still i am getting data when i am executing the DML statements. can u help me on this,300733.0
85472,350800.0,90207.0,Thank u,300733.0
85472,350800.0,90206.0,u need to refresh schema once your query got executed. just right click on companydb schema which is in left pane of your screen and click on refresh.,312746.0
82838,,,nan,
82841,339701.0,,nan,
82841,355975.0,,nan,
85490,351151.0,,nan,
85492,350905.0,,nan,
85492,351017.0,,nan,
85492,351161.0,,nan,
85624,351315.0,,nan,
85624,352026.0,,nan,
87371,360821.0,,nan,
87371,361279.0,91850.0,I have checked both. both are ok. video streaming is seamless,312199.0
87371,361542.0,,nan,
87371,361572.0,,nan,
87371,376863.0,,nan,
85715,351930.0,,nan,
85715,352024.0,,nan,
85715,352527.0,,nan,
88897,368457.0,,nan,
88897,368461.0,93254.0,Sir is not using superstore data for sql . He is referring to the following data schema provided in this page :https://learn.upgrad.com/v/course/208/session/15786/segment/79808,313691.0
88897,368461.0,93256.0,The data shown in the video for SQL can be loaded into the MySQL workbench by running this mysql script file :https://cdn.upgrad.com/UpGrad/temp/ec1b0d95-11ee-40ae-86d9-7f4c8cacc9e7/company_db.sql,313691.0
88897,368461.0,93255.0,My bad.. I was thinking about tablaeu. You are right.,320195.0
88897,368458.0,93257.0,ok. thank you so much.,305335.0
88897,368458.0,93253.0,You will have access to the database schema in the following link: https://learn.upgrad.com/v/course/208/session/15786/segment/79808,313691.0
88897,368458.0,93252.0,"in the video on compound functions and relational operators, sir is showing a data on employee, dept, dept locations, workson, project, etc... i want to know how do i get that data.. using this data we were asked to solve(or rather play around) furthur queries",305335.0
88897,368458.0,93258.0,The data shown in the videos for SQL can be loaded into the MySQL workbench by running this mysql script file :https://cdn.upgrad.com/UpGrad/temp/ec1b0d95-11ee-40ae-86d9-7f4c8cacc9e7/company_db.sql Refer to the first video on this page on how to load the file: https://learn.upgrad.com/v/course/208/session/15786/segment/79810,313691.0
84052,345871.0,89047.0,wow thanks a lot !!,308437.0
84052,346006.0,,nan,
84052,346267.0,,nan,
84052,352113.0,,nan,
84052,348533.0,,nan,
86946,358104.0,,nan,
86946,358272.0,,nan,
84055,,,nan,
93785,395312.0,98963.0,already did that. first thing i did. No use. Basically MySql service wont start even though I have not done any configuration changes,317149.0
93785,395341.0,98970.0,I did check the services . It cannot be restarted . Thanks,317149.0
93785,395494.0,,nan,
93785,395504.0,98991.0,"No. I stopped looking as I already lost a lot of time trying to figure this out. My best guess is number of ports set in while installing anaconda, one of them is 3306",317149.0
93785,395504.0,98990.0,Do have another DBMS in your system apart from MySQL?,319721.0
85655,351882.0,,nan,
86523,357039.0,,nan,
86523,357760.0,91218.0,CREATE TABLE 'bankdb_demo'.'new_table'(name VARCHAR(20)); Source : https://dev.mysql.com/doc/refman/5.5/en/create-table.html,311502.0
86523,358268.0,,nan,
86110,,,nan,
86121,354129.0,,nan,
86118,354130.0,,nan,
86114,353229.0,,nan,
86114,353231.0,,nan,
86114,356507.0,,nan,
86251,,,nan,
86933,357887.0,,nan,
86933,357954.0,,nan,
86261,354491.0,90787.0,Visual C++ and .net will be required only for Windows OS. Linux: it automatically installs all the depended library.,317845.0
86261,354491.0,90715.0,Why we have to install Visual C++ and .net as mentioned in the guide?,315661.0
86261,354491.0,90798.0,Thanks Mr Rajat that was helpful.,315661.0
86261,354491.0,90803.0,Thanks Mr Amani Prasad.,315661.0
86261,354491.0,90800.0,Welcome @bhagyashree and yes as @amani said u won't be needing visual c++ and dot net. They are required in Windows machine to integrate a few components in the compiler for proper functioning of the software.,318495.0
86261,355299.0,90799.0,Thanks Amani!,315661.0
86358,355657.0,,nan,
91468,382623.0,,nan,
91468,382679.0,,nan,
91468,382724.0,,nan,
91468,382758.0,,nan,
88882,368690.0,93304.0,"Beginning configuration step: Writing configuration file Ended configuration step: Writing configuration file Beginning configuration step: Updating Windows Firewall rules Attempting to delete a Windows Firewall rule with command: netsh.exe advfirewall firewall delete rule name=""Port 3306"" protocol=TCP localport=3306 Deleted 2 rule(s). Ok. Adding a Windows Firewall rule for MySQL1 on port 3306. Attempting to add a Windows Firewall rule with command: netsh.exe advfirewall firewall add rule name=""Port 3306"" protocol=TCP localport=3306 dir=in action=allow Ok. Successfully added the Windows Firewall rule. Adding a Windows Firewall rule for MySQL1 on port 33060. Attempting to add a Windows Firewall rule with command: netsh.exe advfirewall firewall add rule name=""Port 33060"" protocol=TCP localport=33060 dir=in action=allow Ok. Successfully added the Windows Firewall rule. Ended configuration step: Updating Windows Firewall rules Beginning configuration step: Adjusting Windows service Attempting to grant Network Service require filesystem permissions. Granted permissions. Adding new service New service added Ended configuration step: Adjusting Windows service Beginning configuration step: Initializing Database Deleting the data directory from a previous (failed) configuration... Attempting to run MySQL Server with --initialize-insecure option... Starting process for MySQL Server 8.0.12... Starting process with command: C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld.exe --defaults-file=""C:\ProgramData\MySQL\MySQL Server 8.0\my.ini"" --console --initialize-insecure=on --lower-case-table-names=1... 2018-10-02T16:32:56.780072Z 0 [Warning] [MY-011071] [Server] option 'read_buffer_size': unsigned value 0 adjusted to 8192 2018-10-02T16:32:56.780139Z 0 [Warning] [MY-011071] [Server] option 'read_rnd_buffer_size': unsigned value 0 adjusted to 1 2018-10-02T16:32:56.780282Z 0 [Warning] [MY-010915] [Server] 'NO_ZERO_DATE', 'NO_ZERO_IN_DATE' and 'ERROR_FOR_DIVISION_BY_ZERO' sql modes should be used with strict mode. They will be merged with strict mode in a future release. 2018-10-02T16:32:56.780401Z 0 [System] [MY-013169] [Server] C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld.exe (mysqld 8.0.12) initializing of server in progress as process 5000 2018-10-02T16:32:57.014782Z 1 [ERROR] [MY-012576] [InnoDB] InnoDB: Unable to create temporary file; errno: 2 2018-10-02T16:32:57.014841Z 1 [ERROR] [MY-012929] [InnoDB] InnoDB: InnoDB Database creation was aborted with error Generic error. You may need to delete the ibdata1 file before trying to start up again. 2018-10-02T16:32:57.015190Z 0 [ERROR] [MY-010020] [Server] Data Dictionary initialization failed. 2018-10-02T16:32:57.015221Z 0 [ERROR] [MY-010119] [Server] Aborting 2018-10-02T16:32:57.016241Z 0 [System] [MY-010910] [Server] C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld.exe: Shutdown complete (mysqld 8.0.12) MySQL Community Server - GPL. Process for mysqld, with ID 5000, was run successfully and exited with code 1. Failed to start process for MySQL Server 8.0.12. Database initialization failed. Ended configuration step: Initializing Database",311870.0
88882,369159.0,94256.0,It didn't work for me so I am trying to use paiza.io online MySQL,311870.0
88882,369159.0,94463.0,"Hey, so can you do it now?",319721.0
91397,382531.0,,nan,
87408,360727.0,92040.0,Thanks Vinay,318335.0
87408,361545.0,,nan,
92625,390302.0,97531.0,I have started the server.Still getting issue.,318493.0
92625,390302.0,97535.0,"Does your script tab say ""unconnected""?",319721.0
92625,390810.0,,nan,
91927,385408.0,,nan,
91927,385376.0,96492.0,"thank you , got it!! But can you explain why this happens?",319846.0
93741,395052.0,,nan,
93741,395054.0,,nan,
92734,390532.0,,nan,
92734,391035.0,,nan,
92734,392084.0,97995.0,"Thnx Sush , my question is I uploaded the one given sql file and that is asking count city ..now what and which button I hv to click. Whats I understand is the schema is already there bcz file data is already loaded",319969.0
92734,392084.0,97991.0,"Hey Dinesh. I hope you have gone through the lecture video. https://learn.upgrad.com/v/course/208/session/15786/segment/79809 In the video mentioned above, the third video has all the steps. If there is any question related to the logic of a coding question, pelase specify the problem statement of the question.",319721.0
87655,363963.0,92771.0,"Yes, it is confusing some times when you have correlate the english statements with the logical operators.",310974.0
87655,363864.0,,nan,
87655,363873.0,92128.0,we don't want only Female employees right?? it could be Male as well!,316349.0
87655,363873.0,92130.0,yeah. but only whose salary <=25000 the first condition fetches the employee with salary<=2500 (irrespective of their gender) the second condition fetched all the Female employees,305845.0
87655,363924.0,,nan,
87655,369922.0,,nan,
88219,364909.0,,nan,
88219,367276.0,,nan,
88219,365076.0,,nan,
90073,375072.0,,nan,
90073,375086.0,94750.0,Thank you Vinay. It worked :),315121.0
90073,375094.0,,nan,
92086,386873.0,,nan,
92086,388841.0,,nan,
92086,387302.0,,nan,
92086,387049.0,,nan,
90281,376281.0,95094.0,not yet,310008.0
90281,376281.0,95770.0,no 32 bits Visual C++ Redistributable file are available,310008.0
89700,373701.0,,nan,
89700,373598.0,94502.0,I have enough space in C drive and the prerequiates of Dot Net framework and VC++ are also satisfied. I have attached screenshots below.,318762.0
89700,373705.0,,nan,
89700,373709.0,101232.0,did u get solution for the above error . since i am facing the same problem,303115.0
89700,373709.0,101248.0,I just ignored the error and went ahead. It worked.,318762.0
89700,374499.0,96557.0,"This did not resolve my problem. On googling, I found this problem reported by othjer users too at https://bugs.mysql.com/bug.php?id=85908. Can you please suggest an alternative way to proceed?",318762.0
89700,408610.0,,nan,
90282,376278.0,,nan,
91630,383634.0,,nan,
91630,383637.0,,nan,
91630,384475.0,,nan,
91957,385658.0,,nan,
91703,384041.0,96396.0,"i have installed it on windows 8.1,it is working. i had completed graded questions on oracle sqldeveloper but had to install mysql server 8.0 for the course . i will try my luck on installing it on linux 7.2 also",318005.0
92481,389575.0,,nan,
92481,389542.0,,nan,
92481,389552.0,,nan,
91985,385896.0,,nan,
91985,385907.0,,nan,
91985,387578.0,,nan,
91992,385944.0,,nan,
91992,386069.0,,nan,
91992,387574.0,,nan,
95799,405839.0,,nan,
91783,384438.0,,nan,
91783,384427.0,,nan,
92333,388805.0,97110.0,"It's great what you are doing here! Let me pin this post, I'm sure it's going to benefit all the learners! If everyone shares their errors and how they resolved them, it'll make the process of coding much efficient.",319721.0
92333,388805.0,97111.0,"Hey, Pranesh Can you please edit your post and add a note asking everyone else too to share their roadblocks on errors.",319721.0
92333,388805.0,97158.0,"Cool, I did it.",319721.0
92333,388805.0,97156.0,"I don't have any option to edit my post. If you can do it, please go ahead!",306733.0
92333,388814.0,96986.0,"That's good. Well, if someone does have a problem with finding the paths to the config files on their [linux] machine, they can just type: mysql --verbose --help and find the paths to the config files near the top.",306733.0
92333,388814.0,98882.0,"How to find the mysql.cnf in windows laptop, since i am getting teh same error",307489.0
92333,388814.0,98948.0,"You don't have .cnf files in windows. You have .ini files. They're probably stored in C:/Program Files. If you don't find them there, you can search from the run console. Since you need to be running a server program such as Xamp or Wamp, I suggest you to search those installation directories to find the .ini file. Hope this helps",306733.0
91167,380849.0,95921.0,na,310509.0
91167,380851.0,,nan,
91167,380854.0,95794.0,Rajat - have attached below. Thank you.,310509.0
91167,380868.0,,nan,
91167,380897.0,95878.0,"thank you ...this helped. However, one more question, I am not able to see anything under the ""information"" section on the left hand menu, where the schema was available before. How to get that done? Regards.",310509.0
91167,380897.0,95885.0,"Maybe the sidebar panel is hidden. Click on ""View--> Panels --> Show Sidebar"".",313826.0
91167,380897.0,95896.0,its not hidden...the information section is just coming as blank,310509.0
91167,380897.0,95903.0,its below,310509.0
91167,380897.0,95902.0,Please post a screen shot to look further.,313826.0
91167,381522.0,95906.0,The information panel shows details only down to a db and table. The information panel will start showing details regarding the table.,313826.0
91824,384709.0,,nan,
91824,384713.0,,nan,
91232,382040.0,96015.0,"I got an email today that it has been rectified. I believe, you saw that after it was fixed.",314048.0
91232,382040.0,96016.0,"Oh, that's great!",319721.0
91232,382040.0,96017.0,"I got an email today that it has been rectified after I raised the ticket. I believe, you saw the video after it was fixed.",314048.0
91232,382040.0,96018.0,"I got an email today that it has been rectified after I raised the ticket. I believe, you saw the video after it was fixed.",314048.0
91241,381585.0,,nan,
91241,381438.0,,nan,
91241,381279.0,,nan,
91241,381397.0,,nan,
91241,381639.0,,nan,
91241,381731.0,,nan,
91241,382291.0,,nan,
91354,381854.0,,nan,
91354,381851.0,,nan,
91354,381916.0,,nan,
91354,383254.0,,nan,
91841,384764.0,,nan,
91192,380940.0,,nan,
91192,380953.0,,nan,
92595,390350.0,,nan,
92595,390053.0,97310.0,"also depends on your operating system, in windows queries are not case sensitive for table and column names but in another operating system it might be ..",300694.0
92595,390053.0,97303.0,but its not getting executed,317558.0
92595,390053.0,97304.0,"but in terms of logic there is a mistake - how can a plocation be bellaire AND sugarland? it can only be OR so you want to do select * from project where plocation = 'bellaire' OR plocation = 'sugarland'; while this will give you results, it won't give you the answer to your original question - for that you need to use the operator and also be aware of de morgan's theorem",300694.0
92595,390053.0,97305.0,also try writing it all on one line - it might make things easier,300694.0
92595,390053.0,97307.0,using or also but still its same,317558.0
92595,390053.0,97308.0,is 'select * from project' working; ?,300694.0
92595,390053.0,97309.0,yes,317558.0
92595,390053.0,97311.0,please post a screenshot of the outcome of 'select * from project;' and also 'select plocation from project;',300694.0
92595,390102.0,97314.0,"as replied by him already, he tried OR also",300694.0
92595,390102.0,97315.0,ohh.. didn't see the comments on your answer. if it's not working with an OR as well then it's strange. some spelling issues might be there in that case.,311686.0
92595,390118.0,97328.0,still same. not showing output,317558.0
92595,390849.0,,nan,
92595,390757.0,,nan,
92601,390152.0,,nan,
92601,390140.0,,nan,
92601,390108.0,,nan,
92601,390224.0,97361.0,No...Some other places are there as well,308638.0
92601,390507.0,,nan,
92601,390765.0,,nan,
92764,390644.0,97496.0,Thanks I could able realise after shooting a query!!!!,316399.0
92764,390702.0,97506.0,Thank you for clarification,316399.0
127477,557706.0,,nan,
107032,462279.0,110387.0,If you are trying to separate car name and company try using split method of string on the series and split on white space -> ' ',311160.0
107032,462279.0,110389.0,Replace function should not be used in this scenario,311160.0
107032,462279.0,110398.0,"First use replace as str.replace ('-',' ') and then str.split(' ').str[0] .Hope this will work.",319006.0
107032,462300.0,,nan,
107032,462324.0,,nan,
107032,462442.0,,nan,
107032,463224.0,,nan,
107032,463956.0,,nan,
107032,462357.0,,nan,
103689,445199.0,107458.0,"no , its nothing to do with data cleaning both columns have no NaN values. Further , sample solution countplot also showing that the 36 months term has more defaults . Issue is with bar plot that showed in sample solution I guess.",305652.0
103689,445331.0,107510.0,"ohh cool, got it",301655.0
103689,445331.0,107529.0,"Hi Prathap, Longer term loans has more uncertainty of unforeseen circumstances, as more time adds to the uncertainty. That's why the company also prefers 30 months; you will notice more 36 months loans.",334535.0
103689,445331.0,107512.0,"still not clear , even though bar plot showing fraction ,how 60 months loan term has more defaults than 36 month ? count or fraction it should reflect the correct analysis ?",305652.0
102251,437256.0,,nan,
102251,437214.0,,nan,
102043,436850.0,,nan,
102043,437092.0,,nan,
102043,437381.0,,nan,
99955,428428.0,,nan,
99955,428415.0,,nan,
100421,432403.0,105189.0,can you please elaborate a bit more on what you mean by using our own function using apply?,310509.0
100421,432406.0,,nan,
100421,437180.0,,nan,
100427,435726.0,,nan,
100427,432098.0,,nan,
100399,431632.0,,nan,
100399,431663.0,105497.0,How did you get the figure of 77% ? Is it mentioned somewhere in any of the lectures or assignment?,311729.0
100399,432101.0,,nan,
100399,433227.0,,nan,
100399,435570.0,,nan,
100399,435848.0,,nan,
100399,436247.0,,nan,
102430,438124.0,,nan,
102430,438065.0,106196.0,ok.. ll chk that.. thnx,303670.0
102430,438183.0,,nan,
102430,438482.0,,nan,
102430,438618.0,,nan,
101070,435124.0,105582.0,"Actually I got it. When I checked in Notepad++ or even in Python, dates are looking correct. So basically date is not give in the data its just the month and year, so may be excel has interpreted it differently..Thanks",304814.0
101070,435124.0,105581.0,"Hi Veena, I got 39718 rows but issue date is only 2018. Moreover, the first row which you mentioned is actually the second row in my dataset. What am I missing here?",304814.0
101070,435545.0,105554.0,"Download the file from Upgrad. When you open it in excel, it conatins issue_d having dates from 2007-2011",304319.0
101070,435550.0,,nan,
101070,435999.0,,nan,
102376,437768.0,,nan,
102376,437806.0,,nan,
102376,437956.0,,nan,
102376,438812.0,,nan,
102376,446144.0,,nan,
101154,435143.0,105488.0,"Thanks for the explanation. But what I don’t get is how it impacts the study? So I was looking at total_invstmnt and total_invstmnt_inv and I noticed completely different values. Also, I did go through the data dictionary but there’s not much of an explanation there.",318381.0
101154,435143.0,105553.0,I think you need not bother much about them. Just take any one of them for your study as there is a difference of only 5% or less between the two values.,304319.0
101154,435143.0,105647.0,Does it matter from where loan sanctioning authority is providing the loans (i.e. will it be considered as a driving factor)?,311117.0
101154,435143.0,105821.0,So is that mean bank is lending money from investor and giving it to borrower when his loan got approved ?,301108.0
101154,436002.0,,nan,
101194,435560.0,105569.0,"Hi Veena, You need to review the columns in loan.csv, the descriptions of the columns is mentioned in data dictionary. And as Ruchita mentioned, there are lot of columns in loan.csv that are needed to be analyzed.",334535.0
101194,435856.0,,nan,
101194,435855.0,,nan,
100561,432214.0,,nan,
100561,432409.0,,nan,
101208,435559.0,,nan,
101208,436044.0,,nan,
101236,435568.0,,nan,
101236,436010.0,,nan,
101236,436108.0,,nan,
100349,,,nan,
101223,435542.0,105572.0,"Hi Ranjana, In addition to what Chandan mentioned, note ""Charged-off"" is a financial/accounting terms and it means written off, or, the loss has been accepted.",334535.0
101223,435910.0,,nan,
101223,436008.0,,nan,
101220,435546.0,105573.0,"Correct. And you need to do this part carefully giving reasons/comments why the columns/rows can be deleted. Equally analyze if instead of deleting, the data can be imputed.",334535.0
101220,435912.0,,nan,
101930,436351.0,105749.0,"This was helpful.Still its not proving to be a very effective variable.Which i think it should,if your version is correct.Although it makes sense.",305655.0
101930,436351.0,105750.0,"Yes, I am also of the same opinion it’s a very effective variable. On the face of it may seem unhelpful but as you dig deeper and start analyzing may be you can weave these bits and pieces and create a story on which person is more likely to default.",318451.0
101930,436344.0,105748.0,I had read this ditto answer somewhere too.din’t help,305655.0
101930,436866.0,,nan,
101264,435690.0,105608.0,"Correct. The bank or in this case Gramener can approve or ""fund"" loan up to the amount applied or ""loan amount""",334535.0
101264,435846.0,,nan,
101264,435743.0,,nan,
101249,435691.0,105609.0,"Hi Ranjana, Hemant Good inputs Hemant. Ranjana, generally if there are large number of columns/attributes, it actually is a good practice to group columns.",334535.0
101249,435691.0,105648.0,"@ Mangesh, do we state the hypothesis on our observation as well? N then test it?",310508.0
101249,435691.0,105639.0,Thank you for clarifying that..!,310508.0
101249,435693.0,105604.0,"Hi Ashish, How are you calculating loss rate?",303674.0
101249,435693.0,105618.0,whatever is written off is the loss...for the assignment we want to.see of the total loans booked how much is written off...so loan amount written off /sum of total loans booked,310629.0
101249,435693.0,105653.0,Hi Ashish written off means ?,303674.0
101249,435693.0,105682.0,there is loan status column whihc states written off.. Written off are those loss where the financial institute believes that potential of recovery from these clients are minimal and hence they write off the loan from the accounts. The bank books losses against these loans. The recovery procedure of the Banks/FI continues even after the account is written off. If any recovery is received against these accounts it goes directly to the income.,310629.0
101249,435742.0,,nan,
101931,436347.0,,nan,
101931,436333.0,,nan,
101316,435931.0,,nan,
101316,435997.0,,nan,
101316,436376.0,,nan,
101323,436193.0,,nan,
101323,435994.0,,nan,
101938,436342.0,,nan,
101938,436330.0,,nan,
101324,436011.0,,nan,
101324,436043.0,,nan,
101325,435963.0,,nan,
101336,436045.0,,nan,
101336,436104.0,,nan,
101336,436360.0,,nan,
101336,436518.0,,nan,
101351,436166.0,,nan,
101351,436281.0,105788.0,THANKS,305656.0
101351,436516.0,,nan,
101969,436463.0,,nan,
101969,436521.0,105845.0,Does that mean we can rely on this column to tag a customer as a potential defaulter?,316889.0
101969,436521.0,106001.0,got it thanks,316889.0
101969,436521.0,105846.0,"Grading is used by investor to decide whether to give Loan or not . Grading alone cant be a deciding factor in finding a faulty customer . However, it can be one of the factors.",315560.0
101974,436502.0,,nan,
102281,437384.0,,nan,
102281,437369.0,,nan,
101978,436805.0,,nan,
101978,436455.0,,nan,
101978,436588.0,,nan,
101978,437540.0,,nan,
101978,436998.0,,nan,
101993,436553.0,,nan,
101999,436599.0,105812.0,"You need to mention data quality issues, but for relevant columns. Ideally dropping columns with NaN values can also be categorized as tackling data quality issues for those columns. For retaining columns you can see duplicates (if applicable) or outliers etc.",313767.0
101999,436599.0,105811.0,"Thought the same but in the uber assignment, they have deducted marks for not mentioning the data quality issues of columns that I've not considered for the analysis.",310974.0
101999,437265.0,,nan,
98697,420295.0,,nan,
98697,420317.0,,nan,
98697,426006.0,,nan,
98697,426079.0,,nan,
98697,427042.0,,nan,
102021,436690.0,,nan,
102021,436705.0,,nan,
102021,437278.0,,nan,
102021,437436.0,,nan,
102031,436721.0,106034.0,"The given information is past data based on which we have to decide if loan can be sanctioned, loan amount to be reduced or interest rate has to be increased based on various factors. (Rejected loan data set is not provided to us)",301121.0
102031,436721.0,106024.0,The assignment mentions that we don't have data for loan rejection. The dataset is of people to whom a loan has been approved.,318078.0
102031,436800.0,,nan,
102031,436834.0,,nan,
102031,436863.0,105907.0,I am also thinking the same,308636.0
102031,436993.0,,nan,
102031,437034.0,,nan,
102040,436755.0,,nan,
102040,436792.0,,nan,
102040,436844.0,,nan,
102040,437599.0,,nan,
102040,437098.0,,nan,
102100,436973.0,,nan,
102100,436887.0,,nan,
102054,436840.0,105955.0,"Since this data will be used for weeding out future applicants, can we assume that we will have such data from other companies they might have loans with? Or would we not have loan data on those who don't have loans with us?",319357.0
102054,436810.0,105953.0,"Sorry Tanay, my question is not related to the loan_status. It is related various columns in the data pertaining to the loan the clients have with the consumer finance company. Since this data will not be applicable to future applicants, should those be considered? Since presumably they won't already have loans with this company,.",319357.0
102054,436810.0,105958.0,"Hi Akbar, Your's is a valid point, and can be accordingly highlighted in the analysis.",334535.0
102054,436810.0,105956.0,The variables which you derive from the analysis will stand as a matrix for future applicants on which they will be evaluated. For time being the task is to analyse the available data find out strong drivers of default.,318451.0
102102,437011.0,,nan,
102102,437263.0,,nan,
102105,437009.0,,nan,
102105,436954.0,,nan,
102106,436951.0,,nan,
102106,437005.0,,nan,
102107,436950.0,,nan,
102107,436974.0,,nan,
102107,437000.0,105918.0,Thanks Mangesh for verifying and motivating!!,318451.0
102107,437000.0,105914.0,Well answered Tanay.,334535.0
102108,436948.0,,nan,
102108,436971.0,,nan,
102108,436972.0,,nan,
102108,437061.0,,nan,
102108,437047.0,,nan,
102109,436946.0,,nan,
102109,436970.0,,nan,
102109,436997.0,,nan,
102109,437097.0,,nan,
102112,436943.0,,nan,
102112,437021.0,,nan,
102112,437046.0,,nan,
102112,437096.0,,nan,
102143,437010.0,105908.0,"what is the range that i should consider to filter outliers ? like 1.5 times the interquartile range etc.,",313691.0
102143,437115.0,,nan,
102143,438253.0,,nan,
102151,437108.0,,nan,
102151,437056.0,,nan,
102184,437091.0,,nan,
102184,437067.0,,nan,
102218,437192.0,,nan,
102218,437212.0,,nan,
102218,437395.0,,nan,
102248,437209.0,,nan,
102248,437260.0,,nan,
102248,437378.0,,nan,
102235,437298.0,105991.0,Technically all columns need to be analysed that haven't been dropped,300694.0
102235,437191.0,,nan,
102235,437211.0,106016.0,"Hi there, used the above way , but still get data as Month-Year. It still prints as series, instead of datetime object.",307492.0
102235,437211.0,106018.0,Refer this link - https://www.datacamp.com/community/tutorials/converting-strings-datetime-objects,318451.0
102235,437211.0,106050.0,Should we not just consider month-year rather than trying to convert it to date-month-year?,318078.0
102220,437158.0,105949.0,Thankyou,312019.0
102220,437244.0,,nan,
102241,437189.0,106012.0,Evaluation Rubrics says: https://learn.upgrad.com/v/course/208/session/20136/segment/102524 New Metrics to be derived .,312019.0
102241,437189.0,106169.0,how would you derive that Accept or Reject status - we have already been told all the data are that of accepted loans; there is no data for rejected loans,300694.0
102241,437208.0,,nan,
102275,437322.0,105989.0,Though number of records for charged off is more for 36 months but if you calculate the percentage you will find 60 months term has higher chances of defaulting.,318756.0
102275,437341.0,,nan,
102275,438717.0,,nan,
102275,437393.0,,nan,
102271,437320.0,,nan,
102271,437377.0,,nan,
102288,437388.0,106013.0,"We did refer to this earlier, but, somehow, above clarity is not achieved. Can you please check and help us out if this is the correct direction (because of lack of business domain knowledge)",314084.0
102288,437388.0,106033.0,"Yeah, the reason why this is happening is that of the way the data has been collected. The 30% score is generally applicable when you don't have a loan on you. Since the borrowers in the dataset already have taken a loan from the lending club, it is natural that they'll have a very high revolving utlization rate. So they won't be able to apply for a new loan easily, since they already have a loan to pay.",313517.0
102288,437388.0,106014.0,"Also, considering the parameters for the credit line: https://www.nerdwallet.com/blog/credit-cards/what-fico-score-considered-good-credit/ Amount of credit used (30% of FICO score): The lower the amount of available credit you use, the better. Keep your credit balance below 30% of your total credit line. Here, less is more, and it’s even better if you can keep your balance below 10% of your credit. But, in the data set, revolving_util is always way too high and it doesnt fit in 30% at all.",314084.0
101348,436101.0,105691.0,I guess we need to make assumptions then. How do we figure out which ones are relevant? One wrong assumption could derail the entire hypothesis.,319357.0
101348,436110.0,,nan,
101348,436215.0,,nan,
101348,436263.0,,nan,
101348,436119.0,,nan,
102295,437396.0,,nan,
102297,437399.0,,nan,
102298,437404.0,,nan,
102298,437437.0,106030.0,"Yes, it is a generic statement only.",313517.0
102298,437489.0,,nan,
102298,437406.0,,nan,
102298,438486.0,,nan,
102304,437424.0,106117.0,"Yes, you can.",313517.0
102304,437424.0,106037.0,I've taken the values between Q1 - 1.5 x IQR & Q3 + 1.5 x IQR for outlier treatment as this also is one of the most widely accepted methods. where IQR is Q3 - Q1. Is there a problem if I do this? And is it ok to apply this to all the columns which has outliers?,310974.0
102304,437430.0,,nan,
102304,438722.0,,nan,
102304,437435.0,,nan,
100746,432971.0,,nan,
100746,432992.0,,nan,
100746,436176.0,,nan,
102330,437586.0,106064.0,"Other categorical values like IT,Federral etc",318476.0
102330,437586.0,106107.0,"How can we find where most of the applicants work? Even to categorize, we will have to select the values from the column. How to do that? Manually? What if there are millions of records?",318078.0
102330,437779.0,106241.0,"My question was a general curiosity on how to deal with such columns in the future, not just limited to this assignment... so I thought to ask in the discussion forum.",318078.0
102330,437810.0,,nan,
102330,438182.0,106240.0,"My question was a general curiosity on how to deal with such columns in the future, not just limited to this assignment... so I thought to ask in the discussion forum.",318078.0
102330,438338.0,106239.0,"My question was a general curiosity on how to deal with such columns in the future, not just limited to this assignment... so I thought to ask in the discussion forum.",318078.0
102334,437584.0,,nan,
102334,437780.0,,nan,
102334,437597.0,,nan,
102350,437621.0,,nan,
102350,437809.0,,nan,
102350,438241.0,,nan,
102355,437704.0,106096.0,post that you can convert into int by using astype(int),305129.0
102355,437808.0,106157.0,Thanks :),305129.0
102369,437709.0,,nan,
102369,437807.0,,nan,
102369,437972.0,,nan,
102312,437509.0,,nan,
102312,437811.0,,nan,
88648,367078.0,,nan,
88648,366993.0,,nan,
88648,367046.0,,nan,
88648,367115.0,,nan,
88648,367212.0,,nan,
88648,367243.0,,nan,
88648,367225.0,,nan,
88648,367027.0,,nan,
88648,367719.0,,nan,
88699,367404.0,,nan,
88699,367432.0,,nan,
88699,367382.0,,nan,
88699,367641.0,,nan,
88699,367543.0,,nan,
88699,367738.0,,nan,
88699,369125.0,,nan,
88703,367366.0,93041.0,please check my answer below and let me know if you still have any confusion.,317998.0
88703,367366.0,93039.0,I have posted the quiz question and options,311857.0
88703,367374.0,,nan,
88703,367481.0,,nan,
88703,367489.0,,nan,
88703,367377.0,,nan,
88703,367642.0,,nan,
89526,371534.0,,nan,
89526,371545.0,,nan,
89526,371652.0,,nan,
89526,371568.0,,nan,
88608,366853.0,,nan,
88608,366849.0,,nan,
88608,366827.0,,nan,
88608,366832.0,,nan,
88608,367067.0,,nan,
88608,367065.0,,nan,
88608,367218.0,,nan,
88608,367403.0,,nan,
88608,367536.0,,nan,
88720,367445.0,,nan,
88720,367446.0,,nan,
88720,367467.0,,nan,
88720,367507.0,,nan,
88720,367434.0,,nan,
88720,367550.0,,nan,
88720,367621.0,,nan,
88720,367728.0,,nan,
88720,367848.0,,nan,
88720,367885.0,,nan,
88720,368689.0,,nan,
88720,368824.0,,nan,
88894,368452.0,,nan,
88894,368456.0,,nan,
88894,368794.0,,nan,
88894,368702.0,,nan,
89143,,,nan,
89208,369893.0,93663.0,You will learn those tools and methods in the course further. Few are mentioned above.,301557.0
89208,369923.0,93548.0,Tableau is data visualization tool. I doubt it can be used for data preparation or cleaning.,318458.0
89208,370847.0,,nan,
89206,369882.0,,nan,
89206,369936.0,,nan,
89651,372157.0,,nan,
89651,372178.0,,nan,
89651,372224.0,,nan,
89651,372333.0,,nan,
89833,373464.0,,nan,
89833,373652.0,,nan,
89367,370763.0,,nan,
91010,379839.0,95685.0,Thanks,304812.0
91010,379840.0,95686.0,Thanks,304812.0
91010,380124.0,,nan,
91010,380173.0,,nan,
90914,379008.0,,nan,
90914,379028.0,,nan,
90914,379294.0,,nan,
90914,379428.0,,nan,
90914,380015.0,,nan,
90914,380033.0,95669.0,Thanks Rishi,303227.0
90914,380033.0,95758.0,Please upvote and select my answer as the solution. Thanks!,300734.0
90914,380690.0,,nan,
90914,379329.0,95546.0,"Instead of functools.reduce, directly use reduce. Instead of print(functools.reduce(lambda x,y:x*y, a)), use print(reduce(lambda x,y:x*y, a))",317987.0
90914,379329.0,95547.0,Thanks aman. It worked,320257.0
90156,375556.0,,nan,
90156,375559.0,,nan,
90156,375579.0,,nan,
90156,383176.0,,nan,
90156,375641.0,,nan,
88764,367712.0,,nan,
88764,367722.0,,nan,
88764,367730.0,,nan,
88764,367739.0,,nan,
88764,367824.0,,nan,
88764,367743.0,,nan,
88764,369348.0,,nan,
90851,378664.0,,nan,
90851,379192.0,,nan,
90851,378703.0,,nan,
90851,378658.0,,nan,
88727,,,nan,
88693,367288.0,,nan,
88693,367932.0,,nan,
88693,367935.0,,nan,
88693,367291.0,,nan,
88693,367316.0,,nan,
88693,367342.0,,nan,
88693,367347.0,,nan,
88693,367435.0,,nan,
88693,367640.0,,nan,
88693,367964.0,,nan,
88693,367872.0,,nan,
88693,367967.0,,nan,
88693,368499.0,,nan,
88693,368501.0,,nan,
88693,368418.0,,nan,
88693,368817.0,,nan,
88693,367305.0,,nan,
89116,369455.0,,nan,
89116,369467.0,,nan,
89116,369530.0,,nan,
89116,369457.0,,nan,
88853,368253.0,,nan,
88853,368271.0,,nan,
88853,368285.0,,nan,
88853,368288.0,,nan,
88853,368311.0,,nan,
88853,368579.0,93272.0,Can you please provide screenshot of your code to help you better ?,317991.0
88853,368579.0,93296.0,You have to use the reduce() function already available in python. More more details refer to the following link : https://www.geeksforgeeks.org/reduce-in-python/,304319.0
88853,368579.0,93572.0,This is a graded question. It's unethical to ask for screenshots of the potential answer.,319721.0
88853,368579.0,93585.0,"Bhaswati, can you try and tell me what more would you want to know? Please be specific when you're asking questions. I understand that you still did not get clarity for your questions from the answers. However, there are relevant answers which might have answered part of your questions. But if you specifically ask what your doubts, the discussion forum can be a help. Also, please do not take this for granted and ask solutions for graded questions. It is unethical to do so and UpGrad doesn't entertain this.",319721.0
88853,368968.0,,nan,
88853,369430.0,,nan,
88853,370075.0,,nan,
90624,377993.0,,nan,
89441,371098.0,,nan,
89441,371105.0,,nan,
89458,371261.0,,nan,
89458,371183.0,,nan,
89439,371108.0,,nan,
89439,371111.0,,nan,
89439,371220.0,,nan,
88783,367809.0,,nan,
88783,367814.0,,nan,
88783,367816.0,,nan,
88783,367826.0,,nan,
88783,367853.0,,nan,
88783,368966.0,,nan,
88783,367801.0,,nan,
88783,369440.0,,nan,
89725,372775.0,,nan,
89725,372671.0,95040.0,It shows partially correct,308958.0
89725,372778.0,,nan,
89725,373217.0,,nan,
89725,376572.0,95041.0,"If we define it in a if condition initially,",308958.0
89497,371445.0,,nan,
89497,371336.0,,nan,
89497,371499.0,,nan,
89497,372338.0,,nan,
89497,372520.0,,nan,
88792,367874.0,,nan,
88792,367877.0,,nan,
88792,368029.0,,nan,
88792,367922.0,,nan,
88809,,,nan,
88835,368185.0,,nan,
88835,368198.0,93246.0,its not a graded question,318017.0
88834,368146.0,,nan,
88834,368154.0,,nan,
88834,368721.0,,nan,
88837,368504.0,,nan,
88837,368176.0,,nan,
88837,368199.0,,nan,
88837,368098.0,93180.0,"Hi Vinay, thank you for answering. I am not looking at any coding question specific, but an understanding if there is a difference between the two",315383.0
88837,368098.0,93190.0,"Comprehension is a technique where in certain logic is applied on an existing data-set and the result is saved as a list, dict or a set as the need may be.",313826.0
89445,371241.0,,nan,
89445,371345.0,,nan,
89445,373285.0,,nan,
89445,380631.0,,nan,
89445,381278.0,,nan,
89749,372803.0,,nan,
89749,372950.0,94385.0,"Actually i followed from prep classes what ever given example on set operations , so that's why one test case is passed another test case failed . Thaq madanjit , Instead of hardcoding where we can find best ways to solve questions",305804.0
88869,368370.0,,nan,
88869,368376.0,,nan,
88869,368711.0,,nan,
88869,369034.0,,nan,
88869,369280.0,,nan,
88869,369755.0,,nan,
88869,375293.0,,nan,
88869,376128.0,,nan,
90239,375949.0,94967.0,Thanks Amani,319860.0
90239,375974.0,94968.0,Thanks Subarna !,319860.0
90107,375197.0,94761.0,I am working with same browser and cleared previous questions now just reached in this page and this issue persists,319969.0
90107,375211.0,,nan,
90107,375259.0,,nan,
90107,375275.0,94777.0,after reloading started . thanks,319969.0
90107,375288.0,,nan,
90661,378080.0,,nan,
90661,378105.0,,nan,
88916,368530.0,,nan,
88916,368785.0,,nan,
88916,368550.0,,nan,
88936,368612.0,93282.0,I came across this issue while I was in the factorial code. I hadn't handled the code for input of 0 which was why one test case failed,319898.0
88936,368619.0,93294.0,"Hi @Rajarshi Palit, Can you please look at my query below?",306248.0
88936,368621.0,93293.0,Thanks,300687.0
88936,368666.0,93632.0,https://learn.upgrad.com/v/course/208/session/19861/segment/101047 Please go through FAQs. .,319721.0
88936,368705.0,93301.0,Thanks :),306248.0
88936,368705.0,93305.0,Cheers!! glad to know that you found it:),310481.0
88936,368705.0,93302.0,I got my answer at http://help.upgrad.com/coding-console/coding-solution-is-not-accepted,306248.0
88936,368774.0,,nan,
88936,369131.0,,nan,
88936,370336.0,,nan,
88936,370376.0,,nan,
88936,370510.0,,nan,
88940,368638.0,93286.0,perfect! thanks :),305652.0
88940,368639.0,,nan,
88940,368637.0,,nan,
88940,368651.0,,nan,
88940,369277.0,,nan,
88940,380669.0,,nan,
88950,368681.0,,nan,
88950,368770.0,,nan,
88950,370338.0,93787.0,Thanks Sushmitha.,301107.0
88979,368878.0,,nan,
88979,368956.0,,nan,
88979,368851.0,,nan,
88979,368861.0,,nan,
88979,368858.0,,nan,
88979,368883.0,,nan,
90842,378639.0,,nan,
90842,378718.0,,nan,
90842,380052.0,,nan,
88748,367638.0,,nan,
88748,367657.0,,nan,
88748,367671.0,,nan,
88748,367846.0,,nan,
88748,367999.0,,nan,
88748,367653.0,,nan,
88986,368890.0,93325.0,"But if I share the code, won't it count as negative. Let me try and explain more properly, I am trying to convert the main input list into a set, so that 4th part of the question can be solved. I could successfully convert the 3 sports list into the respective sets, but facing problem when converting the main list into a set.",318335.0
88986,368896.0,93326.0,Okay. I got your point. Thanks,318335.0
89029,369091.0,93348.0,"@prashant sahu Thanks. I did not try it on Jupyter notebook, I don't know why it did not strike to me. My solution got rejected on both attempts I think I have to be more careful hereafter.",304389.0
89029,369202.0,,nan,
89029,369836.0,,nan,
88893,368444.0,93279.0,"Sorted() function is already converting the sequence passed to it into a list internally, as mentioned by Vinay too. In that case, how is it possible that the conversion to list and then sorted() function made it more efficient? Also, in our case, total time = time taken in converting set to list + time taken to sort. Also, in the reference link, it is not clearly mentioned as why one of those would work fast.",317987.0
88893,368440.0,,nan,
88893,368533.0,,nan,
88765,367726.0,,nan,
88765,367758.0,,nan,
88765,367785.0,,nan,
88765,367841.0,,nan,
88765,367842.0,,nan,
88765,368963.0,,nan,
89087,369309.0,93410.0,I have tried that as well and considered the case of 0! also. The code is running fine in python.,300719.0
89087,369357.0,93419.0,I am not getting any error for any input value. My code is running in python. Its just that when i verify my code it says the output is not matching whereas it is showing correctly in python.,300719.0
89087,369357.0,93428.0,generally when it says output is not matching then it gives the input values expected value and the solution value so will you be able to share that and also are you using the reduce function as well because its one of the criteria,300687.0
89087,370506.0,,nan,
89087,369371.0,,nan,
89087,371064.0,,nan,
89144,369629.0,,nan,
89144,369619.0,93471.0,it's not showing the error . i'll add the screenshot,301117.0
89144,369623.0,,nan,
89144,369632.0,93644.0,got it . thank you :),301117.0
89144,369685.0,93869.0,"As this is a grading question, it's unethical to ask for screenshots of a potential answer.",319721.0
89144,369718.0,93645.0,thank you :),301117.0
89144,369718.0,93774.0,welcome:),305656.0
89144,370375.0,,nan,
89144,370478.0,,nan,
89144,370503.0,,nan,
89144,370689.0,94007.0,"if I do so, it says return outside function.",300721.0
89144,370689.0,94652.0,"Check for the input number using if block. If the number is 0, just print 1 right there. Otherwise enter else block where the reduce function can be used to multiply for a range from 1 to (number + 1).",318007.0
89144,374566.0,,nan,
89147,369875.0,,nan,
89147,369953.0,,nan,
89152,369687.0,,nan,
89152,370731.0,,nan,
89152,369691.0,,nan,
89152,369708.0,93489.0,what about negative numbers? all other cases would include them as well.,317998.0
89152,369708.0,94854.0,"I used this hint and tried to run.. I am getting ""No sample testcases passed. (0/2)"" while clicking on Verify but when I click on ""Run code"" it says ""Code execution successful!"".. what to do? Since its a graded one I want to be sure ..",310508.0
89152,369727.0,93701.0,"i am getting an indentation error while inserting value 0 saying name 'reduce' is not defined, eventhough I kept "" from functools import reduce"" after else, rest of the values i am getting the correct factorial...help me",320606.0
89152,370330.0,93870.0,Hey please understand that this a graded question. Asking for many details on the solution is unethical. Conceptual details can be asked but directly asking to solve errors is not the right way. I hope you understand. I'll have to delete few of your questions.,319721.0
89152,370524.0,,nan,
89152,375647.0,,nan,
90817,378508.0,,nan,
90817,378494.0,,nan,
90817,380695.0,,nan,
90832,378576.0,,nan,
90832,378577.0,,nan,
90832,379703.0,,nan,
90832,378597.0,,nan,
90958,379388.0,,nan,
90958,379417.0,,nan,
90958,379599.0,,nan,
90958,380051.0,,nan,
90959,379386.0,,nan,
90959,379416.0,,nan,
90959,379456.0,,nan,
90959,379482.0,,nan,
89219,369925.0,,nan,
89219,371021.0,,nan,
89219,371001.0,,nan,
89219,370877.0,,nan,
89219,371100.0,,nan,
89219,369940.0,93876.0,"Hello Susmitha, Hello Sushmitha, I completely got your points and agree with you. I can not discuss any graded questions and code here. But i just want to know that This questions are coming under below module (highlighted) but i dont know why it is showing as python graded questions. This is not a graded questions. Please clarify me so i will not do repeat again. Intro to Data Managementkeyboard_arrow_rightRESOURCE Data Structures and Algorithms in Python",320195.0
89219,369940.0,93555.0,Thanks Aditya for good information. Still i am doing some mistake in below code. please let me know where i am going wrong.,320195.0
89219,369940.0,93806.0,"Hi Nishan, Assuming that 'ar' is a list and n is the length of the list, the logic is absolutely correct. You can try and run the code mentioned below which contains the logic which you are using. testList = [1,1,2,2,3,4,4,5,5] result = testList[0] for i in range(1,9): result = result^testList[i] print(result) Are you getting some exception ?",318355.0
89219,369940.0,93872.0,"Hey, Nishan. This is a graded question and you cannot discuss direct solutions for these on the discussion forum. It is unethical to ask answers for questions you'd be evaluated on. If you're doing this we might have to exclude you from the discussion form. I hope you understand.",319721.0
89228,369968.0,,nan,
89228,369939.0,,nan,
89228,369974.0,,nan,
89228,369951.0,,nan,
89230,370108.0,,nan,
89230,369963.0,,nan,
89230,370500.0,,nan,
89230,372654.0,,nan,
89230,373890.0,,nan,
89230,374431.0,,nan,
89184,369786.0,93500.0,output exactly matches with answer,315455.0
89184,369786.0,93504.0,"Your code is providing correct answer for sample test case. But when you submit, your code is tested for various test cases apart from the sample test case. If it fails in these test cases, then it gets rejected.",313691.0
89184,369786.0,93506.0,check this page to get a better understanding of how the upgrad coding console works : https://learn.upgrad.com/v/course/208/session/15859/segment/80225,313691.0
89184,369802.0,93512.0,i am not denying it about simplifying code. But question is that when code produces correct output why its got rejected,315455.0
89184,369790.0,93501.0,it's 16 only,315455.0
89184,369790.0,93507.0,modify the input in your jupyter notenbook so that you get two or more values as output and check if they are sorted,308635.0
89184,369854.0,93520.0,1.when I have single value sorting is not required 2. my logic may be complex but output is important in this scenario and output exactly maching with given answer ref screen shot,315455.0
89184,369854.0,93521.0,"When you do a verify , the code will be tested with few of the input values. But there could be many other input values configured which will be executed when you do a submit of the Code. One of the input values, where your code for Condition 3 could be failing is C = [2,5,9,12,13,15,16,17,18] F= [2,4,5,6,7,9,13,16,19] H =[1,2,5,9,10,11,12,13,15,19] When you execute your code over this input, the output will be [12, 15, 16] but it should be [12, 15, 16,19]",318804.0
89330,370402.0,93875.0,"Hey Vishwanath, the test cases are hidden from you because you are expected to code an efficient and generic code. As mentioned try various test cases by yourself and run your code and verify it. Then try sybmitting it.",319721.0
89330,370399.0,93649.0,Make your code generic. Don't hardcode any value. The test cases are hidden. You can't see them,314547.0
89330,370449.0,,nan,
89330,370396.0,,nan,
90787,378363.0,,nan,
89340,,,nan,
89346,371083.0,,nan,
89346,370638.0,,nan,
89346,370953.0,,nan,
89846,373527.0,,nan,
89846,373535.0,,nan,
89846,373672.0,,nan,
90356,376681.0,95043.0,"Create a series using list = [6,7,8,9,2,3,4,5] and print the output series as the square of each number in the list. Hint: If input series = 1,2,3 the output series should be 1,4,9 Hint: First create the series and then using apply and lambda find the output series.",310008.0
90356,376681.0,95033.0,Solution output 0 6 1 7 2 8 3 9 4 2 5 3 6 4 7 5 dtype: int64 0 36 1 49 2 64 3 81 4 4 5 9 6 16 7 25 dtype: int64 Expected output 0 6 1 7 2 8 3 9 4 2 5 3 6 4 7 5 dtype: int64 0 36 1 49 2 64 3 81 4 4 5 9 6 16 7 25 dtype: int64,310008.0
90356,376738.0,95233.0,i'm taking his logic only and do it in my way,310008.0
90356,376738.0,95049.0,i do same,310008.0
90356,376738.0,95050.0,Apart from the error mentioned above anything else is coming in error ?,317991.0
90356,376738.0,95051.0,no bt it is not show verify,310008.0
90356,376738.0,95052.0,coz of this error,310008.0
90356,376738.0,95223.0,"shristi, can you please share your code. The code Vipul mentioned works fine. If you post yours, we can try adn figure where it's going wrong.",319721.0
89114,369472.0,,nan,
89114,369581.0,,nan,
89114,371077.0,,nan,
89114,371080.0,94359.0,"This doesnt make sense, since the Memory is being computed from the underlying compiler how does the web console makes a difference? Do you think an int would use more memory than 36 bytes even if it is a web console.",300734.0
90092,375141.0,94756.0,Tried all these,316399.0
90092,375147.0,94757.0,"I contacted my student mentor through whatsapp, based on his suggestion I posted in the forum",316399.0
90092,375381.0,94882.0,I did the same. Now I am able to. Thank you all for your advices!!!!!,316399.0
90092,375381.0,94957.0,Happy Learning!!,318328.0
89576,371834.0,,nan,
89576,371826.0,,nan,
89576,371837.0,,nan,
89576,371890.0,,nan,
90806,378434.0,,nan,
90806,378497.0,,nan,
90806,378516.0,,nan,
89959,374552.0,,nan,
89959,374664.0,,nan,
90098,375213.0,94816.0,"Hey, please do not ask direct answers for the graded questions. Go through the pinned post here, https://learn.upgrad.com/v/course/208/question/89445",319721.0
90098,375185.0,,nan,
90098,375202.0,94762.0,the closing syntax is not correct remove one ),318476.0
90098,375208.0,,nan,
90098,375222.0,,nan,
90098,375209.0,,nan,
89400,370835.0,,nan,
90099,375219.0,95045.0,"Hi, I respectfully disagree. The point of solving on your own first is to apply your own logic and there are multiple ways to execute code logic. The question doesn't specify that set operators HAVE to be used. The explanation later is to learn a better/different way of doing something.",312608.0
90099,380199.0,95704.0,"Hi Yash, In the starting of the questions, it is written that , Set Operation, so it has to be with sets",306243.0
90099,380199.0,95723.0,"Hi Yash, I agree with you in the previous question it is mention. I also solved the football cricket question using for loop in the first attempt. But in the second attempt i used set operation. we can ask this topic with some of the TA members and get their view. I possible please raise this question again in the discussion forum and we can ask the TA.",306243.0
90099,380199.0,95705.0,Please refer previous question if there is loss of marks then they clearly mentioned that what is needed like factorial question needs to be done with the help reduce() function only that what I got from description.,318011.0
89590,376486.0,,nan,
90493,377476.0,,nan,
90493,377484.0,,nan,
90493,377495.0,,nan,
90493,377810.0,,nan,
90493,378527.0,,nan,
89350,370648.0,,nan,
89350,370783.0,,nan,
89350,370539.0,93723.0,Got it. Thanks! https://docs.python.org/3/library/functools.html#functools.reduce,308637.0
89350,370532.0,93729.0,"In that case I believe we can drop those records, and that will be part of data cleaning phase",318554.0
89350,370532.0,93722.0,"I accept the case about factorial being not defined. However, when we are crunching huge lists, I think there will be issues if we give exceptions. This is why I asked the question.",308637.0
88917,368546.0,93268.0,aware of first one..thanks for other one,319869.0
88917,368564.0,,nan,
88917,368558.0,,nan,
88917,368573.0,,nan,
88917,368575.0,,nan,
88917,368630.0,93308.0,thanks,319869.0
88917,369522.0,,nan,
90589,377887.0,,nan,
90589,377882.0,,nan,
90736,378149.0,,nan,
90736,378178.0,95352.0,instead of '=' use '==' and it will work :),318495.0
90736,378343.0,,nan,
91069,380274.0,,nan,
91069,380756.0,,nan,
91069,381569.0,,nan,
91038,380022.0,,nan,
91038,380050.0,,nan,
91038,380047.0,,nan,
91038,380147.0,,nan,
90015,374770.0,,nan,
90015,374778.0,,nan,
90015,375178.0,,nan,
90015,374798.0,,nan,
95730,405404.0,,nan,
95730,405418.0,,nan,
95730,405634.0,,nan,
95730,405726.0,,nan,
96310,410357.0,,nan,
96310,410727.0,102041.0,"Hi Harsha, Can you please elaborate more how we can have balls in multiple of 100.",320103.0
96310,410727.0,102043.0,ohh...I was thinking we have to make sure that ball faced should be 100..thanks for clarifying,320103.0
96310,410727.0,102042.0,It is (100 * runs_scored/balls_faced),311160.0
96310,413036.0,,nan,
96550,411430.0,102087.0,I also include the student who submit after 2nd deadline but not get the right answer.,310419.0
96550,411449.0,102089.0,Answer i got not in the option,310419.0
96550,411523.0,102563.0,You can create a new column and assign 1 if he has missed the first deadline else 0. You can add up the 1s to know the count.,304319.0
96550,413125.0,,nan,
96550,414284.0,102562.0,That won't be required as we would be checking for submit_date > deadline. So 2016 would not match.,304319.0
96647,411702.0,,nan,
96647,413127.0,,nan,
96647,414879.0,,nan,
97077,415039.0,,nan,
97077,415352.0,,nan,
97077,414507.0,,nan,
97077,416094.0,,nan,
96201,409731.0,101751.0,"When that is done, it doesn't match with the expected output. So I'm confused about that.",310505.0
96201,409731.0,101753.0,It does match with the output as you would be rounding off the values,311160.0
96201,409731.0,101757.0,"No... When you calculate as average, it shows different values. For example, if you take average rating of the 2nd row, the average rating is supposed to be (2+2+1)/3 = 1.7 and not 2.0. But in the expected output, it shows 2.0.",310505.0
96201,409731.0,101760.0,Ok... I see. Thanks.,310505.0
96201,409731.0,102410.0,"I agree with you Revathi, I think the question could have been a bit elaborated. Once you see the expected output in the environment window only then it becomes a bit clear..",310508.0
96201,412355.0,,nan,
96201,413039.0,,nan,
96201,413395.0,,nan,
96201,413884.0,,nan,
96226,409726.0,,nan,
96226,410735.0,,nan,
96226,410739.0,,nan,
96226,413101.0,,nan,
97195,,,nan,
97157,415453.0,,nan,
97157,415450.0,,nan,
98603,419883.0,,nan,
98603,419712.0,,nan,
98603,419926.0,,nan,
98748,420687.0,103593.0,So you want to count the frequency of date occuring in your dataset.. you can use seaborn.countplot,316349.0
98748,420687.0,103592.0,Why I cannot plot date and time. The datetime can itself be an axis to see the distribution through out a particular day or certain number of dates.,303085.0
98748,420691.0,,nan,
98748,420725.0,,nan,
96411,410662.0,,nan,
96411,410669.0,,nan,
96411,410903.0,,nan,
96411,411058.0,,nan,
96411,411632.0,,nan,
95616,404768.0,100984.0,"Yes there are many datasets, but when we are going to use some DataSets available online, under which licence we can use them?",317984.0
95616,404774.0,100988.0,is this applicable for all the datasets?,317984.0
95616,404774.0,101050.0,"It should be. All these are public datasets. But, double check the specific dataset page if you are not just using it for personal use.",318329.0
95616,405537.0,101220.0,"yes u can, even Kaggle has provided his own dataset also for a competition going on",317984.0
95616,405537.0,101230.0,yes,303082.0
95616,406551.0,101426.0,scraping is not always legal,318329.0
95616,406551.0,101409.0,can you explain more on this please,311741.0
95616,407355.0,,nan,
95616,412279.0,,nan,
95616,410002.0,,nan,
95616,411062.0,,nan,
95616,415071.0,,nan,
96512,411957.0,,nan,
96512,413065.0,,nan,
96462,410964.0,,nan,
96462,411746.0,,nan,
95781,405721.0,101243.0,"I didn't come across 'Segmented Univariant analysis' till now.. let's hear from others,.. sorry..",316349.0
95781,405721.0,101240.0,about Segmented Univariant analysis,317993.0
95781,407000.0,,nan,
95781,411259.0,,nan,
95781,412016.0,,nan,
97134,415108.0,,nan,
97134,415102.0,,nan,
97018,414118.0,,nan,
97018,414574.0,102611.0,thank u,320606.0
97018,416060.0,,nan,
96694,412047.0,,nan,
96694,412164.0,,nan,
96694,413151.0,,nan,
96694,412996.0,,nan,
96711,412092.0,102208.0,We need to find answers in python?,307495.0
96711,412137.0,,nan,
96711,412811.0,,nan,
96711,412838.0,,nan,
96716,412146.0,,nan,
96840,413980.0,,nan,
96840,413254.0,102382.0,"1. Select any cell in the sheet having data. Click on ""Insert"" then click on ""PivotTable"" then ""Ok"". 2. This takes you to a new sheet which has a pivot table. Now, on the right hand side search for the ""Watch.TV"" filed and drag it to the ""Row Lables"". 3. Similarly, drag ""Science.."" to the ""Values"" Section. By default, the calculation done is a ""Count"". Click on the downward-arrow symbol , click on ""Value Field Setting"". Then choose ""Average"" and click ""Ok"".",313826.0
96840,413254.0,102379.0,Hi Vinay! What did u used to get the avg values for marks?,318427.0
96849,413178.0,,nan,
96476,411136.0,,nan,
96476,411142.0,,nan,
96476,411747.0,,nan,
96476,411756.0,,nan,
96640,411691.0,,nan,
96640,412251.0,,nan,
96513,411333.0,,nan,
96513,411368.0,,nan,
96513,412118.0,,nan,
96513,412185.0,,nan,
96377,410494.0,,nan,
96377,411101.0,,nan,
96377,412242.0,,nan,
96377,414307.0,,nan,
96970,413769.0,,nan,
96806,412779.0,102299.0,i have given file path again it is throwing an error,306996.0
96806,412797.0,,nan,
96806,413047.0,,nan,
96806,413708.0,,nan,
96806,413582.0,,nan,
95878,407090.0,101579.0,Even after converting it into excel and then the column settings to date time could not see year 2008,318451.0
95878,407090.0,101606.0,Could you check if it shows in Python or not? BTW which excel version are you using?,313517.0
95878,407090.0,101659.0,2013 version,318451.0
95878,407090.0,102076.0,I too am facing the same issue. The original csv file does not have any data for year 2008.,308435.0
95878,407090.0,102148.0,Could you verify if opening the dataset in Python also has the same issue or not?,313517.0
95878,407090.0,102207.0,its showing 2018 not 2008,301651.0
95878,407090.0,102217.0,"Try using this method: Open a new excel sheet. Go to the data tab. Choose ""Get data from text/csv""(from top left) option and open the csv file. Also check out this link to see if it helps:https://support.office.com/en-us/article/format-a-date-the-way-you-want-8e10019e-d5d8-47a1-ba95-db95123d273e. BTW could someone do a filter and show a screenshot of what all years are being shown in the csv file?",313517.0
95878,407090.0,102345.0,"i tried to change the date setting as shown above, but still cannot see 2008 data in excel.",310509.0
95878,407090.0,102524.0,in excel if we format its changing to 2018 but in python its working fine post changing to dat format,301651.0
95878,412248.0,,nan,
95878,411581.0,,nan,
95878,411582.0,,nan,
97005,413953.0,,nan,
97005,414869.0,102639.0,Ok,313228.0
96783,412619.0,102278.0,"df[['SilverPrice','GoldPrice']].corr()",319444.0
96783,412619.0,102277.0,yes i used corr function in python but to to get only for 2008,319444.0
96783,412725.0,,nan,
96783,412640.0,,nan,
96783,412748.0,,nan,
96783,413383.0,,nan,
96813,412805.0,102311.0,So is it working now or still getting error ?,317991.0
96813,412805.0,102310.0,i have placed file in folder and i have given a path,306996.0
96813,413317.0,,nan,
96813,413392.0,,nan,
97090,414958.0,,nan,
97090,414566.0,,nan,
97090,414570.0,,nan,
97090,415059.0,,nan,
97090,415190.0,,nan,
97090,415475.0,,nan,
97090,415310.0,,nan,
96437,410751.0,,nan,
96437,410771.0,,nan,
96437,410966.0,,nan,
96437,412188.0,,nan,
96138,408739.0,,nan,
99043,421723.0,104069.0,Thanks for the response.Can't we have any solution segments after each graded questions like we had previously for numpy and Pandas....,308964.0
99043,421723.0,104131.0,Not possible for graded questions. Thanks.,301912.0
99043,421723.0,104070.0,Thanks for the response.Can't we have any solution segments after each graded questions like we had previously for numpy and Pandas....,308964.0
96847,413520.0,,nan,
96847,413220.0,,nan,
96847,413374.0,,nan,
96847,413389.0,,nan,
96847,415055.0,,nan,
96847,415299.0,,nan,
96847,415057.0,,nan,
96847,413155.0,,nan,
96535,411400.0,,nan,
96535,411409.0,,nan,
96535,411412.0,,nan,
96535,411550.0,,nan,
96535,411667.0,,nan,
96535,415461.0,,nan,
96559,411490.0,102106.0,"Interesting! It is actually very easy to filter, pivot and get basic idea of the data in Excel. However, when working with very large dataset in python, are there any outliers for slice operation?",308637.0
96559,411490.0,102234.0,"Hi Yerragudi, Please check out https://www.digitalocean.com/community/tutorials/how-to-index-and-slice-strings-in-python-3 https://www.geeksforgeeks.org/interesting-facts-about-strings-in-python-set-2/",334535.0
96559,411554.0,,nan,
96559,412642.0,,nan,
96559,413318.0,,nan,
96639,411656.0,,nan,
96639,411633.0,,nan,
96639,411785.0,,nan,
97642,417760.0,,nan,
97642,417769.0,,nan,
97642,417852.0,,nan,
96257,409714.0,,nan,
96257,409724.0,101750.0,The question asks to remove rows with 5 missing values and not less than 5 missing values. so the condition should be !=5,313826.0
96257,409736.0,,nan,
96233,409780.0,,nan,
96233,409564.0,,nan,
96233,409575.0,,nan,
96233,410349.0,,nan,
96414,410660.0,,nan,
96414,410680.0,,nan,
96414,410798.0,,nan,
96414,410971.0,,nan,
96414,411027.0,,nan,
96414,411305.0,,nan,
96414,411748.0,,nan,
96427,410684.0,101955.0,Glad I could help.. :-),310508.0
96427,410684.0,101954.0,Thanks,306996.0
96427,411043.0,,nan,
96427,411067.0,,nan,
96427,411696.0,,nan,
96456,410968.0,,nan,
96456,411158.0,,nan,
96456,411292.0,,nan,
96492,411270.0,,nan,
96492,411388.0,,nan,
95689,405130.0,,nan,
95689,405137.0,,nan,
95689,410944.0,,nan,
95689,410484.0,,nan,
95689,410448.0,,nan,
95689,405139.0,,nan,
95689,411671.0,,nan,
96698,412400.0,,nan,
96698,412044.0,102197.0,can you please share the screen shot or link of the web page please.,301110.0
96698,412064.0,,nan,
95637,405497.0,,nan,
96519,411342.0,,nan,
96519,411356.0,,nan,
96519,411435.0,,nan,
96519,411670.0,,nan,
97284,416511.0,,nan,
97284,416348.0,,nan,
97284,416344.0,,nan,
97284,416340.0,,nan,
97284,416967.0,,nan,
96846,413164.0,,nan,
96846,413378.0,,nan,
96846,414175.0,,nan,
96878,413396.0,,nan,
96878,413367.0,,nan,
96878,413371.0,,nan,
96878,413519.0,,nan,
96878,413736.0,,nan,
96957,413640.0,102452.0,Thanks Man.,310502.0
99126,422243.0,,nan,
99126,422253.0,,nan,
97333,416564.0,,nan,
97333,416613.0,102782.0,Thanks a lot ...It work ed,308638.0
97333,416613.0,102960.0,Can you please explain why this issue occur and how you resolve it becasue I too was facing the same issue. Please clarify. Thanks.,310179.0
97333,417546.0,,nan,
96317,410073.0,,nan,
96317,410048.0,,nan,
96317,410227.0,,nan,
96406,410634.0,,nan,
96412,410661.0,,nan,
96412,410667.0,,nan,
96412,411452.0,,nan,
96412,412101.0,,nan,
96860,413504.0,,nan,
96860,413614.0,,nan,
96860,413710.0,,nan,
96860,413314.0,,nan,
97361,416823.0,,nan,
96558,411487.0,,nan,
96558,411485.0,,nan,
96558,411790.0,,nan,
96965,413838.0,,nan,
96965,414294.0,,nan,
96395,410580.0,,nan,
96395,411152.0,,nan,
96395,411241.0,,nan,
96395,412489.0,,nan,
96968,413831.0,,nan,
96968,413761.0,,nan,
96968,414122.0,,nan,
96968,414108.0,,nan,
96805,412774.0,102293.0,"thanks..however not clear on the above. Also, why did my code not work? any idea?",310509.0
96805,413076.0,102366.0,Thanks Vipul. I was trying to remove leading and trailing spaces only - so not my code above is not working in that case?,310509.0
96805,413076.0,102428.0,I think then the issue is with the way you are trying to slice the dataframe. can you try using df.loc[1].str.strip()?,318397.0
97023,414142.0,,nan,
97023,414117.0,,nan,
97023,414234.0,,nan,
97023,414353.0,,nan,
96153,409125.0,,nan,
96188,409251.0,,nan,
96188,410455.0,,nan,
97089,415037.0,,nan,
97581,417583.0,,nan,
96090,408504.0,101598.0,"yes,i used quantile([0.5,0.10,0.25,0.50,0.75,0.95]) but i wanted to do it graphically",318005.0
96090,408504.0,101602.0,You can plot the percentage values on X axis and the corresponding quartile values on Y axis?,318329.0
96090,408954.0,101624.0,"i think we cant do it , we can find every percentile value ,plot it in subplot1 as simple line plot,and can use subplot2 for boxplot and compare ,,not sure",318005.0
96090,408954.0,101623.0,"let values be random number between 1 to 10000,,and we hv to show on box plot 60%tile also with 25,50,75 etc also",318005.0
96090,411697.0,,nan,
96189,409282.0,101690.0,"Thanks Deval.. Since, it was small data exercise, so not impacted on answers, But it wud definitely impact on big analysis.",311117.0
96189,409331.0,101703.0,Thanx,311117.0
96189,409806.0,101791.0,"Simply replace DNB and TDNB to 0 in runs column, it will replace all the DNB & TDNB values to 0 as as well as ""-"" in 4s column.",311117.0
96189,410923.0,,nan,
96189,411728.0,,nan,
97681,417972.0,,nan,
97681,418029.0,,nan,
97681,418208.0,,nan,
95598,405350.0,101103.0,Nah! the link gives just 1 example of retaining values. The excercise however talks about a different rule for retaininf values. But the same process can be followed. The basic concept is to retain rows where shares <= value for 95 percentile(simple loc operation) and save it back to the same df. Try this. It worked for me.,310511.0
95598,405350.0,101086.0,"I tried doing that and all such combinations but no luck. Lemme check again. But, it's been asked to follow the same procedure as in the link which said retain values between mean -(2*std) to mean + (2*std) Ideally, this should be only case that the question should consider and looks like the answers doesn't match.",318329.0
95598,405350.0,101779.0,I was struggling with this too!! It worked now. thanks Rajashri,312756.0
95598,405350.0,102305.0,"<= instead of < does the trick but it should have been clear from the question ""Thus, the last article you include will be the one at the 95th percentile.""",313515.0
95598,404565.0,100920.0,The question specifically asks for the mean What is the mean number of shares after removing the outliers?,318329.0
95598,404793.0,101043.0,"Hi Anshu, I forgot to mention that i was referring to the graded question and the mean value i got is not in the list of answers",318329.0
95598,404793.0,102030.0,"Hi @Sujit, After considering the values between 0 and 95%, I got correct answer",318329.0
95598,404793.0,102026.0,@ Nagaraju: I am also facing same issue. Did you find any solution to your problem?,318362.0
95598,405592.0,,nan,
95598,411700.0,,nan,
98351,418473.0,,nan,
98351,418497.0,,nan,
96202,409328.0,,nan,
96202,411695.0,,nan,
96202,412053.0,,nan,
96443,410826.0,,nan,
96443,411721.0,,nan,
96443,411997.0,,nan,
96674,411847.0,,nan,
96674,412249.0,,nan,
96674,412347.0,,nan,
96674,411892.0,,nan,
96674,412689.0,,nan,
96674,411865.0,,nan,
96674,411914.0,,nan,
96674,412056.0,,nan,
96674,412161.0,,nan,
96674,412187.0,,nan,
96674,413700.0,,nan,
96674,413713.0,,nan,
96674,415621.0,,nan,
95728,405349.0,101106.0,you have removed the 95th percentile and above value and then calculated the mean,303674.0
95728,405349.0,101110.0,The basic concept is to retain rows where shares <= value for 95 percentile(simple loc operation) and save it back to the same df. Try this. It worked for me.,310511.0
95728,405349.0,101108.0,i dont know why my answer is not matching even i have removed the 95th percentile and above,303674.0
95728,405349.0,101113.0,Thankyou so much i got the answer i was taking only values grater than 95 percentile thats why i was stuck,303674.0
95728,405349.0,101097.0,Are you getting right answer ?,303674.0
95728,405349.0,101104.0,Yes.,310511.0
95728,405349.0,101116.0,You are welcome,310511.0
95728,405430.0,101114.0,bro you ll get the correct answer remove the values which are greater than 95 percentile and include 95 percentile as well,303674.0
95728,405430.0,101105.0,I am also stuck here,303674.0
95728,405430.0,101102.0,"Yeah, even I'm getting the same problem. I suspect there might be a problem with the answer. It is blocking my progress. What can we do? Raise a question to TA directly? Even I posted the same question on discussion forum.",318329.0
95728,405430.0,101096.0,hi are you getting the answer ? because if i remove 95percentile and above from dataset i am not getting right answer,303674.0
95728,405430.0,101098.0,i tried (to remove mean +2std and mean -2std from dataset then also i am not getting th answer which matches in this option ),303674.0
95728,405430.0,101156.0,"Ohh, Great. I will try that and come back. Even in that case, the method that is being given in the link is different right? Not just including belore 95% quartile?",318329.0
95728,405430.0,101115.0,i just got it,303674.0
95728,405597.0,,nan,
95728,412244.0,,nan,
95728,413022.0,,nan,
96732,415579.0,,nan,
96732,415580.0,,nan,
96732,412322.0,102290.0,"i was looking for an explanation of the code itself, i already did the analysis in excel",310509.0
96732,412322.0,102318.0,Tell me what is your confusion in the code.,315560.0
96732,412322.0,102553.0,One example of confusion - what is the purpose of x.name? How does it help?,310509.0
96740,412309.0,,nan,
96740,412501.0,,nan,
96920,413502.0,,nan,
96920,413483.0,,nan,
96920,413610.0,,nan,
96920,413525.0,,nan,
96920,413630.0,,nan,
96920,413936.0,,nan,
96920,413465.0,,nan,
101332,436670.0,105886.0,"I got to know from WhatsApp group via student mentor that they will provide Python interview questions soon. However, this is very confusing that someone from Upgrad itself is saying yes and someone saying No.",317987.0
126768,558276.0,,nan,
126768,552845.0,,nan,
126768,552388.0,,nan,
135743,,,nan,
104511,,,nan,
120924,524954.0,,nan,
120924,524957.0,,nan,
120924,525604.0,,nan,
138146,596225.0,,nan,
138147,596247.0,,nan,
134325,584414.0,130953.0,"For a regression problem can you help me out how RFE can be used for algo such as RF, Gradient boost? Just like we need to pass their object to RFE function?",315028.0
134325,583841.0,130952.0,"Ok Vipul. Suppose on trying to solve a regression problem, we didn't see any linear relationship between IV and DV. What result RFE will give if we use linear regression object. Can we use RFE with different models to just see what are te tp variables and then start building with features of maxm count.",315028.0
134325,583872.0,130956.0,"Can you elaborate on the point feeding the coefficient value ? For any regression problem, can we use RFE with Random forest, Gradient boost?",315028.0
116639,504513.0,,nan,
116639,505339.0,,nan,
118097,509008.0,,nan,
117874,508371.0,,nan,
98345,418417.0,,nan,
98345,418476.0,,nan,
97335,,,nan,
99579,426676.0,,nan,
99579,426020.0,,nan,
97327,416541.0,102786.0,You can check in installed or installed for seaborn.,318429.0
97327,416541.0,102777.0,When click on Updatable seaborn not present there.,310419.0
97327,416608.0,,nan,
97329,416536.0,,nan,
97329,416563.0,102876.0,"In general, this works. As it is a crucial part of the assignment we could not be specific to the assignment here and cannot provide you the code.",332326.0
97329,416812.0,,nan,
97331,417061.0,,nan,
97331,416554.0,102933.0,Times of total trip completed,308639.0
97331,416554.0,102932.0,How do we calculate the times?,308639.0
97331,416705.0,,nan,
97331,416742.0,,nan,
97331,417077.0,,nan,
97331,417119.0,,nan,
99386,424452.0,,nan,
96306,410789.0,101974.0,"here supply means (completed trip ) vs demand (cars canceled ,or no car availble ) so difference between these two in one plot is gap on a particular time slot .this is what is expected am i right ?",303674.0
96306,410789.0,101975.0,Yes you can assume that.,313517.0
96306,410789.0,102574.0,"demand is all the requests including completed, cancelled and else while supply is only completed trips",318329.0
96306,410789.0,102617.0,thanks bro,303674.0
97211,416034.0,,nan,
97211,416035.0,,nan,
97211,416040.0,,nan,
97211,416041.0,,nan,
97211,416114.0,,nan,
97211,416805.0,,nan,
97352,416651.0,,nan,
97352,416670.0,,nan,
97350,417453.0,,nan,
97350,417245.0,,nan,
97350,417037.0,,nan,
97350,417073.0,,nan,
97350,417060.0,,nan,
97350,416661.0,,nan,
97350,416744.0,103028.0,We should take the subset of records for Demand =number of cab requests ? Or only has to save?,318846.0
97350,417157.0,,nan,
98303,418221.0,,nan,
98303,418283.0,,nan,
98303,418237.0,,nan,
97682,417971.0,,nan,
97682,418008.0,,nan,
97682,418313.0,103182.0,Thanks... conda update seaborn did the thing for me...,318479.0
97367,416896.0,102877.0,Thank you so much. It worked.,304319.0
97367,416822.0,,nan,
97367,416932.0,,nan,
97369,416780.0,,nan,
97369,416782.0,,nan,
97375,416814.0,102863.0,"Which will be more appropriate to create slot - Lambda, select function or pd.cut()?",302742.0
97375,416793.0,,nan,
97376,416820.0,,nan,
97376,416817.0,,nan,
97376,416795.0,,nan,
97376,416971.0,,nan,
97376,416993.0,103053.0,It is actually interchanging the value of month and date when converting the datatype of column.,318448.0
97379,416819.0,,nan,
97379,416818.0,,nan,
97379,417058.0,,nan,
97111,414856.0,,nan,
97111,415978.0,,nan,
97111,414874.0,,nan,
97111,415448.0,102627.0,"Hi Harhsa, I had checked and data looks fine...",300687.0
97111,415448.0,102610.0,"Hi Maya, I might be wrong here, but I am afraid, you might have few rows with wrong months and dates (mm interchanged with dd). Please verify, if you do not find any issues - super cool!",311160.0
97111,414910.0,102577.0,I am also using the to_datetime() function. Please note that I am not manipulating the values in anyway like replacing '/' by '-' etc. I am using some other parameters present in to_datetime() function to handle the different date formats. Please take a look at the documentation of to_datetime() and you will get a hint :) .,313826.0
97111,414910.0,102566.0,which python function you are using for that? I'm using to_datetime and it's giving me error on such cases on converting those values to NAT,318741.0
97381,416816.0,,nan,
97381,416813.0,102922.0,"So in that case consider ""Request id"" 619, and 867. This driver having id ""1"" has consecutive trips from Airport on the same date. How is this possible?",311741.0
97381,416813.0,102925.0,He could have gone back to airport without passenger. That's the only possibility.,317991.0
97381,416813.0,102927.0,Cool.. Cleared !,311741.0
97381,416813.0,102926.0,619 Airport 1 Trip Completed 11-07-2016 11:51 11-07-2016 13:00 867 Airport 1 Trip Completed 11-07-2016 17:57 11-07-2016 18:47 See the time taken from airport to city. It is 1:10 hr. Now see the time between two consecutive trip from airport it is 4:06 hr.,317991.0
97381,417120.0,,nan,
97118,415829.0,,nan,
97118,414954.0,,nan,
97118,415007.0,,nan,
97118,415030.0,102712.0,I downloaded the csv into python and checked the first 20 values. I am not seeing a difference in the timestamp in the 5th anf 6th rows!,316416.0
97118,415052.0,,nan,
97118,415398.0,102662.0,"The date conversion has to be done in Python only. I would suggest you to freshly download the csv file, import in python and perform date conversion.",313826.0
97118,415552.0,,nan,
97118,415977.0,,nan,
97118,416059.0,,nan,
97118,416360.0,,nan,
97392,416862.0,,nan,
97392,416928.0,102908.0,"I am trying to sort the index, but its not working",318335.0
97392,416928.0,103006.0,can you please elaborate on the error you are getting,332326.0
97392,416928.0,103158.0,I got it now. Tried a different approach,318335.0
97392,417051.0,,nan,
97395,416861.0,,nan,
97395,416865.0,,nan,
97395,417045.0,,nan,
97395,417321.0,102921.0,"I have tried this, but asking for array what should write? uber= pd.cut(np.array([]),3, labels=[""morning"", ""afternoon"", ""evening""])",308639.0
97395,417099.0,,nan,
98308,418252.0,103159.0,So it has to be in the PPT original form. Won't that look cumbersome?,318335.0
98308,418252.0,103161.0,Okay Rashmi. Thanks!,318335.0
98308,418252.0,103160.0,I don't think so! Also it's less than 100 words and you can pick the arrangement of plot and write up accordingly.,302742.0
98308,418285.0,,nan,
98308,418353.0,,nan,
97131,415827.0,,nan,
97131,415649.0,,nan,
97131,415631.0,,nan,
97131,415110.0,102579.0,thanks for your help Ram.:),302738.0
97131,415065.0,102580.0,thanks Vinay.:),302738.0
97131,415157.0,,nan,
97131,415546.0,,nan,
97131,415652.0,,nan,
97131,416054.0,,nan,
97131,416080.0,,nan,
97131,416127.0,,nan,
97131,416404.0,,nan,
97131,416535.0,,nan,
97131,416550.0,,nan,
97406,417076.0,,nan,
97406,416902.0,,nan,
97406,416894.0,,nan,
97406,416960.0,,nan,
97406,416944.0,,nan,
97406,416941.0,,nan,
97406,416992.0,,nan,
97406,416996.0,,nan,
97406,417035.0,,nan,
97406,417154.0,103064.0,"as per my comment above, that is open to interpretation and it is not clear that the data file ONLY contains to and from airport -> it is an assumption that if pickup is city then destination must be airport in the csv file",300694.0
97406,417154.0,103135.0,As the statement says we need to consider the to and from the Airport so we need to make few assumptions that pickup point will be city or airport.,314183.0
97406,417371.0,,nan,
97406,418005.0,,nan,
97415,416970.0,,nan,
97415,417005.0,,nan,
97416,416968.0,,nan,
97416,416969.0,,nan,
97416,417529.0,,nan,
97199,416058.0,,nan,
97199,417062.0,,nan,
97199,416078.0,,nan,
97130,415074.0,102581.0,"Okay.. I am getting the format updated but the datatype sets to ""object"" should this be okay or is it mandatory to have the type as datetime64[ns]?? :(",300727.0
97130,415074.0,102583.0,"https://learn.upgrad.com/v/course/208/session/19534/segment/99356 Under ""Data Cleaning and Preparation - Hints"" ,, one of the hints is to ""Ensure that the dates and time are in the proper format"". So, we need to convert them to datetime format.",313826.0
97130,415074.0,102725.0,"Yes, the hint was helpful. Thanks Vinay",316416.0
97130,415074.0,102721.0,Thanks Vinay the hint on to_datetime() function with additional parameters was very useful,317514.0
97130,415295.0,,nan,
97130,415566.0,102693.0,Are you sure we don't have to use formatting Anuj? I did it without formatting but there are folk saying that some difference need to be accounted for like '/' and '-' etc?,316416.0
97130,415566.0,102813.0,I did not see any issue. Before : 0 619 Airport 1.0 Trip Completed 11/7/2016 11:51 11/7/2016 13:00 1 867 Airport 1.0 Trip Completed 11/7/2016 17:57 11/7/2016 18:47 2 1807 City 1.0 Trip Completed 12/7/2016 9:17 12/7/2016 9:58 3 2532 Airport 1.0 Trip Completed 12/7/2016 21:08 12/7/2016 22:03 4 3112 City 1.0 Trip Completed 13-07-2016 08:33:16 13-07-2016 09:25:47 5 3879 Airport 1.0 Trip Completed 13-07-2016 21:57:28 13-07-2016 22:28:59 After ------0 619 Airport 1.0 Trip Completed 2016-11-07 11:51:00 2016-11-07 13:00:00 1 867 Airport 1.0 Trip Completed 2016-11-07 17:57:00 2016-11-07 18:47:00 2 1807 City 1.0 Trip Completed 2016-12-07 09:17:00 2016-12-07 09:58:00 3 2532 Airport 1.0 Trip Completed 2016-12-07 21:08:00 2016-12-07 22:03:00 4 3112 City 1.0 Trip Completed 2016-07-13 08:33:16 2016-07-13 09:25:47 5 3879 Airport 1.0 Trip Completed 2016-07-13 21:57:28 2016-07-13 22:28:59 6 4270 Airport 1.0 Trip Completed 2016-07-14 06:15:32 2016-07-14 07:13:15 7 5510 Airport 1.0 Trip Completed 2016-07-15 05:11:52 2016-07-15 06:07:52,312479.0
97130,415698.0,,nan,
97130,415979.0,,nan,
97130,415982.0,,nan,
97438,417004.0,,nan,
97438,417229.0,,nan,
97438,418707.0,,nan,
97438,417994.0,,nan,
97445,417041.0,102874.0,ty sham,320606.0
97445,417020.0,,nan,
97448,417100.0,,nan,
97448,417109.0,,nan,
97448,417112.0,,nan,
97448,417115.0,102881.0,"I am not aware of how to create the pivot charts and hence won't be able to comment on how to handle this. However, explore if there are any parameters that can be used to pass the headings instead of dragging them. Also, you could have a markdown cell giving instructions on which headings to use and how to use.",313826.0
97448,417999.0,,nan,
97457,417079.0,,nan,
97457,417071.0,,nan,
97457,417087.0,,nan,
97457,417111.0,,nan,
97457,417326.0,,nan,
97457,417584.0,,nan,
97457,417628.0,,nan,
97459,417095.0,,nan,
97459,417069.0,,nan,
97471,417106.0,,nan,
97471,417891.0,,nan,
97493,417179.0,,nan,
97493,417205.0,,nan,
97502,417243.0,102900.0,Thanks a lot!,308967.0
97502,418105.0,,nan,
97515,417315.0,,nan,
97515,417316.0,102947.0,use dayFirst=True while converting,318329.0
97515,417316.0,102943.0,"By using the above function, it is interchanging the month and date value for the ones which are in dd/mm/yy format.",318448.0
97515,417311.0,102913.0,Already my new time column has time in format 11:11:11... From this am not able to extract hours,308437.0
97515,417311.0,102920.0,Also you can check this link too https://stackoverflow.com/questions/39370879/extract-hour-from-timestamp-with-python,303673.0
97515,417311.0,102917.0,"After the conversion,details like date,time,month,hour,year etc can be extracted from the converted field using .dt accessor objects. kindly go through below link https://stackoverflow.com/questions/39662149/pandas-extract-date-and-time-from-timestamp",303673.0
97528,417487.0,,nan,
97528,417348.0,,nan,
97528,418010.0,103283.0,So as long as we are answering the above 5 queries as a part of question 2.. we are good. Right? I am asking this because I find Question 2 to be very similar to Question 1. Please confirm.,310508.0
97530,417356.0,102950.0,OK right I knew this...but wanted to know if it's possible to use groupby and then plot..thanks,308437.0
97530,417890.0,103063.0,Thanks a lot,308437.0
97354,416675.0,,nan,
97354,416938.0,102862.0,"I have a little issue with it though.... On the legend of the plot, the color of categories in accordance with hue variable is not visible. I ended up using a different kind of plot.",301652.0
97354,416938.0,102893.0,"Hi Ashsis, did you try palette=""Set1"" to get color of categories?",307495.0
97354,417257.0,,nan,
97354,417330.0,102956.0,"Thanks,... just had to use .add_legend() :)",301652.0
97354,417330.0,102971.0,"I am glad, i am able to help. :)",307495.0
97541,417393.0,,nan,
97541,417458.0,,nan,
97541,417499.0,,nan,
97541,417637.0,,nan,
97541,417949.0,,nan,
97546,417448.0,,nan,
97546,417416.0,102946.0,"Because I want to plot graphs in tableau and show the time slot morning, evening etc.",310419.0
97546,417416.0,102951.0,You could export the timeslots created in python to csv and use that?,318329.0
97546,417416.0,102968.0,Just do df.to_csv('filename.csv) and import that csv file into tableau as you would normally,318329.0
97546,417416.0,102961.0,how to import that can you just help me?,310419.0
97546,417459.0,102952.0,pd.cut categorizes the timeslots based on equal intervals? I have defined timeslots with unequal intervals and I think pd.cut cannot be used in that case,318329.0
97546,417459.0,102957.0,"No, we've to define the slots in all the three cases.. I guess none of it divides equally.. or it could but unsure of it.. :)",316349.0
97546,417459.0,102963.0,I want to do it in excel because in python I already done this part but for tableau I want it in my excel sheet.,310419.0
97546,417459.0,102967.0,"hey, pull the dataframes which you might have created in Python and use the same file in Tableau.. you shouldn't re-do everything in Excel again!",316349.0
97546,417459.0,102976.0,how to do that?,310419.0
97546,417459.0,102993.0,df.to_csv(),316349.0
97546,417459.0,103029.0,"hey, how to take care of the case for a time slot with timing like 22:00 to 04:00 by pd.cut?",311686.0
97546,417750.0,,nan,
97390,416974.0,,nan,
97390,417053.0,,nan,
97390,417673.0,,nan,
97552,417440.0,,nan,
97552,417442.0,,nan,
97552,417455.0,,nan,
97552,417646.0,,nan,
97552,417952.0,,nan,
99401,424498.0,,nan,
99401,424480.0,,nan,
97569,417534.0,,nan,
97569,417538.0,102966.0,thanks,319302.0
97569,417538.0,102965.0,Please go thorugh the link discussed on same. https://learn.upgrad.com/v/course/208/question/96748,303673.0
97569,417530.0,,nan,
97591,417651.0,,nan,
97591,417591.0,,nan,
97588,417610.0,103025.0,what to do create such a derived column?,318329.0
97588,417610.0,103066.0,"maybe group by driver id, sort by time field and compare the difference between two rows to find idle time?",300694.0
97588,417610.0,103071.0,"yo, I got it using shift functionality of Pandas",318329.0
97588,417610.0,103202.0,But idle time would be difference of drop time and next request time,304319.0
97588,417610.0,103213.0,Yep @Ruchita,318329.0
97588,417610.0,103266.0,"Correct me if i am wrong, to calculate idle time we also need to ensure the consecutive rows should have pickup point different. If pickup point is city then drop will be airport and if the next row have pickup point as airport then only if we take the difference of drop timestamp of 1st row and request timestamp of 2nd row we will be able to get the idle time.",318756.0
97588,417610.0,103268.0,yes Ranip,318329.0
97588,417610.0,103281.0,I got the mean around 6 hours and values ranging from 2.5-8 ignoring the outliers,318329.0
97588,417610.0,103279.0,I am getting the average idle time in airport to be 7 hours. Is it correct?,318756.0
97588,418386.0,,nan,
97594,417594.0,,nan,
97594,417600.0,102984.0,"[*] basically means that something is going on, so if you are running a script with some complicated computations, you'd better wait a little bit.",302742.0
97594,417600.0,103174.0,I am not sure how or why you want to plot a histogram for the status. Histograms are typically plotted for a distribution of values. There are no values in the Status column for you to get a normal distribution etc...,316416.0
97594,417882.0,,nan,
98324,418347.0,,nan,
98324,418352.0,,nan,
98324,418703.0,,nan,
98324,418320.0,,nan,
97592,417635.0,102989.0,"thanks i am able to plot the chart and get the frequency,but i would like to show values as label on the bar of the category",300687.0
97592,417650.0,,nan,
97592,417761.0,103062.0,Thanks Nagaraju,300687.0
96081,408506.0,101610.0,hmm.. it means we can tag hours in different slots on our own?,311686.0
96081,409215.0,,nan,
97601,417643.0,,nan,
97601,417655.0,,nan,
97601,417649.0,,nan,
97601,417881.0,103107.0,Thanks Harsha,300687.0
97601,418367.0,,nan,
97632,417724.0,,nan,
97632,417725.0,103026.0,Thx for suggestion but if i apply this post importing its giving errors. Where as if apply this while importing its working fine anyway thx.,315455.0
97637,417776.0,,nan,
97637,417735.0,,nan,
97637,417885.0,,nan,
97637,418260.0,103173.0,How do you import the python curated dataframe into Tableau?,316416.0
97637,418260.0,103363.0,"once you have your final data frame (call it df) ready, export using ""df.to_csv('TestOut.csv')"" [ replace ""df"" with your dataframe's name; and ""TestOut..csv"" with the filename of your choice. Open Tableau and import this csv (this is already covered in (Visualization using Tableau introductory topics) : https://learn.upgrad.com/v/course/208/session/15813/segment/79964",309211.0
97637,418705.0,,nan,
97638,417733.0,,nan,
97638,418234.0,,nan,
97638,418710.0,,nan,
97644,417752.0,,nan,
97644,417888.0,,nan,
97651,417789.0,,nan,
97651,417823.0,,nan,
97651,418019.0,,nan,
97656,418253.0,,nan,
97656,417815.0,103059.0,Thanks,308964.0
97656,417821.0,103058.0,Thanks,308964.0
97656,417821.0,103227.0,"how to use cut for this. I have taken time in a seprate column and now wants to put morng, eve condition in other colum.",318802.0
97656,417821.0,103229.0,with cut I'm getting erroe of pandas not have time range attributes.,318802.0
97656,417821.0,103267.0,"Himanshu, you first need to extract hour from the Request timestamp. then use pd.cut(dataframe name, [bins],labels=""""). This will return a series which use assign to the new column Refer to the link https://stackoverflow.com/questions/45751390/pandas-how-to-use-pd-cut",304319.0
97656,417886.0,103228.0,"can you share the format, using lambda",318802.0
97656,417886.0,103250.0,"I can give you an example here: def timesection(x): 'divide the time of the day in categories' if x < 6t: return ""X timeofday"" elif 6 <= x < 12: return ""XY timeofday"" else: return ""XYZ timeofday"" & then apply Lambda. Following link can guide you further. https://stackoverflow.com/questions/41993598/how-to-add-a-column-to-pandas-dataframe-based-on-time-from-another-column Hope this helps you resolve your query..!",310508.0
97656,417989.0,,nan,
97656,418205.0,,nan,
97656,418266.0,,nan,
97658,417833.0,,nan,
97658,417838.0,,nan,
97658,417887.0,,nan,
97658,418022.0,103095.0,Agree!,310508.0
97658,418268.0,,nan,
97657,417850.0,,nan,
97657,417872.0,103060.0,it gave me an idea of the average time that the driver spends at airport before getting a new request to city and is a significant number,318329.0
97657,417853.0,103061.0,*the difference between drop timestamp to be made,318329.0
97657,417853.0,103192.0,"Data provided is only from/to airport,service provided within the city is not mentioned,Not sure if marking the idle time in this case without within the city parameter is correct",300684.0
97657,417853.0,103118.0,Doesn't this assume that all drivers waited to get a fare back from the Airport ? Data does not bear this assumption out. Or am I missing something?,305653.0
97657,417853.0,103125.0,"I liked the thought process. However, I think this approach works best only if the pickup point of the 1st column is City and the 2nd column's pickup point is Airport.",311160.0
97657,417853.0,103126.0,"It works for both. But, it's better to prepare idle times based on pickpup point and prepare two different columns (idle time at airport, idle time at city)",318329.0
97657,417853.0,103191.0,"I tried to find the idle time using this method, it shows the idle time is more during the same time periods of the day for bot cit and Airport pickup! Am i missing anything?",311741.0
97657,417853.0,103193.0,Aaaahh!! this is really awesome.. Thanks Nagaraju!! it works really well.. :),316349.0
97657,417853.0,103194.0,"Data provided is only from/to airport,service provided within the city is not mentioned,Not sure if marking the idle time on driver id without 'within the city' parameter is correct",300684.0
97657,418027.0,,nan,
97657,418350.0,,nan,
97657,418398.0,103197.0,"-1 day comes when you consider all the trips. You have to filter the dataframe for status 'Trip Completed' and calculate idle time at airport alone. You will get correct results thought there is a lot of variation in the results we get, we should be good if we consider average alone",318329.0
97657,418994.0,103398.0,the data is not proper to do the analysis.,318756.0
97657,418994.0,103391.0,thats why drivers cancel trips,315455.0
97657,418994.0,103357.0,Ideal time helps in RCA and it may influence driver decission,315455.0
97657,418994.0,103383.0,"yes, ideal time is coming too much 11 hours, 5 hours that does not make sense..",300735.0
97657,418994.0,103419.0,"This is little tricky Take time diff between drop and next pickup as idle time by grouping date and driver id Use filter Pickup=City you will get desired output since you have problem with cancellation at mornings i.e from city to airport(at BI Tool) df['NextPickup']=df.groupby(['Drid','Status','PickupTimeDate'])['Pickuptime'].shift(-1) df['IdleTime'] = (df['NextPickup']-df['Droptime']).astype('timedelta64[m]')",315455.0
97661,417848.0,103051.0,I have the analysis but need ideas for recommending the solutions,318329.0
97661,417848.0,103065.0,u can use images examples for recommending ways,317982.0
97661,418028.0,,nan,
97661,418023.0,,nan,
97667,417876.0,,nan,
97667,417961.0,,nan,
97667,417922.0,,nan,
97667,418850.0,,nan,
97667,418391.0,,nan,
96094,409228.0,101654.0,hi.. that conversion needs to be done in Python only.. right? m not able to do so as the function used for it works on the entire column in one go.,311686.0
96094,409228.0,101656.0,"Yes, you need to do it in python. You can convert the entire column into correct format in one go.",319696.0
96094,409228.0,101658.0,struggling in doing that only.. two different formats in the column are not getting taken in one go in the function of conversion,311686.0
96094,409228.0,101667.0,You can replace '/' by ' - ' and then can use inbuilt function like to_datetime to convert into correct date time format.,319696.0
96094,409228.0,101668.0,ohh.. thanks a lot. let me try with this. thanks again.,311686.0
96094,409228.0,101674.0,welcome :),319696.0
96094,409228.0,102163.0,"when trying to convert using to_datetime after replacing '/' by '-' they are getting converted to mm-dd-yyyy format which is not correct, it should be dd-mm-yyyy format. not sure how to go about this.",302741.0
96094,409228.0,102190.0,use the dayfirst option,311857.0
96094,409228.0,102215.0,"After conversion to datetime format, the NaN values are getting converted to NaT . Do NaT values behave the same as NaN values? Any implications of having the NaT values or should these be converted to NaN?",313826.0
96094,409228.0,102312.0,"@Vinay, Yes, you can clean the data replacing the NaN with NaT vaules. Hint:You can use isNaT instead of isNaN, refer to python assignment for clarity to clean data.",312756.0
96094,409228.0,102451.0,NaN or NaT doesn't matter as we are not going to use that column in the analysis.,310974.0
96094,409228.0,102468.0,We may have to for duration,318329.0
96094,409228.0,102558.0,How to tackle seconds in the 'format' for different values in Request Timestamp?,318741.0
96094,415179.0,,nan,
96094,415363.0,,nan,
96094,416091.0,,nan,
98337,418392.0,,nan,
98337,418403.0,,nan,
98337,419081.0,,nan,
98337,418712.0,,nan,
98340,418401.0,,nan,
98340,418412.0,,nan,
97695,418021.0,,nan,
97695,418025.0,,nan,
97695,418356.0,,nan,
98346,418423.0,,nan,
98346,418979.0,,nan,
98368,418484.0,103218.0,This should work conda install -c anaconda seaborn=0.9.0,318329.0
98368,418486.0,103217.0,Try conda install -c anaconda seaborn,318329.0
98368,418486.0,103216.0,"No, You will have to upgrade seaborn manually for now. Follow above link",318329.0
98368,418559.0,,nan,
98368,418974.0,,nan,
98368,419229.0,,nan,
98361,418568.0,,nan,
98361,418621.0,,nan,
98361,419352.0,,nan,
98366,418713.0,,nan,
98366,418518.0,,nan,
98366,418466.0,,nan,
98366,418668.0,,nan,
98366,419097.0,,nan,
98366,419241.0,,nan,
98366,419748.0,,nan,
98366,418696.0,,nan,
98344,418419.0,103286.0,but there are even records where the driver's first ride as well as next ride for the same day are from the airport. How can that be ? If a ride starts from airport then it ends at city. The next ride should be from the city itself. It doesnt makes sense that the driver again goes back to the airport burning fuel and wasting time. Please clarify my doubt.,318479.0
98344,418419.0,103289.0,understood...thanks...,318479.0
98344,418419.0,103287.0,"are they on the same day? If No, then it's explainable. If yes, then driver might have taken offline rides and came back. If there are such cases in the data, it's difficult to consider all such possible combinations and is not necessary for the current case study at all.",318329.0
98344,418419.0,103284.0,"you are right but even according to your point of view, some entries are coming more than 12 Hrs. It doesn't make any sense for the Driver to wait for the next passenger for more than 12 Hours .",315560.0
98344,418419.0,103285.0,there may be some cases. suppose let's say driver started from city at 5 AM and started from airport at 9 PM. idle time would be more than 12 hours. there may be cases where driver wouldn't login but continue taking rides without app.,318329.0
98344,418419.0,103322.0,"Right,, there are many reasons for the Driver's Idle Time, but from the Calculations we did, a single explanation can't define the ambiguity in the Data.",315560.0
98344,418421.0,103199.0,You could By Shifting the request_timestamp by -1 day and subtract from drop_timestamp. You have to consider only 'Trip Complete' requests and also consider the same day requests which you get by grouping.,318329.0
98344,418421.0,103200.0,Subtracting the request timestamp from drop timestamp will give you the travel duration.... right? How can we assume that to be the idle time?,318084.0
98344,418421.0,103212.0,"Hi @Premnath, subtracting the next request timestamp from drop timestamp will give you the idle time. THat is the reason I am using shift(-1)",318329.0
98344,418421.0,103201.0,"According to me, Idle time is a relevant metric only at Driver level. Are you calculating it at that level?",318084.0
98344,418724.0,,nan,
98344,418874.0,,nan,
98372,418538.0,103233.0,They both can't be in same plot if you want to segment by timeslot as far as I know before supply is part of demand requests.,318329.0
98372,418538.0,103232.0,"Hi Nagaraju ,i know this but i want to plot them in 1 graph with timeslots then how to approach ?",303674.0
98372,418538.0,103235.0,Thanks then the gap part .How it can be shown ? How to approach that,303674.0
98372,418538.0,103236.0,combine Cancelled and No Cars Available as Gap into new column with Trips Completed as supply in that column plot a graph with this new column,318329.0
98372,418538.0,103237.0,ohhh broif this is the expected answer then i have done this only thankyou somuch for verifying,303674.0
98372,418538.0,103238.0,there is no one answer on how you do it. any method can be followed is you feel the answer is right,318329.0
98372,418538.0,103239.0,"Hi Nagaraju, if i understand you correctly, we dont need to create new derived metrics to plot it?",307495.0
98372,418538.0,103240.0,"hi harshal, If you want to clearly plot supply and Gap on same graph, you need to derive a new column",318329.0
98372,418538.0,103243.0,"Thanks Nagaraju. In this derived column, we will have only 2 categories, demand and gap? Am i correct.",307495.0
98372,418538.0,103247.0,How can i derive supply and supply gap to plot graph?,307495.0
98372,418538.0,103246.0,it's going to be Supply and Supply Gap. Demand is the sum of these two,318329.0
98372,419742.0,103463.0,thankyou,303674.0
98379,418663.0,,nan,
98379,418695.0,,nan,
98385,418662.0,,nan,
98385,418976.0,,nan,
98407,418742.0,,nan,
98407,418753.0,,nan,
98407,419245.0,,nan,
98407,419737.0,,nan,
98406,418723.0,,nan,
98406,418781.0,,nan,
98398,418684.0,103264.0,Numbers are coming exactly same,315464.0
98398,418684.0,103269.0,You will have compare supply and Gap for the specific timeslot and pickup point and they are not the same,318329.0
98398,418690.0,,nan,
98398,418692.0,,nan,
98398,418783.0,,nan,
98398,418967.0,,nan,
98398,418981.0,,nan,
98414,419317.0,,nan,
98414,418778.0,,nan,
98414,418823.0,,nan,
98414,418860.0,,nan,
98414,418938.0,,nan,
98414,418987.0,,nan,
98414,419014.0,,nan,
98414,419238.0,,nan,
98414,419222.0,,nan,
98421,418840.0,,nan,
98421,419233.0,,nan,
98421,418768.0,,nan,
98418,418752.0,,nan,
98418,418774.0,,nan,
98418,419237.0,,nan,
97544,418011.0,,nan,
97544,417413.0,,nan,
97544,418014.0,,nan,
98440,419348.0,,nan,
98440,418843.0,103295.0,If gap is coming high then let it be.,317991.0
98440,418843.0,103294.0,"I have no doubt in number of time slots. Based on the gap, should i choose one or more time slots with the highest gap for subsequent analysis on question 2b ?",313691.0
98440,418891.0,,nan,
98471,419049.0,,nan,
98471,418965.0,103432.0,Thanks,318240.0
98471,419809.0,,nan,
98442,418892.0,,nan,
98442,418945.0,,nan,
98442,419240.0,,nan,
98447,,,nan,
98452,418893.0,103300.0,I am getting NAT after applying above solution,301641.0
98452,418893.0,103349.0,"Before doing above step make sure the data is sorted by driver I'd ,request date time",318476.0
98452,418935.0,,nan,
98452,418999.0,,nan,
98456,418934.0,,nan,
98456,419051.0,,nan,
98456,418992.0,,nan,
97599,417619.0,,nan,
97599,417622.0,,nan,
97599,417644.0,,nan,
97599,418177.0,,nan,
97599,418273.0,,nan,
97599,418683.0,,nan,
97599,420590.0,,nan,
98460,,,nan,
98462,418933.0,103312.0,Try using count plot. That's what I used,310974.0
98462,418933.0,103311.0,Thanks Ram for your answer. I plotted using tsplot as below: sns.tsplot(df_time) as per uber assignment i need to plot different timeseries dataframes in a single plot. So was trying to use plot. Any suggestion how to proceed on that.,317156.0
98472,418964.0,,nan,
98472,419342.0,,nan,
98477,418997.0,,nan,
98483,,,nan,
98465,418942.0,103315.0,Thanks,318240.0
98465,419031.0,,nan,
98484,419338.0,,nan,
98484,419043.0,,nan,
98484,419230.0,,nan,
98484,419569.0,,nan,
98497,419677.0,,nan,
98497,419350.0,,nan,
98518,419287.0,103381.0,"Thank you, Vipul. I was missing to include datefirst argument.",316253.0
98518,419334.0,,nan,
98520,419407.0,,nan,
98520,419267.0,,nan,
98520,419517.0,103401.0,Thanks,314629.0
98535,419416.0,,nan,
98535,419565.0,,nan,
99425,424909.0,,nan,
99425,424998.0,,nan,
99425,425269.0,,nan,
98572,419734.0,,nan,
98572,419733.0,,nan,
98572,419564.0,,nan,
98576,419562.0,103417.0,great :),311686.0
98576,419562.0,103400.0,"Thanks, Chandan! But it worked for me after using 'dayfirst' parameter.",315661.0
98576,419603.0,,nan,
98576,419645.0,,nan,
98576,419938.0,,nan,
98576,419717.0,,nan,
98576,419948.0,,nan,
98576,420229.0,,nan,
97287,416347.0,102730.0,What error you are getting after upgrading ?,317991.0
97287,416347.0,102729.0,i did the following but still no help,318017.0
97287,416542.0,103181.0,i dont get to see seaborn when selecting updatable.,301114.0
97287,417082.0,,nan,
97287,418327.0,,nan,
97218,416721.0,,nan,
97218,416231.0,102722.0,Same with the Driver Id. It is null for the status as car not available.,304319.0
97218,416231.0,102731.0,Yes that is true.,317460.0
97218,416231.0,102847.0,Yes the NA value in drop time reflects the true practical scenario,318814.0
97218,416086.0,,nan,
97218,416088.0,,nan,
97218,416102.0,,nan,
97218,416176.0,102820.0,absolutely right !! its mentioned at the beginning,308437.0
97218,416236.0,,nan,
97218,416470.0,,nan,
97218,416539.0,,nan,
97218,416559.0,102776.0,"You can leave them as it is. Python can deal with them. Replacing with ""0"" would not add any value, you may face ambiguity in results. As I have mentioned above: Request time(pick up time) needs to be worked on as compared to ""Drop request"" column.Drop has ""3914""NA & they can be left as it is. Thank you",310508.0
97218,416559.0,102903.0,Thanks,307494.0
97218,416559.0,102899.0,Do not fill the NA values with zeros.,314048.0
97218,416887.0,,nan,
97218,416963.0,,nan,
97218,417088.0,,nan,
97218,417156.0,,nan,
97221,416110.0,102716.0,thank you,307494.0
97221,416117.0,,nan,
97221,416178.0,102717.0,Thanks Nagraju..,307494.0
97221,416178.0,102709.0,"To answer your question, Yes, Frequency of requests is the count of requests",318329.0
97221,416219.0,,nan,
97221,416226.0,,nan,
97221,416553.0,,nan,
98669,420118.0,,nan,
98669,420120.0,,nan,
98669,420686.0,,nan,
96748,412878.0,,nan,
96748,415568.0,,nan,
96748,415535.0,,nan,
96748,412461.0,,nan,
96748,415952.0,,nan,
96748,416084.0,,nan,
96748,417632.0,,nan,
98170,418037.0,,nan,
97258,416243.0,,nan,
97258,416520.0,,nan,
97258,416246.0,,nan,
97258,416264.0,,nan,
97258,416357.0,,nan,
97258,416473.0,,nan,
97258,416912.0,,nan,
97250,416247.0,102715.0,Now understood. Thanks :),311741.0
99446,424994.0,,nan,
99446,425113.0,,nan,
98203,418118.0,,nan,
98203,418359.0,,nan,
98212,418121.0,,nan,
98212,418357.0,,nan,
98213,418130.0,,nan,
98213,418133.0,,nan,
98213,418355.0,,nan,
97174,415778.0,102733.0,It is already NaN in the dataframe,301114.0
97174,415813.0,,nan,
97174,415856.0,102734.0,i wanted to clear the NaN values first from dataset. So for Drop Timestamp which is of type object i could not apply isnan. I referred the links above but no luck.,301114.0
97174,415856.0,102795.0,You can leave NaN values as it is from Drop Timestamp.,317991.0
99572,426091.0,,nan,
99572,426182.0,104339.0,Data Analysis 44/70,318329.0
99572,426226.0,,nan,
99572,426335.0,104367.0,"Hi Vinay, I have done both. Displayed numbers as labels and summarised the number of requests pertaining to demand and supply gap and percentages of gap at City/Airport and during the timeslots as summary",318329.0
99572,426978.0,104455.0,Hi Kunal. Agreed! Talked to my mentor and will proceed through that channel :),318329.0
99572,426681.0,104405.0,"Labels are there, still the feedback point says ""Results in numbers are not explicitly mentioned. """,318329.0
100480,431926.0,,nan,
100480,432065.0,,nan,
99672,,,nan,
88479,366539.0,92869.0,Yes.. that'll be great! 👍,318495.0
88479,366539.0,92868.0,Thank you! I'll keep the question open so that I can gain more links from others also.,308637.0
88479,368443.0,,nan,
88479,367365.0,,nan,
88550,381815.0,,nan,
88550,366884.0,95805.0,its not working,318723.0
88933,368627.0,,nan,
89013,368991.0,,nan,
89013,369007.0,,nan,
89013,370708.0,93807.0,"Absolutely Upgrad and mentors are there to help us. How previous Cohort can help - 1. What are the common mistakes that can/should be avoided. 2. How course is helping them in their work area, if it is. ""Learning from the other's experience is intelligent way to learn than experience ourselves and then learn.""",318458.0
89650,372223.0,,nan,
90013,376218.0,,nan,
90013,376283.0,,nan,
90013,376286.0,,nan,
89307,371101.0,,nan,
89307,371152.0,,nan,
89307,372148.0,,nan,
88821,369834.0,,nan,
88821,404081.0,,nan,
88821,404046.0,,nan,
92693,390521.0,,nan,
92693,390489.0,,nan,
124944,545183.0,,nan,
82268,337181.0,,nan,
82268,336828.0,,nan,
82268,336884.0,,nan,
82268,339795.0,,nan,
85179,,,nan,
77209,308578.0,,nan,
77209,308483.0,,nan,
77209,324005.0,84697.0,"Thank you Veena, I got it.",301110.0
77209,324005.0,84690.0,"It is part of Course 2 of prep course : ""Statistics and exploratory data analytics"" . Check out Courses under the Program tab, you should find it.",300717.0
82287,337584.0,,nan,
82287,337678.0,,nan,
82287,337050.0,,nan,
82287,337071.0,,nan,
82287,337149.0,,nan,
82287,337287.0,87381.0,Yea i also thought that but in very first exercise they expect that who’s don’t know Language they can’t do that In first exercise,310634.0
74870,297992.0,,nan,
74870,297874.0,,nan,
74870,298385.0,,nan,
74870,298461.0,,nan,
74870,298476.0,,nan,
74870,298747.0,,nan,
74870,299324.0,,nan,
74870,302547.0,,nan,
74870,319078.0,,nan,
74870,326516.0,,nan,
84779,347442.0,,nan,
84779,347548.0,,nan,
84779,347802.0,,nan,
84779,372842.0,,nan,
81384,331674.0,86115.0,"I understand that, however why not learn both the languages as well if the opportunity presents.",310617.0
81384,331929.0,,nan,
81393,331561.0,,nan,
81393,334920.0,,nan,
81393,332013.0,,nan,
84820,347538.0,,nan,
84820,348227.0,,nan,
84820,370243.0,,nan,
81850,333714.0,,nan,
81850,333707.0,,nan,
81850,334021.0,,nan,
81850,333680.0,,nan,
81850,338298.0,,nan,
81859,333703.0,,nan,
81859,335494.0,,nan,
77024,307060.0,,nan,
77024,307343.0,,nan,
77024,307520.0,82530.0,Go for python it will be the language of data science followed by R,305838.0
77024,307810.0,,nan,
77024,317002.0,,nan,
77024,339053.0,,nan,
75903,302239.0,79929.0,"Thank you for your response. Though I agree on the part that we need to learn multiple programming languages to be relevant to the market, the question was in the context of how to choose one of these two programmes, R & Python to go ahead without any prior knowledge about any of the two. However the introduction has some statistics that's seems to be in favor of Python. Any additional point of view would certainly help in choosing the right one for us.",302734.0
75903,302239.0,80012.0,Could not agree more.,302734.0
75903,302239.0,80009.0,"Learning curve for R is more when compared to Python. Moreover Python being a multi purpose language (scripting, web development), it will come handy in the future. Though my personal favorite is R, I can see a lot of momentum in favor of Python these days. End of the day, there is no clear winner (at least as of now).",301276.0
75903,302358.0,79931.0,"Thank you, yes I have seen this in the introduction part. इस there any other point that we need to consider while choosing the programme to proceed! Thanks",302734.0
75903,304474.0,,nan,
75903,306582.0,,nan,
75903,326515.0,,nan,
77130,307567.0,,nan,
77130,307769.0,,nan,
77130,307838.0,,nan,
77130,308659.0,,nan,
77130,308576.0,,nan,
77130,309069.0,,nan,
77130,311795.0,,nan,
77130,312076.0,,nan,
77130,313038.0,,nan,
77130,331228.0,,nan,
77130,319237.0,,nan,
84932,349010.0,,nan,
84932,348060.0,,nan,
84932,347949.0,,nan,
84932,347962.0,,nan,
84932,347996.0,,nan,
84963,348111.0,89581.0,Thank you!!,318822.0
82502,338230.0,87628.0,"Yeah Praphull, Nice to hear that.. Mutual learning is always more enjoyable.. and this discussion forum platform is part of group study.. :)",312746.0
82502,338230.0,87624.0,Thanks Alok! More than content I am looking forward to learn from my fellow classmates with mutual learning and collaborative efforts!!,313767.0
83412,343129.0,,nan,
83412,343108.0,,nan,
83412,342590.0,,nan,
83412,342629.0,,nan,
83412,343012.0,,nan,
83412,343081.0,,nan,
83412,343337.0,,nan,
83412,344227.0,,nan,
85283,349814.0,,nan,
85283,351744.0,,nan,
85283,350420.0,,nan,
85317,349989.0,90037.0,"Thanks a lot About the first one, is it worth the money? coz I have no idea about python. I've to start from the basics",315831.0
85317,349989.0,90062.0,you can download the pdf. Its free.,312746.0
85317,349989.0,90097.0,thanks man,315831.0
85317,350423.0,,nan,
85317,351592.0,90555.0,Hi Amani Im having 28 yrs manufacturing ind exp and B Tech in mech . No software no programming knowledge . can you guide me hiq to atart python understanding or which book or web site I hv to read and practice first in layman language. DC Bhatt from SRFLTD south africa Durban . my whats app no. +917771013132.,319969.0
85317,375065.0,,nan,
82639,338839.0,87892.0,Thank You Alok :),312063.0
82639,338922.0,87893.0,Thank You Amit:),312063.0
83479,342933.0,,nan,
83479,342988.0,,nan,
83479,342932.0,88500.0,Thanks...Got it,315765.0
87005,358793.0,,nan,
85750,353366.0,,nan,
85750,353374.0,,nan,
88711,367413.0,,nan,
88711,367395.0,,nan,
88711,367398.0,,nan,
88711,367400.0,93020.0,Thanx to all got it..!!,305847.0
88711,367553.0,,nan,
88711,367636.0,,nan,
88711,367668.0,,nan,
88711,367887.0,,nan,
88711,368105.0,,nan,
88711,368667.0,,nan,
87446,361336.0,,nan,
87446,361286.0,,nan,
87761,364330.0,,nan,
90066,375013.0,94736.0,"Hi Sandhya, I'd like to be a part",316253.0
90066,375013.0,94738.0,we need 2 more person,317990.0
90066,375013.0,94741.0,"Sandhya, Let's take the discussion to WhatsApp? we will have a group link shared here once anyone is interested to join. We can decide how dependent the case study is going to be and maybe split or work together depending on our calendar. If you are okay, you can text me over 7842516108",316253.0
90066,375013.0,94769.0,I am not joining any other group. We have Anchal Agrawal for our group. We need one more person now,317990.0
90066,375013.0,94742.0,"Sandhya, just in case if you are joining any other group instead, please let me know. And, so will I.",316253.0
90066,375013.0,97161.0,"Hi Sandhya, is it still open to join your group? I am also looking for one, do let me know in case it is possible",316889.0
90066,375265.0,94768.0,Sure...we need one more person for the Group now.,317990.0
90074,375263.0,,nan,
90074,375092.0,94753.0,Thanks Ram :),318019.0
89850,373852.0,,nan,
88284,365380.0,,nan,
88284,365621.0,,nan,
92611,390219.0,97347.0,asking password,312357.0
92611,390219.0,97348.0,Try using your root password. Hope this helps.,319006.0
92611,390219.0,97350.0,"In Mysql workbench , go to Database menu --> Manage server connections --> New Here create new connection and under that upload the script.",319006.0
92611,390219.0,97349.0,for installing SQL i never created password on any step so i wonder which password,312357.0
92611,390219.0,97366.0,let me go to will update,312357.0
123899,546070.0,,nan,
125095,545538.0,,nan,
85083,348604.0,89769.0,"In a way, yes. But it's important that you go through these without miss atleast before the stats part of the course begins.",310974.0
85083,348604.0,89762.0,"Thanks Ram. So it’s best to skip now, learn all the 5 software modules before the course starts in Sep end, and then come to it when required a reference. Did I get it right?",318335.0
85083,348673.0,,nan,
78757,318061.0,,nan,
82418,338019.0,87545.0,i had tried earlier with ( ) as well for all required but still it showed error.,302734.0
82418,338019.0,87563.0,its working now thanks,302734.0
82418,338019.0,87550.0,"does it work now? if not, please share that error",301652.0
82418,338171.0,,nan,
77935,,,nan,
82430,337916.0,,nan,
77964,312621.0,82557.0,Thank you very much,305847.0
77964,313031.0,82556.0,THANK YOU VERY MUCH,305847.0
77964,317342.0,,nan,
77964,338435.0,87692.0,i get this answer earlier.,305847.0
77964,339181.0,,nan,
83548,343357.0,,nan,
83548,343322.0,88566.0,you are welcome .,300687.0
83548,343322.0,88564.0,Thanks Ms. Maya Kavuri That helped me.,315661.0
83548,343688.0,,nan,
83889,345520.0,,nan,
89376,370669.0,,nan,
89376,370675.0,93878.0,"To add to this, generally, spaces are used to make the code look clean and neat. So, that it doesn't get clumsy. And it's just that 4 spaces is a widely accepted practice to make our code more readable.",319721.0
89376,370845.0,,nan,
88876,368417.0,,nan,
88876,368427.0,,nan,
111724,481356.0,114253.0,it's very sad to know this,318353.0
88691,367289.0,,nan,
88691,367255.0,,nan,
88691,367332.0,,nan,
88691,367437.0,,nan,
90433,377136.0,,nan,
92637,390300.0,,nan,
92637,390312.0,,nan,
92637,390458.0,,nan,
92637,390866.0,,nan,
92637,390484.0,,nan,
92637,391106.0,,nan,
91426,389927.0,,nan,
91426,382275.0,,nan,
91426,382281.0,,nan,
91426,382342.0,,nan,
91426,387357.0,,nan,
88678,367160.0,,nan,
88678,367162.0,92965.0,"why have you written input_str = 'analytics' this part? after writing this, you have basically changed the input to 'analytics' and thus output will always be YES. try again after removing that line.",317998.0
88678,367162.0,92967.0,I am not clear .Sorry,304338.0
88678,367162.0,92968.0,"The question description Write a code to check if the string in input_str starts with a vowel or not. Print capital YES or NO. For example, if input_str = 'analytics' then, your output should print 'YES'.",304338.0
88678,367162.0,92971.0,did it work?,317998.0
88678,367162.0,92969.0,"just remove this line ""input_str = 'analytics'"" from your code. ie. the 3rd line. remove it entirely and try again.",317998.0
88678,367165.0,,nan,
88678,367166.0,,nan,
88678,367176.0,,nan,
88678,367219.0,92982.0,"string variable already there in question, you need to write code from ""if"" condition.",318319.0
88678,367219.0,92981.0,"SO meaning , we no need to declare string variable ?",304338.0
88678,367219.0,92985.0,Let me check Rohit .,304338.0
88678,367263.0,92992.0,Tried Ankur . Thanks .,304338.0
88678,367652.0,,nan,
88678,367873.0,,nan,
88678,367886.0,,nan,
89232,370033.0,93925.0,Thank you,318461.0
89232,370228.0,93697.0,it creating row indexing but i need column heading should me multi indexed for example Sno | xy | yz | | a | b | a | b | 1 | 1 | 2 | 3 | 4 | 2 | 3 | 4 | 5 | 6 |,318461.0
112491,485393.0,,nan,
112491,485411.0,114562.0,"like the below code does not work. cust_error = customer[customer[""CIF_NO""].str.len() > 50] open('CUSTOMER_TEST.log', 'w').write(cust_error)",301650.0
112491,485411.0,114561.0,Yes. This works to some extent. Looking for solutions like appending more test to the same file including contents of a data frame.,301650.0
112491,485564.0,114607.0,I tried that already. It is easy to use but the default line size is making the file unreadable. Is there anyway to increase the linesize?,301650.0
112491,485536.0,,nan,
89267,370144.0,93600.0,thank you. Alphabetical ascending .,317149.0
89267,370195.0,93601.0,thank you.,317149.0
116442,503369.0,,nan,
116442,503405.0,,nan,
116442,503382.0,,nan,
118062,509051.0,,nan,
118422,513344.0,120362.0,"2. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html 3. I understand that it is not preferable to scale the dummy variables and you are correct about this as well. Normally it does not have any effect, but just to err on the side of caution, it is preferable not to do it.",304281.0
118422,513344.0,118651.0,"hello, providing more inputs on points 2 and 3 as requested in below comments",310509.0
118422,513490.0,,nan,
116596,504082.0,,nan,
116596,504134.0,117448.0,thanks,318814.0
116596,504138.0,,nan,
116596,505017.0,,nan,
116597,504081.0,117418.0,Yes may be for Lasso you don't need but definitely needed for Ridge,310974.0
116597,504081.0,117415.0,"Hi Ram, Lasso and Ridge are in itself can be used for feature elimination and are classified as Embedded feature selection method, whereas RFE is Wrapper elimination method. I am still learning, but not sure about the use case of RFE+Reg Regression",304814.0
116597,504147.0,,nan,
116597,504661.0,,nan,
116597,504860.0,,nan,
116597,504776.0,,nan,
117995,508724.0,,nan,
118442,510548.0,,nan,
118442,510995.0,,nan,
117642,507644.0,,nan,
117642,507507.0,,nan,
117642,507484.0,,nan,
118058,510320.0,,nan,
118058,509980.0,,nan,
118064,508909.0,,nan,
118505,511039.0,,nan,
118505,511106.0,,nan,
118505,511823.0,,nan,
118505,516793.0,,nan,
117308,506220.0,,nan,
115987,501590.0,,nan,
115987,502198.0,,nan,
115989,501666.0,,nan,
115989,502211.0,,nan,
115989,503868.0,,nan,
138769,599020.0,134210.0,"Is there any method called Boosting, used for performance enhancement?",300721.0
117324,506094.0,,nan,
117324,506104.0,,nan,
117324,506403.0,,nan,
117051,504662.0,,nan,
117075,504895.0,132263.0,Thanks,304319.0
117077,504796.0,,nan,
117077,505001.0,,nan,
117077,506986.0,,nan,
116126,502256.0,,nan,
116126,502474.0,117012.0,"I doubt if the regularization term has anything to do with minimizing bit value of coefficients. If model complexity is a function of coefficients, a feature coefficient with a high absolute value is more complex than one with a low absolute value. And if you have a latter one in your mix in the model, then the sum of squares of coefficients also increases.",310511.0
116126,502474.0,116992.0,Yes Rajarshi. But how does it help in handling 'complexity'?,311686.0
117875,508227.0,,nan,
118209,509678.0,,nan,
137984,596866.0,,nan,
137984,599390.0,,nan,
117135,505010.0,,nan,
117135,505009.0,,nan,
117135,505067.0,,nan,
117135,505338.0,117819.0,Thanks Bhaswati,311404.0
117135,505338.0,117703.0,It is really helpful.,320689.0
117135,505025.0,,nan,
117135,508296.0,,nan,
117135,508788.0,,nan,
116211,502631.0,,nan,
117459,506889.0,,nan,
116233,508410.0,,nan,
116233,502672.0,,nan,
116233,502673.0,,nan,
116233,502685.0,,nan,
116233,502707.0,,nan,
116233,503873.0,,nan,
116233,505645.0,,nan,
116233,508821.0,,nan,
122163,531582.0,,nan,
116241,502663.0,,nan,
116241,502818.0,,nan,
117157,505349.0,,nan,
117157,505404.0,,nan,
116270,503098.0,,nan,
116270,502794.0,117167.0,"No Darshana, its the other way. If we increase lambda, more importance to bring down RSS.",310467.0
116270,502794.0,117141.0,Means if we increase lambda RSS value also increase right?,310419.0
116270,503418.0,,nan,
117867,509882.0,,nan,
116362,503398.0,,nan,
117510,507096.0,,nan,
117510,507907.0,,nan,
117910,508582.0,,nan,
117910,508825.0,,nan,
117916,508488.0,,nan,
117916,508580.0,,nan,
117916,508757.0,,nan,
117879,508250.0,,nan,
117879,508638.0,117975.0,TA please clarify this,318756.0
118034,509026.0,,nan,
118033,508796.0,,nan,
120861,524887.0,,nan,
126340,550933.0,125093.0,can you send the link of the page? I am not very sure of the reference material,305839.0
126340,550933.0,125088.0,But it is mentioned in the additional material that error term increases with higher value of lambda...am not able to understand why?,310509.0
126340,550933.0,125094.0,"if you go to additional resources PA-2 , Ridge and Lasso regression, and the page on Trade off between Ridge and Lasso, somewhere in the middle this comment is made. The link is below. https://learn.upgrad.com/v/course/208/session/31696/segment/167500",310509.0
126340,550933.0,127688.0,"Hi Cheatan, I checked. There no mention of such statement as you are asking. It says. as we increase the value of lambda, the optimum will move towards the origin. As we decrease the value of lambda, the optimum will move away from the origin. It's about optimum and not about regularization.",428646.0
116490,503698.0,117213.0,Features that can be derived from raw attributes - is also how I understood the question except the answer given confused me.,317149.0
116490,503698.0,117222.0,yes. one option confused me as well. :),311686.0
116490,503698.0,117263.0,Can the TA please give a clarification on the choices given as answer on this question? I understand that this is a graded question and the explanation cannot be given until submissions are complete for this section. However I would appreciate an explanation after that time period of submissions are over.,317149.0
116490,503698.0,117305.0,"function of the attributes means , create a function which can be used for creating new features. Like f(x) = X1*X2, g(x) = cos(X3)*Sin(X4) So here X1, X2, X3,X4 ,f(x), g(x) all are the atrubutes for y. Similarly X1, X2 are the attributes for function f",344894.0
116490,503698.0,117270.0,I don't think TAs are going to read this comment chain. suggest you to create a new question for the same.,311686.0
116490,503698.0,117273.0,Question is under review. So hopefully they will :) But i will add another separate message . Thanks,317149.0
116490,503698.0,117323.0,Thank you.,317149.0
116490,504302.0,,nan,
116489,503692.0,117216.0,yes.,311686.0
116489,503692.0,117215.0,"in lecture videos, few examples, shown like with comb. of sin/cos function plus linear equation also, so that way model will be complex. instead of that if we create a featurs out of raw atrributes, we can acheive model in more simple manner. am i correct here?",300733.0
116518,503800.0,,nan,
116518,503797.0,117961.0,https://learn.upgrad.com/v/course/208/session/31156/segment/164595,318007.0
116518,503797.0,117845.0,"In that case why is s(m)=30.99*sqrt(m)*cos(0.53*m)+5.83*m+76.61 used as an example while talking about pattern of the sales - month wavy graph. Only to add confusion? If not in this topic, when is an equation like this allowed as it seemed to have given solution to the problem?",318007.0
116518,504568.0,,nan,
118107,509049.0,,nan,
118107,509191.0,,nan,
118047,510863.0,,nan,
117279,505776.0,,nan,
117279,505833.0,,nan,
117279,506101.0,,nan,
117279,506103.0,,nan,
117279,506235.0,,nan,
117279,508438.0,,nan,
117279,507965.0,,nan,
116021,501762.0,,nan,
116021,502044.0,,nan,
116021,501996.0,,nan,
116042,501851.0,,nan,
116042,501997.0,,nan,
116054,,,nan,
116156,502364.0,,nan,
116156,502325.0,,nan,
116156,509085.0,,nan,
125585,547569.0,124463.0,"I have done it , the R2 remains not much difference",314612.0
116080,502010.0,,nan,
117832,508096.0,118337.0,Yes,310974.0
117832,508096.0,118325.0,So the cost function and loss function are the same thing ?,311466.0
116238,502668.0,,nan,
116238,502652.0,,nan,
116238,502732.0,,nan,
116238,502935.0,,nan,
116238,505094.0,,nan,
116244,502671.0,,nan,
116244,502797.0,,nan,
116244,502813.0,,nan,
116244,502662.0,,nan,
116244,505098.0,,nan,
117158,505194.0,117534.0,"As x1,x2 and x3 are the independent variables, So we can make X1/X2, X1*X2, X1*X2*X3, root(X2) and so on. There are multiple possibilities, but we will create based on our use cases .",344894.0
117158,505194.0,117490.0,"It means , supposing we are having raw data as x1,x2 and x3. So what can be the possible features from these?",318427.0
117158,505194.0,117491.0,none of these is target variable. right?,311686.0
117158,505194.0,117567.0,okay.. thanks,318427.0
117158,505340.0,,nan,
117852,,,nan,
116288,505092.0,,nan,
116288,502891.0,,nan,
116288,502934.0,,nan,
116288,503148.0,,nan,
116290,503235.0,,nan,
116290,502881.0,,nan,
116290,502890.0,,nan,
116290,502942.0,,nan,
117207,505401.0,117444.0,I think the link you have shared doesn't quite answer my question. Can you point me to the exact paragraph in the page were my query has been clarified.,314730.0
117207,505401.0,117533.0,Please read page 1 and 2 for linear model understanding http://home.iitk.ac.in/~shalab/regression/Chapter3-Regression-MultipleLinearRegressionModel.pdf,344894.0
117207,505401.0,117532.0,"Hi Vinod, Subtracting raw attributes is linear because y = X1 - X2 will have one degree If you multiply then it will not be linear because it will become polynomial.",344894.0
117207,505453.0,,nan,
117207,506102.0,,nan,
117207,507974.0,,nan,
117225,505667.0,,nan,
117225,505457.0,,nan,
117225,505690.0,,nan,
117225,506099.0,,nan,
108359,468214.0,,nan,
108359,468233.0,,nan,
108359,468226.0,,nan,
108359,468650.0,,nan,
108404,468647.0,,nan,
108404,468736.0,,nan,
107271,464132.0,,nan,
107271,464422.0,110834.0,Thanks for the update. What is the name of the app you have used? I would like to try using it. Is it free and open source or paid?,304389.0
107493,465094.0,,nan,
107493,546027.0,,nan,
107493,465105.0,,nan,
109076,470887.0,112007.0,Thank You Sambit.,301649.0
109076,470942.0,112010.0,Welcome Rahul..,311117.0
109076,470942.0,112008.0,Thank you Brijesh.,301649.0
107123,464254.0,,nan,
107123,463027.0,,nan,
107123,463590.0,,nan,
107123,463985.0,,nan,
107123,464224.0,,nan,
107123,464472.0,,nan,
107533,465265.0,,nan,
107533,465347.0,,nan,
107533,465374.0,,nan,
107560,465393.0,,nan,
107560,465372.0,,nan,
107560,465384.0,,nan,
108231,467626.0,,nan,
108231,467796.0,,nan,
108249,467689.0,,nan,
108249,467763.0,,nan,
108271,467822.0,,nan,
107936,466279.0,,nan,
107936,466320.0,,nan,
107936,466851.0,,nan,
108166,,,nan,
107937,466272.0,,nan,
107937,466319.0,,nan,
106449,461805.0,,nan,
106449,463998.0,,nan,
106449,464912.0,,nan,
107926,466195.0,,nan,
107926,466194.0,,nan,
107930,466206.0,,nan,
107930,466335.0,,nan,
107933,466290.0,,nan,
115651,499720.0,,nan,
115651,499364.0,,nan,
115651,500062.0,,nan,
115651,500836.0,,nan,
107385,464696.0,,nan,
107385,464680.0,,nan,
110087,,,nan,
107378,464641.0,,nan,
107378,464642.0,110756.0,Okay,301655.0
107378,464638.0,,nan,
114607,494747.0,,nan,
114607,494662.0,,nan,
114607,494851.0,,nan,
108316,,,nan,
107423,464911.0,,nan,
107423,464864.0,110803.0,"I'm not limiting it to train test but post implementation of the model we observe more categories, then how to handle it?",318328.0
107423,464864.0,110804.0,"My understanding is that the emergence of new categories post-implementation could be due to following reasons: 1. The initial data on which the model was built was not good enough as it missed out on these categories. So an investigation would have to be done to identify any loop holes to fix this issue and then rebuild the model. 2. New categories got introduced during course of time and hence these are not part of the model built earlier. Again, we need to capture such data freshly for analysis purpose and rebuild model.",313826.0
107423,464864.0,110838.0,"post implementation of the model if we observe more categories which were not part of the model then we will ignore it, and we will handle as error",344894.0
107423,464901.0,,nan,
107423,465031.0,,nan,
112438,485145.0,,nan,
112438,485276.0,114528.0,Ok....but are we supposed to use the for the case study,301644.0
112438,485276.0,114529.0,p,301644.0
112438,485276.0,114530.0,"Sorry, I did not get your comment, please detailed your doubt",344894.0
112438,485276.0,114683.0,"Thanks for the replies. I think I got the answer. The dataset has mostly categorical variables where PCA is not a good approach, as PCA captures variance, it is best used for dimensionality reduction of numeric variables. We need to use RFE.",301644.0
112438,485276.0,114638.0,"Yes, in my opinion. Using RFE you will have to repeat and analyse the proccess multiple time to identify the right set of variables. With PCA, you will be begin to train the model with variables that capture the max. variance.",318438.0
112438,486067.0,,nan,
108409,468622.0,111472.0,"Thanks, this helped in getting an idea about it",318335.0
108409,468667.0,,nan,
108409,468638.0,,nan,
108409,468702.0,,nan,
108409,468935.0,,nan,
108321,468038.0,111345.0,Thank you krishnan,312019.0
108321,468034.0,,nan,
108321,468424.0,,nan,
108672,469781.0,,nan,
108672,469800.0,133848.0,"Missed reading the rules part: PSI < 0.1 - No change. You can continue using existing model. PSI >=0.1 but less than 0.2 - Slight change is required. PSI >=0.2 - Significant change is required. Ideally, you should not use this model any more. Its fine now thanks.",318598.0
108672,469800.0,133847.0,"I went through https://www.listendata.com/2015/05/population-stability-index.html it shows derivation in excel, but not able to understand what the value would imply. You or anybody having insights on it could help me please?",318598.0
109672,472961.0,,nan,
115787,501050.0,,nan,
108461,469007.0,,nan,
108461,468947.0,,nan,
108323,468029.0,111338.0,Welcome Seetham,311117.0
108323,468028.0,,nan,
108323,468035.0,,nan,
108323,468049.0,,nan,
108323,468046.0,,nan,
108296,,,nan,
108451,468999.0,,nan,
108451,469022.0,,nan,
108457,469045.0,,nan,
108382,468380.0,,nan,
107975,466443.0,111070.0,Thanks a lot for the information,318455.0
107975,466408.0,111071.0,Thank you,318455.0
108200,467518.0,,nan,
108200,468032.0,,nan,
108200,468051.0,,nan,
113485,489515.0,115392.0,Agreed. Its coefficient. Wrongly understood the question.,311117.0
113485,489515.0,115379.0,As far as I remember the top 3 variables should be find out based on variable's coefficient.,317991.0
113485,489574.0,,nan,
113485,489632.0,,nan,
113485,489982.0,,nan,
114771,495382.0,,nan,
114771,495445.0,,nan,
108437,468741.0,,nan,
108437,468821.0,,nan,
108437,468708.0,111443.0,"thanks, but why to create a dummy variable for this, we can simply transform this column into 0 and 1. Then we don't even have to create a new column, and there will not be any multicollinearity.",317073.0
108437,468708.0,111469.0,we can simply transform this column into 0 and 1. But we have to check multicollinearity with other variables as well,344894.0
108437,469695.0,,nan,
108257,467779.0,,nan,
108257,467770.0,,nan,
108257,467754.0,,nan,
108257,467832.0,,nan,
108257,492814.0,,nan,
109432,472012.0,,nan,
109432,472300.0,,nan,
107437,465029.0,,nan,
107437,465091.0,,nan,
107826,465839.0,,nan,
107826,465836.0,,nan,
105925,457189.0,,nan,
105925,457292.0,,nan,
105925,464859.0,,nan,
105925,465088.0,,nan,
107081,462692.0,110442.0,how to identify outliers is not my question. How to handle,315455.0
107081,462706.0,110629.0,"Yes, but it has subtle importance as of now",301655.0
107081,462706.0,110627.0,this would have added as part of course content bcoz its critical,315455.0
107081,463050.0,110904.0,Agree 100%,317514.0
107520,465229.0,,nan,
107520,465607.0,,nan,
107545,465314.0,,nan,
107545,465373.0,,nan,
109399,471959.0,,nan,
107549,465346.0,,nan,
107549,465313.0,,nan,
107549,465383.0,,nan,
107549,465377.0,,nan,
108272,467823.0,,nan,
108272,467817.0,,nan,
108129,467088.0,,nan,
108363,468260.0,,nan,
108142,467329.0,,nan,
108142,467198.0,,nan,
108411,468619.0,,nan,
108411,468633.0,,nan,
108411,468715.0,,nan,
107611,465544.0,,nan,
108339,468229.0,111368.0,"So STEP#1: i.e. Using RFE and finding statsmodel we eliminate all values which are insignificant. STEP#2: Now we calculate VIF & identify that there are some variables with high VIF, we drop one of the column with highest VIF. Step#3: rebuild to see stats using statsmodel, now from here going fw we check those variables with high p value but do not drop it and then calculate VIF too see if they are same but there exist any other column with higher VIF we go ahead n drop them(exclusive of the columns thats of some business relavence). Is that what we are doing here?!",312259.0
108339,468229.0,111370.0,Thank you!,312259.0
108339,468229.0,111369.0,Yes,310974.0
108011,466589.0,111185.0,"Thanks. What I have understood from the above mentioned article that for normal distributed values we use Standard Scaler method while if the SD is low and the values are not normally distributed then we use Min-MAX. Moreover, if the outliers are present Standard Scaler would be a better choice.",304319.0
108011,466579.0,,nan,
108011,466769.0,,nan,
107897,466088.0,,nan,
107897,466165.0,,nan,
107897,466130.0,111022.0,Wt abt Sklearn and stats model uses?,300721.0
108066,466845.0,,nan,
108066,466886.0,,nan,
108066,466842.0,,nan,
108066,466848.0,,nan,
108201,467513.0,,nan,
108201,467522.0,,nan,
108201,467546.0,,nan,
107902,466123.0,,nan,
109019,470765.0,,nan,
109019,470750.0,111909.0,Thanks. That makes sense. :),307495.0
109019,470788.0,,nan,
107004,464276.0,110694.0,"thanks Paras, that still doesn't explain as to why a significant value was dropped in the cases I mentioned above",309211.0
107004,464276.0,110721.0,Please let me know if you have any other question on this,344894.0
107004,464276.0,110720.0,"Hi, Based on count you cannot say significant variable. So the above case is the part of data preprocessing. You can drop anyone of them.",344894.0
107004,465460.0,110943.0,"thank you Rajesh. I know why Multiplelines_no phone service was dropped (and that count wasn't the reason ). the question I asked was why month-to-month and bank-transfer variables were dropped. and the answer is ""You can drop anyone of them"" ; read comments from Paras above. Thanks!",309211.0
107905,466164.0,,nan,
107575,465464.0,,nan,
107575,465443.0,,nan,
107575,465475.0,111534.0,"But our ultimate objective is that irrespective of the data, model should give more or less same results right. We dont want model to get biased - working on same train dataset everytime and producing same results. Can you explain with an example? Maybe my understanding is wrong",308437.0
107575,466085.0,111535.0,this is one of my doubts,308437.0
115634,499123.0,,nan,
107956,466396.0,,nan,
108342,468235.0,,nan,
108342,468232.0,,nan,
108342,468197.0,,nan,
113263,488487.0,,nan,
113263,488491.0,,nan,
113263,488654.0,,nan,
113263,488788.0,,nan,
107517,465244.0,,nan,
107517,465348.0,111531.0,"Thanks Ram...Little difficult to remember this though, but atleast you have provided a flow diagram which helps in eliminating the confusion what I had.",308437.0
107517,465274.0,,nan,
107517,465949.0,,nan,
108027,466693.0,,nan,
108027,466658.0,,nan,
108027,466849.0,,nan,
108027,466766.0,,nan,
107445,464991.0,110808.0,"Agreed. So dummy encoding for ordinal and one-hot for nominal categorical variables is what I also infer from the few reading on the net. This indicates that these two are two different processes, though very similar in nature. However, in the video it appeared that these terms are synonymous (which is contradicting the general view expressed on the net) and hence the query.",313826.0
107445,465026.0,110813.0,Ok. But the doubt that I have is whether they are the same or different?,313826.0
107445,465026.0,110814.0,They are same. One hot encoding is the name of the technique and the result of that is dummy variables.,310974.0
107445,465026.0,110815.0,"Ok..Just to be clear, in both One-Hot and Dummy encoding, we generate n-1 columns for the n categories, right?",313826.0
107445,465026.0,110816.0,We generate n dummy variables using one hot encoding technique and whether we retain one of them or not depends on the model you are building.,310974.0
107445,465250.0,110889.0,"Generally, we drop one dummy variable to reduce features.",344894.0
107445,465250.0,110855.0,"Hi Paras. Thanks for the links. Although the second link starts by saying that these two are different ways, a further reading about Dummy-Variable Trap(https://www.algosome.com/articles/dummy-variable-trap-regression.html) does suggest that we eventually have to drop one of the newly created dummy-variableand hence the two are same.",313826.0
107445,585217.0,,nan,
107445,465934.0,,nan,
114922,496878.0,,nan,
113181,488076.0,,nan,
113181,488296.0,,nan,
114923,495938.0,,nan,
114923,495940.0,,nan,
114923,496085.0,,nan,
114923,496089.0,,nan,
115437,497867.0,,nan,
115437,497961.0,116522.0,"Hi Muthu, Yes you are so very right. I was just trying to understand the underlying concept of log odds in logistic regression. tried implementing the concept, but the problem was, without a continuous dependent variable, the model was unable to train well and the r2 score hovered around 2.5 to 3. I just wanted to understand how log odds became are being used in the practical models we are making. theoretically, it is a powerful tool though. thanks for your explanation, btw!",305839.0
108467,469048.0,,nan,
108441,468799.0,,nan,
108242,467775.0,,nan,
108242,467810.0,,nan,
108242,468398.0,,nan,
109632,472846.0,,nan,
109632,473071.0,,nan,
107484,468020.0,,nan,
107484,465186.0,110843.0,okay got it thanks a lot,303674.0
107484,465186.0,110842.0,"Hi Rajarshi , but how much ? how to know that?",303674.0
109090,470918.0,,nan,
109090,470926.0,,nan,
109090,470940.0,,nan,
107865,466117.0,,nan,
107865,466099.0,,nan,
108099,467062.0,,nan,
108252,468023.0,,nan,
108252,467706.0,,nan,
108252,467789.0,,nan,
108160,467312.0,,nan,
108160,467325.0,,nan,
108160,468010.0,,nan,
108080,466984.0,,nan,
108080,467054.0,,nan,
108080,468013.0,,nan,
108041,466732.0,111124.0,"Hey Vinay, Thank you",301655.0
108041,466753.0,,nan,
107945,466333.0,111059.0,Thank you,308435.0
136385,589868.0,132247.0,Thanks. Crisp and clear.,315028.0
137292,592940.0,,nan,
78415,316874.0,83044.0,Got it @gaurav..corrected.,305847.0
78415,316742.0,83038.0,It's working.. Thanks a lot kiran,305847.0
87100,358658.0,,nan,
90170,375786.0,,nan,
90170,375796.0,,nan,
81314,330858.0,,nan,
81314,331628.0,,nan,
81314,331907.0,,nan,
87280,359664.0,,nan,
87280,359669.0,,nan,
87280,359851.0,,nan,
81869,343034.0,,nan,
81869,334030.0,89530.0,it worked,314183.0
81869,334030.0,86791.0,Yes it did,302734.0
81869,333762.0,86714.0,I can’t reset the window. In the second problem also the run code said it’s fine but showed rejected on submission,302734.0
81869,333775.0,86717.0,They were matching,302734.0
81869,333775.0,86733.0,The exercise wanted to separate first name second name and code which I did and it ran successfully How to know what is the correct answer then,302734.0
81869,333775.0,86718.0,"you should use the query that has been taught in the course for the assignment you are working and see the method of code you are using from the current assigment and also share the full scrren of your code enter,then it is easier to provide better options if feasible",307843.0
81869,333766.0,,nan,
81869,333830.0,,nan,
81869,335859.0,,nan,
81869,338948.0,89533.0,I am also facing the same problem.,314183.0
78218,314424.0,,nan,
78218,314403.0,,nan,
78218,314470.0,,nan,
78218,314478.0,82743.0,Here you are starting for a[3] = 4 and going till a[0] = 1 and pick elements at a difference of 2 in reverse order as earlier. first printed a[3] = 4 then left one element and printed a[1] =2 at difference of 2.Could not go any further as the string is done,300698.0
78218,314478.0,82745.0,ok...got it tq,300693.0
78218,314478.0,83040.0,"Hi Shravan, For this type of scenario u have you follow below. [ : : ] Hi Shravan, For this type of scenario u have you follow below. [ : : ] Then jus follow the characters which is mentioned. Then jus follow the characters which is mentioned. Start =4 Stop=towards initial of the string 2 Step=we have to jump 2 but from last of the string. Answr=42",305847.0
78218,328499.0,,nan,
81336,331237.0,,nan,
81336,331341.0,,nan,
81336,331432.0,,nan,
81336,331283.0,,nan,
85021,349410.0,,nan,
84680,346886.0,,nan,
84680,346918.0,,nan,
84680,352242.0,,nan,
84680,358140.0,,nan,
81342,331289.0,,nan,
81342,331330.0,,nan,
81342,331619.0,,nan,
81342,331338.0,,nan,
84174,346287.0,,nan,
84174,346340.0,89169.0,What is its significance?what doest it do?,300690.0
84174,346340.0,89171.0,"Just visit the link shared by Naga Sai, https://www.geeksforgeeks.org/python-format-function/ you can understand the significance of it.",312746.0
84181,346397.0,89197.0,it's already a list,310585.0
84181,346347.0,,nan,
84181,346300.0,,nan,
84181,349138.0,,nan,
84717,347092.0,,nan,
81892,333847.0,,nan,
81892,334029.0,,nan,
81892,334071.0,,nan,
81892,333880.0,,nan,
81892,335661.0,,nan,
81892,336630.0,,nan,
81892,338968.0,,nan,
81892,338973.0,90383.0,"I've used similar way to split the details. However, urs is much more simpler. In your case we can complete the whole code snippet in 2 lines. Thanks for sharing Prabhal.",318370.0
81892,338973.0,90443.0,Thanks a lot Abhinay,311404.0
81892,355155.0,,nan,
77636,310315.0,,nan,
77636,310686.0,82133.0,Why 'a' variable and 'b' variable address is same? How Python is allocating same address to both variables when value is same..,304397.0
77636,310686.0,82142.0,Please refer below link :) http://scottlobdell.me/2013/08/understanding-python-variables-as-pointers/,300691.0
77636,310686.0,82139.0,Every value in Python is a reference (pointer) to an object. Objects cannot be values. Assignment always copies the value (which is a pointer); two such pointers can thus point to the same object.,300691.0
77636,313341.0,,nan,
77636,314098.0,,nan,
77636,317344.0,,nan,
78790,318230.0,,nan,
78790,318757.0,,nan,
78790,321074.0,,nan,
87151,358947.0,,nan,
87151,359124.0,91667.0,"But what if we want to remove only a few types of elements completely? Even if you convert a list into a set, these few types which I want to completely remove still exist. I will have to manually remove these, it would require one more step. So, it is not ideal.",319721.0
87151,359145.0,,nan,
87151,359740.0,,nan,
80298,325407.0,84913.0,"Thanks, see this. I have tried your way as well check Testcase #1 (sample) Status Passed Execution time 0.31s CPU 0s Memory 6MB Description Testcase passed! The solution's output matches the expected output. Annotation Right answer Input {'Jack Dorsey' : 'Twitter' , 'Tim Cook' : 'Apple','Jeff Bezos' : 'Amazon' ,'Mukesh Ambani' : 'RJIO'} Solution output ['Amazon', 'Apple', 'RJIO', 'Twitter'] Expected output ['Amazon', 'Apple', 'RJIO', 'Twitter'] keyboard_arrow_up close Testcase #2 (weight: 1) Status FailedExecution time 0.32s CPU 0s Memory 392kB Description Testcase failed! The solution's output doesn't match the expected output. Input {'Jack Dorsey' : 'Twitter' , 'Tim Cook' : 'Applo','Jeff Bezos' : 'Amazon' ,'Mukesh Ambani' : 'RJIO'} Solution output ['Amazon', 'Apple', 'RJIO', 'Twitter'] Expected output ['Amazon', 'Applo', 'RJIO', 'Twitte The output of rejected Test case2 automatically shows different ....not sure why....",305790.0
80298,325407.0,84945.0,The expected output in testCase 2 looks wierd. Drop a query to your student mentor.,300748.0
80298,325407.0,85131.0,"No need to use the function list() as the values() or keys() functions return a list only. vipin, your code seems fine.",304319.0
80298,325407.0,85270.0,Thanks a lot!!,305790.0
80298,325407.0,85269.0,Thank you Ruchita!!,305790.0
80298,344038.0,,nan,
80298,352224.0,,nan,
81449,331766.0,,nan,
81449,331814.0,,nan,
81449,332062.0,86262.0,My thoughts exactly.,311315.0
81449,332062.0,86197.0,This make sense,306011.0
81449,331906.0,,nan,
81449,332305.0,,nan,
81449,332009.0,,nan,
81449,332148.0,,nan,
81449,332198.0,,nan,
81449,332329.0,,nan,
81449,332643.0,,nan,
84810,347430.0,,nan,
84810,347433.0,,nan,
84810,347940.0,,nan,
84810,347558.0,,nan,
84810,347469.0,,nan,
84280,346659.0,89262.0,"okay, so that's the same kernel playing its role herein python as well to execute the commands and programs ?",317070.0
84280,346659.0,89307.0,No.. Python kernel is different from OS kernel.,312746.0
84280,346627.0,,nan,
84280,346933.0,89337.0,"Thanks, pretty much clear now!",317070.0
90124,375349.0,,nan,
90124,375355.0,,nan,
90124,375361.0,94802.0,"Can you provide expected output? Will be more clear with that,",318368.0
90124,375361.0,94800.0,"Hi Kapil, Thanks I tried that too but it does not work , any other ideas?",321850.0
90124,375379.0,94973.0,"I did this, its not working. Technical issue?",321850.0
90124,375372.0,94881.0,"Hi Vipul, In your first example, the output would definitely print the unsplitted string as you're printing the same string.",318355.0
90124,375372.0,94903.0,Oh Yes missed that....Thanks for pointing it out.,317991.0
90124,375741.0,,nan,
90124,376393.0,94984.0,strange! what does the error say this time??,316349.0
90124,376393.0,94983.0,"Thanks. it is aligned to left, still does not work. I m guessing a technical error.",321850.0
90124,376375.0,,nan,
78896,318756.0,,nan,
78896,329763.0,,nan,
78895,318951.0,83697.0,Thanks Vicky,305845.0
78895,318755.0,83696.0,okay.. Got it.. Thank you :),305845.0
78895,319361.0,,nan,
78895,329765.0,,nan,
88492,366789.0,,nan,
88492,366178.0,,nan,
88492,366838.0,,nan,
88492,366935.0,,nan,
88492,367098.0,93618.0,"so why can't strVal.join() syntax be used for joining, same as in split?",310509.0
88492,367098.0,93636.0,"Split is used to convert a string into a list by using a delimiter maybe a comma, space etc based on the requirement.",312953.0
88492,367098.0,93635.0,"strVal.join() won't work Instead, try this: strVal = ""abcdefg"" separator = "","" finalValue = separator.join(strVal) print(finalValue) Returns: a,b,c,d,e,f,g The strings in Python are arrays of bytes representing unicode characters. Hence the join argument takes the array and concatinate with the separator in iterative way.",312953.0
81536,332395.0,,nan,
81536,332386.0,,nan,
81536,332165.0,,nan,
75981,314617.0,,nan,
75981,302931.0,,nan,
75981,302514.0,,nan,
75981,302822.0,,nan,
75981,302846.0,,nan,
84869,347657.0,,nan,
84869,348545.0,,nan,
84869,348807.0,,nan,
81555,332249.0,86429.0,Yes Maya,310974.0
81555,332249.0,86416.0,thanks Ram the logic provided is right but syntax should be answer =input_list[2][0],300687.0
81555,332300.0,86417.0,thanks Amit the logic provided is right but syntax should be answer =input_list[2][0],300687.0
81555,332697.0,,nan,
81555,332703.0,,nan,
81555,332869.0,87389.0,() is a syntactical error.,306729.0
81555,332869.0,86434.0,"Hi Maya, In Python, square brackets are used to access a member of the list. Per my knowledge, round brackets are not used to access member of any data structure. So round brackets woudl give you a syntax error.",306725.0
81555,339338.0,,nan,
80168,324667.0,84787.0,Thats a great explanation... Thank u very much 😎,308437.0
80168,324642.0,84783.0,"yes sir, you got it.",304692.0
80168,324642.0,84782.0,So it works on all symbols but only at the extreme ends,308437.0
80169,324649.0,84784.0,Seems true...can any TA verify this?,308437.0
80169,324653.0,,nan,
80169,327093.0,,nan,
83649,343901.0,,nan,
88758,367686.0,93069.0,didnt ask for one,314221.0
88758,367686.0,93068.0,yes..install again. do note the installer will give you an option to create desktop shortcut. choose that. even i had to do that..so remember.,317998.0
88758,367686.0,93073.0,I'm currently installing it again.. still in progress.. will try and let you know,314221.0
88758,367686.0,93066.0,yes.. tried that.. cant find it anywhere.. installing again,314221.0
88758,367686.0,93071.0,C:\Users\asaini\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Anaconda3 (64-bit) replace asaini here with your username of desktop/laptop,317811.0
88758,367686.0,93072.0,"oops. then, do you remember in which folder you have installed. this would have to be specified in the start of the installation. if you remember..go and check in that folder..it should be there. witj an exe file..usimg which you can open it.",317998.0
88758,367686.0,93075.0,yes..but do check and remeber the path of the folder where you are trying to install.,317998.0
88758,367686.0,93080.0,The installation is already in progress.. will share the error screenshot later,314221.0
88758,367686.0,93078.0,"Can you share screenshots of installation windows you are hitting ""Next"". It will help us to understand your issue better.",308966.0
88758,367695.0,93076.0,C:\Users\asaini\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Anaconda3 (64-bit) replace asaini here with your username of desktop/laptop,317811.0
88758,367695.0,93070.0,yes.. cant find it.,314221.0
88758,367831.0,,nan,
88758,368670.0,,nan,
88758,367931.0,,nan,
81671,332766.0,,nan,
81671,337541.0,,nan,
81671,332806.0,,nan,
88685,367282.0,,nan,
88685,367206.0,92979.0,Thanks Amani..will check in evening and chat you in case issue persists,317156.0
88685,367206.0,93057.0,Thanks it resolved issue,317156.0
88685,367963.0,,nan,
81677,332832.0,,nan,
80668,327801.0,87942.0,"this link has explained it well, easy to understand. thanks for sharing it Vicky",306011.0
80668,328082.0,,nan,
80668,328486.0,,nan,
80668,328387.0,,nan,
80523,326911.0,,nan,
80523,327026.0,,nan,
80523,327246.0,,nan,
80523,328070.0,,nan,
80525,326902.0,,nan,
80525,326906.0,,nan,
80525,360999.0,,nan,
80525,328225.0,,nan,
87210,359243.0,,nan,
87210,359246.0,,nan,
87210,359408.0,,nan,
84783,347372.0,89468.0,"yes, that was the case but then i renamed that variable, still getting the error. If variable once created with name , list ; renaming it will not help?",317070.0
84783,347372.0,89484.0,"Can you please paste the code used to rename the variable. afaik, this is not possible in python. You will have to delete the list variable using the command del list you can use dir(),locals(),globals(),vars() etc built-in functions to view the variables declared in your current namespace.",313826.0
84783,347372.0,89574.0,"the variable with name ""list"" would still remain in your current workspace and hence you would still get the error. Delete the variable by running the command del list and then try again.",313826.0
84783,347372.0,89554.0,"no, i haven't used any function to rename the variable declared as list, just renamed it like we rename file/folders, like if i used list and later on renamed as list_1. Will this still create problem ?",317070.0
81702,332990.0,86464.0,thanks Ashish now it works .i was just looking only the code i have written as the rest is already defined but a kind of typo error.,300687.0
81702,332989.0,86465.0,now it works the code written by me was fine but by mistake the first line instead import was pimport .now i have corrected that and it works,300687.0
81702,339479.0,,nan,
81702,347370.0,90944.0,Thank u.,300690.0
81702,347370.0,90938.0,Please try. a=list(input_tuple) b=a.append('Python') tuple_2=tuple(a) print(tuple_2),320195.0
81702,356430.0,,nan,
81705,332942.0,,nan,
81705,332983.0,86466.0,thanks Anirudh. it works,300687.0
81705,349130.0,,nan,
88770,367787.0,,nan,
88770,367794.0,,nan,
88770,367765.0,93144.0,"There's also readability of the code. Writing print(array_2d[1, ]) does not indicate clearly what the code intends to do.",306733.0
87233,359397.0,,nan,
87233,359406.0,91613.0,got it! thanks,316132.0
87233,359407.0,,nan,
87233,359483.0,,nan,
79282,320378.0,84011.0,"Hey jetendra, Check carefully in answer I have done the same but not execute having error while I start the verify. Have you faced any time or not.",305847.0
79282,320262.0,84025.0,nan,305847.0
79282,322005.0,,nan,
79282,322897.0,,nan,
83528,343209.0,88615.0,Thanks for correcting,308639.0
83528,343199.0,,nan,
83528,351974.0,,nan,
85175,349411.0,,nan,
85175,349519.0,90455.0,"Hi Aditya, You are correct with respect to the index representation, I thought of highlighting the key difference in how the arrays are defined and accessed in other languages and in Python. It is with respect to accessing the sub-list or sub-array. In C or C++, we can access or modify a single element whereas there is no representation to create or access a range of values or sub-array. We need to use a conditional statement like While or For to access a sub-array in this case. For example, Sample_Array[] = {1.1, 2.3, 3.2, 2.4, 9.2} We can access or modify a single element, say Sample_Array[4] value which is 9.2, can be accessed and modified to a new value - Sample_Array[4] = 10.0 Whereas we cannot define a range and create sub-array in these languages like the lists in Python - sample_list = [""a"", ""b"", ""c"", ""d"", ""e""] sample_list[0:2] ['a', 'b']",314730.0
85175,349519.0,89958.0,"Hi Vinod, Can you please tell in which language it happens that if we give range [0:2] and it gives us all three elements i.e. at index 0,1 & 2 ? As in Java, we see that the substring function behaves in a similar manner i.e. if we use str.substring(0,4) it will give us characters at 0,1,2,3 as string",318355.0
85175,349552.0,,nan,
85175,349544.0,,nan,
85175,349283.0,,nan,
85175,349693.0,,nan,
85175,349919.0,,nan,
85175,349947.0,,nan,
85175,350079.0,,nan,
85175,350211.0,,nan,
85175,350239.0,,nan,
80545,327081.0,85134.0,"if u try to execute the following : words_in_sentence = a_sentence.split() print(words_in_sentence) Before actually initialising a_sentence, then it would give the error.",304319.0
80545,327081.0,85132.0,How does execution of previous comments matter? Can U explain... Thanks,308437.0
80545,327177.0,,nan,
87666,364025.0,,nan,
87666,363920.0,,nan,
87666,364323.0,,nan,
83672,344015.0,,nan,
83672,344019.0,88863.0,"You are hard coding Input_str you are providing it a value inside the code. You don't have to do it input_str = sys.stdin.read() will do it for you. Remove the line ""input_str = ' This is my first code'"" It will work",305651.0
83672,344040.0,,nan,
83672,345326.0,,nan,
83672,358547.0,,nan,
83717,,,nan,
83803,352233.0,,nan,
83803,345038.0,,nan,
85191,,,nan,
83804,345060.0,,nan,
83804,344990.0,,nan,
80746,328165.0,,nan,
83184,341905.0,,nan,
83184,351352.0,,nan,
83184,341933.0,,nan,
83431,342677.0,,nan,
83431,343093.0,,nan,
83431,342632.0,,nan,
83431,343446.0,,nan,
83820,344959.0,,nan,
83820,345682.0,,nan,
83820,345760.0,,nan,
83820,345830.0,,nan,
83820,348536.0,,nan,
85781,352348.0,90475.0,Thanks Vinay for the answer :),314730.0
85781,352610.0,,nan,
85781,352643.0,,nan,
85781,352726.0,,nan,
85781,352803.0,,nan,
85781,353406.0,,nan,
80530,326951.0,,nan,
80530,326895.0,85162.0,"Ok set difference issue is resolved now how to compute symmetric difference? Input [[1,2,3,4,5,6],[2,3,4,5,6,7,8,9]] Solution output Traceback (most recent call last): File ""/code/source.py3"", line 10, in answer_2 = set_1.setdiff(set_2) AttributeError: 'set' object has no attribute 'setdiff' Expected output {1} {1, 7, 8, 9} keyboard_arrow_up",303968.0
80530,326895.0,85163.0,set_1 = set(list_1) set_2 = set(list_2) answer_1 = set_1.difference(set_2) answer_2 = set_1.setdiff(set_2) above is the line of code which i wrote and shows an error.,303968.0
80530,326895.0,85165.0,"The type of error is an attribute error. If you read the error statement carefully, you'll see that the message says that there's no attribute called setdiff(). That's the bug. You should write set_1.symmetric_difference(set_2) In general the error message gives you all the info you need. And, to know what methods and attributes are available for a given object, type dir(object_name).",306733.0
80530,326895.0,85197.0,You're welcome :),306733.0
80530,326895.0,85176.0,"It worked, Thanks",303968.0
80530,328077.0,,nan,
80530,329213.0,,nan,
80530,339507.0,,nan,
80530,341178.0,90388.0,"you need to return it in list not in set, so convert it into sorted list",318077.0
80530,341373.0,,nan,
80530,341374.0,,nan,
80530,342317.0,,nan,
89468,371219.0,,nan,
89468,371789.0,,nan,
89468,371252.0,,nan,
85249,349481.0,,nan,
85249,349483.0,,nan,
85249,351919.0,,nan,
83361,343085.0,,nan,
83361,342390.0,,nan,
83361,342359.0,,nan,
83361,342421.0,88387.0,Thank you. This helps.,312376.0
89940,374443.0,,nan,
89940,374463.0,,nan,
89940,374467.0,,nan,
89940,374492.0,,nan,
89940,374623.0,,nan,
90635,378040.0,95282.0,"import numpy as np n = int(input()) array_n=np.arange((2,2)) array_m=np.arange((4,4)) print(array_n) print(array_m)",310008.0
79572,321935.0,,nan,
79572,321904.0,,nan,
79572,321909.0,,nan,
79572,321947.0,,nan,
79572,321938.0,,nan,
79572,321914.0,,nan,
83568,343364.0,,nan,
83568,343356.0,,nan,
83568,343639.0,,nan,
83569,343363.0,,nan,
83569,343570.0,,nan,
83569,343579.0,,nan,
83569,343694.0,,nan,
83569,344222.0,,nan,
83569,343437.0,,nan,
85353,350286.0,90184.0,Thank You I shall check,308431.0
85353,350715.0,,nan,
85353,350930.0,,nan,
85353,351196.0,90392.0,Thanks..,308431.0
83419,342579.0,,nan,
83419,342582.0,,nan,
83419,342607.0,,nan,
86470,356244.0,,nan,
86470,356239.0,,nan,
86470,356406.0,,nan,
86470,358180.0,,nan,
83252,341898.0,,nan,
83252,341917.0,,nan,
83252,342873.0,,nan,
83252,345973.0,,nan,
83393,342462.0,,nan,
83393,343087.0,,nan,
83393,342583.0,,nan,
88814,367986.0,,nan,
88814,367989.0,,nan,
88814,367991.0,,nan,
88814,367992.0,,nan,
88814,368021.0,,nan,
88814,368680.0,,nan,
88814,368028.0,,nan,
86985,358185.0,,nan,
86985,358237.0,,nan,
86985,358271.0,,nan,
86985,359226.0,,nan,
86985,359383.0,,nan,
86985,358171.0,,nan,
86985,359720.0,,nan,
83130,341297.0,,nan,
83130,341404.0,,nan,
85446,,,nan,
85455,350769.0,90299.0,and where do I write the test case 2?,316891.0
85455,351835.0,,nan,
85449,350731.0,,nan,
85449,351148.0,,nan,
85449,351192.0,,nan,
85469,350784.0,90203.0,it works....thank u!,318446.0
85469,350805.0,,nan,
85469,351582.0,,nan,
87253,359541.0,91739.0,Thanks for the help!,316545.0
85489,350917.0,90412.0,Thanks Alok. This advice helped.,318335.0
85489,351382.0,,nan,
85462,350768.0,,nan,
85462,350781.0,,nan,
85462,351264.0,90280.0,"answer would be['R','Python']",300690.0
85462,350822.0,,nan,
85491,350907.0,90218.0,"yes, i tried this still no luck. Test case 2 is failing",318353.0
85491,350933.0,,nan,
85491,351373.0,,nan,
85491,351445.0,,nan,
85491,351907.0,,nan,
93767,395185.0,,nan,
93767,396188.0,,nan,
84127,346073.0,89124.0,"Hi Alok, Thanks so much for clarifying. I agree that the while submitting, it will check with some random inputs with trailing spaces. However, in my case, the ""Input"" is different in Test Case 2. As per the question, i have given the input string as ""This is my first code"". After using my code, the Test case 1 is passed, because it has the same input . But in Test Case 2, the input provided is ""This to my first code"". The input string is different when compared to the question. Due to this, the expected out put for Test Case 2 is ""This to my first code"" without spaces. In this case, even if i had used lstring(), it would have shown an error. I am trying to understand how to handle this input changes in different Test Cases, as we go by the question.",316202.0
84127,346151.0,89139.0,"Hi Alok, GOT IT!! Thanks so much. Much Appreciated..",316202.0
84127,349409.0,,nan,
84127,349721.0,,nan,
85520,351078.0,,nan,
85520,351113.0,,nan,
85520,351813.0,,nan,
85520,351879.0,,nan,
85520,368381.0,,nan,
80210,324913.0,,nan,
80210,325166.0,,nan,
80210,325229.0,84904.0,"Thanks a lot for your valuable suggestion, can you see this code as well. Actually I am from non tech background and first time learning coding:)",305790.0
80210,325448.0,,nan,
80210,325963.0,85012.0,"Thanks Prashant however its not like that, if you see in detail my solution is not matching in case of Test Case2 which i have no idea what does that mean, there is only one Q i suppose to do coding which is perfectly fine validated as Test case1 so what is this Test case2 validation all about...please help here!",305790.0
85685,351940.0,,nan,
85685,351724.0,90402.0,Thank you so much for your valuable response :),320195.0
85685,351821.0,90403.0,Thank you so much for your valuable response :),320195.0
85685,351880.0,,nan,
85685,351999.0,,nan,
85685,352016.0,,nan,
85685,351681.0,90401.0,Thank you so much for your valuable response :),320195.0
85685,352039.0,,nan,
85685,352567.0,,nan,
86505,356518.0,,nan,
86505,357814.0,,nan,
86505,364835.0,,nan,
86505,357868.0,,nan,
86505,364834.0,92446.0,Thanks a lot.I am now able to,308638.0
86505,364834.0,92443.0,"Hello Manasa, Are you getting any error?? You need to start jupyter notebook via anaconda navigator. You need to launch notebook from anaconda.",320195.0
86505,358383.0,,nan,
85586,351123.0,,nan,
85586,351130.0,,nan,
85586,351374.0,,nan,
85586,351774.0,,nan,
85613,351367.0,,nan,
87365,360322.0,,nan,
87365,360369.0,,nan,
87365,361582.0,,nan,
87365,363999.0,,nan,
90443,377189.0,,nan,
90443,377191.0,,nan,
87357,360788.0,,nan,
87357,360318.0,,nan,
87376,360816.0,,nan,
87376,361461.0,,nan,
85634,351391.0,90317.0,Thank you Shankar :),318370.0
85634,351391.0,90425.0,Thanks Abhinay for sharing,303227.0
85634,351391.0,90384.0,"In addition to the answer provided by Shankar. The number in all the different code cells is different even though we executed that cell once or twice. Then after researching about it a little bit, it came clear that the number signifies the number of time the execution has happened in the notebook not on the code cell.",318370.0
85634,352002.0,,nan,
85634,352019.0,90457.0,Thanks Rajat. Clear all O/P will definitely be a handy tool,318370.0
88840,370491.0,,nan,
88840,368119.0,,nan,
88840,368117.0,,nan,
88840,368115.0,,nan,
88840,368128.0,,nan,
88840,368134.0,,nan,
88840,368172.0,,nan,
85747,353045.0,,nan,
85747,352383.0,,nan,
85747,370245.0,,nan,
86064,352653.0,,nan,
86064,352938.0,,nan,
93773,395253.0,,nan,
93773,395255.0,98939.0,type(),318458.0
93773,395255.0,98940.0,As python is case-sensitive.,318458.0
93773,395255.0,98941.0,No type() is not recognized by dataframe or Series,301108.0
93773,395255.0,98942.0,it should... see i tried below. It works. >> type(companies) pandas.core.frame.DataFrame,318458.0
93773,395255.0,98944.0,Oh got it .Thanks . I tried something like dataframe.type() thats why i got the error,301108.0
93773,395255.0,98943.0,Did you check isinstance which i posted in my other comment,318458.0
93773,395258.0,,nan,
93773,395283.0,,nan,
93773,395277.0,,nan,
88903,369663.0,,nan,
88903,368528.0,93266.0,I have already used that,310611.0
88903,368494.0,93261.0,no actually the expected outcome is coming but the solution output is not coming due to which I can't see any error in my code as the code gets successful run but still the test cases are getting failed without showing the solution output.,310611.0
88903,368494.0,93262.0,Can you please share the code where your final submission is rejecting. Do not share graded questions code. Yes Expected outcome will come for every code. It should match with your solution output. If it is not matching then your test case will get fail.,320195.0
88903,368494.0,93264.0,"a = int(input()) b = int(input()) def fizz_buzz(n): if n%3==0 & n%5==0: return 'fizz_buzz' elif n%3==0: return 'fizz' elif n%5==0: return 'buzz' else: return n for n in range(1,10): print(fizz_buzz(n))",310611.0
88903,368494.0,93482.0,"Please use below code: for n in range(a,b+1): if n % 3 == 0 and n % 5 == 0: print(""FizzBuzz"") elif n % 3 == 0: print(""Fizz"") elif n % 5 == 0: print(""Buzz"") else: print(n)",320195.0
88903,368513.0,,nan,
88903,368548.0,93477.0,"Description Write a program that prints the numbers from a given 'a' to 'b', but for multiples of '3' print ""Fizz"" instead of the number and for the multiples of '5' print ""Buzz"". And for multiples of both 3 and 5, print “FizzBuzz” Input Fomat: Line 1: The starting point of a range of values, i.e. 'a'. Line 2: The ending point of a range of values, i.e. 'b'. Output Format: Print all the numbers in the given range of values with ""Fizz"", ""Buzz"" and ""FizzBuzz"" printed at appropriate places. Example: Input 1: 10 15 Output 1: Buzz 11 Fizz 13 14 FizzBuzz Explanation: Since, 10 is a multiple of 5, ""Buzz"" is printed instead of 10. 11 isn't a multiple of either and hence, it is printed as it is. 12 is a multiple of 3, so ""Fizz"" is printed in the place of 12. 13 and 14 are multiples of neither. Then, 15 is a multiple of both 3 and 5, and hence, we print ""FizzBuzz"" in its place. Execution Time Limit",320195.0
88903,369651.0,93478.0,"It is working fine now. for n in range(10,16): if n % 3 == 0 and n % 5 == 0: print (""FizzBuzz"") elif n % 3 == 0: print(""Fizz"") elif n % 5 == 0: print(""Buzz"") else: print(n)",320195.0
88903,369667.0,93479.0,Thank you. I got this logic.,320195.0
86063,352639.0,,nan,
86063,352863.0,,nan,
86063,354075.0,,nan,
90370,376801.0,95167.0,"so it open only in IE browser, or chrome browser?",318083.0
90370,377582.0,,nan,
90370,388628.0,,nan,
90370,377847.0,,nan,
89760,,,nan,
86159,353508.0,,nan,
86159,354081.0,,nan,
84087,345975.0,,nan,
84087,346071.0,,nan,
84087,346011.0,,nan,
86155,353407.0,,nan,
86155,353479.0,,nan,
86155,355598.0,,nan,
88951,368741.0,93311.0,Thanks for the link..,309451.0
88951,368804.0,,nan,
86168,353764.0,,nan,
86168,354184.0,,nan,
86168,354614.0,,nan,
86168,354112.0,90922.0,"Thanks :) . I have also installed IDLE IDE, that is working fine",311227.0
86168,358184.0,,nan,
86168,368892.0,,nan,
88957,368789.0,,nan,
88957,368832.0,93320.0,Then it is weird behaviour. No body is facing this types of issue. You can go for download the video and complete your module.Happy Learning 😊,320195.0
88957,368958.0,,nan,
86223,356124.0,,nan,
85373,350336.0,,nan,
85373,350545.0,90879.0,#Custome code 003 starts with letter 0 i.e from 3rd string position ands ends at 13 so select [11:14],317240.0
85373,352028.0,,nan,
85373,351147.0,,nan,
88961,368739.0,,nan,
88961,368740.0,,nan,
88961,368757.0,,nan,
88961,368901.0,,nan,
88961,368761.0,,nan,
83864,345392.0,,nan,
83864,345456.0,,nan,
83864,346411.0,,nan,
86229,355795.0,,nan,
86229,356029.0,,nan,
86229,356397.0,90978.0,Did you try in some other browser?,305845.0
86229,356397.0,91342.0,Parul Thanks . It does work with Chrome.,314197.0
86229,356397.0,91384.0,Happy to help Vikas :),305845.0
86235,354374.0,90953.0,Thank you...Helped your explanation,318846.0
84113,346042.0,,nan,
86352,355614.0,,nan,
86352,355629.0,90815.0,Thank you prasad,316926.0
90244,375966.0,,nan,
90244,375958.0,,nan,
90244,376020.0,,nan,
90244,375962.0,,nan,
86342,355370.0,,nan,
86342,357023.0,,nan,
86342,359416.0,,nan,
86279,354609.0,,nan,
86279,355268.0,,nan,
86355,355635.0,90821.0,it's not working in coding section provided in upgrad..,320103.0
86355,355635.0,90816.0,Please ignore the above comments. The code which was given by you is executing fine on Python IDE.,311502.0
86355,355928.0,,nan,
86355,355643.0,90822.0,sure..thanks,320103.0
86355,358388.0,,nan,
89976,374661.0,,nan,
89976,374844.0,94749.0,"alright, thanks Rajat, I'll check it.",315423.0
89976,375050.0,,nan,
89969,374652.0,,nan,
89969,374663.0,94667.0,Thanks for reply..will try,317156.0
89969,374698.0,,nan,
87194,359211.0,91577.0,this is something far in the course I think. Thank you for the answer.,318851.0
87194,359211.0,91580.0,"Try this: from nltk import tokenize p = ""Hello Mr. Scott, today I am not well. So kindly inform Mr. Peter that I won't be able to come today."" tokenize.sent_tokenize(p) output: ['Hello Mr. Scott, today I am not well.', ""So kindly inform Mr. Peter that I won't be able to come today.""] The quotes are working both ways.(tried double quoted instead of single). The doubt can only be cleared later in the course, I guess.",317418.0
87194,359211.0,91581.0,"Yes, i am new for python so I will get this later in course. I am working with R for past 2 years.",318851.0
87194,359211.0,91583.0,"Ok, in that case we will have lot to learn here.",318851.0
87194,359211.0,91582.0,"No worries mate, even I am new to both R and Python. :D.",317418.0
85153,349008.0,89862.0,"thanks, but I'm really at the very start of Python.. and haven't came across split funtion..",316349.0
85153,349175.0,,nan,
85153,349183.0,,nan,
85153,349394.0,,nan,
85153,349546.0,,nan,
85153,349553.0,,nan,
85153,350117.0,,nan,
87425,360997.0,,nan,
87425,361003.0,,nan,
87425,361579.0,,nan,
87429,361455.0,91869.0,Yes but on mobile it's too small to watch. Not easy to see the figures. Is it possible to copy those videos from mobile to laptop. Is that possible in the Mac os ecosystem?,302734.0
87429,361455.0,92439.0,With out internet the app does not work properly. How do i watch even if i have the downloaded videos. Is there any way to see those videos on laptop. Even if i managed to see them on Mobile its tiny and for that miss out a lot of things for comprehension.,302734.0
87429,361560.0,91879.0,I will try that. thanks,302734.0
87429,364828.0,,nan,
87429,364820.0,,nan,
87428,361128.0,92102.0,"That's right. Also, refer to the link below, https://docs.python.org/3/library/functools.html?highlight=reduce#functools.reduce as specified by Vinay.",319721.0
87428,361203.0,,nan,
87428,363916.0,,nan,
89103,371067.0,,nan,
89107,369411.0,,nan,
89107,369412.0,,nan,
89107,369421.0,,nan,
89107,369433.0,,nan,
87281,359787.0,,nan,
87281,359836.0,,nan,
87281,359849.0,,nan,
87281,359863.0,,nan,
87281,359753.0,,nan,
87281,363919.0,,nan,
87418,360957.0,,nan,
87418,360946.0,91821.0,thanks it workef,318791.0
87418,361557.0,,nan,
87418,361775.0,,nan,
87418,362752.0,,nan,
87493,361848.0,91913.0,"Thanks Vinay for the response. Since it is memory address in CPython...in my example I have two different lists (Da_Languages & new_list) having same elements. But still id is same for both the lists. Ideally i am thinking since its memory address we should have different id's both lists,right?",312623.0
87493,361848.0,91916.0,"Please note that, the id() function returns the ""Identity"" of the Object and not of the variable name. In python, variable names are just like references or labels that point to a particular object. Check out the below link which tries to explain the concept. https://realpython.com/python-variables/ Looks like you had done something like: DA_languages = new_list beacuse of which both the variable names are referencing the same object. Hope this helps.",313826.0
87579,362908.0,,nan,
87579,363912.0,,nan,
87579,362997.0,92314.0,Thanks . Please let me know if my understanding is correct: For Stack - adding of item will happen from the end and removal of item will also happen from the end. For Queue - Adding of item will happen from the end and removal of item will happen from the front(beginning of list),311119.0
87579,362974.0,92247.0,"That is the right way to implement stack and queue. In addition to this, you can refer to the link below too. https://www.geeksforgeeks.org/using-list-stack-queues-python/",319721.0
89788,373084.0,,nan,
89788,373944.0,,nan,
89788,373068.0,94354.0,The lambda function is multiplying by 2. Need to use ** for exponentiation.,313826.0
89788,373068.0,94414.0,thanks vinay as you said i am tying ** so that only i am getting error every time...,306242.0
89788,373068.0,94433.0,what is the error u are getting,317822.0
89788,373088.0,,nan,
89788,373178.0,,nan,
89788,374860.0,,nan,
89788,375773.0,,nan,
89789,373085.0,,nan,
89789,373083.0,,nan,
87648,363800.0,,nan,
87648,363866.0,,nan,
87648,363898.0,92142.0,"Yes pranesh, it has. I have attested the code. Please let me know.",318347.0
87648,363909.0,92144.0,"Yea, it's as I said. first_name, second_name and customer_code are not set to variables, but to ouputs of print() statements. Instead of setting first_name = print(input_str[6:10]), set first_name = input_str[6:10] and do the same for the other vars too. And, as I recall this exercise requires the use of the split() method. So, use that to get the answer or your solutions will be rejected during the final submission.",306733.0
87648,364023.0,,nan,
87648,364058.0,,nan,
87648,364379.0,,nan,
87648,364340.0,,nan,
87676,363959.0,,nan,
87676,364293.0,,nan,
89816,373246.0,,nan,
89816,373270.0,,nan,
89816,373298.0,,nan,
89816,373309.0,94407.0,"thanks for your explanation ,",305804.0
87743,364275.0,,nan,
87747,364280.0,,nan,
89268,371095.0,,nan,
89268,383795.0,,nan,
86101,352949.0,,nan,
86101,352959.0,,nan,
86101,352984.0,,nan,
86101,354063.0,90673.0,a complete answer....thanks. :),319898.0
86101,354220.0,90726.0,Test cases are basically inputs. if you click on the details after submission u will get the info required. I have attached a screenshot for testcase2 as an answer below.,319898.0
86101,354583.0,,nan,
86101,354617.0,,nan,
88236,364963.0,,nan,
88236,365089.0,,nan,
88234,364946.0,,nan,
88234,365522.0,92601.0,"Nevermind, it appeared briefly before it noted a connection issue. So will need to work on my network quality (connected via ethernet and it appeared fine).",315022.0
88234,365577.0,,nan,
90799,378499.0,,nan,
90799,378409.0,,nan,
89370,370698.0,,nan,
89370,370684.0,,nan,
89370,370676.0,,nan,
89381,370746.0,,nan,
89381,370741.0,,nan,
89381,370745.0,,nan,
89381,370752.0,,nan,
89381,370742.0,,nan,
89397,370865.0,,nan,
89397,370827.0,,nan,
87404,360759.0,,nan,
87404,360718.0,,nan,
87404,360716.0,91807.0,Thank Vinay! I corrected myself and submitted the code. It worked this time! Thanks for the link as well. I read through and realized what was wrong.,300699.0
88288,365376.0,92636.0,thanks ira really helpful,300687.0
88288,365361.0,,nan,
90036,374907.0,,nan,
90036,374946.0,,nan,
90036,374915.0,,nan,
90036,388630.0,,nan,
90036,376293.0,,nan,
87311,360075.0,,nan,
87311,360486.0,,nan,
87311,359899.0,91737.0,i was showing that by default end='\n' .,320073.0
87311,359899.0,91734.0,Thanks Pratap. But steps 1and 3 gives same output. So do we reaaly need to use \n character ...without using that also we get same output..,312623.0
87311,364818.0,,nan,
89715,372638.0,,nan,
89715,372721.0,,nan,
89425,371027.0,,nan,
89425,370967.0,,nan,
89425,370961.0,,nan,
89425,371055.0,93862.0,"list1.sort() does a in-place sorting and returns ""None"". Hence, assigning list2 = list1.sort() would mean that the variable list2 will be assigned a value 'None' and hence should not be used.",313826.0
89425,371055.0,93868.0,"Yes it is a mistake, Not able to edit it now,",317689.0
89425,371069.0,,nan,
89425,372554.0,,nan,
91622,383627.0,,nan,
91622,385224.0,,nan,
90420,377045.0,,nan,
90420,377039.0,,nan,
90420,377070.0,,nan,
94815,400128.0,99940.0,But when I execute this code I get error 'dict' object is not callable,314629.0
94815,400145.0,,nan,
94815,400155.0,99957.0,"ok thanks , i'll check once again",314629.0
94815,400521.0,,nan,
90459,377293.0,,nan,
90459,377306.0,,nan,
90459,377324.0,,nan,
91743,384453.0,,nan,
91743,384059.0,,nan,
90540,377656.0,,nan,
90540,377655.0,95188.0,no i think can you share me the ques i posted,310008.0
90540,379116.0,,nan,
90540,377662.0,95192.0,"df_1 = df.groupby(['month','day','rain','wind']) print(df_1.head(20))",310008.0
90540,377662.0,95193.0,output error,310008.0
90540,377662.0,95195.0,"As mentioned in the question you have to groupby = 'month' and 'day' only. And then you have to find mean of 'rain' and 'wind' column. So you have to do like this: df = df.groupby(['month','day']) df_1 = df['rain','wind'].mean() print(df_1.head(20))",317991.0
90540,377662.0,95196.0,Hope this will solve your problem.,317991.0
90540,377662.0,95206.0,"while using pivot table df=df.pivot_table(['month','day']) df_1 =df['rain','wind'].mean() is this correct",310008.0
90540,377662.0,95204.0,"df = df.groupby(['month','day']) will change the dataframe df to a DataFrameGroupBy Object. Although, in this case it is of no consequence, but it is advised not to follow this practice as you would lose the original dataframe.",313826.0
90590,377895.0,,nan,
90590,377896.0,,nan,
90590,377909.0,,nan,
88923,368586.0,,nan,
88923,368631.0,,nan,
88923,368589.0,,nan,
88923,368782.0,,nan,
88923,368582.0,,nan,
93918,395940.0,,nan,
93918,395952.0,,nan,
95116,401934.0,,nan,
95116,402006.0,,nan,
95116,402733.0,,nan,
95116,402055.0,,nan,
95116,402051.0,,nan,
102729,439318.0,,nan,
102729,439899.0,,nan,
102729,439662.0,,nan,
102729,440491.0,106682.0,Thanks and that is interesting,301121.0
102729,445198.0,,nan,
142184,614361.0,,nan,
142184,614124.0,,nan,
81422,331649.0,,nan,
81422,331801.0,,nan,
78093,313263.0,82567.0,thanx a lot @Prakash,305847.0
78093,323176.0,,nan,
78093,332865.0,89926.0,Learn to Program: https://www.youtube.com/playlist?list=PLGLfVvz_LVvTn3cK5e6LjhgGiSeVlIRwt,310217.0
78093,332865.0,88561.0,it will be better if you share the links here.,315560.0
78093,365832.0,,nan,
80628,327690.0,,nan,
80628,327694.0,,nan,
80628,327812.0,85288.0,"Thanks, as I am completely new to all this I had started with R and had planned on continuing to Python. But now I focusing on understanding python first and look at R later on. 👍",302734.0
80628,328099.0,85467.0,Thanks I hope I can manage time to cover up R as well.,302734.0
80628,328099.0,85962.0,Ok 👍,302734.0
79885,323470.0,,nan,
79885,326132.0,,nan,
80591,328201.0,85345.0,Data quest is a website or video collection? Would wanna try after I complete Upgrad,308437.0
80591,328201.0,85376.0,Of course go for it..!!,305847.0
80591,327287.0,85196.0,You're welcome :),306733.0
80591,327289.0,,nan,
80591,327370.0,,nan,
80591,327675.0,,nan,
80591,327281.0,,nan,
80591,328152.0,,nan,
80591,334925.0,,nan,
80591,336258.0,,nan,
83573,343382.0,,nan,
83573,343428.0,,nan,
83573,343700.0,,nan,
83573,343577.0,,nan,
83573,345705.0,,nan,
86433,358057.0,,nan,
86433,358519.0,,nan,
83572,343360.0,88579.0,are u asking regarding sciPy and numPy packages?,310585.0
83572,343367.0,,nan,
83572,343368.0,,nan,
83572,343459.0,,nan,
83572,343697.0,,nan,
83572,343701.0,,nan,
83572,344223.0,,nan,
87255,,,nan,
129603,,,nan,
79317,320406.0,,nan,
79317,320384.0,,nan,
79317,320370.0,,nan,
79317,322604.0,,nan,
87117,358741.0,,nan,
87117,358750.0,,nan,
87117,358914.0,,nan,
87117,358925.0,91496.0,Thanks Ranjith. I will try this way too,318335.0
87117,358730.0,91464.0,Ok Bhanu. THanks. Solved it using the list,318335.0
87117,358965.0,,nan,
87117,359592.0,,nan,
87117,359749.0,,nan,
87117,361567.0,,nan,
90406,376993.0,95095.0,Thanks Vinay for correcting the code,316084.0
90406,377015.0,,nan,
90406,377081.0,,nan,
81930,337079.0,,nan,
81930,334059.0,88936.0,Well explained!,310585.0
81930,334059.0,90484.0,Thanks Amit for patiently explaining.,318370.0
81329,331208.0,,nan,
81329,331220.0,,nan,
81329,334522.0,,nan,
81329,341994.0,,nan,
83913,345615.0,,nan,
83913,345755.0,,nan,
83913,346394.0,,nan,
83913,346353.0,,nan,
83913,345981.0,,nan,
83913,346404.0,,nan,
83913,349069.0,89870.0,Oh yeah... Thanks...Noticed after posting the query...,318479.0
83913,349069.0,89869.0,"You have missed to put a : at the end of ""if"" condition",313826.0
83913,353686.0,,nan,
83913,354304.0,91341.0,"import ast,sys input_str = sys.stdin.read() if input_str[0] in ['A','E','I','O','U','a','e','i','o','u']: print(""YES"") else: print(""NO"")#Write your code here #Use capital YES or NO",320195.0
83913,356337.0,,nan,
83913,359368.0,,nan,
84705,347228.0,,nan,
84705,349810.0,,nan,
84705,351639.0,,nan,
84175,346341.0,,nan,
84175,346344.0,,nan,
84175,346303.0,89159.0,(n+1)***,310585.0
84175,346591.0,,nan,
84175,346621.0,,nan,
84175,346864.0,,nan,
84175,348524.0,,nan,
84264,346622.0,,nan,
84264,346928.0,89328.0,"Ya checked both of them, will see when it happens again. Thanks.",315028.0
84797,347395.0,,nan,
84797,347789.0,,nan,
81458,331920.0,86206.0,Super example involving all data structures 🙏🏻,308437.0
81458,331920.0,86214.0,"No specific reason for the conversion, just to show that map can be used with any iterable :) Tupe to set conversion & vice-versa is allowed. This is shown in the examples in python module.",310974.0
81458,331920.0,86208.0,But in dis example u have converted map object into tuple and then into set ? Is it possible? What is the reason to do this twin conversion?,308437.0
81477,332316.0,,nan,
81477,331891.0,,nan,
81477,331935.0,86186.0,It worked Thanks!,303968.0
81477,332068.0,,nan,
81477,337076.0,,nan,
81477,351482.0,,nan,
81477,353689.0,,nan,
84713,347327.0,,nan,
84713,347104.0,,nan,
84713,347106.0,,nan,
84713,347174.0,,nan,
84713,347111.0,89353.0,Thanks.,310585.0
84713,347111.0,89567.0,This is the most pythonic way to solve this problem.Neat and clear !!,318756.0
84713,347107.0,,nan,
84713,347942.0,,nan,
84713,347364.0,,nan,
84713,348054.0,,nan,
84713,348959.0,,nan,
82111,337495.0,,nan,
82111,335347.0,87036.0,I am saying if a=b is not true then he knows he has to execute else but which else?we hv else in nested statement as well y didn't compiler executed that?,300721.0
82111,335403.0,87056.0,My question was different.,300721.0
82111,335424.0,87052.0,"Yes , you are correct.It takes indentation into account.",307830.0
82111,335424.0,87061.0,yes identation plays a major role here,301648.0
82111,335424.0,87058.0,Thanks for confirming.,300721.0
82111,336050.0,,nan,
82111,336627.0,87191.0,"Absolutely, its the indentation which defines the precedence of if else statement.",300734.0
82111,336633.0,,nan,
82111,337628.0,,nan,
82111,356347.0,,nan,
88506,366204.0,,nan,
88506,366207.0,,nan,
88506,366212.0,,nan,
88506,366217.0,92792.0,"False also have value 0, thats why code works, else it will throw erroe.",318319.0
88506,367001.0,92951.0,thank you for the confirmation.,317998.0
88506,367001.0,92949.0,"correct, return value should be there in lanbda, else will get error.",318319.0
88506,367149.0,,nan,
88506,367131.0,,nan,
88506,366206.0,,nan,
84893,347804.0,89917.0,"As per my understanding upper() is not necessary since only ""S"" is specified not ""s"" or ""S""",311857.0
84893,347807.0,89507.0,I was facing this issue of using if-else in lambda. Thanks Rahul!,310585.0
84893,347814.0,90717.0,code not working.,318429.0
84893,347814.0,90114.0,"How doe the sum function work here?, the x[0] == ""S"" should return true or false. How does sum work on boolean values?",303084.0
84893,347814.0,90728.0,"@Rajat, The code is working for me >>> input_list = ['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] >>> count = sum(map(lambda x : x[0] == ""S"",input_list)) >>> print(count) 3",313826.0
84893,347814.0,90727.0,"@Nikhil, internally the boolean value TRUE is represented by numeric 1 and FALSE by numeric 0. Here sum() is just adding 1's and 0's returned by the map function. Hope this answers your query.",313826.0
84893,347865.0,,nan,
84893,347945.0,90113.0,This does not work. Hope you tried it,303084.0
84893,347945.0,90175.0,"Sorry.Instead of map,filter would be used here.",300690.0
84893,347945.0,90720.0,filter also doesn't work in your code.,318429.0
84893,347945.0,90793.0,upper would be replaced by upper().see the below attachment.,300690.0
84893,355326.0,,nan,
81652,332791.0,,nan,
81652,332700.0,86396.0,yeah that will also do,301648.0
81652,332700.0,86395.0,"print(sum(count)) will do the job. print(count) only will display a list of True, False values.",301652.0
81688,332839.0,,nan,
81688,332842.0,,nan,
81688,333850.0,86746.0,If you are trying in shell then add the following line as the first line: from functools import reduce,300748.0
82450,338040.0,,nan,
82450,338063.0,,nan,
82450,338022.0,88284.0,What If I am suppose to display the count of such words from the given list? Cuase I have tried Print(len(sp)) in the next line which gives error.,300721.0
82450,338022.0,88607.0,"It will not give any error see below , also pls post input_list = ['hdjk', 'salsap', 'sherpa'] sp = list(filter(lambda x: x[0]=='s' and x[len(x)-1]=='p',input_list)) print(sp) len(sp) ++++++++++ and if you want to use in upgrade console just you can use below where console ask you to write code sp = list(filter(lambda x: x[0]=='s' and x[len(x)-1]=='p',input_list)) print(sp) len(sp)",307843.0
82450,338277.0,,nan,
82450,338029.0,87546.0,can u plz explain the difference between 'and' and '&' or give any reference link?,300690.0
82450,338029.0,87552.0,Sure... https://www.quora.com/What-is-the-main-difference-between-a-logical-operator-and-a-bitwise-operator,301652.0
82450,338029.0,87553.0,"Notice... This works -> sp = list(filter(lambda x:(x[0] == 's') & (x[-1] == 'p'), input_list)) This doesn't -> sp = list(filter(lambda x:x[0] == 's' & x[-1] == 'p', input_list))",301652.0
82450,338493.0,,nan,
82450,338837.0,,nan,
82450,342077.0,,nan,
82450,349365.0,,nan,
82450,351619.0,,nan,
82450,354420.0,,nan,
82450,359875.0,,nan,
82450,365453.0,,nan,
79614,322599.0,,nan,
79614,322500.0,84404.0,"Please declare any value nd assign this value in range(1,5). What you get output share me snap.. I am not getting square value that's why I asked.",305847.0
79614,322455.0,84402.0,No,305847.0
79614,322455.0,84403.0,"Hi Ashish, When I apply same in my code.Then output is getting different in square case only n**2 is applying. That's why I asked.",305847.0
79614,322673.0,84562.0,As I can see in your code there is use of symbol(**) but in same code when I use symbol(^) I get the different output. I know the meaning of ^ is square but in syntax output is reflecting different.,305847.0
79614,322673.0,84586.0,"The confusion is probably due to the language you are referring to (R/Python). I have added an answer. May be it helps you get better understanding. If not, please let me know.",301561.0
79614,322673.0,84582.0,"prashant in english format in question they have written n^2 which means n square in solution you have to use ** instead of ^ , because ^ represent xor bitwise in python x^u means int for u only for example 9^ 2 is 11 so this is completely different from ** . Hope now its clear",305838.0
79614,323197.0,,nan,
79614,323625.0,84677.0,Thanx a lot @Sudhir..got it.,305847.0
81961,337096.0,,nan,
81961,334449.0,,nan,
81961,334452.0,,nan,
81961,334517.0,,nan,
81961,334747.0,,nan,
88541,366397.0,,nan,
87250,359616.0,91644.0,Python is a dynamically typed language. which means the type of the value assigned to variable when it is first encountered during running of the code is considered as the type of variable. the first time x is mentioned in the code is x=1 which is an interger. You can also think of it as certain amount of intelligence in python The type check occurs at runtime. so if u should mention somewhere before the for loop that x=[] or x= text or any other type then u will get an error message from python saying Type mismatch.,319898.0
87250,359616.0,91642.0,"But this is the first time I have used x, how come python knows that it’s value should be an integer?",318335.0
87250,361585.0,,nan,
79999,337157.0,,nan,
79999,324051.0,84720.0,thanks a lot. :),302738.0
79999,325473.0,85180.0,Thanks a lot.:),302738.0
79999,325473.0,86016.0,"Good one, very interesting",310974.0
79999,325473.0,88866.0,"Since x is a string you can directly call the starts with function - z=sum((list(map(lambda x:x.startswith('S'),input_list))))",311857.0
79999,331251.0,,nan,
79999,323725.0,84719.0,thanks a lot. :),302738.0
79999,323725.0,86017.0,Good one,310974.0
79999,323725.0,89371.0,"the sum, yes. I was stuck here!",312376.0
79999,323723.0,84718.0,thanks a lot. :),302738.0
88576,366639.0,93127.0,"def sqr(num): out = num**2 print(out) sqr_4= sqr(4) print(""sqr_4 is""+str(sqr_4)) ​ ​ 16 sqr_4 isNone 16 sqr_4 isNone i have executed the code but i am receiving the solution as mentioned below 16 sqr_4 isNone",307496.0
88576,366639.0,93128.0,"hi, pls ignore previous comment as i have also executed the code but i am receiving the solution as mentioned below 16 sqr_4 isNone",307496.0
88576,366639.0,93176.0,"16 is printed due to the print(out) command mentioned within the function of sqr(). sqr_4 isNone , is printing as None as there is not return mentioned with the the function; sqr() Hope this helps, let me know if you still facing any difficulties",312259.0
88576,366730.0,,nan,
83314,343099.0,,nan,
83314,342138.0,88303.0,Yes i didn't include capital. Your code is correct and as per upgrad coding console there is a sample test cases with sample input is given . Here sample input was used as alpha. ay be this is the reason your verification got rejected .,301648.0
83314,342138.0,88302.0,"Thank you for the answer. But I think Capitals are not being included in this code. Also, will it make any difference if I use input_str[0] directly instead of assigning input_str[0] to first? My code worked in jupyter notebook even with 'beta', but it got rejected in the submission. So, I just wanted to know what's the error in my code :)",312507.0
83314,342149.0,88304.0,Thank you :),312507.0
83314,342456.0,88381.0,Don't indent 'else'. Try this: a = 'virat' vowel = 'aeiou' if a[0] in vowel: print('yes') else: print('no') Though this will not work if your string is in capitals,312507.0
83314,342456.0,88395.0,Please check the screenshot I have posted in the answer section.,312507.0
83314,342456.0,88384.0,It is giving error.It is expecting indented block.,300690.0
83314,342464.0,,nan,
83314,342472.0,88403.0,ok.resolved.,300690.0
80436,326259.0,85032.0,Thanks Pranesh Krishnamurthy! I'll try,302742.0
80436,326259.0,85050.0,Glad I could help :),306733.0
80436,326259.0,85034.0,Done! Thanks :),302742.0
80436,326406.0,85035.0,Thank you :),302742.0
80436,326724.0,85073.0,Yes done for both caps and small! thanks,302742.0
80436,326724.0,85142.0,"instead of checking for 10 characters, convert the str[0].toupper() and the use ""in"" keyword. I.e., str[0].toupper() in [AEIOU].. not sure of the syntax but this can be achieved in single line",305845.0
80436,327121.0,,nan,
80436,327226.0,,nan,
80436,329759.0,,nan,
80436,341995.0,90040.0,if input_str[0] in 'aeiou': print('YES') else: print('NO'),318476.0
87640,363760.0,92115.0,I have to add to this that you can take two inputs x and y in your lambda function even though it is unnecessary for your objective but filter has the limitation of accepting only one input. so if you use filter u have to ensure that only one input is passed.,319898.0
87640,363760.0,92349.0,"Sure, Thanks for your help. I was able to resolve the error.",304389.0
87640,363760.0,92260.0,"Hey, Rohit, you can try and implement the suggestions given by Ranhith and also refer to the code given by Amani. I'm sure you'll be able to resolve them.",319721.0
87640,363791.0,92350.0,"Sure, Thanks for your help. I was able to resolve the error.",304389.0
87640,363899.0,92351.0,"Sure, Thanks for your help. I was able to resolve the error.",304389.0
87640,369199.0,,nan,
87595,363145.0,92052.0,Thanks. Issue resolved,307494.0
87595,363150.0,92051.0,Thanks,307494.0
87595,363150.0,92050.0,"yeah, true. Executed successfully",307494.0
80920,,,nan,
89449,371151.0,,nan,
89449,371145.0,,nan,
80922,328892.0,,nan,
80922,329129.0,,nan,
80922,329133.0,,nan,
80922,331291.0,86082.0,better,305655.0
80891,328729.0,,nan,
80891,329131.0,,nan,
80891,331894.0,,nan,
80876,328639.0,,nan,
82367,337755.0,87451.0,"Why is there indentation for print(name)? Remove it, it should work.",310974.0
82367,337755.0,87452.0,because we have to get our output as combined as stored in name..,305129.0
82367,337755.0,90344.0,"use the below code name = list(map(lambda x, y : x +' ' + y,first_name, last_name)) print(name)",318353.0
82367,337754.0,,nan,
82367,337746.0,87446.0,What is the error that you are getting and what is the method you are trying?,310974.0
82367,337746.0,87447.0,the above method which you gave is showing syntax error with print(name),305129.0
82367,337745.0,,nan,
82367,337762.0,,nan,
82367,337763.0,,nan,
82367,337758.0,87450.0,"instead of 11 and 12 i can use x,y right?",305129.0
82367,337758.0,87454.0,"ok got it, i reseted the solution and wrote the same code and it worked.. :)",305129.0
82367,337758.0,87453.0,"yes Abhishek.. you can use x,y in place of l1 and l2",312746.0
82367,338041.0,,nan,
82367,338042.0,,nan,
82367,348619.0,,nan,
82367,338044.0,,nan,
82367,348643.0,,nan,
82367,350414.0,,nan,
82367,351240.0,,nan,
82367,351597.0,,nan,
80894,328725.0,,nan,
80894,329121.0,,nan,
80894,328730.0,,nan,
80894,330439.0,,nan,
80894,331892.0,,nan,
80914,328900.0,,nan,
80914,329128.0,,nan,
80914,329857.0,,nan,
85246,349550.0,,nan,
85246,349614.0,,nan,
85246,350315.0,,nan,
85246,349867.0,,nan,
85246,350386.0,93097.0,Looks exciting. Looking forward to reach this level of coding in Python.,301650.0
85246,351581.0,,nan,
85246,352539.0,,nan,
85246,362988.0,,nan,
85246,365425.0,,nan,
87602,363450.0,,nan,
87602,365229.0,,nan,
87602,402003.0,,nan,
80780,328233.0,85375.0,I know my syntax is wrong because I am getting error as u can see. I am looking for correct syntax to run for set of query.,305847.0
80780,328233.0,85410.0,what is the problem statement?,300748.0
80780,328399.0,85416.0,Awesome! :),306733.0
80780,328399.0,85415.0,Got my answer... Ya it was syntax error..!!,305847.0
80699,328019.0,85332.0,"I wrote the same thing again lol! Yea, follow Ashish' s code",306733.0
80699,328019.0,85330.0,"input(""Enter your word : '')",301652.0
80699,328276.0,85403.0,"Thanks, but still not working. File "" "", line 1 my_word = input(""Enter your word"": "") ^ SyntaxError: invalid syntax",305790.0
80699,328276.0,88925.0,@vipan even after getting the answer you are not paying attention. Take a break bro,315560.0
80699,328276.0,85490.0,"my_word = input(""Enter your word"": "")--------See the single double quote just after word. You have to remove it. The message that you have to display must be enclosed within double quotes. It must be -------input ("" "")",301652.0
86374,355931.0,,nan,
86374,356025.0,,nan,
86374,356074.0,,nan,
86374,356093.0,90958.0,Nice approach,318554.0
85308,349917.0,,nan,
85308,349910.0,,nan,
85308,350829.0,,nan,
85308,359079.0,,nan,
88786,367806.0,,nan,
88786,367811.0,,nan,
88786,367812.0,,nan,
88786,367838.0,,nan,
88786,367847.0,,nan,
88786,367969.0,,nan,
88786,368055.0,,nan,
88786,367976.0,,nan,
83423,342580.0,,nan,
83686,344138.0,,nan,
83686,344176.0,88721.0,Print function in python is inbuilt function means all the definition has been defined and put in library therefore we don't need to explicitly define and the print function does not have return statement that is why when it assigned it does not return any value and hence it returns none,307843.0
83686,344176.0,88718.0,"""definition of print function is that it return none after evaluate"", could u please elaborate on that?",310598.0
86439,356047.0,90864.0,Thanks.,320103.0
83289,342108.0,89394.0,This doesn't use map function,313228.0
83289,342168.0,,nan,
83289,342355.0,88669.0,Converting while applying the functional would be better though. rslt = list(filter(...)),306733.0
83289,342453.0,,nan,
83289,346339.0,,nan,
83289,347198.0,,nan,
83289,347326.0,,nan,
83750,344602.0,,nan,
85364,350334.0,,nan,
85364,353791.0,,nan,
85364,351202.0,,nan,
85737,355192.0,,nan,
85737,352103.0,,nan,
88830,368063.0,,nan,
88830,368057.0,,nan,
88830,368070.0,,nan,
88830,368084.0,,nan,
88830,368192.0,,nan,
88830,368269.0,,nan,
88830,368821.0,,nan,
88830,368997.0,,nan,
88830,369299.0,,nan,
80804,328457.0,85496.0,what is the code to apply lamda function there?,300690.0
80804,328392.0,85422.0,thanks !!,306738.0
80804,328393.0,85423.0,thanks !!,306738.0
80804,329853.0,,nan,
80804,331434.0,,nan,
80804,333676.0,,nan,
87530,362218.0,,nan,
87530,362278.0,,nan,
87530,362237.0,,nan,
85471,350799.0,90427.0,"Hello Denny, your code and mine code is same the how did u get the output. Still I didn't get the output",308639.0
85471,352006.0,,nan,
85471,352027.0,,nan,
81266,330525.0,85936.0,Input ()always takes string as input?? Any other input function in python which takes only numbers (integer/float) as inputs??,308437.0
81266,330525.0,85955.0,"By default the input function takes an input in string format. For other data type you have to cast the user input. Th casting is illustrated in the above example. 2nd Method you can type cast the function when declaring itself eg float(input(""enter the Height"")) these are only two options available.",307843.0
81266,330764.0,,nan,
81266,330765.0,,nan,
81266,330767.0,,nan,
81266,330768.0,85984.0,Even this didn’t work,308437.0
81266,331032.0,86028.0,"I'm not aware of any other function to get an input from the user. Check the docs, you might find something. It might be cumbersome for the programmer. But, when writing programs that other people use, strings can be useful to provide meaningful error messages when the program crashes. And having good error messages is the first step to building better programs.",306733.0
81212,330285.0,,nan,
81212,330032.0,,nan,
81212,330103.0,,nan,
81212,331431.0,,nan,
81212,331884.0,,nan,
85677,351650.0,,nan,
85677,351655.0,,nan,
85677,352479.0,,nan,
85677,354410.0,,nan,
85677,376909.0,,nan,
85648,351673.0,,nan,
85648,352257.0,,nan,
82349,337511.0,,nan,
82349,337600.0,,nan,
82349,337622.0,,nan,
82349,337768.0,87505.0,thanks for the clarification.now its been helpful,300687.0
82349,337768.0,87535.0,welcome :),301652.0
82349,337804.0,,nan,
82349,338054.0,,nan,
82349,349369.0,,nan,
82349,350120.0,,nan,
82349,370986.0,,nan,
86169,,,nan,
86185,353717.0,90618.0,Thanks!,313767.0
86185,353794.0,,nan,
86185,354014.0,,nan,
86185,354065.0,,nan,
86185,358524.0,,nan,
86185,359892.0,,nan,
96664,411778.0,,nan,
96664,411864.0,,nan,
86225,354119.0,90674.0,There's aslo the issue of readability and LOC. It would be bothersome if one had to define a function everytime they wanted to perform a simple operation.,306733.0
86225,354119.0,90685.0,Exactly @pranesh Lambdas are short and sweet and helpful for simpler operations.,318495.0
86225,354119.0,90805.0,Agreed @vinod,318495.0
86225,354119.0,90804.0,"Great explanation Rajat, but I don't agree that lamda's simplicity can replace def in anyway or vice versa. My view points to ponder are as below. - using lambda, we cannot write medium to complex functions having more validations and actions. - def cannot be used as a replacement for lambda in map, filter and reduce constructs.",314730.0
86225,356143.0,,nan,
86226,354115.0,,nan,
86226,354106.0,,nan,
86192,353922.0,,nan,
86192,354076.0,,nan,
86192,354107.0,90667.0,"@Rajat, the question is ""How to write list comprehension for nested for loop with if else condition?"" for which I have provided a sample code.",313826.0
86192,354107.0,90669.0,Thanks and apologies..I guess it's my bad..i read last 2 lines and thought he wants know about the functioning! So i just added explanation to your answer! All credits to you(@vinay) for the sample code.. if that's the issue! :),318495.0
86192,354107.0,90671.0,Cheers to you too.. your answers are great on the forum! Keep up the good work mate! :),318495.0
86192,354107.0,90670.0,Now worries!!Just wanted to keep the focus on the main question. Cheers!!!,313826.0
86192,356104.0,,nan,
86728,357729.0,91217.0,Thank you. I will explore these.,312376.0
86728,358522.0,,nan,
86728,358743.0,,nan,
89936,374416.0,94693.0,Thank u very much :),316084.0
89936,374624.0,,nan,
89936,374419.0,,nan,
89936,375776.0,,nan,
89766,372898.0,,nan,
89766,372993.0,94702.0,"Thanks, it helped :)",320603.0
86360,355679.0,90817.0,Yes you are right !,318427.0
87277,359667.0,91670.0,Ty Ram,318335.0
87436,361190.0,,nan,
87436,361446.0,,nan,
89064,369265.0,93440.0,I'm asking using Ternary operator ? :,320687.0
89064,369265.0,93444.0,"it is the same thing. ""?:"" is just a syntax (used in some languages like c) for doing ternary operations. in python it is "" A if conditon else B"" Refer this for more details: https://stackoverflow.com/questions/394809/does-python-have-a-ternary-conditional-operator",317998.0
89064,369265.0,93483.0,i searched on Stackoverflow but didn't find Thanks Harshit,320687.0
89064,371065.0,93867.0,"Hello Sushmitha, did i miss something in my answer in response to this question? I also, essentially, wrote the same thing in my answer na ? or, have i overlooked something? please if you could clarify, thank you.",317998.0
87385,360504.0,91784.0,Hey Amani thanks a lot for debugging it . I had overlooked that,314313.0
87385,361577.0,,nan,
89101,369402.0,,nan,
89101,369385.0,,nan,
89101,369382.0,,nan,
89101,369370.0,,nan,
89101,369694.0,,nan,
89101,369489.0,,nan,
87678,364247.0,92263.0,"Oh ok, thanks for the code.",318404.0
87678,363975.0,,nan,
87678,363988.0,,nan,
87678,366329.0,,nan,
87678,367146.0,,nan,
87678,370091.0,93612.0,the 4th solution was not explained in the tutorial right?,310509.0
87730,364182.0,,nan,
87730,364252.0,,nan,
89666,372254.0,,nan,
89666,372256.0,,nan,
89666,372263.0,,nan,
89666,372270.0,94209.0,"yeah it helps, there was actually no need of using split. thanks a lot",315464.0
89311,370292.0,,nan,
89311,370303.0,,nan,
89311,370473.0,,nan,
89311,370288.0,,nan,
89311,392443.0,,nan,
90781,378320.0,,nan,
90781,378338.0,,nan,
89812,373220.0,94421.0,"thank you, I just submitted the code, had issues running it in Upgrad Console",319876.0
89812,373221.0,94422.0,"thank you, I just submitted the code, had issues running it in Upgrad Console",319876.0
89812,374287.0,,nan,
88679,367256.0,92989.0,Thank you Ankur. Will definitely share more as I come across. Please upvote the post if you found it interesting.,317998.0
88679,367231.0,,nan,
88679,367234.0,92991.0,Thank you Ashish.,317998.0
88679,367178.0,,nan,
88679,367210.0,,nan,
88679,367399.0,,nan,
88679,368209.0,,nan,
88679,368256.0,,nan,
88679,368267.0,,nan,
88679,368351.0,,nan,
89342,370636.0,,nan,
89342,370548.0,,nan,
89342,370470.0,,nan,
89342,370450.0,,nan,
89342,370454.0,,nan,
89342,370467.0,,nan,
89342,370793.0,,nan,
89342,371079.0,,nan,
89342,379264.0,,nan,
88238,365116.0,,nan,
88238,364962.0,,nan,
88238,365004.0,,nan,
88238,365188.0,,nan,
86217,354078.0,,nan,
86217,354083.0,,nan,
86217,355806.0,,nan,
86217,354116.0,,nan,
89398,370830.0,,nan,
89398,370841.0,,nan,
89398,370859.0,,nan,
89398,370869.0,,nan,
89398,370873.0,,nan,
89398,371022.0,,nan,
96566,411545.0,,nan,
88320,365478.0,92592.0,"later i figure it out and executes successfully, thanks for replay",305804.0
88320,365477.0,,nan,
88320,369197.0,,nan,
89433,371046.0,,nan,
89433,371047.0,,nan,
89433,371048.0,,nan,
89433,371368.0,,nan,
89433,371362.0,,nan,
89304,370282.0,,nan,
89304,370279.0,,nan,
89295,370248.0,,nan,
89295,370249.0,,nan,
89295,370290.0,,nan,
89295,370257.0,,nan,
89295,370442.0,,nan,
89295,370477.0,,nan,
91229,381236.0,95853.0,got it thanx,317558.0
91229,381244.0,95854.0,got it thanx,317558.0
91229,381234.0,95851.0,got it thankyou,317558.0
91229,381579.0,,nan,
91239,381277.0,,nan,
91239,381256.0,,nan,
91239,381439.0,,nan,
91239,382013.0,96024.0,"Jupyter notebook does automatic indentation using a tab wherever necessary. In .ipynb file, ensure that you are writing code in a ""code cell"".",313826.0
91239,382013.0,96050.0,Jupyter notebooks do support automatic indentation.,319302.0
91239,382343.0,,nan,
91239,382318.0,,nan,
90738,378162.0,,nan,
90738,378166.0,,nan,
90738,378173.0,,nan,
90738,378342.0,,nan,
90738,378313.0,,nan,
95134,402014.0,100410.0,"glad, you found it by yourself.",318458.0
95135,402041.0,,nan,
95135,402011.0,100397.0,"Thanks Manish, By mistake I typed flower brackets instead of square braces",314629.0
95197,402317.0,,nan,
95197,402386.0,,nan,
95197,402539.0,,nan,
95197,402280.0,100468.0,Thanks Siva,314629.0
95197,402843.0,,nan,
95197,403182.0,,nan,
95197,404621.0,,nan,
112616,,,nan,
112413,,,nan,
112334,,,nan,
111211,,,nan,
111402,485327.0,114550.0,eda will give fair idea,317982.0
111402,485327.0,114549.0,Do we have to apply any Machine Learning Concept such as logistic Or We can predict the team based on EDA analysis. ? Which approach to follow and idea,306243.0
111402,485356.0,,nan,
111402,479818.0,,nan,
111402,479855.0,,nan,
97057,414357.0,,nan,
97057,414362.0,,nan,
95506,403966.0,100797.0,No that's not the case. Check my response and see if it helps you :),308962.0
95506,403966.0,100880.0,"Hi Utkarsh, I have not used %matplotlib and I can see the plot being populated without mentioning plt.show()",311160.0
95506,404007.0,100838.0,Utkarsh...If you check again then my question is the exact opposite. I did not had to use plt.plot() or %matplotlib inline for getting the plot displayed. Please refer the code in the screenshot I have attached.,318479.0
95506,404270.0,100876.0,"Hi Arihant, I didnt get it. How is your answer relevant to my question ?",318479.0
95506,404270.0,100885.0,"This is default behavior of jupyter notebook to show the result of last statement in a cell. So, no need to call plt.show() explicitly if you are using jupyter and your last statement is plt.plot(x,y). But if you are writing a python script and running that script directy plt.show() is required to plot.",317689.0
95506,404270.0,100929.0,"I am not sure if it is default behavior of jupyter notebook. I cannot find any references online for it. Also if you have gone through the lecture video in the professor's demonstration, plt.plot(x,y) resulted in the plot object. He had to use plt.show() to explicitly display the plot. Agreed that plt.show() is required for python scripts written in powershell as I have tried it there.",318479.0
95506,405658.0,,nan,
95506,405763.0,101268.0,"I checked on windows powershell, plt.show() command is required to display the plot. Plot is opened in a new window.",318479.0
95522,404021.0,,nan,
95522,404052.0,,nan,
95522,405650.0,,nan,
96142,408882.0,101619.0,"I had the same view that Box plot is right plot. But the supposedly correct answer to the question is ""scatter"" plot. There lies my confusion.",318762.0
96142,408882.0,101622.0,I first chose Box plot and then chose Histogram. Both turned out to be wrong. It looks like a mistake to me. Would like a TA to clarify.,318762.0
96142,408882.0,101620.0,Even i chose Box plot in first attempt and It was wrong answer for me. Then I chose scatter plot. But I dont know what would be second variable.. Should we report it as mistake?,318458.0
96142,408827.0,,nan,
96142,408839.0,,nan,
96142,408843.0,101618.0,You can use a dummy variable and then create a scatter plot. That would suffice your requirement.,318340.0
96142,409016.0,,nan,
95548,404238.0,,nan,
95929,406679.0,,nan,
95929,406992.0,,nan,
95929,407320.0,,nan,
95929,407449.0,,nan,
95569,404405.0,,nan,
95569,404479.0,,nan,
95569,404939.0,,nan,
95591,404567.0,,nan,
95591,404672.0,,nan,
95593,404563.0,,nan,
95593,404572.0,,nan,
95593,404577.0,,nan,
95593,404889.0,,nan,
95593,404688.0,,nan,
95599,404556.0,101063.0,Thanks. I had not initialised y.,311119.0
95599,404562.0,101065.0,"I could tell it looking at the error. Although your code looks fine. It could have been possible that while playing around with your code you somehow altered the variables by re-running cells either at the top or bottom. Try a fresh run, it should work.",306248.0
95599,404562.0,101060.0,"Could u please elaborate how x has 30 items .My understanding is : I have mentioned x = linspace(0,10,20) . It will return 20 items only.",311119.0
95610,404633.0,100930.0,"The last argument is the position of the sub-plot in the array of plots and not just an identifier. Try this: 1. Comment the line ""plt.figure(2)"" in the cell and execute and you will notice that subplots 1&3 get plotted in the first position and 2&4 in the second position. 2. Now change the last argument for subplot3 to 3 and subplot4 to 4 and execute and you will notice that a matrix of 2*2 plots will appear.",313826.0
95610,404641.0,,nan,
95610,404744.0,,nan,
95610,404662.0,100934.0,"Welcome. For 3, not necessary over here. But I think this example was to clarify the concept of multiple figures and subplots. For 1 and 2 I think TAs will be in a better position to answer the objective of the example and the correctness of comment/code.",318479.0
95610,404662.0,100933.0,"Thanks for such an elaborate answer. However, I still stick to my observation that the code is incorrect for the following reasons: 1. The comment at the start of the code mentions ""# Example: Create a figure having 4 subplots"" meaning the code that follows is creating 4 subplots and there is no mention of number of figures to be used , so it is assumed to be a single figure. 2. As mentioned in the above point, no where it is mentioned regarding the use of figures. So, either the comment is wrong or the code is wrong, take your pick. 3. Moreover, is it really necessary to create 2 figures when everything can be done in a single figure itself??",313826.0
95639,404893.0,,nan,
95639,404899.0,,nan,
95993,407328.0,,nan,
95993,407431.0,,nan,
95661,404948.0,,nan,
95661,408389.0,,nan,
96014,407502.0,101525.0,This is the first command of the module i downloaded from sessions. So there is no such command used before this code. What is interactive mode of pyplot?,318458.0
96014,407502.0,101528.0,import matplotlib.pyplot as plt from matplotlib import interactive interactive(True),318017.0
96014,407502.0,101527.0,"Python has two basic modes: script and interactive. The normal mode is the mode where the scripted and finished .py files are run in the Python interpreter. Interactive mode is a command line shell which gives immediate feedback for each statement, while running previously fed statements in active memory",318017.0
96014,407916.0,,nan,
95692,405346.0,,nan,
95692,405145.0,,nan,
95692,405272.0,,nan,
95692,405594.0,,nan,
95353,403369.0,100736.0,"Hi Ravindra, Please check out https://stackoverflow.com/questions/19410042/how-to-make-ipython-notebook-matplotlib-plot-inline",334535.0
95353,404006.0,,nan,
96287,409841.0,,nan,
96287,409848.0,,nan,
96287,409949.0,,nan,
95469,403742.0,,nan,
95469,403847.0,100864.0,reasoning is notclear,310509.0
95469,403847.0,100863.0,how does this help and what's the difference?,310509.0
95545,404199.0,,nan,
95545,404256.0,,nan,
95545,405762.0,,nan,
95940,406748.0,101424.0,restart ed but same Isuse. background of machine,315455.0
95940,406758.0,101425.0,no luck. this is strange last time i installed couple of packages i haven't faced any issues,315455.0
95940,406902.0,,nan,
95940,407301.0,101514.0,this worked last time when i had problem how ever this time not worked some strange reason. Thx i found answer for the,315455.0
95940,407419.0,,nan,
95940,407513.0,,nan,
95499,403936.0,,nan,
95518,404106.0,,nan,
95518,404012.0,,nan,
95518,404050.0,,nan,
96991,413880.0,,nan,
96991,413882.0,,nan,
96991,414286.0,,nan,
96991,414377.0,,nan,
96991,414633.0,102796.0,"You are right, but, just imagine as a programmer, why one would use IDE instead of simple editors like Notepad++? Its just that, it would aide development and also fasten it. All the optimizations in today's world refers to optimization of time. Once I've cleansed the data and EDA , its time to off-load the visualization to Tableau. Of course, from a cost perspective, its more effective to use Python, but, from a time perspective, this would make life lot more simpler.",314084.0
95511,403976.0,,nan,
95511,404180.0,,nan,
95294,403082.0,100643.0,It's visible for me.,318329.0
95294,403082.0,100633.0,the image is not visible too,303082.0
95294,403137.0,,nan,
95294,403540.0,,nan,
97256,416424.0,,nan,
96963,413698.0,,nan,
95776,405728.0,,nan,
95776,405660.0,,nan,
95778,405699.0,,nan,
95778,405684.0,,nan,
95778,405688.0,101233.0,what does 10 and 8 signifiy? are the measures in inches?,310509.0
95778,407319.0,,nan,
95778,407315.0,,nan,
95787,405725.0,101242.0,Thank you,318846.0
95787,405766.0,,nan,
95791,406085.0,101310.0,"Team, I have emailed internal team to check and fix the problem. Will let you know.",334535.0
95791,406085.0,101299.0,I am also getting the same response,300698.0
95791,406085.0,101293.0,"Can anyone go through this modules.? Asked because I am getting this reply while clicked on above links ""The page you are looking for does not exist.""",317991.0
95791,406085.0,101298.0,The page you are looking for does not exist,318846.0
95791,406085.0,101351.0,"Hi Rajani, Can you try the above links again? Also, if a concept is not clear, please raise a separate question and be as specific as possible to get is addressed faster",334535.0
95796,406004.0,,nan,
95796,406006.0,,nan,
95796,406018.0,,nan,
95823,,,nan,
95827,406100.0,,nan,
95827,406906.0,,nan,
95822,406093.0,,nan,
95822,406108.0,,nan,
95822,406125.0,,nan,
95860,406240.0,,nan,
95860,406252.0,,nan,
95860,406289.0,,nan,
95860,406291.0,101421.0,yeah they haven't supplied the image.,318756.0
95860,406291.0,101457.0,"Koustav, Arihant, Randip, can you provide details, URL, regarding which image is missing?",334535.0
95860,406291.0,101554.0,"Expecting an detailed explanation/session on reading image, there is no specific example or set of explanation on reading image. Just a small example of image is being shared.",314313.0
95889,406452.0,101366.0,I believe it is a function of matplotlib.pyplot,311160.0
95889,406452.0,101374.0,Right. it is a matplotlib.pyplot function.,313826.0
95889,406452.0,101552.0,Still facing issue.. Please refer below screenshot. FYI: matplotlib is imported,314313.0
95889,406470.0,101376.0,Please import matplotlib.pyplot as plt (if not already done) and then call plt.tight_layout() function before call to plt.show().,313826.0
95889,406470.0,101501.0,"I have not faced it, but yes it is a new and good thing to know.",311117.0
95889,406470.0,101557.0,Can you post a screenshot of the code as well as the subplots please,313826.0
95889,406470.0,101551.0,I have already imported matplotlib.,314313.0
95889,410402.0,,nan,
97103,415031.0,,nan,
97103,415043.0,,nan,
95626,404798.0,101027.0,To what ?? Looks like it is just a warning that it is deprecated and density option needs to be used and the next patch would fix this to python software,310501.0
95626,404798.0,101159.0,"Hi Naveed, in case your query is still unresolved please provide more details of the problem.",334535.0
95626,404798.0,101042.0,"I seemed to have misinterpreted your question as referring to warnings being generated. Are you asking how to use ""sub-plot"" or ""distplot"" function??",311857.0
95626,406084.0,,nan,
95715,405323.0,,nan,
96833,413004.0,,nan,
95785,405715.0,,nan,
95785,406009.0,101282.0,this should help in suppressing the warning: import warnings warnings.filterwarnings('ignore'),319302.0
95785,406009.0,101301.0,Thanks Anand,311115.0
95785,406009.0,101291.0,Good input Anand,334535.0
95785,406143.0,,nan,
102897,440126.0,106566.0,"it is generally a fraction, so it might be some ratio i guess",318344.0
102897,440126.0,106570.0,"yes correct, it takes ratio to fit all the values in a bin to let the plot visualized correctly.. it shows the density of values accumulated in a bin.. :)",316349.0
102897,440126.0,106664.0,"It's still not clear. What exactly it is, some example, how it is calculated.",318344.0
102897,440126.0,106783.0,"Hi Ashish, Review the link posted by Swathi. There the norm_hist parameter has been explained.",334535.0
102897,440114.0,106565.0,i don't think values always add up to 1,318344.0
102897,440114.0,106665.0,"No, It is not the correct answer. Still, TA verified.",318344.0
102897,440114.0,106726.0,It is the total area of all frequencies sum upto 1. Refer https://stackoverflow.com/questions/51666784/what-is-y-axis-in-seaborn-distplot,318804.0
102897,440114.0,106782.0,"Hi Ashish, Review the link posted by Swathi. There the norm_hist parameter has been explained.",334535.0
102897,440114.0,106736.0,"see, your statement total area of all frequencies sum up to one. Frequency is no of time one event is happening. How can it be a fraction? Area under the curve equal is true, i agree but it does not answer my question",318344.0
95492,404071.0,,nan,
95492,404829.0,,nan,
95949,406868.0,101434.0,You are right.,317991.0
95949,406861.0,,nan,
95800,405856.0,,nan,
95800,405914.0,,nan,
95800,405912.0,,nan,
95937,406750.0,,nan,
95937,406836.0,,nan,
95937,406974.0,,nan,
95944,406862.0,,nan,
95944,406908.0,,nan,
95944,406921.0,,nan,
95666,405127.0,,nan,
95666,404975.0,101039.0,"They are not indicators of min and max values. Try out the below code just before barplot to get a feel of the mean,median,max and min values of sales grouped by product category df.groupby('Product_Category')['Sales'].agg(['mean','median','max','min'])",313826.0
95666,404975.0,101045.0,CI means confidence interval and this link also would be helpful i hope. https://stackoverflow.com/questions/46125182/is-seaborn-confidence-interval-computed-correctly,300733.0
95666,404975.0,101044.0,"yes, i ran the above code, values are different. so my answer is incorrect. But i searched in internet and found some answers about the black line on bar plot. i found that they are error bars and if we mention ci=None in sns.barplot() command, the error bars are not drawn. If ci=None is not entered, then the confidence intervals will be drawn which is black line. please find below links by which i hope you can understand better. https://seaborn.pydata.org/generated/seaborn.barplot.html https://stackoverflow.com/questions/40088585/turn-off-error-bars-in-seaborn-bar-plot https://stackoverflow.com/questions/30771864/what-do-the-vertical-lines-represent-in-seaborn-bar-charts",300733.0
95666,404975.0,101048.0,"Hi Praneeth, Thanks for the answer. I also did some more reading and discovered the same information.",313826.0
95666,404990.0,101040.0,"They are not indicators of min and max values. Try out the below code just before barplot to get a feel of the mean,median,max and min values of sales grouped by product category df.groupby('Product_Category')['Sales'].agg(['mean','median','max','min'])",313826.0
95666,405056.0,,nan,
95666,405209.0,,nan,
95666,407774.0,,nan,
95674,405316.0,,nan,
95674,405017.0,,nan,
95674,405201.0,,nan,
95575,404400.0,,nan,
95575,404812.0,101049.0,"if content was irrelevant/outdated, it must be updated; I hence asked the question. I know we can ignore the warnings but that is not the solution",309211.0
96007,407486.0,,nan,
96007,407482.0,,nan,
96007,407801.0,,nan,
96033,407637.0,,nan,
97166,418383.0,,nan,
97166,415675.0,102624.0,Thanks Vipul. Found that catplot is introduced in seaborn version 0.9. I was running 0.8. Updating the version solves the issue.,309451.0
97166,415675.0,102676.0,Were you able to upgrade seaborn from 0.8 to 0.9 ?,317991.0
97166,415675.0,102706.0,Actually I tried this one but was getting some other error during updation. Resolved now.,317991.0
97166,415675.0,102699.0,yes i have updated..and it resolves the issue.check this if you want to upgrade https://stackoverflow.com/questions/51422146/install-the-latest-version-of-seaborn-0-9-0-through-pip,309451.0
88601,366783.0,,nan,
88623,366854.0,,nan,
88623,367807.0,,nan,
89543,,,nan,
78546,317380.0,83265.0,thanks Manish Anand,304692.0
78546,317961.0,,nan,
75927,305718.0,,nan,
78628,318754.0,,nan,
76388,304788.0,,nan,
76388,305225.0,,nan,
76388,305704.0,,nan,
76388,305323.0,,nan,
78207,317003.0,,nan,
78207,317009.0,,nan,
78214,314434.0,,nan,
78214,319223.0,,nan,
84160,346320.0,,nan,
84160,346328.0,,nan,
84160,348531.0,,nan,
75536,301210.0,,nan,
75536,303165.0,,nan,
75536,304963.0,,nan,
75130,299006.0,,nan,
75130,299058.0,,nan,
75136,300058.0,,nan,
75325,301011.0,,nan,
75325,301335.0,,nan,
75325,318834.0,84008.0,Please try this https://stackoverflow.com/questions/32540919/library-is-not-writable,307016.0
75325,318834.0,84222.0,Thanks akram. This has been solved by changing environment Variables https://msdn.microsoft.com/en-us/library/mt628929.aspx From above link I got those details,306008.0
76595,305824.0,,nan,
76595,306656.0,,nan,
76595,312638.0,,nan,
75181,299059.0,,nan,
75181,299775.0,,nan,
75181,344631.0,,nan,
75181,342886.0,,nan,
80286,325310.0,,nan,
80286,325305.0,84899.0,"Yes it does. But, they don't need to be of the same lengths. The lengths just need to be multiples of one another. There won't be any warning as long the lengths are multiples of each other.",306733.0
80286,325302.0,84893.0,"Correction: R actually expects vectors lengths to be multiples. And, R won't complain if the vectors are of unequal lengths, given that the lengths are multiples of one another.",306733.0
80286,326769.0,,nan,
75467,300568.0,79352.0,Thank you,301655.0
75467,300568.0,79353.0,👍🏻,300689.0
75467,300568.0,79395.0,👍,300689.0
75467,300568.0,79393.0,Great explanation Dhirendra,300688.0
75467,303297.0,,nan,
75559,301207.0,,nan,
75559,304481.0,,nan,
75559,305535.0,,nan,
75559,306969.0,,nan,
76643,305668.0,,nan,
76643,312632.0,,nan,
79080,319058.0,,nan,
79080,319075.0,,nan,
79080,319351.0,,nan,
79080,319448.0,83928.0,Thanks...Good Answer!,301652.0
79080,319448.0,85864.0,"Thanks Ashish for raising this query. Even I have the similar one. But I doubt, RC is not using any global variable in his session! Any idea!",303227.0
79080,319448.0,85880.0,I think <<- wasn't intentional,301652.0
79080,319731.0,83927.0,You're welcome :),306733.0
79080,319731.0,83925.0,Thanks,301652.0
79080,321066.0,,nan,
80587,327257.0,85178.0,Try the link below:- https://learn.upgrad.com/v/course/208/session/15776/segment/79764,305129.0
80587,327257.0,85579.0,Thanks Abhishek,307494.0
80587,327407.0,,nan,
80587,328103.0,,nan,
80587,345239.0,,nan,
80587,347623.0,,nan,
80587,348448.0,,nan,
80643,327682.0,85350.0,Thanks ... I was able to view the content now. At first content appeared to be locked but it is not,302744.0
80643,328217.0,,nan,
80643,328783.0,,nan,
80643,331908.0,,nan,
80643,334019.0,,nan,
79097,319173.0,,nan,
79097,320235.0,,nan,
79097,320237.0,,nan,
79097,321068.0,,nan,
79097,320485.0,,nan,
79097,322462.0,,nan,
79204,319744.0,,nan,
78288,321704.0,,nan,
78288,315068.0,,nan,
78288,316855.0,83220.0,"Thanks Praful, My bad. I didn't noticed at the result properly. Yes, it shows the result with warning.",304812.0
78288,316855.0,83254.0,Oh so sorry :),304812.0
78288,316855.0,83243.0,Parul*.. :),305845.0
78288,326024.0,,nan,
78288,343840.0,88738.0,I had also observed that later after watching other discussion. Thank you very much for the explanation.,311729.0
78288,343840.0,88673.0,"Yes we are getting a warning message due to the vector’s length mismatch. But the output is also displayed. Incase you want to avoid the warning message, use vectors of same length.",307488.0
78288,344023.0,,nan,
78288,344026.0,,nan,
77951,312941.0,82534.0,thank you Sudheer..,305845.0
77951,313028.0,82537.0,quite logical... Thank you Kiran,305845.0
77951,316999.0,,nan,
79246,320024.0,83975.0,thank you,305652.0
79246,325343.0,,nan,
79646,322585.0,84401.0,"Hi Kiran, My bad, I meant myvector[4-1:4] and this prints 3 2 1",304398.0
79646,322611.0,84483.0,Thank you. A very clear explanation.,304398.0
79646,322611.0,84587.0,Good discussion here . Nice to see !,301561.0
78411,316858.0,,nan,
78411,316998.0,,nan,
78411,317105.0,83124.0,Thank you for the coding standards of assigning in R,304692.0
87422,363917.0,,nan,
87422,360974.0,,nan,
80405,326168.0,85097.0,"If i am not wrong, the syntax goes like this: read.table((file.choose())) please check",301561.0
80405,326233.0,85052.0,"That's not an error message. It's just a warning message. So, check if the file's been read in properly. If it has, you can probably make necessary changes to the file within R.",306733.0
80405,326596.0,,nan,
80405,326932.0,,nan,
80405,334101.0,,nan,
82560,338511.0,,nan,
82560,338642.0,,nan,
86921,357873.0,,nan,
86921,357961.0,,nan,
86921,357761.0,,nan,
86962,357941.0,,nan,
83151,341382.0,,nan,
83151,341397.0,88207.0,"Not really, the plus sign indicates incompleteness of the code. R expects you to add required character/input to complete the code.",301561.0
83151,341397.0,88252.0,"if you dont close a bracket - what is get is + symbol and its not an error If you abruptly close a bracket or dont close all open brackets, then you get error",301561.0
83151,341397.0,88221.0,"What is the difference in incompleteness and error here if someone miss the bracket it can be either error or incomplete code just take an exaple of print of expression like cos or log in that case when you need to come out of console we must use ESC, or correct the code and rerun",307843.0
83151,341554.0,,nan,
86474,357047.0,,nan,
86474,361589.0,,nan,
88824,368025.0,,nan,
87349,360135.0,,nan,
87349,360139.0,,nan,
87351,360149.0,,nan,
87351,360803.0,,nan,
87351,363922.0,,nan,
83643,343809.0,88672.0,"Ok, as far as I can see, that's the only error in your code. Can you provide a screen shot your environment pane? Or run, ls.str() and provide a screen shot of the output.",306733.0
83643,343809.0,88670.0,Even adding comma is not working out.,314128.0
83643,343856.0,,nan,
83643,346614.0,,nan,
83643,346885.0,,nan,
86051,352575.0,,nan,
86051,352658.0,,nan,
86051,352788.0,,nan,
85724,,,nan,
85763,352371.0,,nan,
85763,352378.0,,nan,
85763,352496.0,,nan,
85763,352609.0,,nan,
85763,353704.0,,nan,
83765,344635.0,,nan,
83765,344722.0,,nan,
83765,345878.0,,nan,
86116,353084.0,,nan,
86241,354574.0,,nan,
84116,346063.0,,nan,
84116,346326.0,,nan,
86336,358419.0,,nan,
86336,355582.0,,nan,
87632,363688.0,,nan,
87632,363696.0,,nan,
87632,363703.0,,nan,
87632,363903.0,92196.0,"Tried installing it from command line in RStudio - it displays the same error : > install.packages(""swirl"") Warning in install.packages : unable to access index for repository https://cran.rstudio.com/src/contrib: cannot open URL 'https://cran.rstudio.com/src/contrib/PACKAGES'",319759.0
87632,364140.0,,nan,
87632,365624.0,,nan,
87590,363022.0,,nan,
87590,363908.0,,nan,
87590,367442.0,,nan,
87724,364162.0,92417.0,"based on the snap you have shared, you are trying to install swril instead of swirl. Can you try with the correct package name, it should work.",319302.0
87724,364268.0,,nan,
87724,364342.0,,nan,
87724,364731.0,,nan,
87725,364160.0,,nan,
87725,364271.0,,nan,
87744,364262.0,,nan,
87729,364263.0,,nan,
76634,306970.0,93158.0,Link is not working,320074.0
76634,317489.0,,nan,
83651,343897.0,,nan,
83651,343894.0,,nan,
87137,358938.0,,nan,
87148,358941.0,,nan,
82066,336025.0,,nan,
82066,334968.0,87014.0,"Ranjana seems it is question of Advanced R, since we read in the tutorial about getting the functionaly on the data sets using Which function. However I am not sure about excel as it has not been taught still in the prep course to work on the exce. My guess only to read the cells of excel and apply which function could give your result.",307843.0
82066,334968.0,87045.0,Yes. Hopefully it will be cleared in upcoming sessions. Thank you.,310508.0
82066,334964.0,,nan,
82066,335216.0,,nan,
82066,336999.0,,nan,
88424,365894.0,92776.0,OK. Thanks.,320008.0
77355,308606.0,81559.0,Close with a } :-),300717.0
77355,308605.0,,nan,
77355,310620.0,82295.0,what is that you are trying to achieve with the above code ??,300688.0
77355,317001.0,,nan,
77355,312176.0,84368.0,Sorry for the late reply. Thank you that solved the purpose :),301114.0
82312,337336.0,,nan,
80164,324638.0,,nan,
80164,324930.0,,nan,
80164,325300.0,,nan,
80164,325337.0,,nan,
80164,325313.0,,nan,
76886,311483.0,,nan,
76886,309587.0,,nan,
79124,319649.0,,nan,
79124,319247.0,,nan,
79318,320509.0,,nan,
79318,320363.0,,nan,
79318,321361.0,,nan,
82199,336529.0,,nan,
82199,336625.0,,nan,
82199,336951.0,,nan,
81746,333125.0,86527.0,"Yes, I was referring to the file named ""Functions"". Thank you.",304389.0
81746,333124.0,86526.0,Thank you. I shall do the same.,304389.0
81746,333120.0,,nan,
84979,348109.0,,nan,
84979,348296.0,89867.0,Mention exact steps you followed.,301557.0
84979,348296.0,89846.0,I'm already on R Studio and followed exact same steps.,308962.0
84979,349061.0,89855.0,assuming you used in setwd command to the bank.csv path as well while reading from the bank.csv,311227.0
84979,349061.0,89854.0,I can see as same as the video. Which version of R Studio you are using?,311227.0
80624,327499.0,85209.0,thanks ....I think I got the answer,308442.0
80624,331896.0,,nan,
80376,325850.0,,nan,
80893,329769.0,88408.0,"@Thulasiram, Googling/Stacktrace or internet in general does not help for such OS specific errors; You need to look into the error logs and troubleshoot /choose the appropriate action to fix it",309211.0
80893,328789.0,,nan,
80893,328764.0,,nan,
80893,328720.0,,nan,
80893,341522.0,88406.0,"This was what I followed to get it resolved : When on MacOS High Sierra , although lot of webpages suggest Homebrew for installation, please do not select it. Go with the approach suggested below : 1) Install ""R-3.5.1.pkg"" from ""https://cran.r-project.org/bin/macosx/"" 2) Install ""RStudio-1.1.456.dmg"" from ""https://www.rstudio.com/products/rstudio/download/#download"" 3) Fire up RStudio and just add this as shown by RC in the tutorial: >install.packages(""swirl"") >library(""swirl"") >Swirl [ Works in the latest Mac (MacOS High Sierra 10.13.6) ]",309211.0
85237,349556.0,,nan,
85237,350114.0,,nan,
85237,349442.0,,nan,
85237,349448.0,,nan,
85237,349636.0,,nan,
85237,350392.0,,nan,
82596,338735.0,,nan,
82596,338704.0,,nan,
82596,338702.0,,nan,
82596,338844.0,,nan,
82596,338755.0,,nan,
88283,365382.0,92659.0,Thanks,314361.0
88283,365345.0,92658.0,Thanks,314361.0
88283,365563.0,92657.0,Thanks a lot. Got it.,314361.0
83757,344611.0,91484.0,"Even i notice same as you said. marital status is ""divorced"". we didnt write in looping conition.",305804.0
83757,344815.0,,nan,
80627,327521.0,,nan,
80627,349198.0,,nan,
80627,349201.0,,nan,
83514,343171.0,,nan,
86987,358168.0,91364.0,"thanks for the reply, but actually i am discussing if we have imported ""bank.csv"" file and made a program to check which all people can be issued credit card or not . For ref. i am pasting the code of program:- for (i in 1:nrow(bank)) { if(p1$marital == ""married"") { if(p1$housing == ""yes"" | (!is.na(p1$salary) & p1$salary) > 60000) { bank[i,""my_decision""] <- ""yes"" } else { bank[i,""my_decision""] <- ""no"" } } else if(p1$marital == ""single"") { if(p1$education == ""tertiary"" & !is.na(p1$salary) & p1$salary > 40000) { bank[i,""my_decision""] <- ""yes"" } else { bank[i,""my_decision""] <- ""no"" } } }",307496.0
86987,358168.0,91366.0,In this mentioned program if i need to check condition for some selected rows let from 5th row to 10th row then what instruction should bw used in place of for (i in 1:nrow(bank)) {,307496.0
86987,358168.0,91403.0,Since you are working on the bank file then obviouly for( i in 1:nrow(bank)) is only good choice however for the list we have seen we can user range function as well.,307843.0
86987,358280.0,91377.0,thnks it was helpfull,307496.0
87596,363207.0,,nan,
87596,363957.0,,nan,
88228,364906.0,92564.0,"Ok fine...but in my code whatever is inside is.na() if that is true only then which function will be valid right, else it will not return that particular row...",308437.0
87187,359044.0,91540.0,Looks like there is strict validation based on the code pattern and not on the output.,313826.0
87187,359044.0,91538.0,"k now got it correct , but why in swirl '==' sign is needed for which() function",307496.0
90873,392797.0,,nan,
90873,381008.0,,nan,
90873,379372.0,,nan,
90873,381928.0,,nan,
90873,388364.0,,nan,
91085,381602.0,,nan,
91085,380405.0,95727.0,yup i got the point....but the table structure is so confusing,300693.0
91085,380421.0,,nan,
91085,380739.0,,nan,
88701,367351.0,,nan,
88701,367429.0,93559.0,"Hi Veena, Where is typo error? My answer is clear for the same.",305847.0
88701,367429.0,93553.0,"There is a small typo error in your response Prashant Sahu ! Second part of your response needs to be ""questions with 1 attempt""....",300717.0
88701,380411.0,,nan,
88701,380783.0,,nan,
89008,368992.0,93333.0,Thanks ram . Yeah true after going the question I I realised and completed,300687.0
89008,368992.0,98317.0,correct,313228.0
89008,369994.0,,nan,
89008,380394.0,,nan,
92397,389164.0,,nan,
92397,389190.0,,nan,
90797,378422.0,,nan,
90797,378387.0,95361.0,"I got ~77% retained rows so I guess I am in right direction, thanks :-)",317577.0
90797,378387.0,95360.0,"Thanks Vipul, will verify after checkpoint :-)",317577.0
90797,378459.0,,nan,
90797,379697.0,,nan,
93437,,,nan,
134277,583728.0,,nan,
133986,582920.0,130554.0,Thank you for replying Harshit. Was analysing the dataset present on kaggle. Need to find out if it isn't exactly the same present on the corestack,311160.0
133986,582967.0,,nan,
133986,583011.0,,nan,
133986,582995.0,,nan,
133986,583044.0,,nan,
133913,582860.0,130529.0,Thankyou understand. going beyond max int value. ok,312019.0
133344,580933.0,,nan,
134212,583534.0,,nan,
134212,583538.0,130726.0,There are 2 ways to tackle this: 1. Rename the column names in the dataframe and then create the TempView. This way you will be able to write queries in a simple way. 2. The other way is to use ticks ` symbol (the one usually below Esc button) around the column names having space in them.,313826.0
134212,583538.0,130731.0,ok thank vinay,314612.0
134212,583533.0,,nan,
134219,583557.0,130730.0,Oh my God...this wasn't taught...thanks a lot 🙏🏻🙏🏻🙏🏻,308437.0
134234,583582.0,130738.0,"Why did isnull and isnan commands failed to detect this?? Am surprised... Checking the blanks by using ""Nan"" as string helped...strange 🙄🙄🙄",308437.0
134234,583582.0,130936.0,"isnull and isnan failing as the cell is not blank but has values as string and the string value is ""NAN""",317514.0
134234,583856.0,,nan,
134234,583747.0,130775.0,"True! That is why you would be called a data ""Scientist"". :) These data quality issues, that this course is getting us acquainted with, are some of the typical quality issues that we might face. But, like you said, there might be more and as a Data Scientist it is your/our job to do our best with the analysis. :)",317998.0
134234,583747.0,130774.0,How can we even think of these things?? I mean Nan as a string...!!! If we think like these there could be lot of other things as well which r hidden...and such a large dataset we cannot even cross check in excel what r the issues present in each column...!!!,308437.0
134238,583635.0,130813.0,Thanks for the clarification.,313826.0
134238,583635.0,130812.0,Only one .ipynb file.,428646.0
134238,583635.0,130826.0,Thanks,318358.0
134238,583635.0,131134.0,thanks sumit,305650.0
134238,584913.0,,nan,
134238,583592.0,,nan,
134238,583615.0,130742.0,That I also observed but there is no point in creating pdf/doc file if we are writing our subjective observations in jupyter notebook itself. Need TA to verify which files are required for submission.,317991.0
134238,583615.0,130744.0,Yaa. TA could you please replay asap. Because same outputs we have to paste it in doc/pdf if required. Nothing special. Or we can write in code file ipynb only ?,312019.0
134238,585265.0,,nan,
134059,583006.0,,nan,
134059,583007.0,,nan,
134059,583024.0,,nan,
134254,583687.0,130755.0,i tried multiple times by restarting the kernel .but still I am getting same issue,301108.0
134254,583858.0,,nan,
134254,583764.0,,nan,
134782,585478.0,131267.0,Thanks Sumit for the response,305650.0
134782,585476.0,,nan,
134288,583765.0,,nan,
134288,583767.0,130777.0,I think you can perform null/nan value treatment in the beginning also....just make sure to mention your assumption.,317991.0
134288,583767.0,130776.0,"i have taken care of incorrect values when ever mentioned but again if we will treat nan values as mentioned in question 5 , ques 1-4 will have some different results. I had only doubt that why null value treatment is not asked in beginning",318005.0
134288,583767.0,130779.0,thanku,318005.0
134367,583938.0,,nan,
134388,583987.0,130947.0,That just depends on how many rows you want to show . Use .show(n) function .,318427.0
134388,583987.0,130890.0,Hi I have used group by followed by order by now I'm want to show top 5 only !,311466.0
134388,583967.0,,nan,
134388,585045.0,,nan,
134422,584085.0,,nan,
134422,584084.0,130884.0,"Since it is related to assignment, can help with the approach only.",317991.0
134422,584084.0,130882.0,"I understand what to do but able to figure out how to do that in yesterday’s session hint, I understand but I am struggling how to apply this",300735.0
134222,583852.0,130980.0,"I referred a StackOverflow link for the conversion as I couldn't find that time format reference anywhere else online. I didn't see a reference for the given time format conversion anywhere else. So I'm not sure what to do. The function is working for most rows, but some are becoming null.",310505.0
134222,583852.0,131071.0,"Refer to my live session, I have provided with the hint.",428646.0
134222,583793.0,,nan,
134677,585102.0,,nan,
134447,584168.0,,nan,
134447,584169.0,,nan,
134453,584179.0,,nan,
134453,584814.0,,nan,
134453,584180.0,,nan,
134442,584155.0,,nan,
134442,584159.0,,nan,
134442,584166.0,,nan,
134337,583861.0,130817.0,Thanks sumit...I discovered that this column is messier than I thought...!! Will go through the video,308437.0
134337,583861.0,130960.0,Hi - do you know where I can find the link?,317149.0
133586,581578.0,,nan,
133586,581573.0,,nan,
134119,583367.0,130698.0,I don't think that will work,300694.0
134119,583367.0,130821.0,You were right. Tried it yesterday. Didnt work :(,317998.0
134119,583510.0,,nan,
134119,583480.0,,nan,
134119,583319.0,,nan,
134119,583309.0,,nan,
134119,584511.0,131089.0,Yes..i used groupby and rank to work this out.. Thankyou,312096.0
133094,580439.0,,nan,
134487,584317.0,,nan,
134487,584438.0,,nan,
134487,584307.0,130933.0,I have raised the concern with corestack team... looking forward to hear back from them,306005.0
134225,583563.0,130734.0,Thanks Vipul,316211.0
134744,585309.0,131142.0,"Agree, No harm in adding a pdf/doc file.",317991.0
134744,585328.0,131143.0,"Agree, No harm in adding a pdf/doc file.",317991.0
134744,585344.0,,nan,
134744,585490.0,131178.0,Thanks a ton! Sumit,318355.0
134744,585444.0,,nan,
134501,584335.0,,nan,
134501,584434.0,,nan,
134501,584818.0,,nan,
134501,584532.0,,nan,
134501,585043.0,,nan,
134501,585292.0,,nan,
134501,585252.0,,nan,
134093,583324.0,,nan,
134093,583246.0,,nan,
134093,583291.0,,nan,
134093,583809.0,,nan,
134093,583835.0,130816.0,"The name of your dataframe is df1. So use df1 inplace of 'F'. It should be : new_df = df1.withColumn(""`Registration State`"", when(df1.col(""`Registration State`"") == '99' ,'R').otherwise(df1.col(""`Registration State`"")))",304319.0
134093,583835.0,130879.0,"from pyspark.sql.functions import when df1 = df.withColumn(""`Registration State`"", when(df[""`Registration State`""] == ""99"",""NY"").otherwise(df[""`Registration State`""]))",301643.0
134093,583835.0,130877.0,"Hi Ruchita, F is from pyspark.sql import functions as F",301643.0
134046,583325.0,130662.0,Thanks for the clarification,308437.0
134046,583052.0,,nan,
134046,582972.0,130556.0,Also can TA verify this?,308437.0
134046,582972.0,130555.0,Ok as u said wherever data cleaning is required it is asked in the question itself so have to perform only that...😎 thanks,308437.0
134046,582963.0,,nan,
134069,583057.0,,nan,
134069,583045.0,,nan,
134069,583035.0,,nan,
133837,582438.0,,nan,
133837,582597.0,,nan,
134089,583355.0,,nan,
134089,583129.0,,nan,
133517,581505.0,130413.0,What is actual precinct here?,310974.0
133517,581789.0,130376.0,Thank you!,311254.0
133517,581620.0,,nan,
134096,583304.0,,nan,
134096,583295.0,,nan,
134096,583337.0,130677.0,"Hey Arpit, As I said whatever you are getting in your analysis you can report them. Since this is an assignment I can help you with the explanation. The question asked is to check and find a way to treat nulls, you can answer accordingly.",428646.0
134096,583337.0,130674.0,I've done data cleaning part according to me and what I have asked and what kind of answers you guys are providing are not upto the mark. Dude I have that intellectual to understand what's written in the assignment and I'm trying to solve accordingly but my piece of code has worked fine to find Null and Nan Values but the answers returned isn't correct (i.e 0 but there nan and null values present I checked it manually ),311466.0
134096,583255.0,,nan,
133538,581465.0,,nan,
134542,584550.0,,nan,
134542,584513.0,,nan,
134542,584939.0,,nan,
134542,584747.0,,nan,
134542,585296.0,,nan,
134542,585254.0,,nan,
134097,584429.0,,nan,
134097,583175.0,,nan,
134097,583303.0,,nan,
134556,584709.0,,nan,
134556,585352.0,,nan,
134559,584555.0,130976.0,My doubt is. do we need to focus only on newyork state data or all,311952.0
134559,584555.0,131011.0,Not necessary at all. Please do mention your intention.,301121.0
134559,584584.0,,nan,
134559,584938.0,,nan,
134559,584751.0,,nan,
134565,584591.0,130982.0,thank u,317149.0
134565,584937.0,,nan,
134568,584643.0,131027.0,TA already confirmed Refer below link https://learn.upgrad.com/v/course/208/question/134238,310634.0
134568,584720.0,,nan,
134568,584936.0,,nan,
134568,585315.0,,nan,
133624,581767.0,,nan,
133624,581676.0,,nan,
133624,581834.0,,nan,
133897,582784.0,130628.0,You have to use filter() in conjunction with year() function. I don't have the syntax handy right now.,313826.0
133897,582784.0,130627.0,"Vinay after using use . option(""inferSchema"", ""true"") i could get the required records for the year 2017 using tempview sql. But what if i dont want to use the sql and query the records using only the pyspark dataframe? i have tried using year function but i am getting error. Can you please let me know the correct syntax please.",301114.0
133897,582675.0,,nan,
134111,583245.0,,nan,
134111,583302.0,,nan,
134111,583281.0,,nan,
132888,579600.0,129850.0,"Thanks , yes I assumed the same thing as no point in learning SparkR just for the sake of assignment. let TA confirm.",309211.0
132888,579548.0,129851.0,"Thanks , yes I assumed the same thing as no point in learning SparkR just for the sake of assignment. let TA confirm.",309211.0
132888,579941.0,,nan,
133985,582932.0,,nan,
133985,583247.0,,nan,
133985,582916.0,,nan,
133985,582924.0,,nan,
134627,584804.0,131026.0,You have to transform the bucketizer output to dataframe bucketedData = bucketizer.transform(dataFrame),317991.0
134627,584804.0,131022.0,But bucketizer object column is not accessible...,308437.0
134627,584824.0,131028.0,Yes got it..silly mistake thanks,308437.0
134627,585449.0,,nan,
134627,584827.0,,nan,
134666,585463.0,,nan,
134666,585071.0,131092.0,It depends whether one want to make assumption to keep these or filter it out.,317991.0
134666,585071.0,131086.0,"We can filter them right? Because in all our future queries, we dont need these rows and there are only around 40-50 such rows in entire dataset",308437.0
134380,583950.0,130886.0,it is showing 5 rows,302741.0
134380,583950.0,131136.0,"Sir, Kindly look at my answer below and verify if it solves the problem mentioned in the question.",317998.0
134380,583937.0,,nan,
134380,584119.0,,nan,
133484,581228.0,,nan,
133484,581699.0,,nan,
133484,581179.0,,nan,
133711,582892.0,,nan,
133711,582043.0,,nan,
133711,581988.0,,nan,
133824,582547.0,,nan,
133824,582407.0,,nan,
134057,583004.0,,nan,
133708,582042.0,130412.0,Thanks!,315423.0
133708,581995.0,,nan,
134266,583698.0,130814.0,"Since in this question it is mentioned to ignore 0 and report other, please don't use 0.",428646.0
134266,583686.0,,nan,
134266,583749.0,,nan,
134297,583789.0,130832.0,"I created nyc_prk as below nyc_prk = spark.read.format(""csv"").option(""header"",""true"").load(""/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv"") nyc_prk when tried using spark.createDataFrame(nyc_prk), it throwed an error mesage the object is already a dataframe, not sure where it went wrong",302741.0
134297,583787.0,130837.0,I don't find anything wrong here ! just check the type(your_df_name) if its returning list then you must have done something wrong in between !,311466.0
134297,583787.0,130828.0,"nyc_prk is the name of my dataframe which i created as below nyc_prk = spark.read.format(""csv"").option(""header"",""true"").load(""/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv"") nyc_prk",302741.0
134297,583787.0,130829.0,is the creation of dataframe is incorrect?,302741.0
134297,583787.0,130830.0,"nyc_prk is a dataframe which i have created, but not sure why it is throwing error, please find below the syntax nyc_prk = spark.read.format(""csv"").option(""header"",""true"").load(""/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv"") nyc_prk",302741.0
134297,583787.0,130839.0,"Surendra, Check something is missing in between. Your error is showing that the variable is a list. You can again load in the data and try.",428646.0
134668,585488.0,,nan,
134668,585313.0,,nan,
134668,585072.0,131159.0,Some time ago TA has mentioned to submit two files 1. One jupyter notebook 2. Pdf/Doc file containing subjective insight/plots. https://learn.upgrad.com/v/course/208/question/134744,317991.0
134668,585312.0,,nan,
134071,583059.0,,nan,
134071,583054.0,,nan,
134071,583252.0,,nan,
134044,582960.0,,nan,
134693,585941.0,,nan,
134693,585460.0,,nan,
134693,585220.0,,nan,
134694,585457.0,,nan,
134694,585285.0,,nan,
134694,585237.0,,nan,
134324,583839.0,131070.0,"if i want to see dataframe contents and NOT to use show, how can I do then?",308437.0
134324,583839.0,131069.0,I dont know why pyspark is so non-user friendly. This is one among many issues I Have been facing in pyspark. We didnt have these kind of limitations in pandas. df.head() was so simple.,308437.0
134324,583834.0,130806.0,What is meant by row level or column level? I am trying to add M to every row in the column...,308437.0
134324,583834.0,130808.0,"I ran the code without show() in the end, it is working fine. The error was coming because of show(). Reason is already mentioned by Vinay below.",317991.0
134324,585100.0,,nan,
133686,581865.0,,nan,
133686,581868.0,,nan,
134780,585470.0,131168.0,Thanks Bindu but this is not fair at all!,318355.0
134780,585477.0,,nan,
134780,585479.0,,nan,
134780,585491.0,,nan,
133541,582875.0,130684.0,how did you installed the pyspark in local jupyternotebook?,313676.0
133541,582875.0,130694.0,"In Anaconda Navigator. Click on Enironments. click on base(root) -> click on open terminal. then type pip install pyspark After installation done click on again base(root) ->select open with Jupyter note book It opes windows cmd commands terminal and also firefox jupyter notebook We go head and use it Inbetween coding, in case if its crashed with java exception, any of pyspark code commands my not work from there this time again do same base(root) ->select open with Jupyter note book you can type and check spark.version and start with spark object creation with code. everything works fine i gave file name with absolute path from pyspark.sql import SparkSession spark = SparkSession.builder.appName(""parking"").getOrCreate() df=spark.read.format(""csv"").load(""F:/dir1/Parking_Violations_Issued_-_Fiscal_Year_2017.csv"",header=True,sep="","") then as usual code to follow",312019.0
133541,581476.0,130228.0,"No..nothing in running status, stil lthe issue persists.",313676.0
133541,581476.0,130231.0,yes.. have raised the same. :(.. Thanks,313676.0
133541,581476.0,130230.0,"Try logging out from corestack and jupyter, then login again to both and see if it resolves the problem If it still persists, then suggest to raise issue with corestack by clicking on support button on bottom right of corestack login page.",313826.0
133541,581779.0,,nan,
133541,581514.0,130637.0,"This does not solve the problem.. What I have observed that this seems to a machine problem, I am able to connect to the Jupyter successfully on my home machinne, but the same is not accessible through office laptop, may be some settings done by my organisation which are not letting me connect to the kernal of jupyter.",313676.0
133541,581514.0,130638.0,Corestack uses http and not https. This is the reason your office laptop is not getting connected.,428646.0
133541,581514.0,130650.0,Ticket #22385 is already raised to corestack.. they were of the mind that this is due to internt connection issue :(,313676.0
133541,581514.0,130644.0,"so this means, I will not be able to connect from office laptop, or does Corestack has a solution to it?",313676.0
133718,582033.0,130358.0,The scope of the analyses are tickets for year 2017 and only those records that conform to this condition should be considered.,313826.0
133718,582033.0,130352.0,"Hi Vinay, Data for 2017 is about 50% and rest of the data is other than 2017.So are you recommending to delete 50% of data ?",317991.0
133718,582548.0,130482.0,I created new dataframe by using logical operator. Original dataframe is undisturbed.this is also fine right?,308437.0
133718,582548.0,130578.0,"No issues, but mention this is your notebook.",428646.0
133718,582548.0,131171.0,"Hi Sumit, To one of the question you have mentioned that consider whole dataset as 2017. So if we consider whole dataset then answer for the question ""analyzing tickets over 2017"" will be different as against the answer we get when we filter only 2017 year data. So which approach to follow for this question ?",317991.0
133718,582549.0,,nan,
133909,583248.0,,nan,
133909,582787.0,,nan,
133909,582721.0,,nan,
134795,585694.0,,nan,
134795,585581.0,131193.0,"Unix is a wrong approach. Use some other way. Refer to my live session, I have provided with the hint",428646.0
134795,585581.0,131187.0,When I convert into Unix timestamp I get a NULL date for the record with P . I have the date format as 'yyyy-MM-dd hh:mm a' if I convert hours to 'HH' it works fine for 24 hour clock but it then disturbs the record with 12 hour clock time which needs 'hh',314197.0
134795,585594.0,,nan,
133577,581529.0,130238.0,Thanks Vinay! There must be moderators to handle the duplicate questions as in the StackOverflow,318355.0
133577,581529.0,130239.0,"True. However, this being a discussion forum which is largely driven by peers, there isn't any moderation done. So, if I have a query, I first try out all avenues including upgrad's discussion forum and post only if still unanswered.",313826.0
133577,581526.0,,nan,
133975,583493.0,,nan,
133975,582912.0,130643.0,I don't this you have been asked to remove nulls,428646.0
133975,582912.0,130563.0,so its valid or erroneous. those observations can be deleted ?,312019.0
113932,491678.0,,nan,
113932,491445.0,,nan,
113932,491629.0,115871.0,thanks,317073.0
113932,491629.0,115715.0,"As I said, it would depend on the final modelling work. Here we are only doing dimensionality reduction. So the outlier analysis may be done before or after since the final models would be on clustering. Also, you do this so that you can visualise the final clusters using a scatterplot. If your task was to use the PCs to predict something, similar to a recommender system, then it would be wiser to do the outlier analysis beforehand. The concept of a recommender system hasn't been taught but it is similar to the SVD of Books example which the Prof taught you. What you do here is take a list of ratings, try and understand the user behaviour and then recommend them something. A simpler example would be if you watch Youtube videos and out of the last 25 videos that you've watched 24 were music videos and 1 was a movie trailer. Here the recommender system should model your behaviour accordingly and suggest you a music video only. If you consider that 1 observation as an outlier and remove it, it won't hamper your recommendations. Now, this example might be a bit primitive because a lot of algorithms go behind recommendation systems, but the basic idea remains the same.",313517.0
112117,483294.0,114309.0,Getting error,308638.0
112117,483294.0,114394.0,Thanks Pradeep.But this has to be done after choosing clusters right One small doubt original data frame refers to data frame which contains country right.And this not been standardised.,308638.0
112117,483294.0,114396.0,"yes , exactly .. it's data frame with country and do after clustering",318732.0
112117,483294.0,114397.0,Data frame which contains country has not been standardised.Does it affect.,308638.0
112117,483294.0,114398.0,One more doubt in syntax Data frame frame refers to Data frame where outlier has been done which has less rows than original In first line of syntax we are reseting index right,308638.0
112117,483294.0,114403.0,"yes , for that we are resetting index and standardization may effect slightly but not so important as per my knowledge",318732.0
112117,483294.0,114406.0,Thanks...'ll try,308638.0
112117,483294.0,114423.0,Am getting key error 'index'.PCA dataframe refers to clustered one right,308638.0
112780,486533.0,114784.0,you need to calculate the mean value for the column provided in the dataset which you thought can represent a country's socio economic factor properly.After that of you visualize it for different cluster then it will be easier to find out the results.,301648.0
112780,486518.0,,nan,
112780,486663.0,,nan,
112780,486553.0,,nan,
112113,483297.0,,nan,
112113,483666.0,,nan,
112113,483329.0,,nan,
111868,481888.0,,nan,
111868,482103.0,,nan,
111868,482145.0,,nan,
112132,483436.0,114327.0,"Since PC contains information about original variables, so probably they want us to visualize how original variables are scattered inside PC's. Just a thought.",317991.0
112132,483436.0,114326.0,"I also have the same feeling, but don't understand the need for this.",313826.0
112132,483443.0,,nan,
112132,483610.0,,nan,
112132,483698.0,114372.0,Make sense. Had same doubt. But now its clarified. Thanks,317991.0
112132,483672.0,114371.0,But Step 9 is before clustering so we don't have clusters till step 9. So we can't use color dimension on clusters at this point.,317991.0
112019,482898.0,114246.0,I merged PCA clustered df & original data df. Checking variables vs clusters now. That is what I could only do with PcA df,312093.0
112019,483111.0,,nan,
112126,483391.0,114322.0,I have converted the PCA np.array to datframe and then did added country column through Concat command and treated outlier after that running the hopkins statistics,314197.0
112126,483391.0,114328.0,"Output of PCA is a numerical array. Convert that into a dataframe and that should be the input to Hopkins ,,, hopkins(data_frame)",301121.0
112126,483391.0,114390.0,Well answered Muthu,334535.0
112126,483498.0,,nan,
112126,483451.0,,nan,
112126,483711.0,114399.0,Thanks,317991.0
112126,483711.0,114389.0,Well answered Vipul,334535.0
112021,482778.0,,nan,
112021,482770.0,,nan,
112021,482804.0,114234.0,that is confusing me more :|,314221.0
112021,482942.0,114405.0,"You concatenate the columns to PC's dataframe to see how the original variables are distributed according to the PCs. You perform the outlier analysis on the transformed PCA dataset. And then you continue to the clustering process and after that, you can add the countries back to see which country belongs to which cluster.",313517.0
112021,483107.0,,nan,
112262,484553.0,114460.0,Any example ? As I hav checked internet also for example as well as videos didn’t find any particular code to have an idea,318797.0
112262,484553.0,114491.0,"Hi Meghana, Apart from the course sessions, here is a good tutorial https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/",334535.0
112209,484022.0,,nan,
112209,484021.0,,nan,
112323,485503.0,,nan,
112323,484846.0,,nan,
112323,484862.0,,nan,
112323,484616.0,,nan,
112323,485288.0,,nan,
112323,485667.0,,nan,
110724,476684.0,113201.0,"Hi Mahima, this is not clear. Can you please elaborate? When you say ""Outlier Analysis"" i hope you mean just the analysis not the treatment.",310974.0
110724,476684.0,113216.0,"What I meant was that do the clustering without the outlier values and then proceed to reassign the same outlier values to the respective cluster centres that were found in the clustering process in the previous step. For example, if you had 35 data points out of which 5 points came out as outliers, then do clustering on only those 30 points. Let's say you got 3 cluster centres. Now reassign those 5 outlier points to these 3 cluster centres based on their similarity or Euclidean distance measure.",313517.0
110724,476684.0,113471.0,"If I go by the above approach, out of 167 records , 39 records treated as outliers . I am doing PCA first to reduce dimensions, then will apply K means. Am i going in the right way?",307494.0
110724,476684.0,113569.0,"I didnt get that. we have 167 records. When we use describe using percentiles we see for few, values are not gradually increasing. For those columns we calculate interquartile range i.e. IQR = Q3 - Q1 to treat the outliers. Is that the correct approach? This is prior doing the PCA and after that we proceed with the clustering right? Can someone explain this please? Also not clear with the TA's answer. Confused with another post too : https://learn.upgrad.com/v/course/208/question/111214",301114.0
110724,476684.0,113605.0,Very helpful,320687.0
110724,476684.0,114227.0,@Mahima Thank you so much!,318355.0
111296,479288.0,113966.0,"i need help here Now i have 9 pca, pc1 to pc9 with feature(pc1 is first feature, pc2 is second features etc ??) Made scree plot and found the optimum no of componenets and checked he correlation matrix fine. Now how to make the relation with this data for country which needs money, need some help here",312019.0
111296,479372.0,,nan,
111296,479296.0,,nan,
111601,480763.0,,nan,
111608,480752.0,,nan,
111608,480723.0,,nan,
111608,480776.0,,nan,
111961,482348.0,,nan,
111961,482398.0,,nan,
111961,482555.0,,nan,
112231,484427.0,,nan,
112231,484507.0,,nan,
112940,487045.0,,nan,
112940,487109.0,,nan,
111230,478979.0,113558.0,Thanks Vinay..,307494.0
111230,478979.0,113575.0,Yes but how exactly to do that is the question :),310974.0
111230,478979.0,113628.0,How do we assign the outlier removed in step 1 to the clusters based on whichever cluster they are nearest to?? was this even taught??,316349.0
111230,478979.0,113629.0,"No, I have the same doubt for reassigning to the nearest cluster",307494.0
111230,478979.0,113643.0,3. Clustering (as per steps described above) Hierarchial or K means anything we can choose? Assignment eval rubric says do both...can somebody provide clarity on this? thanks,308437.0
111230,478979.0,113630.0,"No. This has not been taught. I am trying to figure out how to do this. Without this, I am getting highly imbalanced clusters.",313826.0
111230,489389.0,,nan,
111311,479362.0,,nan,
111311,479515.0,,nan,
111820,481905.0,114137.0,Thank you! Now I can merge both answers to get a complete picture of what is expected.,308637.0
111820,481916.0,114136.0,Thanks a lot. That was VERY helpful,308637.0
110776,477024.0,,nan,
110776,477034.0,,nan,
110776,477943.0,,nan,
110776,478518.0,113573.0,"I am not sure what i am doing wrong but for me , 100% is getting captured for dimension 1 and above.Any suggestion.",320103.0
110893,477623.0,,nan,
110893,477414.0,113299.0,could not convert string to float: 'Zambia' throws me this error?? Is there change to be done with the datatypes??,314313.0
110893,477414.0,113305.0,"tried with both integer and float . float is throwing an error with Zambia, integer is throwing with Afghanistan... How do i deal with it? Please help",314313.0
110893,477414.0,113303.0,Linear regression cannot be done on categorical variable. You need to convert it into integers or floats.,317689.0
110906,477850.0,,nan,
110906,477610.0,,nan,
110950,477716.0,113409.0,but we can measure these figures in one go for many Ks. will it do?,311686.0
110950,477716.0,113431.0,what all figures you mean?,318329.0
110950,477716.0,113454.0,silhouette scores and SSD...,311686.0
110950,477716.0,113506.0,ok. thanks.,311686.0
110950,477716.0,113504.0,You could capture them in a list and plot them in a single plot. the same is done in the provided notebooks as well,318329.0
111287,479215.0,113576.0,In sample content for PCA at the end it's mentioned such code. Is that for logistic regression,320103.0
111287,479215.0,113578.0,Thanks,320103.0
111287,479215.0,113577.0,Yes,310974.0
111287,479262.0,113592.0,So for doing PCA test and train split is not required and we need to continue based on original dataframe.,320103.0
111514,480819.0,,nan,
111514,480352.0,113812.0,Thanks ..rechecked - yes i have standardized the data and using the correct dataframe :(,319759.0
111514,480352.0,113813.0,Your Principal components are not correctly calculated. They're too higher (almost 10 digit number).. please check your code for PCA from the starting..,316349.0
111514,480352.0,113832.0,Yes..found the error ..thanks,319759.0
111514,480352.0,114036.0,Hi Bindu.. Even i am getting the similar sort of scree plot. Can you tell me what mistake you did? guess i did something wrong,301114.0
111107,478441.0,113458.0,"No, not after doing PCA. I am referring to the step before doing PCA in HousingCaseStudy . https://learn.upgrad.com/v/course/208/session/25598/segment/131666",301114.0
111107,479962.0,,nan,
111805,481787.0,,nan,
111805,481661.0,,nan,
111805,481666.0,,nan,
110933,477606.0,113330.0,I think we still need Xtrain and Ytrain data split to apply PCA on it.,300718.0
110933,477606.0,113336.0,Got it. Thanks.,300718.0
110933,477606.0,113527.0,there is no response variable Y here...how do you split it into train and test data?,308437.0
110955,477652.0,113354.0,"It discusses about what happens if we don't do it and how to do it. But, My question is, do we really need to do it if outliers are natural.",318329.0
110955,477652.0,113445.0,"Hi Nagaraju, In case outliers are due to data issues, they definitely are to be dropped and not be considered at latter stages. In case, outliers are natural, they are to be kept aside and then k-means is to be found. After this the outliers should be assigned to related clusters. Some can continue to be kept isolated.",334535.0
110955,478442.0,,nan,
110955,477857.0,113446.0,"Hi Vinay, Yes, outlier treatment is necessary. As noted above, in case outliers are due to data issues, they definitely are to be dropped and not be considered at latter stages. In case, outliers are natural, they are to be kept aside and then k-means is to be found. After this the outliers should be assigned to related clusters. Some can continue to be kept isolated.",334535.0
110955,477857.0,113456.0,"Hi Mangesh, Thanks for your response. Would the approach be same in case of Hierarchical Clustering also??",313826.0
110944,477618.0,113328.0,Do you mean the bar plots of means against each PC ?,318329.0
110944,477618.0,113358.0,Yep,310974.0
110944,477673.0,113356.0,I don't think so. We do clustering on the dataset obtained after PCA.,318329.0
110944,477673.0,113496.0,but then how to do it on original variables then? its not quite clear...,310509.0
109905,473939.0,112603.0,"I am aware of what happens when we dont treat outliers. But here my question was from the business point of view. If we drop outlier values, we will end up losing 25% of 167 country records which wont come into any clusters. Is that recommended here?",310511.0
109905,474398.0,112699.0,Ya facing a lot of issues,318451.0
109905,474398.0,112698.0,"Exactly my point. But if we don't do outlier treatment, the cluster are coming up pretty weird. Can some TA please confirm.",310511.0
109905,474722.0,112752.0,"Yeah, the count of countries will certainly be high at the threshold values, but it will make your clusters even better and relevant I guess.",318344.0
109905,474722.0,112748.0,"That's a good idea. But if I do this, frequency of countries will be high at the threshold values. This might end up putting bigger chunk of the records in some clusters. Nevertheless, I will try this. Thanks.",310511.0
109905,476603.0,113135.0,Done.,311686.0
109905,476603.0,113132.0,I even asked my SM to bring this to the TA'S notice. Can you repost this?,310511.0
109905,477933.0,,nan,
109905,480675.0,,nan,
111902,482160.0,,nan,
111902,482151.0,,nan,
111835,481973.0,,nan,
110341,475379.0,,nan,
110341,477934.0,,nan,
112145,483589.0,,nan,
112145,483582.0,,nan,
112145,483705.0,,nan,
112145,483826.0,,nan,
112041,482893.0,114273.0,Well answered Naseem,334535.0
112041,482893.0,114285.0,Thanks Mangesh.,302738.0
112041,482918.0,,nan,
112041,483039.0,114274.0,Well answered Ashish,334535.0
111129,478531.0,,nan,
111129,478537.0,,nan,
111129,478538.0,,nan,
111129,478539.0,,nan,
111129,478581.0,,nan,
111129,479461.0,,nan,
111129,480129.0,114162.0,Thanks Mahima... I think I am in right direction :-),304814.0
112096,483193.0,,nan,
112096,483281.0,114354.0,"Hi, After doing PCA we have got dataframe where columns are named 0-4 (for component=4). How to map them to original columns for outlier analysis and clustering. Confused. Need help.",317156.0
112096,483437.0,,nan,
111143,478587.0,113483.0,There will not be any response variables for this assignment.. The objective of this is to do the dimension reduction using PCA and do the clustering. We don't need to do the test train split as well.,316202.0
111143,478587.0,113482.0,"What can be the response variable, if I drop the country variable from the frame.",317418.0
111143,478587.0,113480.0,"Thanks Sham for the response. Does the data requires splitting for PCA , as for the clustering split process is not required.",317418.0
112046,482961.0,,nan,
112046,483034.0,,nan,
112153,483587.0,114356.0,Thanks for you response. After doing PCA got data similar as below: PC1 PC2 PC3 PC4 Feature 0 0.145085 0.527184 -0.472710 -0.536052 child_mort This need to be merged with original dataframe having country name?,317156.0
112153,483587.0,114357.0,Yes,301655.0
112153,483587.0,114380.0,"Thanks for your reply. AFter doing so, got data as below: country child_mort exports health imports income inflation life_expec total_fer gdpp PC1 PC2 PC3 PC4 0 Afghanistan -0.537758 -0.411011 -0.565038 -0.432275 -0.808245 0.157336 -1.619092 1.902882 -0.679180 0.145085 0.527184 -0.472710 -0.536052",317156.0
112153,483796.0,,nan,
111151,479841.0,113757.0,Thanks Jetendra.,307494.0
111151,478772.0,113536.0,Thanks,307494.0
111151,478665.0,113537.0,Thanks,307494.0
111151,478797.0,113535.0,"Thanks Ram, really helps..",307494.0
112335,484701.0,,nan,
112335,484672.0,,nan,
112335,485416.0,,nan,
112047,482912.0,,nan,
112047,483032.0,114341.0,Thanks,318814.0
112047,483032.0,114765.0,this worked for me thank u ashish...given u upvote..cheers,320606.0
112047,483303.0,,nan,
111205,479217.0,113584.0,"Good inputs Ram. Madhusudhan, Also ensure that you fulfill all requirements in rubrics for section Model building and evaluation https://learn.upgrad.com/v/course/208/session/25998/segment/134276",334535.0
111538,480458.0,,nan,
111538,480688.0,113874.0,Which note book are you referring to please,344598.0
111538,480891.0,,nan,
111585,480616.0,,nan,
109856,474109.0,,nan,
109856,476633.0,,nan,
109856,476683.0,113151.0,Hmmm.. saw that. How many clusters do you think is appropriate for a dataset of size 167?,310511.0
109856,476683.0,113144.0,"Hmmm.. makes sense... !! BTW , TA has responded to my post on Outlier one. Have a look.",311686.0
109856,476683.0,113165.0,i think 4-5,311686.0
109856,476683.0,113171.0,"If I go with 4-5 clusters, frequency of countries in them is highly unequal. If I go with 8-10, I am getting somewhat of a fair frequency distribution.",310511.0
109856,476683.0,113174.0,Why do we need to worry about the number of countries in a cluster? There is no rule that they count has to be same or in the same range in each cluster right? Am I missing something?,310974.0
109856,476683.0,113175.0,"No it doesnt. But I personally feel if the bulk of the records are concentrated in only a few clusters, then the clustering ain't right. There's room for breaking down the clusters further. This is my own understanding though!",310511.0
109856,476683.0,113324.0,All the entries of countries are unique. Why would you do a frequency distribution of countries?,318329.0
109856,476683.0,113339.0,"Not frequency distribution of countries, but of clusters. No of countries per cluster. I feel if we tell the NGO, that cluster# 1 is in dire need of aid, and that cluster contains half the countries in the list, thats not a good solution. Clusters should be close knit and stable.",310511.0
109856,476683.0,113355.0,Got it. Let me check the distribution of clusters :),318329.0
109856,478717.0,113508.0,They have asked us to visualize on PCs and original variables. not build models. We create PCs so that we dont have to deal with original features while model creation.,310511.0
109856,480672.0,,nan,
109856,481917.0,114174.0,Thanks,310511.0
111236,479019.0,,nan,
111236,479064.0,,nan,
111236,479366.0,,nan,
111236,479238.0,,nan,
113418,489309.0,,nan,
113418,489403.0,,nan,
111935,482191.0,,nan,
112162,483790.0,,nan,
111312,479373.0,113606.0,I got it,320687.0
111312,479334.0,,nan,
111312,479360.0,,nan,
111312,479444.0,113659.0,all columns we should do outlier treatment with quantiles 25 and 75 what we did in course. like below we did for course example RFM = RFM[(RFM.Amount >= Q1 - 1.5*IQR) & (RFM.Amount <= Q3 + 1.5*IQR)] Do we need to do for all columns except country ?,312019.0
111312,479444.0,113684.0,"Create a function to remove the outlier def remove_outlier(df_in, col_name): q1 = df_in[col_name].quantile(0.25) q3 = df_in[col_name].quantile(0.75) iqr = q3-q1 #Interquartile range fence_low = q1-1.5*iqr fence_high = q3+1.5*iqr df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)] return df_out You need to call for each column.",320687.0
111312,479444.0,114146.0,"I don't think this is a very good way of removing outliers as you are effectively calling it iteratively and each time you are impacting the mean of the subsequent call. Say 4 columns. Say you first drop outliers in column 1 -> so say row 4, col 1 is an outlier (4,1) -> you drop that - so effectively you have changed the mean or columns 2,3,4 -> then say you find (6,2) and (10,2) are outliers -> then you have affected the mean and outliers of columns 3 and 4 (and 1 too ofcourse) -> and so on; In my opinion it is best to remove outliers in one statement from the entire df rather than one column at a time. https://stackoverflow.com/questions/51879812/how-to-detect-and-remove-outliers-from-each-column-of-pandas-dataframe-at-one-go",300694.0
111312,479444.0,114315.0,Thanks Nitesh J,320687.0
111312,479444.0,114436.0,@Nitesh J I'm unable to understand the above link . Please explain it,320687.0
111300,479285.0,113662.0,Also check out query https://learn.upgrad.com/v/course/208/question/111230,334535.0
111300,479285.0,113661.0,Also check out query https://learn.upgrad.com/v/course/208/question/111230,334535.0
111300,479967.0,,nan,
111300,481048.0,114378.0,"HI Gupta.. I have same doubt.. Can you please help me , if u have an answer",311472.0
111321,479389.0,,nan,
111321,479379.0,,nan,
111318,479542.0,,nan,
111318,479380.0,,nan,
111331,479549.0,,nan,
111331,479599.0,,nan,
111331,479395.0,113610.0,I am also trying to figure this out.,313826.0
111331,479395.0,113609.0,in that case how to check the actual dimension names which the PCA results in?,310509.0
111347,479530.0,113764.0,"U mean cluster 1,2,3 are less than overall mean for child_mort?",308437.0
111347,479540.0,113763.0,"u mean to say once cluster IDs are obtained , append these IDs to original dataset containing original features and then do groupby cluster ID and calculate mean for each feature - then get insights?",308437.0
111347,479473.0,,nan,
111361,479532.0,113645.0,Thanks for the info,314313.0
111361,479532.0,113717.0,"how do we add the country back post PCA and clustering, a simple concatenation operation would work?",316036.0
111361,479543.0,,nan,
111685,481116.0,,nan,
111685,481345.0,,nan,
111685,481130.0,113958.0,Does this works like set.seed,318455.0
111685,481130.0,113962.0,Okay.. Got it.. Thanks,318455.0
111685,481130.0,113959.0,It is similar to random_state that we have used in linear / logistic reg.,313826.0
111691,481155.0,,nan,
111691,481476.0,,nan,
111716,481172.0,113974.0,"Hi Muthu Krishnan, Imputing with Mean values would definitely misinterpret the data, what about imputing with - If the outlier value is more than the Q3 value (third quartile) then impute with Q3 values and the once which are less than Q1 replace with Q1 values?",300727.0
111716,481213.0,,nan,
111716,481338.0,114025.0,"Correct. Looking at the number of queries asking for merging back outliers, want to suggest that give less weightage to this part. Focus on the PCA and k-Means part. You may do outlier mergeback and analysis at end; but focus more on PCA, k-Means and hierarchical Also, review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
111716,481338.0,114034.0,Isn't the outlier treatment to be done after PCA and identifying the principal components? Brijesh's answer suggests other way around.,313826.0
111716,481338.0,114085.0,"in the session it was mentioned to do pca first, then outliers analysis and then clustering",311254.0
111716,481338.0,114216.0,But how do we add the outliers back? Do we have to save the outliers in another Dataframe? Or is there another way?,310472.0
112213,484051.0,114419.0,"no for one cluster only , that decision is taken depending upon all variables behaviour on each cluster . we have to select one cluster out of it and look which countries are under that cluster",318732.0
112213,484051.0,114416.0,For every individual cluster ? Can u be more clear,318797.0
112213,484166.0,,nan,
112213,485507.0,,nan,
113466,489398.0,,nan,
111738,481288.0,,nan,
111738,481327.0,,nan,
111739,481325.0,114026.0,Yes. And also review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624,334535.0
111855,481836.0,,nan,
111740,481321.0,114029.0,Well answered Brijesh,334535.0
111740,481271.0,113984.0,ok. which one should be used?,300698.0
111740,481271.0,114028.0,"Well answered Muthu. Ravindra, Both are needed. Review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
111740,481260.0,114038.0,Thank you:),302738.0
111740,481260.0,114027.0,Well answered Naseem,334535.0
111416,479974.0,114023.0,But doing that is not reducing the number of columns to 5. Do we need to mandatorily apply incremental PCA to the data to get the required number of columns?,316416.0
111925,482112.0,,nan,
111925,482159.0,,nan,
111946,482320.0,,nan,
111946,482330.0,,nan,
111421,479984.0,113718.0,ok. thnk u so much.,304319.0
111421,479984.0,113716.0,So the clustering would be on the Countries with their identified PCAs correct? and not the original independent vairables as those variables would get transformed in to a lower number of PCA - correct?,316036.0
111421,479984.0,113730.0,@Prateek - Yes. clustering is based on the PCs. Get the PC DF and then do K-means or hierarchical on that.,311857.0
111965,482384.0,114205.0,"Aniruddha, Add country after clustering. Also, review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
111965,482384.0,114195.0,when we should add country is it before or after clustering? and how can you please give some suggestions .,301648.0
111968,482396.0,,nan,
111968,482466.0,,nan,
111968,482489.0,,nan,
112050,482945.0,,nan,
111408,479995.0,113737.0,"Ram, you mean changing the outliers value to the 3rd quartile bucket??",301115.0
111408,479995.0,113790.0,That is one possible way,310974.0
111408,479836.0,,nan,
111408,479896.0,,nan,
111408,480053.0,,nan,
111408,480068.0,,nan,
111408,480143.0,,nan,
111408,480884.0,,nan,
111426,479982.0,113760.0,It should still be good.,311857.0
111426,479982.0,113735.0,what would happen if incremental pca is applied to a smaller dataset. is there any side effects of it?,301114.0
112087,483155.0,114434.0,"Hi, do we need to merge the PC data set with the original data set before performing clustering?",310504.0
112087,483286.0,,nan,
111749,481352.0,,nan,
111749,481471.0,114063.0,"Hi Ayushya , once you get PCS you have to do clustering on PCS and then when clustering is done you have to merged the clusters to orignal data frame which has country and 9 columns and then visualise it am i correct or am i wrong ?",303674.0
111749,481492.0,,nan,
111749,481531.0,114108.0,I'm also looking for the same.,311004.0
112378,484916.0,,nan,
112378,485506.0,,nan,
112378,485042.0,,nan,
112008,482653.0,,nan,
112081,483140.0,114457.0,Any answer to above question.After merging got NaN value after 9 rows,317156.0
112081,483140.0,114438.0,Thanks a lot.But I am getting Nan values. How to overcome,308638.0
112081,483151.0,,nan,
111216,478924.0,,nan,
111451,480089.0,,nan,
111451,480045.0,113796.0,check the size of the PCA and the df they should have same number of rows. If not you get NAN values,311857.0
111451,480045.0,113768.0,"Thanks Pradnya, but the issue i am facing is before contactenate both the individual DFs have the row count as 97, after reset index and using concatenation the row count in the new DF becomes 137 where some have NAN values in PCAs or in country column",316036.0
111451,480045.0,113848.0,hope you are using concat with axis=1 as we need to concat column wise.. Ensure that the size of the dfs is same or else you will get NaN values.,304319.0
111731,481488.0,,nan,
111733,481257.0,,nan,
111733,481264.0,,nan,
111733,481335.0,,nan,
111733,481489.0,,nan,
111452,480087.0,,nan,
111452,480091.0,,nan,
111454,480086.0,,nan,
111454,480088.0,,nan,
111454,480197.0,,nan,
111454,480265.0,113786.0,Please see if the link below is useful for clustering https://learn.upgrad.com/v/course/208/question/111440,301121.0
111454,480265.0,113785.0,"I did silhouette_score and squared distances to calculate K on the df_train_pca which was output from PCA with 5 dimensions. It had shape of (116,5). I guess i am on track. So are you saying that post identifying optimal value of K. I need to append country back to df_train_pca and then see the clustering. How do i see the clustering i dont see the syntax anywhere in notes please",344598.0
111458,480155.0,,nan,
111169,478779.0,,nan,
111169,478784.0,,nan,
111169,482142.0,,nan,
111819,481708.0,114148.0,Using this method cluster_id are assigned alphabetically as value of index plays important role in clustering.,317689.0
111482,480205.0,113769.0,"Not all. Some Outliers may have to be deleted permanently while assigning , as TA had mentioned.",301121.0
111482,480205.0,113773.0,"Though it would be subjective, some may turn out to be obvious. For certain, not all of us can be identical.",301121.0
111482,480205.0,113772.0,How to decide those 'SOME' outliers? this could be subjective right? because while identifying outliers i used IQ range as the tool which threw some outliers.,308437.0
111482,480191.0,,nan,
111482,480238.0,113771.0,OK. But should we also ensure we get same results for both types of clustering - maybe this is a cross check that what we did is correct?,308437.0
111482,480238.0,113774.0,Yes.,311117.0
111482,480238.0,113822.0,ok done thanks,308437.0
111482,481174.0,,nan,
111506,480821.0,,nan,
111506,480579.0,113899.0,Thank You Mangesh. This was indeed helpful,318455.0
111506,480321.0,113792.0,"Well this was a generic question not pertaining to assignment. Suppose you’ve 30-40 variables n u want to know which are the ones who are making most significant changes or affecting the model accuracy, then in that case there’s no point of considering PCA instead one should go ahead with traditional (RFE) approach.",318455.0
111506,480321.0,113793.0,"I believe my answer is generic. And if you have more variables, find the ones which make sense in the context of the business problem at hand and do the interpretation I think.",310974.0
111506,480321.0,113946.0,"but how to know which variables were selected after running incrementalPCA on the dataset? the result of incremental PCA is with column ref. 0,1,2, etc. - there is no way to link it back to the original variables...this part is very confusing",310509.0
111984,482576.0,114210.0,And clustering also is done on Principal components,317514.0
111520,480391.0,,nan,
111520,480349.0,,nan,
111828,481742.0,114140.0,Akbar's answer solved my doubt. Thank you...,308637.0
111828,481742.0,114118.0,"Please elaborate... I did not understand your answer... From my understanding, outlier treatment is required to get correct values mean and sd which are required for scaling. We need to normalise data before PCA so that all factors weigh in equally and scaling is done there.",308637.0
111828,481920.0,114139.0,Thank you. That answered the question.,308637.0
112022,482764.0,114245.0,Do I add the countries to the dataset?,310472.0
112022,482764.0,114272.0,"Aaditya, yes rename as mentioned by Vinay. Other steps are needed before adding countries. Review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
112022,482764.0,114287.0,"How do I know which index belongs to which columns ? eg, after PCA, 0 is child_mort , imports, exports ? So basically How do I know which index corresponds to which column from the main dataset ?",318780.0
112022,483029.0,,nan,
111524,480689.0,113897.0,"Hi Hemant, Reassignment will be using manual/visualization techniques. Using this, to determine 1. Keep outliers as they are 2. Group outliers and label them as one or more seperate clusters 3. Merge outliers with cluster Thus in your analysis you can include the outliers in the presentation to senior management. Appropriate reasons have to be given for the same.",334535.0
111524,480689.0,113888.0,"I did the outlier treatment; the question is how do i assign it back to the clusters so formed with 'no outlier' data? Also, the link shows some graphs; how do you create those in Python? can you guide on which function will generate such graphs?",316349.0
112168,483788.0,114391.0,Thanks,318814.0
111526,480770.0,,nan,
111526,480421.0,,nan,
111526,480400.0,113821.0,"Yes, I do agree with TA verified answers for those questions. But my question here is different. I understand that we need to scale the data before PCA. My question here is specific to outliers. And I do agree that we need to scale the outlier data before we manually assign the clusterID (based on the nearest cluster centers) I need verification on the below steps For non outliers - we do a fit and transform for Scale as well as PCA For outliers - We use the same fit as non outliers and just transform for Scale and PCA.",310467.0
111546,480454.0,113836.0,Do we need to do SVD?,314313.0
111546,480451.0,113825.0,Thanks :),314313.0
111546,480451.0,113840.0,"After the clustering is done, you can merge it with the clustered data",311160.0
111546,480451.0,113837.0,When to join the country column?,314313.0
111591,480685.0,,nan,
111582,480632.0,,nan,
111583,480574.0,,nan,
111583,480595.0,,nan,
111555,480481.0,113828.0,OK this is done....but I guess we should get scatter plot of clusters on 2D? Is that what is asked for?,308437.0
111555,480637.0,,nan,
112003,482664.0,,nan,
111587,480628.0,113870.0,"Hi Vinay, I did the same. On the non outlier data did a fit and transform. Used the same fit obtained from non outlier and transformed the outlier data. Later I joined both the PCAs (outlier and non outlier). But it then shows correlation (max 0.6) in corrmatrix, which looks wrong.",310467.0
111587,480628.0,113882.0,I think that is bound to happen as we are introducing new data after initial fit_transform. Better to have it confirmed by the TA.,313826.0
111587,480628.0,113891.0,"Hi Keerthi, I am also getting a similar thing. I Think better to have to looked in by the TA to guide us further.",313826.0
111587,480628.0,113892.0,Thanks Vinay! I posted this as a new question https://learn.upgrad.com/v/course/208/question/111624,310467.0
111587,480628.0,113977.0,"Hi Vinay, If we do standardization after removal of outliers then there will be 2 means one for non outlier data and one for outliers. So standardization will also takes place separately for these two datasets. Now in the end if we want to reassign the outliers back then it will not be in synch with non outlier data right?",318579.0
111587,480628.0,113981.0,"yeah Vinay i agree, but even these tools also standardize based on mean and standard deviations. if we do this only on inlier data then mean and standard deviation of inlier data will be considered by the tool. then what about the importance of outlier data here",318579.0
111587,480628.0,113979.0,"For standardization, we should use a preprocessing function like standardscaler() or minmaxscaler(). First, we fit_transform inlier data and then just transform the outlier data.",313826.0
111587,480686.0,,nan,
111596,480662.0,114004.0,It gave the number of components as 4,302741.0
111596,480662.0,114003.0,"for me PCA is giving the components as 0,1,2... how can we arrive which columns we have to use...",302741.0
111596,480786.0,,nan,
111596,480725.0,,nan,
111917,482133.0,,nan,
111567,480589.0,113937.0,Ok. Thnk you.,304319.0
111215,478918.0,113532.0,yes. we will need to do that only to do PCA. but at the end you need to bring back the country column again to map countries to different clusters formed.,311686.0
111215,478918.0,113531.0,"yes correct. In that sense, should we drop the variable country and go ahead with fitting remaining numerical columned data into PCA?",308437.0
111215,478976.0,,nan,
111606,480783.0,113906.0,Yes..I have normalized..plz see the screeshot belowl,319759.0
111606,480683.0,,nan,
111606,480779.0,113905.0,Yes..its coming to 8.97. Not sure where i went wrong,319759.0
111606,480920.0,,nan,
111606,480938.0,,nan,
111380,479687.0,,nan,
111367,479601.0,,nan,
111313,479358.0,113620.0,how to do the last step? step 4? was it even covered in the module.??,316349.0
111313,479358.0,113789.0,Check this https://learn.upgrad.com/v/course/208/question/111230,318448.0
111313,479374.0,,nan,
111840,481959.0,,nan,
111840,482135.0,,nan,
110631,476778.0,,nan,
111773,481469.0,,nan,
111773,481698.0,114106.0,Just now checked it is not working now. Don't know why.,317991.0
112049,483121.0,,nan,
111541,480447.0,,nan,
111541,480413.0,,nan,
111624,480859.0,,nan,
111624,480970.0,,nan,
111624,480901.0,,nan,
111624,481535.0,114058.0,Any inputs on first step.,308638.0
111624,481465.0,114032.0,Please clarify the step1. Indeed there are few columns expressed as % of other columns. what does it mean absolute/non-relative/non% basis?,301114.0
111624,481721.0,114264.0,"Shouldn't we remove outliers before doing PCA, as PCA is sensitive to outliers? https://stats.stackexchange.com/questions/378751/why-is-pca-sensitive-to-outliers",301652.0
111624,481721.0,114155.0,"7. Add non PCA columns back to PCA dataset 8. Perform outlier analysis and discard outliers I am bit confused here, because we have pc1 to pcn cols with feature. But to do insights etc we need data in actual variables cols. some one help with example from pc components to keman start. we have pca components cols with feature and original has different variables with country. We need those variables like income etc to see proper on insights.",312019.0
111624,481721.0,114251.0,"That step is optional and meant to check the relationship between the original columns and the given PCs. For the outlier analysis use the given PCs only. And for the further clustering process, use the given PCs only. For example, let's say you got 3 PCs, Now create a dataframe with the original variables and the given PCs to see how the original features are represented on the Principal Components. (This is similar to the PCA Telecom Churn example where you found the relationship between the Principal Components and the original columns like phone service, partner, paperless billing,etc.) But the outlier analysis is done on the dataset (Transforming the original data to the PCs that you chose)that you obtained in Step 6. and then further clustering process works in the same fashion.",313517.0
111624,481721.0,114270.0,You can do that. But you can also do it before clustering as the methodology explained above states.,313517.0
111624,483233.0,114381.0,"There are a few steps about this process which I guess can cause problems. First of all, you have to do PCA on the dataset for this assignment. If you're just asking for an alternative approach, then this looks good. You also do need to take care when you are calculating the scoring and then scaling because unless it is forming some sort of distinct cluster boundaries (basically the scores for high-low-medium developed countries should be fairly spaced out so that you can clearly demarcate them) it would be a bit impractical to use it. Also for the first step that you mentioned, applying PCA would do exactly that only. You would be getting directions in which the entire dataset is aligned. So in those specific directions, the most important variables would have high loadings and hence you can proceed to perform Clustering on them since the most important variables' importance is already captured by the PCs.",313517.0
111624,484364.0,,nan,
110639,476296.0,,nan,
111620,480787.0,,nan,
111620,480775.0,,nan,
111620,481035.0,,nan,
111420,479966.0,,nan,
111420,479899.0,113745.0,"Team, please note, you will have to do combination of 1. outliers removal (permanently) and performing clustering 2. data imputation and performing clustering 3. temporary removal and assigning back after clustering.. You will have to give reasoning for the approaches you took.",334535.0
111420,479899.0,113794.0,Thank you Mangesh and Muthu K,310472.0
111420,479899.0,113788.0,Thanks Mangesh,318448.0
111420,479993.0,113845.0,But for this case (current dataset) it is required to remove outliars even if we are loosing 40 records.,300721.0
111420,479993.0,113860.0,Try imputing,310974.0
111417,480137.0,113779.0,But how do we do this in Python?Do we need to use supervised classification algorithm (Logistic Regression) to assign the cluster to the outliers?,304319.0
111481,480203.0,113843.0,Thank you Mangesh for additional information.,301121.0
111481,480203.0,113838.0,"Hi Madhusudan, In addition to the above, please also check out http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/",334535.0
111430,479983.0,,nan,
111888,481991.0,114452.0,by the loadings given to the variables in PCs,301115.0
111888,481991.0,114243.0,how do we know which variables have heavy influence? by looking at the PCA model?,300694.0
111888,482031.0,,nan,
111888,482094.0,114242.0,"still not clear. Say we drop 20 countries after doing PCS as outliers and then we do clustering with 5 clusters. The x and y axis are not based on the original features anymore, but instead they are based on the principal components. So how do we know whether the center of the centroids are influenced by GFPP or health or income - or a perfect combination of all of them (well that IS the case). I have worked out the GDPP mean and buckets like was said in the first live session, but how do I know if maybe child_mort doesn't have a greater influence in creating the clusters, etc",300694.0
111888,482094.0,114256.0,I got how to achieve this now .. ignore this comment ;-),300694.0
111888,482141.0,114249.0,how can they be reassigned? This has not been covered by Upgrad,316416.0
112183,483865.0,,nan,
112183,483975.0,,nan,
112180,483832.0,114402.0,and what will be variance?,315423.0
112206,484138.0,,nan,
112206,484013.0,,nan,
112208,484026.0,,nan,
112208,484028.0,,nan,
111911,482076.0,,nan,
111911,482181.0,,nan,
111911,482315.0,114319.0,Jayashree did you reolve this ? I am facing the same issue,314197.0
111911,482315.0,114333.0,"Yes. Say the dataframe after you drop the country is named as countrydf. You will apply standardscaler function and do pca on this. Next you should again convert it back to dataframe, so do pd.DataFrame(countrydf) This you need to pass the argument to Hopkins function. Do not add the country before passing it to Hopkins function.",301114.0
111911,482315.0,114336.0,No problem hope its resolved,301114.0
111911,482315.0,114335.0,Thanks Jayashree !!!,314197.0
112176,483834.0,114395.0,"HI Deval, thanks for replying , I understand that but I my question is more in line with Professor Jupiter code as he concatenate columns , can i also concatenate country column. ------------------------------------------------------ RFM_km = pd.concat([RFM, pd.Series(model_clus5.labels_)], axis=1) ------------------------------------------------------ I am confused because unable to understand how the rows will map , PCA was in numpy and then converted to Dataframe..., It will be complete mismatch",311386.0
112176,483834.0,114560.0,"Hello, You can follow the below approach to understand the concatenation process pca_train_norm1.index = pd.RangeIndex(len(pca_train_norm1.index)) pca_train_norm1_km_pca = pd.concat([pca_train_norm1, pd.Series(model_clus4.labels_)], axis=1) The data set is first re -indexed so that there is no missing values in its index and then it is simply added with the cluster series. You can display the cluster label to see that it is a series with linear index. The concatenation is based on the index of the rows. So if you have a country column which has the same index you can concatenate the country column as well in this step. Hope this helps",318374.0
112176,483991.0,114425.0,How to concatenate or map the cluster IDs for the entire data (which you would have reduced already by doing outlier treatment.),308638.0
112176,483991.0,114444.0,"Gagan is correct. This is one of the ways achieve the results. What you need to ensure is that you perform K-means for data which already went through the outlier removal. In the 2nd step, After K-Means, add the column cluster IDs to previously concatenated data frame.. This way there will be a perfect match as the rows would remain same.",301121.0
112176,483991.0,114441.0,"So what I tried is 1) concat datframe just before outlier analysis and after PCA 2) Merged the dataframes back after K-means It worked, reasonable well",311386.0
112200,483965.0,,nan,
112200,483973.0,,nan,
111633,480869.0,113907.0,"sorry, still unclear; I'm using below code which gives me a dataframe with 'No outlier' Q1 = NGO.gdpp.quantile(0.25) Q3 = NGO.gdpp.quantile(0.75) IQR = Q3 - Q1 NGO = NGO[(NGO.gdpp >= Q1 - 1.5*IQR) & (NGO.gdpp <= Q3 + 1.5*IQR)] How do i get the negation of this? 'Only outlier dataframe'?",316349.0
111633,480869.0,113969.0,Use the below code: NGO= NGO[(NGO.gdpp Q3 + 1.5*IQR)] NGO[NGO.gdpp.notnull()],311160.0
111633,480869.0,114089.0,"Thanks Hemant, I will go through the same. Seems like both of us helped each other. Please upvote if you feel my answer helped you.",319357.0
111633,480869.0,114073.0,"First, you need to rewrite this so that you don't overwrite the original dataset: New_NGO = NGO[(NGO.gdpp >= Q1 - 1.5*IQR) & (NGO.gdpp <= Q3 + 1.5*IQR)] Then you could try, either of the following: 1. outliers = NGO[~((NGO.gdpp >= Q1 - 1.5*IQR) & (NGO.gdpp <= Q3 + 1.5*IQR))] 2. outliers = NGO[~(NGO.country.isin(New_NGO.country))]",319357.0
111633,480869.0,114078.0,Great! thank you! but after yesterdays session I'm now again confused as to how to remove the outliers after PCA!,316349.0
111633,480869.0,114080.0,"Also, Harsha, your code ""NGO[(NGO.gdpp Q3 + 1.5*IQR)]"", selects everything because of the ""|"" instead of ""&""",319357.0
111633,480869.0,114082.0,"Why does one need to remove outliers after PCA ? Honestly though, I feel outlier treatment in this case too is pointless because as long as we know that the data is accurate, outliers will help us understand which countries may be the most in need. For instance if child mortality is beyond Q3 in some countries, they are the most in need. We should not remove them but group them in a cluster of countries that need help the most. It just does not make any logical sense.",319357.0
111633,480869.0,114084.0,"I too had the same question, but it was answered by Manhima yesterday in the sesssion; you may want to go through the recording.. It was a good session most of the doubts were answered. that said, I'm still not able to visualize the difference in treating outliers before/after PCA.. my bad..",316349.0
111633,480869.0,114093.0,"Yep, sure! thanks again:)",316349.0
111633,481038.0,,nan,
111633,481467.0,,nan,
111633,480875.0,113908.0,"sorry, still unclear; I'm using below code which gives me a dataframe with 'No outlier' Q1 = NGO.gdpp.quantile(0.25) Q3 = NGO.gdpp.quantile(0.75) IQR = Q3 - Q1 NGO = NGO[(NGO.gdpp >= Q1 - 1.5*IQR) & (NGO.gdpp <= Q3 + 1.5*IQR)] How do i get the negation of this? 'Only outlier dataframe'?",316349.0
111633,480875.0,113915.0,"Use ""~"" sign when you are filtering out the values, this will give you all the outlier values.",301655.0
111214,478925.0,113583.0,I'm confused; how do we map these outliers to the nearest centroid? was this taught to us in the course?,316349.0
111214,478925.0,113622.0,how to identify such outliers from each feature and filter them? any readymade functions?,308437.0
111214,478925.0,113617.0,Not sure exactly. But hope that we have some ready-made functions for measuring Euclidean distance.,311686.0
113937,,,nan,
130523,570063.0,,nan,
130524,570242.0,,nan,
130675,571339.0,,nan,
130675,570576.0,,nan,
129752,569261.0,,nan,
129752,566777.0,129490.0,"after going to HDFS browser, how to find the location of my database? Will it have my Hue username in it?",301644.0
129752,566777.0,127491.0,Thank you,301655.0
130265,568683.0,,nan,
130459,569858.0,,nan,
130459,569926.0,,nan,
130268,569406.0,,nan,
129499,565736.0,,nan,
129499,566032.0,127250.0,hey this is for the creation of the external table na,304692.0
129499,566032.0,127257.0,yes. data is already loaded in the common area,311857.0
129499,566032.0,127270.0,thanks,304692.0
129499,565661.0,127193.0,hoo thanks,304692.0
130270,568703.0,,nan,
131667,574795.0,,nan,
129522,565887.0,127285.0,"I have tried it, it didn't fix the issue",311160.0
129522,566247.0,,nan,
129522,566042.0,,nan,
135626,587061.0,131519.0,Okay so in which situation we should use ROW FORMAT DELIMITER and ROW FORMAT SERDE?,311466.0
135626,587276.0,,nan,
135626,587325.0,,nan,
130323,568945.0,,nan,
130323,569022.0,,nan,
129695,566769.0,127650.0,Didn't work for me.,318585.0
129695,566753.0,,nan,
129695,566889.0,127758.0,"Then check the error log in the job browser , that have the exact error details.",329936.0
129695,566889.0,127684.0,"if yes, then what?",318585.0
129695,567529.0,,nan,
130151,568193.0,,nan,
130151,568207.0,,nan,
129958,567256.0,,nan,
129958,568235.0,,nan,
129482,566125.0,127271.0,"If that's the case, then the columns are not mapped correctly with the dataset.",310505.0
129482,566125.0,127274.0,"If you want, you can create table with all column but the questions asked in the assignment required only the column asked. You are free to do the analysis the way you want, no marks will be deducted for that if your table has all the columns.",329936.0
129482,566125.0,127272.0,"If columns are created as per the question, then we get wrong answer for average total collection of movies.",310505.0
129482,565711.0,127211.0,Then asking to create specific columns makes no sense if we need to create columns as per the dataset. There seems to be an ambiguity in the question then. Hopefully this can be clarified.,310505.0
130221,568477.0,,nan,
130221,568544.0,,nan,
130443,569510.0,,nan,
130443,569515.0,,nan,
130443,569647.0,,nan,
130443,569596.0,,nan,
130443,569944.0,,nan,
130237,568552.0,127941.0,But I have been trying for almost half an hr...,308437.0
130237,568552.0,127945.0,Thanks...that is understandable 😆,308437.0
130237,568552.0,127944.0,Same here....its taking lot of time to execute simple query. Since tomorrow is deadline for module completion so probably many learners are accessing corestack at same time.,317991.0
130967,571909.0,,nan,
129767,566813.0,,nan,
129767,566835.0,,nan,
129776,566883.0,,nan,
129806,566973.0,,nan,
129806,566994.0,,nan,
129806,567481.0,127985.0,for some strange reason why am i getting 1105 as unique carriers?,310509.0
129806,567169.0,,nan,
129539,565894.0,,nan,
129539,565951.0,,nan,
129539,565878.0,,nan,
130896,571632.0,,nan,
130896,571779.0,,nan,
134331,583894.0,131224.0,"This is for all the participant either of one way , upgrad team can take a note for future but we are part of current session so my thinking is that if something is for current batch we can speak now instead of waiting TA. May be I am wrong but this is generic question if got answer at the platform is better.",307843.0
134331,585933.0,,nan,
129366,565095.0,,nan,
129366,565134.0,,nan,
129369,565128.0,,nan,
129369,565081.0,127078.0,thanks but we abt the 12 hrs thing,304692.0
130375,569916.0,,nan,
130376,569235.0,,nan,
130376,569240.0,128098.0,:),302742.0
129082,563683.0,,nan,
129082,563878.0,126870.0,This will be 6 weeks journey!!!,308439.0
129082,564149.0,,nan,
129082,564997.0,,nan,
129961,567269.0,,nan,
129961,568240.0,127878.0,great...,329936.0
129961,568240.0,127874.0,"the issue is resolved now, the queries are working fine and there is no error which i was getting earlier.",318851.0
129964,,,nan,
129980,567347.0,,nan,
129919,567859.0,128447.0,Thank you so much for your response!,318355.0
129919,567271.0,127757.0,Someone's sending 3 links in response to my question. If I would have to go to these many links then its better to search on google rather than asking here :D !,318355.0
129265,564586.0,126984.0,Thanks.,320103.0
129265,564622.0,127131.0,absolutely,318435.0
129265,564622.0,127109.0,"Also, instead of copying the url you can click on the openUrl icon in the Auth Url",311857.0
130238,569393.0,,nan,
129327,564876.0,,nan,
129413,566039.0,127435.0,Thanks Khusbu It worked like a charm!!,312033.0
129413,565285.0,127146.0,"Also, if it is CSV file:- then use below:- REATE EXTERNAL TABLE table name (col1 datatype1...........) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/common_folder/airlines/' tblproperties (""skip.header.line.count""=""1"");",320103.0
129413,565408.0,127150.0,"I am also getting the same error. ""There was a problem loading the samples. Insert bollywood_movies_new sample query at cursor"". How to fix the error?",307494.0
129413,565408.0,127161.0,I fixed the issue by using 'org.apache.hadoop.hive.serde2.OpenCSVSerde' as the file is a CSV file.,307494.0
129413,565408.0,127242.0,Thanks ...Issue is now resolved. Thanks a ton!,312033.0
129413,565985.0,127794.0,'org.apache.hadoop.hive.serde2.OpenCSVSerde' Use this,318770.0
129413,566437.0,,nan,
129448,565527.0,,nan,
129448,565735.0,,nan,
129448,565510.0,,nan,
129448,565438.0,,nan,
129483,565639.0,127203.0,"so, is it a third-party jar file. Or, is it provided by Hive community ?",312479.0
129631,566259.0,127320.0,"sorry pls ignore select query. create table when we do i see all columns set to string, instead different data types. how can we make it proper data type in create.",312019.0
129631,566259.0,127334.0,Drop and recreate the table.,311254.0
129631,566329.0,,nan,
129631,566340.0,,nan,
129472,565671.0,,nan,
129472,565532.0,,nan,
129693,566887.0,,nan,
129693,566756.0,,nan,
129574,566076.0,127299.0,"Hi Kumar, Can you let me know what the issue was?",311160.0
129574,566076.0,127331.0,"No need to create tables manually,use the + sign on the dashboard and import the csv file from local system.",300684.0
129574,566057.0,,nan,
129574,566043.0,,nan,
129553,566038.0,,nan,
129553,565947.0,,nan,
130369,569604.0,,nan,
129962,567265.0,,nan,
129962,567261.0,,nan,
130048,567620.0,,nan,
130048,567630.0,,nan,
130142,568185.0,,nan,
130052,567618.0,,nan,
129582,566144.0,,nan,
129586,566127.0,,nan,
129586,566118.0,,nan,
129586,566089.0,,nan,
129586,566132.0,,nan,
129586,566133.0,,nan,
129586,566240.0,127301.0,try below alter command ALTER TABLE tablename CHANGE colname colname newdatatype,320103.0
129294,565216.0,127144.0,Thanks Prabal. Issue is resolved ..,307494.0
129294,564836.0,127033.0,Thank you Kapil. It worked !,307494.0
129294,564836.0,127054.0,"I also faced same problem, you saved my day :)",317412.0
129294,564836.0,127065.0,Thanks Kapil!,318355.0
129294,564836.0,127058.0,Do not put space between -p and your password.. this might help resolve the issue -pyour_password,318822.0
129294,564977.0,,nan,
129294,565066.0,127076.0,Thanks Arihant... It worked..,307494.0
130114,567879.0,128210.0,how did you resolve it?,300734.0
130114,567879.0,128047.0,what is answer ??,318770.0
130114,567879.0,128216.0,Add the Jar file once again to resolve the issue,312093.0
130114,567879.0,128874.0,"It doesnt after some R&D I was able to digout the Solution. Its a bit camouflaged response, the problem was the Table had no Data.",300734.0
130114,567941.0,127979.0,Thanks. Was getting the same issue,314431.0
130114,567941.0,127820.0,Add the Jar file once again to resolve the issue,312093.0
130114,567941.0,128054.0,yes thanks,308644.0
130114,569810.0,129055.0,I am also facing the same issue and cannot find a solution,310509.0
131282,,,nan,
131554,574192.0,,nan,
130150,568113.0,,nan,
130150,568270.0,,nan,
132396,578055.0,,nan,
132396,578297.0,,nan,
136810,591996.0,,nan,
136810,591765.0,,nan,
130308,568947.0,,nan,
130308,569428.0,,nan,
132919,579679.0,129868.0,that is related to Bollywood data and I'm able to get Bollywood data. I'm not able to get iris data which was discussed in videos. Please let me know if you agree able to get iris data or cities data,317073.0
132919,579709.0,,nan,
130181,568280.0,,nan,
130568,570361.0,,nan,
130362,569446.0,,nan,
130362,569552.0,,nan,
130362,570358.0,,nan,
130212,568396.0,,nan,
130212,568462.0,,nan,
130212,568556.0,,nan,
130213,568467.0,,nan,
130213,568557.0,,nan,
130246,570241.0,,nan,
130246,568611.0,,nan,
130246,568704.0,128755.0,why doesnt upgrad team specify this clearly???,318791.0
130508,570010.0,128252.0,Yup that worked for me.,318448.0
130508,570010.0,128256.0,Good to hear that! :),318355.0
130508,570010.0,128473.0,Thanks. It worked.,304319.0
130508,570251.0,,nan,
130508,570353.0,,nan,
129813,567824.0,,nan,
130826,571950.0,128876.0,"Yes, I was able to perform. I'll post the tutorial here.",318355.0
130826,571950.0,128872.0,Yes . In WebConsole. Seems WebConsole is only for sqoop. Last chapter sqoop any one success in doing lab practice for import/export. pls pls clarify,312019.0
130826,572260.0,,nan,
130668,570582.0,,nan,
129942,567864.0,,nan,
130367,569564.0,128174.0,I have connected to sqoop by MySQL but latter in the lecture there is code for importing table at that moment I am getting the error,308635.0
130367,569564.0,128224.0,"In your import command you need to provide username and password, try to give password as suggested. As the error you have posted is regarding authentication issue.",329936.0
130367,570599.0,,nan,
130364,588547.0,132013.0,There should not be any space between password keyword and actual password. This resolved the issue.,307494.0
130364,570556.0,132012.0,There should not be any space between password keyword and actual password. This resolved the issue.,307494.0
130364,569185.0,132011.0,There should not be any space between password keyword and actual password. This resolved the issue.,307494.0
129765,566801.0,127549.0,"As you can see from the snapshot, there is no space between -p and the password entered.",310505.0
129765,566801.0,128449.0,It has to do with the period in the username. It must be replaced instead with underscore.,310505.0
129765,574424.0,,nan,
130365,569557.0,128171.0,Thanks. Bollywood table is working fine.,317991.0
130365,574482.0,,nan,
130160,568291.0,128233.0,Still getting the same error while export.,318756.0
130160,568291.0,128235.0,Kindly post the command and the error you are getting.,329936.0
130160,568291.0,128234.0,"Exactly followed the steps you have posted, but getting the same error.",318756.0
130664,570584.0,,nan,
130664,570783.0,128397.0,"yes, that's a common one.",317987.0
129473,566134.0,127553.0,Same error is coming even after executing the below command : sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/johnykrrish_gmail --username johnykrrish_gmail --password **** --table iris --export-dir iris_data/*,318756.0
129473,570957.0,128433.0,i used --export-dir as /common_folder/bollywood/*,308636.0
129473,570957.0,128425.0,did you use /common_folder/bollywood/bollywood.csv?,311857.0
129473,571015.0,128450.0,No data in the table :(,311857.0
129473,571015.0,128443.0,It looks fine. Plz check your Mysql table. Sometimes though Mapreduce job completes successfully it shows error but data gets inserted in table in Mysql.,308636.0
129473,567776.0,128395.0,"When i execute command ""hadoop dfs -ls"" , there is no directory as ""iris_data"" or so. Will you please explain how to upload iris_data directory and actual input data file in HDFS?",308636.0
129473,568598.0,,nan,
132853,579476.0,,nan,
130271,568714.0,,nan,
130128,568001.0,,nan,
130128,568179.0,,nan,
130306,568857.0,,nan,
130306,569052.0,,nan,
130306,568956.0,,nan,
130338,569021.0,,nan,
130338,569026.0,,nan,
130338,569072.0,,nan,
130338,569518.0,,nan,
130292,568839.0,128041.0,thanks,300721.0
131553,574185.0,,nan,
130310,568878.0,128036.0,"Tried all the steps as mentioned in these discussions, but no success. Thanks",310210.0
130310,569492.0,128173.0,"Query for creating partitioned table: This is executed first and then insert command. create external table if not exists amazon_reviews_year_month_partitioned (reviewerid string, asin string, reviewername string, helpful array , reviewtext string, overall double, summary string, unixreviewtime bigint) partitioned by (yr int, mnth int) location '/user/msg4vikas_gmail/hive/warehouse/Partition_Vikas';",310210.0
130310,569492.0,128172.0,"Query being executed: insert overwrite table amazon_reviews_year_month_partitioned partition(yr, mnth) select reviewerid, asin, reviewername, helpful, reviewtext, overall, summary, unixreviewtime, year(from_unixtime(unixreviewtime)) as yr, month(from_unixtime(unixreviewtime)) as mnth from amazon_reviews_table;",310210.0
130310,569547.0,,nan,
130805,571676.0,,nan,
130244,568572.0,,nan,
130244,568574.0,127950.0,Most welcome. Happy to help.,317991.0
130254,,,nan,
130377,569307.0,,nan,
130377,569358.0,,nan,
130377,569354.0,,nan,
130377,569468.0,,nan,
130377,569904.0,,nan,
130857,572890.0,,nan,
130857,571630.0,,nan,
130327,568933.0,128046.0,"Thanks: select length(reviewtext) as length , overall as rating corr(length, rating) from amazon_reviews_year_month_partition_orc where yr =Error while compiling statement: FAILED: ParseException line 2:0 missing EOF at 'corr' near 'rating' 2008 and mnth = 1; this returns a error :",319759.0
130327,568959.0,128055.0,You need to specify it in select clause.,317991.0
130494,569910.0,,nan,
130494,569851.0,,nan,
130333,569018.0,,nan,
130333,569085.0,,nan,
130333,569328.0,,nan,
130333,569023.0,,nan,
130689,571240.0,,nan,
130689,570858.0,,nan,
130689,570671.0,,nan,
130391,569339.0,128123.0,https://learn.upgrad.com/v/course/208/question/130114 is asking to add jar file again. tries the same but issue is not resolved.,318770.0
130391,569339.0,128124.0,https://learn.upgrad.com/v/course/208/question/130184 is asking to clear the HDFS folder. Already cleared the same. But issue is not resolved.,318770.0
130391,569339.0,128137.0,That's strange I faced the same problem but I tried combination of solution provided in these links and that solved it.,317991.0
130391,569339.0,128125.0,https://learn.upgrad.com/v/course/208/question/130072 Here TA verified an answer which propose to try after some time. Since last night I am stuck here and now re-trying in morning but issue is not fixed.,318770.0
130391,569951.0,,nan,
131392,573454.0,129021.0,So you are able to create the table and do basis selection of records......? I believe you have written below statement ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar,311861.0
131392,573454.0,129019.0,"It’s happening all the time it’s making whole progress very slow restarting again n again, I am running to create the table",300735.0
131700,575010.0,,nan,
130007,567627.0,,nan,
130007,567929.0,127873.0,you can use the web console to login,318476.0
130007,568247.0,,nan,
130007,568255.0,,nan,
130184,569077.0,128065.0,"Also instead of insert into, the command is insert overwrite as: insert overwrite table amazon_reviews_year_month_partition_samp partition(yr,mnth) select reviewerid,asin,reviewername,helpful,reviewtext,overall,summary,unixreviewtime, year(from_unixtime(unixreviewtime)) as yr, month(from_unixtime(unixreviewtime)) as mnth from amazon_reviews_table;",304319.0
130184,569077.0,129086.0,"Hi Ruchita, how do you insert data for specific years or months? Do you use another where clause to specify month is 11 and 12 and year 2017 etc?",316416.0
130184,569077.0,129321.0,in this query u can add the 'where' clause. like where (mnth=11 or mnth=12) and yr = 2017,304319.0
130184,568259.0,,nan,
130184,568281.0,,nan,
130050,567650.0,127716.0,Most of us are getting same error. It is server problem. Wait for sometime and try.,317991.0
130050,567645.0,,nan,
130050,567622.0,127765.0,Thanks. Issue was resolved after clearing the cache and cookies. But this issue is occurring occasionally due to load or other issues.,311502.0
130072,567817.0,,nan,
130072,567943.0,,nan,
130072,567978.0,127809.0,Dont see these to be the issue. Some other internal error as I'm facing this error during select query,316211.0
130072,568078.0,127892.0,"can you give any hints, what error you encountered? i am directly using the code provided in the upgrad solution code file, but still getting the error",316036.0
130134,568229.0,,nan,
130134,568264.0,,nan,
136934,592685.0,,nan,
129643,566321.0,,nan,
131607,575422.0,,nan,
131607,574522.0,,nan,
129504,565708.0,,nan,
129504,566743.0,,nan,
129504,565769.0,,nan,
130155,568111.0,127830.0,what is hashing here?,300721.0
130155,568274.0,,nan,
129416,565312.0,127135.0,"Hi Khusbu, I already ran those commands.",312756.0
129416,565393.0,127169.0,Thanks it worked.,312756.0
129416,567360.0,,nan,
131743,575851.0,,nan,
131743,577856.0,,nan,
130229,,,nan,
129726,566767.0,,nan,
129726,566737.0,127446.0,yes I had did that too,304692.0
129726,566772.0,127447.0,it is working with that folder can't create on my data base,304692.0
129726,566772.0,127448.0,then permission is not provided to read from your location.,320103.0
129726,566772.0,127449.0,but the instructions are to create our own directory na ok then,304692.0
129726,566772.0,127453.0,ok,304692.0
129726,566772.0,127452.0,create your own db and create tables inside that but read data from given HDFS path.,320103.0
129726,567049.0,,nan,
130349,569529.0,,nan,
129928,567205.0,,nan,
129928,567208.0,,nan,
130359,569211.0,,nan,
129922,567257.0,,nan,
129922,567280.0,127685.0,Didn't help me!,318585.0
129922,567845.0,,nan,
129978,567510.0,,nan,
129611,566150.0,,nan,
132120,577880.0,,nan,
132120,577260.0,129452.0,Please refer this discussion for details https://learn.upgrad.com/v/course/208/question/131722,313826.0
132120,577260.0,129450.0,Do we have to do bucketing to the orc table ?,303228.0
132120,577261.0,,nan,
130054,571454.0,,nan,
130054,567619.0,,nan,
130054,568256.0,,nan,
131549,574172.0,,nan,
131549,574312.0,,nan,
131570,574305.0,,nan,
131570,574258.0,,nan,
130447,569601.0,,nan,
130447,569603.0,,nan,
130447,569936.0,,nan,
125493,,,nan,
125781,549269.0,124342.0,ok. thnk u.,304319.0
124702,542598.0,123361.0,Then what is the best method to handle the class imbalance,318804.0
124702,542598.0,123734.0,Please follow the below link https://learn.upgrad.com/v/course/208/question/125020,344894.0
124712,542594.0,,nan,
124716,543460.0,123798.0,"Hi Paras, For Step 1 - We already have columns total_ic_mou_9 and total_og_mou_9. Do we need to consider other *_*_mou columns also to calculate total incoming and outgoing minutes of usage ? Similarly for Step 2 also. Just a clarification needed.",317991.0
124716,543460.0,123935.0,"Hi Vipul , As they have already given these columns directly , i also understand the same that we need to consider them only for churn prediction.",318427.0
124717,543649.0,,nan,
124717,544718.0,,nan,
124717,545469.0,,nan,
125512,547399.0,,nan,
125512,547562.0,124360.0,"Nitesh, one doubt. For calculating total recharge data for a month, are u multiplying the total_rech_data*avg_rech_data? or using avg_rech_data only for that month? As total_rech_data contains the count. But how can avg_rech_data be greater than the max_rech_data?",304319.0
122726,533942.0,,nan,
122726,539006.0,,nan,
124739,543432.0,123683.0,it has been said that we can drop the date columns,300694.0
124739,542894.0,,nan,
124739,545464.0,123719.0,Thank you.,311502.0
124739,545645.0,,nan,
124739,545707.0,,nan,
124759,543721.0,,nan,
124759,543096.0,,nan,
124090,544364.0,,nan,
124090,539247.0,123506.0,Yes. It means no recharge has been done.,304319.0
124090,539247.0,122799.0,How are you handling the null values in av_rech_amt_data_6 and av_rech_amt_data_7?,310511.0
124090,539247.0,122821.0,imputed them with 0,304319.0
124090,539247.0,122822.0,"I think we need to take sum of av_rech_amt_data and total_rech_amt to get the total recharge for a month. And then tk the avg of the 2. TA, please confirm.",304319.0
124090,539247.0,123022.0,I think av_rech_amt_data_6 and av_rech_amt_data_7 are only for Data recharge,318791.0
124090,539247.0,123135.0,"Yes, even i think so.",304319.0
124090,539247.0,123308.0,Ruchita are you imputing with zero for av_rech_amt_data_6 ? as well as for av_rech_amt_data_7?,303674.0
124090,540800.0,,nan,
124090,540945.0,,nan,
124090,540965.0,,nan,
124090,542723.0,,nan,
125808,548720.0,124251.0,Thanks. All the variables needs to be normalized ?,314197.0
125222,546064.0,123799.0,"3. Tag churners and remove attributes of the churn phase Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are: total_ic_mou_9 total_og_mou_9 vol_2g_mb_9 vol_3g_mb_9",318476.0
125222,546124.0,,nan,
125811,548782.0,,nan,
125811,548738.0,,nan,
125811,548951.0,,nan,
125811,548986.0,,nan,
125811,549036.0,,nan,
122941,534733.0,,nan,
122941,536566.0,,nan,
122941,536982.0,,nan,
122941,539004.0,,nan,
122941,539257.0,,nan,
125841,548898.0,,nan,
125841,548875.0,,nan,
125841,548928.0,,nan,
125841,549950.0,,nan,
125846,548901.0,,nan,
125846,548902.0,,nan,
125834,548849.0,,nan,
125834,548931.0,,nan,
126872,558268.0,,nan,
126872,552827.0,,nan,
126872,552650.0,,nan,
126872,553920.0,,nan,
125499,547663.0,124094.0,Thanks a lot.,310511.0
125499,547663.0,124089.0,Can we use SMOTE technique from imblearn package ?,317991.0
125499,547636.0,124177.0,"Hi Vipul, I used oversampling of minority class. But I guess, we can use SMOTE as well.",310511.0
125499,547636.0,124195.0,Does oversampling gives better result than using class_weight = 'balanced' ? And at what point you are performing oversampling before PCA or after ?,317991.0
125499,547636.0,124209.0,yes. Before PCA. I am getting better results with oversampling.,310511.0
125499,547636.0,124217.0,Ok thanks,317991.0
125499,547636.0,124265.0,Which function to use for oversampling ?,313691.0
125499,547707.0,,nan,
125499,547840.0,,nan,
125860,548982.0,124295.0,"Yes, correct. That is why we sometimes replicate the data or synthesize the data. Synthesizing the data will not cause duplicates",312093.0
125860,548982.0,124294.0,"If we perform before train-test split don't you think test data will likely have duplicated samples from the training set (because we have over-sampled/under-sampled the training set)? This means that testing performance wouldn't necessarily be on new, unseen data.",317991.0
125860,549311.0,,nan,
125871,549001.0,124297.0,"so its the same for all the columns then right like we have arpu,_offnet_mou and others?",301114.0
125871,549001.0,124506.0,"Yes, we should not merge the columns belonging to different months. But if you feel so, you can merge the columns of same month. Like if you need tot_vol, then you can merge 2g and 3g cols but of same month.",304319.0
125871,549037.0,,nan,
125872,548998.0,124299.0,Thanks,310009.0
125873,549019.0,124301.0,Thanks,310009.0
125873,549004.0,124300.0,Thanks,310009.0
125874,549003.0,124302.0,Thanks,310009.0
125418,546926.0,123986.0,Hi can anyone explains the significance of these columns plz.(considering only for july) total_rech_num_7 total_rech_amt_7 total_rech_data_7 av_rech_amt_data_7,301114.0
125418,546926.0,124025.0,1. Total NUMBER OF TIMES recharge done. 2. Total Amount of Recharge done. 3. Total NUMBER OF TIMES DATA recharge done. 4. Average AMOUNT OF DATA Recharge done.,317998.0
125418,546872.0,,nan,
125418,547560.0,,nan,
127035,554782.0,,nan,
127035,557082.0,,nan,
127035,557952.0,,nan,
127035,558198.0,,nan,
127035,558739.0,,nan,
127035,559590.0,,nan,
127035,553461.0,,nan,
127035,553776.0,,nan,
126118,549690.0,,nan,
126118,550230.0,,nan,
125495,547172.0,,nan,
125495,547203.0,,nan,
125495,547559.0,,nan,
125495,548034.0,,nan,
125495,548088.0,124215.0,yes you can drop circle_id,314048.0
125495,548240.0,,nan,
125915,549043.0,124314.0,What functions do we need to use for oversampling/ undersampling ?,313691.0
125915,549043.0,124306.0,But to get the whole summary of the model like p values we have to use statsmodel right?,318579.0
125915,549164.0,,nan,
125777,548461.0,124216.0,Thanks,310009.0
125813,548971.0,,nan,
125813,548823.0,,nan,
125813,548910.0,,nan,
125813,548984.0,,nan,
125813,549033.0,,nan,
123194,536631.0,122506.0,Should we consider the dataset less than 70%ile? Could you please confirm this??,318084.0
123194,536631.0,122596.0,"I am sorry, but can you clarify this. Specifically, does total_rech_amt include topup and data recharge? If not, then I don't see a variable which gives data recharge amount. There is total_rech_data, which seems to be the number of recharges and then max_rech_data, which is the maximum amount recharged.",305653.0
123194,536631.0,122747.0,I dont even see any top up recharge column.. unfortunately..,303670.0
123194,536631.0,122789.0,We need to consider the dataset greater than oe equal to 70 percentile.,304319.0
123194,536631.0,122785.0,I have used av_rech_amt_data_6 and av_rech_amt_data_7 for the data recharge. total_rec_amt is the topup recharge.,304319.0
123194,549583.0,,nan,
124174,539834.0,,nan,
125671,548116.0,,nan,
125671,547979.0,,nan,
125671,549294.0,,nan,
126003,549060.0,124326.0,Yes you can use random forest too.,317991.0
126003,549060.0,124324.0,Have a query. we need to build LR after doing PCA and we can use LR without PCA and using RFE to determine the features. Also instead of doing LR after finding PCA instead we can use random forest classifier . so any model is correct??,301114.0
126003,549507.0,,nan,
126003,550301.0,,nan,
126003,550556.0,,nan,
126003,551169.0,,nan,
126344,550624.0,,nan,
125287,546610.0,,nan,
125289,546361.0,,nan,
125289,547361.0,,nan,
125291,546413.0,,nan,
125291,546897.0,123933.0,Looked at the link but the question still stands. Can SMOTE or ver/under sampling instead of class weight or not?,310511.0
125291,546897.0,123923.0,"Hi Deepak In the above link one of the method for class imbalance handling is SMOTE. And it comes under imblearn package. In the link below https://learn.upgrad.com/v/course/208/question/124702 TA mentioned not to use imblearn package to handle class imbalance. When performing other techniques like under-sampling, using class_weight parameter to handle class imbalance the evaluation metrics gives poor result. And SMOTE gives better result. So should we use SMOTE technique or not ? Clear clarification needed here, because in lectures class imbalance handling is not covered.",317991.0
125291,547615.0,124194.0,Which value are you using for scoring parameter in GridsearchCV?,304319.0
125291,547615.0,124401.0,SMOTE improves the metrics considerably. But TA is discouraging it.,304319.0
125291,547615.0,124386.0,"Ya I think you are hinting correctly. When I had accuracy it looked good,. but using recall.. Its terrible. Think I will hv to SMOTE",300694.0
125206,546388.0,123827.0,"Hi Paras What I understand from this is to decide which model is best we should check Sensitivity, Specificity and AUC score only. No need to check for Precision and Recall. Is my understanding correct ? Or we need to calculate all these four metrics ?",317991.0
125206,547480.0,124047.0,"specificity and Recall are the same i.e Of all the People who churned how many our Model was successfully able to predict. Thus a high Recall will mean our model is good at predicting Churns which I think is the primary Objective for this Case Study. while Specificity or TNR measures the proportion of Non-Churns(0) that are correctly classified and precision is, of all the P+(1) our model predicted how many are actually true. thus a high precision classifier is capable of flagging the positives at a better rate(which i think we can bargain for a better recall for this case study), and a high specificity classifier is better at detecting the negatives.",317984.0
125206,547429.0,,nan,
125206,547527.0,124110.0,"""specificity and Recall are the same"" - Actually Sensitivity is equivalent to Recall",318438.0
125206,547527.0,124118.0,Sensitivity is same as Recall - So I think we can choose either Sensitivity/Spec or Precision/Recall,317991.0
125206,547527.0,124418.0,@Vipul we can only choose one Right? as precision/recall are inveresely proportional. Increasing Precision will result in A massive drop in Recall Score as this data is highly Imbalanced which is exact opposite to the Main Business Objective given in Problem Statement,317984.0
125206,547527.0,124417.0,"oops my bad , typo mistake as i mentioned specificity for both Precision and Recall. Editing thanks",317984.0
125303,546506.0,,nan,
125303,546708.0,,nan,
125303,546818.0,123925.0,Order is correct. Look at the purpose of EDA .,301557.0
125303,546818.0,123924.0,"If we follow the rubrics order, then data cleaning and EDA will be done on the entire data but the model will be built on filtered data. We will not get useful insights for model building if we do EDA on entire data.",313691.0
125303,547613.0,,nan,
124316,540960.0,,nan,
124316,544228.0,,nan,
124316,543594.0,,nan,
125319,546604.0,,nan,
125319,546726.0,,nan,
124215,540151.0,122958.0,"yes that was may initial view as well, but then wasn't sure how to handle null values. Putting a 0 against null dosent seem right",316036.0
124215,540151.0,123137.0,"Null value in the date col means that the customer has not recharged during that month. After extracting the day, the Nan values can be imputed by any invalid value - 0 or -1. If you think that the day of recharge is not adding any value, you can drop the date columns.",304319.0
124215,542710.0,,nan,
124215,545821.0,,nan,
126008,,,nan,
121405,527599.0,122110.0,"Hi Paras, The definition of high value customers is defined as ""Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase)."" So, we have total_rech_amt_6 and total_rech_amt_7. Finding the average of these two results in 29.9k rows as mentioned in the problem statement. total_rech_data has more than 75% missing values and I dont think is relevant from modelling perspective. If we include that to find the average and 70%ile from that, the resultant dataframe contains only 5500 odd rows. So, kindly request you to validate and confirm, if it is ok to just take total_rech_amt_6 and 7.",318084.0
121405,527599.0,123305.0,Did u find clarity over it ??,318770.0
121405,527599.0,123329.0,"Hi, Please follow the below question https://learn.upgrad.com/v/course/208/question/124154/answer/540261/comment/122923",344894.0
121405,527599.0,123810.0,Hi Paras even i am getting 5500 records when trying filter with 70th percentile. Please provide more clarity on how the filtering needs to be done.,300725.0
121405,536986.0,,nan,
126010,549110.0,,nan,
126105,550077.0,,nan,
125004,545004.0,123622.0,Thanks for confirmation,310974.0
125004,545173.0,,nan,
125020,545037.0,,nan,
125020,545120.0,,nan,
125020,545651.0,,nan,
125020,546448.0,,nan,
125014,544960.0,,nan,
125014,545463.0,,nan,
125021,545044.0,,nan,
125021,545124.0,,nan,
125021,546377.0,,nan,
123329,537140.0,,nan,
124411,541022.0,123098.0,cut_off = (raw_data.total_rech_amt_6 + raw_data.total_rech_amt_7)/2 avg = cut_off.percentile(0.7) len(data.loc[((data.total_rech_amt_6) >= avg) | ((data.total_rech_amt_7 ) >= avg) ]) I am trying something like this. But i am not getting expected number of rows.,311741.0
124411,541022.0,123213.0,"Hi , Please follow the below link https://learn.upgrad.com/v/course/208/question/124154 https://learn.upgrad.com/v/course/208/question/124433",344894.0
125546,547561.0,,nan,
125546,548031.0,,nan,
125546,548032.0,,nan,
125546,550596.0,,nan,
124425,541065.0,123080.0,Even if we consider total_rech_amt is for voice. we don't have the total for data services...?,311466.0
124425,541065.0,123093.0,"Also, lets wait for TA answer for the question posted here https://learn.upgrad.com/v/course/208/question/124433",311502.0
124425,541065.0,123079.0,"Yes, Please I'm really getting confused over here ... TA please confirm",311466.0
124425,541065.0,123092.0,I could not find any column/variable something similar to total_amt for entire service..,311502.0
124425,541290.0,123174.0,"Have you done it yourself? After filtering, how many rows are you getting?",310511.0
124425,541290.0,123679.0,After filtering I get around 29.9k records,318476.0
125053,545245.0,,nan,
125053,545460.0,123884.0,hi..say we have 30k rows and few columns have only one value repeated say 20k times..then should we drop the column or retain it?,305335.0
125053,545460.0,124061.0,"Personally, at those ratios, I would keep it",300694.0
125053,545194.0,,nan,
126217,550156.0,,nan,
126217,550307.0,,nan,
126217,550271.0,124534.0,welcome :),317998.0
126217,550271.0,124531.0,Thank you!,311218.0
124154,539586.0,122833.0,what do you mean by average of the four columns?,310511.0
124154,539586.0,122865.0,"yes , you are on the right track. ""Shayari"" means the same thing what you did , you took the avg of four, as per your code, however i think these are the two you have used : ""total_rech_amt"" and ""av_rech_amt_data"", then you have done everything right.",315560.0
124154,539586.0,122894.0,"Hi Aditya, i have one Question the total recharge amount will be -----(Data (3g,2g)+talktime) ? is this right ? or the total rech amt -is for the talktime and av_rech_amt_data is for Data (3g/4g)",303674.0
124154,539586.0,122904.0,"total_rech_amt_6 , 7 . As per my understanding.",315560.0
124154,539586.0,122910.0,total_rech_6 = topup_6 + data_6 total_rech_7 = topup_7 + data_7 for data and voice two different columns are there .consider both,315560.0
124154,539586.0,122908.0,"so the total-rech_amt 6 , 7 is the total amount(consisting of Data i,e 3g and talktime recharge) for the respective months am i right ?",303674.0
124154,540261.0,122923.0,"Couple of doubts here. In no 2., by total recharge amount, do you mean total call recharge or total call recharge + total data recharge ? In no 3, by average recharge done by customer in June and July, do you mean we have to find 2 averages separately for 2 months, or one average together for both? Help much appreciated.",310511.0
124154,540261.0,123214.0,"Hi, 1. total recharge amount for June and July --> call recharge amount (total_rech_amt)+ data recharge amount 2. For call >>> total_rech_amt & for data total_data_rech 3. one average together for both",344894.0
124154,540261.0,123341.0,"In the same context can TA please suggest if the number of recharges done both for data and calls also need to be used to calulate the average recharge in good phase so as to get the value customers. Else or otherwise please suggest if the average rechage to find the value customer means average monthly recharge. e.g. consider a case June -> 2 times recharge -> recharge values 20 and 30 ==> total monthly recharge = 50 July -> 3 times recharge -> recharge values 10,30 and 40 ==> total monthly recharge = 80 the average recharge in this case is (20+30+10+30+40)/(2+3) = 26 and average monthly recharge = (50+80)/2 = 65 Which among above should be taken as the basis for evaluating the value customers?",311729.0
124154,540261.0,124175.0,"Hi Jaikrishna, yes, I did impute the nans with zero.",310511.0
124154,540261.0,123889.0,"Hi Amit, Did you get clarity over this doubt of yours?? I also have the same doubt as you have shown with an example. @Paras Sir, please if you could clarify this doubt. thank you.",317998.0
124154,540261.0,124161.0,"Hi Rajarshi Palit, did u impute all the av_rech_amt_data,total_rech_data with zero or followed some other method to handle the null values ?",313691.0
124154,540261.0,124317.0,"HI Amit! Even, I have the same doubt. Please let me know which approach you are following? I guess, in your example, we should consider the method yielding 26 as output. Please suggest.",320683.0
124154,540802.0,123168.0,I did some math and it seems here data recharge amount is not part of total recharge amount filed...,318791.0
124154,540925.0,123061.0,Please TA comment on this!,311466.0
124154,540925.0,123212.0,the total recharge amount = total voice amount(total_rech_amt) + data amount (total_data_amt),344894.0
124154,540925.0,123218.0,Just curious to know where is the total_data_amt attribute in the input file provided.,316215.0
124154,540925.0,123219.0,Please advise Paras,316215.0
124154,540925.0,123221.0,"Paras, your response was different for the similar question. below is the link: https://learn.upgrad.com/v/course/208/question/121405",316215.0
124154,540925.0,123278.0,"Hi, You need to drive total_data_amt = number of recharges(total_rech_data) * av_rech_amt_data",344894.0
124154,540925.0,123299.0,Thank you Paras. Its worked well with this logic.,316215.0
124154,540925.0,124232.0,"as it was asked like After filtering the high-value customers, you should get about 29.9k rows. So we can go with >70th percentile value check of X. I got 29953. it should be ok .",312019.0
124154,540925.0,123300.0,Do we need to consider X is more than or equal to 70 percentile or only more than 70 percentile. With more than 70 percentile records count is 29954 and with more than or equal to 70 percentile records count 30002. Which should be considered please advise Paras.,316215.0
124154,541799.0,124119.0,"Hi Sampath did u impute all the av_rech_amt_data,total_rech_data with zero before doing the above the calculations?",301114.0
124154,542090.0,,nan,
124154,542411.0,123310.0,"Hi Sampath, i have one Question for handling the missing values total_rech_data you imputed with mean or you imputed with 1 or 0 ?",303674.0
124154,542411.0,123603.0,"Hi Sampath, This calculation yields 30001 records.Can you please suggest what could be the error.I am using >= for comparison with 70th percentile of average amount of recharge.",304696.0
124154,542411.0,123687.0,"Ayush, we are using 0 to impute blanks or nulls",316215.0
124154,542411.0,123688.0,"Shivalik, 30001 is right record number we believe.as per problem statement it is greater than equal.",316215.0
124154,542411.0,123689.0,if we use only greater than will get 29954,316215.0
124154,542411.0,123690.0,as per problem statement we should use greater than or equal,316215.0
124154,542411.0,124333.0,"hi Sampath I did the same process but still getting 5575 rows as a result instead of ~30k * telecom['avg_total_amt_6_7'] = (telecom['total_rech_amt_6'] + telecom['total_rech_amt_7'] + (telecom[""total_rech_data_6""] * telecom['av_rech_amt_data_6']) + (telecom[""total_rech_data_7""] * telecom['av_rech_amt_data_7']) )/4 * seventy_percentile_avg = telecom.avg_total_amt_6_7.quantile(0.7) * telecom[telecom.avg_total_amt_6_7 >= seventy_percentile_avg]",318772.0
124154,542724.0,123353.0,"it does not matter whether you take the 70 percentile of average or the sum, the resultant number of rows would be same.",304814.0
124154,542724.0,123434.0,but the records appearing as value customers might go some difference,311729.0
124154,542724.0,123572.0,"Why? consider 5 records having sum of 4 column values as row-1 10 row-2 15 row-3 20 row-4 25 row-5 30 60th percentile and above would give you row-3,row-4 and row-5 If you take the average, then this would become row-1 : 2.5 row-2 : 3.75 row-3 : 5 row-4 : 6.25 row-5 : 7.5 Still the 60th percentile would give you row-3,row-4,and row-5",304814.0
124154,544048.0,,nan,
126050,,,nan,
125127,545665.0,,nan,
126052,550047.0,,nan,
125142,545732.0,,nan,
125142,546040.0,124149.0,Check the below link for handling class imbalance. Good insight was provided with various techniques https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/,312093.0
125142,546359.0,124192.0,SMOTE is to be done at the time of data preparation i.e before the PCA.,304319.0
125142,546359.0,124151.0,"How did you apply SMOTE? As after PCA, I am unable to apply SMOTE technique. Can you please clarify this",312093.0
124433,541793.0,123256.0,Which column represent total data recharge amount for a month? av_rech_amt_data_6 --> is the avg recharge amount total_rech_data_6 --> total no of data recharges in a month. Do we have to multiply these 2 to get the total data recharge fora month ie. total_data_rech_amt_6 = av_rech_amt_data_6*total_data_rech_6.,310511.0
124433,541793.0,123276.0,yes,344894.0
124433,541793.0,123279.0,"Using this calculation, the number of high value customers comes to 30001, and not 29.9k",318438.0
124433,541793.0,123439.0,"Could some one explain me in details please.i could not able to understand fully. As i understand we need to make two new features 1. total recharge amount for June and July --> call recharge amount (total_rech_amt)+ data recharge amount total_rech_amnt = (using total_rech_amnt_6, total_rech_amt+7) + data recharge amount How to find make data recharge amount here. i could not see any direct features at present, we should use some other columns to find data recharge ? 2. For call >>> total_rech_amt & for data total_data_rech I didn't understand this 3. one average together for both",312019.0
124433,541793.0,123342.0,"In the same context can TA please suggest if the number of recharges done both for data and calls also need to be used to calulate the average recharge in good phase so as to get the value customers. Else or otherwise please suggest if the average rechage to find the value customer means average monthly recharge. e.g. consider a case June -> 2 times recharge -> recharge values 20 and 30 ==> total monthly recharge = 50 July -> 3 times recharge -> recharge values 10,30 and 40 ==> total monthly recharge = 80 the average recharge in this case is (20+30+10+30+40)/(2+3) = 26 and average monthly recharge = (50+80)/2 = 65 Which among above should be taken as the basis for evaluating the value customers?",311729.0
124433,541796.0,123420.0,I got the similar result for taking average of four columns.,306736.0
124433,541796.0,123422.0,The result is due to not imputing Nan Values. Corrected it.,306736.0
124433,541796.0,123579.0,how did you impute these values,311864.0
124433,541796.0,123581.0,I did it with 0s,306736.0
124433,541796.0,123584.0,Are you considering these 4 columns for calculating average of total recharge for june and july..? av_rech_amt_data_6 av_rech_amt_data_7 total_rech_amt_6 total_rech_amt_7 Please confirm once.,304696.0
124433,541796.0,123625.0,"Yes, I followed the steps mentioned in this thread. https://learn.upgrad.com/v/course/208/question/124154 total_data_amt = number of recharges(total_rech_data) * av_rech_amt_data so that updated steps: - (total_rech_amt_6 + total_rech_amt_7+total_data_amt_6+total_data_amt_7)/4 = Average of total recharges for 6th and 7th months(good phase) - 70th percentile of average for all records of ""average total recharge amount for 6th and 7th months"" - then compare the average from step1 >= 70th percentile from step1",306736.0
124433,541841.0,123280.0,"Yes, I wanted the TA to be clear on the answers he is providing",318438.0
124433,541841.0,123254.0,I think total_data_rech_amt_6 = av_rech_amt_data_6*total_data_rech_6.,310511.0
124433,542725.0,,nan,
126055,549283.0,,nan,
126055,549326.0,,nan,
125148,545823.0,,nan,
125148,546146.0,,nan,
125148,546157.0,124168.0,av_rech_amt_data_6 is required for metric creation even its more than 70% missing. We should drop ? This case study is bit confusing for data preparation for what to drop and what not to drop.,312019.0
125148,546157.0,124293.0,"both the columns are used to derive variables, for total recharge spent on data and general usage . so can we input 0 's for NA's instead of dropping the columns",302750.0
125148,546157.0,124341.0,You can impute with 0.,329936.0
126280,550223.0,,nan,
126280,550253.0,,nan,
125724,548096.0,,nan,
125724,548291.0,,nan,
126066,558311.0,,nan,
126066,549358.0,,nan,
126066,549357.0,,nan,
126066,549496.0,,nan,
126066,549956.0,124645.0,No link to the question he asked.,318756.0
126066,549956.0,124688.0,I assumed he is asking number of PC's(i.e Transformed features) as mentioned after PCA in question.,317984.0
126065,549288.0,,nan,
126065,549444.0,,nan,
126065,549360.0,124351.0,how would you classify this customer ? total_ic_mou_9 total_og_mou_9 vol_2g_mb_9 vol_3g_mb_9 Churn 0.00 1.63 0.00 0.00 1,314197.0
126065,549360.0,124352.0,If we follow all OR then in that case a person just not using outgoing service and using rest mobile data service and incoming service will also Churn. And practically that should not happen,317991.0
126065,549360.0,124354.0,"I think the spoken meaning of OR is different to the logical meaning of OR - it is a common mistake so as per your example of ""a person just not using outgoing service and using rest mobile data service and incoming service will also Churn"" -> This is not true -> here the total ( total_ic_mou_9 + total_og_mou_9 + vol_2g_mb_9 + vol_3g_mb_9 ) will be >0 so they would not have churned",300694.0
126065,549360.0,124357.0,Got it what you are trying to say. Thanks.,317991.0
126065,549360.0,124355.0,"Vikas Question: ""how would you classify this customer ? total_ic_mou_9 total_og_mou_9 vol_2g_mb_9 vol_3g_mb_9 Churn 0.00 1.63 0.00 0.00 1"" Answer: The total is >0 so they have not churned",300694.0
126065,549449.0,,nan,
124576,542305.0,,nan,
124576,542714.0,,nan,
124477,541377.0,,nan,
124588,542064.0,123277.0,This is fine,344894.0
124588,542064.0,123273.0,"Exactly my concern Vipul, is the number of rows mentioned by Shijesh fine to proceed ahead with ?",301655.0
124588,546251.0,,nan,
124594,542091.0,,nan,
124594,542204.0,123281.0,"Please read the question. I am asking about the unit of VBC, and whether it should be included in the monthly recharge cost. If no, why not?",318438.0
124594,542204.0,123509.0,"According to my understanding, The value under VBC is a part of the avg_rech_data or total_rech_amt. These two cols take care of the money spent by the customer for any kind of usage.",304319.0
124594,542204.0,123327.0,"Hi Ashish, I mean you should include while building a model and it will give predication power to your model, but do not use while filtering high-value customers Thanks",344894.0
124631,542073.0,,nan,
124631,545814.0,,nan,
124631,542103.0,,nan,
125196,545956.0,,nan,
125196,546068.0,,nan,
123645,537946.0,122627.0,That Prepaid is most commonly used does not mean it is exclusively used. Can't hurt to check,305653.0
123645,538017.0,,nan,
123645,537981.0,,nan,
125759,548314.0,124188.0,Thanks Vipul.I was thinking in the same direction,308638.0
125759,548342.0,124367.0,"We should not impute even with 0 because even if 0 means the person is not using that particular service but in actual it might be the case that the person is using the service. So, imputing with 0 would make your dataset different from the actual but unrecorded value(just as nulls here). Rather than making the dataset incorrect, dropping the columns with high null values would keep us on the safer side.",318355.0
125759,548844.0,,nan,
125759,549039.0,,nan,
125759,549292.0,,nan,
125759,549561.0,,nan,
126337,550257.0,,nan,
126337,550278.0,,nan,
126337,550302.0,,nan,
124647,542228.0,,nan,
124647,545481.0,,nan,
125763,548429.0,124214.0,I am getting better sensitivity when I use scoring = 'accuracy' than when I use scoring='recall'. Don't understand the reason.,304319.0
125763,548429.0,124304.0,we can use cut off probability to determine recall in case of log regression,318005.0
125763,548429.0,124318.0,"For LR, the sensitivity is good. i was referring to decision tree.",304319.0
125763,548429.0,124329.0,for randomforest recall and accuracy were nice with gridsearch and choosing best params,318005.0
125763,549038.0,,nan,
125766,548355.0,124199.0,Thank you.,308637.0
125766,548354.0,124250.0,No problem...always happy to help.,317991.0
125766,548354.0,124224.0,"Dear Vipul, I couldn't find it in search. So, thank you very much for the links you have shared.",308637.0
125766,548388.0,,nan,
140089,605938.0,,nan,
140589,608238.0,,nan,
140091,604739.0,,nan,
140133,605709.0,,nan,
140133,605855.0,,nan,
140153,605963.0,,nan,
142216,,,nan,
139482,601502.0,,nan,
138495,597904.0,,nan,
138495,600313.0,134045.0,Strongly Agree with you.,320689.0
138495,600313.0,134042.0,Thanks for the information. But to plan properly they should have released other modules too. This module is smaller one seems.,312019.0
138416,597477.0,,nan,
138416,599363.0,,nan,
138585,602158.0,,nan,
138585,597902.0,,nan,
139110,600809.0,,nan,
139110,600328.0,134105.0,refresh the page or app,317982.0
139110,600328.0,134111.0,module 2 has been activated,308438.0
139110,600328.0,134106.0,"Yes, they have released it now. thanks for the info :)",317998.0
139110,602155.0,134329.0,Thanks Shubham. Completed already :),311686.0
140856,609082.0,,nan,
140856,609053.0,,nan,
119208,516936.0,,nan,
118738,513502.0,,nan,
120557,522625.0,,nan,
120557,523133.0,,nan,
120557,523689.0,,nan,
119544,518363.0,,nan,
119544,521036.0,,nan,
120010,520638.0,119556.0,"i understand the method bhanu, what i am debating is the applicability of such a process for multiclass scenarios",305839.0
120010,520638.0,126130.0,"i understand the method , what i am debating is the applicability of such a process for multiclass scenarios",305839.0
120010,520638.0,127693.0,"A support vector machine is only a binary classifier: that is, it can only classify two classes at a time. Therefore, in order to classify multiple classes, i.e., more than two, it has to train two or more binary classifiers by selecting groups of classes to belong to one or the other class in a pair. This is known as “multi-class classification.” One scheme for doing this is “one-vs-one.” Two pairs of classes are selected at a time and a binary classifier trained for them. This is done for every possible pair of classes thus there are n(n-1)/2 of them where n is the total number of classes. During the classification phases, all the binary classifiers are tested. For each of them, a “win” for one class is a vote for that class. The class with the most votes wins. PS: Copied from Quora",428646.0
120010,521345.0,119659.0,good point ! 😂,305839.0
120010,527062.0,,nan,
119290,558831.0,,nan,
119600,518197.0,,nan,
119600,518550.0,,nan,
119600,518223.0,,nan,
119326,517991.0,,nan,
119333,515841.0,,nan,
119333,516190.0,,nan,
119305,515657.0,,nan,
119305,516293.0,,nan,
119305,515692.0,,nan,
119305,516477.0,,nan,
119305,515666.0,,nan,
119057,514138.0,118745.0,Thanks for the information. which one is best to use directly int or folds(KFold) ?,312019.0
119292,515708.0,,nan,
119292,516298.0,,nan,
119292,515679.0,,nan,
119323,515780.0,,nan,
119052,514101.0,118738.0,Thanks Paras.,307494.0
118662,512091.0,,nan,
118662,512165.0,,nan,
118662,513026.0,,nan,
120276,521713.0,,nan,
120682,523509.0,119980.0,"Hi As per the plot, pixel300 is being used more for recognizing the digit 0 than others and is being used least for digit 1. That is the best interpretation I could get from these. Similar kind of information, you can get from other pixels; These would be utilized more for image processing, I presume.",301121.0
120682,523509.0,119973.0,EDA is for deriving insights. So from the graphical plots you mentioned how can we make any interpretations?,301644.0
120682,523509.0,119993.0,Thanks,301644.0
119643,519509.0,,nan,
119360,516733.0,,nan,
119360,516000.0,,nan,
119360,517050.0,,nan,
119361,515995.0,,nan,
119361,516440.0,,nan,
119361,517353.0,,nan,
118910,513563.0,,nan,
118910,513879.0,,nan,
118910,515633.0,,nan,
118910,516862.0,,nan,
118551,511320.0,,nan,
118551,511321.0,118316.0,"You got it wrong. It is not ""When Y=1, then W1*X1-X2+W0 > 0"", but when the point is in class 1(red) then Y=1, and the value of W1*X1-X2+W0 would be negative.",311857.0
118551,511321.0,118332.0,I was trying to point out the discrepancy in the diagram vs the question,311857.0
118551,511323.0,,nan,
118551,511660.0,,nan,
118852,513050.0,118629.0,"Here in the statement (b−w1a−w0)∗(q−w1p−w0)<0, we have simply asked about the sign of the equation after multiplication and so a +ve term multiplied by -ve term would definately give a negative term.",318427.0
118852,513050.0,118788.0,@Deepesh There is no specific need for the multiplication but this is only for testing your concepts learned. This is what I'm able to understand.,318355.0
118852,513038.0,,nan,
118852,515107.0,,nan,
119022,513992.0,,nan,
119022,514020.0,,nan,
119022,514190.0,,nan,
119022,515012.0,,nan,
119022,517354.0,,nan,
118808,512999.0,,nan,
118808,513170.0,,nan,
118808,525524.0,,nan,
121042,526343.0,,nan,
121042,526071.0,,nan,
118663,512038.0,,nan,
118663,512032.0,118434.0,"Clustering is an unsupervised learning model. You do not have labels, what you are doing is finding inherent patterns in data and 'clustering/grouping' similar data points.",318438.0
118663,512032.0,118562.0,I did not get you. Isnt this what we do in clustering as well? Group/cluster similar data points in one group?,304814.0
118663,512032.0,118569.0,"Although, Classification and Clustering may seem very similar, there is one major difference between the two. In Classification, we already know the class or the label. For example, in Email Classification, we already know which of the mails are Spam and which are Ham.Since the label is already known, this is termed as supervised technique. On the other hand, in Clustering, we do not have the label at all. For example, in the assignment related to clustering, where in we were supposed to make clusters of countries based on various socio-economic and health parameters. The end goal was to come up with cluster of countries like Backward,Developing , Developed etc. BIt nowhere we were given the details of which countries were falling under which category. That was derived based on the various parameters. Since the label is missing, this is termed as unsupervised technique.",313826.0
118663,512032.0,118589.0,"Theoretically whatever you are saying makes sense. Clustering is unsupervised, and yes it does try to group data items of same pattern in the unknown groups (or we can call them as unknown labels). But what difference it would make if we know the clusters before (i.e. labels) and performed clustering? In Email e.g., if we try to perform Clustering instead of SVM, then shouldn't output be similar? Logically, clustering should also form the groups containing data points of same pattern. How different SVM is here?",304814.0
118663,512032.0,118598.0,"Actually you know what, when I went through Support Vector classifier, I got all the answers. This indeed is supervised learning where in we are given the labels and we do not have to group the records based on the pattern, but we have to group the records based on the given label values.",304814.0
118663,512020.0,,nan,
118663,512732.0,,nan,
118663,512896.0,,nan,
119096,514459.0,,nan,
119096,514587.0,,nan,
119096,516852.0,119268.0,"This is again going back to the mathematical formulation. Given a set of points (as below): x=np.array([ [1,5], [0.5,1.5], [1.5,4], [2,1], [3,1], [1,4], [2,0.5], [1.5,3.5], [2.5,4.5], [1,0.5] ]) y=np.array([0, 1, 0, 1, 1, 0, 1, 0, 0, 1]) plt.scatter(x[:, 0], x[:, 1], c=y, s=50, cmap='autumn'); How can we calculate the equation of the separating line manually? Can you help with the step-by-step calculations?",318438.0
119096,516852.0,119390.0,Please can you explain more about your question? As my understanding above link explains about SVM formulation,344894.0
118717,,,nan,
119239,515245.0,,nan,
119239,515983.0,,nan,
119239,516296.0,,nan,
118897,513222.0,118626.0,"yes, we need TA confirmation",320687.0
118897,513222.0,118622.0,"I too think the same, but I wanted to know is this error or what we are thinking is right",316399.0
118897,513222.0,118786.0,"@Deepesh If the value of the Y is -1 then the output of the hyperplane equation would also be negative (as the point would lie below the line if Y =-1). Hence, negative multiplied by negative would give us a positive output.",318355.0
118897,513537.0,118722.0,"This is to have a common equation. with this, we have only 1 equation representing both the situations instead of having 2 equations.",318438.0
118897,513664.0,,nan,
118897,517008.0,,nan,
120879,524713.0,,nan,
120879,524876.0,,nan,
120879,525186.0,120229.0,great :),311686.0
118893,513203.0,118619.0,Not margin error. need a formula to calculate Margin distance between closed points and hyperplane,311952.0
118893,513203.0,118621.0,https://www.toppr.com/guides/maths/three-dimensional-geometry/distance-of-a-point-from-a-plane/ This should help you,308673.0
118893,514191.0,,nan,
118893,517357.0,,nan,
138141,596955.0,133545.0,Still didn’t get the exact answer of it !,311466.0
138141,596955.0,133554.0,I didn't get the exact answer I'm looking for! please answer it as simple as possible!,311466.0
138141,596955.0,133790.0,Thanks Paras,311466.0
138141,596955.0,133771.0,We pass matrix to our model while doing traing and in svm we train the kernel. So for Kernel matrix need to be stored in memory.,344894.0
138141,599368.0,,nan,
119495,517152.0,119333.0,Got it. Thanks Shayari,318335.0
119495,517323.0,119335.0,Got it. Thanks Kanika!,318335.0
119495,517408.0,,nan,
119495,519146.0,,nan,
119495,517351.0,119332.0,Got it. Thanks Vipul,318335.0
119077,514647.0,,nan,
119077,514787.0,,nan,
119383,516147.0,119024.0,Doesn't 1 and 3 option refers to the same region?,316147.0
119383,517390.0,,nan,
118978,513770.0,,nan,
118978,514784.0,,nan,
118278,,,nan,
119492,517437.0,,nan,
118280,511311.0,,nan,
118280,512593.0,,nan,
118280,512668.0,,nan,
118855,513064.0,,nan,
118857,513120.0,,nan,
118857,513265.0,,nan,
118857,513127.0,,nan,
118857,513307.0,,nan,
118857,514417.0,119033.0,"I agree with you. In the cost video, the professor clearly states that the blue point which got incorrectly classified would have impacted the hyperplane as it would be considered as the support vector, thus creating an incorrect classifier. Hence, it appears that SVC are not completely immune but relatively immune to outliers",310509.0
118857,516303.0,,nan,
119494,516962.0,119079.0,but what if the support vector itself is the outlier?,310509.0
119494,516962.0,119083.0,support vector are the ones which are near to hyperplane. As far i understand it couldn't be an outlier.,306735.0
119494,516962.0,119084.0,suggest you check the video on cost ..that example seems to suggest such,310509.0
119494,516962.0,119088.0,"did some googling, there are some mistakes in videos and they are adding note to under it. I got this, pls have a look. outliers are away from the plane when you consider + and - . https://www.youtube.com/watch?v=WxAO6ByCvew anyhow TA could help to confirm this",306735.0
119494,518089.0,,nan,
119494,518174.0,,nan,
119494,518316.0,,nan,
118719,512616.0,118548.0,It shows object as the type. So I don't know what is the issue exactly.,310505.0
118719,512616.0,118565.0,I got my mistake. It was a different issue.,310505.0
118719,512616.0,118566.0,Different issue.,310505.0
118719,512616.0,118567.0,Thanks Revati,344894.0
118719,512616.0,118966.0,"I am getting a similar error, what did you do to solve it ?",316132.0
119176,514939.0,,nan,
119470,517194.0,,nan,
119470,517361.0,,nan,
119470,517372.0,,nan,
119069,514420.0,,nan,
119069,514700.0,,nan,
119069,514785.0,,nan,
119069,515009.0,,nan,
119069,515023.0,,nan,
119069,515622.0,,nan,
118871,514110.0,118739.0,Thank you,312019.0
119606,517936.0,119164.0,Okay! Thank you :),318355.0
119606,518168.0,,nan,
119119,515964.0,,nan,
119119,514630.0,,nan,
119119,514858.0,,nan,
119119,515316.0,,nan,
119119,515013.0,,nan,
141333,610575.0,135587.0,still not able to resolve this,315423.0
141333,610575.0,135594.0,This should solve the issue.,428646.0
141333,610575.0,135589.0,"x=x.fillna(0), before scaling",318005.0
140328,,,nan,
140341,606923.0,,nan,
140341,606479.0,,nan,
140435,607588.0,135592.0,raw data file,318770.0
140435,607588.0,135588.0,what is raw.dat here ?,315423.0
139491,601508.0,,nan,
139491,611409.0,,nan,
139491,606433.0,,nan,
140377,,,nan,
141435,,,nan,
140451,607594.0,135170.0,I am getting this error. Screenshot attached as an answer below,318335.0
140451,607594.0,135171.0,"Sorry, my bad.. try this, df_movie_features = train.pivot_table(index= 'UserId' , columns = 'movieId' , value = 'ratings' , fill_value=0 )",329936.0
140451,609231.0,,nan,
140138,611419.0,,nan,
140138,606170.0,,nan,
140380,609109.0,,nan,
140380,607573.0,,nan,
141027,609239.0,135188.0,"No, I still don't get it. What do you mean transfer the data? When we split the data using train_test_split, there shouldn't be any repetitions. What am I missing here? Please elaborate in your next answer.",310974.0
141027,609239.0,135213.0,"Yeah, even I'm confused as to why this is happening. It's understandable that the user id remains the same because same user rates multiple movies, but how can the movie ids be same after splitting in train and test.",318397.0
141027,609366.0,,nan,
141027,611392.0,,nan,
141027,610952.0,,nan,
140332,607409.0,135196.0,"Frist, thank you very much for the reply. It helped me understand the topic a bit more clearly. Apologies for the delayed reply as I was down with fever and could not accept your answer immediately. I also wish to know if there are alternate tiered architecture exists to deal with cold start. I am trying to understand if something similar to how we implement SoA transition in organisation with legacy systems:The adaptation is staged and as we mature, we move towards a perfect implementation. A product-centric recommendation is easier to implement before user-centric strategy is adapted.",308637.0
141453,611686.0,,nan,
140249,606064.0,,nan,
140249,606437.0,,nan,
140249,605734.0,,nan,
140249,607283.0,,nan,
141180,609794.0,,nan,
140725,608601.0,135175.0,Does that mean the no of the column should be equal to the no of rows for this subtraction?,315028.0
140725,608601.0,135152.0,It can be done. See this example > a = [1 2; 3 4; 5 6; 7 8] > b = [1 -1] > a - b ans = 0 3 2 5 4 7 6 9,329936.0
140725,608601.0,135142.0,Isn't it a matrix ? and matrix subtraction/Addition can happen only when they are of same dimension.,315028.0
140704,608741.0,135146.0,it's giving an error when I'm using trying to use it inside also,311466.0
140704,608741.0,135147.0,Still getting error.,310505.0
140704,608741.0,135184.0,Thanks abhigyan,311466.0
140704,608741.0,135165.0,"Sorry, my bad I have given the incomplete answer to the question. I intent to suggest this - df_movie_features1 = train.pivot_table( index='userId', columns='movieId', values='rating', fill_value=0)",329936.0
140704,609124.0,,nan,
140704,609128.0,,nan,
140704,609132.0,135149.0,Try using pivot_table ! Read about whats the difference between using Pivot and Pivot_table !,311466.0
140704,609132.0,135168.0,"You can try, as we do not have to do any aggregation here. df_movie_features1 = train.pivot_table( index='userId', columns='movieId', values='rating', fill_value=0)",329936.0
140704,609232.0,135207.0,"After using this, I am getting below error: ValueError: Unstacked DataFrame is too big, causing int32 overflow",314547.0
140970,609237.0,,nan,
141654,613337.0,,nan,
140058,605296.0,,nan,
140058,607192.0,,nan,
140058,606431.0,,nan,
140058,604484.0,,nan,
141449,612051.0,,nan,
141449,610997.0,,nan,
141975,614353.0,,nan,
141975,613330.0,,nan,
109235,471414.0,,nan,
109235,471424.0,,nan,
109235,471421.0,112097.0,Welcome..,311117.0
109235,471421.0,112096.0,Missed to notice it. Thanks.,310501.0
109235,471662.0,,nan,
109235,471449.0,,nan,
109327,471639.0,112374.0,Well explained !!,318756.0
109327,471668.0,,nan,
109327,471671.0,,nan,
109327,498536.0,,nan,
109368,471820.0,,nan,
109368,471965.0,,nan,
109237,471536.0,,nan,
110006,474304.0,,nan,
110006,474744.0,,nan,
110006,474310.0,,nan,
109145,471175.0,,nan,
109145,471176.0,,nan,
109420,472043.0,,nan,
109420,472328.0,,nan,
109152,471164.0,,nan,
109449,473048.0,,nan,
109449,472068.0,,nan,
108878,470464.0,,nan,
108878,470450.0,,nan,
108490,469263.0,,nan,
108490,469260.0,,nan,
108490,469415.0,,nan,
109512,472403.0,,nan,
109512,472397.0,,nan,
108587,469518.0,111756.0,Thank you Pradya. This helps!!,312756.0
108587,469908.0,,nan,
109586,472603.0,,nan,
109586,472618.0,,nan,
109596,472649.0,,nan,
108641,469610.0,,nan,
108641,470038.0,,nan,
108641,469630.0,,nan,
108817,470197.0,111822.0,👍🏻👍🏻,300691.0
108817,470197.0,111806.0,Thanks alot it helped me in understanding the concept. The lecture in the course was bit confusing.,310179.0
108817,471952.0,,nan,
108822,470234.0,,nan,
108822,470218.0,,nan,
108822,470284.0,,nan,
108973,470752.0,,nan,
108669,469792.0,,nan,
108669,469745.0,,nan,
108669,469787.0,111685.0,Nice article,318370.0
108669,470339.0,,nan,
109144,471128.0,,nan,
109144,471145.0,,nan,
109144,471666.0,,nan,
109144,471613.0,,nan,
109144,471951.0,,nan,
109144,471972.0,,nan,
108825,470274.0,,nan,
108825,470193.0,111726.0,It's essentially the same. It would give the same output!,312376.0
108825,470203.0,,nan,
109124,471072.0,,nan,
109600,472693.0,,nan,
109600,472718.0,,nan,
109600,473046.0,,nan,
108837,470256.0,,nan,
109386,471807.0,,nan,
109386,471833.0,,nan,
108431,468675.0,111445.0,But in clustering also we group entities together based on common attributes or features.,318078.0
108431,468833.0,,nan,
108431,469021.0,,nan,
108431,469437.0,,nan,
108431,469827.0,,nan,
108431,470168.0,,nan,
109611,472759.0,,nan,
109611,472787.0,,nan,
108808,470151.0,111787.0,"The distance between center and each data point should be found out using Euclidean formula, am i correct?",307494.0
108808,470892.0,,nan,
108839,470267.0,,nan,
108839,470254.0,,nan,
108839,471954.0,,nan,
108839,470286.0,,nan,
115642,,,nan,
115643,499247.0,,nan,
115643,500612.0,,nan,
115645,499755.0,,nan,
115645,499202.0,,nan,
115645,500171.0,,nan,
109096,471090.0,112107.0,So choosing the 3rd cluster center will be the largest distance from both cluster 1 & cluster 2 OR only largest distance from cluster 2 ? I believe that was the question..,312093.0
109096,471090.0,112216.0,"Since we have 2 clusters and the distance from the datapoints can be assigned to both clusters. However, in the second iteration there will be points which have large distance from both centers. According to the video we can say that the blue will have some large distance points and green center will also will have some large distanced points from the pool of both program will pick the datapoint which has the largest di^2. For example: Blue cluster might have distances like 2,3,6,9,12 and green cluster might have distances like 4,5,6,2,12,13. So the program will choose the datapoint from green cluster where the di^2 is 13 as new cluster. However, we can have a situation where the both clusters can be on a straight horizontal line.",318370.0
109096,470948.0,,nan,
109096,470944.0,,nan,
108925,470594.0,,nan,
108925,470640.0,,nan,
108925,470592.0,,nan,
108925,471246.0,,nan,
109687,473147.0,112484.0,Thanks Praveen,314629.0
109687,473076.0,,nan,
109009,470747.0,,nan,
109009,470733.0,,nan,
109009,471259.0,,nan,
109147,471168.0,,nan,
110015,474318.0,,nan,
108452,468864.0,,nan,
113975,491680.0,,nan,
113975,491651.0,,nan,
113975,491686.0,,nan,
113975,492302.0,,nan,
108650,469712.0,111622.0,"how did you calculate the initial cluster points (2.3,3.3) and (3.3,1.3). how did you get the x,y for the centre points",308635.0
108650,469712.0,111624.0,ok thanks,308635.0
108650,469712.0,111623.0,"put odd observations in cluster 1 and take avg, similarly put even observations in cluster 2 and take avg. u will get these centre points.",311117.0
108650,469727.0,,nan,
108650,469868.0,,nan,
110080,482400.0,,nan,
110080,474587.0,114221.0,Thanks Mangesh,301121.0
110080,474587.0,114212.0,"Hi Muthu, A correction based on inputs from one of your peers. Hopkins method is not used for finding optimum cluster. Rather a good Hopkins score implies the data is suitable for clustering.",334535.0
109153,471150.0,,nan,
111989,482723.0,,nan,
111989,482671.0,,nan,
111989,482874.0,,nan,
114777,495457.0,,nan,
114777,495622.0,,nan,
114777,495340.0,,nan,
114777,496257.0,,nan,
115647,500611.0,,nan,
115647,499479.0,,nan,
108570,469474.0,,nan,
108570,469517.0,,nan,
108570,470407.0,112000.0,so looks like clustering is one of the tools in EDA?,310509.0
108570,470407.0,112054.0,Clustering could also be considered as EDA process which help us to discover hidden patterns of interest or structure in data It can also work as a standalone tool to get the insights about the data distribution.,314183.0
108369,469158.0,,nan,
108369,468292.0,,nan,
108369,469418.0,,nan,
108369,469590.0,,nan,
108561,469425.0,,nan,
108561,469422.0,,nan,
108561,469428.0,,nan,
108561,469429.0,,nan,
108561,470559.0,,nan,
108580,469526.0,,nan,
108580,470052.0,,nan,
108580,469563.0,,nan,
108903,470523.0,,nan,
108903,470504.0,,nan,
108911,470723.0,,nan,
113271,488509.0,,nan,
113271,488514.0,115310.0,Well answered Shayari,334535.0
109378,471809.0,,nan,
109378,471842.0,,nan,
109378,471840.0,,nan,
110086,474656.0,,nan,
110086,474673.0,,nan,
109474,472190.0,,nan,
109474,472189.0,,nan,
109542,472465.0,,nan,
109542,473045.0,,nan,
109542,472544.0,,nan,
109388,471832.0,,nan,
109388,471828.0,,nan,
109388,472057.0,,nan,
109148,471117.0,,nan,
109148,471140.0,,nan,
109077,470927.0,111966.0,hope it helps.. I am using for my concepts clarifications.. :),310508.0
109603,472688.0,,nan,
109603,472749.0,,nan,
109603,472786.0,,nan,
108853,470321.0,,nan,
108853,470395.0,,nan,
108853,470359.0,,nan,
108853,470505.0,111804.0,The matrix has the distance of a point in the dataset to every point in the dataset including itself. Although distance to self is obvious - 0.,311857.0
108853,470850.0,,nan,
108853,471463.0,,nan,
108853,471674.0,,nan,
109666,473211.0,,nan,
109666,472978.0,,nan,
109666,473051.0,,nan,
109666,473053.0,,nan,
109732,473287.0,112490.0,so it means the property of the clusters can only be found out after the creation of clusters by manual inspection?,310509.0
109732,473287.0,112492.0,added a comment,310509.0
109732,473287.0,112517.0,Correct Chetan. In real life too the analytics team works with business and domain experts to analyze output from preliminary model and continuously refine the mode.,334535.0
109729,473312.0,112491.0,so in that case why cant' the cluster Id be used directly rather than the Pd Series?,310509.0
109729,473312.0,112493.0,added a comment,310509.0
109729,473312.0,112518.0,cluster id already divided the 79 to 5 clusters...so cannot it be used?,310509.0
109729,473312.0,112506.0,How do we assign the column Series of length 79 to the column Series of length 4 ?,311160.0
109668,474146.0,112654.0,"Thanks for the reply Mangesh. I see in the link that there are different scatterplots betwen all the combinations of variables. In case of 3 variables, we need 3 scatter plots. Now how do we decide on clusters? We take the first scatter plot on two variables and do the distance calculations to form say 4 clusters. Are these clusters useful or not? because we still have 2 more scatter plots to analyze. How do all these clusters formed out of each scatter plot (3) converge into agreed set of final clusters?",318007.0
109668,474146.0,112655.0,"Thanks for the reply Mangesh. I see in the link that there are different scatterplots betwen all the combinations of variables. In case of 3 variables, we need 3 scatter plots. Now how do we decide on clusters? We take the first scatter plot on two variables and do the distance calculations to form say 4 clusters. Are these clusters useful or not? because we still have 2 more scatter plots to analyze. How do all the clusters formed out of each scatter plot (3) converge into agreed set of final clusters?",318007.0
109668,473017.0,112416.0,"My question is about the scatter plot with two axes used for distance calculations and cluster formations. When I have 3 variables (frequency, recency, monitory value) , how is a 2-axes scatter plot used? When is the 3rd variable used to fit? A scatter plot only demonstrates relation between 2 variables but not 3 variables right? All the euclidean distance teaching was done using two variables. Bar graphs are after forming clusters. The confusion is about the process to form clusters when more than 2 variables are involved. I hope I am clear now.",318007.0
109702,473194.0,,nan,
109702,473105.0,,nan,
109702,473331.0,,nan,
109211,,,nan,
109216,471657.0,,nan,
109216,471364.0,,nan,
109216,471419.0,,nan,
109216,471437.0,,nan,
109216,472527.0,,nan,
112179,483833.0,,nan,
115388,498852.0,,nan,
115388,497392.0,,nan,
109349,471725.0,,nan,
109349,472887.0,,nan,
108830,470215.0,,nan,
108830,470221.0,,nan,
112264,484527.0,,nan,
112264,485673.0,,nan,
112435,485247.0,114533.0,right Harsha. I manually checked and found some index missing . well thanks for the knowledge sharing,318372.0
109324,471601.0,,nan,
109324,471646.0,,nan,
109322,471603.0,,nan,
109322,471641.0,,nan,
109328,471660.0,,nan,
109328,471635.0,,nan,
108872,470409.0,,nan,
108872,470408.0,111777.0,Links for IRQ :http://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_summarizingdata/bs704_summarizingdata7.html,318476.0
109696,473320.0,,nan,
109696,473177.0,,nan,
109351,471720.0,,nan,
109659,472970.0,,nan,
109659,472919.0,,nan,
109329,471633.0,,nan,
109329,471651.0,,nan,
108395,469117.0,,nan,
108395,469324.0,,nan,
108395,469330.0,111768.0,could you please provide me exact path. Unable to find in logical regression,317156.0
108395,469769.0,,nan,
108395,470057.0,,nan,
108395,470955.0,,nan,
109223,471363.0,,nan,
109223,471415.0,,nan,
109223,471420.0,,nan,
109223,471451.0,112150.0,You're welcome,301655.0
109223,471451.0,112147.0,Thank you Anuj.,310508.0
111686,481133.0,115897.0,"Vinay, Couldn't get the elements still.",315797.0
111686,481133.0,118702.0,"Thank you, sorry for the delay. I got it.",315797.0
111686,481133.0,115900.0,Please post the error message that you are getting. Also post the info() details of the dataframe that you are checking.,313826.0
111440,480095.0,,nan,
111440,480151.0,113949.0,Thanks a lot. It helps.,318780.0
108806,470230.0,,nan,
111857,481911.0,114138.0,Thank you but I figured this out eventually.,303085.0
111857,481877.0,,nan,
111857,481869.0,,nan,
115399,497479.0,,nan,
109455,,,nan,
114455,494525.0,,nan,
110727,476694.0,,nan,
114860,496791.0,,nan,
112456,485226.0,,nan,
115318,496826.0,116368.0,what exactly are you trying? If you give me some details then I can give you some directions,305839.0
115318,496826.0,116363.0,how did it worked in code?,318344.0
115318,496867.0,116361.0,need to be more specific,318344.0
115318,496867.0,116364.0,How can I do it in the code?,318344.0
115318,496867.0,116365.0,"In the example, data['name'] has names 'Tom', 'Nick' and 'Kate'. On encoding they are assigned values 0,1 and 2. The rest of the code is self explanatory I believe.",313826.0
115318,496867.0,116404.0,Thanks Vinay,344894.0
131744,575212.0,,nan,
131744,575224.0,,nan,
131645,574946.0,,nan,
131705,,,nan,
131754,575367.0,,nan,
131754,575280.0,,nan,
131451,573782.0,,nan,
131451,575018.0,,nan,
131451,573988.0,,nan,
131016,572069.0,,nan,
131016,572037.0,128741.0,Yes Json Serde was causing the problem. Thanks a lot.,304812.0
131016,572037.0,129158.0,json serde prob solved but all rows and columns are showing as NULL. Kindly help,308437.0
131016,572102.0,,nan,
131016,572107.0,128773.0,:) Don't forget to Upvote :P,318355.0
131436,573619.0,129171.0,Hi. Vivek.. I am getting same kind of problem.How it was resolved,308638.0
131436,573604.0,,nan,
131739,575149.0,,nan,
131739,575233.0,,nan,
131316,573202.0,128960.0,Thanks.,310210.0
130572,570363.0,,nan,
130676,575181.0,,nan,
130676,570568.0,,nan,
130999,573065.0,,nan,
130999,571989.0,,nan,
130999,571985.0,,nan,
130999,571953.0,,nan,
131399,574644.0,,nan,
131399,573491.0,,nan,
131399,573549.0,129193.0,This worked for me. Thanks Vinay!,315022.0
131727,575378.0,,nan,
131727,575157.0,,nan,
131638,575156.0,,nan,
131638,575586.0,,nan,
131691,574831.0,,nan,
131691,574844.0,,nan,
131029,573802.0,,nan,
131029,572049.0,,nan,
131029,572097.0,,nan,
131029,572399.0,,nan,
131029,572417.0,,nan,
131030,573187.0,,nan,
131030,572055.0,128727.0,Thankq,312096.0
131039,572269.0,128852.0,"We can apply a CASE statement for the bucketing/segmentation. Please check your statement once,",301648.0
131040,572117.0,,nan,
131040,572416.0,,nan,
131644,575185.0,,nan,
131644,574712.0,,nan,
131593,574392.0,129194.0,"But according to the guide lines they have given, we should use percentile_approx(DOUBLE col, p)., to get approximate value",317410.0
131593,575605.0,129227.0,Thanks,317410.0
131740,575155.0,,nan,
131740,575147.0,,nan,
131068,572804.0,,nan,
131068,572272.0,128918.0,erroneous data is data which is not as per problem statement or as per the data dictionary like passenger count cannot be zero and data where passenger count is zero is erroneous data.,320103.0
131068,572272.0,128917.0,What exactly is erroneous data here? I am confused any help will be appreciated.,307710.0
131068,572404.0,,nan,
131068,572412.0,,nan,
131817,575898.0,,nan,
131817,575633.0,,nan,
131387,573467.0,129024.0,"I am not sure I understood the response. when you say ""First you can find co- relation between passengers count and tip amount"", does it include all the passenger_count values (i.e. 1,2,3,4,5 ,7 ) ; and how do I interpret the result (it would be one positive or negative value), and how do I relate that value with solo / multiple travellers ? how is ""average"" tip related to correlation value, can you please explain?",309211.0
131387,573467.0,129027.0,"First find the correlation between passengers count and tip amount with condition both these fields as greater than zero...it will come as negative value.. Negative co-relation m eans as number of passengers increases, the tip amount decreases very slightly.",311861.0
131387,573500.0,,nan,
130787,571666.0,,nan,
130787,571783.0,,nan,
131722,575379.0,,nan,
131722,575006.0,,nan,
131293,573155.0,,nan,
131293,574197.0,,nan,
131293,573119.0,,nan,
131293,575394.0,,nan,
131103,572343.0,,nan,
131103,572407.0,,nan,
131425,575131.0,,nan,
131425,574366.0,,nan,
131116,572634.0,,nan,
131116,572396.0,,nan,
131116,572405.0,128943.0,"Thanks for the suggestion, i will try this way...",302741.0
131116,572405.0,128947.0,i tried by failed with error again,302741.0
131116,572405.0,128948.0,"create external table if not exists nyc_taxi_orc_suri (VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp,passenger_count int,trip_distance double, RatecodeID int, store_and_fwd_flag string, PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location '/common_folder/nyc_taxi_data/'; create external table if not exists nyc_taxi_part_orc_suri (VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp,passenger_count int,trip_distance double, RatecodeID int, store_and_fwd_flag string, PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) partitioned by (yr int, mnth int) stored as orc location '/user/hive/warehouse/your_partition_folder_name_orc' tblproperties (""orc.compress""=""SNAPPY""); insert into nyc_taxi_part_orc_suri partition(yr, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, year(tpep_pickup_datetime) as yr, month(tpep_pickup_datetime) as mnth from nyc_taxi_orc_suri;",302741.0
131116,572405.0,128986.0,you haven't bucketed your data that's why it's falling. There's a difference between bucketing and partitioning. To bucket data you can checkout following link. https://www.google.com/amp/s/data-flair.training/blogs/hive-partitioning-vs-bucketing/amp/,318495.0
131116,572405.0,128949.0,"while executing the insert ""Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask"" was error",302741.0
131701,574876.0,,nan,
131701,575400.0,,nan,
131723,575159.0,,nan,
130907,571571.0,,nan,
130907,571614.0,,nan,
130907,571780.0,,nan,
130907,572113.0,,nan,
130501,569947.0,,nan,
130501,570253.0,,nan,
130663,571420.0,128548.0,"No, I had already cleaned the rows having passenger count of 0 before starting my analysis, when I run Correlation I get NaN.",309211.0
130663,570977.0,128545.0,I'm facing the same problem? is your problem resolved now?,315423.0
130663,570977.0,128550.0,"Not yet ,still see the same problem with valid passenger count and valid tip amount. result of Pesrson correlation coefficient is NaN",309211.0
130663,570977.0,128625.0,kindly paste the code for calculating correlation.,329936.0
130663,570977.0,128651.0,"select Corr(( tip_amount ), ( passenger_count )) AS pearson_corr_coeff FROM harshendra_pgddsc8.nyc_yellow_taxi_orc_partitioned WHERE passenger_count = 1 and Payment_type != 2 and tip_amount > 0 -- pearson_corr_coeff -- NaN",309211.0
130663,570977.0,128985.0,"Why are you giving passenger_count = 1 ? Just calculate the correlated between tip paid and passenger count and by the value you should be able to tell if the solo travellers pay more tip or not. How can you correlate when you freeze the value of passenger_count to one particular category? That defeats the whole meaning of correlation,",310974.0
130663,570977.0,129004.0,"I am not sure I might have explained the question correctly. II think I will go with calculating Correlation( passenger_count , tip_amount) that includes all the passengers (1,2,3,4,5,6,7 , 9) .. and then calculate Correlation that ""excludes"" solo passengers. Difference of the two would provide some value. If Corr (all passengers, tip amount) - Corr (all passengers except solo, tip amount) is positive then solo passengers don't tip much (does not have impact on tip amount).... if the value is negative then solo passengers tip more ( and their non-presence is impacting the Correlation).",309211.0
130663,574370.0,,nan,
130663,570919.0,,nan,
130663,573509.0,,nan,
130505,569921.0,128228.0,"Aman, I know what is partitioning. My question is specific to which column should we consider for this assignment. I believe we need to split the column tpep_pickup_datetime into date and time and then partition by the date by days in Nov and Dec.",314730.0
130505,569921.0,128276.0,"It's upto you to decide according to the business problem, let's not disclose this as it is assignment question.",329936.0
130505,570367.0,,nan,
131885,575978.0,129229.0,"yes , incorrect query does give the error but the query did run properly on the non-partitioned table. Hence the confusion.",317149.0
131885,577833.0,,nan,
130583,570424.0,,nan,
130583,570451.0,,nan,
130583,571468.0,,nan,
130583,571377.0,129197.0,"exactly - drive gave money to passenger, which is impossible !!!",308437.0
131394,574184.0,,nan,
131394,573473.0,129058.0,hey vinay.. its working for me now. Thank You,318455.0
131394,573473.0,129020.0,Tried that.. not working,318455.0
131394,573473.0,129043.0,Strangely it worked for me. I assume you are logging out from the main corestack login page aswell.,313826.0
131394,573492.0,129025.0,Okay.. thanks Vipul,318455.0
131394,575371.0,,nan,
131150,572636.0,,nan,
131150,572609.0,,nan,
131150,575372.0,,nan,
130851,571438.0,,nan,
130851,573293.0,,nan,
131161,572669.0,128857.0,"Even if we mention line.count = ""2"" should not give any exception. Although our table will have one less record.",318355.0
131161,572642.0,,nan,
131305,573371.0,,nan,
131308,573205.0,,nan,
131308,573368.0,,nan,
130885,572750.0,,nan,
130885,571776.0,,nan,
130885,571573.0,,nan,
130885,571618.0,,nan,
130888,,,nan,
131317,,,nan,
130655,570502.0,128322.0,Not sure we should mention that or not. TA can mention same and if we can will post same,320103.0
130655,570502.0,128318.0,Can anyone suggest the total no of rows present when we create the table,307493.0
130655,570508.0,,nan,
130655,570530.0,,nan,
130655,570522.0,128325.0,That's amazing! Thanks for sharing.,318355.0
130655,570535.0,,nan,
131199,572791.0,,nan,
131199,574247.0,,nan,
131199,572808.0,,nan,
130665,570857.0,,nan,
130665,572755.0,128878.0,"I just tested out on all the columns, could not find any null values in any of the columns",318355.0
130665,572755.0,128873.0,"When i do this null value check, i could not find any table column has this null. not sure is this correct ?",312019.0
130665,570510.0,128840.0,what kind of invalid rows we have . Do we need to remove jan and october rows. how can we do null check. need some help on invalid cleanup stuff with sql.,312019.0
130688,570853.0,,nan,
130688,571336.0,,nan,
130688,570667.0,,nan,
130688,571974.0,129089.0,That is my understanding. Have you found out if this is correct?,316416.0
130853,571575.0,128603.0,"I am writing into same directory, that I have created the main table and partitioned table.",317410.0
130853,571612.0,128616.0,for me its /user/ankurjohari.1984_gmail So please your name of folder. Your's Folder name is /user/theerthu14_gmail,311861.0
130853,571612.0,128614.0,"is this the location '/user/hive/warehouse/your_partition_folder_name_orc'', if not give me folder name plz.",317410.0
130853,571785.0,,nan,
130853,572428.0,,nan,
130861,571429.0,128520.0,Central Tendency is the central or typical value for a probability distribution. It may also be called a center or location of the distribution.,318495.0
130861,571429.0,128518.0,and what do we mean by central tendency?,315423.0
130653,570453.0,,nan,
130653,570492.0,128594.0,Thank you so much for supporting everyone! But as the TA has verified Vipul's answer so we should take the datatypes as mentioned in the Assignment details.,318355.0
130653,570590.0,,nan,
130382,569342.0,128264.0,thanks!,315423.0
130382,569953.0,,nan,
130382,569965.0,,nan,
130382,569966.0,,nan,
130382,570365.0,,nan,
130709,571335.0,,nan,
130709,570862.0,128901.0,I assumed that passenger count cannot be 0 just as they informed in problem statement. it logically simplifies my analysis (there are lot many tougher questions to deal with in analysis and I hence assumed this would be the safest approach by keeping it simple),309211.0
130709,570862.0,128841.0,"Do we need to remove passenger_count 0, there may be trips while reaching trip cancelled and its some amount chrgeable. Is it really required to delete those rows passenger_count is 0. what other checks can be done ?",312019.0
131504,574274.0,129495.0,"Let data is not filled in table properly, then how output of distinct can have values higher than that obtained using max command while operating over the same table ? the reference table is same for both the queries.",318770.0
131504,573878.0,129494.0,does it matters how data is stored in hdfs while calculating min or max on any column ?,318770.0
131681,574943.0,,nan,
130679,570580.0,,nan,
130679,571342.0,,nan,
131719,574984.0,,nan,
131719,575021.0,,nan,
131752,575375.0,129192.0,"agree...after going through couple of blogs..am doing the same as mentioned above,wanted to check here if any other better approaches",301115.0
130929,571611.0,,nan,
130932,571623.0,,nan,
130932,573025.0,,nan,
130932,571777.0,,nan,
131544,574170.0,,nan,
131716,574994.0,,nan,
131716,574977.0,,nan,
131874,575950.0,,nan,
131307,573176.0,,nan,
131307,573181.0,128956.0,"but for smaller rows the data is getting inserted, only in bulk it is failing what to do in such situation...",302741.0
131307,573181.0,128957.0,"insert overwrite table nyc_taxi_part_orc_suri partition(yr, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, year(tpep_pickup_datetime) as yr, month(tpep_pickup_datetime) as mnth from nyc_taxi_orc_suri limit 10; the above query executed successfully.",302741.0
131307,573181.0,128963.0,Yes after changing the folder it worked. Thank you for the suggestion. It helped me.,302741.0
131307,573181.0,128962.0,"I am not very sure but maybe there could be some upper cap of data per folder which might not reach for 10 records. Maybe we can experiment with 100, 1000, 10000 records and see where it starts showing an error. However, did the query worked with the changes I suggested?",317987.0
131307,575384.0,,nan,
131307,573207.0,,nan,
131307,573862.0,,nan,
131746,575207.0,129202.0,Which means we should not delete rows...instead filter the right rows and transfer to orc partitioned table?,308437.0
131746,575207.0,129412.0,Right...,329936.0
131746,575397.0,,nan,
131746,575216.0,,nan,
131746,575377.0,,nan,
131411,573543.0,129041.0,"used below but still getiing blank - create external table if not exists newyork_taxi(VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count int,trip_distance double, RatecodeID int, store_and_fwd_flag char(01), PULocationID int,DOLocationID int, payment_type int,fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location '/common_folder/nyc_taxi_data/' tblproperties (""skip.header.line.count""=""1"");",318802.0
131411,573539.0,129042.0,"used below but still getiing blank - create external table if not exists newyork_taxi(VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count int,trip_distance double, RatecodeID int, store_and_fwd_flag char(01), PULocationID int,DOLocationID int, payment_type int,fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location '/common_folder/nyc_taxi_data/' tblproperties (""skip.header.line.count""=""1"");",318802.0
131411,573539.0,129045.0,Have you dropped the table and created it freshly?,313826.0
131411,573539.0,129048.0,nope. refresing the same one. do I need to drop and then create again.?,318802.0
131411,573539.0,129050.0,done. working now,318802.0
131411,573539.0,129049.0,Yes...drop the earlier table and create again. Hope this resolves the issue.,313826.0
131819,575643.0,,nan,
131312,573353.0,129102.0,"Hi Aditya, in my case, while running the query. I'm getting a timeout error which says "" 504 Gateway Time-out 504 Gateway Time-out "" Any idea? Thanks!",316416.0
131312,573353.0,129124.0,"This is network issue, kindly wait for sometime and execute your query again.",329936.0
131443,573783.0,,nan,
130735,570969.0,,nan,
130735,571331.0,,nan,
130735,571263.0,,nan,
130735,571458.0,,nan,
130735,570885.0,,nan,
130991,571900.0,,nan,
130991,572038.0,128816.0,thanks...,318723.0
130991,572038.0,129108.0,How to check the data is populated correctly or not?,320687.0
130991,572038.0,129159.0,"step 1 done data is not populated, everything is shown as NULL Kindly help",308437.0
133049,580213.0,,nan,
133049,580601.0,,nan,
133023,580210.0,,nan,
133023,580100.0,,nan,
132807,579269.0,,nan,
132807,579328.0,,nan,
131459,,,nan,
131260,572942.0,,nan,
131260,572954.0,,nan,
131260,572955.0,,nan,
131260,575389.0,,nan,
131858,575843.0,,nan,
131478,573779.0,,nan,
131187,572883.0,129101.0,"But how to find the total trips in the same query, to find the %age share?",304319.0
131187,573100.0,128924.0,Do we need to create new table with bucketing ?,312019.0
131187,573100.0,128925.0,"You need to get data from the partitioned orc table, no need to create another table.",329936.0
131187,573100.0,128928.0,How do we specify the bins while bucketing? or do we have to create 5 different tables using where clause?,304319.0
131187,573100.0,129006.0,"Found total tip_amount and different tip_amount for above ranges and each percentage share .using basic where command, with five filter commands. above it >20 last bucket. should not be >=20. = already covered in 15-20.",312019.0
131187,575387.0,,nan,
131187,574652.0,,nan,
131415,573554.0,129051.0,"i already looked at those, they are for return code 1. Mine is for return code 2 . I could not find any explanation for the same.",310509.0
131415,574513.0,129462.0,"I am using the below piece of code and cannot find any issue. Please help how to resolve create external table if not exists taxi_data_chetan1_partitioned_bucketed_orc(VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, Passenger_count int, Trip_distance double,Store_and_fwd_flag string, PULocationID int, DOLocationID int, Payment_type int, Fare_amount double, Extra double, MTA_tax double,Tip_amount double,Tolls_amount double, Improvement_surcharge double, Total_amount double) partitioned by (RateCodeID int) clustered by (PULocationID) into 4 buckets stored as orc location '/user/hive/warehouse/taxi_data_chetan1_partitioned_bucketed_orc' tblproperties (""orc.compress""=""SNAPPY""); insert overwrite table taxi_data_chetan1_partitioned_bucketed_orc partition(RateCodeID) select VendorID, tpep_pickup_datetime,tpep_dropoff_datetime, Passenger_count, Trip_distance,RatecodeID,Store_and_fwd_flag, PULocationID, DOLocationID, Payment_type, Fare_amount, Extra, MTA_tax,Tip_amount,Tolls_amount, Improvement_surcharge, Total_amount from taxi_data_chetan;",310509.0
131415,574513.0,129516.0,"Hi Chetan, Nothing wrong in your code. It is memory related issue and I have checked it is working fine on my end. You can try by giving ""Limit 100"" in your insert command. Will notify to the concerned team regarding this. meanwhile you can try removing not required tables from your database. And delete previous location in your warehouse and create new table with new partition location.",329936.0
131415,574513.0,129816.0,raise a ticket to support @ upgrad.com,329936.0
131415,574513.0,129597.0,"Hello Abhigyaan, I tried to do what you said. I deleted previous location of warehouse and created a new folder Chetan_Bihani under warehouse and rerean the command. But i am still continuing to get the same error. What should i do now? Also, when i tried to limit the insert into query to 10, it ran successfully so it is clearly a n infrastructure issue. Kindly suggest next steps as i am not able to move ahead in the assignment because of this",310509.0
131415,573692.0,,nan,
131484,573776.0,,nan,
132303,577786.0,,nan,
131489,574266.0,,nan,
131489,577802.0,,nan,
131491,573806.0,129079.0,"Hi Ankur, Thanks for the response. But can you please go through my question again? When you say bucketing do you mean 'bucketing' in general of 'bucketing' that we did using 'clustered by' statement and thus creating a new table?",311686.0
131491,575169.0,,nan,
131491,574680.0,,nan,
131525,574193.0,,nan,
131608,575163.0,,nan,
131608,575402.0,,nan,
131608,574439.0,,nan,
131608,574505.0,,nan,
131608,574441.0,,nan,
132314,577783.0,,nan,
131454,573780.0,,nan,
131454,574276.0,,nan,
131753,575278.0,,nan,
131796,575537.0,129201.0,Correct me if i am wrong. The new orc table that has been created needs to have the same columns as in the original data set. Only the data has to filtered out.,314313.0
131796,575537.0,129415.0,Correct..,329936.0
131796,575508.0,129211.0,The new orc table that has been created needs to have the same columns as in the original data set. Only the erroneous data has to be filtered out. is it like this??,314313.0
131851,575810.0,,nan,
131831,575889.0,,nan,
131831,575954.0,,nan,
131831,575734.0,,nan,
131357,574194.0,,nan,
131357,575368.0,,nan,
131357,573358.0,128988.0,Thanks for that Aditya. Can i also overwrite the existing external table by filtering all the unnecessary values and then proceed with orc partitioned table.?,301114.0
131357,573358.0,129040.0,You're welcome! I searched a bit about it & it seems that there might be some issues while overwriting an external table in Hive. For your reference: https://www.ericlin.me/2015/05/hive-insert-overwrite-does-not-remove-existing-data/ I would suggest creating an internal table in your directory to perform the operations which you can do by CREATE testTable AS SELECT * FROM externalTableName Executing the above would create an internal table in your database Let me know if you have more queries.,318355.0
131357,573366.0,128997.0,:),318495.0
131357,573366.0,128994.0,Thanks Rajat taken care.,301114.0
131357,573364.0,,nan,
130783,571068.0,,nan,
130783,571326.0,,nan,
130783,571528.0,,nan,
131361,573436.0,,nan,
131361,573426.0,,nan,
131364,,,nan,
131286,573159.0,128968.0,"If you travel or not, they have some minimum charge like 100 for 1st 4km, so even though u have travelled 0.5km u have to pay 100.",320103.0
131286,573159.0,128965.0,"it is possible but total charge for a small trip is very high,charge should be according to trip distance ,,,so i had doubt abt data",318005.0
131286,573159.0,129074.0,considering the pickup location and drop off location might be some help! for your analysis,312259.0
131286,573384.0,,nan,
131368,573791.0,,nan,
131433,573607.0,,nan,
131559,575404.0,,nan,
131559,575614.0,129404.0,"but time difference of 0 was still there in data and it is not valid ,,but not good for avg speed only",318005.0
131559,575614.0,129402.0,i had done that,318005.0
130796,571269.0,,nan,
130796,571286.0,,nan,
130796,571616.0,,nan,
130796,573997.0,,nan,
130796,571669.0,,nan,
130796,571774.0,,nan,
130796,572943.0,,nan,
130796,572395.0,,nan,
130796,573502.0,,nan,
130798,571260.0,,nan,
130798,571322.0,,nan,
130801,571168.0,,nan,
130801,571340.0,,nan,
130801,571526.0,,nan,
131622,574497.0,,nan,
131579,,,nan,
131580,574294.0,129401.0,"Kindly check the data type of the field ""tpep_pickup_datetime"".",329936.0
131580,574294.0,129125.0,"Thanks but that is not working for me not sure what is mistake with below code. insert overwrite table xx_nyc_tlc_part_info partition(yr, mth) select vendorID , tpep_pickup_datetime , tpep_dropoff_datetime , passenger_count , trip_distance , ratecodeID , store_and_fwd_flag , pulocationid , dolocationid , payment_type , fare_amount , extra , mta_tax , tip_amount , tolls_amount , improvement_surcharge , total_amount , year(from_unixtime(tpep_pickup_datetime)) as yr, month(from_unixtime(tpep_pickup_datetime)) as mth from xx_nyc_tlc_part_info; Error while compiling statement: FAILED: SemanticException [Error 10014]: Line 19:20 Wrong arguments 'tpep_pickup_datetime': No matching method for class org.apache.hadoop.hive.ql.udf.UDFFromUnixTime with (timestamp). Possible choices: _FUNC_(bigint) _FUNC_(bigint, string) _FUNC_(int) _FUNC_(int, string)",307843.0
131597,575594.0,,nan,
131597,574416.0,129129.0,"used that as well but getting null values in column. can you share one example. I tried below as well- insert into newyork_taxi_partitioned1 partition(year, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, from_unixtime(unix_timestamp(tpep_pickup_datetime),'yyyy-MM-dd') Pickup_date, from_unixtime(unix_timestamp(tpep_pickup_datetime),'HH:mm') Pickup_Time, from_unixtime(unix_timestamp(tpep_dropoff_datetime),'yyyy-MM-dd') dropoff_date, from_unixtime(unix_timestamp(tpep_dropoff_datetime),'HH:mm') dropoff_Time, year(tpep_pickup_datetime) as year, month(tpep_pickup_datetime) as mnth from newyork_taxi where passenger_count not in ('0') and below as well- insert into newyork_taxi_partitioned partition(year, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, hour(tpep_pickup_datetime) as pick_hour, minute(tpep_pickup_datetime) as pick_min, hour(tpep_dropoff_datetime) as drop_hour, minute(tpep_dropoff_datetime) as drop_min, year(tpep_pickup_datetime) as year, month(tpep_pickup_datetime) as mnth from newyork_taxi where passenger_count not in ('0')",318802.0
133347,581088.0,,nan,
133347,580877.0,,nan,
131482,573778.0,,nan,
131482,573867.0,129095.0,Np. But am not sure if its the right technique. We can wait if TA can clarify on this.,306736.0
131482,573867.0,129088.0,"For Clustered table instead of using TBLPROPERTIES (""orc.compress""=""SNAPPY""); I used TBLPROPERTIES ('transactional' = 'true') . Rest of the Syntax remain same create table( .. .. ) partitioned by (col datatype) clustered by (col name ) into 'n' buckets STORED AS ORC LOCATION '/user/ /path' TBLPROPERTIES ('transactional'='true'); Then the deletion worked.",306736.0
131482,573867.0,129153.0,"Hi Found one way. i.e to create table with both partition and clustering with orc tblproperties (""orc.compress""=""SNAPPY"") and then running alter command to set tblproperties ( 'transactional' = 'true') . It allowed to delete records.. Just updating so that 2 tables is not required.",306736.0
131482,573867.0,129092.0,yes just found that error before i could reply you had already added in comments. Thanks anyways :),301114.0
131482,573867.0,129091.0,"I didnt insert. Data was present in clustered table..To insert we have to use ""insert into"" rather than ""insert overwrite""",306736.0
131482,573867.0,129090.0,"not working for me . create table if not exists taxi_partition_clustered (VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp,Passenger_count int,Trip_distance double, RateCodeID int,Store_and_fwd_flag char(01),PULocationID int,DOLocationID int,payment_type int, Fare_amount double,Extra double, MTA_tax double,Tip_amount double,Tolls_amount double,Improvement_surcharge double, Total_amount double) partitioned by (pickupyr int, pickupmnth int) clustered by (VendorID) into 4 buckets stored as orc location '/user/hive/warehouse/jay_data_orc' TBLPROPERTIES ('transactional'='true'); insert overwrite table taxi_partition_clustered partition(pickupyr,pickupmnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, Passenger_count, Trip_distance, RateCodeID, Store_and_fwd_flag, PULocationID, DOLocationID, payment_type, Fare_amount, Extra, MTA_tax, Tip_amount, Tolls_amount, Improvement_surcharge, Total_amount, year(tpep_pickup_datetime) as pickupyr, month(tpep_pickup_datetime) as pickupmnth from taxi_partition -------------- Error while compiling statement: FAILED: SemanticException [Error 10295]: INSERT OVERWRITE not allowed on table with OutputFormat that implements AcidOutputFormat while transaction manager that supports ACID is in use how did u insert the data into clustered table..i am getting above error.",301114.0
131482,577800.0,,nan,
131891,577065.0,,nan,
131891,577001.0,,nan,
131891,577875.0,,nan,
131891,576361.0,129273.0,"for passenger count 1 it can come nan,as all data for col1 in corr(col1,col2) is 1 and no variablity",318005.0
131655,574685.0,,nan,
131859,575839.0,,nan,
131859,575852.0,,nan,
131859,575844.0,,nan,
130800,571158.0,128655.0,getting error in TEXTFILE,318780.0
130800,571266.0,,nan,
131502,574123.0,129157.0,"I did this but after i used select command ,all columns are displayed but every row and column is displayed as NULL",308437.0
131502,577803.0,,nan,
140584,607526.0,,nan,
90899,378976.0,,nan,
90899,378982.0,,nan,
90899,379804.0,,nan,
92455,389413.0,,nan,
92455,389488.0,97522.0,"Yeah Vipul, that's correct. The marks are updated because of the question which was nullified.",319721.0
90384,376838.0,,nan,
90384,376854.0,,nan,
90384,376861.0,,nan,
90384,376919.0,,nan,
90384,377326.0,,nan,
90384,376835.0,,nan,
90384,376895.0,,nan,
90413,376994.0,,nan,
90413,377007.0,,nan,
90180,379062.0,,nan,
90180,375711.0,,nan,
90180,375861.0,,nan,
90180,375762.0,,nan,
90180,376320.0,,nan,
90180,376945.0,95421.0,same with me too..,319876.0
90180,377112.0,,nan,
90180,379609.0,,nan,
90719,378092.0,,nan,
90719,378085.0,,nan,
90719,378350.0,,nan,
88943,368773.0,93319.0,"Since its a graded question, I am not sure if I wanna try out before confirmation.",318329.0
88943,368833.0,93321.0,We might have to hear back from some TA to confirm on that then.,318329.0
88943,368833.0,93642.0,Should I ignore and consider it as ',318329.0
88943,371197.0,93913.0,I just ignored it and considered it as string and attempted the question and it was fine. I moved ahead.,318329.0
88943,371197.0,94004.0,"copy & paste any of the option of the Quiz#1 in Python Notebook and you'll see the issue below is an example ipl18.loc [0:3, [‘Team’, ‘Points’]] The single quotes around Team & Points are actually some different encoding which cannot be noticed by naked eye.",306248.0
90478,377396.0,95433.0,"It is good to have ""Select all applied options"".",311006.0
90478,377453.0,,nan,
90478,377446.0,,nan,
90478,377421.0,,nan,
90478,377521.0,,nan,
90478,377926.0,,nan,
91033,379995.0,,nan,
91033,380060.0,,nan,
91033,380153.0,,nan,
91033,380653.0,,nan,
91033,380676.0,,nan,
90592,377921.0,,nan,
92360,389419.0,,nan,
92360,388948.0,,nan,
92360,388986.0,,nan,
92360,389087.0,,nan,
92360,389112.0,,nan,
89782,373012.0,,nan,
89782,373026.0,,nan,
89782,373175.0,,nan,
89782,373007.0,,nan,
89782,373696.0,,nan,
89782,374382.0,,nan,
89782,374464.0,95520.0,but this is not that case. In this module there is no 0!. It's a formation of array,317073.0
90975,379558.0,,nan,
90975,379570.0,,nan,
90975,379563.0,,nan,
90975,379572.0,,nan,
90975,379597.0,,nan,
90975,380650.0,96152.0,No It is NOT !,306735.0
90985,379625.0,,nan,
90985,379650.0,,nan,
90985,379768.0,,nan,
90081,375987.0,95742.0,yes,313228.0
90081,376571.0,,nan,
90081,389130.0,97084.0,"Hi Karthik, stay assured that this question isn't considered while calculating the quiz scores. We have nullified this for the complete batch and you can now see the revised scores.",301619.0
90789,378362.0,,nan,
90789,379094.0,,nan,
89410,372065.0,,nan,
89410,371960.0,94112.0,Seems you are facing the same issue,311952.0
89410,371960.0,94625.0,I think it is due to alphabetical order of columns,318009.0
89410,370916.0,,nan,
89410,370917.0,93832.0,I think. Jupyter is giving index to columns alphabetically in my case,311952.0
89410,370917.0,93830.0,Why i am getting wrongly. I did not understand.,311952.0
89410,371139.0,,nan,
89410,371196.0,93898.0,"Hi, When I use 0 as column index in ipl18.iloc[0:4,0] I’m getting Against column data instead of Team column data",311952.0
89410,371196.0,94139.0,I got it incorrect on first attempt due to this ambiguity. Please look into it and devise a reversal in marks deduction for those who got it wrong.,318756.0
89410,371196.0,94006.0,"Hi @Anshul Roy, In the provided .ipynb file the dataframes are created using the dictionaries. When you use the dictionary to create a datafarme the headers get alphabetically sorted and indexes get changed. To avoid that You avoid that you need to pass extra parameter ""columns"" to preserve the header indexes. This parameter is not in the ipynb file and it is leading to ambiguity for the answering the graded Ques#1 Extract top four teams. Hope you this will make it clear and you will do something about it for those who got the answers incorrect.",306248.0
89410,371227.0,,nan,
89410,371469.0,,nan,
89410,370913.0,93818.0,I want to retrieve the team column with first 4 rows,311952.0
89410,370913.0,93819.0,what's the index of team column?,318329.0
89410,370913.0,93820.0,Actually for me 0 is index for agents in Upgrad given notebook but for upgrad they are getting team column. Can u download ipl18 data python notebook and try pandas graded questions,311952.0
89410,370913.0,93823.0,I just checked that the index of team column is 0 unless you have set the team as index.,318329.0
89410,370913.0,93827.0,you can print ipl18.columns to print the order of the columns and use indexes accordingly. I'm not sure why the order for changed for you while I Team is first column for me.,318329.0
89410,370913.0,94358.0,Not resolved,311952.0
89410,370913.0,93828.0,"ipl18 = pd.DataFrame({'Team': ['SRH', 'CSK', 'KKR', 'RR', 'MI', 'RCB', 'KXIP', 'DD'], 'Matches': [14, 14, 14, 14, 14, 14, 14, 14], 'Won': [9, 9, 8, 7, 6, 6, 6, 5], 'Lost': [5, 5, 6, 7, 8, 8, 8, 9], 'Tied': [0, 0, 0, 0, 0, 0, 0, 0], 'N/R': [0, 0, 0, 0, 0, 0, 0, 0], 'Points': [18, 18, 16, 14, 12, 12, 12, 10], 'NRR': [0.284, 0.253, -0.070, -0.250, 0.317, 0.129, -0.502, -0.222], 'For': [2230, 2488, 2363, 2130, 2380, 2322, 2210, 2297], 'Against': [2193, 2433, 2425, 2141, 2282, 2383, 2259, 2304]}, index = range(1,9) ) ​ ipl18 Against For Lost Matches N/R NRR Points Team Tied Won 1 2193 2230 5 14 0 0.284 18 SRH 0 9 2 2433 2488 5 14 0 0.253 18 CSK 0 9 3 2425 2363 6 14 0 -0.070 16 KKR 0 8 4 2141 2130 7 14 0 -0.250 14 RR 0 7 5 2282 2380 8 14 0 0.317 12 MI 0 6 6 2383 2322 8 14 0 0.129 12 RCB 0 6 7 2259 2210 8 14 0 -0.502 12 KXIP 0 6 8 2304 2297 9 14 0 -0.222 10 DD 0 5 ipl18.iloc[0:4,[0]] Against 1 2193 2 2433 3 2425 4 2141 See my output from Python Notebook",311952.0
89410,373172.0,,nan,
89410,375091.0,,nan,
89410,379503.0,,nan,
90028,374854.0,,nan,
90028,374848.0,,nan,
90028,374850.0,,nan,
90363,376715.0,,nan,
90363,376714.0,,nan,
90363,376802.0,,nan,
90363,376855.0,,nan,
90857,378675.0,,nan,
90857,378683.0,,nan,
90857,378695.0,,nan,
90857,378736.0,,nan,
90857,379107.0,,nan,
90857,379815.0,,nan,
90857,380661.0,,nan,
90930,379164.0,,nan,
90930,380711.0,,nan,
90930,381262.0,,nan,
91045,380073.0,,nan,
91045,380235.0,,nan,
91045,380442.0,,nan,
91045,380656.0,,nan,
91045,380797.0,,nan,
91570,383329.0,96224.0,:),317998.0
91570,383329.0,96223.0,"Thanks Harshit! Yes, let's see what the officials say.",318355.0
91570,383463.0,96262.0,Sushmitha Thank you so much for the answer ! Sounds cool!,318355.0
91368,381925.0,,nan,
91368,382201.0,,nan,
91368,382083.0,,nan,
90152,375546.0,,nan,
90152,375552.0,94830.0,You are welcome,318368.0
90152,375624.0,,nan,
90152,375539.0,94831.0,You are welcome :),310511.0
90152,375539.0,94829.0,well explained Thanks Rajarshi,320687.0
90212,375779.0,,nan,
90212,375849.0,,nan,
90212,375804.0,,nan,
90212,376010.0,,nan,
90122,375323.0,94795.0,"The following coding question has a video solution, the link to which will be present in the sample solution. The sample solution will appear once you've successfully submitted the code. that one",305804.0
90122,375382.0,94813.0,For some of the questions video solution and video links are available. Once you click on sample solution you have to check whether there is link for videos in comments or not like shown in screenshot.,317991.0
90122,375382.0,94812.0,"for which questions, video links are available?",318429.0
90122,377567.0,,nan,
89440,371075.0,93866.0,https://www.datacamp.com/community/tutorials/python-numpy-tutorial https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html,320195.0
89440,371075.0,93873.0,Thank you,319302.0
89440,371129.0,,nan,
89440,371193.0,,nan,
89440,371338.0,,nan,
89490,371295.0,,nan,
89490,371304.0,,nan,
89490,371300.0,,nan,
89490,371305.0,94916.0,"Its correct. In lecture professor says starting itself he explains , how we retrieve elements in the list the same we can use in the arrays",305804.0
89490,372540.0,,nan,
89490,374497.0,,nan,
90424,377082.0,,nan,
90424,377085.0,,nan,
90424,377084.0,95107.0,"as they have clearly given for even integer, the question is irrelevant but FYI it won't be working for odd integers as they have used integer division method during multiplication.",318372.0
90424,377084.0,95108.0,"dear, for me its working- Check it in jupyter notebook.. n = int(input()) import numpy as np a=np.ones((n,n),dtype=np.int) a[1:-1,1:-1]=0 print(a)",305129.0
90424,377084.0,95113.0,"yes it seems that UPGRAD solution is the same way, but took time to solve.. happy learning..",305129.0
90424,377084.0,95112.0,"Abhishek thanks for your input code. ""UPGRAD code"" to be in-specific. My code is working fine for even as well as odd integers. Cheers.",318372.0
88791,367844.0,93110.0,I am not facing any issue in opening the file separately. The issue is in the fact that the image doesnt get loaded inside the notebook.,310511.0
88791,367844.0,93111.0,It is opening in my notebook.,314183.0
88791,367844.0,93113.0,What is the location of your image file?,310511.0
88791,367844.0,93121.0,Might be browser issue or it might be some settings issue in the notebook itself for opening the image files. Try browsing the notebook options.,307488.0
88791,367844.0,93116.0,"Able to open the images in the notebook. Location is same as that of "" 1_NumPy_Basics.ipynb"" . All files are located under ""/ Downloads / Introduction to NumPy"".",307488.0
88791,367844.0,93119.0,Can this be a browser issue for me?,310511.0
88791,367851.0,93115.0,Tried. Doesn't help. There's no issue with the image file. It opens in a picture editor.,310511.0
88791,367856.0,93117.0,No. Nothing. Any other web image is loading from the notebook. But it doesnt if its a local file.,310511.0
88791,367856.0,93122.0,Posted the screenshots,310511.0
88791,367856.0,93120.0,Did you check the cell type? Can you post the screenshot of the cell and message its showing?,318329.0
88791,367863.0,,nan,
88791,367894.0,,nan,
88791,367895.0,,nan,
88791,367973.0,,nan,
88791,367975.0,,nan,
88791,367957.0,93343.0,This didnt help.,310511.0
88791,368002.0,,nan,
88791,368068.0,,nan,
88791,367857.0,93342.0,This didnt help.,310511.0
88791,367857.0,93351.0,1. Your .ipynb file and image file should remain on the same directory. Paths are relative 2. See if restarting kernel helps,306248.0
89503,372001.0,,nan,
89503,371347.0,,nan,
89503,371388.0,,nan,
89503,371433.0,,nan,
89503,371572.0,,nan,
89522,371481.0,,nan,
89522,371527.0,,nan,
89522,371654.0,,nan,
88827,368141.0,,nan,
88827,368106.0,,nan,
88827,368043.0,,nan,
88827,368062.0,,nan,
88827,368072.0,,nan,
88827,368148.0,,nan,
88827,368250.0,,nan,
88827,368403.0,,nan,
88827,368816.0,,nan,
88827,368942.0,,nan,
88827,369450.0,,nan,
88827,369255.0,,nan,
88841,368181.0,,nan,
88841,368166.0,93183.0,This was a ungraded assignment. That is why I posted the solution.,310511.0
88841,368166.0,93182.0,Please refrain from posting solutions. Instead just post hints or approach.,317689.0
88841,368161.0,,nan,
88841,368190.0,,nan,
88841,368186.0,,nan,
88841,368149.0,,nan,
88841,368124.0,,nan,
88841,368245.0,,nan,
88841,368314.0,,nan,
88841,368328.0,,nan,
88841,368379.0,,nan,
88841,369466.0,,nan,
88841,370104.0,,nan,
88841,370466.0,,nan,
88856,368254.0,,nan,
88856,368252.0,,nan,
88856,368276.0,,nan,
88856,368270.0,,nan,
88856,368277.0,,nan,
88856,368425.0,,nan,
88856,368544.0,,nan,
88856,368777.0,,nan,
88856,369081.0,,nan,
88856,369083.0,,nan,
88856,369085.0,,nan,
88856,369211.0,,nan,
88856,369459.0,,nan,
88856,369499.0,,nan,
88856,370758.0,,nan,
88856,368291.0,,nan,
88856,371307.0,,nan,
88856,371719.0,,nan,
90376,376884.0,,nan,
90376,376772.0,,nan,
90376,376776.0,,nan,
90376,378541.0,,nan,
90376,387251.0,,nan,
88862,368294.0,,nan,
88862,368299.0,,nan,
88862,368313.0,,nan,
89665,372231.0,,nan,
89665,372369.0,,nan,
89665,372542.0,,nan,
90444,377208.0,,nan,
90444,377206.0,,nan,
90444,449155.0,,nan,
90444,377209.0,,nan,
88861,368301.0,,nan,
88861,368309.0,,nan,
88861,368306.0,,nan,
88861,368308.0,,nan,
88861,368303.0,,nan,
88861,369119.0,,nan,
88861,369479.0,,nan,
88861,375526.0,,nan,
88868,368390.0,,nan,
88868,368404.0,,nan,
88868,368465.0,,nan,
88868,368385.0,,nan,
88884,368449.0,,nan,
88884,368430.0,,nan,
89531,371549.0,,nan,
89531,371564.0,,nan,
89531,371641.0,,nan,
89531,371772.0,,nan,
89531,371555.0,,nan,
89532,371553.0,94001.0,"yeah, thanks to you. i hadnt noticed that. :)",317998.0
89532,371553.0,94000.0,:),313826.0
89532,371562.0,94005.0,"Had it been a list, you would have given separately. a = [[1,2,3,4],[5,6,7,8],[9,10,11,12]] print(a[:][0])",310511.0
89532,371554.0,,nan,
89532,371563.0,,nan,
89532,371639.0,,nan,
89532,371643.0,,nan,
89532,372040.0,,nan,
89532,372777.0,,nan,
89533,371861.0,,nan,
89533,371559.0,,nan,
89533,371637.0,,nan,
89533,371565.0,,nan,
89534,371632.0,,nan,
89534,372083.0,,nan,
89534,371570.0,,nan,
89534,371571.0,,nan,
89534,371575.0,,nan,
89534,371647.0,,nan,
89534,371979.0,,nan,
89534,372739.0,,nan,
89534,372789.0,,nan,
89534,373256.0,,nan,
89536,371630.0,,nan,
89536,371591.0,,nan,
89536,371650.0,,nan,
89536,371713.0,,nan,
90660,378031.0,,nan,
90660,378027.0,,nan,
90660,377997.0,95274.0,"Thanks , noted will start reading from today immediately",319969.0
90660,377997.0,95413.0,Thanks Vipul . is there any paid video lectures which I can buy fro me . Because Im in durban,319969.0
90660,377997.0,95412.0,w3 school site is very good . can we have such kind of video lecture from PGDDS for beginners . if we once get those things then Im 100% sure I can crack Python any course,319969.0
90660,378009.0,95264.0,You can start with this link https://www.tutorialspoint.com/python/python_overview.htm,317991.0
90660,378016.0,,nan,
90660,378020.0,,nan,
90660,378101.0,,nan,
90660,378247.0,,nan,
90660,378529.0,95431.0,thanks Pratik,319969.0
89548,371804.0,,nan,
89548,372072.0,,nan,
89548,371732.0,94430.0,"It is not working in case we pass ""n"" as an odd number",307830.0
89548,371732.0,94431.0,It is already mentioned in the question that inputs will be even,318358.0
89548,371732.0,94438.0,np,318358.0
89548,371732.0,94437.0,"Yep , missed it .. Thanks (Y)",307830.0
89548,371973.0,,nan,
89548,371975.0,,nan,
89548,371976.0,,nan,
89548,373422.0,,nan,
89548,374195.0,,nan,
89548,374545.0,,nan,
89908,374134.0,,nan,
89908,374159.0,,nan,
89908,374169.0,,nan,
89908,374236.0,,nan,
89908,374720.0,,nan,
89908,374811.0,,nan,
89908,374814.0,,nan,
89908,374815.0,,nan,
89762,372860.0,,nan,
89762,372859.0,94320.0,i also has same issue in the starting. make a thumb rule that try not to hard code any values and try to change the input to 2 or 3 different inputs and run your code. if the solution work for these inputs then it is ok to submit.,320073.0
89762,372859.0,94318.0,Yeah Bhanu I found the cause of it. But there is no where in Question mentioned a NOTE could have helped. I submitted the code as it was verified successfully.,306735.0
89762,372876.0,,nan,
89762,373447.0,,nan,
89917,374174.0,,nan,
89917,374206.0,,nan,
89917,374234.0,,nan,
89917,374355.0,,nan,
89948,374472.0,,nan,
89948,374484.0,,nan,
89948,374502.0,,nan,
89948,374547.0,,nan,
89948,374617.0,,nan,
89948,375066.0,,nan,
89960,374609.0,94695.0,Got it,304815.0
89960,374585.0,94694.0,Thanks a lot,304815.0
89960,374562.0,,nan,
89960,375536.0,,nan,
88846,368195.0,93188.0,Thanks for your reply. Can you elaborate?,307494.0
88846,368195.0,93191.0,"Last row you can identify as ar[ar.shape[0]-1,:] and last col as ar[:, ar.shape[1]-1]",318433.0
88846,368200.0,93196.0,Thanks Varun. Tied through shape and your way. Bot of them worked Thank you for prompt reply,307494.0
88846,368220.0,93197.0,"Ya, agree. Nice learning. Thanks",307494.0
88846,368229.0,93251.0,thankyou,303674.0
88846,369652.0,93781.0,HAVE YOU TRIED IN YOUR SUBMISSION FOR YOUR GIVEN SOLUTION ..IT IS ACCEPTED OR REJECTED??,305847.0
88846,369652.0,94255.0,accepted,301643.0
88846,370626.0,,nan,
88846,371229.0,,nan,
88846,371397.0,94238.0,"Hi Pranav, -1 always represents the start of the reverse count of index in array or list.",301107.0
88846,371397.0,94239.0,"Right,but that`s not what she has done",305655.0
88846,371397.0,94241.0,aah! i got the it .,301107.0
88846,371397.0,94240.0,she has harcoded 2 for last row.Won't give the correct o/p for anything beyond 3 rows in total,305655.0
88846,372543.0,,nan,
88846,376804.0,,nan,
89018,369039.0,93340.0,Will it work when n is odd number? eg.n is 7.,318458.0
89018,369039.0,93345.0,n should be even for this to work,318358.0
89018,369039.0,93366.0,"Above is brain teaser. Use n as Odd number and create a checker matrix using numPy (not necessarily tile) . eg. [ [0, 1, 0], [1, 0, 1], [0, 1, 0]]",318458.0
89018,369039.0,95173.0,"A_list=[0,1] B_list=[1,0] array_1 = np.array([A_list,B_list]) print(np.tile(array_1,(int(n/2),int(n/2))))",310529.0
89018,369039.0,95172.0,"print(np.tile(array_1,(int(n/2),int(n/2))))",310529.0
89018,369158.0,,nan,
89018,369468.0,93639.0,"I'm not able to understand this logic -> Z[1::2,::2] = 1 and Z[::2,1::2] = 1 ? can you explain me? What is the meaning of ""::"" in python?",318429.0
89018,369468.0,93661.0,"Hi Rajat, There are many possible ways to do this problem. I have one more solution for this x = np.array([[0, 1], [1, 0]]) check = np.tile(x, (n//2, n//2)) print(check) the meaning of ""::"" in python L[x::y] means a slice of L where the x is the index to start from and y is the step size. In array , we have rows and columns , it is two dimentional array.",314183.0
89018,369468.0,93754.0,I also didnt understand this logic. and I didn't get the exact output.,308639.0
89018,369468.0,93957.0,Great..Nice Approach..Appreciate as I learnt something really good.,317811.0
89018,369468.0,94648.0,Question talks about using tile function but is not used here in the solution. Also it is strictly said to not provide complete solutions in the forum. However this is wrong :),318007.0
89018,369808.0,93654.0,"Yes, we can do in one line . I have one more solution for this x = np.array([[0, 1], [1, 0]]) check = np.tile(x, (n//2, n//2)) print(check)",314183.0
89018,371130.0,,nan,
89021,369077.0,,nan,
89021,369104.0,,nan,
89021,369564.0,,nan,
89040,369208.0,,nan,
89040,369117.0,,nan,
89040,369491.0,,nan,
89738,372755.0,94304.0,Thanks Subarna!,300699.0
89738,372757.0,,nan,
89738,372751.0,,nan,
89738,372764.0,,nan,
89738,374538.0,,nan,
89738,375277.0,,nan,
89060,369235.0,93384.0,"I was looking for a single function which could achieve this. Moreover, the reshape solution would work for arange() but not for linspace() , isn't it?",313826.0
89060,369235.0,93386.0,"I cannot think of any inbuilt function for that, as 1-d array is the basic build blocks of higher dimensional arrays, so you would first need create 1-d array and apply reshape() or any other technique to create higher dimension. reshape() works on ndarray objects. Both arrange() and linspace() gives you np arrays so it works on both of them. try this snippet np.linspace(1, 100).reshape(5, -1)",306248.0
89060,369235.0,93398.0,correcto 👍,306248.0
89060,369235.0,93396.0,"reshape() would distribute the already generated array to a different dimension array, right. This would mean that doing something like np.linspace(1, 100).reshape(5, -1) would distribute the 1-d array to a 2-d array, but the individual dimensions themselves will not conform to the linspace condition of evenly spaced numbers over a specified interval.",313826.0
89060,369276.0,93400.0,"true, or maybe we are not aware. we can wait for some other answers, otherwise the TAs will clarify. :)",317998.0
89060,369276.0,93397.0,So turns out there are no inbuilt functions and some workaround has to be done.,313826.0
89060,369250.0,93455.0,I am looking for built in function which can generate a multidimensional array in which the elements on each dimension are evenly spread within a given interval.,313826.0
89060,370148.0,,nan,
89060,370199.0,,nan,
89060,370265.0,93616.0,I am looking for built in function which can generate a multidimensional array in which the elements on each dimension are evenly spread within a given interval. Please let us know if there are any such built-in functions. From the comments received so far. it appears that there are no such built-in functions that can directly achieve this and these needs some workaround to achieve this.,313826.0
89060,370623.0,,nan,
89110,369448.0,,nan,
89110,369576.0,,nan,
89061,369230.0,93378.0,thanks,300687.0
89061,369238.0,,nan,
89061,369246.0,,nan,
89061,369262.0,,nan,
89061,369290.0,,nan,
89061,370557.0,,nan,
89135,369557.0,93588.0,"np.array([1,2,3],[4,5,6]) will create 2 rows and 3 columns right which is a 2D matrix but still y do we use 2 opening n closing braces....",308437.0
89135,369562.0,93452.0,">>> np.zeros((1,2,3)) array([[[0., 0., 0.], [0., 0., 0.]]]) >>> This is 3d array using numpy. Can you please give the example.",318554.0
89135,369562.0,93451.0,But for 3d array again 2 square braces r working in python???,308437.0
89146,369626.0,,nan,
89146,369633.0,,nan,
89146,369637.0,,nan,
89146,369643.0,,nan,
89146,369644.0,,nan,
89146,369758.0,,nan,
89146,369646.0,,nan,
89146,369795.0,,nan,
93866,395538.0,,nan,
93866,395526.0,,nan,
93866,395524.0,,nan,
89192,369829.0,,nan,
89192,370003.0,,nan,
89198,369873.0,,nan,
89198,369888.0,93549.0,Thanks man! nicely explained..,317809.0
89198,369942.0,93550.0,Thanks Arihant!,317809.0
89198,369942.0,93691.0,Hi Arihant!I get your point that the matrix that we get is 2D and not 3D.But isn't the Prof. talking about the dimension of the lists and not the array?Kindly clarify coz m still having this doubt.,305655.0
89198,371730.0,,nan,
89198,371728.0,,nan,
89199,369886.0,,nan,
89199,369933.0,,nan,
89199,369911.0,,nan,
89199,369871.0,,nan,
89817,373306.0,,nan,
89817,373316.0,,nan,
89817,373391.0,,nan,
89817,373336.0,,nan,
89817,375261.0,,nan,
89817,373775.0,,nan,
89817,374535.0,,nan,
89817,375785.0,,nan,
90067,375044.0,,nan,
90067,375024.0,,nan,
90067,375268.0,,nan,
89276,370161.0,,nan,
89276,370150.0,,nan,
89276,370180.0,,nan,
89276,370299.0,,nan,
90755,378192.0,95305.0,"Sure helps.. thanks a lot, brother",310472.0
90755,378188.0,,nan,
89391,370803.0,93809.0,"Ok. but what if the with my code which showes ""Testcase passed"" & matches the solution output with expected output.. ? It should be accepted right?",310508.0
89391,370803.0,93816.0,hmmmmm...,310508.0
89391,370803.0,93814.0,I am not sure how the actual submitted answers are evaluated.,318554.0
89391,370796.0,93885.0,"No. ""Testcase passed"" means just one test case has been passed. your code should be generic such that it can pass any test case. and that will not happen if you hardcode the input.",317998.0
89391,370796.0,93886.0,Ok. noted.. thank you.. !,310508.0
89391,370796.0,93893.0,done...,310508.0
89391,370796.0,93804.0,"Hi, In screenshot #2, I wrote my code (array_x = np.ones((3,3),dtype=int)# Write your code here) which showed ""Testcase passed"" & matched the solution output with expected output.. After submitting it rejected the solution? HOW?",310508.0
89391,370796.0,93812.0,"you have hardcoded the rows and columns aa 3,3. why?! another test case might not have input as 3,3 the input could be 5,5, but your output will still give 3,3. hence it is rejected. so dont hardcode..use the input variables instead. also try to use dtype=np.int hope that helps",317998.0
89391,370796.0,93837.0,"Ok.. but eventually if the with my code which showes ""Testcase passed"" & matches the solution output with expected output.. ? It should be accepted right? Thank you ..",310508.0
89391,370796.0,93887.0,anytime :) please upvote my ans if it helped you. thanks.,317998.0
89391,370796.0,93894.0,thanks :),317998.0
89391,370798.0,93805.0,"Hi, Running the Code (array_x = np.ones((3,3),dtype=int)# Write your code here) showed ""Testcase passed"" & matched the solution output with expected output.. But after submitting it rejected the solution? HOW?",310508.0
89391,370887.0,,nan,
89391,370881.0,,nan,
89391,370860.0,,nan,
89391,370888.0,,nan,
89391,371372.0,,nan,
89391,372073.0,,nan,
89391,377514.0,,nan,
89691,372386.0,,nan,
89691,372429.0,,nan,
89395,370829.0,,nan,
89395,370846.0,,nan,
89395,370872.0,,nan,
89395,370868.0,,nan,
89395,370804.0,93860.0,thanks Vinay.I know i was hardcoding the code by say -3 or so.but was not sure how to get the shape value to variable.its is very helpful.,300687.0
89091,369347.0,,nan,
89091,369338.0,,nan,
89091,369325.0,,nan,
89091,369377.0,,nan,
89091,369384.0,,nan,
89091,369715.0,,nan,
89091,369726.0,,nan,
89091,370184.0,,nan,
89091,370069.0,,nan,
89091,370147.0,,nan,
89091,370484.0,,nan,
89091,370620.0,,nan,
89091,371329.0,,nan,
89091,371793.0,,nan,
89705,372463.0,,nan,
89705,372752.0,,nan,
89705,372474.0,,nan,
90275,376242.0,94950.0,Thanks Deval,320687.0
90275,376187.0,94949.0,i got it Thanks Vinay,320687.0
89706,372480.0,,nan,
89706,373137.0,,nan,
89424,370966.0,,nan,
89424,371050.0,,nan,
89708,372485.0,,nan,
89708,372475.0,,nan,
89708,372506.0,,nan,
89708,372716.0,,nan,
89708,372753.0,94306.0,Thank you,311745.0
89708,372753.0,94305.0,The best site to have dataset is Kaggle. https://www.kaggle.com/datasets Also I found below link very useful for dataset. https://www.analyticsvidhya.com/blog/2016/11/25-websites-to-find-datasets-for-data-science-projects/,317991.0
89708,373015.0,94838.0,These are best sites you will get lot of datasets,305804.0
89708,373109.0,,nan,
89708,373405.0,,nan,
89708,374014.0,,nan,
89708,376902.0,,nan,
89719,372582.0,94559.0,"It is recommended to mention the imports at the top of the code. However, unlike other programming languages like c++, c#, it is not mandatory to mention the imports at the top in Python. You can mention just before you use a particular method belonging to a package in case you are not aware of the logic you are going to implement.",314730.0
89719,372601.0,,nan,
89719,372756.0,,nan,
89719,372784.0,94309.0,Good one.. 😀,313826.0
89719,373052.0,,nan,
89709,372473.0,94244.0,"Please below error in Upgrad console Traceback (most recent call last): File ""/code/source.py3"", line 9, in array_x = array_x = np.full ((lines[1], lines[2]), lines[0], dtype=int) # Write your code here File ""/usr/local/lib/python3.4/dist-packages/numpy/core/numeric.py"", line 298, in full a = empty(shape, dtype, order) TypeError: 'str' object cannot be interpreted as an integer",303083.0
89709,372473.0,94299.0,Thanks Kapil,303083.0
89709,372473.0,94245.0,"Below is the solution for your problem. You need integer and also you need to use converted values in np.full function # Read the input from stdin() import sys import io sys.stdin =sys.stdin = io.StringIO(""1\n2\n3"") lines = sys.stdin.readlines() int_x = int(lines[0]) rows_m = int(lines[1]) cols_n = int(lines[2]) import numpy as np array_x = array_x = np.full ((rows_m, cols_n), int_x, dtype=int) # Write your code here print(array_x)",318368.0
89709,372619.0,,nan,
89709,375278.0,,nan,
90287,376266.0,,nan,
90287,376273.0,,nan,
90287,376307.0,94964.0,"Yes, code is written like that only. But still it is getting rejected.",316084.0
90287,376307.0,94966.0,Ok. For the coding console to verify your output you need to put explict print statements like this at the end of your code print(col_first) print(row_first) print(col_last) print(row_last) in the same order. Suggested you this as I could not see any print statements in the screenshot attached by you.,313826.0
90287,376537.0,,nan,
90287,376805.0,,nan,
90287,377017.0,,nan,
90287,377198.0,96020.0,"I am facing similar problem. When you say remove those extra parentheses , what does it mean? how do you do it ?",315121.0
91664,383779.0,,nan,
91664,383788.0,96917.0,"The initial code stub already has the code to accept the inputs required for the question. So, we need not write any explicit code for accepting the inputs. Further, we should only print what is required from the problem statement and printing anything extra would fail the test cases. Lastly, once you have successfully submitted the answer or you have exhausted the number of submissions for a problem, there will be an option on the right hand bottom called ""Shoe Solution"" which provides the sample expected solution. You can check that to understand if it is any different than how you have implemented.",313826.0
91664,383788.0,96915.0,"Thanks Vinay. Excuse some basic queries: When code is submitted with these changes, not getting an error bu not getting any output as well. How do we ensure that the code will ask for an input for the lists and second ly how do we ensure restrict choice to the integers given? Is this solution played anywhere in the video solutions or in Stackoverflow?",314329.0
91664,383788.0,96928.0,"At some places the 'Show Solution'- the one shown also does not give an output but throws errors, when we copy and paste it into Jupyter. For instance the one shown in this problem. Could you pl confirm The given problem : Perform an element-wise multiplication using list_1 = [2,3,4,5] list_2 = [7,8,9,6] and obtain the output as a list. Hint: Convert the list to an array and after multiplication convert it back to a list.",314329.0
91664,383788.0,96920.0,Thanks for the prompt response !,314329.0
90485,377716.0,,nan,
90485,377452.0,,nan,
90485,377499.0,,nan,
90485,377409.0,95164.0,Got it...Thanks,308638.0
90485,377410.0,,nan,
90485,377424.0,,nan,
90485,377884.0,,nan,
90488,377462.0,,nan,
90488,377443.0,,nan,
90488,377490.0,,nan,
90488,377777.0,,nan,
90481,377469.0,,nan,
90481,377411.0,,nan,
90481,381903.0,,nan,
88887,368519.0,,nan,
88887,368468.0,,nan,
88887,368408.0,93236.0,"Thanks but I already do this multiple times (and hence also mentioned this in my question). I think there must be something else going on linked to the browser settings; if you know it, please let me know. I was not able to locate it in FAQ's",309211.0
88887,368408.0,93243.0,"Thanks, now this has helped. Thanks! This point particularly helped: Try opening this on a different browser or on a separate incognito window with all third party plugins disabled.",309211.0
88887,368408.0,93241.0,There was an FAQ on this. http://help.upgrad.com/coding-console/the-coding-question-does-not-load,317689.0
88887,368420.0,,nan,
88887,368538.0,,nan,
88887,368971.0,,nan,
88887,380154.0,,nan,
88931,368605.0,,nan,
88931,368601.0,,nan,
88931,368628.0,,nan,
88931,368658.0,,nan,
88931,368722.0,,nan,
88931,368781.0,,nan,
90577,,,nan,
89257,370079.0,93573.0,Thanks!!,303673.0
89257,370096.0,,nan,
89257,370092.0,,nan,
89257,370893.0,,nan,
89257,371266.0,,nan,
89257,371573.0,,nan,
89257,371885.0,,nan,
90999,379788.0,,nan,
90999,379759.0,,nan,
89195,369970.0,,nan,
89195,369844.0,,nan,
89195,369841.0,,nan,
89195,370011.0,,nan,
89195,370117.0,93598.0,"according to the me print is missing or maybe you have not pasted that line of code here second is you are hardcoding the array say x[0],y[0] it should be the variable arr1,arr2 try this print( np.tile(np.array([arr1,arr2]),(n/2,n/2)))",300687.0
89195,370128.0,93617.0,"it works for all the even inputs in jupyter notebook ... but i got the point what both of you mentioned, i defined the array but did not use it",308782.0
89195,370128.0,93581.0,"Let me check it in a while , at office so cant run the code.... but why do i need to add another array when i have already defined it",308782.0
89195,370128.0,93582.0,Also this works perfect in the jupyter notebook,308782.0
89195,370128.0,93583.0,"See, even though you have defined arr1 and arr2 as np.array you have NOT USED them. So, there is no use of them. you are using LIST x and y. Also, i am just asking you to add the print function, not to add another array. you can see the difference in output, by adding and removing the print function. Also, x[0:] is same as x since you are starting from 0 index and traversing till the end of the list. same for y. so, you can simple rewrite it as np.tile((x,y),(n//2,n//2)). hope that helps",317998.0
89195,370128.0,93603.0,"you can try print( np.tile((arr1,arr2),(n/2,n/2)) if you say x[0],y[0] it would take the element of the array and may be in jupting if yu give the inpyt 2 it might be working but in python console it has to work for all the input its just my understanding correct me if i am wrong",300687.0
89195,370128.0,93620.0,"yes it works on jupyter, but the output you would be getting is something like this (for n=2): array([[0, 1], [1, 0]]) right? now note the keyword ""array"" is also printed. this will not match with the expected output, which does not have the array keyword printed. honestly, i dont know why that is happening, but to avoid it you can use the print() function to print your output. that would get rid of the keyword ""array"" and give you the ouput as: [[0, 1], [1, 0]] which will match the expected output. hope that helps.",317998.0
89195,370128.0,93637.0,"anytime man :) coincidently, i just came to that part in the session, where this same problem is mentioned in the faqs. anyway, plz upvote my answer if it helped. thnks :)",317998.0
89195,370128.0,93633.0,"thanks harshit, actually this array word with the answer is also mentioned in the exercise worksheet, that is why i did not look at it ans considered that answer would be correct, thanks for help man, appreciated",308782.0
89195,370213.0,,nan,
89195,369874.0,93569.0,My bad.. I mistook it for a graded question,313826.0
91353,381850.0,,nan,
91353,382846.0,,nan,
89736,372750.0,,nan,
89736,372747.0,,nan,
89736,372763.0,,nan,
89736,372880.0,,nan,
89736,373289.0,,nan,
89736,373473.0,,nan,
89736,374210.0,,nan,
90876,378798.0,,nan,
90876,378977.0,,nan,
90511,377562.0,,nan,
90511,377568.0,,nan,
90511,377669.0,,nan,
90511,378183.0,,nan,
95132,402045.0,,nan,
95132,402044.0,,nan,
95132,402007.0,,nan,
102498,438525.0,,nan,
102498,438606.0,106305.0,"Can it be the case that, since pandas is built on top of numpy, can you give an example where certain dataframe operations can be vectorised?",314084.0
102498,438606.0,106398.0,You can easily apply vectorization on any of the numeric columns,306040.0
103003,441043.0,,nan,
103003,440681.0,,nan,
103003,446272.0,,nan,
90151,375531.0,94822.0,You are welcome.,318368.0
90151,375531.0,94821.0,Thank you kapil 😊,320195.0
90151,375574.0,,nan,
90135,375440.0,,nan,
90135,375460.0,,nan,
90135,375455.0,,nan,
90135,375447.0,,nan,
90863,378781.0,,nan,
90863,378714.0,,nan,
90863,378997.0,,nan,
90863,379313.0,,nan,
90863,379507.0,,nan,
96340,410205.0,,nan,
90924,379082.0,,nan,
90924,379282.0,,nan,
90924,379276.0,,nan,
90897,379308.0,,nan,
90897,380310.0,,nan,
90897,378980.0,,nan,
93739,395046.0,,nan,
90887,378881.0,,nan,
90887,378986.0,,nan,
90398,377897.0,,nan,
90416,377079.0,,nan,
90416,377028.0,,nan,
90416,377035.0,95140.0,Also to add with this you can find explanation and fix for https://www.lifewire.com/503-service-unavailable-explained-2622940,317991.0
90416,377270.0,,nan,
90416,377779.0,,nan,
90148,375541.0,,nan,
90148,375549.0,,nan,
90148,375553.0,,nan,
90148,375697.0,,nan,
90739,378159.0,,nan,
90739,378169.0,95676.0,I wanted to understand why the library numpy here,316132.0
90739,378169.0,95679.0,okay..thanks,316132.0
90739,378169.0,95678.0,"isnan' is a function present in NumPy. You might also use the Pandas equivalent functions but since NumPy is faster, isnan is preferred.",306040.0
90674,378057.0,,nan,
90674,378078.0,,nan,
90548,377687.0,95203.0,thanks and where do we get the encoding value or number?,300687.0
89502,371414.0,,nan,
89502,371491.0,,nan,
89502,371423.0,,nan,
90304,376340.0,,nan,
90304,376398.0,94987.0,Thanks alot :),310611.0
90304,376522.0,95011.0,thanks for the help,310611.0
90304,376663.0,,nan,
90304,376698.0,,nan,
90304,377647.0,,nan,
90304,377260.0,,nan,
90452,377249.0,,nan,
90452,377365.0,,nan,
91070,380279.0,95734.0,Thanks Chandan,320687.0
91070,380438.0,,nan,
91070,380706.0,,nan,
91070,381222.0,,nan,
89897,374007.0,94582.0,If it's given what I need to replace and with what value. Then also we need to use category function?,314547.0
89897,374007.0,94592.0,Nop. Its only for analysing data.,318368.0
89897,374800.0,,nan,
89750,372947.0,94353.0,thankyou somuch ihave one more doubt what df=df[]- what this open and close square bracket for - for assigining into dataframe? df = df[~np.isnan(df['Price'])] - what this is doing removing those rows which are where price column is Nan ?,303674.0
89750,372947.0,94355.0,"Square brackets work the same way as in loc function, these are used to fetch all those dataframes having missing values <= 5 Yes to other part of ur question ~ symbol in front is a negation which tells to behave opposite to the statement that follows this symbol",318358.0
89750,372947.0,94735.0,We have a function in dataframes notnull() Use this function on whatever column you want and assign that to dataframe..It will only include notnull values and exclude null values,318358.0
89750,372947.0,94733.0,thankyou so much .i have one more Question if we want to remove null values from a particular row suppose df is my frame so will this work df.isnull(df['column name ']) so will these bring the all the rows that has the null value corrosponding to column name am i right? if yes then if i want to delete this do i have to use ~,303674.0
89750,372821.0,,nan,
89752,373444.0,,nan,
89752,372825.0,94314.0,Yes you need to put location of your file inside the bracket in place of question mark '?',317991.0
89752,372825.0,94312.0,That file location I would copy-paste inside the bracket?,300690.0
89752,372831.0,95549.0,This was very useful.. using raw string solved my error and loaded the file :),318495.0
89752,372832.0,,nan,
89752,372854.0,94317.0,"use pd.read_csv() , you are using CSV in caps. python is case sensitive.",320073.0
89752,372854.0,94319.0,In addition to that you need to specify file name also at the end of path i.e add melbourne.csv at the end of path.,317991.0
89752,372854.0,94338.0,replace read_CSV with read_csv and use .csv at the end of filename.,318532.0
89752,372854.0,94324.0,"Please note that, Python is CASE SENSITIVE Language. So, we have to be very careful while writing the code. Ex - Print('UpGrad') - throws error becuase P is in caps. Similarly, here CSV is in CAPS instead of small letters like csv.",311502.0
89752,372953.0,,nan,
89752,373014.0,,nan,
89752,373498.0,,nan,
89752,373736.0,,nan,
89752,374217.0,,nan,
89752,374793.0,,nan,
89752,375777.0,,nan,
90237,376106.0,,nan,
90237,376551.0,,nan,
90237,376568.0,95015.0,"So, there isn't a way to extract reviews from this site now?",301652.0
90237,376568.0,95013.0,"jsname,jscontroller are not native html tags, so not sure how we can use them.Moreover there are no fixed class name or div tags for reviews only..The site has been rewritten. not a good one for understanding web scrapping",309451.0
90237,376568.0,95032.0,"if you need any help to understand the basics of HTML tags and usage , I am all ears!!",309451.0
90237,376568.0,95031.0,obviously u can extract data..but have to dig deeper to learn the proper syntaxes for that. The course material on this topic is very brief and does not give much details. In order to learn the basics this url is not a good one I would say. I am still trying to find some good resources to learn how to do web scrapping.,309451.0
90455,378354.0,,nan,
90455,377303.0,,nan,
90455,377310.0,,nan,
90455,377312.0,,nan,
90455,377273.0,,nan,
90455,377328.0,,nan,
90455,377353.0,95191.0,"Thanks, this worked. I opened the file in Jupyter. Then entered file name.csv",301644.0
90455,377671.0,,nan,
90559,377734.0,95246.0,thanks alot,300687.0
90559,377734.0,95221.0,When and why is the inplace used please advise.thanks,300687.0
90559,377734.0,95222.0,"inplace=True modifies the dataframe on which the operation is being performed. So, df = df.drop(columns=['BuildingArea','YearBuilt','CouncilArea'] , axis=1) is equivalent to df.drop(columns=['BuildingArea','YearBuilt','CouncilArea'] , axis=1 , inplace=True) The default value of inplace is False and hence drop() function returns a new dataframe. Hope this clarifies.",313826.0
90559,377742.0,95216.0,thanks.it does work,300687.0
90559,377862.0,,nan,
90234,377315.0,,nan,
90234,375925.0,,nan,
90234,375933.0,,nan,
90234,375938.0,,nan,
90572,377820.0,,nan,
90572,377812.0,95224.0,"xyz.loc[np.isnan(xyz['language'],inplace=true)] = xyz[""French""] stills gives same error TypeError: 'float' object is not subscriptable",300687.0
90606,377978.0,,nan,
89915,375038.0,,nan,
89915,374974.0,,nan,
89915,374171.0,,nan,
89915,374207.0,,nan,
89915,374208.0,,nan,
89915,374230.0,,nan,
89915,374358.0,,nan,
89915,374395.0,,nan,
89915,374410.0,,nan,
89915,374821.0,,nan,
89921,374343.0,,nan,
89921,374228.0,,nan,
89921,374342.0,,nan,
89921,374373.0,,nan,
89921,374246.0,,nan,
90453,377702.0,,nan,
90453,377259.0,95146.0,thanks,314678.0
90729,378129.0,,nan,
90729,378134.0,,nan,
90729,378290.0,,nan,
89945,375784.0,,nan,
89945,374465.0,,nan,
89945,374458.0,,nan,
89945,374457.0,,nan,
89945,374622.0,,nan,
89949,374468.0,,nan,
89949,374614.0,,nan,
89949,374540.0,,nan,
89595,371905.0,,nan,
89595,372151.0,,nan,
89360,370781.0,,nan,
89360,370606.0,93745.0,I ran it on command prompt and it installed.,310511.0
89360,370606.0,93744.0,What is anaconda prompt?,310511.0
89360,370606.0,93751.0,It works at both anaconda prompt and command prompt.,317689.0
89360,370606.0,93757.0,How do I open anaconda prompt?,310511.0
89360,370606.0,93796.0,Click on 'Start'-> type Anaconda Prompt-> you will see the below options,311004.0
89360,370606.0,93802.0,In windows just search for Anaconda Prompt. it will be available if you have anaconda installed without any customization.,317689.0
89360,370849.0,93800.0,thanks.,310511.0
89360,370849.0,94429.0,Thanks helped me too,317514.0
89613,371942.0,,nan,
90499,377512.0,,nan,
90499,377565.0,,nan,
90499,377515.0,,nan,
90982,379603.0,,nan,
90982,379626.0,,nan,
90982,379662.0,,nan,
90982,379806.0,,nan,
90982,380332.0,,nan,
92686,,,nan,
89984,374711.0,,nan,
89984,374723.0,,nan,
89638,372052.0,,nan,
89638,372054.0,,nan,
89638,372056.0,94137.0,Please refer below link for basic understanding. https://www.pythonforbeginners.com/json/what-is-json,320195.0
89638,373112.0,,nan,
89638,372181.0,,nan,
89804,373185.0,94384.0,Pls whatsapp me on 9620888890 will send u images for understanding...not able to install anything using pip....thank u,308437.0
89804,373185.0,94528.0,"Hello Kapil, Please let me know if you have any procedure for MAC . I am also getting same error.",320195.0
89804,373820.0,94526.0,"Hello Anshul, What is the procedure in MAC?",320195.0
89804,373792.0,,nan,
89804,375433.0,,nan,
89988,374736.0,,nan,
89988,374735.0,,nan,
89988,377253.0,,nan,
89809,,,nan,
90844,378715.0,,nan,
90844,378633.0,,nan,
90979,379602.0,95640.0,"thanks.. but i am looking for pyPDF2 , pymysql is already there in my system",308634.0
90979,379627.0,95641.0,thanks a lot.. but i want pyPDF2 not pymysql,308634.0
90979,380324.0,95751.0,Thanks a lot...it worked..,308634.0
90977,379595.0,95606.0,thanks,311032.0
89279,370261.0,93739.0,Thanks Vinay...,307494.0
89279,370178.0,93737.0,Thank you..,307494.0
89279,370179.0,93738.0,Thanks Amani. Pretty clear.,307494.0
89837,373436.0,94420.0,Oh.. Thanks :-),315028.0
89837,373882.0,,nan,
90774,378284.0,95348.0,"Getting following error: ""conda is not recognized as internal or external command ,operable program or batch file",303666.0
90774,380091.0,,nan,
90828,378535.0,95389.0,Its not working either ways. The values are different. Solution output Ord_id 0.0 Prod_id 0.0 Ship_id 0.0 Cust_id 0.0 Sales 100.0 Discount 100.0 Order_Quantity 100.0 Profit 100.0 Shipping_Cost 100.0 Product_Base_Margin 100.0 dtype: float64 Expected output Ord_id 0.00 Prod_id 0.00 Ship_id 0.00 Cust_id 0.00 Sales 0.00 Discount 0.42 Order_Quantity 0.42 Profit 0.42 Shipping_Cost 0.42 Product_Base_Margin 1.06 dtype: float64,314313.0
90828,378539.0,95390.0,Bingo,314313.0
90995,379760.0,,nan,
90995,379766.0,,nan,
90995,379846.0,,nan,
90995,380186.0,,nan,
89323,370428.0,,nan,
89323,371312.0,,nan,
90084,375210.0,,nan,
90084,375106.0,,nan,
90084,375142.0,,nan,
89677,372373.0,,nan,
89677,372405.0,94235.0,Its working fine..Thank you..Does always we to follow installing modules both in CLI and Notebook Separately,318846.0
89677,372405.0,94248.0,"When you are working with anaconda/jupyter you will need to install it inside that environment. If you are using package outside anaconda/jupyter, you will need to install it using CLI. Both environment are different and will not share packages installed.",318368.0
89677,372405.0,94488.0,"Hey thanks for that, but where I'm unable to locate the 'Commands tab' on the LHS of my notebook. Could you help, thanks.",308962.0
89677,372405.0,94513.0,"@Utkarsh, kindly check my answer below.",318770.0
89677,372428.0,94242.0,"MySQL should be installed on the machine for sure. Though you will not get that error if you don’t solve first problem. Once you solve problem with pymysql, you will get error regarding MySQL installation.",318368.0
89677,372450.0,,nan,
89677,373697.0,,nan,
89677,373782.0,,nan,
89677,373799.0,94674.0,"bro, thanks for this. It is showing execution failed for me. Screenshot is attached below. Could you please look at it?",308962.0
89677,374197.0,,nan,
89677,374702.0,94675.0,You are not logged in as administrator user. Try after providing administrator privileges to your user.,318770.0
89677,374702.0,94677.0,"For the system or for the application? there is only one user on my system, I can run the anaconda prompt as an administrator though. How do I run Jupyter notebook as Administrator?? What am I doing wrong here?",308962.0
89677,374702.0,94679.0,From screen shot it seems that you are doing nothing wrong but there are permission issue. Try: (in windows 10 ) 1. Right-click on Windows symbol(Left most side) and from the context menu select Run CMD as Administrator. 2. cd C:\Path\to\Anaconda3 3. cd Scripts 4. conda.exe -c install package1 package2,318770.0
89677,375614.0,94936.0,thanks...i tried but it said no proxy server. attached is screenshot. Kindly advise what should i do next,310509.0
89677,375614.0,94896.0,Use this command to see proxy_servers conda config --show If any proxy server is present in the configuration. Kindly use conda config --remove-key proxy_servers. It will remove saved proxy servers from anaconda configuration. Enter conda clean --source-cache,318846.0
89677,375614.0,94897.0,After these commands try to install once again...,318846.0
89677,375615.0,,nan,
89677,376160.0,,nan,
89677,378295.0,,nan,
89825,373350.0,,nan,
89825,373392.0,,nan,
90790,378361.0,,nan,
90790,378375.0,95417.0,when click new terminal it displays closed what to do in this case Thanks damini,310385.0
90790,379318.0,95552.0,"Thanks Aman, this was a cool and quick fix with clear instructions. It helped a lot!",315022.0
89233,370116.0,,nan,
89233,370172.0,93736.0,"i havent reached that part of the session and havent tried it practically, so cant really comment on how to debug. sorry.",317998.0
89233,370172.0,94279.0,type this command 'pip install pymysql 'in command prompt. It will download and install pymysql in the system automatically.It worked for me,311119.0
89233,370608.0,,nan,
89233,374881.0,,nan,
89366,370634.0,93748.0,"Dataframe has a function ""drop_duplicates"" to remove duplicate values. You can use this as df.drop_duplicates() You can also drop duplicate values based on the column. df.drop_duplicates(['EmployeeId'])",318368.0
89366,370634.0,93747.0,It drop only all the attributes are same...?what if I would like to drop based only on specific attribute or attributes?,317993.0
89366,409777.0,,nan,
89366,370904.0,,nan,
89366,409778.0,,nan,
89366,409779.0,,nan,
89797,373189.0,94374.0,"Your output pane is hidden behind the Action Output. Please use below steps to make it visible. 1) Hover over the horizontal line between the ""Action Output"" string and ""100%"" string. It will show you the arrow icon. Click and drag the arrow up wards. You will be able to see ""Result Grid"" which displays your results.",318368.0
89797,373189.0,94382.0,"Tried moving the horizontal line, but its just increasing/ decreasing the width of ""Action Output"". However, I tried running the earlier command on python, it ran. However I noticed that Professor in the explanation video is using a differently named DB than what is given to us. Hence different results are coming up.",318335.0
89797,373189.0,94413.0,Thanks man for your help,318335.0
89797,373189.0,94386.0,"Okay. Great to know its working fine now. But any ways while increasing decreasing the size, there are two kind of arrows will come up. 1) With updown arrow. 2) Only uparrow. You will need to drag when uparrow comes up.",318368.0
89797,373123.0,94362.0,Thanks Kapil for the link. I downloaded the file and could run the script also. However after that I am not able to follow. Could you elaborate on further steps so that a non-CS person like me can understand easily?,318335.0
89797,373123.0,94364.0,"What do you mean by ""could run the script also""? Does this mean you were able to load the data into the database? Tell me the steps you were able to follow so that I will guide you for the next steps.",318368.0
89797,373123.0,94365.0,"I installed the file. Use the run script command on MySQL to load it. After that when I tried to use the commands as per website, such as USE, or SHOW TABLES, they weren't executing",318335.0
89797,373123.0,94371.0,"The comment section wasn't allowing to attach screenshot, so I've attached as an answer to this question. Check it out",318335.0
89797,373123.0,94366.0,Can you perform show databases; And show me the results.,318368.0
89797,376144.0,,nan,
89797,376122.0,,nan,
89682,,,nan,
89865,373818.0,94516.0,I tried to locate them. But couldn’t find it. Hence asked this question,318335.0
89865,373958.0,,nan,
89865,374064.0,,nan,
90361,376711.0,,nan,
90361,376862.0,,nan,
90361,376712.0,,nan,
89951,374550.0,,nan,
89951,374486.0,,nan,
89422,370962.0,,nan,
89422,370981.0,,nan,
89422,371153.0,,nan,
89422,377157.0,,nan,
90276,376178.0,,nan,
90276,376180.0,,nan,
90276,376190.0,95133.0,it worked..thanks :),310509.0
90276,380202.0,,nan,
90362,376797.0,,nan,
90362,376815.0,,nan,
90371,376844.0,,nan,
90371,376786.0,,nan,
90371,376748.0,95046.0,After putting the correct path it showing.. SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape,308639.0
90371,376748.0,95047.0,"Change your encoding = ""utf-8"" to encoding = ""ISO-8859-1""",317991.0
90371,376748.0,95053.0,"import pandas as pd companies.head() = pd.read_csv(""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"",sep=""\t"",encoding = ""ISO-8859-1"") companies.head() Still getting error File "" "", line 2 companies.head() = pd.read_csv(""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"",sep=""\t"",encoding = ""ISO-8859-1"") ^ SyntaxError: can't assign to function call",308639.0
90371,376748.0,95056.0,"Do as follows: companies = pd.read_csv(""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"", sep=""\t"", encoding = ""ISO-8859-1"") companies.head()",317991.0
90371,376748.0,95055.0,"head() works on dataframe. Companies.head() = pd.read_csv(""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"",sep=""\t"",encoding = ""ISO-8859-1"") will not work.",317991.0
90371,376748.0,95057.0,"I Tried your code also still showing the syntax error. companies = pd.read_csv(r ""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"",sep=""\t"", encoding = ""ISO-8859-1"") companies.head()",308639.0
90371,376748.0,95061.0,check the screen shot i have posted just now,308639.0
90371,376748.0,95060.0,can you show your code in screenshot with error?,317991.0
90371,376752.0,,nan,
90371,376771.0,,nan,
90371,376775.0,95062.0,that is doesn't matter I have tried that to remove the r ..,308639.0
90371,376775.0,95065.0,Your problem is resolved or not ?,317991.0
90371,376775.0,95063.0,"import pandas as pd companies = pd.read_csv(""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"", sep=""\t"", encoding = ""ISO-8859-1"") companies.head(5) Or import pandas as pd companies = pd.read_csv(r""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"", sep=""\t"", encoding = ""ISO-8859-1"") companies.head(5) I tried both code its working fine.",317991.0
90371,376775.0,95142.0,"Thanks vipul 2nd one works import pandas as pd companies = pd.read_csv(r""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"", sep=""\t"", encoding = ""ISO-8859-1"") companies.head(5)",308639.0
90371,376888.0,,nan,
90371,376923.0,,nan,
89437,371131.0,,nan,
89437,372624.0,,nan,
89722,372594.0,,nan,
89722,372600.0,94880.0,Can you paste the code that you are trying to run?,310511.0
89722,372600.0,94879.0,"no, this didnt work....",308437.0
89722,372596.0,94877.0,Pls whatsapp me on 9620888890 still not able to open csv file,308437.0
89722,376107.0,,nan,
90101,375207.0,,nan,
90101,375206.0,,nan,
90483,377428.0,,nan,
90483,377415.0,95163.0,dhonobad,302739.0
90383,376839.0,,nan,
90383,377492.0,,nan,
90383,377488.0,,nan,
90383,376840.0,,nan,
90496,377505.0,,nan,
90496,377546.0,95209.0,"if i want colun index of orange and pear so can i give it in one statement df.columns.get.loc(""pear"",""orange"")",300687.0
90487,377435.0,,nan,
90487,377436.0,,nan,
90487,377445.0,,nan,
90487,378294.0,96008.0,thanks!,319860.0
90487,377434.0,95165.0,It still says the same . No test cases passed.,319860.0
90487,377434.0,95166.0,okay got it..you have to do df=df[df.isnull().sum(axis=1)<5],309451.0
90487,377434.0,96010.0,"ie , df = df[df.isnull().sum(axis=1)<=5] sorry lesser than it is",319860.0
90487,377434.0,96009.0,i could pass my test samples when i tried condition greater than and equal to sign ie ' >=',319860.0
90491,377497.0,,nan,
90491,377473.0,,nan,
90491,377496.0,,nan,
90765,378214.0,,nan,
90765,378275.0,95420.0,Thanks Again!,302739.0
90765,378275.0,95682.0,"Hi, Subarna. In the future, please don't post direct solutions on the DF for graded components even if the poster asks directly. Guide them in the right direction, something like what Hemant has done on this post. This is a learning opportunity for you and people should be putting effort from their side in finding the answers rather than getting it from here. :)",306040.0
90765,378277.0,,nan,
90470,377371.0,,nan,
90470,377390.0,,nan,
89368,370639.0,,nan,
89368,370640.0,93755.0,Gotcha! Thanks.,310511.0
89368,370750.0,,nan,
90619,377938.0,,nan,
90619,377947.0,,nan,
90619,377976.0,,nan,
90553,377747.0,,nan,
90113,375272.0,94803.0,"Thanks, bro but it didn't help. My error is for a different reason. Let me know if you can make any sense out of it.",308962.0
90113,375496.0,,nan,
90113,376304.0,,nan,
90113,378921.0,,nan,
90642,380775.0,95836.0,Thank you!!,318386.0
90360,376688.0,95034.0,Thanks it worked,319319.0
90360,376864.0,,nan,
90862,378709.0,,nan,
90862,378728.0,,nan,
90862,379774.0,,nan,
89877,373943.0,,nan,
89877,373844.0,94536.0,Yes I have data,307493.0
89877,373879.0,,nan,
89877,373948.0,,nan,
96468,410995.0,,nan,
102972,441757.0,,nan,
116036,501858.0,116857.0,Would this will work for all the variables? A function is not needed,300735.0
116036,501858.0,116859.0,yes try this and check,310419.0
116036,501858.0,116922.0,Good Read Indeed..,300735.0
116036,501858.0,116860.0,https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba follow above link,310419.0
90858,378677.0,,nan,
90858,378686.0,,nan,
90125,375352.0,,nan,
90125,375358.0,,nan,
90125,375359.0,,nan,
90125,375499.0,,nan,
90149,376346.0,,nan,
90149,375545.0,,nan,
90149,375529.0,,nan,
90149,375534.0,,nan,
90149,375582.0,,nan,
90149,375959.0,,nan,
90149,376007.0,,nan,
90157,375565.0,94953.0,Thanks..its working fine..some issue with how i changed column name: df2.columns.values[3] = 'Customer_id',318436.0
90157,375565.0,94947.0,"df3 = pd.merge(df2,df1,left_on=['Customer_id'],right_on=['Cust_id'],how='inner') its giving error",318436.0
90157,375592.0,,nan,
90157,375591.0,,nan,
90886,378844.0,95449.0,i found good resource here https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/,318005.0
90886,378844.0,95442.0,"it will do mean for rain,wind i want sum for wind",318005.0
90886,378844.0,95443.0,"df.groupby(['month','day'])['wind'].sum()",310974.0
90886,378844.0,95444.0,use agg before sum or mean too,318005.0
90886,378844.0,95445.0,"df.groupby(['month','day']).agg({'wind':mean,'rain':sum})",318005.0
90886,378844.0,95446.0,"--------------------------------------------------------------------------- NameError Traceback (most recent call last) in () ----> 1 df.groupby(['month','day']).agg({'wind':mean,'rain':sum}) NameError: name 'mean' is not defined",310974.0
90886,378844.0,95447.0,np.mean,318005.0
90886,378844.0,95450.0,"Wow, didn't know this was possible, thanks a lot :)",310974.0
90886,378870.0,95453.0,Actually Thank you for such a good question. Even I was not aware and did some googling as I got very curious on how we can approach this. Turns out there is already built in provision to do this. Applied on one of the examples to experience the power of it.,313826.0
90886,378870.0,95452.0,thanku sir,318005.0
90327,376441.0,,nan,
90327,376447.0,,nan,
90327,376883.0,,nan,
90161,375581.0,94841.0,both will give the same results,318084.0
90161,375581.0,94839.0,thanks kapil. i thought after adding we can just set the index.is that not possible?,300687.0
90161,375576.0,,nan,
90161,375580.0,,nan,
90161,375636.0,,nan,
90161,376013.0,,nan,
90161,375693.0,,nan,
90161,376508.0,,nan,
90418,377026.0,,nan,
90418,377048.0,,nan,
90418,377030.0,,nan,
90418,377075.0,,nan,
90418,388768.0,,nan,
89430,371012.0,,nan,
89430,371089.0,,nan,
89430,371054.0,,nan,
89430,371052.0,,nan,
89430,371143.0,,nan,
89430,371201.0,,nan,
90119,375304.0,94785.0,thanks but when i try as list option its giving error,300687.0
90119,375304.0,94787.0,can you suggest any additional resources to refer,300687.0
90119,375304.0,94790.0,i did though its not working will try again,300687.0
90119,375304.0,94788.0,"The list solution works for me. Note that the list should be enclosed within a pair of square brackets. df_2 = df[['month', 'day', 'temp', 'area']]",313826.0
90119,375335.0,,nan,
90119,375339.0,,nan,
90119,375367.0,,nan,
90119,375698.0,,nan,
90119,375754.0,,nan,
90119,375989.0,,nan,
90119,376056.0,,nan,
90119,376141.0,,nan,
90119,376873.0,,nan,
89362,370573.0,,nan,
89362,370586.0,,nan,
89362,370594.0,,nan,
89409,370935.0,93842.0,thanks!!,312479.0
89409,370956.0,94335.0,"To append the dataframes : use df1.append(‘dataframe2’, ignore_index = True)",318532.0
89409,372233.0,,nan,
90294,376316.0,,nan,
90294,376515.0,,nan,
90294,376319.0,,nan,
89349,370785.0,,nan,
89349,370533.0,93725.0,Cool. Concept sake. Thanks,317269.0
89349,371736.0,,nan,
89487,371286.0,93910.0,No.. My question was why values are changing. As you can see in the result some output values are 'NaN',311741.0
89487,371286.0,93923.0,Its not clear as why data type of series is changed and also why those NaN values are comingup,311741.0
89487,371286.0,93911.0,"Oh, that is because you are creating a new series asking pandas to index s_3 using the custom list. Hence it is taking s_3[1], s_3[2] , s_3[3] & s_3[4] which doesn't exist and gives NaN.",310974.0
89487,372697.0,,nan,
89494,371503.0,,nan,
89494,371315.0,,nan,
89494,373990.0,,nan,
90226,376244.0,,nan,
90226,375872.0,,nan,
90226,375855.0,,nan,
90226,376053.0,,nan,
90226,375929.0,,nan,
90225,375848.0,,nan,
90225,375851.0,,nan,
90225,375866.0,,nan,
90225,376882.0,,nan,
90552,377699.0,,nan,
90552,377705.0,,nan,
89507,371984.0,,nan,
89507,371360.0,,nan,
89507,371361.0,,nan,
89507,371382.0,,nan,
89507,371474.0,,nan,
89507,371658.0,,nan,
89507,372609.0,,nan,
89507,373515.0,,nan,
89507,373790.0,,nan,
88850,368218.0,,nan,
88850,368421.0,,nan,
88850,368429.0,,nan,
88850,368837.0,,nan,
88850,368996.0,,nan,
88850,369725.0,,nan,
90132,375436.0,,nan,
90132,375437.0,,nan,
90439,377150.0,95120.0,"Sorry, still not clear. Can you elaborate",300748.0
90439,377150.0,95148.0,"the dictionary elements 'one' and 'two' are of different lengths. As a result of it, python fills the blank values with NaN. By default, Python sets the entire series to float, if NaN is present. Try to fill the fourth value of 'one' to some other number, then you'll see that python recognizes the entire series as Integer. Hope this explains.",318084.0
90439,377158.0,,nan,
90439,377159.0,,nan,
90439,377174.0,,nan,
90953,379326.0,,nan,
90953,379780.0,,nan,
90953,379421.0,,nan,
90953,379357.0,,nan,
90953,382032.0,,nan,
90232,376098.0,,nan,
90232,376096.0,,nan,
90450,377248.0,95130.0,"Thanks , It worked.",318347.0
89529,371540.0,,nan,
89529,371646.0,,nan,
89529,371638.0,,nan,
89529,371901.0,,nan,
89529,371924.0,94343.0,i used the print function like you mentioned last time for my checkerboard problem :),308782.0
89529,371924.0,94342.0,"This is my code # Read the variable from STDIN n = int(input()) import numpy as np import pandas as pd print(pd.Series(np.array(range(1,n+1))**2, index = range(1,n+1))) , but i just wanted to know harshit how can you define series 2 times in your code, thanks",308782.0
88895,368439.0,,nan,
88895,368450.0,,nan,
88895,368453.0,,nan,
88895,368463.0,,nan,
88895,369138.0,,nan,
88895,369253.0,93389.0,Thanks,308635.0
89900,375260.0,,nan,
89900,374026.0,,nan,
89900,374038.0,94570.0,:),317998.0
89900,374027.0,,nan,
89900,374239.0,,nan,
89900,374248.0,,nan,
89900,374370.0,,nan,
89900,376877.0,,nan,
93775,395272.0,98950.0,thanks,318458.0
93775,395268.0,98949.0,thanks,318458.0
93775,395266.0,,nan,
93775,395701.0,,nan,
88899,368479.0,,nan,
88899,368478.0,,nan,
89901,374060.0,94569.0,Not that clear in above link.,320103.0
89901,374086.0,,nan,
89901,374710.0,,nan,
89901,374521.0,,nan,
89901,374103.0,,nan,
90998,379762.0,,nan,
90998,379797.0,95634.0,"Yes, seems like",313228.0
90668,378018.0,,nan,
90668,378103.0,,nan,
89559,371797.0,,nan,
89559,371977.0,,nan,
89559,371812.0,,nan,
89559,372228.0,,nan,
89559,373087.0,,nan,
89559,375180.0,,nan,
89925,374303.0,,nan,
89925,374322.0,,nan,
89925,374360.0,,nan,
89925,374375.0,,nan,
89925,375780.0,,nan,
90728,378124.0,,nan,
90728,378253.0,,nan,
88990,368916.0,,nan,
88990,368943.0,,nan,
88990,369102.0,,nan,
88990,369484.0,,nan,
88990,369465.0,93472.0,"I was doing the same. Yesterday it was not working, but today it worked. thanks..",312518.0
88990,369465.0,93492.0,No problem.,312756.0
88990,377827.0,,nan,
88990,369505.0,,nan,
89955,374612.0,,nan,
89955,374504.0,94651.0,"O jeez, silly me. You're right. Thank you.",316416.0
89965,374680.0,,nan,
89965,374605.0,94656.0,Okay..so those are properties..got it..Thank you :),318436.0
89961,374570.0,,nan,
89961,374574.0,,nan,
89961,374608.0,,nan,
89972,374651.0,,nan,
89972,374638.0,94662.0,thank you nishan.,305843.0
89972,374638.0,95350.0,"I don't really think using square brackets over () will matter here. The output of code is not matching to soln. output because code is using column attribute instead of values attribute. In the above code passing 'rain','wind' in values attribute will give the desired output with both () and []",318495.0
89972,374695.0,,nan,
89972,376932.0,,nan,
89972,374755.0,,nan,
89042,369166.0,,nan,
89042,369259.0,,nan,
89042,369345.0,,nan,
89042,373866.0,,nan,
89044,369403.0,,nan,
89044,369602.0,,nan,
89046,369313.0,,nan,
89046,369301.0,,nan,
89046,369605.0,,nan,
89836,373445.0,,nan,
89836,373439.0,,nan,
89836,373467.0,,nan,
89836,373798.0,,nan,
89836,373885.0,,nan,
89132,369577.0,93467.0,"Thanks, Bro but instead of merging I used by concatenating but after seeing the concatinating is also no use just can do it by adding.",304692.0
89132,369577.0,93468.0,Yes you have to use the add function to add all the arrays,308635.0
89132,369577.0,93902.0,actual by mistake I used the word merge. What I actually meant was to create a single data frame using all 3 frames. You can use the add method to do so,308635.0
89132,369577.0,93899.0,"in the second step You have mentioned to merge seperate dataframes into one single dataftrame using merge , however merge only accepts two dataframes (left and right) how can we give third dataframe? could you explain me about this more clearly?",312756.0
89132,369577.0,94223.0,My problem is solved.Thanks!,304696.0
89132,369577.0,94222.0,"Below are the steps which I followed : 1. Set index to country for all three data frames 2. Added first two data frames using add function and formed a new data frame. 3.Added third data frame to the newly created data frame in 2nd step using add function. 4.Sort the data frame formed in 3rd step by Medals column in descending order. 5.My output in Jupyter notebook shows correctly , also code executes successfuly using 'Run Code' option but on clicking 'Verify Answer' it shows up no test cases passed.Can anyone help why this is happening.",304696.0
89132,370050.0,,nan,
89132,373865.0,,nan,
89025,369058.0,,nan,
89025,369062.0,,nan,
89025,369088.0,,nan,
89025,369095.0,,nan,
89025,369494.0,,nan,
89025,376808.0,,nan,
89025,370122.0,,nan,
89138,369570.0,,nan,
89138,369573.0,,nan,
89138,369579.0,,nan,
89138,369584.0,,nan,
89138,369934.0,,nan,
89138,370619.0,94038.0,similarly (as in your question for pd ) you need to import numpy as np for your current session of jupyter notebook. Looks like you opened your jupyter notebook and tried directly to execute the code in the cell.,300717.0
89140,369601.0,93772.0,Yep; this is absolutely correct,304814.0
89140,369610.0,,nan,
89140,369654.0,93491.0,thank you guys..understood the concept,313676.0
89140,369985.0,,nan,
89140,372402.0,,nan,
89154,369811.0,93525.0,"Apurva, Can you please tell me why rain and wind were taken as values and month and day as index?",314547.0
89154,369811.0,93537.0,typo : As there are more than one column for which values are to be determined we use list.,308635.0
89154,369811.0,93536.0,"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html Use the above link for reference. Basically values = the columns for which we want the details and index = the basis on which we have to get the values As in the above example we want to determine the mean values of rain and wind ,so they become the values as more than one value it is an array",308635.0
89113,369497.0,93445.0,Thanks Indranil. There is a question in Pandas what I posted. It's easy but don't know why the code isn't working.,314547.0
89113,369523.0,93457.0,Thanks Arihant 👍,314547.0
89113,369697.0,,nan,
89113,370168.0,,nan,
89113,370333.0,93990.0,"hi can you tell me why this wont work? import pandas as pd df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') df_1 = df.groupby(['month','day']).mean() df2=df_1.loc[['rain','wind'],:] print(df2)",303674.0
89113,371806.0,,nan,
89113,376931.0,,nan,
89628,372007.0,,nan,
89628,372389.0,,nan,
89628,373429.0,,nan,
89178,369741.0,,nan,
89178,369746.0,,nan,
89178,369981.0,93624.0,thankyou so much,303674.0
89178,370230.0,,nan,
89178,370429.0,,nan,
90836,378721.0,,nan,
90836,378602.0,,nan,
89633,372037.0,94145.0,ok great,318358.0
89633,372037.0,94143.0,I solved this problem through Merge and hence the problem in labels. Now i sorted this out. Thanks.,300718.0
89633,372076.0,,nan,
89633,372139.0,,nan,
89633,372192.0,,nan,
90372,376750.0,95048.0,"thanks deval..but what if only date is given and using that date , we need to sort further on the basis of month and day?",315757.0
90372,376750.0,95096.0,"If you have just date column and you want to sort the data further with month and day. In that case, you have extract the month and day from date column and add as new column df['month'] = pd.DatetimeIndex(df['date']).month df['day'] = pd.DatetimeIndex(df['date']).day Now you have month and day column in data-frame and you sort based on these fields df.sort_values(by=['day','month'],ascending = True)",317845.0
90372,376868.0,,nan,
90372,376870.0,,nan,
90372,376925.0,,nan,
90375,376789.0,,nan,
90375,376841.0,95229.0,"Yes i did use lambda function but it is giving as expecting one positional parameter missing. df['XY'] = df[['X','Y']].apply(lambda x,y: x*y)",315856.0
90375,376879.0,,nan,
90375,378272.0,,nan,
89803,373169.0,94390.0,"Please create the list as we can not assign variable in function. list = [1,3,4,5,67,8]",320195.0
89803,373182.0,,nan,
89803,373226.0,,nan,
89803,373399.0,94415.0,"correcting l1= [6,7,8,9,2,3,4,5] print(list(map(lambda a:a**2,l1)))",306242.0
89803,373400.0,,nan,
89803,374380.0,,nan,
89803,373851.0,,nan,
89218,369948.0,93847.0,"Scala means a single value. So scalar values of same type means collection of values with similar data type Ex: Char Array['a','b','c'] Int Array[1,2,3]",311952.0
89218,369948.0,93840.0,"I want to know specifically about the term ""scalar values"". I understood the context ... but still if it can be elaborated with some examples",318344.0
89218,369952.0,,nan,
89218,369931.0,93545.0,"A pandas series can contain character, a list, an array, a complex, a boolean etc.",318344.0
89218,371148.0,,nan,
91924,385401.0,,nan,
91924,385369.0,96489.0,"Yes, I also provided correct answer same time.",317811.0
91924,385374.0,,nan,
89815,373236.0,,nan,
89815,373364.0,,nan,
90749,378269.0,,nan,
90749,378308.0,,nan,
89656,372173.0,94180.0,But in material this is how it's done I mean new data frame is not assigned to...,308437.0
89656,372180.0,,nan,
89656,372234.0,94221.0,"Pls see In [25] under merging_concatenating material provided....there is no inplace, there is no new variable to which it's assigned...there is no print also...how it worked??",308437.0
89656,372234.0,94243.0,ok...,308437.0
89656,372234.0,94237.0,"I believe this is due to the interactive nature of Jupyter notebook because of which it ""pretty displays "" any variable name or any statement which returns a value to the console. However, when we are writing code in the coding console, there is not such feature as the code written gets submitted as a python script and hence a need to explicitly use the print() statement.",313826.0
89828,373355.0,,nan,
89828,373371.0,,nan,
89828,373395.0,,nan,
89828,373754.0,,nan,
89828,373363.0,94575.0,"df[0::2] with include 0th row as well, only for even number df[2::2] works",301118.0
89828,373363.0,94580.0,"Technically, zero is also an even number. https://en.wikipedia.org/wiki/Parity_of_zero",313826.0
89828,373363.0,95171.0,Agreed with Vinay..but in the question they have specifically mentioned not to include 0th row.. so answer will be df[2::2],318495.0
89828,373892.0,,nan,
89828,373891.0,,nan,
89828,374215.0,,nan,
89828,375790.0,,nan,
89828,375179.0,,nan,
89828,376871.0,,nan,
89829,373387.0,,nan,
89829,373384.0,,nan,
89829,373466.0,,nan,
89829,373474.0,,nan,
89829,373831.0,,nan,
89829,374804.0,,nan,
89829,375781.0,,nan,
89829,375991.0,,nan,
89275,370145.0,,nan,
89275,370170.0,93590.0,Your welcome. Happy to know your problem solved.,317991.0
89275,370182.0,,nan,
90778,378298.0,95397.0,"This was an illustrative example for a dataframe df having columns 'Customer_Segment' and 'Profit'. Make sure you are using the columns belonging to your dataframe. If it is still not working, then please post a screenshot of the code that you have tried along with the info() of the dataframe that you are working on.",313826.0
90778,378298.0,95393.0,Thanks. Its working for me now.,300748.0
90778,378302.0,,nan,
90778,379325.0,,nan,
90784,378327.0,,nan,
90784,378337.0,,nan,
90784,378334.0,95351.0,"Yes, and 0 is an integer too, which makes it even.",306727.0
90022,375048.0,,nan,
90022,374806.0,,nan,
90022,374816.0,,nan,
89293,370232.0,,nan,
89293,370291.0,,nan,
89293,370547.0,,nan,
89310,370328.0,93901.0,It is clear now!! thank you :),312756.0
89310,370441.0,,nan,
89310,371667.0,,nan,
89310,373856.0,,nan,
89310,373064.0,,nan,
89310,373067.0,,nan,
90019,374791.0,,nan,
90019,374792.0,,nan,
90019,374802.0,,nan,
89336,370409.0,,nan,
89336,370435.0,,nan,
89336,370446.0,,nan,
89844,373524.0,,nan,
89844,373531.0,,nan,
89844,373552.0,,nan,
89844,373880.0,,nan,
89844,374824.0,,nan,
90793,378359.0,,nan,
90793,378383.0,,nan,
90793,378381.0,,nan,
90354,376645.0,,nan,
90354,376657.0,,nan,
90354,376881.0,,nan,
90354,376626.0,95024.0,got it . I did it the 2nd way .Thanks !,319860.0
90440,377766.0,,nan,
90440,377156.0,,nan,
90440,377153.0,95121.0,"There is no difference in my output and solution output, I am getting the sum in integer data type and solution output in float data type.",317845.0
90440,377160.0,,nan,
90440,377165.0,95123.0,Thanks it worked.,317845.0
90440,377725.0,,nan,
91006,379820.0,,nan,
91006,379842.0,,nan,
91006,380155.0,,nan,
91006,382030.0,,nan,
90795,378424.0,95378.0,Rounding the values of the columns,300687.0
90795,378368.0,95356.0,thanks will it work if i have nan values ?what shouls i do if it has nan values,300687.0
90795,378368.0,95357.0,"Yes, the same code will work. The result will simply be NaN when you round any NaN value.",301652.0
90795,378368.0,95359.0,I tried the same code. Works fine in my Jupyter Notebook.,301652.0
90795,378368.0,95358.0,thanks but not sure when write the above code it is givening all zero.but thanks a lot for clarifying,300687.0
93930,395950.0,,nan,
93930,395999.0,,nan,
89348,370536.0,,nan,
89348,371734.0,,nan,
90115,375366.0,94808.0,"Adding to this make list1 =range(1,n+1) as list1 =range(0,n+1)",317991.0
90115,375295.0,,nan,
90115,375384.0,,nan,
90115,375443.0,,nan,
90115,375708.0,,nan,
90115,375747.0,,nan,
90115,376858.0,,nan,
89873,373975.0,,nan,
89873,373848.0,,nan,
89873,373849.0,,nan,
89873,373857.0,,nan,
89873,373875.0,,nan,
89873,373952.0,,nan,
89873,374219.0,,nan,
89873,374379.0,,nan,
89873,375112.0,,nan,
90117,375299.0,,nan,
90117,375374.0,,nan,
90117,375712.0,,nan,
90117,375702.0,95317.0,Do you know of any other method of doing this?,318751.0
90117,375702.0,95500.0,"Yes, I will share that soon",317811.0
90117,375294.0,,nan,
89702,372493.0,,nan,
89702,372445.0,,nan,
90268,376251.0,,nan,
90268,376151.0,94934.0,"Your code is generating one tuple, where first element is a function and second element is the original series.",318554.0
90268,376103.0,,nan,
90268,376112.0,,nan,
90268,376114.0,,nan,
90268,376152.0,,nan,
90268,376440.0,,nan,
90268,376857.0,,nan,
90268,376890.0,,nan,
90268,377037.0,,nan,
90369,377844.0,,nan,
90369,376754.0,95067.0,"I know but what will be the difference between both the functions describe() and describe ? As both of these print different values , if you will use them and so why this difference?",318427.0
90369,376754.0,95068.0,hi sahil when you use describe you cant do anything else but you can ask describe to perform differently using describe () like df.describe(exclude=[np.object]),318017.0
90369,376848.0,,nan,
89710,372618.0,,nan,
89710,372617.0,,nan,
89883,373939.0,,nan,
89883,373887.0,,nan,
89883,374258.0,,nan,
89883,374293.0,,nan,
89883,374338.0,,nan,
89883,373874.0,,nan,
89883,373871.0,,nan,
89420,370975.0,,nan,
89420,370970.0,,nan,
89420,371243.0,,nan,
90367,376849.0,,nan,
89956,374528.0,,nan,
89956,374610.0,,nan,
89956,374655.0,,nan,
89718,372575.0,,nan,
89718,372564.0,,nan,
89721,372625.0,,nan,
89721,372704.0,,nan,
89620,371985.0,94115.0,yes sir..but in module what is mentioned in FAQ is incorrect,317811.0
89620,371985.0,94120.0,"I didnot read out the FAQ, I have worked on it in Jupyter notebook",318358.0
89620,372004.0,,nan,
90063,375229.0,94776.0,initially there is no index. i am trying to set the index. it works when i give a list. but does not work when i give a series.,317998.0
90063,375008.0,94774.0,"The length of the series is 9 na? i have appended 2 dataframes to 1. so total length becomes 9. also, why does it work when i give a list instead of a series?",317998.0
90063,375008.0,94775.0,"The length of the series is 9 na? i have appended 2 dataframes to 1. so total length becomes 9. also, why does it work when i give a list instead of a series?",317998.0
90063,376860.0,,nan,
90732,378139.0,,nan,
90732,378158.0,,nan,
90732,379158.0,,nan,
90732,378160.0,,nan,
90732,380071.0,,nan,
90365,376730.0,,nan,
90365,376740.0,,nan,
90365,376731.0,95226.0,"Try this and please state what error that you are facing, Surendra. We can help you to improve your code.",319721.0
90365,376798.0,,nan,
90365,376853.0,,nan,
90365,376929.0,,nan,
90365,378324.0,,nan,
90469,377360.0,,nan,
90469,377357.0,,nan,
90469,377404.0,,nan,
90469,377475.0,,nan,
90469,377641.0,,nan,
90501,,,nan,
90502,377525.0,95174.0,if we don't give axis in sort_index then I guess it's same as sort_data,318495.0
90512,377580.0,,nan,
90512,377578.0,,nan,
90512,377667.0,,nan,
90512,377581.0,,nan,
91756,384150.0,,nan,
91756,384236.0,,nan,
91756,384159.0,,nan,
91756,384815.0,,nan,
91756,384465.0,,nan,
90576,377855.0,,nan,
90576,377860.0,,nan,
90576,377904.0,,nan,
90576,378854.0,,nan,
90584,377871.0,,nan,
90584,377889.0,,nan,
90584,377941.0,,nan,
90584,377969.0,,nan,
90492,377472.0,95170.0,Using apply increases the execution time. 0 1 1 4 dtype: int64 0.0013450000000001516 seconds --> with apply() 0 1 1 4 dtype: int64 0.0011170000000000346 seconds --> without apply,318495.0
90492,377780.0,95336.0,Got your point.,318495.0
90492,377552.0,,nan,
90064,375027.0,94770.0,"yes, it worked but with a little modification. df[column_name] = df[column_name].astype(float) thanks.",317998.0
90064,374996.0,94771.0,This did not work. the values still remained integers.,317998.0
90064,374996.0,94809.0,Can u pls share your code. I tried it and it worked. Only after that I posted the answer.,318084.0
90064,374996.0,94811.0,"The code is the same as mentioned in the question, i just added your suggestion to it. here is the modified code, please check and let me know if it is working for you, because it does not give me float values in jupyter import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({'Country': ['USA', 'France', 'Russia'], 'Medals': [15, 13, 9]} ) silver = pd.DataFrame({'Country': ['USA', 'Germany', 'Russia'], 'Medals': [29, 20, 16]} ) bronze = pd.DataFrame({'Country': ['France', 'USA', 'UK'], 'Medals': [40, 28, 27]} ) x=gold.append([silver,bronze]) x['id']=[1,2,3,4,5,6,7,8,9] x['Medals'] = x['Medals'].apply(pd.to_numeric, downcast = 'float') x.set_index(""id"",inplace=True) groupby_country=x.groupby('Country') #groupby_country['Medals'].apply(pd.to_numeric, errors='ignore') k=groupby_country['Medals'].sum().sort_values(ascending=False) print(k)",317998.0
90064,374996.0,94834.0,"Yes, now it worked. thank you. but, any idea why the column had to be given as a list to change the datatype?",317998.0
90064,374996.0,94833.0,"Use the below code: x[['Medals']] = x[['Medals']].apply(pd.to_numeric, downcast = 'float') It will work",318084.0
90064,375037.0,94772.0,"no, it did not work. anyway thank you.",317998.0
90064,375223.0,94773.0,"yes, a little modification is needed to that. df[column_name] = df[column_name].astype(float)",317998.0
90064,375341.0,,nan,
90791,378376.0,,nan,
90791,378851.0,,nan,
90667,378121.0,,nan,
90667,378800.0,,nan,
90520,377652.0,,nan,
89924,374306.0,,nan,
89924,374374.0,,nan,
89924,375759.0,,nan,
89339,370433.0,,nan,
89339,370535.0,93871.0,"Apparently, the name Panda means : Panel Data meaning multidimensional datasets. https://en.wikipedia.org/wiki/Pandas_(software)",313826.0
89339,371745.0,,nan,
89339,370659.0,,nan,
90870,378739.0,,nan,
90870,378865.0,95521.0,ok Subaran Thanks a lot,302735.0
90870,378865.0,95503.0,"if the question has given you the input list, then use that..convert the given input list to series and proceed.",309451.0
90870,378755.0,,nan,
89202,369884.0,,nan,
89202,369883.0,,nan,
89202,369869.0,,nan,
89202,369928.0,,nan,
89635,372044.0,94136.0,To use iloc we need to have int as datatype.Please verify that,318358.0
89635,372044.0,94135.0,"Hi Madanjit, I am not able to do this operations using df.iloc. df_2 = df.iloc[(df.area > 0) & (df.wind > 1) & (df.temp > 15) , :]#Type your code here. print(df_2.head(20))",320195.0
89635,372188.0,,nan,
89635,373402.0,,nan,
90497,377511.0,,nan,
91139,381818.0,,nan,
91139,381728.0,,nan,
90779,378317.0,,nan,
90779,378340.0,,nan,
90779,378465.0,,nan,
89716,372607.0,94267.0,"Hi Arpit, I do not want to sort on col1 but on the marginal columns.",310511.0
89716,372768.0,,nan,
89716,373401.0,94436.0,Please see my explanation of the problem below.,310511.0
89716,373411.0,94426.0,"Hi Anshul, I wanted to sort the rows of dataframe based on the margin Column 'Total' such that all the rows are sorted but the 'Total' row still stays at the bottom. This is possible in Pivot tables created in excel files. Please see my excel screenshot below. This will help in visualization. Your solution doesnt do that.",310511.0
89716,373411.0,94427.0,"Hi Anshul, I wanted to sort the rows of dataframe based on the margin Column 'Total' such that all the rows are sorted but the 'Total' row still stays at the bottom. This is possible in Pivot tables created in excel files. Please see my excel screenshot below. This will help in visualization. Your solution doesnt do that.",310511.0
89716,373411.0,94435.0,Please see my explanation of the problem below.,310511.0
89716,373551.0,94468.0,"Hi Pulkit. I should have given a description of the pivot table. Age group contains groups of age, eg 10s, 20s etc. The rest of the columns contain percentage response. So the pivot table represents percentage response of bank customers based on age group and different marital status. So I think, your suggestion of making a series where age_group == 'Total' is not a good option.Thanks btw.",310511.0
89716,373551.0,94512.0,"Hi Rajshi, As you are creating a DataFrame from the pivort table lets say 'df', you can try following: df.['Is_Total'] = df.age_group.apply(lambda x: x=='Total') df.sort_values(['is_Total','Total'],ascending=False) Let me know if this doesn't work. Will try to think of something else :)",306725.0
89716,373551.0,94726.0,"Hi Pulkit, pivot['Is_Total'] = pivot.age_group.apply(lambda x: x=='Total') -- gives the following error: 'DataFrame' object has no attribute 'age_group' .",310511.0
89716,373472.0,,nan,
91211,381109.0,,nan,
91211,381250.0,,nan,
91211,381785.0,,nan,
90253,376001.0,,nan,
90253,376016.0,,nan,
90253,376015.0,,nan,
90253,376247.0,,nan,
90253,376254.0,,nan,
90253,376504.0,,nan,
90253,376718.0,,nan,
90718,378096.0,95286.0,That's correct. loc gets rows (or columns) with particular labels from the index. iloc gets rows (or columns) at particular positions in the index (so it only takes integers).,319721.0
90718,378444.0,,nan,
90118,375298.0,94779.0,It worked,318328.0
90118,375342.0,,nan,
90118,375371.0,,nan,
90118,375701.0,,nan,
90915,379114.0,95522.0,"will try,thanks",318005.0
90915,379114.0,95532.0,"There are other simpler solutions too. You can group under a column but compute for multiple columns . Just provide an array as input. eg: manager_name[[employee_exp, emp_id, emp_name]].function()",318007.0
90915,379114.0,95835.0,"i am not able to perform concat,i tried but i cant, plz provide an example for it",318005.0
90915,379114.0,95984.0,thanku,318005.0
90915,379114.0,95871.0,"Here you go df = pd.DataFrame({'employee_id': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], 'employee_experience': ['1', '3', '2', '2', '4', '5', '6', '8','7', '1'], 'employee_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger'], 'manager_name': ['robert', 'charley', 'charley', 'donald', 'charley', 'robert', 'donald', 'charley', 'donald','ramesh'] }) pd.concat([df.groupby('manager_name')['employee_experience'].apply(list), df.groupby('manager_name')['employee_id'].apply(list), df.groupby('manager_name')['employee_name'].apply(list)], axis = 1) This is more concise version df.groupby('manager_name')['employee_experience','employee_id','employee_name'].agg(lambda x:list(x)) Making it generic to any dataframe of course can be done in this line and is good to do in long run especially for projects. But for learning the concept, these simpler versions address the need.",318007.0
90915,379426.0,95607.0,this is not giving the result,318005.0
90915,379707.0,,nan,
90915,379956.0,95985.0,thanku so much,318005.0
90735,378142.0,,nan,
90735,378144.0,,nan,
90735,378436.0,,nan,
90735,378344.0,,nan,
91104,380725.0,,nan,
91104,380765.0,,nan,
94457,398002.0,99668.0,You have to use if else(if else) inside lambda to achieve what you want to. Even i followed the same,318329.0
94457,398002.0,99589.0,If I try to simply replace 0 with 'na'...I won't get the required result,306250.0
94457,398388.0,,nan,
94741,,,nan,
90326,376445.0,,nan,
90326,376453.0,,nan,
90326,376437.0,,nan,
90326,376542.0,,nan,
90326,376454.0,,nan,
90326,376661.0,,nan,
90326,376751.0,,nan,
90326,376765.0,,nan,
90326,376806.0,,nan,
90326,377526.0,,nan,
90326,378587.0,,nan,
89724,372651.0,94553.0,Single slash ‘/‘ returns result with decimal values whereas double slash ‘//‘ returns only the quotient and avoids the decimal part.,314730.0
89724,372792.0,,nan,
89724,372657.0,,nan,
89724,372673.0,,nan,
89724,372748.0,,nan,
89724,372780.0,,nan,
89724,372795.0,,nan,
89724,372895.0,94388.0,"Hi Bhagyashree, you are saying vice versa.// will return whole number and / will return the float value.",306735.0
89724,372895.0,94703.0,"Agreed , its vice versa. Thanks!",320603.0
89724,373274.0,,nan,
89724,373453.0,,nan,
89724,374190.0,,nan,
89724,374281.0,,nan,
88828,368101.0,93267.0,Please check the question asked by original poster. You are explaining why vectorize function is faster than normal and not the issue with the code,306248.0
88828,368560.0,93539.0,"thanks, this was helpful",312490.0
88828,371831.0,94933.0,"Hi, I do not understand how this is different from the code put by Abhijeet. In his case he tried vectorize(lambda ) and you defined the function and then gave it in vectorize(). Why are the results different?",318079.0
88828,371831.0,95042.0,Hi Sai... Thanks for asking this question. It also cleared my concept as well. I think computation time for the lambda function in my answer increased due to some background process. Otherwise there are no performance advantages of using either. Found a stack overflow question asking the same. https://stackoverflow.com/questions/12264834/what-is-the-difference-for-python-between-lambda-and-regular-function https://stackoverflow.com/questions/44840101/in-python-lambda-expression-is-fast-or-the-normal-function,318479.0
89602,371887.0,,nan,
88851,368236.0,93209.0,That is to be done for graded questions. This is an ungraded question!,317998.0
88851,368236.0,93207.0,Please don't post exact answers in comments.,317689.0
88851,368336.0,,nan,
88851,368364.0,,nan,
88851,368853.0,,nan,
88851,368864.0,,nan,
88851,369006.0,,nan,
88851,369311.0,,nan,
90438,377151.0,,nan,
90438,377271.0,,nan,
90438,377182.0,,nan,
90441,377187.0,,nan,
90441,377200.0,,nan,
90437,,,nan,
88890,368474.0,93309.0,then why it is marked as note that it will through an error,312050.0
88890,368474.0,93323.0,"There is a different in statement written and passing list in the function. Statement written is np.vstack(a,b). In this, arguments are passed as individual elements not list. If you want to send list. It will be np.vstack((a,b)).",318368.0
88890,368437.0,,nan,
88890,369027.0,,nan,
89528,371551.0,93999.0,Ah! correct.,317998.0
89528,371648.0,,nan,
89528,371537.0,,nan,
89527,371538.0,,nan,
89527,371531.0,,nan,
89527,371651.0,,nan,
89527,371695.0,,nan,
89544,371705.0,,nan,
89544,371708.0,,nan,
88953,370389.0,,nan,
88953,368717.0,,nan,
88953,368760.0,,nan,
88953,368768.0,,nan,
88953,369035.0,,nan,
88953,369087.0,,nan,
88953,370015.0,94972.0,How can I know the shape to config the size of horizontal and vertical.,320687.0
88953,374109.0,,nan,
89923,374262.0,94618.0,"Thanks, I read the faq earlier but forgot.",318554.0
89923,374330.0,,nan,
89923,374388.0,,nan,
89926,374339.0,,nan,
89926,374335.0,,nan,
89926,374356.0,,nan,
89926,374385.0,,nan,
89926,374625.0,,nan,
90242,375964.0,,nan,
90242,375963.0,94922.0,thanks vinay,313770.0
90242,376022.0,,nan,
90242,375970.0,,nan,
89015,369030.0,,nan,
89015,369038.0,,nan,
89015,369047.0,,nan,
89015,376356.0,,nan,
89015,369053.0,,nan,
89082,369286.0,,nan,
89082,369556.0,,nan,
89082,369451.0,,nan,
89771,373234.0,,nan,
89771,373110.0,,nan,
89771,373077.0,,nan,
89619,372113.0,94153.0,"Take input array as np.array([[0,1],[1,0]]) and use np.tile function to solve this problem and take n as half in the tile function for both rows and columns",318358.0
89619,372113.0,94175.0,Sorry u need to take the input that I provided earlier as a list not an array,318358.0
89619,372113.0,94170.0,"but tile will create an array, I can't feed an array as a parameter.",302739.0
89619,371993.0,,nan,
89621,371980.0,,nan,
89621,372074.0,,nan,
89777,373053.0,,nan,
89777,372962.0,,nan,
89389,370773.0,,nan,
89389,370777.0,,nan,
89389,370792.0,,nan,
89389,370871.0,,nan,
89389,370844.0,,nan,
89780,372999.0,,nan,
89780,372990.0,,nan,
89780,373009.0,,nan,
89780,376512.0,,nan,
89780,383154.0,,nan,
89794,373105.0,,nan,
89794,373147.0,,nan,
90827,378608.0,,nan,
90827,378598.0,,nan,
90827,378648.0,95411.0,True.. bug in python it seems.. thanks!! 👍☺️,316349.0
89291,370216.0,,nan,
89291,370250.0,,nan,
89291,370353.0,,nan,
89291,373201.0,,nan,
89291,370385.0,,nan,
89333,370463.0,,nan,
89333,370424.0,,nan,
89333,373127.0,,nan,
91581,383379.0,,nan,
91581,383383.0,,nan,
89427,375985.0,,nan,
89427,371029.0,,nan,
91553,383170.0,96209.0,welcome. plz upvote the answer if it helped. thanks :),317998.0
91553,383170.0,96208.0,okk got it thanku,317558.0
91553,383169.0,,nan,
89374,370706.0,,nan,
89374,370735.0,,nan,
89343,370511.0,93750.0,Thanks Jai,300708.0
89343,370509.0,93695.0,eg 9 might be stored as 9.0000000000004 or 8.9999999999999999998 something like that. Which is influencing the computaion here.,318554.0
89343,370501.0,93749.0,Thanks Kanika for explanation. Could you send some references which details your explanation. Really sorry I am unable to recollect the concepts.,300708.0
89343,370501.0,93760.0,Thanks.,300708.0
89343,370501.0,93756.0,Please refer below links- https://en.m.wikipedia.org/wiki/Matrix_(mathematics) https://www.cs.mcgill.ca/~rwest/wikispeedia/wpcd/wp/m/Matrix_%2528mathematics%2529.htm https://www.mathsisfun.com/algebra/matrix-introduction.html#,300691.0
89778,372966.0,94336.0,Thanks Madanjit,318814.0
89778,372969.0,94337.0,"I got the answer why my error was there ""the syntax of hstack and vstack functions these take numpy arrays as input"" and I was not doing the same",318814.0
89778,373029.0,,nan,
89778,373049.0,,nan,
89778,373122.0,,nan,
89778,376723.0,,nan,
91550,383236.0,,nan,
91550,383136.0,96200.0,okk Thanx,317558.0
91550,383140.0,96201.0,okk Thanx,317558.0
103721,450633.0,,nan,
103721,445197.0,,nan,
103575,444357.0,,nan,
102465,438235.0,,nan,
102465,438228.0,,nan,
102465,438810.0,106365.0,even mobile app is not working perply for me. Any suggestions...,302741.0
98659,420084.0,,nan,
98659,420071.0,,nan,
98719,420393.0,,nan,
98719,420494.0,,nan,
98719,425058.0,,nan,
98719,433389.0,,nan,
102624,438955.0,,nan,
102624,439995.0,,nan,
103877,446129.0,107909.0,Did you try calling your student mentor ?,301655.0
103877,446129.0,108047.0,"There must be some misunderstanding at their end, let's hope for the best at the earliest",301655.0
103877,446129.0,108043.0,"yes - cc'd him, spoke to him. He said he has fwd'd my request to coaching team couple of times but nothing back from coaching team. Same issue with my loan default EDA assignment - had asked for a re-evaluation but have not heard back from re-eval team. Spoke/emailed my mentor and he said he has emailed them couple of times too but nothing back from them either. And I know exactly what is going to happen - Upgrad will start saying I did nt ask for re-eval in timely manner (which I definitely did ask for in timely manner) - and as there is no direct contact with staff (besides my mentor) - my request and re-eval (as well as maybe my coaching call) will just go into a black hole :-(",300694.0
103877,446384.0,,nan,
102681,439036.0,106362.0,Thanks,310502.0
102681,439108.0,,nan,
102681,439384.0,,nan,
106808,461327.0,110588.0,"This plots only one predictor against the target variable, The question is how you plot multivariate function (all predictors) vs target variable.",310974.0
106808,461797.0,,nan,
106897,461694.0,,nan,
106897,461841.0,,nan,
105789,456354.0,,nan,
105789,456336.0,,nan,
105789,456377.0,,nan,
105795,458092.0,109503.0,But without doing statistical analysis how will you know which column has high p value and hence it is not significant.,313767.0
105795,458092.0,109504.0,In my opinion you need to first do RFE and then do statistical analysis to see which feature is insignificant. Post that remove columns one by one.,313767.0
105795,458092.0,109550.0,"Apply your business logic, if some values are the same for almost all the row, Do you think it will have an impact on the target variable?? Still, if you do the analysis you will be wasting time and making model clumsy which can lead to complex model.",318344.0
105795,458092.0,109565.0,There is difference between almost same and exactly same. Even though I hope your approach generates desired results.,313767.0
105795,458092.0,109552.0,and EDA is always performed before modeling.,318344.0
105795,456488.0,109203.0,Why no of variables more than 100? bcoz of dummy variable creation?,308437.0
105795,456488.0,109254.0,yes due to segregation of brands you have to create multiple dummy variables plus no of categorical variables are also a significant.,313767.0
105795,456488.0,109705.0,May be due to Car Company name and Model name. I am not sure its really required. Without these two i got almost 50 variables,312019.0
105795,456479.0,,nan,
105795,457453.0,,nan,
105799,456521.0,,nan,
105799,458062.0,,nan,
105799,462513.0,,nan,
106913,461761.0,110337.0,"so,how to treat this type of features?Do I need to drop that?",320635.0
106913,461761.0,110443.0,"Yes, drop one and check the VIF for the rest and take further calls",311160.0
106913,461833.0,,nan,
105806,458081.0,,nan,
105806,456525.0,,nan,
105806,456519.0,,nan,
105806,456989.0,109347.0,Training data used for model training will be having more number of columns than the x_train_lm which contains the significant columns only. Please correct me if I am wrong,300719.0
105806,456989.0,109380.0,number of columns in x_train_lm should be equal to the number of columns passed into the final model,344894.0
106918,461789.0,,nan,
106918,461827.0,,nan,
105817,456630.0,109249.0,If we see almost 16 numerical variables. we should do for each variable uni-variate analysis and conclude whether to consider or not ? Please suggest,312019.0
105817,456656.0,109251.0,Yes.,318329.0
105817,456656.0,109250.0,you mean do pair-plot withe few variables ? to understand,312019.0
105817,456906.0,,nan,
105814,456676.0,109282.0,Both should work,344894.0
106930,461845.0,,nan,
106930,462115.0,,nan,
106933,461859.0,,nan,
106933,463165.0,,nan,
106894,461649.0,,nan,
106894,461842.0,,nan,
105823,456625.0,109235.0,Tough to drop columns right...? At first glance I felt not more than 12-13 columns make sense while buying a car...but again how to decide these columns to retain??,308437.0
105823,456625.0,109281.0,Based on VIF and P-Value,344894.0
105823,456625.0,109343.0,How to choose variables which make business sense ? Criteria to eliminate? When ur dropping variables u must be double sure na that’s my concern,308437.0
105823,456659.0,,nan,
105823,458085.0,109573.0,"yeah, go ahead, that's why you do EDA",318344.0
105823,458085.0,109553.0,"There were some insignificant variables found during EDA, but am thinking whether to drop them at first instance....",308437.0
105820,456632.0,,nan,
105820,456628.0,109421.0,"I have doubt here removing 2nd dummy one. for example fueltype dummies will be like diesel and gas. if we remove fueltype ok. but if we remove (n-1) diesel or gas, we may missing insights for one them properly ?",312019.0
105820,456628.0,109549.0,It all depends on the significance of the variable.,310974.0
105820,456628.0,109546.0,"Hello ram, for category variable which has 21 levels creating dummy variable is a good idea???",308437.0
106932,461886.0,,nan,
106938,461914.0,,nan,
106938,462037.0,,nan,
106938,461915.0,,nan,
106938,462059.0,,nan,
106938,462289.0,,nan,
106947,462128.0,110378.0,Yes,318451.0
106947,462128.0,110375.0,"So, should I consider my value as a good and final value ?",301655.0
106947,462454.0,,nan,
105873,456907.0,,nan,
105873,456883.0,109268.0,"Example for fuel-type if we make dummy variable.we have diesel and gas. If we remove one of them, it may be difficult to predict price w.r.t diesel or gas whichever is dropped. Or my understand may be wrong . pls clarify.",312019.0
105873,456883.0,109284.0,"If you remove diesel and keep gas , then in column gas will be look like {1,1,1,0,1,0 ........}, where 0 means no gas , so it means diesel .",344894.0
105873,456883.0,109908.0,Thnakyou. Fine. But for doing insights how to decide diesel system car price is more or gas system car price i s more. Is there any way to decide this ?. Same doubt for more number of levels of varible,312019.0
105873,458091.0,,nan,
114363,492816.0,115914.0,Thanks Krishnan,306243.0
114363,493098.0,115913.0,Thanks Vinay,306243.0
114363,493098.0,115996.0,"If you have scaled using sklearn.preprocessing standardscaler / minmaxscaler, then you can use inverse_transform() method to get the original representation. However, not sure if scaling of the dependent is necessary, see if that can be avoided.",313826.0
114363,493098.0,115987.0,"Vinay, as you said, I have performed individual steps for both the data sets Train and Test. Now one more question: In the train data set I have ""price"" column and in the test data I need to predict the price based on the train data set. I applied scaling function on the price column. After creating Model in the train data set, I applied the train model on the test data set. Now when I checked the y_prediction of the test data (to know the price), it shows the price in scaling not the actual price. Any idea how to decode or rescale that price column.",306243.0
114363,493783.0,,nan,
114363,492833.0,,nan,
105875,456908.0,,nan,
105875,458075.0,,nan,
105879,456916.0,109275.0,"If we explore only model, then we can observe that the number of models that are present will be around 140 and the total number of rows are 205. From this, I feel that it is insignificant.",318329.0
105879,456916.0,109276.0,"We were specifically asked to ignore model. Then, why would we do that? I'm asking about car company not model.",310974.0
105879,456916.0,109270.0,You mean remove car model? And I'm sorry I didn't get what you said,310974.0
105879,456916.0,109278.0,I understood it as the other way around (Y),318329.0
105879,457001.0,109314.0,"The assignment states that Geely Auto shall manipulate the design of the cars, the business strategy etc. to meet certain price level. So which car company which model demands higher price, might be useful strategy to meet certain price level",317514.0
105879,457001.0,109285.0,"I know thats the usual route. But shouldnt we think of busines relevance while building a model? I mean, how can the fact that 'a car being a bmw or a porsche has a high price' be beneficial to Geely Auto? It cannot become a BMW or a Porsche.",310511.0
105879,457001.0,109292.0,Precisely my point,310974.0
105879,457001.0,109506.0,"I don't think it's ALWAYS best to run linear regression model on all the variable, if you add just a column with random values and run regression model you may find VIF and P- values considerable to not drop that column, see with your business knowledge if it is relevant to keep the column, try to derive new column if possible from the column itself for more relevance and try to proceed, don't drop any column till you have a strong reson to drop.",318344.0
105879,457748.0,,nan,
105879,458096.0,,nan,
106951,462117.0,,nan,
106951,462121.0,,nan,
105886,457004.0,109306.0,"Making two set is good idea. sns.heatmap(df.iloc[:,:50].corr()) sns.heatmap(df.iloc[:,50:].corr()) price we should keep in both the sets ?",312019.0
105894,457039.0,,nan,
105894,457101.0,109303.0,"yeah correct...infinity means very high value (ofcourse), so its better to drop such variables from the model ( may be by looking at the correlation with other variables as well)",304814.0
105905,457103.0,,nan,
105905,457095.0,,nan,
105905,457112.0,,nan,
105924,457190.0,,nan,
105924,457204.0,109551.0,Whether u use get_dummies or label encoder both should produce same results right?...u mentioned labelencoder to be used only for ordinal categorical variables....,308437.0
105924,457612.0,,nan,
107063,462540.0,,nan,
107063,462591.0,,nan,
107006,462131.0,110357.0,Thankyou,312096.0
107006,462517.0,,nan,
107006,462372.0,,nan,
107011,462277.0,,nan,
107011,462417.0,110428.0,I am not getting Nan values while doing the model on the train set. The p values and VIF are okay. When i am fitting the model on the test set then i am getting the NAN values. What should be done in such a case?,311254.0
107012,463239.0,,nan,
107012,462175.0,,nan,
107012,462404.0,,nan,
107012,462413.0,,nan,
107015,462169.0,110362.0,I agree with you. My understanding was also the same. Waiting for TA's to confirm,317991.0
107015,462391.0,,nan,
107024,462228.0,,nan,
107024,462269.0,,nan,
107024,462381.0,,nan,
107029,462285.0,,nan,
107029,462265.0,,nan,
107029,462379.0,,nan,
107029,463816.0,,nan,
107031,462283.0,,nan,
107031,462449.0,,nan,
107031,462332.0,,nan,
107035,462315.0,110435.0,"Shital, What is there to correct? mfi & spfi are actual fuel system and are not type. Since there is not much data around it, shall we drop both along with car company mercury. TA please verify",302742.0
107035,462581.0,,nan,
107035,462852.0,,nan,
107036,462348.0,,nan,
107036,462368.0,,nan,
107036,463084.0,,nan,
106015,458376.0,109761.0,Even I have same doubt.Please clarify,308638.0
106015,458376.0,109762.0,I converted symboling from int to string and converted to dummy as it is multilevel categorical variable. Is it right approach,308638.0
106015,458376.0,109784.0,Yes I converted .astype(object). Did dummy variable for this. Seems we can go with this,312019.0
106015,458376.0,109938.0,Thanks NagaPrasad,308638.0
106015,458373.0,,nan,
106015,457486.0,,nan,
106015,457538.0,,nan,
106015,457640.0,,nan,
107051,462574.0,,nan,
107051,462470.0,,nan,
107051,462826.0,,nan,
107042,462370.0,,nan,
107047,462386.0,,nan,
107048,462445.0,,nan,
107048,462833.0,,nan,
107021,462207.0,,nan,
107021,462272.0,,nan,
106961,462457.0,,nan,
106572,460315.0,110485.0,"I dont have any string columns in x_train, still I too get the same error dtypes: float64(13), int64(1), uint8(50) I am trying to add_constant though sm.add_constant(x_train_rfe), not on VIF calculation step",320074.0
107003,462436.0,,nan,
107003,462134.0,110350.0,"def binary_map(x): return x.map({""std"": 1, ""turbo"": 0}) carprice['aspiration'] = carprice[['aspiration']].apply(binary_map) Similar to what they have used in the lectures",317141.0
107003,462134.0,110353.0,"Try this code >> varlist = ['aspiration'] >> def binary_map(x): return x.map({""std"": 1, ""turbo"": 0}) carprice[varlist ] = carprice[varlist].apply(binary_map)",318451.0
107003,462134.0,110354.0,Strange. This works. Do you know why?,317141.0
107003,462134.0,110356.0,varlist = ['aspiration'] May be you had not given this command earlier Also in your earlier code carprice[['aspiration']] < double brackets were not needed,318451.0
106062,457617.0,,nan,
106062,457638.0,109461.0,Yes,344894.0
106062,457638.0,109418.0,Correct....but I should go ahead with these so many dummy variables??55...,308437.0
107059,463237.0,,nan,
107059,462573.0,,nan,
107059,462619.0,,nan,
107059,462823.0,,nan,
107059,463054.0,,nan,
106068,,,nan,
106069,457731.0,109479.0,Then how do i map the 0 and 1 with the different categories??,314313.0
106069,457731.0,109528.0,get_dummies() will do it automatically for you. As Ashish mentioned kindly go through the dummy variable section again to know about it.,311160.0
106069,457731.0,109548.0,But get dummies will results in many many additional columns...that's the worry here...analysis becomes difficult,308437.0
106069,458100.0,,nan,
107061,462542.0,,nan,
107061,462618.0,,nan,
107061,463076.0,,nan,
107061,462812.0,,nan,
106076,457734.0,,nan,
106076,457746.0,,nan,
106076,458045.0,,nan,
106076,458070.0,,nan,
106076,461411.0,,nan,
106080,457839.0,,nan,
106080,457944.0,,nan,
106099,457950.0,109489.0,"Hi Sham, I did not quite understand your answer. If two variables are highly corelated, then why can not we drop one of them? Wouldn't keeping two highly correlated variable anyways lead to high VIF in the later stage of model, so would not dropping one of them early be beneficial? Am I missing something here?",304814.0
106099,458007.0,109514.0,"Don't drop one of them straight away. VIF will take care of high correlations, drop the one with high VIF.",318344.0
106099,458007.0,109580.0,But why? When you can reduce the number of variables early in the game then why not. How would the final result differ using so?,304814.0
106099,458007.0,109638.0,"See Anshul my point is, if you drop A, it is quite highly correlated with B . when modeling it comes out you should drop B not A, as A can be used to make a better model, like keeping A gives a R sq value of 0.85 and keeping B gives 0.83 but you have dropped A so you have compromised on the model before analysis.",318344.0
106099,458007.0,109625.0,"How can you be sure which one to drop, secondly the one you dropped may be more important in the model.",318344.0
106099,458007.0,109629.0,"if two variables A and B are highly correlated, and assume that you drop A, What issue would it have in the final model? If you dont drop A and B,and generate VIF, then I am sure both of them would have high values, so wont you delete one of them then? removal of these highly correlated variables would not help you getting a different model, but would only help to eliminate redundant variable early in the game",304814.0
106099,458007.0,109687.0,How would we decide which one to drop if only based on correlation?,319302.0
106099,458007.0,109721.0,So can anyone confirm this ? Should we drop one of the highly co-related value or should we do continue with building the model and then take decision to delete it? Because I thought that using the heatmap for analysis prior to building the model is to remove the highly co-related value.,301114.0
106099,458011.0,109518.0,Two variable may be highly correlated but there maybe any other story attached with the variable you drop. You may loose information dropping a column without analysis. so let the stats do its work to find out which one to drop.,318344.0
106099,458112.0,,nan,
106081,457840.0,,nan,
107070,462588.0,,nan,
107070,462569.0,110463.0,I checked there is no null value in the cleanup data. But I have only one row with 1 value for these two parameters.Is this because of very less amount of data for that particular value,317980.0
107070,462610.0,,nan,
107070,462801.0,,nan,
107065,462804.0,,nan,
107065,463235.0,,nan,
107065,462541.0,110426.0,Looking for something like where I specify column datatype as object and get unique values of every column in dataframe,317980.0
107065,462541.0,110466.0,df1 = df.select_dtypes(include=['int']) df1.nunique(),344894.0
107065,462541.0,110464.0,Looking for something like where I specify column datatype as object and get unique values of every column in dataframe,317980.0
107065,462570.0,,nan,
106108,458076.0,109498.0,"Whole of the column information in company name seems to be irrelevent, but pulling brand name/company name is important. Assume real life instance where car price depends upon the brand name of the car.",313767.0
106108,457935.0,,nan,
106108,457946.0,,nan,
106108,458109.0,,nan,
106108,458473.0,,nan,
106108,459022.0,,nan,
106108,459026.0,,nan,
107073,462586.0,,nan,
107073,462602.0,110473.0,This means that the value what i have got is not good?,301114.0
107073,462602.0,110541.0,"Hi Jayashree, I found this very useful to understand R-squared. Please go through this https://people.duke.edu/~rnau/rsquared.htm Even my R-squared value is 0.88 .",319006.0
107073,462799.0,,nan,
107080,462683.0,,nan,
107080,462708.0,,nan,
107080,462794.0,,nan,
106137,458107.0,,nan,
106138,458106.0,,nan,
106138,458174.0,,nan,
107085,462720.0,110446.0,"No other variable has VIF greater than 5, stuck in this dilemma",313770.0
107085,462720.0,110447.0,what was your previous R square value prior to dropping this particular variable and the current R square value,311160.0
107085,462720.0,110448.0,906 and 837,313770.0
107085,462720.0,110449.0,"That's quite a dip. After dropping this variable, do you have everything in control? Else, I think you need to re-consider the dropping sequence, maybe few of the earlier dropped variables are relevant.",311160.0
107085,462720.0,110450.0,let me give it a try and check other dropping sequence if possible.thanks,313770.0
107085,462740.0,,nan,
107085,463060.0,,nan,
106155,458910.0,,nan,
106155,458214.0,109665.0,"Not good,Try to reduce gap.",344894.0
106155,458214.0,109545.0,Getting 80% for train and 75% for test. So is my model good ?,311952.0
106155,458214.0,109880.0,Yes Darshna,344894.0
106155,458214.0,109826.0,I got r square value 91% for train data set and 91.2% for test data set. Is my model good?,310419.0
106155,458271.0,,nan,
106181,,,nan,
106174,458347.0,109631.0,almost 0 also can be sorted and they are relative to each other? Where to draw the line?,318329.0
106174,458347.0,109607.0,Yes but what would you do if all p values are almost 0,310974.0
106174,458347.0,109576.0,Hey Ram. This is really helpful. I had the same doubt but follow similar procedure with my intuition/understanding. Where did you find this?/,318329.0
106174,458347.0,109577.0,Lecture notes,310974.0
106174,458347.0,109586.0,"Hi, i have p-values all the variables <0.05 but still some variables have high VIF, how can i proceed ? i have to delete the variable which has high VIF because all those variables have p-values like 0.001 i,e near to 0? please help",300733.0
106174,458347.0,109595.0,You have to drop the variable which has highest VIF,310974.0
106174,458347.0,109596.0,but the point 6 says remove the variable with relatively less significant. doesn't that mean to drop the variable with relatively high p value?,318329.0
106174,458347.0,109632.0,"if you do lm.params_ and sort it, you can see that they are greater than zero and can be ranked.",318329.0
106174,458347.0,109786.0,I have VIF very high values like 60. But they are having very good correlation(max value)with price when i did heatmap. Not sure what decision to be taken. We can drop ?,312019.0
106174,458347.0,109895.0,"Don't worry about correlation. Drop, observe the decrease in R2 and F-statistics. If it's huge, drop something else, otherwise, go ahead.",318329.0
106174,458347.0,109906.0,Thank you,312019.0
106174,458658.0,109679.0,"Yes using RFE would be a better option, then we can manually tweak.",318756.0
106174,459566.0,,nan,
106175,458344.0,,nan,
106175,458345.0,,nan,
106175,458426.0,,nan,
106175,458500.0,,nan,
106175,458405.0,,nan,
106179,458359.0,,nan,
107103,462862.0,,nan,
106199,458446.0,,nan,
106199,458448.0,109729.0,"A predictor variable with low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to regression model because changes in the predictor's value are related to changes in the output variable. Conversely, a larger (insignificant) p-value suggests that changes in the predictor are not associated with changes in the response. But Multicollinearity causes the following as discussed in lectures The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become very sensitive to small changes in the model. It reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. We might not be able to trust the p-values to identify independent variables that are statistically significant. So we look at VIFs as well. Hope this helps u Refer this: https://stattrek.com/multiple-regression/multicollinearity.aspx",308638.0
106199,459586.0,,nan,
106199,458903.0,,nan,
107106,462858.0,,nan,
107111,463042.0,,nan,
107111,462889.0,,nan,
107113,462895.0,,nan,
107113,463039.0,,nan,
106219,458611.0,109670.0,"Hi, Check both parallelly.",344894.0
106219,458611.0,109643.0,So I should delete variable based on p value and check vif...if vif is high then rebuild model by deleting high vif variable and repeat...right??,308437.0
106219,458611.0,109645.0,"I've confusion regarding a case where all p-values are less than 0.05 and vif's greater than 5 and I'm yet to get it clarified. All other cases where p-value is greater than 0.05, drop based on p-value. At least in the videos, this was followed.",318329.0
106219,458611.0,109873.0,we do not look p-value for constant,344894.0
106219,458611.0,109815.0,"there is no defined procedure - drop based on p-value then rebuild model? or drop based on p-value, check VIF and again rebuild model? i have a case wherein the p-value for constant is very high and if i drop it VIF for all other variables is increasing",308437.0
106218,458559.0,,nan,
106218,458668.0,,nan,
106218,461396.0,,nan,
107138,463081.0,,nan,
107131,463012.0,,nan,
107131,463015.0,,nan,
106252,458792.0,,nan,
106252,458807.0,109764.0,I am using random.seed(0) in this case and still getting the same,318576.0
106252,459540.0,,nan,
106318,459082.0,,nan,
107165,463225.0,,nan,
107165,463435.0,,nan,
107168,463230.0,,nan,
107168,463308.0,,nan,
107168,463229.0,,nan,
106263,458791.0,,nan,
106263,458772.0,,nan,
106263,461393.0,,nan,
107585,465467.0,,nan,
107585,465729.0,,nan,
107585,465602.0,,nan,
107586,465512.0,,nan,
107586,465728.0,,nan,
107586,465842.0,,nan,
107174,463284.0,,nan,
107174,463299.0,,nan,
107174,463425.0,,nan,
107174,463914.0,,nan,
107174,464320.0,,nan,
107175,463298.0,,nan,
107175,463324.0,,nan,
107169,463234.0,,nan,
107169,463286.0,,nan,
107169,463303.0,,nan,
108357,468202.0,,nan,
108357,468227.0,,nan,
107181,463384.0,110574.0,"Thanks, it helped. Was getting confused on what takes priority p>0.05 or VIF >10",317514.0
107181,463402.0,110573.0,"Thanks for your response. Regarding ""Also - your examples all had a p value < 0.05; If there were p values higher than 0.05 then those features would be dropped first by me before I look at VIF at all""...even that was my belief but did not get me the right results",317514.0
107181,463815.0,,nan,
107181,463580.0,110869.0,Thanks all,317514.0
107181,463580.0,110854.0,https://cdn.upgrad.com/UpGrad/temp/a852ca8a-d1b9-4779-8f86-fad3e722c1b6/Lecture+Notes+-+Linear++Regression.pdf,344894.0
107181,465196.0,,nan,
106269,458820.0,109707.0,almost is again different.,318329.0
106269,458820.0,109704.0,"Not really, they must be almost the same",301655.0
106269,458928.0,,nan,
106269,458834.0,,nan,
106280,458907.0,,nan,
106280,458922.0,109888.0,It is anyway similar to using the 0 and 1 method I think and also leads to less number of variables,311864.0
106280,459139.0,,nan,
106280,458882.0,,nan,
106279,458884.0,,nan,
106279,458917.0,,nan,
106279,458976.0,,nan,
106285,458892.0,,nan,
106285,458881.0,,nan,
106285,458906.0,,nan,
106285,459602.0,,nan,
106303,458959.0,,nan,
106303,458966.0,,nan,
106303,459132.0,,nan,
106303,461385.0,,nan,
106295,458967.0,,nan,
106295,459138.0,,nan,
106295,459550.0,,nan,
107196,463510.0,110572.0,Is it advisable to drop the intercept or constant in multiple regression ?,308495.0
107196,463828.0,,nan,
107196,463916.0,,nan,
106292,458916.0,,nan,
106292,458929.0,,nan,
106292,458968.0,,nan,
106292,458946.0,,nan,
106292,461089.0,,nan,
106292,459573.0,,nan,
106292,463882.0,,nan,
107217,463698.0,,nan,
107217,463669.0,110598.0,the condition holds for constant also?,300733.0
107217,463669.0,110603.0,"No, the condition doesn't hold good for the constant",311160.0
107217,463669.0,110602.0,Yes!,319006.0
107217,463787.0,,nan,
107217,463836.0,,nan,
107217,463808.0,,nan,
106305,458973.0,,nan,
106305,459129.0,,nan,
106305,459145.0,,nan,
106314,459085.0,109935.0,But how to scale it back ?,310509.0
106314,459322.0,109934.0,I meant x is categorical and not y. So linear regression is still the right approach,310509.0
106307,459177.0,,nan,
106307,459127.0,,nan,
106307,459034.0,,nan,
106341,459211.0,,nan,
106341,459163.0,,nan,
106386,459403.0,109865.0,"VIF for a constant would always be high because it doesn't have any effect on the Independent Variable. As we know that if 2 variables are highly correlated, one doesn't add any value with the presence of others and would have high VIF. Likewise, constant would never add any value to Independent Variable and would have high VIF. Hence, it has been taught to drop constant before calculating VIF.",311160.0
106386,459403.0,109852.0,I know but can u answer the reason behind my observation?,308437.0
106385,459324.0,,nan,
106385,459472.0,,nan,
106228,458612.0,109816.0,"in housing case study pls check, const is deleted at the last step",308437.0
106228,458612.0,109817.0,"in housing case study pls check, const is deleted at the last step",308437.0
106228,458612.0,109875.0,Please read below discussion https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model,344894.0
106403,459436.0,109958.0,oK. Let a TA confirm this.,304319.0
106403,459491.0,,nan,
106415,459524.0,109879.0,Tried this already. Same results,318780.0
106415,459524.0,109936.0,"Hi, Try this https://github.com/statsmodels/statsmodels/issues/588/",344894.0
106415,459524.0,109959.0,"I tried the below methods, still not working",318780.0
106415,459524.0,109977.0,"Just uninstall the statsmodels and reinstall it , also do for patsy",344894.0
106423,459589.0,,nan,
106423,459650.0,,nan,
106423,459680.0,,nan,
106423,459887.0,,nan,
106423,461382.0,,nan,
107149,463158.0,,nan,
107277,464183.0,,nan,
107277,464172.0,,nan,
107277,464202.0,,nan,
107277,464190.0,,nan,
107277,464334.0,,nan,
107269,464158.0,110704.0,"Only 13 numeric variables in the train DF,but 98 uint8 variables in the rfe",301115.0
107269,464158.0,110705.0,You should not create dummy variables for numeric variables.,313826.0
107269,464158.0,110708.0,"sorry I mean to say 98 variables are for non-numeric ones ,13 variables which are numeric",301115.0
107269,464158.0,110738.0,"Thank you,its engine size and was thinking as categorical",301115.0
107269,464158.0,110713.0,"I can see a few variables like 119, 134, 136 etc. How did you get these variables?",313826.0
106443,459841.0,,nan,
106443,459844.0,,nan,
106443,459916.0,110183.0,"I am getting this for Carmake 'mercury' and fuelsystem (mfi,spfi) I also have created a dummy variable for both the columns and the values are 0 and 1 still getting NaN inVIF. TA you have verified the above answer please can you further assist ?",314197.0
106414,459508.0,,nan,
106414,459517.0,,nan,
106458,459906.0,,nan,
106458,459838.0,,nan,
106458,459834.0,,nan,
106461,459866.0,,nan,
106461,461144.0,,nan,
105487,454390.0,108883.0,There are more than 20 values in one such column and 6-7 values in 2-3 columns. Dummy is the only option?,311686.0
105487,454574.0,,nan,
105487,454573.0,108954.0,"Hi Chandan, Yes you can use it. Basically, it will give a numeric value to each category which is accept by model",344894.0
105487,454573.0,109028.0,Thanks Paras. One more question. Can VIF be used before creating dummy variables or before building first model?,311686.0
105487,454573.0,108904.0,"Hi Paras, I explored and read about it. But it will again provide categorical (numeric) values. Can we use such values directly as numerical in our model then? Pls suggest as early as possible. I have already moved ahead with Dummy approach.",311686.0
105487,455300.0,109027.0,I am assuming here that 'Car Company' value might influence price. Not sure though. Yet to prepare the model completely.,311686.0
105487,455300.0,109202.0,car company will definitely be significant,308437.0
105487,457111.0,,nan,
105487,458055.0,,nan,
106499,460036.0,,nan,
106499,460142.0,,nan,
106499,460244.0,,nan,
106501,461369.0,,nan,
106501,460041.0,,nan,
106530,460238.0,,nan,
106530,460140.0,,nan,
106530,461349.0,,nan,
106528,460239.0,,nan,
106528,460139.0,,nan,
106528,461142.0,,nan,
106528,461268.0,,nan,
106528,462052.0,,nan,
106537,461140.0,,nan,
106535,462519.0,,nan,
106535,460236.0,,nan,
106535,460261.0,,nan,
106535,461315.0,,nan,
106535,461345.0,,nan,
106541,460193.0,,nan,
106541,460190.0,,nan,
106541,461339.0,,nan,
106511,461132.0,,nan,
106511,460241.0,,nan,
106519,460088.0,,nan,
106519,460145.0,,nan,
106573,460311.0,,nan,
106340,459165.0,109791.0,I have doubt. You said initially do model with selected variables.fine But again step 1 says build build with all variables ? Which one to follow ?,312019.0
106340,459165.0,109792.0,Final model should be with selected variables. Start with high number of variables. Keep on reducing based on above steps.,317689.0
106340,459165.0,109808.0,You can do RFE to select a suitable number.,317689.0
106340,459165.0,109793.0,High number of variables means ? Suppose i have 50 variables. We should take 50 or some correlation value of these variables with price from heatmap ? Sorry for the trouble.,312019.0
106340,459318.0,,nan,
106765,461097.0,,nan,
106765,461118.0,,nan,
106766,461103.0,,nan,
106766,461497.0,,nan,
106766,461908.0,,nan,
106766,461771.0,110342.0,in case2 if we drop X2 for eg. during feature selection how can we say that constant is for X4 or X2. with this X4 data will be lost right?,318436.0
106766,461771.0,110408.0,if you drop X2 then X1X3X4 == 000 if X2 =1 Y= A1X1 + A3X3+ A4X4 + C' Y = C' Just constant value will changed,344894.0
106766,462890.0,,nan,
106767,461263.0,,nan,
106767,461773.0,,nan,
106768,461791.0,,nan,
106777,461228.0,,nan,
106777,461273.0,,nan,
106474,459902.0,,nan,
106474,459914.0,,nan,
106474,459939.0,,nan,
106474,461063.0,,nan,
106474,461090.0,,nan,
106474,459929.0,,nan,
106474,461670.0,,nan,
106801,462440.0,,nan,
106803,461329.0,,nan,
106542,460187.0,110193.0,Thank you,308435.0
106814,461330.0,,nan,
106814,461647.0,,nan,
106819,461361.0,,nan,
106819,461828.0,,nan,
106827,461780.0,110292.0,"Yes Premnath, need to include all steps, with all results in Notebook, Comments, Explain your results in plain text in notebook.",344894.0
106829,461481.0,,nan,
106829,461390.0,,nan,
106829,461419.0,,nan,
106829,462053.0,,nan,
106831,461389.0,,nan,
106861,461844.0,,nan,
106861,461464.0,110230.0,but its not necessary that every time first variable will have multicollinearity.,318429.0
106861,461464.0,110274.0,Yes you should. For linear regression you need to keep only the numerical variables for modeling. It does not accept categorical/string variables. Once you create the dummy variables drop the categorical variables.,312376.0
106861,461518.0,,nan,
106861,461531.0,,nan,
106863,461569.0,,nan,
106863,461457.0,,nan,
106863,461818.0,,nan,
106863,463195.0,,nan,
106863,463409.0,,nan,
106440,459847.0,,nan,
105824,456624.0,109234.0,Yes this was my doubt..after we extract car company can we discard original column containing full car name?,308437.0
105824,456626.0,,nan,
105824,456689.0,109283.0,"Yes Chandan, If 'audi 100ls' then only 'audi' should be considered in the model.",344894.0
106828,461539.0,,nan,
106828,461600.0,,nan,
106866,462431.0,110506.0,What assumptions it will not follow?,317070.0
106866,463107.0,,nan,
106425,459649.0,110316.0,so we should give a value to n for sure. but i proceeded without giving any value for n and dropped variables one by one. is it ok?,300733.0
106425,459649.0,110270.0,"HI, i can proceed without giving value for n also right? something like rfe(lm). if i use like that, is it correct or do i need to pass some value to n for sure?",300733.0
106425,459649.0,110273.0,you have to pass a number that you would like to reduce the number of features to,318329.0
106425,459662.0,,nan,
106425,459716.0,,nan,
106425,461379.0,,nan,
106876,461814.0,,nan,
106876,461630.0,,nan,
106045,457738.0,,nan,
106045,457618.0,109516.0,Yes. CarName was already dropped. Thanks for the answer,303228.0
106045,457963.0,,nan,
106045,458063.0,,nan,
106045,458911.0,,nan,
106881,461609.0,110264.0,Can a TA verify this please?,312376.0
106881,461853.0,,nan,
106881,462797.0,,nan,
107134,463032.0,,nan,
107129,463026.0,,nan,
107129,463058.0,,nan,
107129,464335.0,,nan,
107176,463300.0,,nan,
107176,463297.0,,nan,
107176,464327.0,,nan,
107176,463839.0,,nan,
110716,476660.0,,nan,
128992,563389.0,,nan,
128992,563433.0,,nan,
128360,560304.0,,nan,
128414,560495.0,,nan,
128414,560830.0,,nan,
128414,560842.0,,nan,
127816,557723.0,126863.0,Thank you!,308637.0
127816,557959.0,,nan,
127816,557858.0,126866.0,"@Vipul, If you can clear one more doubt as part of this question, it'll be a great help. How would we use such tools to measure popularity of a singer after a concert in a remote Himalayan region? This was the example given in the lecture video.",308637.0
127816,557858.0,126864.0,Thank you!,308637.0
127966,558408.0,125666.0,Just to add here data cleansing is part of ETL processes while loading data in traditional datawarehouses which are essentially relational DB at the core.,317689.0
127966,558426.0,,nan,
127966,558968.0,,nan,
128359,560290.0,,nan,
128359,560323.0,,nan,
128359,560723.0,,nan,
132767,579572.0,,nan,
99634,426673.0,,nan,
99634,426450.0,,nan,
97447,417084.0,102885.0,"https://learn.upgrad.com/v/course/208/session/18002/segment/91561 , video shows the same (at 2:30)",309211.0
97447,417097.0,102886.0,"Thank you! I watched video again (https://learn.upgrad.com/v/course/208/session/18002/segment/91561 , video shows the same (at 2:30))",309211.0
97447,419794.0,103428.0,"I already received the answer on the same day I posted the question. you can find it here: https://learn.upgrad.com/v/course/208/session/18002/segment/91561 , video shows the same (at 2:30)",309211.0
97447,420094.0,103513.0,"The balls are being drawn one by one. Meaning you draw a ball, note its colour and put in back before drawing the next ball. You repeat this 4 times, hence 4 blue balls is definitely possible.",300717.0
97447,420094.0,103522.0,"Girish, It's in the video : (https://learn.upgrad.com/v/course/208/session/18002/segment/91561 , the problem was that although professor explained it in that video, I missed making a note of it",309211.0
97447,420094.0,103515.0,good answer,301113.0
97447,420111.0,103486.0,thank you Mahima!,309211.0
97447,420259.0,103521.0,"I already accepted the answer from Harsha Ravi. Thank you! I watched video again (https://learn.upgrad.com/v/course/208/session/18002/segment/91561 , video",309211.0
97447,420388.0,,nan,
98317,418321.0,103599.0,thanks,304814.0
98317,418321.0,103594.0,Yes. And the experimental probability is slightly different from the theoritical probability beacuse of few no. of trials. If the no. of trials tend to infinity then the theoritical probability will be equal to the experimental probability.,304319.0
98317,419312.0,,nan,
98317,419781.0,103946.0,"Yeah, I have written this thing on all question like this still, no one acknowledged this except you. I hope people understand probability rather than applying the formulas.",318344.0
98317,419781.0,103945.0,"I was about to write this before I found it already here. It should be kept in mind that the formula of probability being favorable possibilities/total possibilities applies only if all such possibilities are equally likely. Here since the number of blue and red balls are not same, we cannot take possibilities to be equally likely as obviously picking red is more favorable than picking blue.",318079.0
98317,419781.0,103947.0,Even TA has also verified the answers which I think does not explain the pain points of the one who questioned.,318344.0
98317,419781.0,103995.0,"Anshul thanks for acknowledging, we will keep helping each other.",318344.0
98317,419781.0,103986.0,thanks folks....and Ashish i up-voted your answer on my other post which is similar. Your notes did help Thanks again,304814.0
98317,420140.0,,nan,
98317,420519.0,,nan,
98317,420749.0,,nan,
97288,416434.0,102755.0,Thanks for answering but in the video it never talks about that. It just talks about X = 2 where X is number of red balls.,304813.0
97288,416443.0,,nan,
97288,416452.0,,nan,
97288,416453.0,102763.0,"the concept still remains the same, irrespective of number of balls.",312490.0
97288,416453.0,102762.0,total number of balls is 6.,304813.0
97288,419788.0,,nan,
97288,416534.0,,nan,
101244,435692.0,,nan,
98315,418365.0,,nan,
98315,419773.0,103481.0,"oh yeah, this makes sense....I completely forgot about the basics. Thanks",304814.0
98315,420106.0,,nan,
98315,420137.0,,nan,
98315,420723.0,,nan,
98315,422282.0,,nan,
98660,,,nan,
99097,422016.0,,nan,
99097,422491.0,,nan,
98706,,,nan,
98734,420581.0,103671.0,"You can use basic concept of PnC. In round one you have two diff coloured balls available (blue or red) , no of ways you can pick is 2C1. Same in round 2 and so on. Total number of outcome will be round1*round2....round4 that is 2C1*2C1*2C1*2C1, (one for each round)",316323.0
98734,420672.0,,nan,
98734,420582.0,,nan,
98734,420734.0,,nan,
99148,422335.0,,nan,
99148,422441.0,,nan,
99359,423945.0,,nan,
100950,433633.0,,nan,
99201,424532.0,104158.0,Thanks Kapil,318335.0
99201,422717.0,103974.0,This is what I wanted to ask. What if I don’t do them? Would that impede my progress in future courses?,318335.0
99201,423026.0,,nan,
99201,424872.0,,nan,
99201,432702.0,,nan,
103925,447247.0,,nan,
103925,446533.0,,nan,
106919,461782.0,110301.0,"its same dataset, i have added the constant as well but getting the same error ..",300735.0
99248,423022.0,,nan,
99248,423027.0,,nan,
99248,432700.0,,nan,
99241,423014.0,,nan,
99241,423025.0,,nan,
99241,424836.0,,nan,
99241,423083.0,,nan,
99471,425205.0,,nan,
99428,424902.0,,nan,
99428,424901.0,,nan,
99566,426023.0,,nan,
99566,425902.0,,nan,
99566,426042.0,,nan,
99566,427776.0,,nan,
99566,426654.0,,nan,
99566,432698.0,,nan,
127292,,,nan,
99835,427606.0,,nan,
99835,427597.0,,nan,
99835,427645.0,,nan,
96663,411767.0,102147.0,"Hi Deval,i am using binomial distribution and calculating the prob for (NOrmal,intermediate,High) for 4 people and then multiplying it . am i doing Wrong .?",303674.0
96663,411767.0,102155.0,Thankyou i got it,303674.0
96663,411767.0,103004.0,"i am also using binomial distribution and calculating the probability for (p=0.85, 0.6, 0.3) for 4 people out of 10 peoples and then adding them. But got the wrong answer. Is there any different method required to be used?",311117.0
96663,411767.0,103068.0,Got it..,311117.0
96663,411767.0,103389.0,"Hello, i have calculated the probability of calculating all 3 ans correct - by multiplying normal,intermediate and high probability.. after that calculated the probability of not solving it by 1-above ans and then calculated for exactly x=4 using binomial but not getting the answer,, please help..",305129.0
96663,417687.0,103067.0,Got it..,311117.0
96663,420712.0,,nan,
97558,417483.0,,nan,
97558,420065.0,,nan,
97558,420240.0,,nan,
97558,421518.0,,nan,
97655,417820.0,103855.0,I've written this assuming it might be the probably knowledge I had before doing the videos. Now it's irrelevant. You can follow other accepted answers :),318329.0
97655,417820.0,103723.0,can you explain this ?,311861.0
97655,417825.0,,nan,
97655,417926.0,,nan,
97655,420707.0,,nan,
97655,420766.0,,nan,
98928,420990.0,,nan,
98928,420959.0,,nan,
98928,421053.0,,nan,
98928,421473.0,,nan,
95373,404612.0,,nan,
95373,417412.0,,nan,
98967,421191.0,,nan,
98967,421165.0,,nan,
98967,421202.0,,nan,
98967,421214.0,,nan,
99301,423369.0,,nan,
99301,423493.0,,nan,
99052,421751.0,,nan,
99052,422891.0,,nan,
99052,421820.0,,nan,
99339,423669.0,,nan,
99339,423802.0,,nan,
99339,424000.0,,nan,
99339,424044.0,,nan,
99339,424705.0,104161.0,10C5 = 252 0.5^5 = 0.03125 252 * 0.03125 * 0.03125 = 0.246,313826.0
99339,424470.0,,nan,
99339,424861.0,,nan,
99073,421842.0,,nan,
99073,421846.0,,nan,
99073,421858.0,,nan,
98700,420348.0,,nan,
98700,420356.0,,nan,
98700,422694.0,,nan,
103863,446052.0,,nan,
103863,446102.0,,nan,
103863,446169.0,,nan,
103863,446813.0,,nan,
103863,447517.0,,nan,
100100,428980.0,,nan,
100100,429807.0,,nan,
98972,421292.0,,nan,
98972,421332.0,103712.0,"how can data validity can be checked by this, please explain",318344.0
98972,421596.0,103744.0,"Hemat you came up with a very good point. I totally agree with what you said. But my question is more focused to Data quality issue once you have collected it. I man once survey or any sample is collected and you are given the data to analyze you must check whether the data is valid or not are there any ways to do.. some are mentioned in the video itself, but i would like to dissuss more.",318344.0
98972,421596.0,103752.0,"Yeah, I agree to your point that there can not be a perfect sample to analyze, there will be some quality issues, and we want to minimize those issues, for a better analysis. All we can do is to filter out completely vague entries from the data, like extreme numbers etc.",318344.0
98972,421596.0,103751.0,"Thanks! :) But, yes there will be some certainty in results as you pointed out since we're dealing with the set of data (samples).. It was complete random data which was picked for analysis; one thing which we might want to consider is to include most of the scenarios captured up in the samples that we'll be analysing.. But either way we do, there will be some quality issues; there will be some variance in the expected value and may be that is the reason we take ""Probablity"" of data to predict the trends which will never be ""1"" .. :) I know your main question does remain unanswered, how do we minimize the quality issues in the selected sample.. :)",316349.0
98972,422487.0,103932.0,"Thanks a lot, for a very good answer.",318344.0
99192,422662.0,,nan,
99192,422634.0,103981.0,But how it is calculated .97 as 1.88 ( as in z table the value is .8340),312518.0
99205,422641.0,,nan,
99205,422757.0,,nan,
99204,422758.0,,nan,
97677,417993.0,,nan,
97677,423076.0,,nan,
97677,418146.0,,nan,
99237,422945.0,104002.0,How did we arrive at the values in the y-axis in the range of 0.01 to 0.06 which you have mentioned as count of employees.,314730.0
99237,423505.0,104125.0,Thanks,318814.0
99237,423505.0,106505.0,"Though this question is sometime old, just wanted to add some light on this topic since, it was pretty much confusing from the content of the course which didnt had explanation on why plotting of the graphs shifted from probability to probability density and how is this calculated. Coming to point on how we calculate the probability density: Since, its a continuous variable, an interval is considered. We need to take the probability at both the end points of the interval and then divide it by the interval size, we get the probability density. In a layman term, when the probability density is multiplied by the interval, we should get the probability at tat point. When we observe the plotting of the graphs from bar graph to distribution plot, we can actually observe this change from actual probability to probability density.",314084.0
99249,423461.0,,nan,
99249,423034.0,,nan,
99249,423041.0,104017.0,"Thanks Mahima for the explanation. I have one more query.. Q:What is the probability that the tablet that has been selected by the authority has a paracetamol level below 550 mg? In this we do P(X<550) = P(Z<{550-510}/20) = P(Z<2) = 0.977, or 97.7%. why don't we do the same as above like P(Z>2)+P(Z<2)=1 in this case?",309451.0
99249,423095.0,104021.0,"In the question, we are essentially asked the question of calculating P(Z<2). Now the Z-tables are formulated in a way such that you can get P(Z<z) where z is the Z-score associated with the problem(which is 2 in this case). Thus we can directly use the table to calculate P(Z<2)=0.977 in this case and we don't require to use that equation here. However, if the question would have been asked like to find paracetamol level more than 550, then, the equation needs to be used to calculate the value of P(X>550) or P(Z>2), since the value can't be directly obtained from Z-table. So the general rule of thumb is that check the sign- if it's a < then directly report the value from the Z-table, else subtract it from 1 and that would be the answer.",313517.0
99244,423008.0,,nan,
99244,423016.0,,nan,
99244,423079.0,,nan,
99251,423044.0,,nan,
99251,423048.0,,nan,
99251,423056.0,104014.0,Oh yes it should be P(Z<-2.2).,317991.0
99251,423056.0,104012.0,Shouldn't that be P(Z -2.2)?,313517.0
99258,423115.0,,nan,
99261,423126.0,,nan,
99261,423149.0,,nan,
99001,421440.0,,nan,
99001,421415.0,,nan,
99001,421465.0,,nan,
99001,421487.0,,nan,
99001,421590.0,,nan,
99001,422258.0,,nan,
99001,421434.0,103713.0,It worked. Thanks!,312756.0
99001,421434.0,103711.0,I have already did that but answer isn't matching to the options provided. any hints?,312756.0
99027,421635.0,,nan,
99027,421623.0,,nan,
99027,421636.0,,nan,
99027,422495.0,,nan,
96776,412544.0,102264.0,thanks,303674.0
96776,412544.0,103091.0,"The question saying, under- or over-estimates the distance by less than 500 Kms? Does it mean less than 1000 kms?",311117.0
96776,420115.0,,nan,
96776,421457.0,,nan,
96776,422260.0,,nan,
99099,422018.0,103851.0,Thanks..,320103.0
99099,422062.0,103850.0,Thanks..,320103.0
99099,422755.0,,nan,
98743,420636.0,103975.0,Its is not going 2330 from the mean value to the left or right but X>2330,304319.0
98743,420870.0,,nan,
98743,421454.0,,nan,
98743,422259.0,,nan,
99159,422416.0,,nan,
99159,422406.0,,nan,
99159,422479.0,103926.0,"I was using the same logic but at the place 0.97 ,I was using 0.03. thanks",314183.0
99159,422479.0,103949.0,I was doing the same mistake but corrected it after google about it ...,311466.0
99405,424649.0,104183.0,you have reiterated the solution and to get clarify only option to understand video,307843.0
99405,424649.0,104163.0,I do not think you clarified my question. You reiterated what I am aware of.,318007.0
99408,424488.0,,nan,
99408,424494.0,,nan,
98436,420754.0,,nan,
98436,420879.0,,nan,
98436,422072.0,,nan,
98923,434136.0,,nan,
98923,420922.0,,nan,
98923,421001.0,103673.0,"Sorry, I meant .33% is added to 50.5%. Sumit Saxena is correct.",301652.0
98923,421087.0,,nan,
99260,423134.0,,nan,
99260,423125.0,,nan,
99395,424698.0,,nan,
99395,424409.0,,nan,
99395,447254.0,,nan,
99424,424795.0,104203.0,Need more example,301641.0
99424,425727.0,,nan,
98573,420751.0,,nan,
99037,421633.0,,nan,
99037,421627.0,,nan,
99037,422101.0,,nan,
99037,422314.0,,nan,
99331,423746.0,,nan,
99331,423808.0,104098.0,Ok... I am yet to do that module...,318479.0
99063,422781.0,,nan,
99063,421800.0,103781.0,Yeah I got it. I did wrong subtraction. Thanks!!,311004.0
99063,421800.0,103780.0,"I did the same, but it give me different ans. Let me check once again.",311004.0
99063,422039.0,,nan,
99063,422322.0,,nan,
99441,425001.0,,nan,
99106,422071.0,,nan,
99106,422069.0,,nan,
99106,422077.0,103948.0,hi. Can you please put a link to the FAQs where this explained?,317149.0
99106,422077.0,103985.0,Thank you,317149.0
99106,422077.0,103982.0,https://learn.upgrad.com/v/course/208/session/18005/segment/91591 FAQ at the bottom of this page,315471.0
99106,422227.0,103873.0,Thanks again Ashish! I am talking about the 1.65 Z* value which is not calculated here. From where did we get that 1.65 value. There is actually some explanation given in the FAQs. I wanted clarification on that part. I have no problems calculating the probabilities using the Z table,315471.0
99106,422235.0,,nan,
99106,422248.0,,nan,
99106,422218.0,103866.0,Thanks Ashish!! But I am looking for the Z* values. Not the Z values. These are clearly different as mentioned in the module.,315471.0
99106,422218.0,103865.0,You can see the steps of integration here. (https://www.symbolab.com/solver/definite-integral-calculator/%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%7D%5Cint_%7B-%5Cinfty%7D%5E%7B0%7De%5E%7B%5Cfrac%7B%5Cleft(-x%5E2%5Cright)%7D%7B2%7D%7D%20dx),301652.0
99106,422218.0,103864.0,Although integration is difficult. It has error function erf(x). (https://www.quora.com/How-do-we-calculate-the-definite-integral-of-e-x-2-from-0-to-1),301652.0
99106,423764.0,,nan,
98181,420210.0,103611.0,"I don't think this answer is correct, still TA has verified it. If I am wrong TA please comment.",318344.0
98181,420210.0,103615.0,Yes. Please verify,311952.0
98181,420210.0,103630.0,I second that. the answer provided is incorrect,309211.0
98181,420649.0,,nan,
98181,420878.0,,nan,
98181,420934.0,103638.0,Could you specify the segment for the video?,313517.0
98181,420974.0,,nan,
98181,420982.0,103643.0,ok awesome. thank you Mahima! appreciate the response. I felt that it was assumption. it was better to check with you than assume myself and everyone had all sorts of answers that further confused me. much appreciated!,309211.0
98181,421575.0,103761.0,"Aditya, please refer to TA verified answer. ""the value of 9900 as the (Xi-X)^2 has been assumed in the video""",309211.0
98181,421575.0,103768.0,"yes, i know that, already. However yuor question is not clear to anyone here. Please, ask questions more concisely in future.",315560.0
98181,421575.0,103797.0,"Thanks for the suggestion, it really wasn't required ; kindly go through all the responses before adding your own response. Once someone posts a question, it cannot be edited after 10 minutes which I believe is the forum functionality ; which happened to me. I was unable to edit original question and I then added this to clarify my question : =========== My question is that : As per video, I don't see any calculation basis on arriving at 9900 . It just says ""Let's just say we find by calculation that summation of (Xi - X(bar) square is 9900"". I know that X(bar) is 36.6 ; and I know sample size is 100 ; what is X1, X2, X3 , X4 within samples ? how is that arrived at ? is there any spreadsheet that gives us X1, X2 , X3 ....X100 values from which we subtract 36.6 and sum it up ; and square it to get 9900 ? where is it? ======= to which TA's response is this: ""Yes, the value of 9900 as the (Xi-X)^2 has been assumed in the video."" hope you got your ""Conciseness"" here",309211.0
98181,421575.0,103788.0,"Please read the entire set of answers carefully; Your response isn't adding any value ; TA has answered the question and accepted the correct answer. not sure if you read this, since you did not , mentioning it again ""concisely"" below: ================================================================ My question is that : As per video, I don't see any calculation basis on arriving at 9900 . It just says ""Let's just say we find by calculation that summation of (Xi - X(bar) square is 9900"". I know that X(bar) is 36.6 ; and I know sample size is 100 ; what is X1, X2, X3 , X4 within samples ? how is that arrived at ? is there any spreadsheet that gives us X1, X2 , X3 ....X100 values from which we subtract 36.6 and sum it up ; and square it to get 9900 ? where is it? ================================================================",309211.0
99284,423268.0,104900.0,In what cases Z-Score Negative?,317418.0
99284,423268.0,104899.0,Does the P-Value lie in (100-y)/2 range??,317418.0
99284,423268.0,104175.0,It is pretty clear. Thanks Mahima.,318002.0
99284,423268.0,104901.0,p-value doesn't come up in confidence interval. It is related to hypothesis testing.,313517.0
99284,423268.0,104903.0,"Thanks Mahima, but I think the p-value lies in the either side of the confidence interval.",317418.0
99284,423268.0,104905.0,"Got it now, Thanks",317418.0
99284,423268.0,104904.0,"What I meant to say is that the concept of confidence interval comes in inferential statistics whereas the concept of p-value comes in hypothesis testing. So , I don't think you can use the same terminology for both the cases",313517.0
99284,424866.0,,nan,
99284,436995.0,,nan,
99284,446033.0,,nan,
99169,422442.0,,nan,
99169,422448.0,,nan,
99169,423188.0,104041.0,I think the necessary changes have been made. I believe it is sort of understood when mentioning Z-value that you are calculating a cumulative probability only. But you do raise a valid point,313517.0
99169,423188.0,104038.0,So shouldn’t the FAQ mention calculate the cumulative probability instead of probability when calculating for Z score?,310509.0
99170,422445.0,,nan,
99170,422446.0,,nan,
99170,422599.0,,nan,
99170,423340.0,,nan,
99179,423755.0,,nan,
99179,422523.0,,nan,
99179,422593.0,,nan,
99088,422081.0,,nan,
99088,422063.0,,nan,
99088,422279.0,,nan,
99088,422681.0,,nan,
99088,422509.0,,nan,
99088,424210.0,,nan,
99340,423772.0,,nan,
99340,423850.0,,nan,
105880,456945.0,,nan,
105880,457067.0,,nan,
107046,462593.0,,nan,
107046,462627.0,,nan,
106102,457917.0,,nan,
106104,457904.0,,nan,
106104,458005.0,,nan,
106114,458053.0,,nan,
106115,457979.0,,nan,
106142,458114.0,,nan,
106142,497879.0,,nan,
106142,458130.0,,nan,
106184,458355.0,109579.0,Agreed.,318329.0
106184,458355.0,109578.0,I strongly believe we should use it in evaluating the model but how to do that hasn't been taught. Otherwise what's the point of doing it?,310974.0
104985,452241.0,,nan,
104985,452291.0,,nan,
104985,452599.0,,nan,
104985,453026.0,,nan,
105010,452363.0,,nan,
105010,452364.0,108553.0,Yes but the R-Squared value for the model with all independent variables practically conveys the same idea isn't it?,304022.0
105010,452364.0,108556.0,"Let's forget Sales for a while, if the R-squared value of a model with TV,Newspaper, Radio is taken and is found to be 0.9, can't we say TV,Newspaper although considered independent have a lot of influence on Radio. My point being, arithmetically, since VIF = 1/(1-R squared), VIF is solely dependent of the R-squared of the model with only Independent variables. It might be a norm but not convinced logically",304022.0
105010,452364.0,108555.0,"R-Squared doesn't tell how the values of TV is explained by newspaper and Radio or vice-versa. There can be a scenario that values of sales can be explained perfectly by values of TV, newspaper and Radio (then R-squared can be > 0.9) but VIF of TV, Newspaper and Radio is 0 because they may not be interdependent among themselves.",318328.0
105010,452476.0,,nan,
105010,452635.0,,nan,
105010,452710.0,108609.0,Agree. It's the simplicity of interpreting the VIF rather than R-squared which becomes a round-about way.,311857.0
105010,452659.0,,nan,
105010,453022.0,,nan,
107142,463113.0,,nan,
107142,463484.0,,nan,
106284,458920.0,,nan,
106284,458972.0,,nan,
105205,,,nan,
106402,459429.0,,nan,
106402,459474.0,,nan,
106402,459682.0,,nan,
105226,453208.0,,nan,
105235,453226.0,,nan,
105235,453223.0,,nan,
105235,453561.0,,nan,
105235,454617.0,108933.0,"sorry, please ignore my answer. It looks like the scaling don't have any impact on the model accuracy.",314244.0
105236,453225.0,,nan,
105236,453222.0,,nan,
105236,453314.0,,nan,
105301,453416.0,,nan,
105301,453515.0,,nan,
105301,453559.0,,nan,
105301,453536.0,,nan,
105353,453692.0,,nan,
105353,453720.0,108982.0,Right.,311117.0
105353,453720.0,108936.0,it means you are agree with my points . i.e. this depends what variable u r adding .,319969.0
105353,453751.0,,nan,
105353,453946.0,,nan,
105353,454468.0,,nan,
106452,459908.0,,nan,
106452,459835.0,111262.0,Are u Sure? Can you elaborates it !!,318770.0
106452,459880.0,,nan,
106460,459901.0,,nan,
106460,459864.0,,nan,
106460,459878.0,109963.0,gives a syntax error,319759.0
106460,459878.0,109964.0,Try without [] around the 1 and use columns in place of column,320073.0
106460,460072.0,,nan,
106460,460150.0,,nan,
106460,460185.0,,nan,
106483,460063.0,,nan,
106483,460124.0,,nan,
107896,466078.0,,nan,
107896,466079.0,,nan,
107896,466107.0,,nan,
105385,453983.0,,nan,
105385,453934.0,,nan,
105385,453975.0,,nan,
105516,454564.0,,nan,
105516,454595.0,,nan,
105516,454853.0,,nan,
105557,454845.0,,nan,
105557,454830.0,,nan,
105582,454993.0,,nan,
105582,455051.0,109006.0,Please read the question again.,310974.0
105582,455766.0,109068.0,Still doesn't answer my question. I'm asking how the algorithm knows the dropped variable is significant or not.,310974.0
105582,455766.0,109094.0,"I repeat my question, manually we are able to figure about that 0,0 corresponds to the dropped category. How does model know this? How does it represent the missing category in the form of coefficients?",310974.0
105582,455766.0,109082.0,"furnished semi-furnished unfurnished 1 0 0 1 0 0 0 1 0 1 0 0 1 0 0 Now, you don't need three columns. You can drop the furnished column, as the type of furnishing can be identified with just the last two columns where — 00 will correspond to furnished 01 will correspond to unfurnished 10 will correspond to semi-furnished Let me elaborate with the example you are referring too So in the above case we want to find whether the apartment is furnished or not so we have coded it 1 and 0 respectively. Model will see the occurrence of the 1 and 0 pattern. As they mentioned above 00 will correspond to furnished 01 will correspond to unfurnished 10 will correspond to semi-furnished You can as well drop any of the other two the pattern of determination will change. If you decided to drop unfurnished the pattern will be as follows: 10 will correspond to furnished 00 will correspond to unfurnished 01 will correspond to semi-furnished If you wished to drop semi furnished the pattern will change as follows: 10 will correspond to furnished 01 will correspond to unfurnished 00 will correspond to semi-furnished",318451.0
105582,455766.0,109279.0,"Let's say you have only one independent variable which is categorical with 3 categories . Approach 1 : Create three dummy variable Y = A1X1 + A2X2 + A3X3 + C Where x1, X2, X3 >> {0,1} If X1 = 1 than X2 = 0 and X3 = 0 so Y = A1 + C >> A1 + C is a constant , let's say it is equal to C1 So I can write equaction as Y = A2X2 + A3X3 + C1 Approach 2: Remove X1 Y = A2X2 + A3X3 + C2 From Approach 1 and Approach 2 to you can see that only constant coefficient will change if you remove the variable, and at the end C1 = C2 Which will learn model from training",344894.0
105613,,,nan,
105674,455851.0,,nan,
105674,455860.0,,nan,
106802,461328.0,,nan,
106802,461271.0,,nan,
106802,461285.0,,nan,
106802,461547.0,,nan,
106802,461289.0,110430.0,"We try to fit and transform on the variables that area present before dropping. so, we should try on those variables",318329.0
106802,461289.0,110400.0,"Hi Nagaraju, could you specify what you mean by this? Thanks",316416.0
106810,461324.0,,nan,
106810,461579.0,110289.0,Thanks Muthu Krishnan,314629.0
106822,461372.0,,nan,
106822,461353.0,,nan,
106822,461566.0,,nan,
106822,461679.0,,nan,
115908,501679.0,,nan,
115908,501442.0,116788.0,"I know it's vast, as my area of intrest to write own algorithm which generate model instead of using existing one. Same type of Career opportunity also I am searching.",318319.0
115908,501442.0,116808.0,Where can I get docummentations of algorithm which we are using.,318319.0
115908,501442.0,116799.0,"Rohit, if you think closely, all that you are studying are basically algorithms of varying complexity. Normally algorithm development falls under the umbrella of research and not service. If you are interested in algorithm development I would suggest first learn the existing algorithms that are used. Mind you, that will be a long list as there are a lot of research activities in this field. Normally a Masters student spend 2/3 years understanding what there already is. Then you will have to understand if there are any flaws/scope of improvement in the algorithms. Then you device a collection of finite steps(algorithm) that is possible to executed in finite time(Turing Completeness) and you have your own algorithm . What we normally do in service company, is we customize the existing algorithm to our needs. So there you go, First step towards algorithm development - know what already is there.",305839.0
122877,534362.0,,nan,
122877,534302.0,121828.0,Thanks Arihant for the response. Got my answer,312623.0
114072,491886.0,115737.0,I couldn't relate this to my question. can you elaborate it plz,317410.0
114072,492054.0,,nan,
114072,492130.0,,nan,
122876,534723.0,,nan,
122876,534623.0,122353.0,Thanks Sambit Thanks for sharing nice link.Got clarification on feature scaling.,312623.0
122878,534622.0,,nan,
115622,499231.0,,nan,
115622,499055.0,,nan,
115622,499075.0,,nan,
114921,495942.0,,nan,
115339,496959.0,,nan,
134141,583529.0,,nan,
134141,583486.0,,nan,
134141,583887.0,,nan,
105313,453547.0,108921.0,Thank you Deepak Kumar. It worked fine.,301121.0
105313,454447.0,,nan,
105313,454462.0,,nan,
105796,456429.0,,nan,
105796,456431.0,,nan,
105464,454481.0,,nan,
105464,454630.0,,nan,
105464,454875.0,,nan,
106038,457526.0,,nan,
106038,457769.0,,nan,
106038,457967.0,,nan,
104828,451682.0,108466.0,True. I got it.,310511.0
104828,451711.0,,nan,
104828,451943.0,,nan,
104830,451681.0,108465.0,But how is β1 linked to the mean value? Isnt β1 the slope of the line?,310511.0
104830,451709.0,,nan,
104830,452327.0,108551.0,Got it,310974.0
104830,452327.0,108550.0,The line is fitted based on sample data. Hypothesis testing is done to see if slope holds true for population data as well.,310511.0
104830,452478.0,,nan,
104854,451836.0,,nan,
104854,451821.0,,nan,
104854,452265.0,,nan,
104854,451941.0,,nan,
104878,451959.0,,nan,
104878,451976.0,,nan,
104878,451936.0,,nan,
104878,451927.0,,nan,
104878,453262.0,,nan,
105493,454503.0,,nan,
105493,454366.0,108967.0,"If you change the random-state , your training and test data will change, so it might change the R-square , Adjusted r-square , f-statistic , t-ststistic, coefficients. Always fix the random_state and improve the model",344894.0
105493,454366.0,108894.0,"Thanks! I agree that it has to be same value for multiple runs of code. I need to know what is statistical significance of the number . if it is just a number, why will R-square , Adjusted r-square , f-statistic , t-ststistic, coefficients change if we change random_state?",309211.0
105493,454790.0,,nan,
104911,452093.0,,nan,
104911,452145.0,,nan,
104983,453180.0,,nan,
105000,452360.0,,nan,
105000,453181.0,,nan,
105015,452398.0,,nan,
105025,452422.0,,nan,
105025,452410.0,108560.0,Its not working for me. Did it worked for you ?,312019.0
105025,452410.0,108566.0,it worked for me if i used values.reshape. I think it worked for you too?,318329.0
105025,452410.0,108564.0,"correction x_train_lm=x_train_values.lm.reshape(-1,1) x_test_lm=x_test_lm.values.reshape(-1,1)",312019.0
105025,452747.0,,nan,
105036,452495.0,,nan,
105036,452497.0,,nan,
105036,452498.0,,nan,
105036,452573.0,,nan,
105036,452626.0,,nan,
105049,452559.0,,nan,
105049,452565.0,,nan,
105049,452571.0,,nan,
105049,452572.0,,nan,
105049,452584.0,,nan,
105049,454016.0,,nan,
105032,452529.0,,nan,
105032,452490.0,108586.0,Thanks. This worked.,310505.0
105092,452685.0,,nan,
105092,452778.0,,nan,
105137,452823.0,,nan,
105137,452815.0,,nan,
105137,452935.0,,nan,
105171,452970.0,,nan,
105171,453139.0,,nan,
105171,453179.0,,nan,
105171,453291.0,,nan,
105150,452842.0,108628.0,T score is 24.772 . I could not this value from t- table . Values are in range of 1 to 5 . So I am puzzled how to figure Probability for t score .,301108.0
105150,452842.0,108624.0,Hi Deval I got that. My Question is how to calculate P(t) from given t score in Summary Statistics table .,301108.0
105150,453185.0,108898.0,I am asking about calculation of Proabability from the given t score. From T table I am not able to find Probability for T score 24.722,301108.0
105150,453185.0,108974.0,Please follow the attached video https://www.youtube.com/watch?v=tI6mdx3s0zk https://www.youtube.com/watch?v=z-HSsVARNnk,344894.0
105038,452622.0,,nan,
105038,452583.0,,nan,
105038,452528.0,108658.0,"Hi Aman, One of the efficient way to compare between the full and reduced linear regression models is the apply the General Linear Hypothesis Test (GLHT). If p-value is larger than 0.05, the reduced model is sufficient to represent the problem and no need for the full model. You can also compare model based on R2 value.",344894.0
105038,452528.0,108657.0,"Hi, I am aware of these theoretically, however, my question is, is there a specific numerical score or something that can directly state that the model is best possible or comparatively better instead of subjectively analyzing the parameters discussed above (F stat., T-score and P value). For instance, is there any weight assigned to F stat and R square and P value? If there are two models, one with better F stat and other with better P-value, which one will be considered better and will there be any numerical comparison or purely subjective?",317987.0
105229,453205.0,,nan,
105161,453023.0,108884.0,How do we know tat how much percentage should be distributed to train data and test data? it would be just random percentages? could you please explain on this.,312756.0
105161,453023.0,108887.0,"there is no thumb rule.. as Rahim said, it completely depends on business understanding.. usually its 80-20 or 70-30",316349.0
105161,453023.0,108979.0,Thanks Hemant. This helps.,312756.0
105161,452937.0,,nan,
105161,453187.0,,nan,
105228,453206.0,,nan,
105228,453385.0,,nan,
105228,453719.0,,nan,
105231,453203.0,,nan,
105231,453227.0,,nan,
105231,453321.0,,nan,
105231,453476.0,,nan,
105308,453448.0,,nan,
105308,453556.0,,nan,
105308,453605.0,,nan,
105308,453542.0,,nan,
105302,453419.0,108714.0,but what are those 2 columns?,310509.0
105302,453565.0,,nan,
105302,454373.0,,nan,
105305,453454.0,108721.0,But my question is not that...I am asking how can one model with high F statistic have Low R squared?,310509.0
105305,453699.0,,nan,
105319,453511.0,,nan,
105319,453514.0,,nan,
105319,453518.0,,nan,
105319,453950.0,,nan,
105346,453674.0,,nan,
105346,453949.0,,nan,
105381,454518.0,,nan,
105381,453893.0,,nan,
105381,453939.0,,nan,
105377,453862.0,,nan,
105377,454585.0,,nan,
105412,454142.0,108837.0,"What are the values that we are plotting here for frequency? This is my primary question. If it is actual minus predicted, how is a normal distribution graph plotted at each X value?",318007.0
105412,454142.0,108860.0,"at each X, you will have one actual y and the predicted y. the difference is the residual. when plot these as a frequency curve, you will get a distribution curve",318329.0
105412,454142.0,108892.0,"The confusion was around the multiple normal distribution curves in the same diagram. I am now assuming that it is just repetition of the same curve and only adds confusion to viewers,",318007.0
105412,454143.0,108840.0,"How can we be lucky for mean 0 in all the distribution curves drawn at each X value? This is far from assumption, the inline questions highlighted that the predicted value is on the line drawn and hence said that the mean will be zero. I did not understand this.",318007.0
105412,454143.0,108855.0,Just think of it as the deviation from the actual line. The deviations must be centered around zero is what we assume for regression.,318329.0
105412,454143.0,108841.0,"""The means of each of these normal distributions are shown to be lying on the line. Now if the error term lies on the line itself, that would mean that the error term is actually zero."" - notes from the answer given for a question.",318007.0
105412,454361.0,108891.0,"All this was done in a later section. I have completed the later sections and have better understanding now. However, I have to say that the speaker did not give proper clues while introducing the concept.",318007.0
105396,454148.0,108908.0,"I was bit confused basically I was looking it from Graph point of view.Now looking it from the mathematical sense,as you said,it does makes sense",318386.0
106490,461240.0,110294.0,what is the meaning of formula?,320687.0
106490,461240.0,110298.0,"Hi Deepesh, Please go through the below link https://stackoverflow.com/questions/51126928/statsmodels-formula-api-ols-does-not-show-statistical-values-of-intercept",344894.0
106490,461240.0,110637.0,I got it,320687.0
105542,454734.0,,nan,
105542,454776.0,,nan,
105531,454694.0,,nan,
105531,454882.0,,nan,
105568,454994.0,,nan,
105568,454909.0,,nan,
105568,454950.0,,nan,
105568,455409.0,,nan,
105585,455044.0,109246.0,thanks,305804.0
105585,455000.0,109245.0,thanks,305804.0
105585,456786.0,109247.0,thanks,305804.0
105604,455266.0,,nan,
105604,456450.0,,nan,
105604,455747.0,,nan,
105604,458111.0,,nan,
105607,455406.0,,nan,
105607,455540.0,,nan,
105607,455734.0,,nan,
105622,455721.0,,nan,
105643,455903.0,,nan,
105643,455707.0,,nan,
111126,478509.0,,nan,
111126,478524.0,,nan,
111126,478503.0,,nan,
126734,552225.0,,nan,
115366,497194.0,116377.0,"what exactly the penalty means, any idea?",300733.0
115366,497394.0,,nan,
115366,497312.0,116398.0,"There was a typographical error. I meant ""It will not decrease""",301121.0
115366,497619.0,,nan,
115366,499759.0,,nan,
115366,500691.0,,nan,
126755,558281.0,,nan,
126755,552179.0,,nan,
126755,552155.0,124750.0,"Let us talk about Health Care domain, where we are building a model which decides possiblity of Cancer cure, client already has a model with 99% accuracy now he wants to make sure accuracy is 100 so that he does not loose any patient. How to convience the client?",300721.0
114966,496175.0,,nan,
114966,496267.0,,nan,
114966,496207.0,,nan,
115365,497212.0,,nan,
115365,497195.0,,nan,
115365,497321.0,,nan,
114468,493763.0,,nan,
114468,494734.0,,nan,
114468,493928.0,,nan,
114900,495953.0,,nan,
114900,496279.0,,nan,
126754,552373.0,124783.0,But I guess we perform residual analysis only on train data not on test data and we are talking about Y Predicted Vs Error.,300721.0
126754,552449.0,,nan,
126754,552806.0,124933.0,"To comprehend the answer you please do the following even though it is in the reverse order. Draw a line on the positive quadrant with some slope m=1 or 1.2 and select 5 points on that line. For each of those 5 points, add dots on either side of the line in such a way that mean error term equals zero. Now go back to my answer and try Situation 1 and 2. I am sure this would help.",301121.0
126754,552806.0,124928.0,"But I still have doubt because I have few images in my mind, while fitting a linear line for evry single point there are some gap between original point and the predicted point on predicted line which we call an error and that doesn't remain same for all the points. Note- Consider a scatter plot of all the data points and fit a line.",300721.0
105884,457006.0,,nan,
105884,457106.0,,nan,
107014,462172.0,110363.0,"yes, that's true. but my question is different. my question is should we need to use drop_first while crafting dummies",317073.0
107014,462172.0,110412.0,Yes Ayyappa,344894.0
107016,462162.0,,nan,
107016,462185.0,110364.0,Thanks,305655.0
107016,462338.0,,nan,
105885,456972.0,,nan,
105885,457005.0,,nan,
105885,457015.0,,nan,
105885,457066.0,,nan,
105885,458054.0,,nan,
107109,462856.0,,nan,
107109,463044.0,,nan,
107137,463092.0,,nan,
107173,463301.0,110548.0,"Thanks Nitesh, I got answer in the video itself once after I posted the question",314629.0
107173,463301.0,110552.0,then feel free to upvote my answer ;-),300694.0
107173,463429.0,,nan,
107173,463382.0,,nan,
107152,463272.0,110549.0,"Yes because we want it that way. If we choose say 4-5 variables only, we might get a fit model right away.",304319.0
107152,463272.0,110544.0,no it is not - there is a lot that needs to be done after RFE has run - that is the easy part,300694.0
107152,463272.0,110551.0,well you are talking about a very specific made up case. why would I use RFE to choose 4-5 variables? I am talking about generic case where n >= 10 even >=20- which is what we generally do with RFE In my question I clearly stated 4 steps - of which RFE is only one - so I am looking to automate all 4 steps; not just stop after RFE,300694.0
107152,463178.0,110645.0,Cool,301655.0
107152,463178.0,110639.0,"thanks - had a quick look - looks something like what I wanted, although it is in R; I should have been clearer and asked specifically for Python .. I will take a deeper look at the link at some point",300694.0
107152,463856.0,110638.0,yeah something like auto regressioning - thanks for the liinks - I will check them out; looking for something specifically in Python,300694.0
107203,,,nan,
107229,463786.0,,nan,
107229,463824.0,,nan,
107229,464035.0,,nan,
106364,459317.0,,nan,
106364,459309.0,109845.0,Thanks tried this approach and could complete the model.,313767.0
107237,463822.0,,nan,
107237,464029.0,,nan,
106807,461326.0,,nan,
106429,459707.0,,nan,
106430,459709.0,,nan,
106430,459702.0,109904.0,No price is not a outcome variable it is a dependent variable. Dependent variable means that the nature of price is dependent on various variable available in the data-set. And we as a data scientist/modeler try to find out the best variables/variables having strong relation that explain price. The relation may be positive or negative.,318451.0
106430,459702.0,109900.0,So here we pop out price variable as outcome variable and compare with other correlated variables i.e. feature variables right.,310179.0
106430,459904.0,,nan,
106430,460004.0,,nan,
106430,461483.0,,nan,
106432,459907.0,,nan,
106432,459795.0,,nan,
105318,453479.0,108726.0,Thanks Chandan for the clarification,318455.0
105318,453487.0,108728.0,"Hi Utkarsh, I do understand about train and test data. However if you see Rahims video, he has further divided the train data into xtrain and ytrain. So my question was “is there any specific reason behind the same”",318455.0
105318,453487.0,108730.0,The Y-train has the dependent or the target variable 'price'. The X-train has all the predictor variables. Helps when you want to map price against predictor variables,311857.0
105318,453487.0,108732.0,Thanks Pradnya,318455.0
105318,453487.0,108747.0,Thank You everyone,318455.0
105318,453487.0,108746.0,As Pradnya mentioned - it is done to divide the independent variables from the dependent ones.,308962.0
105318,453655.0,,nan,
105318,454618.0,,nan,
105318,454655.0,,nan,
105359,453749.0,,nan,
105359,453901.0,,nan,
105359,453941.0,,nan,
105359,453737.0,,nan,
105389,453935.0,,nan,
105436,454207.0,,nan,
105436,455433.0,,nan,
105436,455973.0,,nan,
105431,454177.0,,nan,
105431,454525.0,,nan,
105431,455559.0,,nan,
105405,454023.0,108812.0,"Thanks, can you please explain a bit .. ?",312479.0
105405,454144.0,,nan,
105405,454869.0,,nan,
105476,454322.0,,nan,
105476,455495.0,,nan,
105476,455962.0,,nan,
105499,454389.0,,nan,
105499,454569.0,,nan,
105499,454938.0,,nan,
105499,455959.0,,nan,
108666,469819.0,,nan,
108666,470042.0,,nan,
108666,469876.0,,nan,
105537,454716.0,,nan,
105372,453833.0,,nan,
105372,453842.0,,nan,
105656,455946.0,,nan,
105656,455703.0,,nan,
105656,455861.0,,nan,
105664,455941.0,,nan,
105664,455857.0,,nan,
105670,455854.0,,nan,
105670,455952.0,,nan,
105670,456444.0,,nan,
105670,456456.0,,nan,
106786,461232.0,,nan,
105592,455955.0,,nan,
106514,460081.0,,nan,
106811,461322.0,,nan,
106824,461358.0,,nan,
106824,461783.0,,nan,
107163,463313.0,,nan,
107163,463211.0,,nan,
118945,513674.0,,nan,
118945,514123.0,,nan,
104698,451267.0,,nan,
104698,451335.0,,nan,
104698,451265.0,,nan,
104698,451367.0,,nan,
104698,451540.0,,nan,
104698,451814.0,,nan,
104698,452283.0,,nan,
104693,451219.0,,nan,
104693,451352.0,,nan,
104693,451266.0,,nan,
104693,451344.0,,nan,
104693,451365.0,,nan,
104693,451481.0,,nan,
104693,451393.0,,nan,
104692,451199.0,,nan,
104692,451220.0,,nan,
104692,451358.0,,nan,
104692,451268.0,,nan,
104692,451342.0,,nan,
104692,451363.0,,nan,
104692,451379.0,,nan,
104692,451486.0,,nan,
104709,451337.0,108412.0,thanks buddy! It was right in front of me but couldn't see it :D,308962.0
104709,451332.0,,nan,
104709,451373.0,,nan,
104709,451396.0,,nan,
104718,451407.0,,nan,
104718,451383.0,108899.0,R^2 = 1 means that all the points lie on the fitted line.,318756.0
104718,451370.0,108426.0,so OLS can be the Gradient Descent method or Derivative method to find B0 and B1 right?,317984.0
104729,451403.0,,nan,
104729,451397.0,,nan,
104729,452038.0,,nan,
104729,452152.0,108527.0,Refer this blog as well:http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-degrees-of-freedom-in-statistics,308638.0
104820,451707.0,,nan,
104820,451725.0,,nan,
104820,454566.0,,nan,
104821,451670.0,,nan,
104821,451721.0,108475.0,is line of best fit and linear line same?,318353.0
104821,451721.0,108484.0,i can draw linear line using y=mx+c. How can you draw line of best fit?,318353.0
104821,451721.0,108480.0,"line of best fit means the line which captures the dataset at its best. linear line means a straight line, it may not be the best line.",318451.0
104821,451663.0,,nan,
104821,451819.0,,nan,
104866,451880.0,,nan,
104866,451950.0,,nan,
104866,451979.0,,nan,
104872,451928.0,,nan,
104872,451931.0,108578.0,let me check,319969.0
104872,451931.0,108576.0,In that case what wil be my x and y data,319969.0
104872,451931.0,108577.0,"for RSS, you would only focus on y and y predicted. but future projection, if it was a simple linear regression, you could extend the line. for multi linear regression, I'm not sure yet.",318329.0
104872,451931.0,108579.0,I need to build model for next year month wise sales budget . I hv last 4 year month wise actual sales,319969.0
104872,451956.0,108575.0,"Thanks Sambit , Yes you understand my question . I need next year sales budget",319969.0
104908,,,nan,
104909,,,nan,
104903,452141.0,,nan,
104903,452053.0,,nan,
104903,452681.0,,nan,
104952,452294.0,,nan,
104952,452134.0,,nan,
106129,458116.0,109527.0,"But, the graph has been accepted. Is there any example where residual graph comes out not acceptable. If yes, what will be the next steps to counter it?",318404.0
106129,458155.0,,nan,
104949,452091.0,,nan,
104989,452262.0,,nan,
104989,452289.0,,nan,
104989,452292.0,,nan,
105031,452471.0,,nan,
105031,452628.0,,nan,
105031,452721.0,,nan,
105048,452560.0,,nan,
105048,452620.0,,nan,
105048,452567.0,,nan,
105048,452737.0,,nan,
105048,453116.0,,nan,
105048,453212.0,,nan,
105152,452883.0,,nan,
105152,452888.0,,nan,
105155,452887.0,,nan,
105155,452886.0,,nan,
105259,453319.0,,nan,
105259,453330.0,,nan,
105259,453403.0,,nan,
105259,453651.0,,nan,
105259,453657.0,,nan,
105259,453793.0,,nan,
105272,453391.0,,nan,
105272,453402.0,,nan,
105272,453660.0,,nan,
105272,491114.0,,nan,
105272,492897.0,,nan,
105290,453388.0,,nan,
105290,453711.0,,nan,
108713,470053.0,,nan,
108713,470044.0,,nan,
108713,470075.0,,nan,
108713,470084.0,,nan,
108713,470066.0,,nan,
108713,487440.0,,nan,
108713,499109.0,,nan,
105485,454475.0,,nan,
105485,490153.0,,nan,
105485,499117.0,,nan,
105588,455042.0,,nan,
105588,455067.0,,nan,
105588,455070.0,,nan,
105504,454576.0,,nan,
105504,454603.0,109000.0,Thanks for the Curve Diagram. Understood.,317984.0
106405,459434.0,,nan,
106405,459486.0,,nan,
106405,497623.0,,nan,
106405,498944.0,,nan,
105921,457192.0,,nan,
105921,457193.0,,nan,
105921,457842.0,,nan,
105973,457307.0,115283.0,Nice explanation but question was specifically for MinMaxScaler vs StandardScaler !!,318770.0
105973,457405.0,,nan,
106121,458169.0,,nan,
106182,458356.0,,nan,
106182,458398.0,109581.0,My doubt is also if the population data is within the range of sample data ?,313691.0
108909,470525.0,,nan,
108909,470572.0,111824.0,So since MLR states 2 or more independent variables and one dependent variable then in this case I will have to use 3 MLR right?,301114.0
108909,470572.0,112279.0,"Thanks deval , Jayshree , Brijesh , Paras by all your input s I am going forward , let me see the result",319969.0
108909,471029.0,,nan,
115615,498848.0,,nan,
115615,500075.0,,nan,
115615,499071.0,,nan,
115615,500706.0,,nan,
105523,454720.0,108944.0,so what I h to do,319969.0
105523,454720.0,108945.0,so what tool I hv to apply,319969.0
105523,454720.0,109019.0,I think first you need to figure out the node in this case the system parameters which are import to classification where the sheet will be produce defective or not defective ..test the model with both historical data and the real time data ..if it's work when put a alert system based on the nodes threshold level so that to know which system generated the alert,318476.0
105523,454737.0,,nan,
105336,453573.0,,nan,
105336,453654.0,,nan,
105336,453666.0,,nan,
105336,453798.0,,nan,
105383,454002.0,,nan,
105383,454150.0,,nan,
105383,454383.0,,nan,
105383,453989.0,,nan,
105383,454471.0,,nan,
105383,454590.0,,nan,
105383,454967.0,,nan,
105383,459431.0,,nan,
105380,453898.0,,nan,
105380,454482.0,,nan,
105565,454976.0,,nan,
105565,455338.0,,nan,
105565,454907.0,,nan,
105639,455536.0,,nan,
105639,455712.0,,nan,
105639,456486.0,,nan,
105639,464737.0,,nan,
105678,455868.0,109077.0,Yes you are right. I checked now and found that the day of the data starting from 01-03-2017 was passed to the system an that's how this is more specific to the data and correct in this scenario. Thanks for asking the right question and it clarified my doubt.,311729.0
105678,455868.0,109108.0,glad that it helped.,318329.0
105678,456472.0,,nan,
105696,456039.0,109112.0,"Yep. If it's after the feature dropping, it would mean inverse relation. Our intuition may be opposite to what the data might reveal in some cases.",318329.0
105696,456039.0,109111.0,"Right. Thats true. So here we know that Character A isnt correlated to any of the other selected features. But negative sign would without multicollinearity would signify adverse impact, isnt it? That is totally opposite to business's belief in Character A. So I was surprised to see it included as one of the final features.",310511.0
105696,456489.0,,nan,
105692,455984.0,109286.0,"No Jayaram, It is an example of Poly regression Model. Here they want to just explain about interpolation and extrapolation",344894.0
105692,455984.0,109216.0,As per my understanding the hyperplane will be an (n-1) dimensional space if we are dealing with n dimensional multiple regression. Would we be able to plot it? So is the line D2P2 the projection of the hyperplane in 2-d?,318334.0
105692,456279.0,109176.0,"Agreed that the relationship between the independent and the target variable could be any function. The question though was if it is ""mulitple linear regression"" then is the result a line. As rightly pointed by Ram it is an equivalent of a line....a 'hyperplane'.",311857.0
105692,456485.0,109219.0,So is the above graph a correct example of linear regression? Would we be able to obtain a curved line in 2-d if we project a hyperplane?,318334.0
114990,496183.0,,nan,
114990,496455.0,,nan,
114990,497622.0,,nan,
114990,498849.0,,nan,
136584,591660.0,,nan,
116475,503627.0,,nan,
116475,503599.0,,nan,
116494,504083.0,,nan,
116494,503717.0,,nan,
116025,501748.0,,nan,
116025,502563.0,,nan,
116469,503591.0,,nan,
116469,503551.0,,nan,
116469,503662.0,,nan,
116469,503858.0,,nan,
116469,507243.0,,nan,
115820,500922.0,116785.0,You comments are confusing and ambiguous,313526.0
115820,500934.0,,nan,
115820,501044.0,,nan,
115820,501693.0,,nan,
116188,502454.0,,nan,
116188,502492.0,,nan,
116188,502834.0,,nan,
116188,502509.0,,nan,
116427,503367.0,,nan,
116427,503826.0,,nan,
116271,502805.0,,nan,
116271,502789.0,,nan,
116271,502807.0,,nan,
116271,502836.0,,nan,
116271,502795.0,117140.0,outlier and multicollinearity are also reason for high variance?,310419.0
116271,503442.0,,nan,
116271,503518.0,,nan,
123574,537480.0,,nan,
123574,538429.0,,nan,
116448,503450.0,,nan,
116448,503465.0,,nan,
116454,503437.0,,nan,
116454,503726.0,,nan,
116669,504557.0,,nan,
116669,504588.0,,nan,
116669,504699.0,,nan,
116457,503433.0,117183.0,"Thanks Sambit! If number of features can be a hyperparameter in linear regression then that means the stackexchange answers needs to be rectified, right?",318355.0
116457,503433.0,117196.0,No not exactly ..If you check the method definition of OLS if not take any hyperparameter.But Xtrain and Xtest can be derived using RFE and RFE can use hyperparmeters and cross validation to find the number of features.,318476.0
116457,503433.0,117550.0,can you please post me a [good quality] link where it says 'number of features' is a hyperparameter?,300694.0
116457,503663.0,,nan,
116457,503723.0,117319.0,the input arguments to machine learning algorithm model are considered as hyper parameters as they impact the model outcome..,303666.0
116457,503723.0,117549.0,I said 'input rows' (ie. data; infact I said it twice :-) ) and not 'input arguments'; anyway just because something affects the model outcome does not mean it is a hyperparameter - and that can be seen by the fact that number of features is NOT a hyperparameter YET the model IS affected,300694.0
115855,501143.0,116712.0,"I read the discussion before posting this question here. The link is helpful, but TA needs to give their views and best answers,",318344.0
115855,501143.0,116746.0,looking for answer here how to optimally decide number of folds,317156.0
115855,501143.0,116744.0,"Hi, It depends on the size of your data set. If you think 50% of data is enough for training then you can choose 2 Fold. If you think you need 80% of data for training purpose, then you should choose 5 fold validation. Thanks",344894.0
115855,501463.0,,nan,
115935,,,nan,
115915,501405.0,,nan,
115915,501368.0,,nan,
115915,502753.0,,nan,
115947,501466.0,,nan,
115947,501802.0,,nan,
115947,502045.0,,nan,
115941,501500.0,116791.0,Yeah. got that after going through the next slide. thanks.,310511.0
125798,548783.0,124264.0,TA can you please verify on this,318455.0
116020,502214.0,,nan,
116020,501741.0,,nan,
116027,501747.0,,nan,
116027,502642.0,,nan,
116027,503185.0,,nan,
116031,501945.0,116959.0,Why is professor considering 10 and 20 in the end?,314313.0
116031,502756.0,,nan,
116053,501907.0,,nan,
116053,502017.0,,nan,
116053,502751.0,,nan,
116087,502232.0,,nan,
116087,502024.0,,nan,
116087,502374.0,,nan,
116087,502743.0,,nan,
116087,503296.0,118059.0,"If we go ahead with PCA feature selection, then lasso optimization becomes irrelevant. We can just go ahead and run ridge regression to create a model ?",302735.0
116089,502067.0,,nan,
116089,503299.0,,nan,
116089,502008.0,,nan,
116033,501968.0,116904.0,"Sorry Rajesh. Not so clear. I understand the logic for 5. But what I cant understand is how it can only be 13. Either 1 feature is good enough or 2 features is good enough or 3 and so on.... And each 1, 2 or 3 features and so on can be selected in nCr ways. Hope you get my point.",318084.0
116033,501968.0,116921.0,You are using RFE to select the key features and not randomly selecting. If you are selecting features random than nCr will apply. Here you are using RFE and based on the ranking will return the n top features,317514.0
116033,501968.0,116947.0,"Hi Premnath, PLease read the updated answer of Rajesh",344894.0
116033,501968.0,116950.0,If you are using RFE for selecting the number of parameter then how is it called hyper parameter tuning through GridSearch CV?,318084.0
116033,501968.0,116983.0,My doubts are still not clarified. I have added it in the comments. Request TA to clarify it.,318084.0
116033,501968.0,117024.0,"Hi, Let's say at the first time you want to pass 15 variables into RFE, and after that you got some results and statistics of variables & VIF. You find that you need to add more varibles or drop variables. In that case again either you will run RFE with new no. of features or you will add or drop variables manually. So basically you are not sure that how many variables you need to pass through RFE. So you need to choose best. That's why it becomes hyper-parameter",344894.0
116033,501968.0,117516.0,"Hi Paras, I understand what is a hyper-parameter. I think we are deviating from the actual doubt. If there are 13 features and I want to do hyper-parameter tuning using GridSearch Cross validation (5 fold). Then the number of times the algorithm runs should be 8191 different combinations (nCr - 13C1, 13C2..... 13C13) multiplied with 5 for 5 fold cross validation, the total comes to 40955. Instead of that, why is it mentioned as 65 in the lecture? Can you help me understand what am I missing here?",318084.0
116106,502070.0,,nan,
116138,502195.0,,nan,
116138,502286.0,,nan,
116158,502369.0,,nan,
116158,502396.0,117034.0,"Thanks for the reply Keerthi, I understand the difference between feature selection and CV",302735.0
116158,502396.0,117035.0,Thanks Keerthi,344894.0
116158,502644.0,,nan,
116301,503019.0,,nan,
116301,503156.0,,nan,
116301,503255.0,117199.0,"mark the TA response accordingly so TA is alerted that the question is not answered, if you have not already done so",300694.0
116301,503255.0,117160.0,Exactly Paras!! this is what we're saying and this is what StackOverflow link that i provided says. But then graded question call it as hyperparametrer. Take a look at graded question number 03 in Model Evlauation now!!,316349.0
116301,503255.0,117185.0,Exactly Paras!! this is what we're saying and this is what StackOverflow link that i provided says. But then graded question call it as hyperparametrer. Take a look at graded question number 03 in Model Evlauation now!!,316349.0
116301,503255.0,117202.0,Yes Hemant,344894.0
116301,503255.0,117223.0,Report an error to Upgrde,344894.0
116301,503255.0,117214.0,so what do we do? if you agree that this graded question is incorrect then are we all getting the marks for it? or what do you guys decide?,316349.0
116301,503365.0,117325.0,"Agree, even I got this doubt when tried to answer this question.",314730.0
116303,503025.0,,nan,
116303,502900.0,117072.0,"Understood the concept of Regularization that you've explained. My question is more oriented towards mathematical view like on changing the value of Lambda (either high or low), how's the error term (only the error term and not the overall expression) getting affected?",318355.0
116303,502900.0,117075.0,"As stated in the image, if the error term is (Yi-AXi-B)^2, in both the scenarios i.e. scenario 1 as well as 2, the error term won't be affected.",318355.0
116303,502900.0,117074.0,"Assume scenario 1 where lambda =0 and scenario 2 where lambda =1. Which model will be more complex? In which scenario, the model will return lower error term - scenario 1 or scenario 2?",317514.0
116303,502963.0,,nan,
116307,503266.0,117186.0,"As this is a graded question, can this be looked into and the scores be adjusted",306725.0
116307,503266.0,117175.0,There is something wrong with either the question or the answers provided.,318741.0
116319,502953.0,117081.0,"but, in the python video it was done before the split..",318403.0
116319,503016.0,,nan,
116319,503061.0,,nan,
116319,503204.0,,nan,
116319,503172.0,,nan,
116319,503288.0,,nan,
116343,503207.0,,nan,
116343,503430.0,,nan,
116346,503094.0,,nan,
116346,503079.0,,nan,
116286,502811.0,,nan,
116286,502814.0,,nan,
116286,502991.0,,nan,
116286,502820.0,,nan,
116286,503293.0,,nan,
116286,503332.0,,nan,
118632,511978.0,,nan,
118632,519989.0,,nan,
118632,520580.0,,nan,
118412,510962.0,,nan,
116368,503203.0,,nan,
116368,503239.0,,nan,
116368,503294.0,,nan,
116046,501908.0,,nan,
116046,501989.0,,nan,
116046,502184.0,,nan,
119458,517376.0,,nan,
95417,403556.0,100843.0,"use split function or INT(TRIM( SPLIT( [Field], ""*"", 1 ) )) command.",318448.0
95417,403556.0,100726.0,may i know how to remove the * symbol?,320606.0
95417,403571.0,100747.0,thank you this worked.,320606.0
95417,403694.0,,nan,
95417,403784.0,,nan,
95417,403825.0,100844.0,"We can create a new column after removing ""*"" and then we can use both wherever required.",318448.0
95417,404157.0,,nan,
95417,403924.0,,nan,
95471,403821.0,100774.0,I'm asking for alternative or workaround,310518.0
95471,403821.0,100898.0,no there is not. only method,315560.0
95471,403821.0,101594.0,Looking for workaround/alternative without using spilt/trim,310518.0
95471,403953.0,,nan,
95471,403692.0,100765.0,"I am not sure, if Tableau is designed to ignore such things as it involves the data type of it. You can try splitting the values on the source it self and load the data to Tableau",311160.0
95471,403692.0,100762.0,is there any option without custom spilt,310518.0
95471,403671.0,100763.0,"@Nishan Patel, I'm looking to have a arithmetic calculation method which accepts string instead of Numeric value. Here the score is a string value not a numeric. For Example: 102* this is Not out score. Hope you understand my question.",310518.0
95471,403783.0,,nan,
95471,404076.0,,nan,
95471,404155.0,,nan,
95471,404683.0,,nan,
95471,403912.0,,nan,
95489,403823.0,,nan,
95489,403827.0,,nan,
95489,403842.0,,nan,
95489,403913.0,,nan,
95489,403920.0,,nan,
95489,403833.0,,nan,
95489,404104.0,,nan,
95489,404286.0,,nan,
95489,404682.0,,nan,
95489,405375.0,,nan,
95489,405578.0,,nan,
95285,403052.0,,nan,
95285,402962.0,,nan,
95285,403145.0,,nan,
95285,403675.0,,nan,
95285,404109.0,,nan,
95515,404685.0,,nan,
95515,404034.0,,nan,
95515,404140.0,,nan,
95515,404874.0,,nan,
95515,404000.0,,nan,
95515,408203.0,,nan,
95530,404108.0,,nan,
95530,404100.0,,nan,
95530,404173.0,,nan,
95530,404500.0,,nan,
95530,404684.0,,nan,
95530,405163.0,,nan,
95530,405270.0,,nan,
95845,406179.0,101402.0,you have to see the growth rate from Q2 to Q3 for the given year,311219.0
95845,406179.0,101305.0,Thank you Deval,308434.0
95845,406203.0,,nan,
95845,407187.0,,nan,
95854,406219.0,101349.0,"Below is the exact copy of the question 3 asked in the Tableau graded question. Where is the explicit mention of using the interval of 10? Whenever Virat Kohli has scored 90-100 runs (exclude 100), what has been the average strike rate?",314730.0
95854,406219.0,101322.0,"Tableau automatically identifies the ranges as mentioned in the lectures. If we are to arrive at the right answer to the question, I believe they need to mention explicitly in the question itself that the default bin ranges need to be modified.",314730.0
95854,406219.0,101329.0,"See the previous questions already mention that you need to modify the bin ranges to get the right answer. It is possible that you got the correct answer in the previous question without changing the bin-widths, which is a wrong method to apply here. Since the previous question also explicitly mentions the interval by giving options that have an interval of 10, it is imperative that you use the same histogram to solve this question. Also, this specific concept of binning has been discussed in the practice questions of the Histogram segment. Please check.",313517.0
95854,406215.0,101319.0,"Hi Abhishek, Tableau automatically identifies the ranges as mentioned in the lectures. If we are to arrive at the right answer to the question, I believe they need to mention explicitly in the question itself that the default bin ranges need to be modified.",314730.0
95854,406215.0,101323.0,"They have written it in question, you have missed it. Check question 2 & 3.",318458.0
95854,406215.0,101324.0,"When question say - ""Whenever Virat Kohli has scored 90-100 runs (exclude 100),"" that itself means range is 90-99.",318458.0
95854,406215.0,101328.0,"It is indirectly written in question. In case, you have already got answer, we can conclude this chain. If not, then use the information suggested by me. Thanks!",318458.0
95854,406215.0,101326.0,The question says to use the previous histogram created for earlier question to find the runs interval and did not mention to modify the bin ranges.,314730.0
95852,406205.0,101314.0,"Hi Deval, If I create a formula with total number of runs / number of innings and plot the graph, the answer I am getting is different to what I am getting by doing the average runs.",314730.0
95852,406208.0,101333.0,The formula you used would be for individual values instead of aggregate values. The question asked you to calculate the best batting average. So you need to find the number of runs scored against a particular opposition and divide it by the number of innings played against them. That is how you need to proceed in this question. The mistake you did was you changed the aggregation from the start and it changed the way the data is represented.,313517.0
95852,406208.0,101325.0,"If I plot the graph using the formula (the number of runs scored / the number of innings), the answer I selected was mentioned as incorrect in the feedback comments and a different solution is suggested. I do not understand cricket terminology and if this formula and average runs are the same, this need to be explicitly stated in the question.",314730.0
95852,406294.0,,nan,
95852,407295.0,,nan,
95866,406280.0,,nan,
95866,406288.0,,nan,
95866,406351.0,,nan,
95866,407306.0,,nan,
95882,406358.0,,nan,
95882,406360.0,,nan,
95882,406397.0,101377.0,"Thanks for your help, Ashish!",318827.0
95882,406397.0,101502.0,Not able to find a way to remove * from a runs column.. Any help,311227.0
95882,406905.0,,nan,
95536,404137.0,,nan,
95536,404143.0,,nan,
95536,404297.0,,nan,
95536,404348.0,,nan,
95550,404195.0,,nan,
95550,404237.0,,nan,
95550,405267.0,,nan,
95564,404335.0,,nan,
95564,404343.0,,nan,
95582,404512.0,100909.0,Thanks Akhilesh.. one confusion.. what is score-1?,305845.0
95582,404512.0,100924.0,that's probably the name he has given to extra column that comes after the split..,319898.0
95582,404512.0,100931.0,ohh okay.. got it..,305845.0
95582,404512.0,100938.0,Be aware that it is not sum(innings) but it is count(innings).,318007.0
95582,404512.0,100946.0,"But Tez,.. count(innings) will give you the average per match.. for average per innings don't you have to use sum(innings)..?",319898.0
95582,404512.0,101033.0,ohh ... i also had the similar thought the value of innings is the number of innings.. Thanks for the explanation,305845.0
95582,404512.0,101036.0,thanks Harsha..,319898.0
95582,404512.0,100995.0,"Hi Ranjith, Sum(innings) shouldn't be used as the innings number just indicates 1 if the batsman's team has batted first and 2 if the team has batted second. So, summing those numbers should be avoided.",311160.0
95582,404512.0,101079.0,Yes correct it should be COUNT(inns) .,315679.0
95582,404523.0,100932.0,Cool Explanation... Thanks Harsha,305845.0
95582,404523.0,100939.0,Good explanation!,318007.0
95956,406937.0,101450.0,"I am not getting any edit option on right clicking on any of the bins. Also no edit option on the drop down from Runs (bin) pulled in the columns. Applying a filter will probably solve my question, but that would not help if I want the same range throughout the histogram.",315471.0
95956,406952.0,,nan,
95608,404634.0,,nan,
95608,404872.0,101306.0,This is very helpful.,318458.0
95608,404690.0,,nan,
95636,404848.0,,nan,
95636,404880.0,,nan,
95636,404868.0,,nan,
95636,405701.0,,nan,
95643,405723.0,,nan,
95643,404908.0,,nan,
95643,405878.0,,nan,
95284,403001.0,,nan,
95284,403433.0,,nan,
95284,403824.0,,nan,
95284,403922.0,,nan,
95284,405707.0,,nan,
95288,404484.0,,nan,
95288,404474.0,,nan,
95288,403467.0,,nan,
95288,403022.0,,nan,
95288,403102.0,,nan,
95288,403972.0,100839.0,Avg Salary ? what is that ? who is giving Salary to Virat ??,310501.0
95288,403972.0,100897.0,"sorry not the salary ,, runs..:) average runs",315560.0
95288,403972.0,100877.0,Avg. salary ?,310952.0
95288,404087.0,100845.0,"Even i am getting the same 82.7, can anyone help?",318448.0
95288,404087.0,100862.0,yes. Im surprised too. I too got the exact same numbers and got it wrong,311857.0
95288,404087.0,100865.0,Hint: Try sorting mins,311857.0
95288,404087.0,101586.0,maximum time means sum of all the time he spend the respective ground for all the matches. Then find which ground he spend maximum time and then average strick rate in that ground.,318461.0
95288,404148.0,,nan,
95288,404175.0,100895.0,I am also not able to get properly. not sure some cleanup to be done for mins column data ?,312019.0
95288,404175.0,100896.0,do a sort,311857.0
95288,404175.0,101585.0,Yes. maximum time means sum of all the time he spend the respective ground for all the matches. Then find which ground he spend maximum time and then average strick rate in that ground.,318461.0
95288,404176.0,100868.0,sort the mins,311857.0
95288,404176.0,100866.0,What ? Explain,310501.0
95288,408197.0,,nan,
95331,403346.0,100781.0,Can you elaborate more on your explanation? Or redirect me to some similar example please,311741.0
95331,403346.0,100899.0,Please check https://learn.upgrad.com/v/course/208/question/95374,313517.0
95331,403917.0,,nan,
95337,403534.0,100743.0,"In Tableau, select Analysis > Create Calculated Field. In the Calculation Editor that opens, do the following: Enter a name for the calculated field. In this example, the field is called, Discount Ratio. Enter a formula. This example uses the following formula: IIF([Sales] !=0, [Discount]/[Sales],0) This formula checks if sales is not equal to zero. If true, it returns the discount ratio (Discount/Sales); if false, it returns zero. sam you have to do for batting average",303082.0
95337,403345.0,,nan,
95337,403382.0,100757.0,try creating a new column for runs without any special character,311686.0
95337,403382.0,100756.0,how do we include such runs where he was not out?? as soon as i change the datatype it creates null in such cases..,316349.0
95337,403435.0,,nan,
95337,403672.0,,nan,
95337,403794.0,,nan,
95337,403950.0,,nan,
95337,404153.0,,nan,
95337,404293.0,,nan,
95337,406181.0,,nan,
95733,405386.0,101161.0,Thanks Rashmi,305845.0
95733,405419.0,,nan,
95733,405536.0,,nan,
95733,405705.0,,nan,
95374,404105.0,,nan,
95374,403916.0,,nan,
95374,403944.0,100796.0,Edit the bin size by right clicking on runs-split (dimension).,318335.0
95374,403497.0,,nan,
95374,403498.0,100710.0,great,315560.0
95374,405348.0,,nan,
95374,404184.0,,nan,
96128,408577.0,,nan,
96128,409047.0,,nan,
96128,408847.0,,nan,
96131,408583.0,,nan,
100265,430941.0,,nan,
100265,430988.0,,nan,
95463,,,nan,
95482,404825.0,,nan,
96047,407793.0,101555.0,"Btw, which question are you reffering to ? It's not clear from your question.",318329.0
96047,407904.0,,nan,
96047,409054.0,,nan,
95171,402174.0,100465.0,Thanx for reply. Did with the same also. No response matched.,311117.0
95171,402174.0,100467.0,check u are using avg balance and not sum(balance),309451.0
95171,402174.0,100806.0,"campaign outcome is not a measure but dimension. Campaign outcome cannot be measured, aggregated, or used for mathematical operations.",317514.0
95171,402174.0,100847.0,I think you have misinterpreted my answer Rajesh. Here campaign and campaign outcome(poutcome) are different entities. campaign is a measure while poutcome is a dimension.,309451.0
95171,402556.0,,nan,
95171,402801.0,,nan,
95171,402823.0,100664.0,Thanx Buddy.. It helped,311117.0
95173,402154.0,,nan,
95173,402736.0,,nan,
95173,402847.0,,nan,
95173,402187.0,,nan,
95186,402261.0,,nan,
95186,402586.0,100528.0,"Thanks for the help, it should last contacted month as May, the question has been updated, i had raised a query.",304813.0
95186,402586.0,100927.0,I couln't able to see the numbers.What option has to check,318846.0
95186,402586.0,100936.0,drag variable to label,318005.0
95186,402586.0,101900.0,"Thanks Abhishek, I afraid i am missing so many points from online training videos.",310502.0
95186,402830.0,,nan,
95186,403165.0,,nan,
95187,402247.0,100526.0,It would be last contacted.,304813.0
95187,402247.0,100525.0,you can now see the question has been updated in the module. I had raised a query regarding that.,304813.0
95187,402589.0,,nan,
95187,402592.0,100527.0,"Thanks for the answer, it would be last contacted month as month, the question has been updated. please check.",304813.0
95187,402825.0,,nan,
95987,407490.0,,nan,
95987,407272.0,,nan,
95987,407293.0,,nan,
95987,407296.0,101496.0,All values in 'pOutcome' column are 'Unknown',314799.0
95987,407296.0,101500.0,"u sure about that ?.. bcs I can find other values like success, failure, others in pOutcome column",319898.0
95987,407296.0,101535.0,"From my bank-marketing.csv, some data was missing. Thanks for helping me figure this out.",314799.0
95264,402800.0,,nan,
95264,402837.0,,nan,
95264,402784.0,100609.0,"so if I have two excel file, can I use join to combine the data ?",311741.0
95264,402784.0,100641.0,Yes that should be try it.,301648.0
95264,402784.0,100677.0,Yes. try it.,301557.0
95281,402934.0,,nan,
95281,403164.0,101062.0,Hi Can you tell me how did you plot the above graph '% difference b/w months',317410.0
95281,403164.0,100737.0,Thanks Ram. But I'm still not able to understand how this connects to the answer required by the question. See as per the question shouldn't we Calculate the Average Salary over all year minus the Avg (Salaries) for the month of July? But here it is just calculating the difference between Avg Salaries of months June and July. How are both related? This is where my confusion is. The question asked doesn't seem to convey this. I hope you understand my logic.,318335.0
95348,403381.0,,nan,
95348,403545.0,,nan,
95348,403469.0,,nan,
95354,403386.0,,nan,
95354,403383.0,,nan,
95354,404445.0,,nan,
95368,403573.0,,nan,
95368,403587.0,,nan,
95368,403611.0,100751.0,Yeah. :),304026.0
128741,562097.0,,nan,
128741,562205.0,,nan,
128741,562236.0,,nan,
128741,562164.0,,nan,
128741,563300.0,,nan,
128272,560180.0,,nan,
128272,560168.0,,nan,
128272,560376.0,,nan,
128272,561038.0,,nan,
128272,559995.0,,nan,
128272,559843.0,,nan,
130511,570259.0,,nan,
128839,562644.0,126604.0,No this is the one of the feature of hadoop cluster which comes with replication factor in HDFS. https://www.dummies.com/programming/big-data/hadoop/replicating-data-blocks-in-the-hadoop-distributed-file-system/,307843.0
128839,562644.0,126599.0,But will this not cause data redundancy ?,312259.0
128839,562801.0,,nan,
128839,563348.0,,nan,
127985,558507.0,,nan,
127985,558511.0,,nan,
127985,558550.0,,nan,
127985,558822.0,,nan,
128051,559211.0,126174.0,Correct.. I had read somewhere that Yahoo has more than 4K nodes in its Hadoop cluster.. so there would definitely be multiple data nodes and possibly spread across multiple Data centres (for DR purpose). Hadoop has its own mechanism to ensure 'Data Locality' when it runs the MapReduce jobs on the Data nodes such that the job essentially runs on the DN where the required data blocks are residing.. to avoid N/W traffic,301116.0
128051,558932.0,,nan,
128051,561567.0,,nan,
129100,563879.0,,nan,
129100,563965.0,,nan,
129505,565752.0,,nan,
128624,563537.0,126900.0,"Hi Nidhi, the second part of your answer is not up-to-date. Hadoop provides high availability for the NameNode with the configuration of a Standby Namenode which acts as a hot standby for the primary and the failover is automatic",301116.0
128624,562985.0,,nan,
128624,561994.0,,nan,
128624,561594.0,,nan,
128624,561660.0,,nan,
128623,561593.0,,nan,
128623,561871.0,,nan,
128900,562981.0,,nan,
128900,563170.0,,nan,
128900,563372.0,,nan,
128900,563350.0,,nan,
128146,,,nan,
128648,561910.0,,nan,
128648,561998.0,,nan,
128405,560392.0,,nan,
128405,560952.0,,nan,
128405,560880.0,,nan,
128869,562812.0,,nan,
128869,564169.0,,nan,
128869,563360.0,,nan,
128869,562834.0,,nan,
128726,562059.0,126529.0,"No. it stores all the copy of meta data of the Name Node on timely basis. So when the Name Node fails and restarts again, this snapshot will help to restore the Name Node",308673.0
128726,562059.0,126480.0,So basically Secondary node is of no big use??,314313.0
128726,562227.0,,nan,
128313,603534.0,,nan,
128313,560205.0,,nan,
128313,560016.0,,nan,
128313,561367.0,,nan,
128332,560147.0,,nan,
128332,560160.0,,nan,
128332,560187.0,,nan,
128332,564140.0,,nan,
128370,603527.0,,nan,
128370,560277.0,126076.0,thanks khusbu,320251.0
128513,561073.0,126210.0,Yeah.. I have gone through the link.. but does it really answer my query ??,318770.0
128513,561449.0,,nan,
129061,563892.0,,nan,
128145,564028.0,,nan,
128145,559743.0,,nan,
128646,603531.0,,nan,
128646,561984.0,,nan,
128646,562899.0,,nan,
128646,563283.0,,nan,
135386,586250.0,,nan,
135386,586169.0,,nan,
139028,600138.0,,nan,
92351,388878.0,,nan,
92351,389418.0,,nan,
92351,393638.0,,nan,
90243,375961.0,,nan,
90243,375965.0,,nan,
90243,375969.0,,nan,
90243,375971.0,,nan,
90243,375968.0,,nan,
90243,375976.0,,nan,
91168,380884.0,,nan,
91168,380895.0,,nan,
88646,366970.0,,nan,
88646,366994.0,92955.0,if you completed all segments and MCQs then it should allow you to complete...so pls check it,318454.0
88646,367050.0,,nan,
88646,367081.0,,nan,
88646,367974.0,,nan,
88646,368668.0,,nan,
90467,377363.0,95180.0,Please follow the below link and optimise ur solution https://softwareengineering.stackexchange.com/questions/254475/how-do-i-move-away-from-the-for-loop-school-of-thought,318358.0
90467,377363.0,95152.0,Which operator you would suggest for this?,311741.0
90467,378756.0,,nan,
88625,366984.0,93085.0,Yes i also followed the same approach,318756.0
88625,366842.0,,nan,
88625,366881.0,,nan,
88625,366910.0,,nan,
88625,366915.0,,nan,
88625,367043.0,,nan,
88625,367085.0,,nan,
88625,367214.0,93114.0,"Hi Vijay, please do not provide direct answers as general practice. We'll have to take strict action if such a thing is repeated. Thanks for understanding !",301618.0
88625,367221.0,,nan,
88625,367401.0,,nan,
88625,367714.0,,nan,
88625,367720.0,,nan,
88625,367883.0,,nan,
88625,367913.0,,nan,
88625,369350.0,,nan,
88625,368762.0,,nan,
88625,369528.0,,nan,
88683,367249.0,,nan,
88683,367254.0,,nan,
88683,367477.0,,nan,
88683,367921.0,,nan,
88683,403812.0,,nan,
88590,366719.0,,nan,
88590,366779.0,92950.0,Thanks for pointing out Nishan. :-),315028.0
88590,366779.0,92926.0,Bangalore should be 238. I think so you have not calculate upper case of Bangalore Rest of the data looks good..,320195.0
88590,366966.0,,nan,
88590,366856.0,,nan,
88590,367560.0,,nan,
88715,367486.0,,nan,
88715,367407.0,93021.0,"Hi Bishnu, glad it helped. please upvote the answer if you liked it. thanks.",317998.0
88715,367402.0,,nan,
88715,367408.0,,nan,
88715,367453.0,,nan,
88715,367501.0,93043.0,"Hi Bishnu, please upvote my answer if it helped you. thanks.",317998.0
88715,367502.0,,nan,
88715,367531.0,93058.0,"No need to manually count. counta( ,'female'), counta( ,'male') does the job",305653.0
88715,367544.0,,nan,
88715,367666.0,,nan,
89303,370285.0,,nan,
89303,370272.0,,nan,
89303,370337.0,,nan,
89303,370398.0,,nan,
88642,366975.0,,nan,
88642,366977.0,,nan,
88642,367678.0,,nan,
88642,367331.0,,nan,
88642,367217.0,,nan,
88642,367040.0,93067.0,"Hi Ritesh, Thanks for the information. Still i am not successful doing this. Could you give details steps still more. PGDDS+Cohort+8+September+2018.csv",312019.0
88642,367040.0,93079.0,"Sure Naga, can you let me know at which step you need help? I thing I need to mention is once you standardize your spelling you need to manually refresh your pivot table under data tab.",318021.0
88642,367040.0,93089.0,"select the data range. how to set this. can you have skype id. Not sure how to discuss these clarifications in live , so it will be faster. your mobile no pls ?",312019.0
88642,367040.0,93102.0,Things works perfect. Really appreciate your detailed procedure Ritesh. Thank you so much.,312019.0
88642,367040.0,93094.0,"okay let me explain in detail 1. select the entire data in the sheet using the mouse (click on the first cell containing the data and drag till the end) -> go to insert tab -> select pivot table (by default it will insert in new sheet) -> press enter. 2. Once the pivot table is inserted in the new sheet you should be able to see pivot table fields on the right (if not, then select Analyze tab ( becomes active on inserting pivot table) and click on field list) -> drag the column name “city” to row in field list -> drag the column name “city” to values as well in field list (by default, it will give you the count of the city names appearing the dataset). 3. By default Pivot table will sort the city name in ascending order 4. Manually scroll through the pivot table data to see spelling variants for the same city name eg. “Bangalore” , “Bangalore “ (space after e), “bangaloru” etc. 5. Now go back to the original data set and select the entire data range as mentioned in step 1 -> go to data tab -> select filter 6. Now select the drop down from city and select all spelling variants for bangalore (as mentioned in step 4) and update it with Correct spelling (to do that select the city column after filtering -> Press “Ctrl + y” (selects only visible cells) -> press delete -> fill the correct spelling in one of the blank cells and press “ctrl +c” -> now select the entire deleted cell range -> press ctrl+y to selet visible cells and press ctrl+v to paste 7. Now go back to the pivot table sheet and under Data tab press refresh -> Now you should see that all the variants of Bangalore are updated with the correct spelling and the count updates too. 8. Now simply, you can drag the city name again to values and right click under values area and select value field settings -> under show value as tab -> select show values as % of column total and you'll have your answer. Hope that helps.",318021.0
92662,390386.0,,nan,
123967,538388.0,,nan,
124006,538650.0,,nan,
124006,539164.0,,nan,
122868,534413.0,121980.0,"very helpful, thank you. However, what's the command to get a logistic model with odds as the output instead of co-efficients?",310509.0
122868,537337.0,,nan,
122909,534726.0,,nan,
123085,535880.0,,nan,
123085,536872.0,,nan,
123085,537035.0,,nan,
123085,538271.0,,nan,
123085,538877.0,,nan,
123085,540237.0,,nan,
123085,546649.0,,nan,
122236,532311.0,,nan,
123422,536962.0,,nan,
123462,537034.0,,nan,
123462,536940.0,,nan,
133015,580026.0,,nan,
99599,426175.0,,nan,
99599,426217.0,104334.0,Thank you for the Swan example.. you made it much clearer..👍,310508.0
99599,426256.0,,nan,
99599,426576.0,,nan,
99599,426414.0,,nan,
99934,428234.0,104645.0,"Smaller the level lesser the accuracy for inability to reject null hypothesis,since the critical region is smaller .Isn't it?",305655.0
99934,428321.0,,nan,
99934,429572.0,,nan,
99934,429687.0,,nan,
99964,428429.0,,nan,
99964,429697.0,,nan,
99964,431664.0,,nan,
99967,428461.0,,nan,
99968,428464.0,,nan,
99034,421632.0,,nan,
99034,421634.0,103753.0,"yes, a claim con not directly be a null hypothesis.",318344.0
99034,423894.0,,nan,
99034,424825.0,,nan,
99814,427424.0,,nan,
99814,427453.0,,nan,
99820,427432.0,,nan,
99820,427607.0,,nan,
99679,427326.0,,nan,
99821,427420.0,,nan,
99821,427449.0,,nan,
99821,428247.0,,nan,
99682,426834.0,,nan,
99682,426910.0,,nan,
101064,,,nan,
100179,429717.0,,nan,
100179,429719.0,104771.0,Thanks Rohit,314629.0
100183,429700.0,,nan,
100183,429699.0,,nan,
100183,431328.0,,nan,
100182,429703.0,104739.0,Looks like there can be cases where Null hypothesis can be rejected but the claim statement can hold good. I could not realize this in the videos.,318007.0
100182,429703.0,104789.0,can you elaborate? any example?,311686.0
100182,429708.0,104740.0,"As I mentioned in my question, I do know the process and solved the problem. It is just that I am trying to understand the semantics behind it. Looks like there can be cases where Null hypothesis can be rejected but the claim statement can hold good. I could not realize this in the videos.",318007.0
100182,431176.0,,nan,
100196,430942.0,,nan,
100199,429981.0,,nan,
99707,427070.0,,nan,
99707,427335.0,,nan,
99707,427901.0,,nan,
98707,420321.0,,nan,
98707,420362.0,,nan,
98707,424818.0,104176.0,"Thank you , I have already got my response from Mahima which is verified and accepted. please see above",309211.0
98707,426941.0,104512.0,"Thank you , I have already got my response from Mahima which is verified and accepted. please see above",309211.0
99259,424816.0,,nan,
99259,423150.0,,nan,
99259,423895.0,,nan,
99259,426622.0,,nan,
99715,427040.0,,nan,
99715,427067.0,,nan,
99461,425180.0,,nan,
99461,425681.0,,nan,
99461,425181.0,,nan,
99461,425335.0,,nan,
99464,425175.0,104223.0,yes in th context of data given there is no need but you have to divide the 3 percent error chance by 2 as its a two tail example,318017.0
99464,425175.0,104218.0,i can understand that it can be both ways but the alternative hypothesis is what is trying to be proven. In this case since there is a warning of heatwave management is trying to test whether it will be >350 units or not? There is no point in checking <350 units considering the context of the question,310509.0
99522,425635.0,,nan,
99522,425696.0,,nan,
99522,425777.0,,nan,
99522,425632.0,,nan,
99524,425647.0,,nan,
99524,426571.0,,nan,
99524,426308.0,104373.0,Thanks Keerthi. This makes sense :),308962.0
100144,429327.0,,nan,
100144,429441.0,,nan,
99590,426082.0,,nan,
99590,426418.0,,nan,
99590,426262.0,,nan,
99731,427075.0,,nan,
99731,427073.0,104468.0,I know its mentioned in the course but curious as to How have we derrived at the below rule? The null hypothesis always has the following signs: = OR ≤ OR ≥ The alternate hypothesis always has the following signs: ≠ OR > OR < because it seems like in some cases we're considering the claim to be null hypothesis and in other cases the opposite of the claim as null hypothesis.,307176.0
99731,427073.0,104471.0,"In all the cases,we are checking the claim to follow the rules = OR ≤ OR ≥ to decide on Null hypothesis. Anything opposing that , we are taking it as Alternate hypothesis. Regarding the derivation of the rules...it is not covered in the module. We need to research on that. However all the examples given in the module are following the above rules.",311254.0
99924,428166.0,,nan,
99924,467423.0,,nan,
99924,428179.0,,nan,
99462,425178.0,,nan,
99462,425589.0,,nan,
99462,425794.0,,nan,
99462,425340.0,,nan,
99462,425142.0,,nan,
99940,428858.0,,nan,
99940,428319.0,,nan,
99962,428424.0,104731.0,"The values in Standard Z-table are calculated using the formula given in this segment just below the table image. https://learn.upgrad.com/v/course/208/session/18004/segment/91584 Basically, since we know the mean (0) and standard deviation (1) of a standard normal distribution, we can calculate the area under the curve for the normal PDF to get the probability. If you are familiar with basic calculus, this area calculation is the same as integrating it between two values. That is how the formula in the above segment goes by in calculating the Z-values. Once the cumulative probability has been calculated, the probability between 2 different Z-values can also be easily found out by subtracting one from another.",313517.0
99962,428424.0,104728.0,i want theoretically explanation,311386.0
99962,428441.0,,nan,
99903,427988.0,,nan,
99903,428189.0,104730.0,Thanks Vinay for sharing the technique of ANOVA value directly. I was trying to understand the logic of computing the values manually,317514.0
99903,428943.0,104737.0,Thanks for the clarification,317514.0
99903,429775.0,,nan,
100925,433585.0,,nan,
100925,440195.0,,nan,
99953,429631.0,,nan,
99953,429600.0,,nan,
99953,429641.0,,nan,
99952,428373.0,,nan,
99952,428971.0,,nan,
100097,428743.0,,nan,
100097,429799.0,,nan,
100097,430091.0,,nan,
99806,427313.0,,nan,
99806,427800.0,,nan,
100417,431762.0,104936.0,It's the same data. This is the result in the videos.,311857.0
100417,431943.0,,nan,
99699,426878.0,,nan,
99699,427374.0,,nan,
99699,428362.0,,nan,
101125,434965.0,,nan,
100189,429793.0,,nan,
100189,430993.0,,nan,
100195,431064.0,,nan,
100219,430446.0,,nan,
99452,425251.0,104226.0,"Yes, actually results of a Z and T-test are almost same for n> 30.",318344.0
99452,425234.0,,nan,
99452,425047.0,,nan,
99452,427910.0,,nan,
99641,426668.0,,nan,
99641,426510.0,104437.0,but it should be 9% and not 18%..you can double check by looking at cumulative probability of Z-score of 1.34 = .9099. Hence defective portion is only ~.09.,310509.0
99641,426909.0,,nan,
99514,426695.0,,nan,
99514,425593.0,,nan,
99514,425895.0,,nan,
99954,428982.0,,nan,
99954,429735.0,,nan,
99954,428370.0,,nan,
99908,428256.0,,nan,
99908,428183.0,,nan,
99908,428087.0,,nan,
100526,432094.0,,nan,
100526,433936.0,,nan,
100171,429528.0,104713.0,"I am aware of this. But, can we do one sample mean test in excel?",314730.0
100171,430213.0,,nan,
100176,431144.0,,nan,
100173,429589.0,,nan,
100310,431351.0,,nan,
100310,431948.0,,nan,
99978,428554.0,,nan,
99978,431949.0,,nan,
100948,433625.0,,nan,
100029,429147.0,,nan,
100029,428694.0,,nan,
99957,428396.0,,nan,
99957,428417.0,,nan,
99957,428467.0,,nan,
99957,431941.0,,nan,
99965,428439.0,,nan,
99965,429822.0,,nan,
99966,428450.0,104632.0,"Yes, take one more sample, conduct a hypothesis test again and see if the test produces a significant result or not.",313517.0
99966,428450.0,104631.0,means we should take fresh sample ?,319969.0
99974,428504.0,,nan,
99974,428497.0,104779.0,"Thats the point. In your case, you have used >= and in good year case, its <= when we claim 'at least'",314084.0
99974,428497.0,104780.0,"Thats the point. In your case, you have used >= and in good year case, its <= when we claim 'at least'",314084.0
99974,428497.0,104806.0,"Good year's claim statement was that the tyres travels more than 7500 miles. Thus it has a > symbol associated with it. By convention, the null hypothesis only has =, >= or <= symbol associated with it. If the claim statement's symbol matched with it, then the null hypothesis would be denoted by the claim statement itself. However, since it doesn't whatever Goodyear claimed became the alternate hypothesis. The null hypothesis was the opposite of what has been claimed by Goodyear.",313517.0
99775,427263.0,,nan,
99775,427284.0,,nan,
99775,427720.0,,nan,
99775,427764.0,,nan,
99564,425859.0,,nan,
99564,425869.0,104300.0,"My bad, i was adding it to the sample mean 2.6 and not the population mean 2.5",316349.0
99564,426330.0,,nan,
99803,427328.0,104804.0,Check if this post clears your query:https://learn.upgrad.com/v/course/208/question/99920,313517.0
99803,427328.0,104741.0,TA pls explain this in simple manner with an easy example..Why alpha and beta cannot have low values simultaneously?,308437.0
99697,426906.0,104447.0,You statement is true for the 'p-value method ' whereas for the 'critical value method' it is critical points.. See the first point of Central Limit Theorem: Sample Distribution Mean = Population mean,316349.0
99697,426906.0,104441.0,Mu(X-bar) will be always equal to Sample Mean,315679.0
99697,431953.0,,nan,
101090,434450.0,,nan,
101090,434714.0,,nan,
100124,428949.0,,nan,
100124,431945.0,,nan,
99714,427133.0,104681.0,As per your answer I have already saw the upgrad content but want more example or link to Clear this topic. If u have then share..,305847.0
99714,427262.0,,nan,
99714,429001.0,104684.0,Thanks manmeet,305847.0
99719,427007.0,,nan,
99719,427330.0,,nan,
99719,428489.0,,nan,
99648,426509.0,104379.0,Thanks for explaining Ram,314048.0
99648,426509.0,104384.0,"Yes, you are right, I missed it :)",310974.0
99648,426509.0,104383.0,Nicely explained and I believe you have missed including NOT in your first statement :) Type 1 => It's a false -ve. Null hypothesis is true but we are concluding that it is NOT true.,311160.0
99648,426888.0,,nan,
99648,426579.0,,nan,
99648,427258.0,,nan,
99648,427172.0,,nan,
100156,431936.0,,nan,
100156,429413.0,104847.0,This doesn't answer when to use it.,301641.0
100156,429549.0,104848.0,It doesn't answer when to use them,301641.0
100156,431952.0,,nan,
99173,422594.0,104022.0,I think it will follow uniform distribution.,318344.0
99173,423178.0,104096.0,"Yes, I agree to the fact that there is no meaning of hypothesis if something is always true/false. But the question was also a hypothetical one, so the answer.",318344.0
98739,,,nan,
99736,432109.0,,nan,
99736,427101.0,,nan,
99736,427107.0,,nan,
99667,426711.0,,nan,
99667,426659.0,,nan,
99667,426885.0,,nan,
99667,426889.0,,nan,
99667,427280.0,,nan,
91369,381946.0,,nan,
91369,381941.0,,nan,
92470,389482.0,97146.0,first 19 are equal in 20 MA and 49 are equal in 50 MA. how to generate the signal as these are equal,311952.0
92470,389482.0,97149.0,"For generating signal ,you consider if difference of short term MA and long term MA cross each other. For nth row you have to check difference and compare it with MA difference of (n-1) th row. If nth row difference is positive and (n-1)th row difference is negative then signal is buy, for sell condition is opp, for all other case signal is hold.",310419.0
92470,389482.0,97443.0,Darshna please correct me if my understanding is wrong. For a particular date you have to find the difference between 20 day MA and 50 Day MA. If 20 Day MA is greater than 50 Day MA signal is BUY as shorttem MA is greater than Longterm MA. It is intuitive also. As the MA's till the row 49 are not valid it's better to remove the rows till 49 or we can insert NULL,318579.0
92470,389496.0,,nan,
92470,389525.0,,nan,
92470,389538.0,97444.0,I think we can not apply where class here. If we apply where class it will start calculating the MA's by ignoring the previous rows.,318579.0
92470,389935.0,97445.0,"As the MA's for these rows are not valid we can't decide whether to BUY, SELL or HOLD so these rows are not for the evaluation or prediction.",318579.0
92470,389918.0,,nan,
92470,389946.0,97268.0,same question i also want to know shall we put HOLD or NULL ?,318756.0
92470,389946.0,97300.0,it means you can't assess whether to hold/sell/buy so ignore. more explicitly it can be null,311857.0
92470,389946.0,97302.0,so first 49 rows need to inserted NULL not HOLD right ?,311952.0
92470,389946.0,97335.0,Yes.,311857.0
92470,390282.0,,nan,
92470,390519.0,,nan,
92470,390790.0,,nan,
90911,379065.0,95505.0,"Now, after import using LOAD LOCAL INFILE, i'm getting 889 in Eicher and 890 in all the others",310974.0
90911,379065.0,95507.0,are for other files column headers also being treated as row?,311686.0
90911,379065.0,95527.0,no. although I am working in a different environment and not exactly workbench.,311686.0
90911,379065.0,95525.0,"No, the columns are not coming as rows. Now, one more problem, i'm getting so many nulls in all tables for date column except for eicher. I'm handling the date formatting during the import. Did you also face this?",310974.0
90911,379442.0,95997.0,"i am facing the same issue, getting 888 rows for all the files!!",314565.0
90911,379442.0,96007.0,"Ramya, check the way you are importing, the issue will most probably with the way you are formatting the date while importing",310974.0
90911,379442.0,96255.0,even i am getting 888 rows for all files. i am not formatting the data even then.please advise,300687.0
90911,387910.0,,nan,
92488,389621.0,,nan,
92488,389649.0,,nan,
92488,389774.0,,nan,
92488,390074.0,98622.0,"no, problem statement says that just being abobe or below does not make golden or death cross",317811.0
92488,390074.0,98616.0,"but the problem statement is just to compare 20dayMA and 50dayMA, isn't it a right understanding?",316889.0
92488,390661.0,,nan,
92488,393972.0,,nan,
92447,389376.0,97129.0,GETTING 1064 ERROR IN THIS WAY. :(,318322.0
92447,389376.0,97282.0,"Use default import wizard, issue is with date column. Import as text and then change the data type to date after import.",315679.0
92447,389386.0,97131.0,"SAMPLE CASE OF THE WAY I DID. LOAD DATA INFILE 'c:/tmp/discounts.csv' INTO TABLE discounts FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS; GETTING ERROR IN SCRIPTING ALSO.",318322.0
92447,389416.0,97135.0,"sir, i know that its because of null values, i tried wizard option so rows got reduced because of null values, again i tried in scripting way but still getting an error.",318322.0
92447,389451.0,97173.0,Check the warning it gives for truncating a row.,300717.0
92447,389451.0,97140.0,"yes, i did. but 888 rows only imported sir.",318322.0
92447,389451.0,97145.0,may be because of SQL version.. what is the version,301113.0
92447,389451.0,97176.0,it's version 8.0.13,318322.0
92447,389451.0,97177.0,sorry 8.0.12,318322.0
92447,389451.0,97331.0,out of 889(rows except labels row) 888 rows is imported (i.e 1 row is missing). I directly imported without any modifications. while importing also I didn't get any truncate warning :(,318322.0
92447,390231.0,98627.0,This works very fine,313228.0
92447,393345.0,,nan,
92457,389446.0,,nan,
92457,389439.0,,nan,
92457,389424.0,,nan,
92457,389499.0,,nan,
92457,389879.0,,nan,
92457,389896.0,,nan,
92457,390187.0,,nan,
92457,390036.0,,nan,
92456,390541.0,,nan,
92456,389441.0,97136.0,Thanks,311952.0
92456,389441.0,97132.0,Can you help me in generating HOLD signal,311952.0
92456,389441.0,97134.0,i can't help directly byt a hint; you've to use LAG window function with IF condition in select clause to get the results.. LAG will check the previous rows..,316349.0
92456,389441.0,97526.0,"Hey, Rambabu, when you generate a table, HOLD will be the default value. And you have to update the table with buy and sell which might replace the default value. To do so, we use MA 20 and MA 50.",319721.0
92456,389422.0,,nan,
92456,389447.0,,nan,
92456,390343.0,,nan,
92354,388921.0,,nan,
92354,388952.0,,nan,
92354,389199.0,,nan,
92354,389375.0,,nan,
92354,390028.0,,nan,
92354,390127.0,,nan,
92502,389677.0,97247.0,Thank you,319444.0
92502,389619.0,,nan,
92502,389659.0,,nan,
92502,389933.0,,nan,
92505,389670.0,,nan,
92505,389643.0,97269.0,Really need a TA to clarify this since this is a major confusion among the peers,318756.0
92505,389643.0,97253.0,"For signal we at least need 2 days data, current date and previous date to check the crossover.",315679.0
92505,389656.0,,nan,
92505,389661.0,97411.0,SMA merely being above or below LMA cannot be a crossover at any stage? Can you please explain this. I didn't understand.,318846.0
92505,389661.0,97898.0,"meaning day 1 MA20 - 580 MA50 - 570 day 2 MA20 - 550 MA50 - 575 above is example of cross over, where the 2 graph crosses each other below is not a cross over, as d graphs donot cross day 1 MA20 - 580 MA50 - 570 day 2 MA20 - 550 MA50 - 555",308495.0
92505,389660.0,,nan,
92505,389766.0,,nan,
92505,389768.0,,nan,
92505,389674.0,,nan,
92505,389858.0,97252.0,Always hold,315679.0
92505,389858.0,97578.0,In case getting 20DayMAPrevious with lag is throwing error like window function cannot be used. can you help here.,312019.0
92505,389911.0,,nan,
92505,390636.0,,nan,
90176,375782.0,94899.0,thanks. I think when I will work on that problem then it will become clearer to me.,311686.0
90176,375782.0,95905.0,Can you explain it further what does intersection point mean here ?,317689.0
93677,395001.0,,nan,
93677,394739.0,,nan,
93677,394783.0,,nan,
93677,394763.0,,nan,
93677,394997.0,,nan,
92511,389682.0,97195.0,"No it will not change the datatype of the column , to do that use alter command once you have updated the column with str_to_date",319056.0
92511,389682.0,97193.0,when we use str to date will it change the data type,314678.0
92511,389669.0,,nan,
92511,390078.0,97456.0,"I tried to change it, getting this error as per the data imported, currently is varchar SET `Date` = STR_TO_DATE(`Date`, '%e-%m-%Y') Error Code: 1411. Incorrect datetime value: '31-Jul-18' for function str_to_date 0.000 sec",315028.0
92458,390158.0,,nan,
92458,390339.0,,nan,
92458,389445.0,97137.0,"getting 1146 error sir, table 'filename.tablebame' doesn't exit :(",318322.0
92458,389445.0,97139.0,sir u need to replace values in this query with the table you created in which the imported data will go,317811.0
92458,389445.0,97142.0,"Okay, Thank U sir.",318322.0
92458,389445.0,97541.0,getting error 1148 (i.e used command is not allowed with this MySQL version) but I am using MySQL 8.0.12 version (latest only).. what is the best version to do assignment??,318322.0
92458,390906.0,98255.0,"my error code , ERROR 1064 (42000) at line 1: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Date,Open Price,High Price,Low Price,Close Price,WAP,No.of Shares,No. of Trades,' at line 1",311083.0
92458,390906.0,98253.0,i am also unable to import data files..,311083.0
92514,389701.0,97196.0,Your welcome,304814.0
92514,389697.0,97599.0,"That's right Anshul. Unfortunately or fortunately you needd to use MySQL for the Assignment. Looking at upside of using MySQL, you can explore it too pretty well and get comfortable with this too.",319721.0
92514,390080.0,,nan,
92417,389272.0,,nan,
92417,389253.0,,nan,
92417,389274.0,97090.0,only 1 day would do..,316349.0
92417,389274.0,97083.0,how many days history is requiered to compare,301641.0
92417,389444.0,,nan,
92417,389453.0,,nan,
92417,389607.0,,nan,
92417,389917.0,,nan,
92417,389692.0,,nan,
92417,390133.0,,nan,
92464,389470.0,,nan,
92464,389507.0,97154.0,exactly.. that's why thought of clarifying it with someone.. thanks!,316349.0
92464,389565.0,,nan,
92464,390077.0,,nan,
92464,390789.0,,nan,
92476,389514.0,,nan,
92476,389521.0,,nan,
92476,389544.0,,nan,
92476,389558.0,,nan,
92476,391954.0,,nan,
93558,394297.0,,nan,
93558,394046.0,,nan,
93558,394303.0,,nan,
92431,389314.0,97117.0,not working,300723.0
92431,389322.0,97109.0,"I donot have program data in c drive, rather have program files",300723.0
92431,389322.0,97118.0,in program files i have created folder Uploads and tried but didnt work out.,300723.0
92431,389322.0,97127.0,"Yes, true... Got. It thanks",300723.0
92431,389322.0,97120.0,i think program data is hidden in my machine. Can you change the setting to view the hidden files and check if program data is visible,318084.0
92431,389452.0,,nan,
92515,389708.0,,nan,
92515,389765.0,,nan,
92515,389719.0,,nan,
92515,389778.0,,nan,
92186,388339.0,96837.0,"if you don't make them null, you need to have another column such as RANK so that the signal generation can be ignored. isn't it?",318329.0
92186,388421.0,,nan,
92187,388297.0,,nan,
92187,388356.0,,nan,
92187,388374.0,,nan,
92187,388418.0,96850.0,"agreed, but this is the definition of UDF. My question is pretty specific to Q3 and Q4 mentioned in SQL assignment. We anyways have to calculate signal in bajaj2 table, so if you put a date in very simple select query then you would get the signal. If the purpose of the question is just to make sure that we know how to create a function, then it is okay. Moreover, this function also needs to be used using the select query, so I am baffled how this would help users. Simple query would look like :: Select signal from baja2 where date= Function query would look like : select signal_function(date) from bajaj2 where date=",304814.0
92187,388654.0,96976.0,"agreed, but this is the definition of UDF. My question is pretty specific to Q3 and Q4 mentioned in SQL assignment. We anyways have to calculate signal in bajaj2 table, so if you put a date in very simple select query then you would get the signal. If the purpose of the question is just to make sure that we know how to create a function, then it is okay. Moreover, this function also needs to be used using the select query, so I am baffled how this would help users. Simple query would look like :: Select signal from baja2 where date= Function query would look like : select signal_function(date) from bajaj2 where date=",304814.0
92187,388654.0,97179.0,"The question is not, if it is going to help end user or not. It is 1. To check if you know how to do this. 2. Here UDF is simple and straight (may be), but in future UDF's are complex. We write it so that user need not undergo writing or understanding the complex UDF requirement.And it is easy to use.",301555.0
92187,388654.0,97191.0,"yeah I totally get the usage of functions i.e. complex logic should be encrypted for users. For this particular assignment usage is not that great, but if assignment is to test the knowledge of UDF then I am good. ( However for future batches I would recommend an introduction of completely new UDF so as to remove the confusion). I hope you agree",304814.0
92187,388654.0,97927.0,I feel the stored procedure would be more appropriate here. Can we make that instead of UDF?,304319.0
92187,388654.0,97923.0,Even I am not able to get the use of UDF in this case. How do you access the signal inside the UDF? You need to pass it in the function and then simply return it????,304319.0
92187,391677.0,98123.0,faced issue some time back. close open of the mysql work bench and try.,312019.0
92978,391558.0,97834.0,"Hi Hemant, I checked the datatype and it is datetime for Bajaj2. In my UDF, I'm using datetime. How is the entry made for using the UDF for datetime? The date is entered as yyyy-mm-dd right? As in like 2015-06-06? This should work right?",316416.0
92978,391558.0,97842.0,"Thanks Hemant. Passing it as a string worked. I'm not sure I understand why just yet, but it worked :P",316416.0
92978,391558.0,97835.0,you'll anyway be calling it out in select clause as yyyy-mm-dd so in UDF you actually should pass only a single variable like a or b or c etc..,316349.0
92978,391730.0,97840.0,Thanks a lot. That worked :),316416.0
92978,391745.0,,nan,
92767,390670.0,97504.0,I did using MySQL workbench import tool,318814.0
92767,390670.0,97550.0,"Yes, you can use that its just Set your datatype for ""Deliverable Quantity"" and ""% Deli. Qty to Traded Qty"" as ""Text"" while importing the data .. and you will get 889 rows",305129.0
92767,390678.0,,nan,
92767,390701.0,,nan,
92767,390743.0,97641.0,Hi hemanth ! Can you please explain why do we have to change the data type of deliverable Quantity and %deli.Qty to traded Qty as 'Text' ?,319860.0
92767,390743.0,97663.0,MySQL is not accepting NULL as integer value and hence we've to import it as TEXT,316349.0
92767,390743.0,97996.0,Thanks Hemant for the resolution but I have concern here if we change data type now to text doesn't it impact on any of the results? I just want to confirm our results shouldn't be changed because of changing data types.,312623.0
92767,390743.0,98004.0,Thanks Hemant,318814.0
92767,390743.0,98002.0,"i don't think so.. and anyways even if it does, you can always convert the date format from string using str_to_date code..",316349.0
92767,392191.0,,nan,
92767,392126.0,,nan,
92771,390665.0,,nan,
92771,390673.0,,nan,
92771,390699.0,97527.0,use case statement to do this.,317689.0
92771,390699.0,97524.0,I too thought that but somehow I am not able to write a code to start with ROW_NUM() > 19,320197.0
92771,390907.0,,nan,
92771,391062.0,,nan,
92771,391733.0,,nan,
93205,392305.0,98035.0,got it !,301648.0
93252,392481.0,,nan,
93252,392479.0,,nan,
93252,392480.0,98184.0,"that was a typo, thank you",312259.0
93252,392531.0,,nan,
93255,392499.0,,nan,
93255,392500.0,98249.0,Somehow I could not achieve this. A little-elaborated answer is welcomed.,310522.0
93255,392500.0,98250.0,Also I am able to join two tables but could not succeed on 6 tables.,310522.0
93255,392500.0,98267.0,Please refer to the link below and you can join any number of tables you want. https://stackoverflow.com/questions/4361774/mysql-update-multiple-tables-with-one-query,304281.0
93255,392500.0,98266.0,are you trying to update the close price of 6 stocks into master table?,307710.0
92773,390683.0,,nan,
92773,390698.0,,nan,
92773,391061.0,,nan,
93256,392487.0,,nan,
93256,392493.0,,nan,
90737,378167.0,,nan,
90737,378170.0,,nan,
90737,378187.0,,nan,
90737,379501.0,,nan,
93259,394008.0,,nan,
93259,392508.0,,nan,
93259,392501.0,,nan,
93259,392514.0,,nan,
93259,392510.0,,nan,
93259,392509.0,98192.0,"hey i think you are answering about stored procedure,",319770.0
93259,392509.0,98195.0,Ofcourse there is a return clause. Otherwise how will you return a value.,304319.0
93259,392509.0,98196.0,Please clarify. There is no return statement in the Stored Procedures.,304319.0
93259,392509.0,98200.0,"Mam, I am trying to say that the query you have written does not have a return clause. It specifies 'returns' in the UDF definition but what value will actually be returned is written in the 'return' clause.",304281.0
93259,392509.0,98202.0,It is there. Pls look carefully - return (sig);,304319.0
93259,392509.0,98201.0,"ok, thanks for clarifying, the statement meant in UDF's you cannot use return .",319770.0
93259,392505.0,98208.0,Thnx. Finally the error has gone away. But when I call the UDF as : select UDF(DATE) from bajaj2; it is returning a whole list of signals and not a single signal.,304319.0
93259,392505.0,98217.0,"just call select UDF(date) , inside the actual implementation you will have a select which compares the date and return a single value",319770.0
93259,392505.0,98342.0,Thnx Sujeet. It is working now.,304319.0
93259,392527.0,,nan,
93259,392582.0,98227.0,semi colon at the end of ),312019.0
93259,392636.0,98328.0,thnx so much. I did this way. but it is displaying same signal multiple times.,304319.0
93259,392636.0,98653.0,"hope you got the solution for this! The approach is correct, can be improved a little further though!",311803.0
93259,393528.0,,nan,
91377,382053.0,,nan,
91377,382054.0,,nan,
91377,382037.0,,nan,
93262,392502.0,,nan,
93262,392506.0,,nan,
93262,392522.0,,nan,
93211,392321.0,,nan,
93264,392534.0,98209.0,No it has to be a sql file and a pdf file. These two should be zipped.,304281.0
93264,392534.0,98207.0,the files inside zip file should be of any name as well? it need not to be .SQL extension..right?,307710.0
93264,392520.0,,nan,
91383,382175.0,,nan,
91383,382196.0,,nan,
91383,382346.0,,nan,
91331,381702.0,,nan,
93210,392315.0,,nan,
93210,392318.0,,nan,
93610,394411.0,,nan,
93610,394499.0,,nan,
93610,394492.0,,nan,
93209,392313.0,,nan,
93209,392328.0,,nan,
91882,385079.0,,nan,
91882,385440.0,,nan,
91882,385640.0,,nan,
91882,386128.0,96591.0,"Better not to . If you analyze the whole problem, it has nothing to do with the null value columns. Just by deleting a row, nothing might change much as we are asked for observations and it is very unlikely that observations deviate just because of one row.",318007.0
91882,386128.0,96628.0,"Nope, as we are analysing only date and close price columns , it's better to change the null value to zero or change the format for loading CSV data.",305652.0
91882,387741.0,,nan,
93267,392546.0,,nan,
93267,392554.0,98224.0,"No, you are not missing any thing. I was just going through various options, had some doubt and got it clarified. Thanks.",318479.0
93274,392590.0,,nan,
93274,392586.0,,nan,
93274,392583.0,,nan,
93275,392613.0,,nan,
93275,392588.0,,nan,
92223,388658.0,96901.0,can you give some link or syntax about case statements,300687.0
92223,388658.0,96900.0,what is a CASE statement? dont think this was covered in the course?,310509.0
92223,388590.0,,nan,
92223,388472.0,96876.0,"I used the below command as suggested - update tcs set date = str_to_date(date,'%d-%M-%Y'); but i am getting error code 1411 as above. so not solving..",310509.0
92223,388472.0,97869.0,Hi Chetan Is your issue solved. I am facing the same issue,318822.0
91043,380059.0,,nan,
91043,380128.0,,nan,
91043,389421.0,,nan,
90634,377944.0,,nan,
90634,378934.0,95526.0,Resolved this,310974.0
90634,380652.0,,nan,
90634,394171.0,,nan,
92371,389108.0,,nan,
92371,389158.0,97042.0,"after importing ,use str_to_date() for conversion and finally change data type of date back to date",318005.0
92986,391596.0,,nan,
92986,391611.0,,nan,
92986,391628.0,,nan,
92986,391626.0,,nan,
92986,391654.0,,nan,
92986,391750.0,,nan,
92986,392574.0,,nan,
92373,389116.0,97046.0,My question was about identifying crossing but not about calculating averages.,318007.0
92373,389116.0,97058.0,Moreover question 2 is where you calculate averages right. Question 3 is about populating signal in the table. How do I populate signal by identifying exactly 20dma and 50MA cross.,318007.0
92373,389116.0,97062.0,I already answered your question o your comment on different post. If you are not able to find..i am very sorry sir,317811.0
92373,389116.0,97093.0,Just digged in for that. Lag function. Will read more about that. Thanks.,318007.0
92373,389157.0,97051.0,"whenever their is a change from low to high and high to low ,it is buy/sell signal,and if it remains at same level it is not",318005.0
92373,389157.0,97047.0,My question was about identifying crossing but not just about populating tables. .,318007.0
92373,389167.0,97199.0,"When did video talk about lag function. I do not remember however. Completed anyways, tnx",318007.0
92373,390128.0,,nan,
91447,382469.0,,nan,
91447,382468.0,,nan,
91447,382410.0,96067.0,after using semicolon again it shows same error,310419.0
91447,382410.0,96071.0,The error in select statement is misleading though.... Did you use ADD COLUMN? Alter table abc add column xyz varchar(20); Run the alter table command separately and then run the select statement separately,318084.0
91447,382410.0,96090.0,Done separately same error get,310419.0
91447,382410.0,96094.0,"If the above step does not help in fixing the issue, please check the below link and see if any of the solutions suggested is relevant for you. https://stackoverflow.com/questions/23515347/how-can-i-fix-mysql-error-1064",318084.0
91447,382410.0,96092.0,"Did you shutdown mysql and then restarted it and then executed the statements in a new SQL script window separately? If not, can you please try that as I dont see any syntax issues.",318084.0
91447,382496.0,96091.0,Done again get the same error,310419.0
91447,382544.0,,nan,
91447,383321.0,,nan,
91447,383516.0,,nan,
91449,382452.0,,nan,
91449,382446.0,,nan,
91449,382440.0,,nan,
91453,382461.0,,nan,
91453,382593.0,,nan,
91453,382755.0,,nan,
91453,382582.0,,nan,
92791,390780.0,,nan,
92791,391006.0,,nan,
92791,390784.0,,nan,
92353,388885.0,96983.0,"In the example shared in the link, it is only Over and Below, nothing about holding :(",313676.0
92353,388885.0,96982.0,"Hi Maya, the example is really helpful, though I am finding it difficulty in how to make all the tradings as Hold and only one as 'Sell' or 'Buy', depending on the first Cross over. :( I hope you understand the problem I am facing.",313676.0
92353,388925.0,,nan,
92353,388962.0,,nan,
92353,389617.0,,nan,
92353,389115.0,,nan,
92353,390123.0,,nan,
92353,390121.0,97394.0,Thanks a lot sir.. it was quite helpful.,313676.0
91071,380587.0,95746.0,I figured it out. thanks Ram.,318329.0
91071,380642.0,,nan,
91071,380644.0,,nan,
91071,380737.0,,nan,
91071,380753.0,,nan,
91071,381079.0,,nan,
91071,388663.0,,nan,
92465,389475.0,,nan,
92465,389481.0,,nan,
92465,389489.0,,nan,
92987,391650.0,,nan,
92987,391631.0,,nan,
92987,393175.0,,nan,
91083,380390.0,95732.0,I want to select different columns from multiple table for that I create a table in which I select different columns of multiple table and after that I join those column like table1.date=table2.date in where clause,310419.0
91083,380647.0,,nan,
91083,380751.0,,nan,
93089,391912.0,98789.0,My question is not answered..TA please?,308437.0
93089,391982.0,,nan,
89007,368994.0,,nan,
89007,378226.0,,nan,
89007,388171.0,,nan,
92790,390783.0,97532.0,Thanks a lot for clarifying most of the open questions. What is the expected value of signal for the very first row? Should it be neutral or buy/sell depending on the crossover condition?,318084.0
93057,391811.0,97868.0,dont panic go step by step reading the whole thing wont help in analysis you cant go to a answer you need to create a path for it :),318017.0
93057,393203.0,,nan,
92395,389215.0,97376.0,Yes we need to change the datatype to string and then again convert it into datetime.,318448.0
92395,389177.0,,nan,
92395,389193.0,,nan,
91485,382929.0,96147.0,i am trying to covert but getting null only,317982.0
91485,383263.0,,nan,
91485,382712.0,96132.0,"I have tried using str_to_date n date format it is giving all null values, my date is in text format, i have tried to convert column to date as well but its not working as format is day-mm-yyyy..",317982.0
91485,382733.0,,nan,
91485,382818.0,,nan,
91485,382839.0,96142.0,"my datatype is text, its not getting convereted to date or string using str_to_date,cast,convert etc functions.Do i need to change datatype while importing from excl",317982.0
92679,390468.0,,nan,
92679,390476.0,,nan,
92679,390561.0,,nan,
92680,,,nan,
91923,385383.0,96498.0,Welcome. I am glad that It works for you.,317811.0
91923,385383.0,96497.0,Thank you So much. Got the result.,307494.0
91923,385467.0,96528.0,Thanks Darshna..,307494.0
91923,387214.0,,nan,
92682,390438.0,97407.0,i am trying to update the table using the case statement,300687.0
92682,390438.0,97406.0,mysql,300687.0
92682,390474.0,,nan,
92682,390487.0,,nan,
91492,382820.0,,nan,
91492,385448.0,,nan,
92077,,,nan,
91498,382814.0,,nan,
91498,385435.0,,nan,
91493,382806.0,,nan,
91493,382817.0,,nan,
91493,382832.0,96203.0,"Thanks nitesh, i have an idea about altering column name, but just to clarify one thing m if i use import wizard, would it corrupt the data some how or some fields will be left blank ? because i did for one table (bajaj) and all the data was fine, in addition i imported date as text, because when i changed the data type to datetime it was not importing ? is it correct ?",308782.0
91511,382900.0,,nan,
91001,379790.0,95631.0,Oh yeah! I overlooked the question.,311363.0
91001,379790.0,95630.0,"HI, This is for the mysql assignment, not python.",310511.0
91001,380107.0,95689.0,"I wanted to know why the error was coming. Other wise I found another indirect way of doing it. I used table import wizard to create a table bajaj_auto. Then used insert command to load my `bajaj auto` table using this table. That way, I dont have to use load command.",310511.0
91001,380618.0,,nan,
91001,381078.0,,nan,
91001,381144.0,96558.0,"I tried, but its not working for me! Please suggest",311004.0
91001,381673.0,,nan,
91001,391192.0,97661.0,You can use the STR_TO_DATE() function and give the correct format of input date.,310511.0
91001,391192.0,97797.0,"Yes, I got it...thanks",317990.0
90810,378450.0,,nan,
90810,378452.0,,nan,
90835,379611.0,,nan,
90835,388036.0,,nan,
90835,379395.0,,nan,
90835,380099.0,,nan,
90835,386524.0,,nan,
92685,,,nan,
92619,390246.0,,nan,
92619,390250.0,,nan,
92619,390255.0,,nan,
92619,390251.0,,nan,
92532,389817.0,,nan,
92532,389844.0,,nan,
92532,390324.0,,nan,
92999,391724.0,,nan,
92999,391976.0,97985.0,Matching .,301648.0
92999,391957.0,97942.0,Pls Ignore ! :) Gotcha Silly Mistake !,306735.0
92999,391679.0,97948.0,matched :),317990.0
92999,391679.0,98016.0,Yes matched all,312019.0
92999,391679.0,98097.0,great! 👍☺️,316349.0
92999,391679.0,98294.0,Perfect.!!,318756.0
93597,394388.0,,nan,
93597,394611.0,,nan,
93597,394797.0,98818.0,It’s not mentioned CLEARLY what the table names should be. It was just addressed by a TA in discussions.,312376.0
93597,395595.0,,nan,
92535,389815.0,,nan,
92535,389842.0,,nan,
92535,389995.0,,nan,
92535,390323.0,,nan,
91875,385221.0,,nan,
92539,389841.0,,nan,
92539,390318.0,,nan,
92540,389839.0,,nan,
92540,391087.0,,nan,
92683,,,nan,
90932,379191.0,95537.0,After restart again show same error while running query,310419.0
90932,390404.0,,nan,
90932,379266.0,,nan,
92688,390453.0,97422.0,check it manually..,316349.0
92688,390453.0,97420.0,Bajaj2 is created from bajaj1 which has all the dates...what I am wondering is how to check the date format to be used?,310509.0
92688,390469.0,,nan,
92688,390472.0,,nan,
90752,378191.0,95583.0,yes. LAG worked. however correlated sub-query should also work as per the information shared in the link posted by you.,311686.0
90752,378191.0,95588.0,yes. indeed.,311686.0
90752,378191.0,95585.0,"Great, but what a relief after solving this :) I'm sure you guys must've felt the same.",310974.0
90752,378191.0,95312.0,So do we ignore or we mark the signal for these rows as 'hold ' since values for short term and long term MAs for these initial rows will be of the same value?,310511.0
90752,378191.0,95313.0,"I am planning to populate 'hold' there. Requesting TAs to verify. however, am struggling to generate signal. not finding a way to do so in a single query. could you do that?",311686.0
90752,378191.0,95314.0,Conditional statement can be used to generate signal in sql query.,310511.0
90752,378191.0,95372.0,conditional statement with some logic to compare values of same row as well as tracking one value of above row. Right?,311686.0
90752,378191.0,95379.0,why do u want to compare rows with one above?,310511.0
90752,378191.0,95380.0,to check if earlier row is 'sell' or 'buy' or 'hold'. suppose if it is already 'buy' then even if 20MA is more than 50 MA we should set signal as 'hold'. this is my understanding. am I wrong somewhere?,311686.0
90752,378191.0,95381.0,my understanding is that the signal should be buy if 20MA > 50MA no matter what the previous signal was. the problem statement is kind of vague eho doesnt know this domain and is open for interpretations. Can you post this confusion as a separate question in the forum and ask for TA's confirmation?,310511.0
90752,378191.0,95425.0,"Hey. After reading the problem statement again, I think your understanding of the signal logic is right - compare values of same row as well as tracking one value of above row.",310511.0
90752,378191.0,95432.0,I am waiting for some TA to respond to your question. if this is the correct problem statement then it is difficult to do so in a single query.,311686.0
90752,378191.0,95434.0,"Hmmm...if no one replies by tomorrow aft, i will bring it to SM's notice.",310511.0
90752,378191.0,95435.0,right. m wondering where have the TAs gone. also what other students are doing on this.,311686.0
90752,378191.0,95436.0,i dont think many ppl have started working on this assignment. everyones busy with tomorrow's submission.,310511.0
90752,378191.0,95438.0,yes. it seems so.,311686.0
90752,378191.0,95570.0,great. can you throw some light on it. what should be the approach? struggling since last 2 days,311686.0
90752,378191.0,95561.0,"Yes, Chandan is absolutely right.",310974.0
90752,378191.0,95568.0,"Ram, could you figure out how to write code for this task? It is not possible in a single query I think.",311686.0
90752,378191.0,95569.0,"It is possible, since 3 hours i've been struggling and finally got it :)",310974.0
90752,378191.0,95572.0,Hint: You need to use correlated sub-query.,310974.0
90752,378191.0,95573.0,I am using sub-query. but didn't get the meaning of correlated sub-query. can you pls share some reference links on stackoverflow or similar portals?,311686.0
90752,378191.0,95574.0,I believe you can do this using join/sub query also but I used correlated sub-query. Reference => http://www.mysqltutorial.org/mysql-subquery/,310974.0
90752,378191.0,95575.0,ok thanks. will try. I understand we need to use conditional operators as well for this.,311686.0
90752,378191.0,95576.0,Yes we have to use conditional operators also but this is definitely not a simpleton task.,310974.0
90752,378191.0,95577.0,yes. I agree. it is a tough task.,311686.0
90752,378191.0,95578.0,Sub query isnt required. Theres a window function called lag. Check out.,310511.0
90752,378191.0,95579.0,yes. could solve it using LAG. thanks to both of you. :),311686.0
90752,378191.0,95582.0,"So, you guys used LAG() and conditional statements?",310974.0
90752,378191.0,95584.0,yes.,310511.0
90752,378191.0,95599.0,now next doubt. what exactly is needed in executive summary report? the analysis on Bajaj stock? as we defined a function for it. or overall analysis seeing the Master table? or even more generic analysis?,311686.0
90752,378191.0,95613.0,Overall analysis on the assignment I guess.,310511.0
90752,378191.0,95829.0,"The OVER() function when used with AVG for MA, I am not getting NULL's for initial rows as well. Was this the same for you guys?",318329.0
90752,379496.0,95587.0,I know CROSS is the terminology over there in the financial domain but it makes it easier for ppl solving this if INTERSECT is used. Just a thought.,310974.0
90752,379496.0,95592.0,"I do not get when you say 'CAN NOT calculate moving average'. For eg, if you calculate 5Day MA, the first 5 rows will have a constant value and then it will start to change. And it is the difference of short term MA and long term MA that will be zero. Correct me if I am wrong.",310511.0
90752,379496.0,95712.0,"To calculate the 5 Day Moving Average, you need the data for 5 days and that you do not have for the first four days, hence calculating 5 Day Moving average serves no purpose. We can not assign it as zero cause that would be wrong and a misrepresentaation.",304281.0
90752,379496.0,95594.0,"I have the same understanding as you Rajarshi, let's see what they say to this. I even tried finding a way to populate NULLs where 20 MAS = 50 MAS same as the one shown in the problem introduction but couldn't find any.",310974.0
90752,379496.0,95731.0,"I understand it serves no purpose. But when you apply window function to calculate 5 Day MA, the first 4 days do hold some value. They wont be null or 0.",310511.0
90754,378189.0,95304.0,"The date format is weird in the csv file. While importing, if change the datatype it throws error. Even if I try to alter it afterwards, it cannot recognize the format.",310511.0
90754,378189.0,95306.0,"Format is DD-MONTH-YYYY (or D-MONTH-YYYY) only. May be your Excel is opening it in some other default setting format. Try opening one of those csv files in Notepad, you will see the exact format. It is like 31-July-2018.",311686.0
90754,378189.0,95538.0,i figured the conversion is not needed for this assignment.,310511.0
90110,375239.0,,nan,
90110,375253.0,,nan,
90110,375274.0,,nan,
90110,375385.0,,nan,
91997,385962.0,96574.0,"Clear, but you've made me to think more to get to the answer now! thanks Premnath!",316349.0
91997,386702.0,,nan,
91997,386799.0,,nan,
90991,379755.0,95629.0,Am also thinking on similar lines. Hope some TA verifies this.,311686.0
90991,380103.0,,nan,
90991,380129.0,,nan,
90991,380131.0,,nan,
90991,393655.0,,nan,
92405,389169.0,97067.0,thanks for the explanation,308442.0
92405,389261.0,,nan,
92405,389162.0,,nan,
92405,389188.0,97066.0,thanks a lot for the answer,308442.0
97365,416759.0,,nan,
97365,416831.0,,nan,
89305,370319.0,93627.0,Works for me! Thanks!,318381.0
89305,390760.0,,nan,
91560,383235.0,,nan,
91560,383320.0,,nan,
93087,391903.0,98785.0,Yes I have mentioned column name inside braces... It's not working...!! While typing here I missed that's all...,308437.0
93087,391903.0,98787.0,If required I ll send snapshot but don't know if I can attached in dis forum,308437.0
93087,391903.0,98786.0,TA can u pls answer this...bcoz of this I lost lot of time in sql assignment stock analysis...,308437.0
91005,380064.0,,nan,
91005,380192.0,,nan,
92543,389861.0,,nan,
92543,390269.0,,nan,
92543,391098.0,,nan,
91935,385629.0,,nan,
91935,385487.0,96505.0,"but in the example of MA5 and MA10, it was given null values there. so, in assignment i thought for MA20 first 19 rows , for MA50 first 49 rows will be null values. so its fine even average calculated for first 19 and 49 rows",300733.0
91935,385635.0,,nan,
91935,386426.0,96642.0,CASE statement is like giving conditions in SELECT clause . It is normally used when you have to store/ display values based on certain criteria.,308439.0
92547,389875.0,97769.0,"Thank you, it worked!",315797.0
92549,389888.0,,nan,
92549,390071.0,,nan,
92549,390294.0,,nan,
92549,390570.0,,nan,
92551,389898.0,,nan,
92551,389908.0,,nan,
92551,390044.0,,nan,
92551,390290.0,97381.0,When i try to apply the LAG function on one of the condition my Select query shows an error not sure why.. CASE WHEN ((`20 Day MA` `50 Day MA` )) THEN 'Buy' Any inputs pls.. ?,300727.0
92551,390842.0,,nan,
92551,390889.0,97567.0,in update case is not allowing. It says expecting set.,312019.0
92551,390908.0,,nan,
92551,389886.0,97255.0,"select if(condition1 and LAG() over()), buy, if(condition2 and LAG() over()), sell, hold); something of this sort... :) think about the appropriate conditions..",316349.0
92551,389886.0,97254.0,"If Possible can you help with an example syntax on how to frame the query ? Just the body how it looks like ... Hope, i am not asking you the exact query :)",300727.0
91600,383518.0,,nan,
91600,383531.0,,nan,
91600,383927.0,,nan,
91600,383979.0,,nan,
92550,389877.0,,nan,
92550,389909.0,,nan,
92550,390227.0,97627.0,this did not worked on my sql installation. I went with importing tables instead.,300698.0
92550,391271.0,,nan,
91615,383641.0,,nan,
91615,383983.0,98315.0,Will I be losing marks if I don't round off ? Please Advise.,306735.0
91945,385607.0,96634.0,"yes. you are right. every time there is a cross, you will have buy or sell.",311686.0
91945,385607.0,96612.0,so there can be multiple buy and hold? as for eg if 20MA>50MA buy and if that continues it will be hold.. now if it cross again to the downside its signal for sell... so again it can again cross.. so it will be buy signal again likewise.. correct me if its wrong?,305129.0
91945,385661.0,,nan,
91945,385631.0,,nan,
92193,388292.0,,nan,
92193,388371.0,,nan,
92193,388404.0,,nan,
92193,388517.0,,nan,
92193,388623.0,,nan,
92100,387557.0,,nan,
92100,387907.0,96782.0,"yes , correct you need to create a table structure first then use load data infile command . Below are the links for the threads in discussion forum for importing data task: https://learn.upgrad.com/v/course/208/question/92045 https://learn.upgrad.com/v/course/208/question/91973",305652.0
92100,387907.0,96777.0,"ok. so first, I have to create a table tn load data into it.. what is the way of using command line .?",318802.0
92100,387242.0,,nan,
92100,389160.0,,nan,
91949,385667.0,,nan,
91949,385630.0,96592.0,"Ok, so the decision computed today (in this assignment) is to be assumed as suggestion for tomorrow.",318007.0
92765,390643.0,,nan,
92765,390746.0,,nan,
93055,391810.0,,nan,
93055,391816.0,,nan,
93055,391837.0,,nan,
93055,391865.0,97922.0,the udf is selecting the signal for a particular date right ? if the date itself is absent in the table then how will u return a signal?,318756.0
93055,391865.0,97926.0,"For your answer On a holiday, you do not do any transactions. Means that you get a HOLD signal for a Holiday or weekend. This is the response from one of the TA for similar kind of question. Below is the link: https://learn.upgrad.com/v/course/208/question/92112 And for date outside of the dataset in my view you can return NULL.",317991.0
93055,391865.0,97924.0,I am sorry I wrongly posted other questions solution here. Apologies for this.,317991.0
93055,391938.0,,nan,
93055,392543.0,98292.0,"Thanks mate, you really solved this confusion",318756.0
92085,387054.0,96697.0,"yup, in that case I am getting 888 rows, 1 is missing.",314547.0
92085,387054.0,96703.0,"Hi, What will be the exact row counts for all table ? 888 or 889?",320195.0
92085,387054.0,96701.0,yeah it skip null rows,311004.0
92085,387054.0,96707.0,What is the data type of 'Date' column after you import the data from csv file? Is it coming as 'text' or 'date'?,308442.0
92085,387054.0,96719.0,"it will be text, I think, just convert it to varchar, if needed.",318802.0
92085,387054.0,97008.0,it's showing text,318398.0
92085,386896.0,96670.0,"LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Bajaj Auto.csv' INTO TABLE bajaj FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' (@date1, `Open Price`, `High Price`, `Low Price`, `WAP`, `No.of Shares`, `No.of Trades`, `Total Turnover (Rs.)`, `Deliverable Quantity`, `% Deli. Qty to Traded Qty`, `Spread High-Low`, `Spread Close-Open`) IGNORE 1 ROWS (@date1) SET date = str_to_date(@date1, '%d-%M-%Y'); this way?",314547.0
92085,386896.0,96668.0,Thanks Ankit. this works A different error now: Error Code: 1262. Row 1 was truncated; it contained more data than there were input columns,314547.0
92085,386896.0,96669.0,"(@date, column2, column3,col3, col4..uptolastcolumn of table) -- read one of the field to variable but mention all columns of table inside the parentesis SET date_time_column Yes, you need to mention all the columns of the table in () inside this",317811.0
92085,386896.0,96673.0,Again you are mentioning wrong date format. See my first answer to your question which tell what is the date format that you need to mention,317811.0
92085,386896.0,96776.0,"Compare the comment 2 above yours by varun in this thread with your code, you missed a line",317811.0
92085,386896.0,96688.0,"Its giving me and error 1292 Incorrect date value this is my code LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Bajaj Auto.csv' INTO TABLE `Bajaj Auto 1` FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS SET Date = STR_TO_DATE(@date1, '%e-%M-%Y')",318451.0
92085,386889.0,96666.0,Got the same error again. I am using the Datatype - date,314547.0
92085,386889.0,96671.0,There are two options. First declare the type as text and import second follow the below to set date function while importing as per below link http://www.mysqltutorial.org/import-csv-file-mysql-table/,307843.0
92085,386945.0,,nan,
92085,387003.0,,nan,
92085,387490.0,96878.0,"When I am importing the table using Table data import wizard, it is only returning 888 rows. I have tried multiple times but still the same output",319302.0
92085,387735.0,96849.0,but if we do this method then one row is getting skipped,318455.0
92085,387804.0,96778.0,not able to update it. getting error. syntax issue may b.,318802.0
92085,387804.0,96781.0,"update tablename set date=str_to_date(date,'%d-%M-%Y');",318005.0
92085,387804.0,96791.0,"UPDATE `assignment`.`bajaj auto` SET `bajaj auto`.`Date` = str_to_date( `bajaj auto`.`Date`, '%d-%M-%Y' ); This is working for me",311861.0
92085,387804.0,96799.0,it is nice,318005.0
92085,387804.0,96867.0,"After converting back to date, it is also giving error.",318497.0
92085,387804.0,96855.0,"i tried both above, but still its not working.",310509.0
92085,388502.0,97001.0,"No, convert it to date by using update tcs set Date = date(str_to_date(date,'%d-%M-%Y')) and you can use it as primary key.",318802.0
92085,388665.0,,nan,
94034,396282.0,,nan,
94034,396260.0,99192.0,companies['permalink'] = companies['permalink'].str.lower()),317811.0
94034,396260.0,99191.0,i changed the code to below companies_lowercase['permalink'] = companies['permalink'].apply(lambda x: x.astype(str).str.lower()) it isnt working,308495.0
94034,396260.0,99193.0,thanku ankit it worked,308495.0
91650,383713.0,,nan,
91650,383715.0,,nan,
91650,383775.0,96483.0,"My bad. You are right. It should have been first row instead of first column. Unfortunately, there is no way to modify the answer now.",313826.0
91650,383775.0,96482.0,"The first row is header, not the column. It is dangerous if someone takes your comment literally as it chops off the date column.",318007.0
91650,383935.0,,nan,
91650,385335.0,,nan,
91958,385677.0,96555.0,"after executing query, it is running for around 30 sec, then error is occurring. what to do?",300733.0
91958,385677.0,96621.0,"Looks, some issue in your server connectivity. Check the server running status and also check for error logs.",306735.0
91958,385734.0,,nan,
91958,387528.0,,nan,
91958,389763.0,,nan,
91958,391272.0,,nan,
92106,387176.0,,nan,
92106,387143.0,96693.0,thanku,318005.0
92106,387143.0,96698.0,"but if we plot graph with time on one axis and ma for both 20 ,50 on other ,their should be a crossover if any price lowers or rises from previous value",318005.0
92106,387143.0,96696.0,"if on the 50th day 20 ma is greater than 50 day ma,can be generate signal",318005.0
92106,387158.0,,nan,
92106,387234.0,96706.0,"yes, for crossing the graphs of both ma should cross ?",318005.0
92106,387234.0,96710.0,"if the graph are constantly disjoint ,it cant generate signal on basis of cross over ,only for the first day it can give signal?",318005.0
92106,387234.0,96713.0,"ok,thanku",318005.0
92106,387234.0,96712.0,"Only when 20DayMA crosses 50DayMA, a BUY or SELL signal is raised. For all other days, we have to give a 'HOLD' signal.",313826.0
92106,387488.0,97035.0,"Can you please let me know, what should be the signal for the first 50days as MA50 can be caluclated only from the 50th row..!!",311472.0
92106,387488.0,97037.0,for any date if u cant obtain but/sell it will be automatically hold.u can achieve it by making cases in query,318005.0
92106,387456.0,,nan,
92498,389606.0,97283.0,Better to do the null handling using NVL and convert to 0 fir NULL.,315679.0
92498,389611.0,,nan,
92498,389773.0,,nan,
92498,390073.0,,nan,
92498,390333.0,,nan,
92498,390522.0,,nan,
92498,390955.0,,nan,
92499,389681.0,,nan,
92499,389610.0,97227.0,How do you use aliases in joins since columns in tables already have names?,316416.0
92499,389610.0,97245.0,in selext clause.. you can change the name of computation to age or anything..,316349.0
92499,389610.0,97295.0,Thank you,316416.0
92499,389769.0,,nan,
92499,389949.0,,nan,
92499,389989.0,,nan,
92499,390331.0,,nan,
94032,396261.0,99225.0,"even i am facing the same problem and i have written it as ""update bajaj1 set `Date` = str_to_date(`Date` ,'%d-%M-%Y'); "" but its showing an error telling you are using safe update mode and you tried to update a table without a WHERE",319846.0
94032,396272.0,,nan,
94032,396284.0,,nan,
94032,396315.0,,nan,
94032,396592.0,,nan,
94032,397192.0,,nan,
91676,383859.0,,nan,
91676,383919.0,96330.0,Your issue is not clear to me,318084.0
91676,384162.0,,nan,
91676,384186.0,,nan,
91676,385317.0,97625.0,User case statement,312259.0
91676,385317.0,97631.0,"Thanks Hemant, issue is already resolved",307494.0
91676,385317.0,97628.0,or else simply update the first 19 rows as null,316349.0
91692,385413.0,,nan,
91692,383932.0,96461.0,"Hi Nitesh, That's a great explanation .Could you please check this error while loading data from csv , I am facing below issue: Error Code: 1366. Incorrect integer value: '' for column 'Deliverable Quantity' at row 230 Any suggestions ?",305652.0
91692,383932.0,96496.0,"By checking in csv file, found that at mentioned row there is null value. so, need to handle that null value while importing",300733.0
91692,383932.0,96471.0,"Hi Nitesh , while loading data from CSV file to MySQL , I am facing below error Error Code: 1366. Incorrect integer value: ' ' for column 'Deliverable Quantity' at row 230 There is empty data in column 'Deliverable Quantity' at row 230 . Any suggestions , how to proceed with this issue ?",305652.0
91692,384110.0,96486.0,This statement is about the calculation of moving average and not about null values in the row to start with on Deliverable* columns.,318007.0
91692,384110.0,96520.0,"Deliverable* columns(null values ) is not considered anywhere in any calculations, so this implies the answer",301115.0
91692,384110.0,96594.0,The question is about a normal row null and your answer was about a moving average null. The columns here are different. Normal row null is about Deliverable* columns. Moving average is about Closing Price. It is good to give direct answers than formula answers to those who are already confused.,318007.0
91692,385050.0,,nan,
91681,383882.0,96359.0,yes,301555.0
91681,383867.0,,nan,
92098,387026.0,,nan,
92098,387541.0,,nan,
93085,391901.0,,nan,
93085,391911.0,97914.0,Thank you Vipul,307495.0
92099,387507.0,,nan,
91969,385857.0,96661.0,"no. of Columns are correct, i tried to insert null rows which causes this error.",311004.0
91969,385857.0,97330.0,"under load infile command you give all the columns like say from (@col1, @col2... tile @col13) set tableColum1 = @col1, tableColumn2 = @col4",319770.0
91969,388606.0,,nan,
91702,384026.0,,nan,
91702,384050.0,,nan,
91702,385255.0,,nan,
92112,387736.0,,nan,
92112,387426.0,96992.0,"I think we are unnecessarily over complicating as this would need exception handling which is not the intent of the assignment. In a proper solution, I would agree but I do not think NULL stuff is the intent of the assignment.",318007.0
92112,387367.0,,nan,
92112,387520.0,,nan,
92112,388014.0,,nan,
92112,388243.0,,nan,
92112,388535.0,,nan,
92112,388616.0,96988.0,"Hi Anshul, this scenario is not mentioned in the tasks given . If such is a case then it should be mentioned in exceptions. If the assignment is calculated for other use cases the it becomes difficult . I have already submitted the assignment and this miscommunication will decrease my marking.",305845.0
92112,388616.0,96911.0,"Since the Moving Averages are being calculated on the 'Close Price', as signal will not be generated till the end of business hours. This ,I assume, implies that the signal generated would be used to take decisions on the next working day only. In such a scenario, I thought that when a holiday is passed to the UDF it should either return the signal from the previous working day or a message (like 'HOLIDAY' or 'NO SIGNAL').",313826.0
92112,388703.0,,nan,
92112,388707.0,,nan,
92112,388972.0,,nan,
92112,392537.0,,nan,
91966,385756.0,,nan,
92119,387419.0,,nan,
92119,387537.0,96767.0,"I'm not able to import date column from LOAD data statement, its getting NULL value.",318429.0
92119,387537.0,96846.0,"i was able to load 889 rows by changing the data type, however, when i try to change the data type of Date again then it is giving me an error. If i change using alter table command, then it is giving error type 1292 -Incorrect date value '31-July-2018' for Column Date at Row1. IF i try to follow your method above by using update table, then it is also not working as its giving error 1064 - syntax error. So, not sure how to fix?",310509.0
92119,388618.0,,nan,
91971,388608.0,96883.0,Thanks,301643.0
91971,385886.0,,nan,
94072,396448.0,,nan,
94072,396450.0,,nan,
94072,396444.0,,nan,
94072,396500.0,,nan,
94072,397366.0,,nan,
91973,385943.0,,nan,
91973,386009.0,96694.0,"Thanks Pooja, how about the Date column?",314547.0
91973,386009.0,96690.0,"Hello Vanarun, ELBY , you may use varchar format for uploading initial data..",311004.0
91973,386009.0,96663.0,"Hi Upendra, how do we fix the issue them?",314547.0
91973,386009.0,96682.0,how do we fix this issue?i'm also facing same issue.,310598.0
91973,387149.0,,nan,
91973,387509.0,,nan,
91973,389455.0,,nan,
93062,391839.0,,nan,
92122,387362.0,,nan,
92122,387382.0,96715.0,Can you share the query?,318085.0
92122,387382.0,96714.0,"Tried it, not working.",300748.0
92122,387412.0,,nan,
92122,387396.0,,nan,
92122,389163.0,97044.0,"the same concept is for hour,minute,second hh:mm:ss (03:56:32)",318005.0
92122,389163.0,97077.0,"thanks for that, but it did not work out. I'm still getting the error",308962.0
92122,389163.0,97078.0,"(@Date_Index, Open_Price, High_Price, Low_Price, Close_Price, WAP, No_of_Shares, No_of_Trades, Total_Turnover, Deliverable_Quantity, Percentage_Deli_Qty_to_Traded_Qty, Spread_High_Low, Spread_Close_Open) SET Date_Index = STR_TO_DATE(@Date_Index, '%d-%M-%Y');",308962.0
92122,389163.0,97094.0,use %m,318005.0
92122,389163.0,97079.0,example dates are like 02-Jul-18 29-Jun-18,308962.0
92122,389163.0,97095.0,use %y,318005.0
93075,391861.0,,nan,
92492,389673.0,,nan,
92492,389596.0,,nan,
92492,390335.0,,nan,
92492,390953.0,,nan,
91736,384009.0,96342.0,"Hi Hemant, do i have to convert CSV Date field to DATE TYPE is it right ? SET Date=STR_TO_DATE(@Date,'%e-%b-%y') i am doing this but yet not working ?",303674.0
91736,384009.0,96344.0,"Nope, you're expected to do all the changes in the SQL :) you're on right path, just try to play more with 'Str_To_Date' function.",316349.0
91736,384009.0,97572.0,"Hi Hemant , I have one question for assignment part 3 .How can i frame a Query .I am struggling for past 5 days ? . select x,y, if((signal='XYZ' and lag(signal,1)over(order by date_time asc)='something '),'hold',NULL) from bajaj1; do i have to do somethinf like this ?",303674.0
91736,384009.0,97576.0,you're too close.. just think about the condition inside LAG..,316349.0
91736,384009.0,97585.0,bro i am getting error in select its showing syntax error,303674.0
91736,384009.0,97597.0,Great!,316349.0
91736,384009.0,97596.0,Thankyou so much bro finally i got it,303674.0
91736,384038.0,,nan,
91736,384203.0,96370.0,"Hi Kailash, Thakyou but i have one doubt we have to convert all the fields which we import from csv or only the date field ? SET Date=str_to_date(@Date,'%e-%b-%y'); i am still getting error for this ?",303674.0
91736,384105.0,,nan,
91983,385883.0,,nan,
91983,386331.0,,nan,
91983,386806.0,,nan,
91983,387151.0,,nan,
92144,387689.0,,nan,
91733,383978.0,96335.0,Can you please elobarate on Signal Calculation ? Based on my understanding for individual record MA 50 vs MV 20 . Signal is decided . i.e MV 50 > MV 20 : Buy MV 20 > MV 50 : sell please correct me if i am wrong,301108.0
91733,383978.0,96336.0,"The day when 20DMA > 50DMA, signal should be BUY, after that 20DMA will continue to be higher than 50DMA, but signal should be ""HOLD"" in that case. The day when 20DMA < 50DMA, signal shoule be SELL, after that 20DMA will continue to be lower than 50DMA, but the signal should be ""HOLD"". Hope it is clear now.",318084.0
91733,383978.0,96340.0,My Bad for re-iterating .Is it mean consider we table below after calculating MV 20 & MV 50 Date Closing Price MA20 MA50 ------------------------------------------ 1-2-2011 200 168 144 1-2-2012 215 192 191 1-2-2013 150 151 121 1-2-2014 250 181 221 Calculating the signal above table: Date ClosePrice Signal ------------------------------ 1-2-2011 200 168> 144 : Buy 1-2-2012 215 192 > 191 : Hold 1-2-2013 150 151 > 121 : Hold 1-2-2014 250 181 < 221 : sell please confirm ?,301108.0
91733,383978.0,96345.0,thanks,301108.0
91733,383978.0,96343.0,yes. Thats right,318084.0
91733,384329.0,,nan,
91733,384897.0,,nan,
91733,385354.0,,nan,
91733,387543.0,,nan,
91750,384101.0,,nan,
91750,384281.0,,nan,
91750,384901.0,,nan,
92146,387776.0,96769.0,"how to check after first value whether its increasing or decreasing, i am not able to get that",300733.0
92146,387776.0,96771.0,that's the real challenge :) play around with Frames concept with Lag function,316349.0
92146,387776.0,96813.0,Could you please explain it? I mean what does Cross means? I get that simply MA20 being greater than MA50 does not mean buy as question mentions that MA20 and MA50 should cross. What is cross here??,304814.0
92146,387658.0,,nan,
92146,387686.0,96818.0,"Its very simple... When will it cross.... Assume 20DMA is less than 50DMA on 22nd Oct. On 23rd Oct, 20DMA is more than 50DMA, then crossover happens. So on that day, the signal has to be BUY. After that 20DMA will continue to be above 50DMA and the signal should be hold. Again when the crossover happens in the opposite direction, it is a SELL. What I explained in my first post is the only way to interpret it. If you are still finding it difficult to visualize it. Calculate 20DMA and 50DMA in an excel sheet and plot a graph between Date, 20DMA and 50DMA. You'll be able to visualize it better. Hope this helps.",318084.0
92146,387686.0,96812.0,I don't think this is correct as in the question it is clearly mentioned that MA20 being simply greater than MA50 does not mean it is Buy. Both MA20 and MA50 should cross. Now even I am not sure what does this cross mean,304814.0
92146,388642.0,,nan,
92036,386561.0,,nan,
92036,386588.0,,nan,
92036,386601.0,,nan,
92214,388401.0,97125.0,i tried this but getting error 1290 or 1148. :(,318322.0
92214,388401.0,97128.0,Can you explain more about the error which you are getting.,317689.0
92214,388401.0,97143.0,For file priv option refer to other questions. https://stackoverflow.com/questions/32737478/how-should-i-tackle-secure-file-priv-in-mysql,317689.0
92214,388401.0,97138.0,"0 21 13:29:34 LOAD DATA INFILE 'C:/Users/HasvithaSai/Desktop/IIITB/mysql/project/copy/Assignment/Bajaj.csv' INTO TABLE bajaj FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS Error Code: 1290. The MySQL server is running with the --secure-file-priv option so it cannot execute this statement 0.016 sec 0 23 13:31:38 LOAD DATA LOCAL INFILE 'C:/Users/HasvithaSai/Desktop/IIITB/mysql/project/copy/Assignment/Bajaj.csv' INTO TABLE bajaj FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS Error Code: 1148. The used command is not allowed with this MySQL version 0.000 sec",318322.0
92214,388407.0,96880.0,"its not excluding ..see the show logs while importing,it says import failed for a row",301115.0
92214,388449.0,96881.0,check @Anshul Tomar comment if u would like only to use import wizard,301115.0
92214,388449.0,96856.0,Well it’s the 31st August 2017 row,318455.0
92214,388449.0,96885.0,Thanks Guys!! :),316349.0
92214,388441.0,96912.0,Thanks Anshul,318455.0
92214,388838.0,,nan,
92214,389048.0,,nan,
92214,389037.0,,nan,
92214,389675.0,,nan,
92210,388656.0,,nan,
92210,388369.0,,nan,
92210,388403.0,96844.0,but there is no difference b/w q3 and q4 correct? apart from testing our functions knowledge :-),304814.0
92210,388403.0,97160.0,Q3 is to create tables that contains the signals for all days for the different stocks. Q4 is a UDF to query for a particular date. There is a difference .,300717.0
92210,388415.0,,nan,
92210,388718.0,,nan,
91759,384172.0,96366.0,e.g. for bajaj I found only 24 occurrences where it was not 'HOLD'. Hope I am on the right track?,306250.0
91759,384172.0,96395.0,I got 23 for bajaj.,310511.0
91759,384172.0,96399.0,How about the very first line (50th line). It should be a SELL if 20 Day MA < 50 Day MA?,318084.0
91759,384172.0,96429.0,We should handle 50th record ....it is not a Sell case in my view,306250.0
91759,384172.0,96443.0,FYI...I have written code to ignore 50th record.,306250.0
91759,384172.0,96438.0,Can someone TA confirm this? Is it still ok to make assumptions and make a comment of it in the sql scripts that we attach?,318084.0
91759,384462.0,,nan,
92899,391331.0,97838.0,"well,no it's not a part of question..am trying to understand what if this scenario occurs when we try and implement our learning from this course in a real life scenario",306995.0
92899,391331.0,97940.0,"Oh OK. In that case, I would take a call about it while cleaning data.",318007.0
91593,383642.0,96298.0,"Are you sure you can calculate BUY, SELL and HOLD for the entire dataset using LAG function with window/frame logic..... Initially it can be a Golden cross, followed by Death cross and this will keep repeating. BUY/SELL has to be updated for Signal only when 20DMA crosses or goes below 50DMA. Only for that one day when this happens. Rest of the time, we have to update the signal as HOLD. Are you sure this can be achieved using LAG function with window/frame logic?",318084.0
91593,383642.0,96329.0,yes. it is possible. I used that only. you need conditional clause as well. i.e. CASE,311686.0
91593,383642.0,96517.0,Thanks Chandan,301115.0
91593,383642.0,96535.0,actually we should thank Rajarshi Palit for this. :),311686.0
91593,383761.0,96295.0,what is difference between crossing over and greater than(crosses the 50 Day MA and 20 Day MA is greater than 50 Day MA ) isnt it same thing,317982.0
91593,383761.0,96294.0,"If it is just greater than and less than, then it is easy. We have to derive three signals - BUY, SELL and HOLD. It is a BUY only when the 20 Day MA crosses the 50 Day MA and 20 Day MA is greater than 50 Day MA and SELL if the 20 Day MA goes below 50 Day MA and cross over happens. In all other cases, it is a HOLD. If the 20 Day MA is above or below for a sustained phase, then it is a HOLD. Hope it is clear now.",318084.0
91593,383761.0,96296.0,"Crossover is the day when 20 Day MA rises or goes below 50 Day MA, that is when we have to mark it as BUY or SELL. After the crossover, 20 Day MA could still be either over or below 50 Day MA. In that case, the signal should be updated as HOLD. Hope it is clear now.",318084.0
91593,384139.0,96357.0,I understand this logic.. my question was more on the technical implemention of that logic... whether stored procedure/udf is there only way or can we still achieve it through normal windowing/frames...,318084.0
91593,385258.0,,nan,
90585,388886.0,,nan,
90585,378185.0,,nan,
90585,378929.0,95467.0,I thought the same. let some TA verify your answer. :),311686.0
90585,378929.0,96775.0,"Hi, did you get reply?",301643.0
92149,387773.0,,nan,
92149,387680.0,,nan,
92149,387908.0,,nan,
92149,388641.0,,nan,
92149,389155.0,,nan,
91760,384611.0,96563.0,"Hi Tanay, is that issue resolved for u now? can u suggest me what to do even i am getting same error after following above things also",300733.0
91760,384169.0,,nan,
91760,384205.0,,nan,
91760,384459.0,,nan,
91760,384683.0,,nan,
91760,384786.0,,nan,
91771,384241.0,,nan,
91771,384327.0,,nan,
91771,384373.0,96814.0,In Step3 for all rows assign the BUY/SELL/HOLD based on the condition. In step4 Based on the input date returns the signal value from bajaj2 table,318846.0
91771,384456.0,,nan,
91771,391564.0,97817.0,"Bajaj2 table need to used, as whether to buy/sell/hold signal column is in Bajaj2. we need to take date as input and it should returns a signal for that particular day (Buy/Sell/Hold) for the Bajaj stock.",303673.0
92007,386126.0,,nan,
92007,386224.0,,nan,
92007,386838.0,,nan,
91780,384657.0,,nan,
91780,384893.0,,nan,
91780,385161.0,,nan,
91780,384448.0,96418.0,"Hi Rajarshi, I have tried float(10,4) also, but getting the same error.by table import wizard, I am getting 1 row less than that of rows in csv.",300733.0
91781,384317.0,,nan,
91781,384307.0,,nan,
91781,384449.0,,nan,
91781,384734.0,96442.0,Thanks a lot Nitesh for confirming! I completely understand now that all stocks have to processed. My only concern was about writing the same code block multiple times ....which doesn't enhance learning . Would be good to know if we can avoid reproducing the same code block multiple times....,306250.0
92339,388882.0,,nan,
92339,388833.0,,nan,
92339,388844.0,,nan,
92339,389197.0,97106.0,any further suggestion on this?,310509.0
92166,388148.0,,nan,
92166,387978.0,96786.0,full join will be good,318005.0
92166,387978.0,96785.0,yes date is same across all,318005.0
92166,388357.0,,nan,
92166,388510.0,,nan,
93080,391926.0,,nan,
94204,396911.0,,nan,
94204,396937.0,99346.0,i have already done,320687.0
94204,396947.0,99348.0,how to use.,320687.0
94204,396947.0,99424.0,"getting error Row import failed with error: (""Incorrect date value"")",320687.0
94204,397109.0,99419.0,"when loading the data through wizard getting row import fialed with error(""Incorrect date value"")",320687.0
91346,381842.0,,nan,
91346,394757.0,,nan,
91237,381360.0,,nan,
91237,381412.0,,nan,
91237,381529.0,,nan,
91237,381619.0,,nan,
91345,381844.0,95980.0,the order of data matters when you are calculating the moving averages. and the window function is applied on that.,310511.0
91345,381844.0,95989.0,Yes. so based on that ordering the MA values will be different. and that will lead to different signals.,310511.0
91345,381844.0,95987.0,In window function you can specify the order :),317689.0
91345,382132.0,,nan,
91345,381980.0,,nan,
91132,380714.0,,nan,
91132,380722.0,,nan,
91132,381076.0,,nan,
91122,380686.0,95757.0,So can we remove first few rows which are not sufficient to calculate moving avg,311952.0
91122,380686.0,95759.0,no. we need not to that. just write your normal code to calculate 20 and 50 day moving average. it will populate some values for initial 20 and 50 rows as well. let it be.,311686.0
91122,380686.0,95760.0,i thought we can remove those rows in new table,311952.0
91122,380686.0,95867.0,it will not give NULL. you are correct.,311686.0
91122,380686.0,95839.0,"For me, AVG(closeprice) over previous 20 or 50 days does not give NULL values for the first 20 or 50 rows. Am I doing something differently from you that you are getting NULL's?",318329.0
91130,381606.0,,nan,
91130,380735.0,,nan,
91130,380755.0,,nan,
91133,381152.0,97027.0,"We will be using SELECT clause along with STR_TO_DATE, with that we can't modify the entire table data with STR_TO_DATE function like we do with alter table.",312756.0
91133,380750.0,,nan,
91133,380728.0,95765.0,"hi Rajarshi, 1. if i change the data type as date while importing through wizard. It is failing as date in different format. 2.if i load data with date columns as test data type. Then alter is failing as date in following format 01-jul-2018",311952.0
91133,380728.0,95774.0,"1. create a table with columns from csv. date column as date type. 2. create 2nd table from import wizard with date type as text. 3. insert into table 1 by values from table 2. I know this is a crude method. But it works. Also for this assignment, you are not needed to change the date datatype.",310511.0
91133,380728.0,96169.0,"While getting the running avg, inside the window function, you do the text to date conversion for sorting. No need to actually convert the date column in the table.",310511.0
91133,380728.0,96149.0,"how do we get the running avg if we do not sort it, as sort is not working on text column.",317982.0
91133,381612.0,,nan,
91133,382101.0,,nan,
92167,388040.0,,nan,
92167,387989.0,96787.0,I restarted many times but still same issue.. even re-booted my system also.,318802.0
92167,388300.0,,nan,
92167,388514.0,,nan,
92167,388644.0,,nan,
92167,389098.0,,nan,
92167,389153.0,,nan,
92016,386145.0,,nan,
92016,386278.0,,nan,
91852,384938.0,,nan,
92285,388626.0,96894.0,"but those are numerical data, so import as char and then convert it to num?",318436.0
92285,388655.0,,nan,
92285,388637.0,,nan,
92285,388659.0,97032.0,"I did import with those fields with text, but not able to convert them to float.",318436.0
92285,388662.0,96939.0,"Better to change string , load it. fill empty to 0, convert back to int",312019.0
92285,388662.0,96934.0,"If we edit csv file and they execute the sql what we uploaded, may not run for them right .",312019.0
92285,388697.0,,nan,
92285,389046.0,,nan,
92285,389327.0,,nan,
92285,389533.0,,nan,
92285,389556.0,,nan,
92285,389823.0,,nan,
92283,388577.0,96895.0,I am stil facing the same error,300687.0
92283,388577.0,96893.0,still the same error,300687.0
92283,388577.0,96896.0,"use dash also.. copy the same code i gave above., copy after bajaj1",305129.0
92283,388577.0,96918.0,thanks it worked,300687.0
92283,388577.0,96919.0,Welcome :),305129.0
92283,388577.0,97082.0,"its asking for where clause in this, what to mention in this?",318723.0
92283,388684.0,,nan,
92283,388699.0,,nan,
92283,389372.0,,nan,
92283,389608.0,,nan,
92283,389678.0,,nan,
92283,389635.0,,nan,
92173,388139.0,96801.0,Yes I am using it in average then also its giving me error,301655.0
92173,388139.0,96835.0,can you check previous commands and the current command,318329.0
92173,388299.0,96831.0,"thanks for the suggestion, will try this out",301655.0
92173,388651.0,96904.0,"I've used it with AVG function, still it is giving me the same error avg(column name) over (order by column1) as 'Sample column'",301655.0
92173,388651.0,96993.0,I am getting an error in over keyword,301655.0
92173,389620.0,,nan,
91590,383427.0,,nan,
91590,383435.0,,nan,
91590,383530.0,,nan,
91590,389301.0,97126.0,Thankyou,312019.0
91590,389301.0,97123.0,"Hi Naga, 6 new tables have to be created with those 4 columns. These columns have to be populated with information from the bajaj table and MA values have to come from the close price of the Bajaj table. Hope this clarifies what needs to be done :)",316416.0
92328,,,nan,
92042,388289.0,,nan,
92042,386531.0,96687.0,yes.... the hold signal will change when 20 DMA < 50 DMA. In that case it becomes SELL and then back to hold till such time when the cycle turns again.. Hope this helps.,318084.0
92042,386531.0,96675.0,"Thanks for your reply. Just for my clarification, if we got signal as Buy once, for all the other records (20 DMA > 50 DMA) , the signal should be HOLD. Is my understanding correct?",307494.0
92042,386529.0,,nan,
92042,387919.0,,nan,
92042,387927.0,96994.0,Is it possible to determine HOLD and crossing (SELL/BUY) into bajaj2 table just by a normal insert (with select) query? Not able to think of a solution. Please suggest.,318007.0
92042,387927.0,97063.0,thanks Ankit.,307494.0
92042,387927.0,97024.0,"use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92052,386542.0,,nan,
92052,386646.0,,nan,
92052,386859.0,,nan,
92052,387393.0,,nan,
92052,387692.0,,nan,
92052,388686.0,,nan,
92052,389060.0,,nan,
92052,388955.0,,nan,
92052,389039.0,,nan,
92046,386551.0,,nan,
92046,386684.0,,nan,
92046,388869.0,,nan,
91324,,,nan,
91325,381642.0,,nan,
91325,381644.0,96407.0,What inference can we make ? To predict when to buy which stock?,317689.0
92045,386566.0,,nan,
92045,386719.0,96646.0,"tried it too ,after setting local infile =true ,it also do not worked",318005.0
92045,386719.0,96647.0,Had you removed the Priv Mode from preferences in EDIT tab,308439.0
92045,386719.0,96649.0,yes,318005.0
92045,386719.0,96660.0,"in edit ->preferences there is no priv mode ,,i had removed that setting from configuration file my.ini",318005.0
92045,386719.0,96681.0,Bravo!!!,308439.0
92045,386719.0,96676.0,"local i used but it gave error not for this mysql version , the issue is solved with restoring default secure prev path nd putting csv there",318005.0
92045,386640.0,96633.0,"i hv removes secure priv settings and also put my csv in ***/uploads,but i didnot solved",318005.0
92045,386640.0,96636.0,Please take restart of mysql services and try again.,311004.0
92045,386640.0,96638.0,"restarted server,system it dont work,",318005.0
92045,386640.0,96884.0,"When i am running query - SHOW VARIABLES LIKE ""secure_file_priv""; its showing NULL and not able to upload file. Its showing error - Error Code: 1290. The MySQL server is running with the --secure-file-priv option.",320103.0
92045,386640.0,96935.0,"Hi Pooja, I think I found the my.ini file but how do you disable the parameter there? Do you write a code and save it since it is in the notepad format?",316416.0
92045,386640.0,96937.0,remove secure-prev line,318005.0
92045,386640.0,96938.0,or put your csv in path given by secure-priv,318005.0
92045,386640.0,96941.0,just comment it by putting # and save it,311004.0
92045,386640.0,97064.0,I used LOCAL word in the command and now the error message is Error Code: 1148. The used command is not allowed with this MySQL version I am using MySQL v8.012.,300713.0
92045,386640.0,97061.0,"Even after commenting out the above entry in file and restarting the service , I am getting same error message. Any help is highly appreciated.",300713.0
92045,386883.0,96662.0,"i always write it on top ,not happening",318005.0
92045,387024.0,96679.0,Great... Good Luck!!!,308439.0
92045,387024.0,96930.0,"Hi Abhishek, did you find the Program Data file in C drive or did you use the 'Program File' as Program Data? Similarly did you find an Uloads folder or did you create it manually? Do let me know. Thanks!",316416.0
92045,387024.0,97575.0,Issue Resolved. Thanks..,318013.0
92045,387024.0,97553.0,"Hi Abhishek, I have put my CSV files in ""C:/ProgramData/MySQL/MySQL Server 8.0/Uploads"" and tried to load . But still it's throwing ""Error Code: 1290. The MySQL server is running with the --secure-file-priv option so it cannot execute this statement"" Can you please let me know if anything needs to be changed in my.ini",318013.0
92045,388113.0,96803.0,use data type of deliverable **** nd % de**** as char(30),318005.0
92045,388113.0,96805.0,alter the problem giving column to char(30) before importing,318005.0
92045,388113.0,96806.0,use char (30) for Date,318005.0
92045,388113.0,96807.0,"after importing update date column using update yourtable set Date=str_to_date(date,'%d-%M-%Y');",318005.0
92045,388113.0,96808.0,change date back to date type,318005.0
92045,388113.0,97005.0,working now. thanks,318802.0
92045,388113.0,96998.0,"below error getting while updating date- update tcs set Date = date(str_to_date(date,'%d-%M-%Y')) Error Code: 1175. You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -> SQL Editor and reconnect.",318802.0
92045,388619.0,97360.0,"in the file explorer, check the hidden items. As ProgramData folder is hidden. Then u will find the file in c:/ProgramData/MySQL/MySQLServer8.0/Uploads/",304319.0
92045,388704.0,96931.0,Hidden? How do you unhide it?,316416.0
92045,388704.0,96932.0,In Explorer go to view go to show /hide panel check ☑ the hidden items,319006.0
92045,389027.0,,nan,
92045,389020.0,97017.0,[23/10 12:17 am] abhishek: sudo fs_usage | grep my.cnf [23/10 12:19 am] abhishek: https://stackoverflow.com/questions/10757169/location-of-my-cnf-file-on-macos,318005.0
92045,389020.0,97015.0,"@arjun sharma I am also facing this issue, let me know if you are able to solve it.",318448.0
92045,389020.0,97016.0,find my.cnf,318005.0
92045,389020.0,97018.0,comment secure-priv option,318005.0
92045,389020.0,97019.0,or check if csv files r put in path provided by secure-priv option in the my.cnf,318005.0
92045,389020.0,97299.0,Did you get it to work? if yes please let me know how?,300734.0
92045,389020.0,97281.0,"I have the same issue on my Mac, I have tried all the possible solution, I have created my.cnf as well at /etc/my.cnf even I have tried to set the local_infile variable to on as well on MySQL Workbench but that also Didnt work. Not sure what to do. When I Import using MySQL Workbench it only get 888 rows whereas there are 889 rows.",300734.0
92045,389026.0,,nan,
92045,389032.0,97020.0,"After placing file in path ""C:\ProgramData\MySQL\MySQL Server 8.0\Uploads"" and running LOAD query - it is throwing error ""Error Code: 29. File 'C:\ProgramData\MySQL\MySQL Server 8.0\Uploads\BajajAuto.csv' not found (OS errno 2 - No such file or directory)""",320103.0
92045,389032.0,97021.0,change \ to / in path,318005.0
92045,389119.0,,nan,
92045,389686.0,,nan,
93081,391875.0,98580.0,We can using prepare and execute method.,302739.0
93084,391896.0,,nan,
93084,391913.0,,nan,
92059,387180.0,,nan,
92059,386662.0,96716.0,"The same error was encountered , below is the link for discussion (it has been resolved: https://learn.upgrad.com/v/course/208/question/92045/answer/387024",308439.0
92059,386662.0,96695.0,it created another error code 1148 - this command not allowed with this SQL version,310509.0
92059,386832.0,96700.0,"i cannot see the folder called Program data under C: although i can see Program Files. I saw all the sub folders except ""Uploads"" so created the folder and put the csv files there. However, i cannot still execute the query and it gives same error 1290",310509.0
92059,386832.0,97088.0,same issue here as mentioned by above two,300723.0
92059,386832.0,96926.0,I am encountering the same issue Chetan. I created the uploads file too.. Were you able to address it?,316416.0
92059,387454.0,97086.0,"Hi, in my laptop , MY SQL got loaded in program files. C:\Program Files\MySQL\MySQL Server 8.0 where as my sql is giving me output to store as below. 'secure_file_priv', 'C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\' how to manage..pl suggest im stuck",300723.0
92059,388072.0,,nan,
92059,386935.0,96985.0,yes..,317811.0
92059,386935.0,96984.0,"Hi Ankit, do you mean use / everywhere in the file path instead of \ ? Pls do let me know. I've done everything I'm supposed to in order to address this error but it still persists!",316416.0
92059,386935.0,96996.0,"After executing the above command, it is showing as null . How to set the path for the same?",318448.0
92059,386935.0,97006.0,"Ankit, I did this SHOW VARIABLES LIKE ""secure_file_priv""; Its saying 1 row returned, but nothing is showing in the window pane where output usually comes. Any suggestions?",318335.0
92059,386935.0,97085.0,"Hi, in my laptop , MY SQL got loaded in program files. C:\Program Files\MySQL\MySQL Server 8.0 where as my sql is giving me output to store as below. 'secure_file_priv', 'C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\' how to manage..pl suggest im stuck",300723.0
93086,391898.0,97917.0,Ya. mDate is the primary key. I have already inserted values into it with a similar insert code from another table. So the column is definitely not empty.,312608.0
93086,391898.0,97962.0,"but when you are executing insert Statement as mentioned in your question, it is trying to insert totally new rows in the table. the mDate values existing are old rows.",311686.0
93098,391960.0,97943.0,Bad practice in reputed universities. A lot of Professors do not like if we answer more than what is asked. Up to you and those who evaluate.,318007.0
93098,391924.0,,nan,
93098,391933.0,,nan,
93098,392286.0,98417.0,We should bajaj2 as we are trying to get the signal value as output with the input as date. The signal has been calculated in bajaj2 instead of bajaj1. So use bajaj2.,311745.0
97386,416832.0,,nan,
97386,416833.0,102842.0,thank you.,311046.0
97386,416833.0,102840.0,No need to specify table name and where condition. You can execute it by simple query as follows select SIGNAL_INFO('2015-01-05'),317991.0
97386,416830.0,,nan,
97386,416821.0,,nan,
91312,381617.0,,nan,
91312,381626.0,95974.0,Had to update my.ini file entry as suggested by Arihant below,306250.0
91312,381643.0,95936.0,Thanks Arihant...this option worked,306250.0
91312,381643.0,95949.0,Yes that is an issue. Some columns are having null values which is causing the issue.,317689.0
91312,381643.0,95937.0,But had to convert a few column to text to load the data. Now I am not able to update back,306250.0
91312,381550.0,95909.0,Already moved my file to the directory specified by secure-file-priv (C:\ProgramData\MySQL\MySQL Server 8.0\Uploads)...unfortunately it still gives the same error,306250.0
91312,381550.0,95910.0,Have you tried just giving the file name without the directory?,310511.0
91312,381550.0,95911.0,Yes....unfortunately nothing seems to be working,306250.0
91312,381550.0,95975.0,Had to update my.ini file entry as suggested by Arihant below,306250.0
91312,381918.0,,nan,
93099,391939.0,,nan,
93099,391962.0,,nan,
93099,391968.0,97975.0,How are accessing the signal inside the UDF? Are You calculating it again using bajaj1?,304319.0
93212,392325.0,98058.0,"The name of the columns are the same as provided in the csv. However, I was asking regarding the name of the table and wehther a table created using the 'Bajaj Auto.csv' file needs to have the table name as 'Bajaj Auto' or only 'bajaj' would do? Asking this question as I'm using a dynamic procedure to create the next table (like bajaj1) using the initial table (bajaj).",313826.0
93213,392338.0,,nan,
93213,392336.0,98050.0,"Then how do i fill the values under 20 Day MA with Null and 50 day MA with Null, can you please tell me",318822.0
93214,392344.0,98059.0,I was trying to making the first 19 rows and first 49 rows null,311745.0
93216,392354.0,98095.0,"No, you don't have to code it. You can load it manually using the import wizard also.",319721.0
93216,392354.0,98094.0,Do we need to add the query of importing into SQL in the consolidated SQL file?,305334.0
94312,397248.0,99559.0,Thanks I have imported the data .,320687.0
94312,397229.0,99426.0,where can i use,320687.0
93217,392359.0,,nan,
93217,392364.0,,nan,
93203,392314.0,98106.0,No still error,318814.0
93218,392363.0,98075.0,I used '%Houston%',320687.0
93218,392363.0,98082.0,"did you use this? select ssn from employee where address like ""%Houston%"";",319721.0
93218,392363.0,98088.0,i have already done the same,320687.0
93218,392363.0,98104.0,there?,320687.0
93218,392363.0,98098.0,"It is printing the ssn of address 3321 Castle,Spring ,TX",320687.0
93218,392374.0,,nan,
93218,392394.0,,nan,
92735,390528.0,,nan,
92735,390560.0,,nan,
92735,391067.0,,nan,
92735,390553.0,,nan,
92735,391882.0,,nan,
93105,391958.0,,nan,
93105,391969.0,,nan,
93105,391975.0,,nan,
93105,391986.0,,nan,
93105,392052.0,,nan,
93105,392182.0,,nan,
93105,392385.0,98479.0,"Yes, same operation for all the table .",314183.0
93105,392385.0,99218.0,Ho w to create stock tables with csv,320687.0
93105,392385.0,99464.0,First Import csv data into my sql workbench.,314183.0
93105,392385.0,99560.0,i have imported using wizard,320687.0
93105,392385.0,99768.0,"yes, that's correct.",314183.0
93105,393471.0,,nan,
92737,390545.0,,nan,
92737,390548.0,,nan,
92737,390550.0,,nan,
93106,391972.0,,nan,
93106,391974.0,,nan,
93389,393025.0,,nan,
93389,393155.0,98562.0,A single SQL file with all your queries and a pdf with your summary is required for submission in a zip file.,304389.0
93389,393155.0,98515.0,"hi,can i write my queries in multiple files? or i need to write it in a single file? Please suggest",310504.0
93330,392723.0,,nan,
93330,392738.0,,nan,
93114,392009.0,,nan,
93114,392051.0,,nan,
93114,392150.0,,nan,
92938,391320.0,,nan,
92938,391324.0,,nan,
92938,392472.0,,nan,
92938,391328.0,,nan,
92938,391375.0,,nan,
92938,391448.0,,nan,
92938,391515.0,,nan,
92938,391697.0,,nan,
92938,391978.0,,nan,
92938,392178.0,,nan,
92938,392350.0,,nan,
92938,392298.0,,nan,
94360,397484.0,99484.0,How do I do that?,310472.0
94360,397491.0,,nan,
92454,389487.0,,nan,
92454,389527.0,,nan,
92500,389609.0,,nan,
92500,389767.0,,nan,
92500,390084.0,,nan,
92500,390547.0,97540.0,thanks,306996.0
92500,391020.0,,nan,
93148,392169.0,,nan,
93148,392158.0,98019.0,https://learn.upgrad.com/v/course/208/question/92353,312479.0
93148,392158.0,98014.0,it shouldn't be HOLD.,318756.0
93148,392137.0,,nan,
93148,392192.0,,nan,
93148,393392.0,,nan,
92588,390060.0,,nan,
92588,390092.0,,nan,
93122,392049.0,,nan,
93122,392121.0,,nan,
93484,393582.0,,nan,
93484,393615.0,,nan,
93484,393592.0,,nan,
93388,393084.0,,nan,
93388,393028.0,,nan,
93388,393051.0,,nan,
92561,390579.0,97698.0,"That's right, you need to do the process for all the 6 tables.",319721.0
92562,390281.0,,nan,
92562,390042.0,,nan,
92562,390909.0,97573.0,date format is correct,318329.0
92562,391955.0,97941.0,Pls Ignore ! :) Gotcha Silly Mistake !,306735.0
93139,392105.0,98003.0,Make sure to check the syntax of the query and use inner join on the date.,312376.0
93139,392111.0,,nan,
93139,392115.0,,nan,
93139,392593.0,,nan,
93140,392130.0,,nan,
93140,392096.0,,nan,
93140,392215.0,,nan,
93140,392598.0,,nan,
93140,393428.0,,nan,
92564,389924.0,,nan,
92564,390104.0,,nan,
92564,390556.0,,nan,
92564,390267.0,,nan,
93450,393385.0,,nan,
93450,393553.0,,nan,
93450,393719.0,,nan,
93142,392146.0,,nan,
93142,392127.0,,nan,
93145,392134.0,,nan,
93145,392326.0,,nan,
92565,390537.0,,nan,
92565,389929.0,97568.0,i've used the same code but it is showing error,303673.0
92565,390041.0,,nan,
92565,390066.0,,nan,
92565,390103.0,,nan,
92565,390569.0,,nan,
92565,390337.0,,nan,
92565,391073.0,97662.0,Thanks Vignesh...it worked,317990.0
92565,390244.0,,nan,
93088,391910.0,,nan,
93128,392079.0,,nan,
93128,392116.0,,nan,
93128,392119.0,,nan,
93144,392143.0,,nan,
93144,392202.0,98361.0,"considering optimization/performance/any other factors, which one is considered better or preferred option. Didn't find much info on google",311857.0
93144,393161.0,,nan,
93144,394591.0,98725.0,Use inner join on date for all tables.,312376.0
93144,394591.0,98727.0,used inner join on date only...1000 rows are returned....few rows are repeated,308638.0
93144,394591.0,98728.0,totally 1778 rows are returned ...checked the count of duplicates .It's around 889.To remove I used distinct n tried with delete as well query s being executed giving output as zero rows affected Once again if check count for duplicate rows it's coming around 889,308638.0
93144,394591.0,98731.0,Make sure the logic is right. and the query. Create table tablename as (select * from tablename1 t1 inner join tablename2 t2 on t1.date=t2.date and so on...);,312376.0
93144,394591.0,98757.0,The problem was in initial stage itself ie during import file into table Eichers .So deleted and recreated Eichers and imported file into table.Now 889 rows are returned.Thanks Prashanth,308638.0
93144,394591.0,98733.0,Done in the same.Cant figure it out why it's so...,308638.0
92567,389967.0,97318.0,"Yeah we can escape the column names using back tick, but still the column names are messy. If it is advised to use whatever they have provided then we have to follow that only.",318756.0
92567,389965.0,97319.0,Student mentor suggested to post in discussion forum. :),318756.0
92567,389965.0,97286.0,Would suggest to keep same. They may run some validation script for validating the result using the original column name. You may ask this directly to your student mentor.,315679.0
92567,390062.0,97317.0,Yes same here. Still a little confused if they will deduct marks for it.,318756.0
92567,390259.0,,nan,
92567,390263.0,,nan,
92567,390292.0,,nan,
92567,390782.0,,nan,
92567,391077.0,,nan,
92567,391274.0,,nan,
92592,390052.0,97700.0,"For this, can you try and use downlaod the csv files again and import?",319721.0
92592,390099.0,97333.0,"Nitesh, Thanks for your concern buddy, but I am a die hard open source fan, I would rather like to use any other flavor of Linux than Windows. Thanks for the suggestion though!",300734.0
92592,391283.0,,nan,
92438,389353.0,97301.0,Check for the row count with CSV to table,301115.0
92438,389353.0,97284.0,I imported using the wizard with default data types suggested by wizard and didn’t face any issue.,315679.0
92438,389366.0,,nan,
92438,389390.0,,nan,
92438,390008.0,97294.0,"I also tried the same, but was unable to change the data type, can you please guide me for the same?",304389.0
92438,390008.0,97306.0,"OK thanks, Will try it out.",304389.0
92438,390008.0,97296.0,"For Bajaj : After clicking on import table, change the name of table to bajaj and click next.You'll get a list of all the columns and their data types. Change the data type of Deliverable Quantity & % Deli. Qty to Traded Qty to text.. and click next..",300698.0
92571,390007.0,,nan,
92571,389961.0,,nan,
92571,389962.0,,nan,
92571,390040.0,,nan,
92571,390100.0,,nan,
92571,390338.0,,nan,
92571,390744.0,,nan,
93154,392163.0,,nan,
92449,389378.0,,nan,
92449,389384.0,97114.0,You need to look for a function through which you can access previous row value. :),317689.0
92449,389384.0,97113.0,how to know in sql where these are crossed each other,311952.0
92449,390081.0,,nan,
92449,390136.0,,nan,
92449,390508.0,,nan,
92602,390113.0,97324.0,error code 1064,314678.0
92602,390113.0,97326.0,"This error means that ""Error 1064: You have an error in your SQL syntax"" In the above code I don't see any error in syntax. There is a possibility that you have syntax error in other part of query. Since this is an assignment don't post the whole code. Cross check the whole code once again.",317991.0
92602,390113.0,97343.0,after the average function when i write over clause this error is shown can we do anything about it,314678.0
92602,390134.0,,nan,
92602,390223.0,,nan,
92602,390308.0,,nan,
92602,390518.0,,nan,
92602,390775.0,,nan,
92602,390879.0,,nan,
93565,394089.0,,nan,
93565,394097.0,,nan,
93565,394103.0,,nan,
93565,394157.0,,nan,
93567,394075.0,,nan,
93567,394087.0,,nan,
93567,394158.0,,nan,
93567,394152.0,,nan,
93161,392194.0,98009.0,"I have read the same. When working with data from market, is it not better to use views for analysis ? As you update data in source, views will give you up to date results. you cannot create foreign key for non unique values like closing price. This is why I'm interested in the reasoning behind the choice.",308637.0
93161,392225.0,98027.0,"Thank you, I did not think of a subset for analysis...",308637.0
92895,391147.0,99263.0,"i have commented # Secure File Priv. #secure-file-priv=""C:/ProgramData/MySQL/MySQL Server 8.0/Uploads"" still getting same error",320687.0
92895,391295.0,99260.0,all csv files are in that directory still getting error,320687.0
93466,393469.0,,nan,
93466,393474.0,,nan,
93466,393548.0,,nan,
93466,393459.0,,nan,
93466,416722.0,,nan,
93468,393461.0,,nan,
93468,393506.0,,nan,
93468,393465.0,,nan,
93468,393644.0,,nan,
92577,390075.0,,nan,
92577,389979.0,,nan,
92577,390009.0,97297.0,Thanks It worked for me,311952.0
92577,390260.0,97453.0,Yes you can do that,316202.0
92577,390260.0,97423.0,Modify to text?,303673.0
92577,390768.0,,nan,
92577,391059.0,,nan,
93472,393546.0,,nan,
93472,393501.0,98509.0,thanx :),318772.0
93472,393716.0,,nan,
93472,393487.0,,nan,
94442,397959.0,,nan,
94442,397946.0,99575.0,"no , please check the below link for float, double and decimal differences. Decimal is the correct data type for statistical / financial data. https://stackoverflow.com/questions/19601975/storing-statistical-data-do-i-need-decimal-float-or-double",305652.0
94442,397946.0,99574.0,"I am using insert into statement. But using decimal makes it a string, right?",310472.0
93163,392209.0,,nan,
93163,392188.0,,nan,
93163,392200.0,,nan,
93163,392224.0,,nan,
93163,392859.0,98338.0,Thanks . you are right,319969.0
93163,392857.0,,nan,
93474,393492.0,98506.0,Thank you Nishan Patel,315650.0
93474,393503.0,,nan,
93474,393542.0,,nan,
93164,392213.0,,nan,
93164,392207.0,,nan,
92368,389122.0,,nan,
92368,389083.0,98640.0,"your home directory .../Users/ ... Or , you can choose any other directory ...",312479.0
92368,389083.0,98637.0,where can I find my.cnf?,317995.0
92368,389631.0,,nan,
92368,390869.0,,nan,
93165,392199.0,,nan,
93489,393602.0,,nan,
93489,393601.0,,nan,
93489,393714.0,,nan,
93489,393608.0,,nan,
93489,393989.0,,nan,
93489,394139.0,,nan,
93489,394878.0,,nan,
93489,394993.0,,nan,
92610,390209.0,,nan,
92610,390245.0,,nan,
92610,390573.0,,nan,
93531,393807.0,,nan,
93531,393820.0,,nan,
93531,394054.0,,nan,
93529,393804.0,,nan,
93529,393896.0,,nan,
93529,394049.0,,nan,
92618,390243.0,,nan,
92618,390254.0,,nan,
92618,390342.0,,nan,
92618,391288.0,,nan,
92920,391330.0,,nan,
92920,391262.0,97690.0,this error i didnot get while loading data instead while using window function for avg,308495.0
92920,391262.0,97692.0,That I understand. This error can come because of many reason. Just try by changing the datatype of column in which you are using window function. I faced the same error and changing datatype solved it.,317991.0
92920,391262.0,97693.0,Do let me know if that works or not ?,317991.0
92920,391262.0,97893.0,Try to change Date to varchar(). If this doesn't work try to change decimal to float().,317991.0
92920,391262.0,97888.0,"whose data type i shud change, Date and Closeprice data type is Date and decimal. window function data type is auto generated as decimal which one shud i change ?",308495.0
92920,391237.0,97676.0,yes i used bajajauto,308495.0
92920,391237.0,97689.0,"changed code to create table bajaj1 as (select Date, closeprice from bajajauto) ; alter table bajaj1 add (avg(closeprice) over (order by date asc rows 19 preceding) as '20 Day MA',, avg(closeprice) over (order by date asc rows 19 preceding) as '20 Day MA', avg(closeprice) over (order by date asc rows 49 preceding) as '50 Day MA' ); getting error 1064 syntax error",308495.0
92920,391237.0,97680.0,first create the bajaj1 then try to update MA50..,305129.0
92920,391247.0,,nan,
92920,391299.0,97887.0,"create table bajaj1 as (select Date, closeprice, avg(closeprice) over (order by date asc rows 19 preceding) as '20 Day MA', from bajaauto; i used the window function still getting the error",308495.0
92920,391299.0,98103.0,"sorry i didnt paste complete code, comma is for 50 Day MA. still getting error create table bajaj1 as (select Date, closeprice, avg(closeprice) over (order by date asc rows 19 preceding) as '20 Day MA', avg(closeprice) over (order by date asc rows 49 preceding) as '50 Day MA' from bajajauto) ;",308495.0
92920,391299.0,98101.0,The comma at the end of '20 Day Ma' is creating the issue. Please remove it.,304281.0
92627,390305.0,97372.0,tried in new tab but still getting error,314678.0
92627,390315.0,97373.0,tried that but of no help,314678.0
92627,390340.0,97386.0,tried this but still getting the same error,314678.0
92627,390486.0,97436.0,yes i am using 6.1 workbench...so what can be alternate method as the latest version i am not able to install,314678.0
92627,390486.0,97459.0,I am using version 8.0 but am getting the same error,303673.0
92627,390486.0,97514.0,Got it after changing Column name Close Price to `Close Price` while creating table,303673.0
92627,390598.0,,nan,
92627,391289.0,,nan,
92621,390237.0,97355.0,yes iam using wizard,306996.0
92621,390237.0,97356.0,can you explain me,306996.0
92621,390237.0,97378.0,server is running but i cannot create new schema,306996.0
92621,390237.0,97359.0,Check the server is running or not / try restarting the mysql service.,319006.0
92621,390249.0,,nan,
92621,390253.0,,nan,
93530,393813.0,99675.0,thank u,318461.0
93530,393831.0,,nan,
93530,394051.0,99676.0,thank you,318461.0
92905,391248.0,,nan,
92905,391188.0,,nan,
92815,390919.0,97574.0,"Tried.. It is showing same error I imported data file using Wizard import and did not change a data type initially, is it that creating this error",303673.0
92815,390943.0,,nan,
92815,391005.0,,nan,
92815,391010.0,,nan,
92815,391056.0,,nan,
92815,391463.0,,nan,
92815,391417.0,,nan,
92815,391676.0,,nan,
92815,391752.0,,nan,
92815,391866.0,,nan,
92815,391966.0,,nan,
92815,392581.0,,nan,
92815,392894.0,,nan,
92649,390407.0,,nan,
92649,390483.0,97426.0,"sorting in window function will just help me perform operations on data in a sorted format right? my question is, we dont need to insert/update the table with this sorted format?",317998.0
92649,390483.0,97429.0,"also, My question is, does it have to be STORED in 2015 to 2018 sequence?",317998.0
92649,390483.0,97430.0,There is no need for storing data in sorted format unless we want to access it in a sorted manner.,317689.0
92649,390483.0,97431.0,"ok, thanks. but, are you sure of this? has any TA confirmed on this earlier?",317998.0
92649,390483.0,97434.0,No. But it is a general practice to not store data in any sorted format for date. but while inserting it would be inserted based on dates ascending as it is reading from file. Generally when you load data in a table you would have a auto generated column to keep a row count in ascending order and can be used as primary key or for data distribution purpose.,317689.0
92649,390483.0,97437.0,ok thanks,317998.0
92649,390483.0,97816.0,That's right Arihant.,319721.0
92649,390421.0,,nan,
92649,390432.0,97446.0,"it doesn't matter, as long as the averages are calculated correctly",303229.0
92649,390432.0,97428.0,"My question is, does it have to be STORED in 2015 to 2018 sequence?",317998.0
92649,390432.0,97448.0,ok thanks,317998.0
92649,390534.0,,nan,
92649,391665.0,97928.0,thank you :),317998.0
92911,391207.0,97669.0,"you've to update it as NULL.. or you can just ignore it.. actually, mysql is calculating the average for first 49 rows as well like '2 day average, 3 days average etc till 49th row.. and then after it follows as 50 days average..",316349.0
92911,391207.0,97668.0,"Thanks!!! but in table bajaj2, eicher2 .... it is not displaying NULL value for 49 rows. I can see some value.",303673.0
92911,391293.0,,nan,
92911,392384.0,,nan,
95595,404746.0,101112.0,"Ram, I will have to disagree with you here, because there won't be a lot of signals generated if you used the correct method of crossing over. I am afraid you misunderstood the concepts of the golden and the dead cross. Please go through the solution file. However, the way you summarized the analysis is very well thought through.",319721.0
95595,405538.0,101117.0,Could you pleas share a sample for future reference?,312259.0
92909,391203.0,97666.0,I have attached the end part also. please help!!,314313.0
92909,391204.0,,nan,
92909,391226.0,,nan,
92909,391228.0,,nan,
92909,391236.0,,nan,
92909,391241.0,,nan,
92909,391581.0,,nan,
92909,392218.0,98046.0,"Hi Sudeesh, Does importing using GUI affect the number of rows inserted into database?",318355.0
92909,392218.0,98278.0,Yes. You will have to change the datatype of 'Deliverable Quantity' and '% Deli. Qty to Traded Qty' to text and 'Total Turnover (Rs.)' to bigint if not done while doing an import using the Wizard.,308641.0
92909,392218.0,98499.0,"how to convert date field in all the tables? while importing, it took date field as text also while using `convert ` it gave me below error ""you have an error in your sql syntax check the manual that corresponds to your mysql server version""",318772.0
92909,392218.0,98573.0,@lovish import the date field as text itself and follow this: https://learn.upgrad.com/v/course/208/question/93472 It will definitely give you a solution!,318355.0
92909,392218.0,98569.0,"@Sudeesh You may be correct but as we don't require those columns, 889 rows i.e the exact number of rows which is there in the excel is imported as we need only two rows i.e. date and close_price",318355.0
92909,392393.0,,nan,
92909,392135.0,,nan,
92918,391230.0,97674.0,thanks,308495.0
92918,391380.0,,nan,
92921,391326.0,,nan,
92921,391240.0,97677.0,"changed to code to update bajajauto SET Bajajauto.Date = STR_TO_DATE(Bajajauto.Date, '%Y-%m-%d') still data type is varchar",308495.0
92921,391240.0,97686.0,"changed code to update bajajauto SET Date = STR_TO_DATE(Date, '%d-%M-%Y'); the message says 889 row(s) affected Rows matched: 889 Changed: 889 Warnings: 0 0.234 sec still when i data type its same varchar",308495.0
92921,391240.0,97679.0,"no need to write bajajauto so many times.. if you have written update bajajauto.. that's it, then directly use set date= Str_to_date(date,'%d-%M-%Y');",305129.0
92921,391296.0,,nan,
92921,391378.0,,nan,
92921,391355.0,97886.0,thank you vaishali for clarifying,308495.0
92921,391355.0,98505.0,alter table bajajauto modify `date` date; is this right?,315650.0
92921,391355.0,98436.0,you're welcome Rashmi :),311218.0
92921,391355.0,98504.0,hello rashmi could you help me with following command alter table modify ;,315650.0
92921,391355.0,98514.0,Vaishali could you help me above doubt?,315650.0
92921,391355.0,98516.0,"Hi krunal, this is a command that you can you to modify the data type of any existing column. For this assignment, it's better if we change the data type of Date column from Text to Date because when you would import csv data to mysql table, it would make that column to be text but later we would need to perform queries on this the basis of these dates. I hope it clarifies your your question.",311218.0
92921,391355.0,98518.0,"Inplace of , you need to specify your table name and column name and the new data type",311218.0
92921,391355.0,98523.0,"Thank you Vaishali update bajajauto set `date` = STR_TO_DATE(`date`, '%d-%M-%Y'); alter table bajajauto modify `date` datetime; I have used above code but still getting error",315650.0
92921,391355.0,98537.0,What is the error? Are you using correct name of the column from the table?,311218.0
92921,391355.0,98596.0,"Hi Vaishali, I am getting Error code: 1054. Unknown column ""%d-%M-%Y"" in filed list.",316926.0
92921,391355.0,98766.0,"Hello nagendra, are you using correct column names within ``?",311218.0
92921,392871.0,,nan,
92921,393521.0,,nan,
92917,391231.0,,nan,
92917,391291.0,97972.0,"yes. The variable name Row was creating a problem. when I changed it to row_num, it is working now. Thnx",304319.0
92917,391636.0,,nan,
92917,392397.0,98199.0,Thnx. It got resolved by using a different variable name.,304319.0
93537,393891.0,98593.0,process gets completed and atlast it shows 0 rows imported... i have selected correct schema,317558.0
93537,393891.0,98597.0,chk the two links below... https://dev.mysql.com/doc/workbench/en/wb-admin-export-import-table.html http://www.mysqltutorial.org/import-csv-file-mysql-table/,306244.0
93537,393891.0,98595.0,Try this method http://www.mysqltutorial.org/import-csv-file-mysql-table/,306244.0
93537,393908.0,98609.0,can you please help me with the code,317558.0
93537,394073.0,99222.0,that you shoule google... we are just for guidance,317811.0
93537,394073.0,99221.0,while importing csv files using LOAD DATA getting error,320687.0
93537,394209.0,,nan,
93537,396408.0,,nan,
92819,390926.0,,nan,
92819,390928.0,97580.0,In case and end. with when x>y and lag(x)<lag(y) then 'buy' This says error like < is not valid at the position. and then is not valid at this position. I am really stuck,312019.0
92819,390928.0,97582.0,If i try to add over(partition by Date) . it says you cannot use window function lag in this context. Hmm,312019.0
92819,390928.0,97586.0,Use lag(x 50 Day MA) and lag(20 Day MA < 50 day MA),316202.0
92819,390928.0,97587.0,when x>y and lag(x<y) over(parition by date) then 'Buy'. Same issue Sham.,312019.0
92819,390928.0,97984.0,"Hey, Naga Prasad, please understand that it is a graded assginment. And you cannot display any part of these answers in the discussion forum. If you cannot specific parts of LAG, then ask those conceptual questions but not codes directly.",319721.0
92819,390931.0,97581.0,My MySql workbench version is 8.0,312019.0
92819,391000.0,,nan,
92819,391111.0,,nan,
92819,391281.0,,nan,
92819,391294.0,97747.0,Kudos!!,303673.0
92819,391294.0,97746.0,Hi Venkat. Really appreciate. You have relieved me from pressure. Works keeping in create table level. Thanks again.,312019.0
92819,391519.0,,nan,
92819,391684.0,,nan,
92742,390576.0,,nan,
92742,391030.0,,nan,
92744,390591.0,97477.0,"If it is still not working, try to move the Data sets to the directory specified by ""secure-file-priv"". You can have a look at this link, ""https://stackoverflow.com/questions/32737478/how-should-i-tackle-secure-file-priv-in-mysql""",303229.0
92744,390591.0,97499.0,I am unable to locate the file in Mac. Can you please guide me on this?,310522.0
92744,390614.0,,nan,
92744,391065.0,,nan,
92744,391158.0,97648.0,np still go to location where mysql is installed and try to put csv file there. i think it will work,317811.0
92756,391959.0,,nan,
92756,390615.0,97478.0,"Ankur, my doubt here is while submission of the SQL submission, are you including the queries to create assignment schema and query to import all the csv to tables as the name of the stocks? since they are not part of any grading.. do we still need to do so?",313676.0
92756,390615.0,97479.0,Yes I am including and there is no harm in that. That is why i imported data from csv using queries not by wizard. you are thinking right..include all queries,319869.0
92756,390615.0,97936.0,You do not need to include the import queried. Importing can be done using the Import Wizard too.,319721.0
92756,390680.0,,nan,
92756,390703.0,,nan,
92756,390985.0,,nan,
92748,390649.0,,nan,
92748,391132.0,,nan,
92598,390082.0,97589.0,For 50th Day signal should be null or Hold?,310419.0
92598,390082.0,97608.0,I dont think it matters -it is not being used; only signals from day 51 make sense to me,300694.0
92598,390082.0,97702.0,"It doesn't matter, even if it is NULL or HOLD or anything, as we're not using this row at all.r",319721.0
92598,390088.0,,nan,
92598,390144.0,,nan,
92598,390225.0,,nan,
92598,390564.0,,nan,
92598,390774.0,,nan,
92598,390888.0,,nan,
92189,388295.0,,nan,
92189,388338.0,,nan,
92189,388354.0,96841.0,yup total 889 loaded...excluding header...,300693.0
92189,388354.0,96852.0,I'm able to upload 888 rows and this is happening as 1 column is having null info there. How did you take care of that situation.,318455.0
92189,388354.0,97041.0,Siddhant try declaring those columns float instead of double,301115.0
92189,388354.0,96875.0,"i havent created a table as structured in csv...just created table with date,close_price and other 2 ma columns ,took date and close price from csv and loaded only them instead of complete csv columns ..thats how i did it",300693.0
92189,388354.0,97565.0,Assignment says anyways to remove null values.,315242.0
92189,388406.0,,nan,
92189,389107.0,,nan,
92189,389437.0,,nan,
92189,389330.0,,nan,
92578,390004.0,,nan,
92578,390093.0,97316.0,"Hi arihant i am implementing like this am i on right path ? select *,lag(bajaj1.signal,1)over(order by date_time asc) as `previous day signal` from bajaj1 CASE when (bajaj1.signal='BUY' and bajaj1.`previous day signal` ='BUY') THEN bajaj1.signal ='HOLD' when (bajaj1.signal='BUY' and bajaj1.`previous day signal` ='SELL') THEN bajaj1.signal ='BUY' when (bajaj1.signal='SELL' and bajaj1.`previous day signal` ='BUY') THEN bajaj1.signal ='BUY' ELSE NULL END;",303674.0
92578,390005.0,97336.0,"thankyou i am implementing like this am i on right path ? select *,lag(bajaj1.signal,1)over(order by date_time asc) as `previous day signal` from bajaj1 CASE when (bajaj1.signal='BUY' and bajaj1.`previous day signal` ='BUY') THEN bajaj1.signal ='HOLD' when (bajaj1.signal='BUY' and bajaj1.`previous day signal` ='SELL') THEN bajaj1.signal ='BUY' when (bajaj1.signal='SELL' and bajaj1.`previous day signal` ='BUY') THEN bajaj1.signal ='BUY' ELSE NULL END;",303674.0
92578,390257.0,97365.0,"Thankyou i am implementing like this am i on right path ? select *,lag(bajaj1.signal,1)over(order by date_time asc) as `previous day signal` from bajaj1 CASE when (bajaj1.signal='BUY' and bajaj1.`previous day signal` ='BUY') THEN bajaj1.signal ='HOLD' when (bajaj1.signal='BUY' and bajaj1.`previous day signal` ='SELL') THEN bajaj1.signal ='BUY' when (bajaj1.signal='SELL' and bajaj1.`previous day signal` ='BUY') THEN bajaj1.signal ='BUY' ELSE NULL END;",303674.0
92578,390892.0,,nan,
92578,391279.0,,nan,
92753,390611.0,,nan,
92753,390674.0,,nan,
92753,390704.0,,nan,
92753,391064.0,,nan,
92948,391413.0,,nan,
92948,391424.0,,nan,
92948,391614.0,,nan,
92951,391423.0,97731.0,You are welcome. We usually use tildas when theres a space in the name or the name is an sql keyword.,310511.0
92951,391423.0,97728.0,"Thanks for the input , i did that way as an alternate , but wanted to see if this is feasible or not .",311861.0
92951,391456.0,,nan,
92955,391426.0,,nan,
92955,391430.0,97894.0,Is your problem solved or still error is coming ?,317991.0
92955,391431.0,,nan,
92955,392013.0,,nan,
93243,392454.0,,nan,
93243,392438.0,,nan,
93243,392442.0,,nan,
92931,391325.0,,nan,
92931,391329.0,,nan,
92931,392752.0,,nan,
92755,391956.0,,nan,
92469,389479.0,97151.0,"Hi Vipul, thanks for that. I inserted the data using only select and dropping values from the code. It worked. Is that correct or is ""values"" necessary for some reason?",316416.0
92469,389536.0,,nan,
92469,389567.0,,nan,
92469,390065.0,,nan,
92822,390993.0,,nan,
92822,390957.0,,nan,
92822,390996.0,,nan,
92822,390984.0,,nan,
92825,390987.0,,nan,
92825,390994.0,,nan,
92889,391112.0,,nan,
92889,391151.0,,nan,
93595,394307.0,,nan,
93595,394312.0,98680.0,Thanks i forgot to add delimiter,318772.0
93595,394399.0,,nan,
93595,394403.0,,nan,
93168,392196.0,,nan,
92898,391169.0,,nan,
92900,391269.0,97696.0,now try to uninstall and re-install once again...,306242.0
92900,391269.0,97931.0,"I uninstalled it, but still the data was not getting deleted, and finally after several attempts and rebooting my laptop, I was able to delete it and re-install the software. Thank you for your help.",304389.0
92900,391829.0,97933.0,Good to hear your issue is resolved now.,307495.0
92900,391829.0,97930.0,"I uninstalled it, but still the data was not getting deleted, and finally after several attempts and rebooting my laptop, I was able to delete it and re-install the software. Thank you for your help.",304389.0
93170,392211.0,,nan,
93170,392212.0,,nan,
93170,392234.0,,nan,
92678,390456.0,,nan,
92678,390481.0,,nan,
92678,390491.0,,nan,
92678,390498.0,,nan,
92678,390512.0,,nan,
92678,390566.0,97466.0,Yes Ankur!! It worked! Thanks a lot! :D,310505.0
93171,392254.0,,nan,
93171,392230.0,98023.0,where is Table data import wizard ... can you provide further steps till answering,319969.0
93171,392230.0,98434.0,"Hey, Dinesh, you can check the comments on the question mentioned above.",319721.0
93171,392230.0,98433.0,https://upgradsupport.freshdesk.com/helpdesk/tickets,319721.0
93171,392342.0,,nan,
93174,392231.0,98017.0,Thanks Vipul,317269.0
93174,392226.0,98018.0,Thanks TA,317269.0
93175,393616.0,,nan,
93175,392232.0,98330.0,Thanks,302877.0
93175,392227.0,,nan,
93175,392238.0,,nan,
93175,394182.0,,nan,
93176,392239.0,,nan,
93176,392235.0,,nan,
93177,392241.0,98015.0,"I mean the 50th day data, where we can calculate the moving average difference",318084.0
93178,392242.0,,nan,
93179,392245.0,,nan,
93179,392253.0,,nan,
93179,392311.0,,nan,
93179,392333.0,,nan,
93179,392555.0,,nan,
93179,546025.0,,nan,
92958,391438.0,97732.0,yes i tried its giving 00.00 instead of Nan in all the fields,310504.0
92958,391438.0,97733.0,tried still its returning null values,310504.0
92958,391453.0,97740.0,"LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Bajaj Auto.csv' iGNORE INTO TABLE bajaj FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\r\n' IGNORE 1 LINES (@date_stock) set date_stock= STR_TO_DATE(@date_stock, '%d-%M-%Y');",310504.0
92958,391455.0,97734.0,"tried with removing the NOT NULL constraint, still getting the same result",310504.0
92958,391455.0,97735.0,Can you share the picture of the LOAD DATA INFILE code ?,313691.0
92958,391455.0,97744.0,"In (@date_stock), you should mention all the column names in order like (@date_stock, col_1, col_2, ......., col_n)",313691.0
92958,391455.0,97742.0,"LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Bajaj Auto.csv' iGNORE INTO TABLE bajaj FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\r\n' IGNORE 1 LINES (@date_stock) set date_stock= STR_TO_DATE(@date_stock, '%d-%M-%Y');",310504.0
92958,391455.0,97754.0,"(@date_stock,@Open_Price,@,High_Price,@Low_Price,@Close_Price,@WAP,@No_of_Shares,@No_of_Trades,@Total_Turnover_Rs,@Deliverable_Quantity,@Perce_Deli_Qty_To_Traded_Qty,@Spread_High_Low,@Spread_Close_Open) set date_stock= STR_TO_DATE(@date_stock, '%d-%M-%Y'); thats actually throwing an error, do i need to mention all the column names? since am changing only the data type for date column",310504.0
92958,391455.0,97756.0,Don't put @ for all columns. Put it only for date column since you are only changing that. And mention only columns name for others in the same order that you have done in create table command.,313691.0
92958,391455.0,97758.0,do i need to give the column names as the same as that provided in the sheet? and is it necessary to put the column names inside ' ' while creating the table ?,310504.0
92958,391455.0,97760.0,Yes column names should be same as in the sheet. To have column name with spaces or column name which are SQL keywords like Date use the back tick(`) to enclose the column names.,313691.0
92958,391455.0,97801.0,yes it worked . thank you so much for investing your time,310504.0
92958,391455.0,97804.0,You are welcome. Please upvote my answer if you like it.,313691.0
92958,391470.0,97813.0,it worked.thanks a lot,310504.0
92958,391470.0,97870.0,your welcome,311861.0
92958,391470.0,97755.0,tried.not working do i need to give the column names as the same as that provided in the sheet? and is it necessary to put the column names inside ' ' ?,310504.0
92958,391470.0,97762.0,Yes.. you need to use Tilde function key .,311861.0
92958,392141.0,,nan,
92969,391526.0,97784.0,"Changed the timeout from 30 seconds to 10 minutes, still timing out, will try to increase it further. Using sql workbench 8.0",318844.0
92969,391526.0,97793.0,can you check all your 6 tables as well ? Are they getting selected individually ?,311861.0
92969,391526.0,97792.0,so after you have started it is still not working ?,311861.0
92969,391522.0,,nan,
92969,391572.0,,nan,
92969,391533.0,97786.0,"Thanks, still timing out though after increasing to 10 minutes.",318844.0
92969,391637.0,,nan,
92969,391722.0,,nan,
93187,392307.0,,nan,
92974,391553.0,97794.0,"When i run the select query individually it fetches the Signal on that particular Date from bajaj2 table.. Anyways, it got fixed seems a small tweek is required to the arguments i passed in the function. Thanks..",300727.0
92959,391444.0,,nan,
92959,391446.0,,nan,
92959,391451.0,,nan,
92959,391868.0,,nan,
92959,392229.0,,nan,
93182,392240.0,99775.0,"Yeah, that definitely helps !",308963.0
93182,392257.0,,nan,
93183,392269.0,98024.0,How would it be sorted then ?,318427.0
93183,392269.0,98043.0,ok,318427.0
93183,392269.0,98028.0,The table being imported is sorted in a particular order. You can exploit that to overrule the datatype.,304281.0
93183,392260.0,,nan,
93184,392280.0,,nan,
93184,394150.0,,nan,
93185,392256.0,98021.0,then can't we format the date from 31-july-2018 to general format?,317410.0
93185,392270.0,98032.0,"Hi, Am getting same error. By the way one doubt, i am not able to use column names that were imported from csv file.",317410.0
93185,392270.0,98167.0,`close price` use back tick It's on the left side of numeric key 1.,306243.0
93185,392300.0,98040.0,"One more doubt actually, select 'Close Price' from bajaj; with this command am getting below data which is not correct. Close Price Close Price Close Price Close Price Close Price Close Price Close Price Close Price",317410.0
93220,392372.0,98166.0,"Hi vikas, i think you meant date and not data as input. Please advise",304814.0
93220,392372.0,98197.0,I was trying to find if we can generalize it for any table by passing table name and date field. I do now for assignment is limited to date for specific stock bajaj. Would like to understand if you can use prepare and execute statement or equivalent in UDF.,317514.0
93220,392372.0,98198.0,I was trying to find if we can generalize it for any table by passing table name and date field. I do now for assignment is limited to date for specific stock bajaj. Would like to understand if you can use prepare and execute statement or equivalent in UDF,317514.0
93220,392372.0,98203.0,"Hi Anshul, apologies for that. Hi Rajesh, I am not sure if a table can be considered as an input to a UDF but please do try that. However, another way to solve this could be to join and/or append and create a table containing all the signals and then increasing the input to data plus the name of the stock.",304281.0
93221,392405.0,98213.0,is back ticks used for close price??,301115.0
93221,392370.0,,nan,
92980,391613.0,,nan,
92980,391627.0,,nan,
92980,391655.0,,nan,
92980,391981.0,,nan,
92396,389191.0,97374.0,This was solved by changing the datatype of columns in table .,318448.0
92396,389135.0,,nan,
92396,389212.0,,nan,
92396,389246.0,,nan,
92983,392351.0,98185.0,even with that facing issue,310598.0
92983,392351.0,98211.0,"ELBY, Try my solution above.",314244.0
92983,392351.0,98215.0,"i tried that too,but still same issue",310598.0
92983,392351.0,98221.0,"create table t1 as (select b.dt as date, b.col2 as bj, e.col2..... from table 1 b inner join table 2 e .... so on);",311004.0
92983,392351.0,98228.0,"ok thanks worked, keys should be date data type as well.",310598.0
92983,392351.0,98222.0,try to write in this way ..,311004.0
92983,391575.0,,nan,
92983,391583.0,97805.0,"Thanks, Vipul. Already tried this, does not work.",319302.0
92983,391602.0,,nan,
92983,391605.0,97802.0,I am getting more than 100 MBPS speed so i dont think so it is a network connectivity issue.,320195.0
92983,391605.0,97803.0,Okay .. so even you are facing the same issue. I am talking about sql server database connection.,319006.0
92983,391624.0,,nan,
92983,391741.0,,nan,
92983,392271.0,98252.0,"Using Mac but this did help as reference, thanks.",319302.0
92983,392634.0,,nan,
92983,393422.0,,nan,
93339,392762.0,98306.0,"Thanks for the reply Chandan, So we just had to calculate MA on basis of Close Price right?",315423.0
93339,392762.0,98307.0,Yes.. bot 20 Days MA and 50 Days MA on basis of Close Price.,311686.0
93339,392762.0,98310.0,"Thanks for the quick reply, that resolved my query.",315423.0
93339,392762.0,98308.0,"That's correct we need to calculate MAs for closed prices. Please go through the lectures on window functions, there it's explained.",319721.0
93339,392775.0,,nan,
93339,392819.0,,nan,
92893,391130.0,,nan,
92893,392376.0,,nan,
93396,393083.0,,nan,
93190,392268.0,,nan,
93190,392805.0,,nan,
93180,392249.0,98143.0,"If you are trying to use window function in update command, it will not work as mentioned in the course. You can use a create table as (select ******* ) for that where ******* part holds the window functions.",304281.0
93180,392249.0,98126.0,there was no syntax error,301121.0
93191,392261.0,,nan,
93283,392612.0,,nan,
93283,392664.0,,nan,
93225,392367.0,98077.0,so i can let those values present in my table there only ?,318427.0
93225,393390.0,98755.0,This is just the logic i have given.. look for the appropriate syntax and it's implementation,318427.0
93192,392327.0,,nan,
93192,392264.0,,nan,
93192,392275.0,,nan,
93228,392382.0,,nan,
93404,393134.0,98426.0,Thank you,318329.0
93284,392609.0,98552.0,"M and n are moving average on current row. As I mentioned, how about previous day moving average?",318007.0
93284,392609.0,98247.0,I have comparing 20 day MA with 50 day MA. So in the above logic n is 20 day MA and m is 50 day MA. Is this right logic to get the signal,318814.0
93284,392630.0,98285.0,"your eg was really helpful, so for sell it should be yesterdays ma20 greater than ma50 and today's ma 20 lower than ma50 right?",310598.0
93284,392630.0,98327.0,correct :),317998.0
93193,392272.0,,nan,
93227,392377.0,,nan,
93195,392335.0,,nan,
93195,392408.0,,nan,
93229,392387.0,,nan,
93229,392390.0,,nan,
93229,393432.0,,nan,
93196,392279.0,,nan,
93196,392278.0,,nan,
93231,392402.0,,nan,
93231,392425.0,,nan,
93231,392482.0,,nan,
93231,392445.0,,nan,
93238,392427.0,,nan,
93238,392436.0,,nan,
93427,393252.0,,nan,
93427,393301.0,,nan,
93427,393308.0,,nan,
93427,393475.0,,nan,
93427,393728.0,,nan,
93619,394491.0,,nan,
93619,395496.0,99353.0,"round function doesn't helps, best way is to change data type",311386.0
93198,392290.0,,nan,
93198,392293.0,98117.0,I am using LOAD command.,313526.0
93199,392287.0,98037.0,"Yes , I think we should include all the tasks that we did on raw data.",318741.0
93199,392295.0,,nan,
93233,392410.0,98137.0,What is the point in writing such an answer? You could have as well stayed away.,301121.0
93233,392410.0,98161.0,"@Muthu Krishna... she is a teacher and here to help us with our doubts. you obviously SHOULDNT expect direct codes, since it is a graded question. Anyway, little politeness and respect towards a teacher wouldnt harm you. just my opinion.",317998.0
93233,392410.0,98144.0,"Hey Muthu Krtishna, I'm sorry if it did not help you, but I'm trying to tell you that it's unethical to ask questions like this. If this were an exam and you asked your teacher an answer to the examination question, is it alright? No, similarly, I tried to tell you that you shouldn't be doing it. I hope that explains it. Also, unethical practices can have bad consequenses, and I tried to warn you there.",319721.0
93233,392430.0,98220.0,Thank you very much. Incidentally I had found the solution; At the same I really appreciate the conceptual answer you could share with me.,301121.0
93200,392294.0,,nan,
93200,392299.0,,nan,
93230,392396.0,,nan,
93230,392392.0,,nan,
93350,392830.0,,nan,
93350,392835.0,,nan,
93350,393197.0,,nan,
93136,392099.0,98062.0,"Yes, Had tried this before. But it is not allowing to return the signal. It says data set not allowed.",304319.0
93136,392099.0,98102.0,Tried again. CREATE FUNCTION GEN_SIG(dd Date) returns varchar(4) deterministic BEGIN SELECT sig from bajaj2 where MarketDate=dd; return (sig); END Giving error code : 1415. Not allowed to return a result set from a function.,304319.0
93136,392099.0,98073.0,"Since the udf is called in a select statement. It would eventually become a nested select for such a simple thing. TA, pls respond.",304319.0
93136,392117.0,,nan,
93201,392306.0,,nan,
93201,392302.0,,nan,
93201,392324.0,98141.0,"yes , it is always considered suitable to load only necessary columns that will be further required in your analysis.",318741.0
93201,392324.0,98061.0,Is this a best practice(in real time scenarios) to load only necessary fields from external source to the environment ?,301115.0
93235,392421.0,98132.0,TA has clarified what needs to be done.,317991.0
93235,392417.0,,nan,
93235,392424.0,,nan,
93235,392423.0,,nan,
93202,392346.0,,nan,
93202,392297.0,,nan,
93202,392334.0,,nan,
93306,392660.0,98302.0,"my question was pertaining to the case where previous ma values are same but not nulls. I know it is rare, but nonetheless, should we handle it or not because the current data doesn't have such occurrences",318329.0
93306,392660.0,98321.0,that is what I said. though there are no such values in the current dataset and it is a very very remote case to occur,318329.0
93306,392660.0,98312.0,I'm not sure if I saw any such readings where both ma are equal,316349.0
93306,394256.0,98662.0,The signal would be hold for previous day but about the current day.,318329.0
93306,394256.0,98664.0,"For any day unless the 50 Day MA crosses the 20 Day MA , the signal is hold and since as you mentioned the MAs are same as the previous day, the signal will be hold.",304281.0
93306,394256.0,98665.0,"Let's assume that I am using the following Logic for signal generation Buy If 20 Day MA > 50 Day MA AND Previous 20 Day MA < Previous 50 Day MA Sell If 20 Day MA Previous 50 Day MA Hold otherwise Let's assume that the T-2 Day Signal as Buy and T-1 Day MA's are equal Now if the current day 20 Day MA < 50 Day MA, ideally the signal should be Sell but due to the above logic it is Hold. That's my question. It's very difficult to handle this case.",318329.0
93242,392439.0,98189.0,you need to use with a space,301648.0
93242,392439.0,98157.0,? which is bajaj auto with a space? correct? or can I use bajaj_auto,300748.0
93242,392446.0,,nan,
93242,392450.0,98170.0,"The name of the table mentioned in assignment as bajaj1, bajaj2 etc, whereas the CSV name is 'Bajaj Auto', can you clarify for 'TVS motors' can it not be tvs_motors1 ?",319770.0
93104,391979.0,97974.0,"i did whole assignment using text format only. while apply order by, i just convert str to date.. Result are fine. while loading csv into table, we cant apply any function on the column. that's why i dint convert str to date while loading.",312746.0
93104,392078.0,,nan,
93156,392316.0,98042.0,check also the previou day if 20day ma > 50 day ma and on the previous day its 50 day ma > 20 day ma then its a sell for buy its vice versa on one day we cant say the trend is shifted. hope this help if not reply here,318017.0
93156,392316.0,98045.0,trend should be changing not a single day can result in output,318017.0
93156,392316.0,98164.0,"Hello Deval i have used both the condition 20day ma > 50 day ma and lag(20day ma) < lag(50 day ma), but i found the output is not correct,",311004.0
93156,392316.0,98168.0,lag function should be used over a window . look at the link below to understan more about lag http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/,318017.0
93156,392316.0,98352.0,"how u guys are using lag function? lag function works over a window and it will calculate the signal for complete table. But if u apply this inside a UDF where u have to return only one signal for a date then u need where clause. But as soon as u apply where `Date` = any_date, lag function does fails and gives null as it does not have any window. Where clause limits observation to only one record.",313515.0
93156,392316.0,98371.0,yes.. your observation is valid...,311004.0
93156,392316.0,98584.0,WHEN `20 Day MA` > `50 Day MA` AND 'Lag_20_MA' < 'Lag_50_MA' THEN 'BUY' WHEN `20 Day MA` 'Lag_50_MA' THEN 'SELL' ELSE 'HOLD',318427.0
93156,392404.0,98147.0,sorry rashmi my mistake but without the quetion i didnt knew how could i help you,318017.0
93156,392404.0,98105.0,yes share then only we can helo you,318017.0
93156,392404.0,98115.0,both gave error,308495.0
93156,392404.0,98120.0,lag function is used over a window like lag() over (order by xyz) hope this helps,318017.0
93156,392404.0,98114.0,"create table bajaj2 as ( select date, closeprice, case when (20 Day MA > 50 Day MA) and lag(20 Day MA) < lag(50 Day MA) then ""buy"" when (20 Day MA lag(50 Day MA) then ""sell"" else ""hold"" end as signal from bajaj1 ) ;",308495.0
93156,392404.0,98110.0,"create table bajaj2 as ( select bajaj1.date, bajaj1.closeprice, case when (bajaj1.20 Day MA > bajaj1.50 Day MA) and lag(bajaj1.20 Day MA) < lag(bajaj1.50 Day MA) then ""buy"" when (bajaj1.20 Day MA lag(bajaj1.50 Day MA) then ""sell"" else ""hold"" end as signal from bajaj1 ) ;",308495.0
93156,392404.0,98122.0,"Rashmi, you cannot share your assignment codes here as this is a graded compoment. . I am deleting them",319721.0
93156,392404.0,98124.0,Please try asking questions which do not directly invlove displaying the solutions.,319721.0
93156,392404.0,98129.0,Deval Yadav10 minutes ago yes share then only we can helo you,308495.0
93156,392404.0,98130.0,sorry i thought she might share a code which has error not the complete code my apologies,318017.0
93156,392404.0,98145.0,"how do we use lag() over (order by xyz) and also operator to generate signal, can you share any link which can help me on this, i see most of links giving only Select lag() over (), and not compaiosion of lag with current",308495.0
93156,392404.0,98149.0,you try using lag the way i told you and you can still use conditional operator on it and if you still face issue please check this example : http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/,318017.0
93156,392404.0,98151.0,"no worries, just help me resolve it pls share a link where i can use lag() and also operator to compare the current and previous price,",308495.0
93156,392404.0,98156.0,"select price , (case when lag(price) over (order by emplid asc) > price then 'price increased else 'decresed' end) as 'price analysis' from table 1",318017.0
93156,392404.0,98178.0,yeah :),318017.0
93156,392404.0,98175.0,thanks so much devel it worked,308495.0
93156,392406.0,98116.0,pl suggest the string option in this case.. i know %M when month name is full and %m when month name is abbreviation.. but if column has both types of month then what to use?,318791.0
93156,392406.0,98109.0,look it depends in the kind of format you are working with in mysql the date format is yyyy-mm-dd for all other options they are of string type,318017.0
93156,392406.0,98139.0,https://dba.stackexchange.com/questions/96245/mysql-filter-date-column-using-like-or-month look at this as well,318017.0
93156,392406.0,98133.0,in such a case the data itself is not clean try to change the format in the execl itself,318017.0
93156,392407.0,98108.0,yes it is very much possible to update as many columns you want in a single update,318017.0
93156,392407.0,98111.0,"update table set a,b,c = x,y,z from table 2",318017.0
93156,392407.0,98112.0,"can you point me towards the syntax for this, if possible? I couldn't find one.",312376.0
93156,392407.0,98125.0,https://www.dofactory.com/sql/update,318017.0
93156,392407.0,98146.0,"I understand the update function. I want to know if below is possible. Update table set column A with condition A, set column B with condition B;",312376.0
93156,392407.0,98152.0,yes you can try that also: check out on the link below : https://dba.stackexchange.com/questions/135505/multiple-update-with-multiple-conditions,318017.0
93156,392184.0,98010.0,Try changinf the datatype.,319721.0
93156,392233.0,98020.0,i have to find the average of first 50 rows can i do by finding 49 rows ? likewise the average should start from 50 th row,318017.0
93156,392233.0,98022.0,why you need to round stock price is critical thing some days it rises or fall by just paisa not rupee so dont round it up,318017.0
93156,392233.0,98064.0,Is it necessary to keep them empty or null? I mean I can have moving average for those rows as well but they can be ignored,318576.0
93156,392233.0,98084.0,I would advise not to calculate MA for those rows since it makes no sense. You can keep it NULL,318756.0
93156,392233.0,98119.0,"No, you need not round it, and the top 50 rows will be empty.",319721.0
93156,392237.0,98025.0,you can resolve 1364 by setting a default value there will be something wrong thats why error 1064 appear please check closesly i would recommend watch for these `` join should happen on primary and foreign key for a query to be optimised else you will get repeated rows to set use update command,318017.0
93156,392237.0,98054.0,"1. This happens when I run the following code - INSERT INTO `master` (TCS) SELECT `Close Price` FROM tcs; Error given is - Error Code: 1364. Field 'mDate' doesn't have a default value Just FYI when I execute - SELECT @@sql_mode; I get below result 'STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION' Wondering if the strict setting is the issue",312608.0
93156,392237.0,98057.0,I see the 1364 error despite assigning values to mDate through a different INSERT command previously,312608.0
93156,392244.0,98026.0,you have answered your question correctly :),318017.0
93156,392244.0,98085.0,Yes correct.,318756.0
93156,392251.0,98034.0,"deval why are you even answering without going through the question properly ? I asked about signal of 1st 50 rows, not about moving averages value.I am aware that moving averages will start from 50th row. Don't copy/paste same answer on every question.",318756.0
93156,392251.0,98029.0,i have to find the average of first 50 rows can i do by finding 49 rows ? likewise the average should start from 50 th row i guess the output should be NULL if date passed is a holiday,318017.0
93156,392263.0,98056.0,can i get some sample table...still not clear for me...,318322.0
93156,392263.0,98030.0,you have bajaj1 and like wise other stiock 1 tables for that already,318017.0
93156,392281.0,98052.0,"1. This happens when I run the following code - INSERT INTO `master` (TCS) SELECT `Close Price` FROM tcs; Error given is - Error Code: 1364. Field 'mDate' doesn't have a default value Just FYI when I execute - SELECT @@sql_mode; I get below result 'STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION' Wondering if the strict setting is the issue",312608.0
93156,392281.0,98031.0,join should happen on primary and foreign key for a query to be optimised else you will get repeated rows,318017.0
93156,392285.0,98038.0,check also the previou day if 20day ma > 50 day ma and on the previous day its 50 day ma > 20 day ma then its a sell for buy its vice versa on one day we cant say the trend is shifted. a stock trend trend should shift to get a sell or buy hope this help if not reply here,318017.0
93156,392285.0,98055.0,so if current_day_20 >curren_day50 and prev_20>prev50 then buy and vice a versa for sell,304695.0
93156,392285.0,98582.0,WHEN `20 Day MA` > `50 Day MA` AND 'Lag_20_MA' < 'Lag_50_MA' THEN 'BUY' WHEN `20 Day MA` 'Lag_50_MA' THEN 'SELL' ELSE 'HOLD',318427.0
93156,392304.0,98063.0,https://learn.upgrad.com/v/course/208/question/92790,318756.0
93156,392304.0,98080.0,I think this has to be done for Signals only correct?,300734.0
93156,392304.0,98071.0,follow this one also: https://learn.upgrad.com/v/course/208/question/92618,319444.0
93156,392304.0,98036.0,"Same question here. In one post TA mentioned to either delete them or keep as NULL, it doesn't matter",318756.0
93156,392304.0,98081.0,I think till 50th rows we can keep null for signal from 51 st rows and we can generate signal as suggested by the TA.. need confirmation on the same from TA.,319444.0
93156,392304.0,98332.0,"1st to 49th row will be NULL and 50th rows onwards (including) the 50th row , we will have the signals.",311869.0
93156,392304.0,98495.0,"Ravi, I have done the same, hope we are not making mistake mistake.",300734.0
93156,392304.0,98581.0,if you don't make it null then it will by default give hold,318427.0
93156,392309.0,98039.0,shoot your questions here,318017.0
93156,392320.0,98047.0,moving average is a quantity which is calculated according to previous or future data so think if it need to be sorted or not,318017.0
93156,392320.0,98336.0,"When you apply a Window function on a column, which we have as dates here, Data are sorted first based on that column, only then we can get a correct result. But be cautious here as Date colum is Text here, so sorting here will yield incorrect order unless you process and change the Date column to a date or number data type. Hope it helps.",311869.0
93156,392312.0,98041.0,please change the column length as something you have set is of shorter length then the data,318017.0
93156,392312.0,98121.0,"avg(closeprice) over (order by date asc rows 19 preceding) as '20 Day MA', avg(closeprice) over (order by date asc rows 49 preceding) as '50 Day MA' datatype is autogenerated , it says decmial(14,7) how do i change it",308495.0
93156,392312.0,98138.0,please reply ?,308495.0
93156,392312.0,98142.0,while creating the table please the if you have used the coreect data type for closeprice,318017.0
93156,392312.0,98159.0,"yes decimal (10,3)",308495.0
93156,392312.0,98183.0,ohkk,308495.0
93156,392312.0,98160.0,use double instead you cant be sure that price can be just that much in decimal and if it help do tell,318017.0
93156,392332.0,98049.0,"though what u said is absolutely correct, still for the sake of assignment we need to create 13 tables.",318756.0
93156,392332.0,98340.0,"Rajesh has a much valid point here. Deval we need not create different columns for moving averages pertaining to each stock type. Just one additional column to identify the Stock as suggested by Rajesh is sufficient to calculate 20 Day and 50 Day moving averages and also to generate correct signal. You just need to write your queries accordingly. I think this assignment is to practice more, so that when you repeat a task multiple time, you remember more. Lol",311869.0
93156,392332.0,98053.0,this is done for the sack of understanding you can create a single table with 6 close price 12 ma columns and 6 colums for signal do yo think it is a right approach and more importantly we are analysing each stock as a different identity,318017.0
93156,392358.0,98074.0,thanks,312608.0
93156,392358.0,98069.0,display result.,318756.0
93156,392352.0,98065.0,date is the only column that can be set as key as it doesnt repeat,318017.0
93156,392352.0,98068.0,yes,304693.0
93156,392352.0,98070.0,please answer for folks who were trying to create a key,304693.0
93156,392352.0,98089.0,No need of creating keys. Though creating key will enhance the performance while creating the master table.,318756.0
93156,392352.0,98107.0,yes can we let others know about this? as I saw many questions about creating key's,304693.0
93156,392352.0,98118.0,thanks Ranip,304693.0
93156,392352.0,98113.0,Sure mate go ahead,318756.0
93156,392357.0,98078.0,why are you updating you should create a table with inline joins that would be a optimised way of doing look in a query it runs like FROM Join having where group by select distinct order by,318017.0
93156,392357.0,98090.0,You need to use create table master (select query with join) I am not giving the complete statemnt.,304319.0
93156,392357.0,98127.0,This helps. Let me try,312608.0
93156,392361.0,98072.0,make it null,318756.0
93156,392361.0,98083.0,you have to create a avereage of 20 rows can you do it with 18 or 19 rows ? think about it and work accordingly,318017.0
93156,392361.0,98092.0,it should be null as we will have 20 values then only we will creat MA20 suppose we have to do MA10 then we need 10 values,318017.0
93156,392361.0,98087.0,while calculating 20day moving average then what should be the output for first 19days??,318322.0
93156,392368.0,98086.0,suppose you have calculator you press + then it adds two values just like that we have to create a function in which we have to pass a date and it should get the trend for that date for bajaj2 you should try to create the function optimised so that if stock change you dont have to write whiole code again,318017.0
93156,392368.0,98079.0,The UDF will be passed a date as an input parameter and it will fetch the signal from the bajaj2 table for that specific date and return the signal.,318756.0
93156,392391.0,98096.0,"Both joins and unions can be used to combine data from one or more tables into a single result. ... Whereas a join is used to combine columns from different tables, the union is used to combine rows.",318017.0
93156,392391.0,98093.0,"Dates are same in all the tables, and also verified the count it is 889 in master table.",318756.0
93156,392399.0,98100.0,unions are used to get additional rows,318017.0
93156,392399.0,98099.0,"Both joins and unions can be used to combine data from one or more tables into a single result. ... Whereas a join is used to combine columns from different tables, the union is used to combine rows.",318017.0
93156,392412.0,,nan,
93156,392416.0,98128.0,yes the code will work for 1 row as well as for 1 million rows,318017.0
93156,392416.0,98131.0,thanks,304693.0
93156,392416.0,98135.0,folks were struggling to get all 889 rows I hope it doesn't matter when we can show signal for a particular day,304693.0
93156,392416.0,98136.0,correct,318017.0
93156,392416.0,98587.0,yes,318427.0
93156,392484.0,98186.0,plz reply,308495.0
93156,392484.0,98206.0,Devel please reply,308495.0
93156,392484.0,98588.0,you can verify your buy/sell count respectively.. there is a particular pattern in all,318427.0
93156,392484.0,98754.0,we can observe the bull / sell cycle repeats in a particular pattern for every company... as in after particular number of days,318427.0
93156,392484.0,98626.0,"hi sahil, i dont see much of pattern over all, except that signal of buy and sell has reduced in month of March and Dec... what more can be the inference ?",308495.0
93156,392486.0,98187.0,plz reply,308495.0
93156,392486.0,98205.0,Devel please reply,308495.0
93156,392486.0,98233.0,"while joining it is not necessary to join on primary key. However, having a join on key speeds up the joining process. You will still get the desired result without having the Date as key but at the cost of some lookup time.",318576.0
93156,392486.0,98478.0,"I have created a master table. It's created , I did not get error. Primary key is needed or not.",314183.0
93156,392488.0,98204.0,Devel please reply,308495.0
93156,392497.0,98194.0,yes,318017.0
93156,392497.0,98589.0,yes to create a column with previous values,318427.0
93156,392551.0,,nan,
93156,394825.0,,nan,
93156,394822.0,,nan,
93156,394823.0,,nan,
93245,392457.0,,nan,
93245,392456.0,,nan,
93246,392464.0,,nan,
93246,392463.0,,nan,
93247,392462.0,,nan,
93247,392460.0,,nan,
93247,392461.0,,nan,
93249,392466.0,,nan,
93249,392467.0,98176.0,Actually I am trying to update multiple tables in a single UPDATE command. I need to use table_name.column_name in order to SET values.,310522.0
93249,392468.0,,nan,
93249,392512.0,,nan,
93357,392875.0,,nan,
93357,392848.0,,nan,
93239,392429.0,,nan,
93204,392301.0,,nan,
93367,392918.0,98347.0,how to get buy and sell signals,308432.0
93367,392918.0,98416.0,Thank you,308432.0
93367,392918.0,98414.0,Use window frames and Lag function. You need to generate the signal BUY when 20 DAY MA becomes greater than 50 DAY MA. but it will be BUY only for the 1st time it becomes higher. In all subsequent higher 20 DAY MA you need to generate HOLD. Now when for the 1st time in any subsequent day 20 DAY MA becomes lower than 50 DAY MA then it will be SELL. For all subsequent records when 20 DAY MA is lower than 50 DAY MA it will be HOLD. So basically whenever there is a 'transition' of 20 DAY MA to higher or lower comparative to 50 DAY MA there will be BUY or SELL. otherwise it will be HOLD.,318429.0
93367,393010.0,,nan,
93365,392914.0,,nan,
93365,393075.0,,nan,
93348,392792.0,,nan,
93096,391937.0,,nan,
93096,391943.0,,nan,
94511,398252.0,,nan,
94511,398396.0,,nan,
93372,392946.0,98393.0,Thanks Ashish,315423.0
93317,392751.0,,nan,
93322,392700.0,,nan,
93322,392701.0,,nan,
93322,392824.0,,nan,
93250,392470.0,98226.0,oh sorry!,300706.0
93250,392473.0,,nan,
93250,392475.0,,nan,
93250,392476.0,98225.0,removed it . still the same error,300706.0
93379,393087.0,,nan,
93379,392975.0,,nan,
93379,393004.0,,nan,
94657,399313.0,,nan,
94657,399357.0,,nan,
94657,399529.0,,nan,
94659,399395.0,,nan,
94659,399345.0,,nan,
94659,399307.0,,nan,
94659,399310.0,,nan,
94659,399480.0,,nan,
94659,399810.0,,nan,
95309,403170.0,,nan,
95309,403259.0,100663.0,"Thanks Nagaraju. Yes, you are right. I just watched the complete video and did the assignment again.",301643.0
95309,403277.0,100764.0,Please submit it for reval.,319721.0
93382,392974.0,98367.0,"So No need make that 1 day,2 days etc MA as null right? can we keep that ?",301648.0
93382,393000.0,,nan,
93361,392880.0,,nan,
93361,392917.0,,nan,
93361,393013.0,,nan,
93207,392323.0,,nan,
93207,392349.0,98066.0,"I am pretty sure we need to submit Buy Sell Hold signal for all stocks not just bajaj. So without UDF, how do you plan on generating signals for other stocks. Let TA clear this please.",313515.0
93207,392343.0,,nan,
93207,392355.0,98076.0,thanks Vipul,313515.0
93207,392356.0,,nan,
92053,386858.0,96667.0,"Thanks but I am looking for the float column whose value is null, also in real time we can get every type of data not just string.",307843.0
92053,386858.0,96680.0,"True... In real time scenario we can face multiple issues. You can use string to upload the data and for further analysis you can convert it back to Float, Double or int as per your requirement.",308439.0
92053,386672.0,,nan,
92053,386941.0,96689.0,how can we handle this if two columns have values with null?,310598.0
92053,386941.0,96699.0,", then same command for another column",317811.0
93384,392985.0,,nan,
93384,392996.0,,nan,
93384,393029.0,,nan,
92556,389900.0,,nan,
92556,389907.0,,nan,
92556,391384.0,,nan,
92556,390043.0,97945.0,am getting syntax error,308495.0
92556,390043.0,97944.0,"hi ankit, can u check what mistake i am doing here create table bajaj2 as (select Date, closeprice, CASE when 20 Day MA > 50 Day MA and lag(20 Day MA) < lag(20 Day MA) then 'buy' when 20 Day MA lag(20 Day MA) then 'sell' else 'hold' ; END as Signal) from bajaj1;",308495.0
93412,393152.0,,nan,
93412,393150.0,,nan,
93412,393151.0,,nan,
93409,393200.0,98437.0,Worked thanks,310507.0
93409,393148.0,98430.0,Then you need to update the column using update functions and change its datatype.,319721.0
93409,393148.0,98429.0,"Yes, I tried that already, it shows 0 records imported.",310507.0
93398,393144.0,98428.0,"You can do anything with he first 50 actually because it doesn't really make sense to carry out the given analysis on these rows. So, you can ignore these while addign the signal.",319721.0
93414,393164.0,,nan,
93186,392250.0,98260.0,It should be hold as appropriate data for previous day is not available to conclude the signal.,311729.0
93186,392255.0,,nan,
93186,392258.0,,nan,
93186,394012.0,,nan,
93402,393143.0,98444.0,"when i put the date in function it still shows ok, but in the return select command it shows correct.",314678.0
93402,393143.0,98546.0,still getting the same,314678.0
93402,394045.0,,nan,
93416,393168.0,,nan,
93416,393188.0,,nan,
93416,393354.0,,nan,
93439,393332.0,,nan,
93439,393334.0,,nan,
93439,393389.0,,nan,
93439,393483.0,,nan,
93439,393561.0,,nan,
93439,393724.0,,nan,
93419,393198.0,98446.0,For the Calculation of signals ? Do I need to avoid non 20th and 50th row ?,315423.0
93419,393208.0,98449.0,For the Calculation of signals ? Do I need to avoid non 20th and 50th row ?,315423.0
93419,393309.0,98455.0,yes..19 and 49 rows are anyways Nulls..so can be ignored,307710.0
93419,393309.0,98448.0,For the Calculation of signals ? Do I need to avoid non 20th and 50th row ?,315423.0
93419,393340.0,,nan,
93419,393331.0,,nan,
93436,393336.0,98473.0,"They have not mentioned anywhere, but good to have in that path. Thats way it will become generic path. Thanks",315679.0
93436,393361.0,98474.0,I don't think so. They have written that they will run the SQL. How will they can run if we use Workbench import tools.,315679.0
93436,393386.0,,nan,
93436,393562.0,,nan,
93448,393383.0,98477.0,"Yes, I have put comments in my sql file where ever necessary for better understanding of the code.",313691.0
93448,393383.0,98470.0,You mean we need to comment the whole code? Such a confusion and unnecessary requirements from Upgrad ( if we need to comment),304814.0
93448,393388.0,,nan,
93448,393438.0,,nan,
93448,393559.0,,nan,
93448,393722.0,,nan,
93544,393933.0,,nan,
93490,393629.0,98534.0,"Lovish, since we need atleast 20 days closing price and 50 days data of closing price to calculate the 20MA and 50MA respectively, so for the first 50 days it is not worth to generate the signal. So we can either delete those rows for signal generation or keep them as NULL",319876.0
93490,393711.0,,nan,
93481,393571.0,,nan,
93546,393903.0,,nan,
93546,393987.0,,nan,
93546,394106.0,,nan,
93546,394169.0,,nan,
93542,393893.0,,nan,
93542,393905.0,,nan,
93542,394064.0,,nan,
93573,394142.0,,nan,
93573,394144.0,,nan,
93573,394154.0,,nan,
93573,394275.0,,nan,
93550,393983.0,,nan,
93550,393970.0,,nan,
93550,393992.0,98621.0,"Hello Naren, Please close the SQL connection and reopen. It will work.",320195.0
93550,394020.0,98628.0,Enjoy :),320195.0
93550,394153.0,,nan,
93550,394163.0,,nan,
93550,394189.0,,nan,
93463,393467.0,98615.0,"update stocks.bajaj_auto set Date = str_to_date(stocks.bajaj_auto.Date,'%d-%M-%Y'); There are some syntax issues, I fixed them and it worked..",316926.0
93463,393460.0,,nan,
93463,393509.0,,nan,
93483,393590.0,98526.0,Updated the date field type as text and it worked.,318493.0
93483,393590.0,98525.0,Date field should be text.it also has null row so change those column as text data type else you will get only 888 records but total records excluding the header is 889,300687.0
93483,393609.0,,nan,
93483,393597.0,98527.0,It worked.Is it required to change the data type of date field from text to date time later?,318493.0
93483,393597.0,98530.0,Str_to_date function to change the text to date,300687.0
93483,393597.0,98529.0,yeah did you get all the 889 records.if you have got 888 records there are 2 column with null values change that to text data type.,300687.0
92770,390668.0,,nan,
92770,390675.0,,nan,
92770,390723.0,,nan,
93581,394183.0,,nan,
93581,394214.0,,nan,
93581,394271.0,,nan,
93581,396130.0,,nan,
93589,394501.0,,nan,
93589,394261.0,98673.0,Thanks my observation too,311857.0
93589,394296.0,,nan,
93560,394032.0,98629.0,not sure if i can do that in such short time with no background,308495.0
93560,394032.0,98658.0,I can suggest that don't think too much. convert dat in some simple graphs and you will automatically get some observations.,311686.0
93560,394113.0,,nan,
93560,394161.0,,nan,
93560,394180.0,,nan,
93588,394212.0,,nan,
93588,394233.0,,nan,
93588,394320.0,,nan,
93588,394298.0,,nan,
93588,394224.0,,nan,
93600,394353.0,,nan,
93600,394378.0,,nan,
93600,394358.0,,nan,
93586,394206.0,98660.0,I am not able to alter the column data type. When I imported the csv file the column name is having space by default.,303228.0
93586,394206.0,98985.0,"You can use ` ` back ticks to include the column name with sapces. like, `bajaj auto`",319721.0
93586,394267.0,,nan,
93591,394287.0,98667.0,can you give some hint?,300687.0
93591,394287.0,98671.0,ok thanks,300687.0
93591,394287.0,98668.0,1. Provide the script with all details 2. Like create Table 3. Mentiontion you have imported the data so that during cheeking same can be performed,307843.0
93591,394293.0,,nan,
93591,394292.0,98672.0,thanks Nishan,300687.0
95295,403066.0,,nan,
95295,403257.0,,nan,
95295,403272.0,,nan,
95295,404133.0,100859.0,"But, If my concern is genuine and it comes out that TA says it's invalid request, will penalty be still applied?",318329.0
95295,404133.0,100858.0,Thanks Sahil.,318329.0
112639,486001.0,,nan,
112639,486004.0,,nan,
112639,486012.0,,nan,
112639,486507.0,,nan,
112618,485950.0,,nan,
112618,485980.0,,nan,
112618,485982.0,,nan,
112618,486097.0,,nan,
112618,486385.0,114760.0,Only if same column has entries belonging to different units its an issue,308437.0
112618,486385.0,114769.0,Agreed. Earlier i did not realised this..,311117.0
112618,486644.0,,nan,
112625,485978.0,,nan,
112625,486339.0,,nan,
112625,486049.0,,nan,
112775,486513.0,,nan,
112775,486636.0,,nan,
112775,487004.0,,nan,
112785,486536.0,,nan,
112785,486523.0,114794.0,Pleasure:),302738.0
112785,486523.0,114933.0,Yes. We should at least have 70% of the data.,302738.0
112785,486523.0,114783.0,thank you.,312376.0
112785,486523.0,114932.0,"hi naseem, What is the minimum % of rows we can retain for this assignment? >70% is fine?",308437.0
112785,486523.0,114934.0,thanks :-),308437.0
112785,486631.0,,nan,
112785,486548.0,,nan,
112785,486582.0,,nan,
112785,486711.0,,nan,
112785,487246.0,,nan,
112785,488067.0,,nan,
112802,486570.0,114790.0,Seen this earlier also. But predict() also returns probablities.,304319.0
112802,486577.0,,nan,
112802,486650.0,,nan,
112802,486610.0,,nan,
112820,486629.0,,nan,
112820,486640.0,,nan,
112820,486670.0,114843.0,More than 5 are there: 1. How did you hear about X Education 2. Search 3. Magazine 4. Newspaper 5. Digital Advertisement 6. Through Recommendations 7. Newspaper Article 8. X Education Forums 9. Receive More Updates About Our Courses 10. Get updates on DM Content 11. I agree to pay the amount through cheque 12. Newspaper Article,315560.0
112820,486670.0,114923.0,"This is about the columns which have ""NO"" only.",311117.0
112820,486748.0,,nan,
112820,486815.0,114995.0,Well in Newspaper and Newspaper Article we have both No and Yes. So straightaway those cannot be dropped.,301114.0
112820,487016.0,,nan,
112821,486639.0,,nan,
112821,486637.0,,nan,
112821,486672.0,,nan,
112830,486689.0,,nan,
112830,486686.0,,nan,
112830,487043.0,,nan,
112747,486555.0,114787.0,Yes Tried PCA but giving me only 0.52 accuracy,304692.0
112747,486555.0,114789.0,I am getting an ROC score of 0.88. Hope u have scaled the numeric columns and taken care of the Null values.,304319.0
112747,486555.0,114853.0,ofcourse. this is the score after regression only.,304319.0
112747,486555.0,114845.0,PCA does not give accuracy. It finds the variance in your data and gives the components most important. Results of PCA should be fed to your regression model.,318438.0
112747,486411.0,,nan,
112747,486428.0,,nan,
112843,486752.0,,nan,
112843,486835.0,,nan,
112843,486813.0,,nan,
112843,486761.0,,nan,
114450,493469.0,,nan,
114450,493746.0,,nan,
114450,494586.0,,nan,
112857,486884.0,,nan,
112857,486834.0,,nan,
112857,486799.0,,nan,
112859,486833.0,,nan,
112859,486858.0,,nan,
112859,486880.0,,nan,
112859,486822.0,115000.0,yes. right.,311686.0
112859,486822.0,114994.0,So can we drop this column to proceed since i see it has around 78% missing data?,301114.0
114452,493788.0,,nan,
114452,493466.0,,nan,
114452,493743.0,,nan,
112881,486894.0,,nan,
112881,486926.0,,nan,
112889,486908.0,,nan,
112889,486916.0,,nan,
112889,486903.0,115005.0,country is an important column and hence I feel cannot be dropped - i only dropped corresponding rows..i know country column cannot be imputed. TAs pls confirm,308437.0
112889,487105.0,,nan,
112897,486933.0,,nan,
112897,486952.0,,nan,
112897,487743.0,,nan,
112898,486951.0,,nan,
112898,486929.0,,nan,
112898,486958.0,,nan,
112898,487040.0,,nan,
112898,487106.0,,nan,
112898,487976.0,,nan,
112900,486930.0,,nan,
112900,486950.0,,nan,
112900,486959.0,,nan,
112900,486960.0,,nan,
112900,487038.0,114929.0,I didnt get you Ram..,308437.0
112900,487038.0,115001.0,find it iterseting and wise,318005.0
112926,486998.0,114900.0,and yes we are suppose to make a Lead score column after the predictions.,302738.0
112926,486998.0,114895.0,Thankyou,312019.0
112926,487008.0,,nan,
112924,486991.0,,nan,
112924,487041.0,,nan,
112924,487104.0,,nan,
112947,487302.0,,nan,
112947,487304.0,,nan,
112947,487374.0,115027.0,Thanks for the clarification.,310511.0
112923,486993.0,114888.0,Are you sure? because i had read somewhere (earlier) that test data should not be altered with the mean and std of the entire dataset. hence confused. :(,317998.0
112923,486993.0,114894.0,It was my understanding. But when I searched more I also find the opinion is divided. I found this link bit useful. https://www.quora.com/Should-scaling-be-done-on-both-training-data-and-test-data-for-machine-learning-Can-one-do-scaling-on-only-the-training-data,317991.0
112923,486993.0,114897.0,Thanks for the link. It does have some useful insight. :) Let's wait for the TA's opinion as well.,317998.0
112923,486993.0,114899.0,"Yes, I hope TA's will clarify this valid doubt.",317991.0
112923,486993.0,114992.0,@Harshit @Ramu Most Welcome :-),317991.0
112923,486993.0,114991.0,Thanks for the link Vipul.,310481.0
112923,487000.0,114889.0,"So, if both Train and Test data have to be Scaled, then why not before the splitting??",317998.0
112923,487000.0,114893.0,ok thanks,317998.0
112923,487000.0,114892.0,You do have that option too...,318605.0
112923,487037.0,114910.0,ok thanks,317998.0
112923,487092.0,,nan,
112923,487352.0,115068.0,"Even scaling done afer split on train data. for evaluation we are applying scale transform on test data also. So infact if we do for data before split, no need to scale on test data before evaluation",312019.0
112957,487126.0,,nan,
112957,487132.0,,nan,
112957,487128.0,,nan,
112958,487216.0,,nan,
112958,487371.0,115028.0,"The only difference in the answers to the 2 questions, that I find is that in question 1 , you have to cite top 3 variables (numeric or categorical) and in question2, you have to mention top 3 categorical variables. Is that the gist of it?",310511.0
112958,487371.0,115108.0,Thanks TA for the answer - however some things are still unclear (1) our model does not have three of the original variables. What should we do? Change our model so we can have three of the original variables? or take the prefix of the derived variables (e.g. Lead Source_Google so original variable = 'Lead Source') (2) it would be really helpful to understand the thinking behind asking the first 2 questions in the word document. More specifically what is the point of asking Question 1? What importance does it signify; I can understand question 2 (3) does it have to be positive value of the co-efficients in the model or can it also be negative? ie. is the absolute value the most important thing? To me it makes sense that even if some feature has a high negative value (e.g. Last Activity_SMS Sent) has a coefficient of '-2.33' and it is the highest absolute value coef then that should be in the 'top 3' Again thanks for clarifying these doubts so that our responses are as that expected of the markers.,300694.0
112958,487371.0,115199.0,"eg. Lead Source_Google counted as one variable In model co-efficient can positive or negative but for Q1, you should take only positive co-efficient variables.",344894.0
112958,487371.0,115202.0,"In Q1 ""variables in your model which contribute most towards the probability of a lead getting converted"" is important. In your final model eq. you might have variables with positive and negative coefficient but with positive coefficient , they are toward the predict 1. Please let me know if you have any other question on this.",344894.0
112965,487157.0,,nan,
112965,487225.0,,nan,
112965,487637.0,,nan,
112967,487203.0,115107.0,This is a group assignment and not individual - so you can't compare timing of individual assignment with that of group. Our group started working on this assignment before we started working on individual assignment for PCA,300694.0
112967,487159.0,,nan,
112967,487169.0,,nan,
112967,487315.0,,nan,
112967,487322.0,,nan,
112967,487634.0,,nan,
112994,487349.0,115816.0,Approximately 30%,301121.0
112994,487349.0,115812.0,"in point 1 , what is that certain percentage ?? or can we take it as 30",318772.0
112994,487341.0,,nan,
112994,487413.0,,nan,
112994,487348.0,,nan,
112994,487724.0,,nan,
112991,487303.0,,nan,
112991,487295.0,115003.0,"whats the formula ,(no of conversion/no of leads )*100 or it is a different metric ,plz guide regarding this",318005.0
112991,487411.0,,nan,
112991,487630.0,,nan,
112991,487739.0,,nan,
112991,487399.0,115110.0,To expand .. yes Lead conversion rate (LCR) is simply leads converted/total leads but this does not signify accuracy rate (AR) at all. You could get your predictions completely wrong and predict LCR = 80% but AR at much less,300694.0
112991,487399.0,115106.0,I don't think this is correct,300694.0
113012,,,nan,
113018,487372.0,,nan,
113018,487385.0,,nan,
113019,487421.0,,nan,
112945,487111.0,,nan,
112945,487134.0,,nan,
112945,487231.0,,nan,
112546,485680.0,114622.0,ok. How do you decide whether you need to use RFE or PCA? RFE brings down no of features - you get top N features PCA combines original features into linear combo.,308437.0
112546,485680.0,114609.0,Thanks Nitesh J,320687.0
112546,485674.0,114608.0,Thanks Manasi,320687.0
112546,486070.0,,nan,
112546,487987.0,,nan,
113034,487419.0,,nan,
113034,487513.0,,nan,
112838,486708.0,,nan,
112838,486718.0,,nan,
112838,486724.0,,nan,
112838,486756.0,,nan,
112838,486848.0,,nan,
113072,487686.0,,nan,
113073,487625.0,115088.0,correction https://community.rapidminer.com/discussion/32592/normalising-data-before-data-split-or-after https://hackernoon.com/what-steps-should-one-take-while-doing-data-preprocessing-502c993e1caa,306010.0
113073,487556.0,,nan,
113073,487682.0,,nan,
113073,487731.0,,nan,
113259,488947.0,,nan,
113259,488492.0,,nan,
113259,488556.0,,nan,
113259,488624.0,,nan,
113100,487672.0,,nan,
113129,487836.0,115109.0,Thanks Nitesh !,310467.0
113129,487938.0,115127.0,this is not what the TA said - the TA said it was Precision which is where the confusion is coming from,300694.0
113129,487942.0,,nan,
113106,487675.0,115087.0,welcome :),317998.0
113106,487675.0,115086.0,I was also thinking on same logic..thanks for concurrence,318335.0
113115,487750.0,,nan,
113115,487719.0,,nan,
113114,487713.0,,nan,
113114,488009.0,,nan,
113116,487716.0,115090.0,Ok Harshit..I didn't understand the clubbing part..can you explain some more on how will that help?,318335.0
113116,487716.0,115091.0,"Sure, for example take the country column. There could be values like ""India"", ""USA"", ""China"" etc etc. Now, if the total count of India is 700 (assuming) and USA, china have low counts such as 4 or 5 in the entire column. Then you could make 2 bins such as ""India"" and ""outside india"". then you can club all the other countries under the rubric ""outside india"". this would reduce the number of columns considerably. hope that helps.",317998.0
113116,487716.0,115093.0,welcome :),317998.0
113116,487716.0,115092.0,Oh great..nice idea...we can check ratio wise importance..thanks,318335.0
113116,487758.0,,nan,
113116,487971.0,,nan,
113116,487972.0,,nan,
113119,487736.0,,nan,
113119,487761.0,,nan,
113134,487842.0,,nan,
113134,487846.0,,nan,
113134,487874.0,,nan,
113154,487928.0,,nan,
113169,488010.0,115175.0,"yes,i agree",318005.0
113169,488010.0,115170.0,That means its fine if sensitivity falls because of precision. Thanks for clearing my doubts.,304319.0
113169,488080.0,,nan,
113169,488154.0,,nan,
113169,488959.0,,nan,
113175,488262.0,,nan,
113175,488019.0,,nan,
113175,488124.0,,nan,
113175,488022.0,,nan,
113175,489607.0,,nan,
113190,488102.0,,nan,
113190,488144.0,,nan,
113190,488120.0,,nan,
113190,488295.0,,nan,
112439,485639.0,,nan,
112439,485275.0,,nan,
112439,485392.0,,nan,
112439,485161.0,114645.0,Thanx Vipul.. its really giving a good sight.,311117.0
112439,485686.0,,nan,
112439,485771.0,114639.0,"I think we can drop both of them, as we need the important variables which influence the leads to convert which will in turn help us in getting our lead scores",301655.0
112439,485771.0,114674.0,We needs to find 'Hot Leads' rows. So may be we can keep 'Lead Number '?,312019.0
112439,485771.0,114711.0,Lead number is just an identifier for leads procured. Hot deals are the potential leads which are to be identified. I think we won't require an identifier for identifying those and can drop it,316147.0
112439,485921.0,,nan,
112439,486194.0,,nan,
113201,488205.0,,nan,
113201,488145.0,115207.0,This gives you a good idea of the distribution of text data element present and help you plan your next action.,312199.0
113201,488208.0,115159.0,Thanks Harsha,312096.0
113210,488311.0,,nan,
112613,485964.0,,nan,
112613,485917.0,114840.0,We can use it but we shouldn’t. In this case he asked whether he can use it or not so I said yes. In the other one I suggested him not to. If he still wants to he can.,302738.0
112613,485917.0,114828.0,"This is confusing - both of these links you have posted actually explicitly say NOT to use PCA and the TA has approved THOSE answers. Yet here you have said YES yiou CAN use PCA, and TA has also approved that answer. So that is confusing. Furthermore, you yourself have suggested not to use PCA, here: https://learn.upgrad.com/v/course/208/question/112821 your words ""i would suggest you not to use PCA. instead go for RFE.""",300694.0
112613,485928.0,,nan,
112613,486182.0,,nan,
112613,486846.0,,nan,
113216,488254.0,,nan,
113216,488258.0,,nan,
113216,488261.0,,nan,
113216,488950.0,,nan,
113216,488291.0,,nan,
113216,488672.0,,nan,
113216,489657.0,,nan,
112919,487036.0,,nan,
112919,487049.0,,nan,
112919,486982.0,114881.0,What if a column is important for analysis ? Then also would you suggest to drop that column ?,317991.0
112919,486982.0,114884.0,"Agreed to some points.....What if instead of treating select values as null, we impute value for 'select' and that column eventually be found important ? We would not know this if we drop that column upfront.",317991.0
112919,486982.0,114882.0,"The variable might seem very important for analysis. But, a column having MORE than 40% nulls (or even 30% for that matter) simply wont have enough imformation to help analysis. That is the precise reason why we treat null values, right? so, a column might be very important but if it does not contain sufficient information, how can we even analyse it? I dont think it would help in analysis if it lacks information. Just a personal opinion.",317998.0
112919,486982.0,114885.0,"Agreed. But, if you impute 40% or 30% of the values in a column, You will simply be introducing a bias in the data. 40% is a lot of imputation. In my opinion, imputation should be done only when the missing values are low (max 10% maybe ?) . But, agreed it is subjective and this is just my opinion. I am no expert. :)",317998.0
112919,486982.0,114887.0,True. let's see what the TAs have to say about this. :),317998.0
112919,486982.0,114886.0,Even I am not an expert :-). Just trying to find answer.....Might be possible this kind of scenario can come in future while working in company.,317991.0
112520,485961.0,114709.0,"Since we cannot do an outlier treatment for Categorical Variables , we need to separate numerical columns and perform the outlier treatment.",306729.0
112520,485961.0,114687.0,why this separation needed? you can do on original dataset itself,308437.0
112520,485533.0,114582.0,PCA or RFE right? We can use either of them,308437.0
112520,485533.0,114589.0,I think we should use one of them.,308639.0
112520,485533.0,114633.0,We should use RFE in this case.,318756.0
112520,485681.0,114613.0,yes standardisation has to be done agreed.,308437.0
113217,488290.0,115174.0,thanku,318005.0
113219,488274.0,,nan,
113219,488286.0,,nan,
113219,488304.0,115179.0,"I think it's the other way around: Q1 => Occupation, Country & Lead Source Q2 => Student, India & Google",310974.0
113221,488268.0,115171.0,"that only I'm asking, Precision on Test data or accuracy?",317073.0
113221,488268.0,115172.0,On test,311254.0
113221,488306.0,,nan,
113221,488368.0,,nan,
113221,494704.0,,nan,
113221,488392.0,,nan,
113223,488277.0,,nan,
113223,488287.0,,nan,
113231,488346.0,,nan,
113231,488363.0,,nan,
113231,488518.0,,nan,
113231,489613.0,,nan,
113234,488334.0,,nan,
113234,488362.0,,nan,
113237,488350.0,115190.0,Can you please elaborte?,310974.0
113237,488410.0,115238.0,"Thanks, was helpful",310974.0
113237,488410.0,115260.0,Nicely explained !!,318756.0
113237,488410.0,115258.0,Wouldn't it be appropriate to optimize Precision for Q4 so that we can minimize False Positives and maximize True Positives?,318329.0
113123,487781.0,115136.0,I agree too as per my response to your post .. not sure why they are asking for precision ..,300694.0
113123,487781.0,115100.0,I already have visited the link. And I still have doubt. Why Precision and why not Sensitivity ?,317991.0
113123,487781.0,115101.0,Precision = True Positive / Total True Predictted If Precision is 80 % it means that whatever you are predicting 1 from that 80% will be correct. So your 80% will be conversion rate,344894.0
113123,487781.0,115102.0,"Precision is probability that a 1 predicted is actually 1, so we are finding that out of all 1 predicted as 1 80% time we should correctly predict ? Do correct me if I am wrong.",317991.0
113123,487781.0,115130.0,"Even i think, Sensitivity is a better metric. As sensitivty means from the actual converts, how many have been correctly predicted. Whereas, precision means, from the predicted converts, how many have converted. This does not take into account customers which have converted but not predicted as converts. So the company miss out on those.",304319.0
113123,487791.0,,nan,
113123,487831.0,115111.0,"yes this is also how I would interpret the meaning of target lead conversion rate, and not precision Depending on business requirements, Precision is probably what we should be aiming for so I would say the assignment question is wrong for asking for target lead conversion rate. Asking for a target lead conversion rate of 80% doesn't make sense as it is like asking for a lower precision rate just so that we predict 80% leads to be converted via our model. Only way this makes sense is that CEO wants this level so that his salespeople have extra work to do (considering the current actual rate is less than 40%)",300694.0
113123,487935.0,115142.0,Make sense. Thank you so much for detailed clarification. This really cleared my doubt.,317991.0
113123,487935.0,115143.0,"It took me also a while to figure it out. Hope in right direction. Need good example of Precision and Sensitivity,",317514.0
113123,487935.0,115145.0,Yeah. Thanks again,317991.0
113123,487935.0,115161.0,"Hi, this is accuracy then?",301643.0
113123,487935.0,115178.0,Accuracy is (Actual yes Predicted as Yes + Actual No predicted as No) / Total Leads. In the above case we are not using no False values,317514.0
113123,487935.0,115208.0,This would be precision? TP/(TP+FP) ?,318329.0
113123,487935.0,115213.0,but take care that recall value is also good,317514.0
113123,487935.0,115212.0,yes,317514.0
113123,488946.0,,nan,
113123,489114.0,,nan,
113249,488422.0,115318.0,Thank you. In which case it is ok? Not a problem?,316416.0
113249,488431.0,115399.0,"Then you should not get, Please try the below code for prediction y_test_pred = final_model.predict(X_test_sm)",344894.0
113249,488431.0,115395.0,Yes,316416.0
113249,488431.0,115287.0,Any idea what that might be?,316416.0
113249,488431.0,115359.0,did you standardize your data ?,344894.0
113262,488488.0,,nan,
113262,488568.0,,nan,
113267,488513.0,,nan,
113267,488653.0,,nan,
113267,489204.0,,nan,
112525,485760.0,,nan,
112525,485638.0,,nan,
112525,485963.0,,nan,
112525,485684.0,114624.0,"if u drop rows, u will lose around 45% of data so better drop columns.",308437.0
112525,485701.0,,nan,
112525,485568.0,114599.0,Losing a column is better than losing half of the rows.We can neither delete nor impute the rows corresponding to such large number without losing a lot of data or introducing a heavy bias.,305655.0
112525,485765.0,,nan,
112525,486298.0,,nan,
112525,487073.0,,nan,
113275,488577.0,,nan,
113275,488560.0,115342.0,"You have to merge train and predicted. Note down the number of records after outlier treatment Later check if the merged data frames have the same number of rows. If so , you are in the correct path.",301121.0
113275,488560.0,115339.0,do we need to merge the two sets i.e. train and tests set and then merge with the main data ?,305650.0
113293,488644.0,,nan,
113293,488667.0,,nan,
113293,488766.0,,nan,
113293,488743.0,,nan,
113293,489592.0,,nan,
113302,488752.0,115233.0,I checked those variables it has all 0 values on it.Nothing else then 0,314197.0
113302,488756.0,,nan,
113302,488757.0,,nan,
113302,488762.0,,nan,
113304,488991.0,115278.0,"Based on accuracy, sensitivity and specificity plot for various probabilities from train data set...",318479.0
113304,488991.0,115290.0,"Aren't we supposed to choose cutoff based on that curve only if we are taking accu, speci and sensi as measure of model? Right?",318479.0
113304,488991.0,115284.0,Then obviously they all will be same because you have chosen cutoff based on that curve.,318329.0
113304,488740.0,115253.0,So I have not done anything wrong? The numbers are correct? Because in most of the examples I have never seen them to be equal. Can someone from the TA pls confirm?,318479.0
113304,488740.0,115341.0,"My personal opinion, it is fine.",307495.0
113304,489233.0,,nan,
113308,488739.0,,nan,
113308,488794.0,,nan,
113321,488825.0,115243.0,ok thank you,305335.0
113321,489291.0,115351.0,ok Thank you,305335.0
113313,488787.0,,nan,
113313,489306.0,,nan,
113313,489214.0,,nan,
112527,485682.0,,nan,
112527,485958.0,,nan,
112527,485598.0,114600.0,Losing a column is better than losing half of the rows.We can neither delete nor impute the rows corresponding to such large number without losing a lot of data or introducing a heavy bias.,305655.0
112527,485635.0,,nan,
112527,485696.0,,nan,
113326,489053.0,,nan,
113326,489012.0,,nan,
113326,488977.0,,nan,
113326,488941.0,115276.0,Yes......lead score is this only.,317991.0
113326,488914.0,,nan,
113326,489653.0,,nan,
113326,489704.0,,nan,
113355,489176.0,,nan,
113355,489058.0,,nan,
113355,489229.0,,nan,
113354,489061.0,,nan,
113354,489153.0,,nan,
113354,489219.0,,nan,
113359,489244.0,,nan,
113359,489175.0,,nan,
113359,489256.0,,nan,
113359,489054.0,,nan,
113359,489601.0,,nan,
113311,488754.0,,nan,
113311,488771.0,,nan,
113311,488803.0,,nan,
113311,489619.0,,nan,
112519,485699.0,,nan,
112519,485685.0,114625.0,"yes i agree. PCA for continuous variables, RFE for categorical variables",308437.0
112519,485547.0,,nan,
112519,485562.0,,nan,
112519,485571.0,,nan,
112519,485628.0,,nan,
112519,485670.0,114626.0,Where has TA confirmed this Manasi?,308437.0
112519,485900.0,,nan,
112519,486633.0,114931.0,yes converting categorical variables to derived metrices and then converting those to dummy variables.,313767.0
112519,486633.0,114806.0,Finally some support...! Binning categorical? You mean continuous right? Categorical would anyway be converted to dummy variables...,318479.0
112519,486474.0,114808.0,Categorical variables would anyway be converted to numerical dummy variables...,318479.0
113402,489267.0,,nan,
113474,489719.0,,nan,
113474,489575.0,,nan,
113474,489477.0,,nan,
113474,489421.0,,nan,
113495,489579.0,,nan,
113495,489665.0,,nan,
113495,489980.0,,nan,
113495,489669.0,,nan,
113502,489624.0,,nan,
113515,489773.0,,nan,
113515,489820.0,,nan,
112433,485133.0,114545.0,"Calculate all matrics and choose the matric based on your use cases. Like, you want the best prediction of 1, so based on your knowledge choose the matric and decide the probability threshold",344894.0
112433,485133.0,114544.0,We need to use Precision/Recall or Sensitivity/Specificity for this case study? Or can use one of them?,310974.0
112433,485706.0,,nan,
112697,486229.0,114734.0,We have removed that column.. 46% is too high a null percentage to impute/ignore. Also there is no login explained behind them to impute,300698.0
112697,486229.0,114723.0,the Asymetrique index/profile index and score have almost 46% nulls. is there a way to impute them?,318438.0
112697,486316.0,,nan,
112697,486367.0,114752.0,Thanx for guidance.. got a good sight..,311117.0
112697,486584.0,114901.0,"Mark the Null values as 'Other' to solve the Nulls issue, as this tag looks like an important one. My understanding",312093.0
112697,488204.0,,nan,
112837,486717.0,114822.0,Thanks a ton!!!,300723.0
112837,486705.0,114818.0,i could delete rows using is null. thanks buddy. how to replace null values with some other data for similar kind of object data type,300723.0
112837,486757.0,,nan,
112932,487029.0,115004.0,occupation is an important column - i followed similar approach by grouping blanks/select under others,308437.0
112932,487030.0,,nan,
112932,487031.0,114905.0,the column in question contains less than 3k null values,302738.0
112932,487031.0,114909.0,"oh! in that case only option is to either remove the rows that have those null values or impute with a statistical data such as mean, median or mode. :)",317998.0
112932,487031.0,114911.0,"how can we possibly find mean, median or mode of a categorical variable?",302738.0
112932,487031.0,114912.0,"if it is a categorical variable, then mode can be found, in my opinion.",317998.0
112932,487031.0,114913.0,I don’t really know. You might be correct.,302738.0
112932,487031.0,114914.0,"I am :p lol. see..mean median you know. now mode is basically the value that has the highest frequency/count. for eg, in a column if therr are 50 A, 20 B, 30 C then A becomes the mode since it occurs the maximum no. of time. so, if there are like 10 values missing, you can impute them with A. since A was the mode. hope that helps. :)",317998.0
112932,487031.0,114915.0,Thanks. But I know what mode is. I meant i didn’t check if this can be done in python. Anyways I appreciate you for taking out time to write the answer:),302738.0
112932,487031.0,114916.0,welcome :),317998.0
112932,487031.0,115046.0,"Why this 3000, is there any stander percentage defined?If that hits should be avoided.",300721.0
112932,487044.0,,nan,
112981,487266.0,,nan,
112981,487255.0,,nan,
112981,487974.0,,nan,
113121,487775.0,,nan,
113121,487877.0,,nan,
113121,487876.0,,nan,
113121,487824.0,,nan,
113215,488252.0,,nan,
113215,488253.0,,nan,
113215,488292.0,,nan,
113292,488669.0,,nan,
113292,488746.0,,nan,
113292,488885.0,,nan,
113394,489231.0,,nan,
113224,488332.0,,nan,
113224,488366.0,,nan,
113335,488939.0,,nan,
113335,488912.0,115261.0,"It was changed to 0.3 for the train set after the plotting accuracy parameters. Later, we have used 0.42 for the test set",305653.0
113343,488972.0,115272.0,It is performing well in both train as well as test...,318479.0
113343,489220.0,,nan,
113164,487968.0,115129.0,I agree. But to understand the data it would have been good if unit was known,306736.0
113164,488014.0,115137.0,"May be knowing unit, wouldn't matter for the end goal of this case study. But when we do pair plot /distribution plot, It would have been good for understanding what we are looking at.",306736.0
113164,489610.0,,nan,
113164,488063.0,,nan,
113509,489737.0,,nan,
113509,489778.0,,nan,
112778,486564.0,,nan,
112778,486499.0,,nan,
112778,486569.0,,nan,
113318,488911.0,115262.0,"From the precision/recall curve, we can definitely choose a threshold that can give 80% precision but recall would be low. So, is it expected? Any TA verified this?",318329.0
113318,489070.0,,nan,
112544,485955.0,,nan,
112544,485672.0,,nan,
112544,485679.0,,nan,
112544,485768.0,,nan,
112544,485785.0,,nan,
112544,485845.0,,nan,
112544,486111.0,,nan,
112544,486659.0,,nan,
112544,486127.0,114721.0,Yes even lead number can be dropped,308437.0
112544,486175.0,,nan,
112544,486340.0,,nan,
112544,486383.0,,nan,
114309,492512.0,116069.0,"Instead of removing rows where city is Mumbai and country could be USA/ Canada, why not impute India into Country?",315022.0
114309,492512.0,116077.0,"In fact, they shall be imputed by India. only. If you see first 2 sentences, that is what I meant. I imputed with India in my 'Case Study' While copying and pasting the response from my word file to to response some portion of line is missing,. It should read as ""If country is USA/ Canada and city is Mumbai. impute with appropriate country name ""India"" than remove the rows"" . It is 'than"" and not ""then""; I realized now how it can mislead when a portion of sentence is missing. Thank you, Rohit, for spotting.",301121.0
113171,487999.0,115133.0,"Welcome Anusuya :) For a categorical variable you can use the mode (statistical method) of the column. mode is basically the value that occurs the most number of time in the column. for eg, a col having values as AAAAABBCDD (each letter is a diff value) then the mode would be A. and you can impute tje missing value with A. To find the mode of a colmn you can use the value_counts function of pandas. hope that helps.",317998.0
113171,487999.0,115132.0,"Hi Harshit , Thanks a lot for answering . I have one more question, Lead source is a categorical variable, in that case how statistical methods can be used ?",302750.0
113171,488005.0,,nan,
113315,488805.0,,nan,
113315,488786.0,,nan,
113315,488868.0,,nan,
113315,493333.0,,nan,
112576,485801.0,,nan,
112576,485770.0,,nan,
112576,485783.0,,nan,
112582,485811.0,,nan,
112582,485824.0,,nan,
112582,485841.0,,nan,
112582,485908.0,114867.0,"I think we should drop ""country"" as city column is having city names only from India .",306738.0
112582,486009.0,,nan,
112582,486179.0,,nan,
112582,486255.0,114754.0,"Yes. Just try RFE, it will give you the answer",308437.0
112582,486255.0,114879.0,"Right ,lets decide PCA or RFE to decide",312019.0
111457,480840.0,114018.0,Thanks.,310511.0
111457,484981.0,114714.0,Better to do both and compare the results instead of waiting for TA answering,308437.0
111457,484981.0,114479.0,Thats a good queation. Also what approach did you follow for data cleanup? Some of the columns are having as much as 50% null values.,310511.0
111457,484981.0,114575.0,TA pls answer this question - But I strongly believe PCA is the way - reduce no of features and proceed,308437.0
111457,484981.0,114580.0,Drop the columns/rows having high percentage of NA/Select values. And then convert into dummy. I guess we need to build multiple models. So both PCA and RFE needs to be done.,304319.0
111457,484981.0,114621.0,"Hi Sampath, don't you think, the percentage of data missing is too high to be dropped. If we are removing rows, we wre practically removing 4K records out of 9K. And if we are thinking of removing columns, then its the question of deleting 3-4 columns which I feel are pretty important for the analysis.",310511.0
111457,484981.0,114602.0,"Palit, I have removed the columns having 50% null values and rows which are having null values.",316215.0
111457,484981.0,114603.0,Thank you Madhu and Ruchita for the suggestion. I am using RFE after first iteration of model to find the features and running the models again. I am performing PCA as well after the models iterations to validate the model score is right to get the targetted 80% Lead conversion rate.,316215.0
111457,484981.0,114615.0,"U should do either PCA or RFE, not both - right? TA pls confirm",308437.0
111457,484981.0,114669.0,"Palit, Yes as you mentioned we have not dropped 4k records just dropped columns.",316215.0
111457,484981.0,114670.0,we just dropped around 100 records,316215.0
111457,484981.0,114676.0,Yes Madhu I agree with we need to do either of one PCA or REF. We are going with PCA and then logistics regression. TA can you please confirm on it.,316215.0
111457,484981.0,114686.0,PCA and logistic regression or RFE and logistic regression I think second one - TA pls confirm,308437.0
111457,484981.0,114708.0,i thin k we need to do PCA and Logistic Regression,306729.0
111457,484981.0,114800.0,"Subjective questions ask for most important variable in deciding lead conversion. If we do PCA and it tells us that suppose 12 variables will be enough to build our model but we don't which variables they actually are , so I think we should go with RFE.",318741.0
111457,484981.0,114824.0,we are going with rfe first later using pca for results validation.thank you Madhu and Anshul for suggestions,316215.0
111457,484981.0,115591.0,Thanks Ram. We are done with RFE .,316215.0
111457,484981.0,114926.0,"Guys, obviously you have to go with RFE not PCA as you need to be able to interpret the variables to understand what behaviour of the customer is leading to his/her conversion. With PCA, you can't interpret. So, I would suggest not to waste your time on doing PCA.",310974.0
112626,486087.0,,nan,
112626,486088.0,,nan,
112626,486310.0,,nan,
112626,486551.0,,nan,
112626,486749.0,,nan,
113310,488753.0,,nan,
112665,486099.0,114713.0,But how can we impute values of this column ?,301655.0
112665,486125.0,,nan,
112665,486202.0,,nan,
112665,486312.0,,nan,
112665,486380.0,,nan,
112663,486085.0,,nan,
112663,486095.0,,nan,
112675,486116.0,,nan,
112675,486120.0,,nan,
112675,486634.0,,nan,
112678,486119.0,,nan,
112678,486123.0,,nan,
112678,486198.0,114759.0,"I would suggest the opposite. Convert Select to NaNs. You will not have a dummy variable for NaN values, which will make things simpler. BTW, your answer gave me a hint on a thought that was bothering me. Thanks for that!",318078.0
112676,486115.0,,nan,
112676,486118.0,,nan,
112676,486322.0,114813.0,"The columns you mentioned can be very well understandable for drop. But my question was concerning columns which are not completely identical but nearly identical like ""Do not call"".",311729.0
112676,486322.0,114816.0,"Yes , you can drop",344894.0
112676,486322.0,114844.0,More than 5 are there : 1. How did you hear about X Education 2. Search 3. Magazine 4. Newspaper 5. Digital Advertisement 6. Through Recommendations 7. Newspaper Article 8. X Education Forums 9. Receive More Updates About Our Courses 10. Get updates on DM Content 11. I agree to pay the amount through cheque 12. Newspaper Article,315560.0
112743,486565.0,,nan,
112743,486537.0,,nan,
112743,486632.0,,nan,
112743,486970.0,,nan,
112930,487019.0,,nan,
112930,487028.0,,nan,
112930,487035.0,,nan,
112930,487119.0,,nan,
112930,487354.0,,nan,
113294,488666.0,115303.0,"Even though you will have dummy variables, you will be able to identify the original variables from there. So you can pick 3 unique original variables for Q1",301121.0
113294,488666.0,115302.0,Regarding qustion 1 We are not left with orininal variables in the final model.Should we look at model before RFE,308638.0
113294,488759.0,,nan,
113294,488680.0,115246.0,we should drop country column as it is online course anybody can access it from any where country wont matter much. that what I think,318011.0
113554,490008.0,115476.0,welcome :),317998.0
113554,490008.0,115471.0,Thank you for putting up the link. Good read!,316147.0
112944,487056.0,114920.0,Highly correlated values w.r.t 'Converted' feature variable . Those varaibles to be removed ?,312019.0
112944,487056.0,114921.0,No you need not remove any variables. RFE will automatically select the variables.,302738.0
112944,487056.0,115006.0,Exactly. RFE takes care of this.,308437.0
112944,487056.0,115099.0,https://datascience.stackexchange.com/questions/24975/correlation-and-feature-selection This answer suggests we should remove correlated features before doing RFE for good results.,301652.0
112944,487057.0,,nan,
112944,487120.0,,nan,
113158,487954.0,,nan,
113158,487957.0,,nan,
112537,485700.0,,nan,
112537,485683.0,114627.0,This was requested because for PCA assignment lot of pointers came pretty late.. like treatment of outliers was a question which was clarified on Thursday night after the live session. The 19 pointer approach came pretty late.. many people had to redo a lot of part of assignment.. my request was to avoid this again.. In no way am I requesting for spoon feeding if that's what you make of it. Thank you.,310508.0
112537,485683.0,114628.0,"imho, the 19 point approach was spoonfeeding. it was unfair to just give up the answer like that to the assignment. it shouldn't be given nor expected for future assignments",300694.0
112537,485683.0,114635.0,"i agree with both of you. what Ranjana said is true. i was almost done with my assignment by Thursday and then the 19 points approach came . i had to make a lot of changes according to the steps given as i didnt want to loose marks . Nitesh is right too. this is spoon feeding and unfair. Now that they have done this once, everyone will think they'll provide steps again.",302738.0
112537,485683.0,114722.0,Agree completely with Nitesh. If they hadnt done spoonfeeding then we wouldnt have reworked too :-),308437.0
112537,485683.0,114927.0,"As a data scientist, you would nee to rely on the business team for directions in your approach. It is totally fine but as Nitesh pointed out, replying completely on them isn't expected.",310974.0
112537,485633.0,,nan,
114252,492347.0,,nan,
114252,492319.0,,nan,
114302,492509.0,,nan,
114302,492549.0,,nan,
114302,493195.0,,nan,
113225,488342.0,,nan,
121784,529229.0,120964.0,that was the first step I did...still not working,318479.0
121784,529261.0,,nan,
120700,526185.0,,nan,
122781,534759.0,,nan,
122781,536159.0,,nan,
122781,546024.0,,nan,
122786,536552.0,128854.0,"Rohit, Is it Python-interface for graphviz or the open source graphviz that worked for you.",315797.0
122786,536552.0,129114.0,"Hi Uceymole, I believe it was the open source option that worked for me.",315022.0
122786,536552.0,130159.0,"Thanks, working for me.",315797.0
122786,534158.0,121827.0,done .. did it via anaconda prompt..no success,314313.0
122786,534194.0,,nan,
122394,532461.0,,nan,
122394,533036.0,121650.0,"The above command is not compiling in my system. Tried to install from anaconda. Failed to install any package(pydot,pydotplus,python-graphviz) except for graphviz. Tried with restarting the kernal as well the Jupyter notebook still no success Smoke tested with the path in the code . any other suggestions please",314313.0
122394,533036.0,121652.0,"The above command is not compiling in my system. Tried to install from anaconda. Failed to install any package(pydot,pydotplus,python-graphviz) except for graphviz. Tried with restarting the kernal as well the Jupyter notebook still no success Smoke tested with the path in the code . any other suggestions please",314313.0
122394,533036.0,121737.0,did you extracted the graphviz 2.38 into Program folder ?,344894.0
122394,533036.0,121738.0,"I have downloaded the graphviz 2.38 zip folder from the link mentioned in the platform as well as the graphviz2.38.msi file. Extracted the folders and moved it toProgram Files(X86) Used !pip install graphviz and !pip install pydotplus in jupyter notebook to install the packages. Post all this m getting the above error. Even installed the graphviz in Anaconda but while installing pydot,pydotplus,python-graphviz there werent any package shown. Do i need to even need to install packages in anaconda as well? Please help as I cannot proceed any further with learning",314313.0
122394,532874.0,,nan,
122394,536594.0,122390.0,Yes,314313.0
122394,536594.0,122398.0,"for me, i am getting filenotfound error. Any help on that?",317981.0
122394,536594.0,129961.0,"Koustav & Vipul, Still getting this error. Any help.",315797.0
126103,549604.0,,nan,
115929,501427.0,120434.0,The terminal isn't opening. What exactly could be the issue?,310505.0
115929,501427.0,120678.0,It's working on my system thanks vinay,320687.0
115929,501427.0,120661.0,I followed these steps and it worked perfectly for me.Thanks Vinay,319444.0
115929,501427.0,120700.0,"Yes, it is working. Thanks Vinay",301643.0
115929,501427.0,120752.0,Thanks so much Vinay its work well for me!,311004.0
115929,501427.0,120773.0,I see [CLOSED] on terminal and am not able to type any commands there.,318007.0
115929,501427.0,121380.0,Hi Tez Same thimg here.Could you resolve the problem?,308638.0
115929,501491.0,,nan,
115929,524726.0,120774.0,I have the same problem. Please let me know if you have found a way to get it installed.,318007.0
115929,524726.0,121381.0,I have the same problem.Please let me know,308638.0
115929,527571.0,120775.0,These are not working straight forward hence the issues :),318007.0
115929,527571.0,120792.0,It worked for me :),311227.0
115929,527571.0,120795.0,"Do you have any python installed apart from anaconda suite? could you please run ""where pip"" on command prompt and share the results with me.",318007.0
115929,527571.0,120799.0,The pip commands worked fine when run from Anaconda Prompt tool installed as part of the whole Anaconda/Jupyter suite.,318007.0
115929,527571.0,120810.0,"once we install anaconad/jupyter, pip and conda both command works fine to install packages",311227.0
115929,527571.0,120809.0,"yes. from ""drive:.../anacoda/scripts"" folder",311227.0
115929,528973.0,121387.0,Hi Tez.this helped me out.Thanks,308638.0
115929,529205.0,121581.0,This successfully installs but did not make the libraries available for some reason. Anaconda Prompt utility tool installed as part of Anaconda/Jupyter is a fool proof option.,318007.0
120509,522770.0,,nan,
120509,523815.0,,nan,
122154,531013.0,,nan,
122154,534051.0,,nan,
122154,536115.0,,nan,
121966,530048.0,,nan,
121966,530540.0,121257.0,This worked. Thank you,317149.0
121966,534053.0,,nan,
121248,526610.0,120424.0,"vinays solution below worked for me # Specifing path for dot file. import os os.environ[""PATH""] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz2.38\\bin\\'",308495.0
121248,526610.0,120484.0,"Problem was with the '\' in the path, it is required to use '\\' in my machine...",301113.0
121248,526610.0,120453.0,"Check the folder name you have extracted the files to. When you right click on the zip folder to extract, there's an option to extract to graphviz-2.38. Once you extract to that, update the environment variables and install the packages as mentioned in the steps. You should be able to run the code.",310505.0
121248,526615.0,,nan,
121248,526681.0,,nan,
121248,526696.0,120461.0,What was the process you followed to install graphviz and pydotplus? Have you tried installing through Anaconda? Go to Environment then ensure you search under “All” packages and install from there?,315022.0
121248,526696.0,120425.0,"thanks vinay it worked # Specifing path for dot file. import os os.environ[""PATH""] += os.pathsep + 'C:\\Program Files (x86)\\Graphviz2.38\\bin\\'",308495.0
121248,526872.0,,nan,
121248,527859.0,120718.0,I did,319951.0
121248,529266.0,,nan,
121248,528188.0,,nan,
119674,519823.0,,nan,
121275,527037.0,120641.0,even i use anaconda and getting the same error. I have followed all the instructions not sure where it is going wrong,302741.0
121275,527037.0,120568.0,"I am able to access the prompt now. Have used conda install graphviz command which returned a message that install was successful. But when when importing graphviz in jupyter notebook, I recieve error ModuleNotFoundError: No module named 'graphviz'",319951.0
121275,527037.0,120569.0,why dont you use command prompt and try using the command pip install graphviz? but make sure to follow the instructions prior running graphviz. 1. Downloading the graphviz.zip from the link https://graphviz.gitlab.io/_pages/Download/Download_windows.html 2. Extract the zip file and copy it to c:\Program Files x86 location 3. Add environment variables(Attached screenshot instructions) 4. And then run the pip install command from command prompt. pip install pydotplus pip install graphviz,307176.0
121275,527277.0,120655.0,Didnt work for me. Thanks though,319951.0
120440,522016.0,,nan,
121481,528062.0,,nan,
121481,528063.0,,nan,
121481,528066.0,,nan,
121481,527690.0,,nan,
121515,527783.0,,nan,
121536,527889.0,,nan,
121536,528029.0,120804.0,These are all examples of classification.,318438.0
121536,528250.0,,nan,
121536,528252.0,,nan,
121536,531551.0,,nan,
120981,525374.0,120223.0,Thanks Keerthi AK,320687.0
120981,525374.0,120637.0,"Hi keerthi I have follow the steps of installation packages, but i am not getting the decision tree. Plz help",308639.0
120981,525374.0,120684.0,"Nakul, Can you please let me know the error you are getting? The code that is provided in the module creates a pdf file which contains the decision tree. The pdf will be saved in the folder where your python notebook is saved",310467.0
120981,525374.0,120805.0,I am not able to see the PDF file? where will i get that? all the jupyter notebook is saved in download folder but i am not able to see any PDF there,308639.0
120981,525374.0,120846.0,"Its mentioned in the code that the pdf with the tree will be written in the directory on your system, which can be found using the following commands. import os os.getcwd()",310467.0
120981,527102.0,,nan,
120981,528059.0,,nan,
123241,536163.0,122427.0,i have tried this still getting the error,320606.0
121642,528429.0,120847.0,"did the same, but not working",318377.0
121642,528821.0,,nan,
121642,529074.0,,nan,
123248,535964.0,122426.0,Try installing from command prompt admin mode(if on windows) again with the following command: pip install pydotplus and then install graphviz using pip install graphviz,307176.0
123248,535964.0,122424.0,"yes , i had done that rakesh",320606.0
123248,536174.0,,nan,
121471,527613.0,120669.0,"Paras, I have a dbt regarding the options given . I understand the following condition should be true if a person has heart disease and she has Thal>4.5. Flouroscopy.coloured 0.5 . But the options provided in the question are Flouroscopy.coloured <= 0.5 and Excercise angina< 0.5. Can you please which one of the above 2 is correct?",310467.0
121471,527613.0,120736.0,"Can you please explain, what do you mean by ""% of 0 class"" I am not clear.",320074.0
121471,533848.0,,nan,
123415,536742.0,122454.0,os.path is correctly specified. But still i get that error. Everything is running in administrator mode.,317981.0
123415,536742.0,122683.0,"You have path isssue, Please chechk",344894.0
121362,527094.0,120570.0,It is not helping me. How do i uninstall?,306734.0
121362,527094.0,120754.0,It worked. Thank you.,306734.0
121362,527525.0,,nan,
121362,527091.0,120786.0,"Tez Kurmala, If you read my answer correctly, at first I have pointed out the probably steps that one would have missed to get the error ""InvocationException: GraphViz's executables not found"". Those are 1) Make sure graviz file is unzipped and placed at Program Files (x86) 2) Environment Variable: Add C:\Program Files (x86)\graphviz-2.38\release\bin to User path I had similar error, since I did not follow the above steps correctly. To make it a little bit easier for others , I had pasted all the steps below it.",310467.0
121362,527091.0,120778.0,I do not think you should copy paste the instructions. Most of the folks out there crib because the given instructions are already tried and they do not work. Hence all the noise. What is the use in simply saying the same thing again and again??,318007.0
121608,528255.0,,nan,
121608,528253.0,,nan,
138453,597925.0,133717.0,Could we say that the correlation between the split variable and target variable is too high?,311466.0
138453,597925.0,133748.0,i dont think so.,317998.0
138453,599391.0,,nan,
122292,532253.0,121489.0,i think i confused my question.. i understand what you said but my question is why specifically we check tree performance only on validation dataset which is very small for a performance metric.. and rather do it on train dataset?,316349.0
122292,532253.0,121616.0,"Hi Hemant, Actually, your model is already trained on training data, and your tree is created based on training data. So if you validate on train data it will not work. You always check on validation data set",344894.0
122705,533579.0,,nan,
123026,534784.0,,nan,
122141,531066.0,121743.0,"Hi, If the variable is ordinal then you can do label-encoding for regression as well. If the variable is not ordinal then please go through the OHE",344894.0
122141,531066.0,121730.0,TA please clarify,305650.0
121986,530042.0,121152.0,"Thanks still i am getting below error: pls help --------------------------------------------------------------------------- InvocationException Traceback (most recent call last) in () 5 6 graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) ----> 7 Image(graph.create_png()) ~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py in (f, prog) 1795 self.__setattr__( 1796 'create_' + frmt, -> 1797 lambda f=frmt, prog=self.prog: self.create(format=f, prog=prog) 1798 ) 1799 f = self.__dict__['create_' + frmt] ~/anaconda3/lib/python3.6/site-packages/pydotplus/graphviz.py in create(self, prog, format) 1958 if self.progs is None: 1959 raise InvocationException( -> 1960 'GraphViz\'s executables not found') 1961 1962 if prog not in self.progs: InvocationException: GraphViz's executables not found",300735.0
121986,530042.0,121217.0,"Hi, Please check below link, I think you have problem with your graphviz path https://datascience.stackexchange.com/questions/37428/graphviz-and-pydotplus-not-working",344894.0
121986,530232.0,,nan,
122410,533724.0,,nan,
122410,532570.0,,nan,
122410,546889.0,124167.0,"but the option ""sample solution"" is not visible",318429.0
119784,519653.0,,nan,
119784,519497.0,,nan,
121967,529959.0,,nan,
121730,529028.0,,nan,
119882,519716.0,,nan,
119882,519871.0,,nan,
119882,520881.0,,nan,
119882,528289.0,,nan,
119882,532778.0,121952.0,"Yes, it is confusing",301113.0
121304,,,nan,
122188,531366.0,,nan,
122188,531580.0,,nan,
122188,532246.0,,nan,
121438,,,nan,
121451,528584.0,,nan,
121451,527587.0,,nan,
121451,529573.0,,nan,
121451,529797.0,,nan,
121451,536941.0,,nan,
123123,535326.0,122102.0,"Hi Vinay, Thanks for the response. But a model performing better on test data, doesn't mean that the model accuracy increases, right?",312093.0
123123,535326.0,122156.0,"Here the statement is in comparison with a model which is un pruned i.e., complex and overfitting model.",313826.0
121520,528261.0,121173.0,"Hello Sambit, i tried the above and it worked. However, can you please explain what is the purpose of these commands as they are not explained in the lectures? Neither do i remember using such commands in previous modules..thanks.",310509.0
123418,536908.0,,nan,
123418,536829.0,,nan,
122118,530781.0,,nan,
122118,531489.0,,nan,
122118,532656.0,,nan,
122510,532869.0,,nan,
122510,533910.0,,nan,
126526,551338.0,,nan,
123666,537752.0,,nan,
123666,553914.0,,nan,
120715,524379.0,120087.0,Ok. Havent reached till that point yet.,310511.0
120715,524379.0,120086.0,No I'm not referring to what professor but this was from one of the non graded questions.,310974.0
120715,524475.0,120094.0,Got it 😀,310974.0
120715,530917.0,,nan,
120715,536482.0,,nan,
122714,533782.0,,nan,
122714,533840.0,,nan,
122714,534845.0,,nan,
122264,531578.0,,nan,
122264,531862.0,,nan,
122264,534879.0,,nan,
122740,533951.0,121754.0,"I think, The values that we provide to the hyper-parameters is more by intuition and experience. Further, we provide a range of values for each of the hyper-parameters and find out the best set of hyper-parameters using GridSearchCV to build our final model.",313826.0
122740,533951.0,121753.0,can we take any of the values in the range randomly or is there any point I missed in the lecture,304692.0
122740,534305.0,,nan,
122740,534358.0,,nan,
123983,538595.0,,nan,
123983,539154.0,,nan,
123983,553913.0,,nan,
122289,531904.0,,nan,
122289,531909.0,121563.0,"thanks...this was helpful. Although, would be good to see TA's response as well.",310509.0
122289,531909.0,121615.0,"Hi Chetan, If you use train test data for random forest, it will easy to identify overfitting of your model. It is always good to do train test split or CV",344894.0
124858,544499.0,,nan,
122261,531579.0,,nan,
122261,531837.0,,nan,
122261,532678.0,,nan,
123567,537639.0,,nan,
123286,535963.0,,nan,
123552,537162.0,,nan,
123552,537217.0,,nan,
123570,537161.0,,nan,
123570,553919.0,,nan,
123577,537478.0,,nan,
123577,538398.0,,nan,
123577,553915.0,,nan,
123473,537087.0,,nan,
123473,537484.0,,nan,
123473,537980.0,,nan,
123473,540821.0,,nan,
123571,537481.0,,nan,
127468,556464.0,,nan,
127468,556826.0,,nan,
127468,556409.0,,nan,
138148,595881.0,,nan,
122885,534681.0,121958.0,"Thanks!!, this link is comprehensive.",314730.0
122666,533414.0,,nan,
121768,528903.0,,nan,
121773,529485.0,,nan,
121773,531494.0,,nan,
121773,531988.0,,nan,
121412,527631.0,,nan,
121412,527265.0,,nan,
121412,527622.0,,nan,
121412,534594.0,,nan,
121412,532016.0,,nan,
123411,537485.0,,nan,
123411,538440.0,,nan,
121926,529711.0,121071.0,"It will depend on implementation you select. One can select (a) every ten percentile and find the best suited value (b) Check accuracy by splitting into three equal parts, check the accuracy. Split further in the zone which gives better accuracy",317514.0
121926,529711.0,121064.0,True...but how algorithm will work...will it keep trying on every single digit in salary?,318433.0
121926,530535.0,,nan,
138152,595710.0,133378.0,Please share the distribution of labels when A=0,344894.0
138152,596964.0,133718.0,Thanks Paras :),311466.0
138152,596964.0,133685.0,"OK Arpit, let me do more research on this",344894.0
138152,596964.0,133552.0,I thought you might get it by reading the question as it was asked in the Final Course exam! I was in same doubt that why I have posted it on the Discussion forum!,311466.0
138151,596992.0,133949.0,Could you please reply back?,311466.0
138151,596992.0,133551.0,Please solve the log part for me as I'm not getting it clearly further! Do it on paper I request that will make it understand more way better than reading the whole blog for it!,311466.0
138151,596992.0,133635.0,"will do, meanwhile please go through the Taylor series, so you can understand the solution",344894.0
138151,596992.0,133770.0,please check the updated answer,344894.0
138151,596992.0,133792.0,Yesterday you confirmed the answer is 95 % now you are saying its 92 %! I mean what are you guys doing !,311466.0
138151,596992.0,133797.0,And Paras Sorry to say I'm sure you haven’t correctly solved it! I have done it on my own!,311466.0
138151,598891.0,133948.0,Could you please reply?,311466.0
138151,598891.0,134007.0,"Hi, Below is the correct formula E = - P1*log2(P1) - P2(log2(P2)) = 0.2864",344894.0
138151,598891.0,134008.0,https://www.saedsayad.com/decision_tree.htm,344894.0
138151,600302.0,,nan,
138151,606976.0,,nan,
110743,476790.0,,nan,
110743,476934.0,113281.0,"Thanks that clarifies. I was reading it as '1.539', '2.345' and '3.565'",317514.0
110743,476934.0,113338.0,You are welcome:),302738.0
110993,478127.0,,nan,
110993,477808.0,,nan,
110774,476888.0,,nan,
110774,476895.0,,nan,
110775,476896.0,,nan,
110775,476898.0,113233.0,Thanks,301121.0
110775,476898.0,113228.0,Very well answered Muthu,334535.0
110772,477676.0,,nan,
110772,476854.0,,nan,
110772,476864.0,113172.0,"Thanks, Can you please bit elaborate more.",317410.0
110772,476902.0,,nan,
110363,475498.0,112914.0,"After 90-degree anticlockwise rotation, negative x-axis became y-axis and positive y-axis became x-axis.",311117.0
110363,475498.0,112908.0,"but the basis vector as given in the problem statement is (1,-1) and not (-1, 1) as shown above",310509.0
110363,475498.0,112909.0,its a 90-degree rotation x is negative and y is positive,318451.0
110363,475498.0,112910.0,"which is why i am getting (-4,3) and not (4,-3)..as x is negative and y is positive",310509.0
110363,475405.0,112901.0,please also look at Question 11...its same approach.,310509.0
110363,475466.0,112905.0,"that does not matter here, as the final i and j values are already shown in the matrix calculation",310509.0
110363,475466.0,112903.0,Could you check at what degrees it has rotated and what is the direction of rotation?,311117.0
110363,475534.0,,nan,
110363,475435.0,113060.0,please follow the comments thread....,310509.0
110363,476213.0,113176.0,"negative x-axis becomes y axis for 90deg clockwise rotation, isn't it ? not for anticlock-wise ?",316211.0
110363,476213.0,113386.0,"i did this on paper. I get (-4,3) if i do ant-clockwise rotation and (4,-3) in clockwise rotation ..the quiz Predictive Analytics >Module 5 >Session 1>quiz 4/11 mention anti-clockwise . Please clarify",314450.0
110363,476213.0,113404.0,"Team, Using your left hand, make a L out of your index finger and thumb, with thumb as base or x-axis, and index as y-axis. The back side of your hand is facing your. The point 4, 3 will be in between the L. Now rotate anticlockwise, with the point not moving, but only index finger and thumb rotating (anti clockwise). The thumb/Xaxis becomes y-axis. Not get the hand back to original position, i.e., rotate clockwise. But this time the designated point also moves. So the thumb is back to original position as base of L and as x-axis. And index finger is y-axis. Only this time the point has moved to quadrant between x-axis and negative y axis. So it will have co-ordinates as (4, -3) This is for simplicity stake. In mathematical terms, note answer by Muthu at link https://learn.upgrad.com/v/course/208/question/110700",334535.0
109953,474101.0,112620.0,"That I have done and found the arrays also. My Question is not that. It is different. I am asking how the values of U, S, and Vt arrived with the books matrix. I mean how it is calculated?",311117.0
109953,474101.0,112657.0,"hi Ranjay, I have gone through this also, even searched many more. Could you tell me what would be the matrix for UUt, S, and Vt from the available data?",311117.0
109953,474101.0,112656.0,"Hello Brijesh, I found good explanation for calculations of U, S, V.T at the following site http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm",306250.0
109953,474530.0,,nan,
109953,474531.0,112721.0,"I am asking about what the matrix used from that table for finding the values appearing in U, S, and Vt. not the steps.",311117.0
110720,476652.0,,nan,
110838,477152.0,,nan,
110838,477322.0,113334.0,thank you anuj,320251.0
110722,476655.0,,nan,
110722,476670.0,,nan,
110722,476674.0,,nan,
110722,476873.0,,nan,
110726,476686.0,,nan,
110817,477132.0,,nan,
110817,477135.0,,nan,
110646,476378.0,,nan,
110646,476392.0,113103.0,"Yes thanks. I figured it out. With full_matrices=True, it returns U as (12,12). But with False, it does dimension reduction.",304319.0
110834,,,nan,
110770,476840.0,113240.0,"I have removed the first column ""Name""",304812.0
110770,476840.0,113226.0,"Hi Vivek, Do respond back with how you fixed the issue.",334535.0
110301,475103.0,,nan,
110301,475293.0,,nan,
110843,477141.0,,nan,
110843,477381.0,,nan,
110843,477319.0,,nan,
110843,477190.0,113670.0,can you post the solution?,302739.0
112996,487344.0,,nan,
112996,487321.0,,nan,
112996,487328.0,,nan,
112996,487668.0,,nan,
110700,477621.0,,nan,
110700,476635.0,113398.0,"in you calculation you are stating that you resultant vector after multiplication is 3,4. actually the initial vector is 3,4. so your matrix would need to be multiplied by 3,4 and resultant matrix would be x,y",306725.0
110700,476643.0,113397.0,you are just repeating what's already written as answer. I have already read that,306725.0
110700,476638.0,113402.0,"Hi Pulkit, Brijesh had answered the question in the link and contributed from his side. That apart, let the team know if are still facing problem. If you draw what is mentioned on a paper, it would become clear.",334535.0
110700,476638.0,113396.0,you are just repeating what's already written as answer. I have already read that.,306725.0
110700,478096.0,,nan,
112716,486376.0,,nan,
112716,486559.0,,nan,
112716,486726.0,,nan,
112716,486661.0,,nan,
112716,486309.0,114855.0,Well answered Jaikrishna.,334535.0
112716,486759.0,,nan,
110953,477717.0,113514.0,"Hi Rishi, Also refer to this blog https://intoli.com/blog/pca-and-svd/",334535.0
110953,477911.0,113416.0,My question is to how to determine themes from the m*n matrix ?,305650.0
110648,476339.0,113095.0,"Thanks. Yes in next question they have give this hint. But when i was checking complete s matrix, structure was not diagonal , there was no zero values and even max weight was around 8. Am i seeing this incorrectly?",311006.0
110648,476339.0,113102.0,If you convert to dataframe and print you would get the figures like this in descending order. 0 1.272792e+01 (This is not 1.273 but 12.73 as you have to multiply by 10 ... e+01) 1 1.057704e+01 ( This is not 1.058 but 10.58 as you have to multiply by 10 ... e+01) 2 8.848261e+00 (This is 8.84 only as it is e+00) Other values are quite less and close to zero.,301121.0
110648,476390.0,,nan,
109891,474099.0,,nan,
110141,474926.0,,nan,
110141,475110.0,,nan,
110689,476516.0,,nan,
110689,476529.0,,nan,
110426,475569.0,,nan,
110426,498303.0,,nan,
110024,474409.0,112703.0,Welcome Pradnya..,311117.0
110024,474409.0,112702.0,Ah got it. Thanks.,311857.0
110024,475193.0,,nan,
110438,475675.0,,nan,
109864,473675.0,,nan,
109864,473682.0,,nan,
109864,476950.0,,nan,
109900,473877.0,,nan,
109900,474097.0,,nan,
109900,475358.0,,nan,
110637,476649.0,,nan,
110637,476735.0,,nan,
110637,476701.0,113385.0,PLease arrange the session after 6:00 pm as many of us working and in office may not able to dedicate time,311386.0
110637,476433.0,,nan,
110637,476845.0,,nan,
111982,482674.0,,nan,
111982,482713.0,114298.0,Ok. Thanx for guidance.,311117.0
111982,482713.0,114266.0,"Hi Brijesh, Whether to get absolute or converted values is part of assignment. Please also check the thread mentioned by Prathap.",334535.0
117354,507139.0,,nan,
109849,473872.0,112568.0,ok i got it. I had to drop the Name column.,303670.0
109849,473629.0,,nan,
109849,475320.0,112862.0,"also make sure the name of the dataframe is ""data""... or change the linalg query to use the name of the DF provided by u...",303670.0
109849,475320.0,112858.0,Do it inplace.,317689.0
109849,476107.0,,nan,
110441,475616.0,,nan,
110441,498277.0,,nan,
111013,477853.0,,nan,
111013,477921.0,,nan,
141903,612840.0,,nan,
141903,613326.0,135860.0,Thanks a lot !!,310179.0
110736,476730.0,,nan,
110661,476391.0,113114.0,"Okay, that means we have to plot a heatmap of the correlation amongst the variables, right ?",301655.0
110661,476391.0,113160.0,yes,318476.0
110288,475088.0,,nan,
110288,475165.0,,nan,
110288,475938.0,,nan,
112027,482932.0,114323.0,"Thank you Pradeep. After performing K mean clustering do we need to merge this dataset to original dataframe . Means from PCA Analysis we got PCA columns as ""PC1 , PC2 .. PCN "". We would using dataset for performing K mean clustering which would assign each record in the dataset to an clusterID . Do we need to merge this proceed dataframe with Original Dataframe . For decision making ?",301108.0
112027,483047.0,,nan,
110491,475748.0,,nan,
110491,475789.0,,nan,
110491,475899.0,,nan,
110491,475933.0,,nan,
110491,476677.0,,nan,
110491,476815.0,,nan,
110491,476887.0,,nan,
110491,476932.0,,nan,
110829,477104.0,113224.0,"thank you. so if random state = 42, then does it mean that behaviour of only 42% of the data will be repeated?",310509.0
110829,477104.0,113232.0,No. . It is just that random numbers will be generated between 0 and 42,301121.0
110829,477104.0,113298.0,"ok, then how to decide what random state to choose?",310509.0
110835,477161.0,,nan,
110835,477390.0,,nan,
110621,476244.0,,nan,
110876,477378.0,,nan,
110876,477890.0,,nan,
110876,477846.0,,nan,
110876,477903.0,,nan,
110926,477561.0,,nan,
111219,478960.0,113652.0,Still not working.dataframe has been standardised.PCA was imported from sklearn.still the same issue,305655.0
111219,479859.0,,nan,
111000,477843.0,,nan,
111000,478492.0,,nan,
110542,475870.0,,nan,
110542,475934.0,,nan,
110542,475951.0,,nan,
110542,478021.0,,nan,
111041,479106.0,,nan,
111051,478099.0,,nan,
111051,478194.0,,nan,
111051,478490.0,,nan,
111051,478312.0,,nan,
111082,478282.0,,nan,
111082,478529.0,,nan,
111082,478486.0,,nan,
111082,478283.0,,nan,
110697,476545.0,,nan,
110697,476559.0,,nan,
113244,488435.0,,nan,
113244,489198.0,,nan,
110558,475968.0,,nan,
111908,482068.0,,nan,
111908,482334.0,114215.0,we don't drop original features - they have been reduced to PCs via the PCA process we drop PCs (which are also called features or dimensions) -> we drop based on some calculations to determine how many PCs best explain the variance in all the data at a level we are comfortable with (50? 60? ... 90%?,300694.0
111108,478439.0,,nan,
111108,478469.0,,nan,
111108,478483.0,,nan,
111108,479707.0,,nan,
112070,482995.0,,nan,
110619,558742.0,,nan,
111419,479994.0,,nan,
111745,481349.0,,nan,
111745,481329.0,113995.0,"Each formed cluster can be mapped against original variable's mean using preferably bar plot for the purpose of analysis; Please ensure you add country after clustering analysis is completed,",301121.0
111745,481329.0,113992.0,"But look at the question, it has been emphasized that it must be combination of PCA and other variables",300735.0
111745,481505.0,,nan,
111617,481668.0,,nan,
112093,,,nan,
141905,612805.0,,nan,
140144,605705.0,,nan,
140144,605947.0,,nan,
140289,606016.0,134832.0,yes cibil score is the one and only criteria to issue credit card and personal. for example if you don't have a credit card or loan. your cibil is 0 and you will not be issued either loan or credit card. but student loans can be or may be given with out cibil score,320606.0
140289,608260.0,,nan,
141058,,,nan,
141082,609477.0,,nan,
141358,610815.0,,nan,
141174,610619.0,,nan,
141174,610414.0,,nan,
141174,610796.0,,nan,
140532,608242.0,,nan,
140714,608940.0,,nan,
140806,608316.0,135021.0,Thanks Rashmi. Indeed a good read!,316147.0
140776,608403.0,,nan,
141727,613465.0,,nan,
140151,608711.0,,nan,
140151,607398.0,,nan,
140151,605971.0,134736.0,Yes but how is macro economic data mapped at customer level?,310509.0
140151,605971.0,134771.0,"Calculation of credit information parameters are not part of this course so we would need to search some online resources, if available, to know how agencies maintain these records.",301560.0
140151,605971.0,134853.0,"ok, waiting for further info",310509.0
140151,605971.0,134887.0,"As mentioned in above comment, this is not part of the course so you can search in other online resources for your understanding.",301560.0
140151,608251.0,,nan,
141665,612310.0,,nan,
141665,612403.0,,nan,
141667,612408.0,,nan,
141651,612398.0,,nan,
141348,610623.0,,nan,
141378,610788.0,,nan,
141136,610690.0,,nan,
141136,611539.0,,nan,
141136,611038.0,,nan,
141397,610692.0,,nan,
141397,610783.0,,nan,
141160,610849.0,,nan,
141160,610099.0,,nan,
141160,611099.0,,nan,
141390,610755.0,,nan,
141390,610852.0,,nan,
141390,611480.0,,nan,
141390,610777.0,,nan,
141340,610697.0,,nan,
141440,610940.0,,nan,
141339,610699.0,,nan,
140722,608958.0,,nan,
140838,,,nan,
140604,608245.0,,nan,
140884,610103.0,,nan,
140884,608997.0,,nan,
140884,609077.0,135427.0,"Hi Shreyas, Selecting / rejecting variables one by one would be lengthy process. Can we use RFE here ?",317991.0
141273,610283.0,,nan,
141273,610223.0,,nan,
141279,610268.0,,nan,
141279,610809.0,,nan,
141279,610306.0,135385.0,"Yeah but if we use PCA then we can't find out important predictor variables, but in the assignment they haven't asked for it too. So really confused if PCA is allowed. Can TA please confirm ?",318756.0
141816,613554.0,,nan,
141502,611240.0,135525.0,i am referring to Sort the data points in decreasing order of the probability of response,320606.0
141502,611462.0,,nan,
141503,611246.0,135541.0,what is the cost of acquisition? Is it cost of call * no. Of contacts? But in problem statement it is given as 1*No. Of contacts.,304319.0
141503,612225.0,,nan,
141825,613568.0,,nan,
141507,611249.0,135496.0,ok thanks,310598.0
141514,611627.0,,nan,
141514,611274.0,135503.0,Thanks Vinay. I also had same understanding. Waiting for TA to verify this.,317991.0
141514,611274.0,135553.0,TA please verify this.,318756.0
141526,612230.0,,nan,
141527,611773.0,,nan,
141527,612250.0,135643.0,which dataset to consider for decreasing order,318780.0
141527,612250.0,135666.0,better to choose whole dataset,301560.0
141642,612396.0,,nan,
141853,613334.0,,nan,
141571,611601.0,,nan,
141571,611526.0,,nan,
141882,612680.0,,nan,
141882,612765.0,,nan,
141882,613694.0,,nan,
141884,613697.0,,nan,
141884,612853.0,,nan,
141884,613329.0,,nan,
141660,612359.0,,nan,
141660,612241.0,,nan,
141890,612704.0,,nan,
141590,611651.0,,nan,
141529,611654.0,,nan,
141536,611502.0,,nan,
141536,612012.0,,nan,
141536,611489.0,,nan,
141536,611384.0,,nan,
141604,612050.0,,nan,
141604,611700.0,,nan,
141915,613345.0,,nan,
141915,613166.0,,nan,
141835,612549.0,,nan,
141835,613332.0,135858.0,Are you sure we need to use the mean of train dataset and if yes then why not mean of test dataset.,310179.0
141835,613332.0,135871.0,"As a matter of integrity, we are not allowed to divulge information about test dataset. Please read the below stackoverflow discussion which explains the intricacies very well https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i/174865#174865",313826.0
141835,613332.0,135906.0,ok got it.. thank you.,310179.0
141887,613348.0,,nan,
141887,612675.0,135731.0,"Thanks, Aniruddha",320687.0
141912,613344.0,,nan,
93753,395173.0,,nan,
93753,395170.0,,nan,
93753,395680.0,,nan,
93731,395000.0,,nan,
93731,395008.0,,nan,
93731,395584.0,99029.0,Yes convert them to either lower or upper case for values to be displayed,303673.0
93731,395584.0,99013.0,Do we needs covert all names to capital. seems everywhere case senitive is an issue ?,312019.0
93756,395169.0,,nan,
93756,395112.0,99041.0,Permalink,329512.0
93756,395112.0,98930.0,"for that, would you consider permalink or name?",318329.0
93756,395166.0,,nan,
93756,395222.0,,nan,
93756,395572.0,99015.0,"Yes, In this case you have to change the case .",314183.0
93756,395572.0,99011.0,As per set difference it checks for case sensitivity and gives even name is same. any option we can set to avoid this ?,312019.0
93756,395641.0,,nan,
93756,395681.0,,nan,
93756,395948.0,,nan,
93756,396002.0,99171.0,"Having worked on this, can comfirm case needs to be handled. The answer is different is surely different from what it was when i tried before accounting for duplicacy due to case.",319302.0
93756,396157.0,,nan,
93628,394556.0,,nan,
93628,394628.0,98750.0,We need to handle encoding here. Replace is not an option here.,317689.0
93628,394628.0,98735.0,which link is mentioned for replacing the character?,310419.0
93628,394656.0,98973.0,"Hi Saurabh, I checked the link provided by you but that says it purges all the non ascii charecters but we want them intact right for our analysis or else we may end up losing data. Any idea what needs to be done for that please ?",308967.0
93628,395011.0,,nan,
93629,394664.0,,nan,
93629,394565.0,,nan,
93629,394575.0,,nan,
93629,394627.0,,nan,
93629,394801.0,,nan,
93629,395608.0,,nan,
93630,,,nan,
93636,394635.0,98738.0,"Thanks, I have, but researching out by all possible options to save time.",313228.0
93636,394635.0,98879.0,Not yet,313228.0
91393,382173.0,,nan,
91393,382840.0,,nan,
91393,383070.0,,nan,
91393,383253.0,,nan,
93634,394663.0,,nan,
93634,395459.0,,nan,
93634,394670.0,,nan,
93634,394672.0,98891.0,Is it after cleaning the master frame or before cleaning master frame?,310419.0
93634,394791.0,,nan,
93634,394867.0,,nan,
93634,395577.0,,nan,
93634,396141.0,99679.0,I think you need to convert permalink to lowercase before merging.,320683.0
92461,389526.0,,nan,
92461,389764.0,,nan,
92461,391179.0,,nan,
93764,395161.0,,nan,
93764,395187.0,,nan,
93631,394649.0,,nan,
93685,394762.0,,nan,
93685,394870.0,,nan,
93687,394805.0,99033.0,all this findings for checkpoint 3 have to be coded in python only right ?,319876.0
93687,394785.0,,nan,
93687,395468.0,98986.0,On objectives page there is link for PDF file which list down the English speaking countries,300718.0
93687,395468.0,99126.0,yes got it thanks,300706.0
93687,395468.0,99253.0,Do we need to code this in python to finding the english speaking countries for cross check with pdf and find the count ?,312019.0
93687,395248.0,,nan,
93687,396593.0,,nan,
93692,394802.0,,nan,
93692,394814.0,,nan,
93692,395847.0,,nan,
93742,395057.0,,nan,
93742,395060.0,98884.0,But inner join may miss some values as there might be few cases where table1 data not present in table2 and vice versa,318579.0
93742,395060.0,98954.0,sorry go through the below link https://learn.upgrad.com/v/course/208/question/93606,303673.0
93742,395060.0,98952.0,This has been discussed earlier and TA has replied on the same Kindly go through the link: https://learn.upgrad.com/v/course/208/question/93725,303673.0
93742,395175.0,,nan,
93742,395374.0,,nan,
93742,395677.0,,nan,
93696,394817.0,99144.0,Does data cleaning is requried before caluclating the mean()? for checkpoint 2,318846.0
93696,394842.0,98820.0,any input on how to go for it?,313676.0
93696,394842.0,98832.0,DF col/1000000,301115.0
93696,394842.0,99145.0,Does data cleaning is requried before caluclating the average funding()? for checkpoint 2,318846.0
93696,394842.0,99274.0,Only the raised_amount_usd amount cleaning enough or we need to do all other column dropna also? the master_frame['raised_amount_usd'].dropna(inplace=True),311227.0
93696,394857.0,,nan,
93696,395183.0,,nan,
92425,389693.0,,nan,
92425,389633.0,,nan,
93708,394868.0,,nan,
93708,394873.0,98824.0,Between means inclusive.. either you use in sql or any db language,317811.0
93708,394873.0,98825.0,Yes agreed for sql or other db languages. But the write up is in plain English and in such case between could grammatically mean either inclusive or exclusive. Better let the TA put it clearly whether inclusive or exclusive.,313826.0
93708,394873.0,98827.0,I am also expecting it to be inclusive. Just want to confirm the same with TA!,313826.0
93708,394873.0,98826.0,"As you wish, but i am expecting it to be inclusive.",317811.0
93708,394927.0,,nan,
93708,395240.0,,nan,
93743,395174.0,98928.0,so what should be the approach? to drop the null or replace with 0?,320103.0
93743,395063.0,98888.0,"Hi Venkatesh, Average will differ if we replace value with 0 and drop column because the row count on top of which average will be calculated will change.",320103.0
93743,395678.0,,nan,
93709,394883.0,98830.0,Hmm...let TAs reply to this. Also I guess you meant 5 to 15M.,310511.0
93709,394883.0,98833.0,Right 5-15M. Let's wait for the TA's answer.,313826.0
93709,394883.0,98839.0,"Yes, I confirm",329512.0
93709,396472.0,,nan,
93710,394881.0,98829.0,Thanks.,310511.0
93710,394881.0,98987.0,"Vinay, how many primary sectors are there in master frame which don't have a match? Can u tell me to cross check...thanks",308437.0
93710,394881.0,98845.0,"I am rethinking this and feel that using inner join may not necessarily be the right option to make. Inner join will basically drop rows from master_frame that doesn't have matching values from the mapping dataframe. Instead if we impute null values to 'Others' that will mean we are making a bold design decision but saving important rows. So its sort of a trade off. The case study wants us to make appropriate realistic assumptions wherever required. In light of this, can we not go with the latter option?",310511.0
93752,395084.0,,nan,
93752,395171.0,,nan,
93752,395181.0,,nan,
93752,396912.0,,nan,
93752,397897.0,,nan,
93726,394974.0,98875.0,thanks got it,303674.0
93726,394974.0,98867.0,hi are you sayingit should be like this primary sector - 1 column then major sector column and then the value and then major sector should be written against primary sector along with value ?,303674.0
93726,395299.0,,nan,
93727,395463.0,,nan,
93727,394971.0,,nan,
93727,395009.0,,nan,
93727,395251.0,,nan,
93727,395627.0,,nan,
93763,395165.0,,nan,
93763,395163.0,,nan,
93763,395190.0,,nan,
93612,394498.0,98718.0,"thanks for the reply, you mean to say that we have to do analysis on whole data frame without filtering out English speaking countries. any idea about checkpoint 3 which I have posted above check point 4.",306243.0
93612,394853.0,,nan,
93612,395023.0,,nan,
93612,395245.0,,nan,
93606,394395.0,98693.0,"Yes. I have converted the permalink and company_permalink to lower case. master_frame = pd.merge(rounds2, companies, how=""inner"", left_on='company_permalink', right_on='permalink') This is how I am merging actually.",310522.0
93606,394395.0,98700.0,"Now it is working correctly. Actually, I renamed the data frame and ran the merge query again. Now it is showing values for all the columns. Thank you!",310522.0
93606,394395.0,98770.0,they are already all appearing as lower case....is there a need to change it again?,310509.0
93606,394395.0,98938.0,If we inner join some data which is present in one dataframe and not present in some other dataframe will be missed out right?,318579.0
93606,394431.0,,nan,
93606,394553.0,98998.0,how do you convert the values in a column to lower case?,316416.0
93606,394574.0,98815.0,we have to join the df's on the derived field COMPANY rt.. not on the actual columns permalink=company_permalink?,318846.0
93271,392749.0,98412.0,"Fine, I don't see an option to delete the question.",317990.0
92787,390781.0,97533.0,it is not what we think.. it is what exactly they want😀,301113.0
92787,391177.0,,nan,
92787,390769.0,97528.0,observations is a generic word.. they have to be very specific..,301113.0
92787,391060.0,,nan,
92787,391131.0,,nan,
92787,395460.0,,nan,
93768,395231.0,,nan,
93768,395238.0,,nan,
93768,395256.0,98945.0,FOR CSV import also do we use the same encoding,312016.0
93768,395256.0,98966.0,encoding changes file by file basis. It is not a generic.,318458.0
93768,395288.0,,nan,
93768,396019.0,,nan,
93778,395262.0,99044.0,Yes.,329512.0
93778,395267.0,,nan,
93778,395257.0,,nan,
93778,395282.0,,nan,
93778,395304.0,,nan,
93778,395659.0,,nan,
93778,395802.0,,nan,
93784,395470.0,,nan,
93784,395292.0,,nan,
93784,395297.0,98955.0,true.. agree to this.. thanks!,316349.0
93784,395297.0,98951.0,"Everyone can have his/her assumptions which are not correct/incorrect. That is why we have assumptions section in each software document such as requirements doc, design doc etc.... If I take an assumption and my result matches with those assumptions, then logically I would be right. However, if someone else has different assumptions and try to verify my data, then same correct result would become incorrect for him/her. I have simple request from course faculty to please lay down the assumptions for assignments/ case studies. Because of such petty things, we all loose lot of time",304814.0
93784,395376.0,,nan,
93784,395566.0,,nan,
93784,395602.0,,nan,
93784,395645.0,,nan,
93784,395688.0,99050.0,"Maddula, this is not about graded question. It is simply the assumption. You and I both can have different assumptions. we were taught to replace null with most occuring values, we were also told to remove null rows, we ere also told to ignore the rows, we are also told to remove the rows having >5 nulls. See, we can not just implement what was taught to us, Such assumptions should be laid down clearly as it would lead to different result",304814.0
93784,395688.0,99051.0,"Yes, it will that's why you can go with your assumptions and learning. The evaluation will take care of all the possible methods. Sorry if I sounded loud. You are free to clean data as you like but It should be logical.",329512.0
93784,395688.0,99053.0,"So, if we lay our assumptions clearly, and output matches with those assumptions then everything would be good. Please advise",304814.0
93784,395688.0,99055.0,"Yes, Our team will read your assumptions and if they find it logically correct, then your answer is correct. Also, we will not match the answer fully, we need it to be approximately equal to the expected one.",329512.0
93784,395688.0,99057.0,Cool. Welcome.,329512.0
93784,395688.0,99056.0,Perfect. That makes lot of sense. Thank You,304814.0
93784,396205.0,,nan,
91084,380381.0,95725.0,"A0lytics' is is no way 'Analytics' :D I agree data quality is an issue, but this kind of quality can only be fixed by approximate text search which I don't think would be the scope.",318381.0
91084,380381.0,95726.0,"Also, just to prove a point. Here are some categories which maybe missing: Deep Information Technology Enterprise Hardware Experience Design",318381.0
91084,380381.0,95813.0,"Those are missing, let them be missing, but make sure that the data quality issue is solved.",329512.0
91084,380381.0,95814.0,You need to replace 0 with NA,329512.0
91084,380381.0,98219.0,Is this ok to be done on CSV before loading ?,316211.0
91084,380381.0,98844.0,No. This is required in Python only.,329512.0
91084,380381.0,98859.0,"If i replace 0 with NA then in some cases like Enterprise 2.0, how to handle that case?",310419.0
91084,380381.0,98863.0,"mapping['category_list'].apply(lambda x: x.replace('0', 'na'))",329512.0
91084,380381.0,98870.0,while running above code shows error 'float' object has no attribute 'replace',310419.0
91084,380381.0,98953.0,What kind of challenge is this?,304814.0
91084,380381.0,99349.0,"To replace the '0' with 'na ' we can use the below mapping['category_list'] = mapping['category_list'].str.replace('0','na') please let me know if this doesn't work.",310481.0
91084,380381.0,99374.0,This would not be enough because there are certain valid 0 as well like Enterprise 2.0 which as per the above transformation gets converted to Enterprise 2.na,318381.0
92990,391630.0,,nan,
92990,391642.0,98890.0,Don't strings come under object category? ..if so how can we convert?,305335.0
92990,391689.0,98273.0,"try executing the above command taking into consideration the alphabet case of both the column 'permalink' and 'company_permalink'. Convert both the columns to either lowercase or uppercase and then execute the above command, I hope you will get the desired result.",318374.0
92990,391754.0,,nan,
92990,391947.0,98574.0,"This is a challenge that you need to solve, I will not be able to help you more.",329512.0
92990,391947.0,98568.0,i did both. I tried to correct the encoding issue using your method as well as mentioned in the case study brief. But its still showing error. I also ran the above query and i am getting 114949 rows when it should be 0. Please suggest how to fix encoding issue...this is a technical problem and i think also impact my subsequent analysis.,310509.0
92990,392971.0,98368.0,as per the description of the assignment you need to remove all non-ascii characters - a link is provided in the case study description pages on how to do that,300694.0
92990,392971.0,98566.0,i did that also but still i can see such characters,310509.0
92990,392971.0,98681.0,@nitesh reading the file using an encoding will not remove non-ascii characters ?,306735.0
93827,395334.0,,nan,
93828,395333.0,98979.0,Yes... I've checked the averages with and without Null values there is no difference .,303673.0
93828,395333.0,98978.0,Definitely not! 0 will affect averages .. NaN or NULL will not affect average,300694.0
93828,395363.0,,nan,
93828,395565.0,,nan,
93828,396686.0,,nan,
93832,395339.0,,nan,
93832,395451.0,,nan,
93832,395545.0,,nan,
93832,395696.0,,nan,
93834,395361.0,,nan,
93834,395383.0,,nan,
93835,395348.0,,nan,
93835,395382.0,99054.0,"Please comment with your resolution, so that others may look into it.",329512.0
94249,397006.0,99360.0,"thanks ,i will drop them,but i should be dropped before country analysis or during sector ,i was in confusion ,here low percentage of blanks not affecting results but results can be refined",318005.0
94249,397006.0,99361.0,"Yes, your question is correct, that's why I am always saying to clean your data properly after you have joined countries and round file. I hope this answers your question.",329512.0
94249,397006.0,99362.0,"But I'm in doubt now; don't stick to this case study question but in general if they were not updating some info in database and that suddenly is being dropped and affects your results; is it correct?? like the case he mentioned, subject a and b, if second row for a is dropped then b will be highest.. that's why I said in my post that it depends on business understanding.. it could be a genuine reading which is not updated in database!! please correct it..",316349.0
94249,397006.0,99363.0,So this is a very rare condition that your primary key is missing.,329512.0
94249,397006.0,99364.0,It can be that's why we need to consult to the data provider every time we take a data cleaning step.,329512.0
94249,397006.0,99365.0,exactly that's what I said in my post.. the main answer to Abhishek!! can you recheck it for anything wrong in that??,316349.0
94249,397006.0,99366.0,yes i thought of both possiblities,318005.0
94249,397006.0,99459.0,:),318005.0
94249,397006.0,99368.0,"you asked a wonderful question Abhishek, no doubt in that.. and we should be looking over things on a higher level then sticking to just a case study and assignment.. we'll are experienced professionals.. :)",316349.0
94249,397007.0,,nan,
93838,395360.0,,nan,
93838,395564.0,,nan,
93838,395573.0,,nan,
96473,411019.0,102022.0,Got the point. Thanks for clarification.,317991.0
96473,411019.0,102036.0,Exactly......without doing this step I got same result as provided in sample solution. So this step should have been optional.,317991.0
96473,411019.0,102032.0,"@Naveen, Even I've received the same feedback point and have one more doubt regarding the same. For doing this cleanup, we have gone through the whole column to observe the deviations and found that we need to replace 0's. While doing this, we have observed that the column values are in sentence case and did not feel the need to convert them to lower or upper case. Now, when you say about a larger dataset, even in that case, we should be observing the column data for deviations just as we did for the current dataset. If we observe that there are variations in casing of the column at that time, we would convert them to a common case. I do not understand why it is compulsory to convert to a common case when we are doing the stuff manually and making sure everything's right.",318329.0
93840,395521.0,,nan,
93840,395370.0,,nan,
93840,395417.0,,nan,
93843,395406.0,,nan,
93843,395544.0,,nan,
91499,382836.0,,nan,
93848,395405.0,,nan,
93848,395413.0,,nan,
93848,395543.0,,nan,
93851,395415.0,,nan,
93851,395499.0,99085.0,"After following the solution mentioned in the URL, I am getting ""'ascii' codec can't decode byte 0xc2"" error.",308434.0
93851,395561.0,,nan,
93851,395563.0,,nan,
93851,395597.0,99027.0,yes and it is wrong from whom I copied as it was partially correct .I will keep in mind that other can be wrong,319869.0
93851,395597.0,99018.0,seems you copied the command from others answers,317811.0
93863,395516.0,98993.0,how to know it's encoding type ?,318427.0
93863,395516.0,98994.0,There is no Python library that tells the encoding of a file. You have been provided a list of encoding and you'll have to basically Google and try which one works for the file,300748.0
93863,395708.0,,nan,
93863,395505.0,99003.0,yes i got the solution now..,318427.0
93863,395505.0,98999.0,Yes.. We need to use for csv file also as it contains special characters too,303673.0
93863,395505.0,98989.0,But i guess encoding was used for .txt files only... Isn't it?,318427.0
93863,395505.0,99071.0,kudos!!,303673.0
93865,395530.0,99031.0,Thanks,310504.0
93865,395539.0,99032.0,Thanks,310504.0
93860,395498.0,98992.0,It depends on what columns are concerned. Think if you may lose any vital data if you drop the column or row. you need to make an informed decision.,300748.0
93860,395483.0,,nan,
93860,395540.0,,nan,
93860,395706.0,,nan,
93004,391747.0,97848.0,how to map the values as the required details are in form of columns.. just a direction or hint will work..,305129.0
93004,391747.0,97850.0,can you help me with a logic on how to map the values in Mainsector column..,316349.0
93004,391747.0,97859.0,use idxmax function,317811.0
93004,391747.0,97901.0,yes u need to exclude a column which is not integer while taking max,317811.0
93004,391747.0,97864.0,"its throwing an error - ""reduction operation 'argmax' not allowed for this dtype""",305129.0
93004,391683.0,97833.0,Can you please provide some hint on bring the column names from mapping to fill the main sector...,305129.0
93004,391683.0,97820.0,Thanks.. will try..,316349.0
93004,392469.0,99139.0,"using pd.melt function , i have melted my mapping dataframe , now the next next basically proceeds to merging ?",318427.0
93004,392469.0,99154.0,Yes.,329512.0
93004,392469.0,99588.0,Thanks this solved a big problem I was having,300734.0
93012,391744.0,98605.0,"companies = pd.read_csv(""companies.txt"", sep =""\t"", encoding = ""ISO-8859-1"")",310509.0
93012,391744.0,98564.0,both are in lower case but its still not joining,310509.0
93012,391744.0,97858.0,u should either change to lower or upper case in join columns in both dataframres,317811.0
93012,391744.0,97849.0,"I tried to change the name of the column so both are same and then did the merge. In this approach I should get the records using inner merge right? But I am not getting the same. Contrary, when I try outer merge it works and I am getting more data points than the total number of records which is also strange.",310509.0
93012,391744.0,98565.0,i changed the name of the column in companies file so that both the files have same header. I am trying to merge after that but its giving 0 values.,310509.0
93012,391744.0,98567.0,Share your code.,329512.0
93012,391744.0,98571.0,this is for rename: companies = companies.rename(columns ={'permalink':'company_permalink'}),310509.0
93012,391744.0,98572.0,"this is for merge: master_frame = pd.merge(rounds2, companies, how = ""outer"", on =""company_permalink"", indicator = True)",310509.0
93012,391744.0,98575.0,"please note when i tried how = inner, i am getting 0, But when i tried how = outer, i am getting 181317 rows ( is this because of special characters and python is treating it as unique due to encoding issue?)",310509.0
93012,391744.0,98576.0,"Try this: pd.merge(companies, rounds, how=""inner"", left_on=""permalink"", right_on=""company_permalink"")",329512.0
93012,391744.0,98577.0,it should not be inner join.. instead left join with left table as rounds,317811.0
93012,391744.0,98579.0,still coming as 0,310509.0
93012,391744.0,98585.0,"if i try left join, and specify the columns name ( along with the right table) it actually creates 16 columns. what should be specified under ""how""?",310509.0
93012,391744.0,98586.0,have u used correct enco,317811.0
93012,391744.0,98590.0,"I used this as specified in the case study: companies.permalink.str.encode('utf-8').str.decode('ascii', 'ignore')",310509.0
93012,391744.0,98594.0,n before that iso 8859 1 ?,317811.0
93012,391744.0,98606.0,used the above while loading the file,310509.0
93012,393873.0,98583.0,have u used correct encoding,317811.0
93728,395021.0,,nan,
93728,395551.0,,nan,
93877,395603.0,,nan,
93877,395593.0,,nan,
93877,395745.0,,nan,
93878,395743.0,99070.0,thanks,319869.0
93878,395599.0,,nan,
93878,395703.0,,nan,
93878,395904.0,,nan,
93880,395619.0,,nan,
93880,395885.0,,nan,
93880,395695.0,,nan,
90070,,,nan,
93885,395629.0,,nan,
93885,395630.0,99030.0,"Not mentioned specifically. But can change results. Yes, only null values need to be removed.",304319.0
93885,395686.0,,nan,
93885,395715.0,99076.0,should we treat the nulls for other columns like country_code also at this point? or it can be done at checkpoint 3.,304319.0
93885,395715.0,99159.0,You need to figure this out. :(,329512.0
93885,395715.0,99158.0,i guess it can be done at checkpoint 3,310504.0
93885,395883.0,,nan,
90068,375264.0,94797.0,ok achal,318005.0
90068,375264.0,96686.0,"sorry dear ,but i already joined ,had put it long ago ,,,and now dates r also over for self group formation, it was till 15th after that all who were not part of any group were to be randomly alloted a group",318005.0
90068,375264.0,96684.0,"HI Abhishek , This is bhaskar I too you join if you have place. in group :)",314612.0
90068,375264.0,94798.0,plz contact me on abhisheksaurabh2007@gmail.com,318005.0
90068,375879.0,,nan,
90068,386141.0,,nan,
93889,395738.0,,nan,
93889,395650.0,,nan,
93889,395649.0,99122.0,"so you mean to say :identify the heavily funded sectors ,its investment per sector then the below statement is contradictory Create three separate data frames D1, D2 and D3 for each of the three countries containing the observations of funding type FT falling within the 5-15 million USD range. (does it mean we need to pick rows of funding type FT and invested amount 5-15 M in that particluar observation??)",301115.0
93889,396132.0,,nan,
93893,395675.0,,nan,
93893,395809.0,,nan,
93893,395876.0,,nan,
93760,395242.0,,nan,
93760,395164.0,,nan,
93760,395121.0,,nan,
93760,395217.0,99066.0,"It returns zero values when I remove '~'. rounds.loc[rounds['company_permalink'].isin(companies['permalink']), :]",312376.0
93760,395167.0,,nan,
93760,395489.0,,nan,
93760,395562.0,,nan,
93760,395596.0,,nan,
93892,395674.0,99042.0,AttributeError: 'float' object has no attribute 'split' Now it is showing the above error.,318397.0
93892,395674.0,99043.0,Convert your Category list to object,329512.0
93892,395674.0,99045.0,It is already an object.,318397.0
93892,395674.0,99047.0,Remove Null and try.,329512.0
93892,395674.0,99046.0,Remove duplicates and try.,329512.0
93892,395676.0,,nan,
93892,395879.0,,nan,
93871,395711.0,,nan,
93871,395537.0,99000.0,"my question is, what is the point of creating it then? they have specifically asked to create a separate colmn for it",317998.0
93871,395606.0,,nan,
93891,396918.0,,nan,
93891,395668.0,,nan,
93891,395740.0,99082.0,"Hi Abhishek, I think, you have to first identify the sector in which Stark can invest with its funding amount of 5-15million. From the identify sector, we need to identify top 3 English speaking countries. I believe this is the approach. Please rectify?",306243.0
93891,395740.0,99084.0,Yes you are correct.. first we need to identify the investment funding type and for that particular funding type we need to find the countries with maximum investment in that particular funding type.. and identify the top3,305129.0
93891,395880.0,,nan,
93896,395698.0,,nan,
93896,395718.0,,nan,
93867,395513.0,99039.0,The answer to questions 1 and 2 under table 5.1 would vary depending on whether 'Blanks' sector is included or not.,313826.0
93867,395710.0,,nan,
93867,395709.0,99259.0,"Check Point 4 states, and I quote, ""When we say sector analysis, we refer to one of the eight main sectors (named main_sector) listed in the mapping file (note that ‘Other’ is one of the eight main sectors). This is to simplify the analysis by grouping the numerous category lists (named ‘category_list’) in the mapping file."" Which clearly means that SECTOR ANALYSIS refers to EIGHT MAIN SECTORS, and thus ""BLANKS"" SHOULD NOT be a part of it. So, in checkpoint 5, SECTOR ANALYSIS 2, table 5.1 asks ""Total number of investments (count)"" (and sum also). My question was/is , SHOULD INVESTEMENTS WHERE sector is ""BLANK"" (got from mapping the blanks column) be INCLUDED in this Count ??? How can we include it when the question clearly states that SECTOR ANALYSIS refers to EIGHT MAIN SECTORS ONLY ? YET, in answer to the question in this below link TA has verified answer stating that blanks should be included! Link: https://learn.upgrad.com/v/course/208/question/93980 So, Please clarify, wheather to INCLUDE Blanks as a Sector while calculating the count and sum of total investments in the 1st two questions of Table 5.1, Or NOT ? Because, answers in both the approaches vary. So, kindly clarify. Thank You.",317998.0
93867,395709.0,99282.0,"Blank is not a sector as you said, sorry for the confusion. If you are getting blanks, then you need to check how you have merged the files. It should be inner join to merge so that no nulls are merged.",329512.0
93867,395709.0,99297.0,thank you :),317998.0
93867,395709.0,99291.0,"Thank you for your response Sir. So, the approach I am using is, I DROP the blanks column entirely after reading the mapping.csv. and as a result of that, while merging NO NULLS are present. (because that column was only dropped). so no issue of nulls arise while merging. Is my approach correct sir? or, should I recheck it?",317998.0
93867,395709.0,99294.0,"Cool, no issue.",329512.0
93900,395758.0,,nan,
93900,395757.0,,nan,
93901,395756.0,,nan,
93901,395776.0,,nan,
93901,395872.0,,nan,
93899,395784.0,,nan,
93899,395814.0,,nan,
93899,395779.0,,nan,
93899,396242.0,,nan,
93899,396965.0,99354.0,Check the statement in checkpoint 5. An observation is a row in the table...,308637.0
93907,395798.0,,nan,
93907,395805.0,,nan,
93907,395807.0,,nan,
93907,395813.0,,nan,
93907,396116.0,,nan,
93908,395797.0,99079.0,"There would not be any need to remove category as in mapping sheet there is a main sector for Null. However, for country make an assumption and move forward i.e either remove those or replace it with maximum occuring country",304814.0
93908,395812.0,,nan,
93908,395857.0,,nan,
93912,395823.0,,nan,
93912,395839.0,,nan,
93912,395856.0,,nan,
93912,395871.0,,nan,
93912,396003.0,,nan,
93912,396113.0,,nan,
93912,396136.0,,nan,
93912,396585.0,,nan,
93920,395914.0,,nan,
93920,395954.0,99234.0,Shouldn't it be count-wise as asked in Table5.1 Q9?,307509.0
93929,395944.0,,nan,
93929,395951.0,99109.0,Yes all amounts should be considered. Why would you filter any amount. Even a few thousands of dollars are quite large sum of funding for startups.,317689.0
93929,395951.0,99107.0,"there are values in raised_amount_usd in one, two or three digits, should we take this is consideration when doing analysis.",306244.0
93929,395957.0,,nan,
93915,395870.0,99095.0,Can i draw the conclusion as create new column and then map it with master_frame.?,317410.0
93915,395870.0,99110.0,"few columns have more than one sector.If we place those into one cell, merging of dataframe with Master_dataframe will not be successful. I believe",317410.0
93915,395870.0,99106.0,yes,317689.0
93915,395877.0,,nan,
93915,396106.0,99150.0,That main_sector column need to be created based on that 1 . For new columns it has more than one sector (for ex: last column) we should select only first sector. correct me if am wrong.,317410.0
93915,396106.0,99157.0,"one last confirmation, main_sector column values should contain column names having value 1 or categoty_list. If category_list we can rename it Right..",317410.0
93915,396106.0,99151.0,you have to get main sector for primary category list . mostly you will get one main sector.,317845.0
93915,396139.0,,nan,
93936,395971.0,,nan,
93936,395998.0,,nan,
93919,395922.0,,nan,
93919,395956.0,,nan,
93939,395988.0,99130.0,":) whichever way you find logical.. make sure you specify your assumption while submission.. if you ask me, i don't think there is any parameter which can be imputed though..",316349.0
93939,395988.0,99129.0,Get rid of by removing or filling..,300687.0
93939,395988.0,99173.0,thanks,300687.0
93939,395988.0,99165.0,"It's upto you to choose the cutoff, but keep it logical and add this as a comment in your notebook",329512.0
93939,395989.0,,nan,
93941,395987.0,,nan,
93941,395995.0,,nan,
93941,396148.0,,nan,
93982,396004.0,99117.0,do we need to filter amount while creating data frame or after doing the total amount in each sector,320103.0
93982,396004.0,99146.0,Read the requirements again and it will become clear. Refer to excel sheet investment as well.,317689.0
93982,396025.0,,nan,
93982,396674.0,,nan,
93985,396016.0,,nan,
93985,396022.0,,nan,
93985,396075.0,,nan,
93985,396087.0,,nan,
93985,396187.0,,nan,
93985,396213.0,,nan,
91569,383489.0,97026.0,Are these fields available in your master file column catefory list?,329512.0
91569,383489.0,96267.0,"thanks. I got some null cases. and about the spelling issues, that is what I mean when I mentioned that for some values there are similar (but not same) values in Mapping file. for example in case of 'Games' we have 'Game', 'Videosl Games' , 'Gaming' etc. is it expected that we go through each such individual case and rectify it?",311686.0
91569,383489.0,96788.0,Only for those mentioned in Master file.,329512.0
91569,383489.0,96825.0,sorry Maddula. didn't get you. suppose I have 'Games' in master file. should I make it 'Game' or 'Gaming'? there are many such values in master file which are correctly spelled but to associate it with some value in mapping file we will have to tweak the format/spelling. there are multiple options for tweaking as well like in case of Games.,311686.0
91569,383489.0,97045.0,yes. 'Games' is there.,311686.0
91569,383489.0,97214.0,"Then take only Game from the mapping filr, leave rest.",329512.0
91569,383592.0,99344.0,Need clarification on the same,318756.0
91569,383592.0,99357.0,we need to go for inner join.. so only those which are present in both should be considered,311686.0
93988,396127.0,,nan,
93988,396037.0,,nan,
93988,396186.0,,nan,
93991,396126.0,,nan,
93991,396086.0,,nan,
93991,396185.0,,nan,
93999,396085.0,99178.0,thanks,307491.0
93999,396101.0,99172.0,thanks,307491.0
93999,396179.0,,nan,
93999,398324.0,,nan,
94009,396119.0,99180.0,"You can use tools such as notepad++ for this, but that will also not help with the encoding is mixed.",329512.0
94009,396119.0,99170.0,"i want to know efore loading the files into python how do we know exactly which encoding method is used, ie if Latin is used or utf, is there any command in pythor in text file.",315560.0
94009,396155.0,,nan,
94009,396203.0,,nan,
94010,396221.0,,nan,
94010,396129.0,,nan,
94010,396160.0,,nan,
94010,396174.0,,nan,
94010,396199.0,,nan,
94018,396173.0,,nan,
94018,396168.0,,nan,
94018,396182.0,,nan,
94018,396195.0,99217.0,32 GB RAM size,312019.0
94018,396225.0,99216.0,"Same issue after doing all. even reinstalled anaconda. checked other cases like movies earlier assignment, those merge works. not this special case. same this works fine for my team mates of case study.",312019.0
94018,396225.0,99283.0,Get me the complete error message.,329512.0
94018,396246.0,,nan,
94018,396587.0,,nan,
94018,396720.0,,nan,
93914,396138.0,99163.0,I am afraid I have to say that question is not framed properly.,304814.0
93914,396138.0,99164.0,"No, each row in the file is a round, so the sentence makes complete sense.",329512.0
93914,396138.0,99181.0,"Thanks for your feedback, we will look into it, but as of now please ignore it and use the simple average.",329512.0
93914,396138.0,99176.0,"When did I say that each row is NOT a round. That is specifally my question that we need to rely on each round(row) to identify the best investment type. I will challenge that with an example. Suppose you 3 records of investment type Venture and 1 of Seed as below:- companyA venture 5mil CompanyB venture 5mil Compnay C venture 5 mil CompanyD seed 5mil Average of both Venture and Seed would be 5 mil. But if you have to chose the best investment type then it has to be Venture because it has more rows. So if you simply rely on average rather than the number of rounds (rows), so it could lead to a different result.",304814.0
94015,396158.0,,nan,
94015,396175.0,,nan,
94015,396159.0,,nan,
94023,396211.0,,nan,
94023,396244.0,,nan,
94023,396236.0,,nan,
94024,396210.0,99190.0,Thanks,313526.0
94024,396214.0,,nan,
94024,396235.0,,nan,
93265,392912.0,,nan,
93265,392744.0,,nan,
94038,396292.0,,nan,
94038,396291.0,,nan,
94038,396290.0,99198.0,Thankyou. Do we needs to reduce the dataframe size too ?,312019.0
94038,396290.0,99199.0,"this is a very vague question as there are many ways to reduce the dataframe - some valid, and some not so valid; a lot of discussion has already been held in the forums about what rows/columns to drop - perhaps you can search on 'nan' or null or 'drop' and see some of the discussions",300694.0
94038,396290.0,99203.0,Dropping known. My doubt is part of cleaning we should remove those rows of duplicates ?,312019.0
94038,396290.0,99208.0,Perfect. Thank you,312019.0
94038,396290.0,99204.0,think about why would you drop these duplicates? what is the reason for keeping them vs dropping them? look at what is different in these 'duplicate' rows. I think saying anymore would be giving answers to graded qns,300694.0
94037,396278.0,99205.0,"Thanks for the response, I couldn't see the row having value 'FT' in that column",317410.0
94037,396278.0,99214.0,"The various funding types such as venture, angel, seed and private equity is what it is reffered to as funding type.",329512.0
94037,396281.0,,nan,
94037,396319.0,,nan,
94026,396215.0,,nan,
94026,396243.0,,nan,
94026,396973.0,,nan,
94030,396259.0,,nan,
94042,396297.0,,nan,
94042,396318.0,,nan,
94042,396312.0,,nan,
94045,396316.0,,nan,
94044,396310.0,99207.0,Tried but same error...😞😞,308437.0
94044,396310.0,99209.0,can you please share snapshot,317811.0
94044,396332.0,99238.0,Sujeet it's not working that is what is surprising same thing I tried already...pls whatsapp me on 9620888890 ll share screenshot...,308437.0
94044,396332.0,99215.0,by' should come before 'column' right??,308437.0
94048,396329.0,99211.0,*export,318329.0
94048,396330.0,,nan,
94048,396338.0,,nan,
94048,396360.0,99385.0,Pandas has .to_csv and .to_excel methods on dataframe. You could just say df.to_csv('./output.csv') https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html,318381.0
94048,396374.0,,nan,
94054,396372.0,,nan,
94054,396373.0,99220.0,"At least we must know , how can I finally check whether there is any enoding left or not via some particular command.",315560.0
94054,396373.0,99298.0,"Use this: rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] If you can see some non-english characters from the output, then encoding is not yet resolved.",329512.0
94064,396441.0,,nan,
94064,396494.0,,nan,
94064,396737.0,,nan,
94049,396328.0,,nan,
94049,396359.0,,nan,
94049,396375.0,,nan,
94049,396371.0,,nan,
94050,396336.0,,nan,
94050,396327.0,,nan,
94050,396368.0,,nan,
94050,396366.0,99331.0,Thanks for your help,312016.0
94050,396797.0,,nan,
94050,396658.0,,nan,
94052,396365.0,,nan,
94052,396369.0,,nan,
93931,395976.0,,nan,
93931,395977.0,,nan,
93931,395974.0,,nan,
93931,395993.0,99161.0,thanks Vipul. I acheieved it in different way though.,311404.0
93931,395993.0,99128.0,"Hi Vipul, yes there is null value in category_list. Did u remove all the rows having null value?",311404.0
93931,395993.0,99132.0,"Yeah. You can remove the null values, they are anyway not going to be of use to you as they don't belong any of the major sectors.",318397.0
93931,395949.0,,nan,
93931,396142.0,,nan,
93931,396468.0,99235.0,thanks Neha. I got the point.,311404.0
94058,396391.0,,nan,
94058,396446.0,,nan,
94058,396479.0,,nan,
94068,396427.0,,nan,
94068,396419.0,,nan,
94068,396436.0,,nan,
94068,396465.0,,nan,
94068,396499.0,,nan,
94068,396862.0,,nan,
94051,396324.0,,nan,
94051,396333.0,,nan,
94051,396367.0,99219.0,is it ohk to count rows of master_frame after data cleaning for null values in column Raised_Amount_usd ?,308495.0
94051,396367.0,99265.0,I think we should not count null values as there is no investment. We should count only non null values.,301643.0
94051,396869.0,,nan,
94065,396405.0,,nan,
94065,396414.0,,nan,
94065,396440.0,,nan,
94065,396495.0,,nan,
94060,396392.0,,nan,
94060,396395.0,,nan,
94060,396401.0,,nan,
94060,396445.0,,nan,
94060,396482.0,,nan,
94060,396728.0,,nan,
94053,396453.0,99226.0,"But against the Same Country code filter i am getting 3rd value as 52 (in the first graph - TOP 4), but in the next why to i get 3rd value as 20 when i filter on TOP 3?",316036.0
94053,396787.0,,nan,
94062,396397.0,,nan,
94062,396442.0,,nan,
94062,396483.0,,nan,
91334,390542.0,,nan,
91334,381734.0,,nan,
91334,395189.0,,nan,
94066,396413.0,,nan,
94066,396415.0,,nan,
94066,396477.0,,nan,
94617,398963.0,,nan,
94617,398902.0,,nan,
94617,398997.0,,nan,
94617,399046.0,,nan,
94617,399200.0,,nan,
94124,,,nan,
93725,394979.0,99006.0,Do we have to join companies and round2 data frames?,318427.0
93725,394979.0,98856.0,Ok.,310511.0
93725,395025.0,98874.0,True that. We consider it a business requirement.,310511.0
93725,395381.0,,nan,
93725,395486.0,,nan,
93725,395657.0,,nan,
94129,396540.0,,nan,
94129,396534.0,,nan,
94129,397112.0,,nan,
94132,396751.0,,nan,
94132,396549.0,,nan,
94132,396554.0,99241.0,can i drop the blank column once i import it into a dataframe ?,313691.0
94132,396554.0,99247.0,let it be clubbed into blanks.. just try to find the top sectors,316349.0
94131,396749.0,99328.0,Funding type you need to choose among the four options available.,329512.0
94131,396749.0,99327.0,"Here Country we will consider from top 3 english speaking country. Raised amount between (5-15). But what about Funding Type ? This will be the single most suited funding type or all the four (venture, angel, seed, and private equity) . Please clarify..",312518.0
94131,396749.0,99335.0,"1) I believe we already finalized the funding type in a previous checkpoint. 2) I am not clear on how we should store the #of investments and total Investments in the D1/2/3 dataframes. These dataframes are for each startup meeting the criteria (English Speaking Coutry, Investments between 5-15mn and Fund Type determined earlier). However the #investments and sum of investments are aggregate values of the entire dataframe. Where should they be stored?",318438.0
94131,396550.0,99246.0,you can create it in the same row for all the main sectors.. google will help you.. search with right keyword,316349.0
94131,396550.0,99244.0,Thanks Hemant. in D1 itself we need to create multiple columns based on each main_sector for total investment count and total invest amt invested or we can find out separately for each main_sector,319444.0
94131,396538.0,,nan,
94131,396541.0,99239.0,Thanks Ankit but one doubt do need to create pivot if we want to add total investment count and investment amount for each main sector into a different column.,319444.0
94131,396541.0,99248.0,Use transform function as given below: DataFrame['columnnameofcolumntobeadded'] = Dataframe.groupby(['columnnameforgroupby'])['columnnameforaggregatefunc'].transform('aggregaetfunction'),317811.0
94131,396948.0,99340.0,yes.. because we are grouping by main_sector,319444.0
94131,396948.0,99367.0,any guidance on how we can filter a groupby on multiple columns?,312096.0
94131,396948.0,99404.0,Please read about transform functions.,300717.0
94133,396552.0,,nan,
94133,396578.0,,nan,
93500,394387.0,,nan,
93500,393674.0,,nan,
93500,393721.0,,nan,
93500,393757.0,98604.0,"Thanks, Anuj. This is a perfect answer. I imported the Data Frame successfully.",310522.0
93500,393757.0,99073.0,"used company_text_df = pd.read_csv(""companies.txt"", encoding =""latin-1"", sep=""\t""); still getting "" Zzzzapp Wireless ltd. 66365 Ã_x0081_ERON 66366 Ã_x0094_asys 66367 Ä°novatiff Reklam ve TanÄ±tÄ±m Hizmetleri Tic"" in ""permalink"" column. please help",311227.0
93500,393757.0,99081.0,You have to decode this column. The link is given in the case study introduction.,310522.0
93500,393757.0,99108.0,"I have used after the above read_cv command: company_text_df['permalink'].[str.encode('latin-1').str.decode('ascii', 'ignore'), I have used 'latin-1' instead utf-8 but no help, it is showing the special character, it may be the ""extended unicode"" character",311227.0
93500,393757.0,99206.0,"You have to assign this decode statement again to the column from the Dataframe. company_text_df['permalink'].[str.encode('latin-1').str.decode('ascii', 'ignore') This command displays the decoded values but do not assign them to the column. I have faced this kind issue.",310522.0
93500,393757.0,99213.0,yes i did later that. thanks :),311227.0
93500,394394.0,98701.0,Got the solution by using encoding. googled it.,311117.0
93500,394426.0,,nan,
93500,394658.0,98758.0,once created we need to decode columns having special characters for our analysis..? or we expect the data itslef to be loaded in to data frame as English characters only.?,306735.0
93500,394696.0,98804.0,use the hint given in https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe,301641.0
93500,394789.0,99025.0,"Hi Rahul I did useed to fix the encoding issue in my df's but still I'm getting error records if I use rounds.loc[~rounds['company_permalink'].isin(comp['permalink']), :] as per TA this should not return any records if we have corrected the non English characters correct?",306735.0
93500,395194.0,,nan,
93500,395259.0,,nan,
93500,395491.0,,nan,
93500,395773.0,,nan,
93500,393709.0,98774.0,"As column is of object datatype, we are not able to do above step. Please suggest.",320103.0
93500,393709.0,98776.0,"Also, it will just ignore that special character and then leave link with '-' at end. For e.g. /ORGANIZATION/WEICHE-TECH-Å_x0096__x0082_È½¦Ç§_x0091_Æ_x008a__x0080_ (earlier) and after encoding it will become - /ORGANIZATION/WEICHE-TECH- Is this as expected?",320103.0
93500,393709.0,98961.0,"df.Permalink=df.Permalink.str.encode('utf-8').str.decode('ascii', 'ignore') after running this command please check",314197.0
93500,393709.0,98947.0,"tried with df.Permalink.str.encode('utf-8').str.decode('ascii', 'ignore') still facing same issue",319444.0
94135,396580.0,,nan,
94135,396560.0,,nan,
94139,396574.0,,nan,
94139,396573.0,,nan,
94139,396576.0,,nan,
94139,396572.0,,nan,
94558,398520.0,,nan,
94558,398540.0,,nan,
94558,398530.0,,nan,
94119,396504.0,,nan,
94119,396509.0,,nan,
94119,396746.0,,nan,
94151,396611.0,,nan,
94151,396632.0,,nan,
94151,397201.0,,nan,
94151,397238.0,,nan,
94151,398050.0,,nan,
94151,398254.0,,nan,
94151,398509.0,,nan,
94154,396620.0,99387.0,"use str.replace('0','na')",310481.0
94154,396619.0,,nan,
94154,396621.0,,nan,
94154,396629.0,,nan,
94154,396666.0,,nan,
94154,396755.0,,nan,
94149,396601.0,,nan,
94149,396617.0,,nan,
94149,396599.0,,nan,
94157,396713.0,,nan,
94157,396660.0,,nan,
94157,396644.0,,nan,
94156,396640.0,,nan,
94156,396659.0,,nan,
94156,396664.0,99268.0,She wants to import .text file data to excel,317991.0
94156,396664.0,99269.0,"Could be a typo. I don't see any reason for doing that, and thus I mentioned ""IF"" you want to import to jupyter.......",317998.0
94156,396664.0,99270.0,May be she want to import companies.txt file into excel to cross verify findings,317991.0
94156,396664.0,99272.0,Even I am not sure so I also mentioned if that is the case :-),317991.0
94156,396664.0,99271.0,Ahh..true. could be. :),317998.0
94156,396756.0,99303.0,May be she wants to import companies.txt file into excel to cross verify findings,317991.0
94156,397562.0,,nan,
94156,396989.0,,nan,
94485,398158.0,,nan,
94485,398157.0,,nan,
94172,397268.0,,nan,
94172,396723.0,99292.0,Thnx. One more clarification - Different plots means different sheets??,304319.0
94172,396723.0,99295.0,Yeah.,329512.0
94172,396723.0,99299.0,Ok. Will Try that.,304319.0
94172,396723.0,99296.0,Or use more complex of extracting columns from different files and then plot one plot.,329512.0
94174,396730.0,,nan,
94174,396729.0,,nan,
94174,396732.0,,nan,
94173,396724.0,,nan,
94178,396798.0,,nan,
94178,396789.0,,nan,
94178,396823.0,,nan,
93980,396074.0,,nan,
93980,396149.0,,nan,
93980,396716.0,,nan,
94180,396813.0,,nan,
94180,396822.0,,nan,
94180,396838.0,,nan,
94180,396856.0,99412.0,"Even if we leave them intact, they will have no impact on the analysis.",319302.0
94180,396856.0,99441.0,"But they may interfere , when we will calculate the top9 country and top3 english speaking countries.",320073.0
94180,396898.0,,nan,
94183,396843.0,,nan,
94184,396836.0,,nan,
94184,396900.0,,nan,
94185,396875.0,,nan,
94185,396870.0,,nan,
94185,396832.0,,nan,
94185,396839.0,99321.0,"You didn't get records for master_frame or while doing check point-3 analysis ? If it is master_frame , please check the data import encoding part if it is check point-3 , top 3 countries . these are the requirements: 1. For the selected investment type , based on country we need to find total investment received for each country. 2. Retrieve the top 9 countries with above condition for top-3 english speaking countries in them.",305652.0
94185,396903.0,,nan,
94185,396967.0,,nan,
94185,397167.0,,nan,
94185,397373.0,,nan,
94185,397354.0,,nan,
94187,396868.0,,nan,
94187,396849.0,,nan,
94195,396878.0,,nan,
94195,396882.0,99451.0,unique..as per my understanding.,317998.0
94195,396882.0,99449.0,I have understanding problem here. Top nine countries means possibility of same country in the order of raised_amount_usd may come ? or unique top 9 countries.,312019.0
94199,396906.0,,nan,
94218,396955.0,,nan,
94218,396964.0,,nan,
94218,396962.0,,nan,
91115,380643.0,98841.0,This is not an error.,329512.0
91115,380972.0,98802.0,The data quality issue can be solved manually. Can you please suggest a better way to resolve it?,317990.0
91115,380972.0,98837.0,"You need to use Python for data cleaning and for data quality issue, Please don't do it manually.",329512.0
91115,380972.0,98811.0,Use dataframes and try to cleanup the data. Manually opening and cleaning up the CSV doesn't really make sense.,318381.0
91115,392006.0,,nan,
92163,387991.0,96802.0,is there a way I can completely ensure the encoding issue is completely solved? I don't see any junk characters in the fields after applying a known file encoding.,304027.0
92163,387991.0,97025.0,"Run this: print(len(rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :])) If this gets you 0 as the answer then you are good to go.",329512.0
92163,387991.0,97178.0,"even i am not getting 0, but don't see any visible special charcters!! not sure, if i am missing anything though!",314565.0
92163,387991.0,97192.0,"I got ' 0' , answer to one of the questions :)",304027.0
92163,387991.0,97765.0,i am getting all rows but i don't see any special characters in any rows,311952.0
92163,387991.0,97822.0,i am getting 114949 as the answer when running the above code...is it incorrect?,310509.0
92163,387991.0,97836.0,"Ideally it should be 0, since it takes for all rows present in rounds2.company_permalink that are not present in companies.permalink But I too am getting 114949.",300697.0
92163,387991.0,97916.0,"run this code and see: rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] If you can see some non-english words in permalink then encoding is not solved.",329512.0
92163,387991.0,99094.0,Maddula Naveen - Do we need handle encoding issue to answer Checkpoint 1 Table 1.1 questions?,318458.0
92163,387991.0,99152.0,Yes. Encoding issue is needed to be solved while importing the files to Python,329512.0
92163,395894.0,,nan,
92168,388006.0,97513.0,Can you please elaborate on this point?,310511.0
92168,388006.0,97658.0,Drop them.,329512.0
92168,388831.0,,nan,
92168,391364.0,,nan,
92168,395734.0,,nan,
92168,395637.0,,nan,
94233,396977.0,99351.0,"Reread my answer :) I had also made the same mistake, hence saying.",317998.0
94233,396977.0,99350.0,I am getting multiple companies under the same sector having the highest investment.,314197.0
94233,397128.0,99408.0,"Sir, i had not asked this as a question. i realised my mistake after reading the question again.",317998.0
94235,396976.0,,nan,
94235,396978.0,,nan,
93933,395953.0,,nan,
93933,395992.0,,nan,
91264,381535.0,,nan,
91264,381732.0,,nan,
91264,382478.0,,nan,
94237,396979.0,99420.0,from where to disable the firewall ?,308782.0
94237,396979.0,99431.0,Do you have a antivirus installed? If yes disable it for a moment and then try. If that doesn't work export then export the pandas dataframe to csv and then import it back to Tableau .,329512.0
94237,396979.0,99421.0,i donot know how to disable the firewall,308782.0
93090,391916.0,97925.0,"Once you have got your master datafile, you can clean it.",329512.0
93090,391916.0,97921.0,"So you mean to say, cleaning is required before checkpoint 1",301113.0
91288,381489.0,97231.0,shouldn't it be a csv file ? with this it is saving as txt,305845.0
91288,381518.0,,nan,
91288,382738.0,96130.0,"firstly, you should start your own thread - your 'issue' is totally different to what the OP here asked. Hint: your ' issue' is something that is part of the assignment and you have to figure to solve it yourself - you are basically asking solution to graded assignment",300694.0
91288,382738.0,96229.0,"This is not part of the assignment, Nitesh.",315797.0
91288,390054.0,,nan,
96538,411402.0,102237.0,This is not what i asked,317149.0
96538,412673.0,,nan,
94264,397034.0,,nan,
94264,397042.0,,nan,
94264,397051.0,,nan,
94264,397043.0,,nan,
94246,397102.0,,nan,
94246,397002.0,,nan,
94246,397039.0,,nan,
94246,397236.0,,nan,
94246,397417.0,,nan,
94273,397085.0,99392.0,I am guiding you the way. Put some efforts from your end as well sir.,317811.0
94273,397085.0,99390.0,this command is not working have you tried it yourself?,315560.0
94273,397081.0,,nan,
94278,,,nan,
94272,397084.0,,nan,
94272,397072.0,,nan,
94258,397018.0,,nan,
94258,397019.0,,nan,
94294,397160.0,,nan,
94294,397159.0,99581.0,There’s a format in this case. 0 is surrounded by Alphabets. So regex format can be found. Using this regex format find required text and replace with ‘na’. https://stackoverflow.com/a/46113474/4909087,314511.0
94294,397191.0,99448.0,ok rectified both issues. Thank you,319876.0
94259,397022.0,99370.0,Thanks for your quick reply. I understood the problem here.,308434.0
94259,397022.0,99369.0,But what is the error in my code,308434.0
94297,397155.0,,nan,
94297,397177.0,,nan,
94297,397157.0,,nan,
94275,404247.0,,nan,
94275,404248.0,,nan,
94275,397078.0,,nan,
94275,397088.0,,nan,
94275,397100.0,,nan,
94275,397106.0,,nan,
94275,397123.0,,nan,
94303,397164.0,,nan,
94303,397168.0,,nan,
94303,397176.0,,nan,
94303,397996.0,,nan,
94303,398389.0,,nan,
94307,397281.0,,nan,
94307,397187.0,,nan,
94260,397037.0,99432.0,"Your group by object looks buggy, share your complete code.",329512.0
94260,397037.0,99416.0,getting this error when trying the above code TypeError: unhashable type: 'list',312096.0
94260,397040.0,99378.0,Can you please share snapshot,317811.0
94260,397040.0,99376.0,Was getting an error while trying this..,312096.0
94277,397104.0,,nan,
94277,397126.0,,nan,
94287,397121.0,99395.0,So should i ignore them ?,319860.0
94287,397120.0,99393.0,i did clean,319860.0
94287,397120.0,99394.0,and also a doubt in between .....while merging using inner join the common columns to be considered is category_list only right ?,319860.0
94287,397120.0,99397.0,need more clarity please ...a little confused here,319860.0
94287,397120.0,99398.0,primary sector from master dataframe and categpry list from mapping datframe will be used for inner join,317811.0
94287,397120.0,99399.0,"Did you extracted main_category from category_list? The category_list column contains values such as 'Biotechnology|Health Care' - in this, 'Biotechnology' is the 'main category' of the company, which you need to use to merge.",329512.0
94287,397120.0,99407.0,@maddula yes i have extracted the primary_sector column from the main category list,319860.0
94287,397120.0,99409.0,@ankitsaini make sense ! i had merged it with the common category list from both the dataframes .Now i did it your way but still getting NaN values .My question is the resultant table what i have got contains NANs and is it suppose to be that way if so should i drop the rows containing them ?,319860.0
94287,397120.0,99415.0,i replaced 0 with na using replace() function,319860.0
94287,397120.0,99410.0,"You are not supposed to get NANs in this case for primary sector column and category list column, As you already dropped nulls from category list in master dataframe, hence you will forsure be having primary sector here, and also you have dropped nulls from category list in mapping dataframe. So when you will make inner join by taking primary sector (masterframe)and category list(mappingfrane) you are not supposed to get nulls. I think you have not handled the scenario (If you look in excel and try to find analytics it is given as A0lytics. please provide your inputs, I will tell u how to overcome this.",317811.0
94287,397134.0,,nan,
94309,397217.0,99428.0,"Even, in that case we would receive the same warning. The problem was somewhere before where I am creating one dataframe from another where we have to use .copy()",318329.0
94309,397240.0,,nan,
94309,397221.0,99422.0,I am trying to create a new column here but not the filtering. This is something different i believe,318329.0
94309,397221.0,99423.0,This is same sir. Have u tried,317811.0
94309,397221.0,99427.0,The problem was somewhere even before i created copy of this dataframe from master. I understood the problem and resolved the issue. The link you shared helped me in finding the root cause. Thanks,318329.0
94309,397221.0,99425.0,I need the column to be updated. The code you posted is returning a dataframe. Am i misinterpreting anything?,318329.0
94311,397228.0,99430.0,Thanks Venkatesh really appreciate the help.,318370.0
94316,397250.0,,nan,
94316,397245.0,,nan,
94320,397278.0,,nan,
94320,398091.0,99612.0,Thanks naveen,300687.0
94320,397298.0,,nan,
94320,397277.0,99442.0,thanks.I had done the sorting based on amount and then extracted the top 9 country .as the amount is not blank will it affect. I am sure whether its allowed to ask such question as its graded though:),300687.0
94332,397318.0,,nan,
94332,397319.0,,nan,
94332,397399.0,,nan,
94332,398391.0,,nan,
94322,397280.0,,nan,
94322,398095.0,,nan,
94322,397343.0,,nan,
94337,397340.0,,nan,
94337,397341.0,,nan,
94337,397396.0,,nan,
94335,397335.0,,nan,
94335,397337.0,,nan,
94335,397397.0,,nan,
94335,397355.0,,nan,
94339,397356.0,,nan,
94339,397413.0,,nan,
94339,397461.0,,nan,
94339,397815.0,,nan,
94348,398343.0,,nan,
94348,397409.0,,nan,
94348,397445.0,,nan,
94348,397531.0,,nan,
94348,398186.0,,nan,
94346,397429.0,,nan,
94346,397395.0,,nan,
94349,397421.0,99470.0,So the merged data frame will have a category_list_x and category_list_y column. I just wanted to clarify that's how its supposed to be!,317141.0
94349,397421.0,99472.0,I understand. Thanks.,317141.0
94349,397421.0,99471.0,"by default Yes, these columns would be there. If you dont want these columns, then you can clean your master frame",304814.0
94349,397444.0,,nan,
94349,397527.0,,nan,
94349,397490.0,,nan,
94347,397394.0,,nan,
94347,397424.0,,nan,
94347,398101.0,,nan,
94352,397437.0,99474.0,the answer need not be in column. You just have to perform your analysis using inbuilt python methods and you can print the result,318329.0
94352,397437.0,99473.0,thnks so we first create 3d1 d2 d3 and then in each 2 new columns,317982.0
94352,397450.0,99475.0,I think it has to be new dataframes since Table 5.1 is based on those values,318329.0
94352,397450.0,99476.0,i guess Shristi is referring to checkpoint 6 Tableau plots..,316349.0
94352,397450.0,99481.0,no.. dataframes one where we need to create inrstment count n stuff,317982.0
94352,397450.0,99480.0,"She mentioned as calculate while checkpoint 6 has plots. Anyways, let her confirm",318329.0
94352,397467.0,,nan,
94352,398325.0,,nan,
93111,392002.0,,nan,
93111,392710.0,,nan,
94351,397435.0,,nan,
94351,397440.0,,nan,
94351,397464.0,,nan,
94351,397524.0,,nan,
94350,397436.0,,nan,
94350,397441.0,,nan,
94350,397525.0,,nan,
93108,391984.0,,nan,
93108,391991.0,98539.0,Can you elaborate on that?,318335.0
93108,391991.0,98611.0,"yes, please elaborate.thank you",310509.0
93108,392709.0,,nan,
93334,392724.0,98290.0,thanks,311686.0
93334,393014.0,,nan,
93334,398122.0,,nan,
94357,397482.0,,nan,
94357,397493.0,,nan,
94356,397486.0,,nan,
94356,397522.0,,nan,
93117,392046.0,98218.0,"Yes, I am looking for less lengthier way.",317990.0
93117,392046.0,98946.0,"HI sandhya , I am in your group for case group project. pls contact me asap",318083.0
93117,392714.0,,nan,
93118,392042.0,98011.0,Sorry I meant they are not in scope,310974.0
93118,392042.0,97986.0,"If they are in scope, then they need to be handled right? Or did you mean to say they are NOT in scope?",313826.0
93118,392465.0,,nan,
93118,392716.0,,nan,
94367,398165.0,,nan,
94367,397512.0,99497.0,"no..i mean they are asking us to split category_list at "" | "" but there is no "" | "" in category_list",315560.0
94367,397512.0,99499.0,"You will find two type of values in category_list 1. Application Platforms|Real Time|Social Network where each value is separated by "" | "" 2. Media which is a single value. You need to split those value which has multiple value in it like Application Platforms|Real Time|Social Network Media one.",317991.0
94367,397512.0,99500.0,at which row exacty can i find,315560.0
94367,397512.0,99501.0,you have to do it for whole column,317991.0
94367,397512.0,99502.0,row number plz,315560.0
94367,397512.0,99504.0,media is at row 434 and there is no seprator there,315560.0
94367,397512.0,99506.0,"i know .. i have to apply split to whole column,,but my question is before applying split func on col ,, at which row can i exactly see this seprator value in column category ist",315560.0
94367,397512.0,99505.0,You have to apply split function to whole column not any specific row,317991.0
94367,397521.0,,nan,
94368,397530.0,,nan,
94368,398072.0,,nan,
94373,398816.0,,nan,
94373,397556.0,,nan,
94373,397591.0,99556.0,"Misread it as ""Banks""",308440.0
94373,397635.0,,nan,
94369,397517.0,99508.0,master_frame has more than 90K rows and separator can exist in plenty of rows. But why do you want any specific row when you have to apply split function on whole column.,317991.0
94369,397517.0,99507.0,"Before applying split function on Column Category List,, where can i see exactly see this seprator in mapping..at which row which value is that",315560.0
94369,397520.0,,nan,
94369,397545.0,,nan,
94369,397523.0,,nan,
94383,397600.0,,nan,
94383,397614.0,,nan,
94383,397612.0,99539.0,"The dataframes on which you have worked can be exported to csv files. For example, if 'df' is the name of the dataframe which you have worked on and you want to export it to csv, then you can use the below python command in notebook df.to_csv(""my_df.csv"") This will create a csv file in the current working directory by the name ""my_df.csv"" which you can further import into Tableau as a ""Text File"" and then go on to generate the plots. Hope this clarifies.",313826.0
94383,397660.0,,nan,
94383,398382.0,,nan,
94383,398809.0,,nan,
94372,397534.0,99613.0,"Currently when we create DataFrame based on Country 1 & country 2 & Country 3 . Each dataframe do has multiple rows for each sector. So , when we are creating the new column i.e count of Investment and sum of Investment . Do this value remain same for multiple rows in same sector as we need to need group the respective sector inside dataframe ? Is that mean if master frame contains mains sector ""Entertainment"" & Health ,",301108.0
94372,397536.0,,nan,
94385,397609.0,,nan,
94385,397611.0,,nan,
94385,397615.0,,nan,
94385,397636.0,,nan,
94001,396104.0,,nan,
94001,396102.0,,nan,
94001,396178.0,99233.0,yes mam.. read carefully what i wrote it is saying same as u mentioned and is correct.,317811.0
94001,396178.0,99232.0,its which company received the highest investment means sum of funds per company (not count),301115.0
94001,396206.0,,nan,
94387,397637.0,99555.0,ok thanks,308442.0
94387,397620.0,,nan,
94396,397937.0,,nan,
94396,397699.0,,nan,
94396,397694.0,,nan,
94396,397702.0,,nan,
94380,397593.0,99515.0,It didn't work either :(,318240.0
94380,397593.0,99517.0,Thanks Vinay,318240.0
94380,397593.0,99516.0,"There was a typo in my previous reply. Please try pd.read_csv(""C:/IIITB/Group Assignment/rounds2.csv"")",313826.0
94401,397734.0,99609.0,"yes ,thanku ,,using D1,D2 ,D3 ,top9 for plots",318005.0
94401,397734.0,99656.0,yes,301115.0
94401,397734.0,99610.0,"but doubt was only that investment type analysis should be done after extracting primary sector and mapping ,but also think there are various dimensions of analysis",318005.0
94423,397809.0,99579.0,Panda display format can be used https://stackoverflow.com/questions/20937538/how-to-display-pandas-dataframe-of-floats-using-a-format-string-for-columns This will avoid exponential notion,314511.0
94423,397822.0,,nan,
94423,397831.0,,nan,
94423,397930.0,,nan,
94423,397982.0,,nan,
94430,397880.0,,nan,
94430,397857.0,,nan,
94430,397858.0,,nan,
94430,397932.0,,nan,
94430,397980.0,99784.0,"in PPT , you paste the plot screenshots and then save as PDF",318454.0
94430,397980.0,99623.0,I think Tableau plot should be pasted into the ppt presentation doc which will finally be submitted in a pdf format.,318481.0
94430,398282.0,,nan,
94430,398448.0,,nan,
94430,399197.0,,nan,
94413,397818.0,99563.0,"in dataframe its float Name: raised_amount_usd, dtype: float64.",300687.0
94413,397818.0,99573.0,actually i am try to do some checks in excel to check my out put. when i do pivot and change amount as sum it gives zero.tried all ways to change to number or float nothing works,300687.0
94413,397818.0,99572.0,"after it's imported to tableau,is it coming under measure in the worksheet??",301115.0
94413,398136.0,,nan,
94416,397759.0,99549.0,Thanks Vinay,319444.0
94416,397985.0,,nan,
94420,397808.0,,nan,
94420,397933.0,,nan,
94421,397859.0,,nan,
94421,397881.0,,nan,
94421,398019.0,99660.0,Flowchart should contain all the steps taken for analysis broadly. It should indicate the input files and output files at appropriate positions. Also the checkpoints.,304319.0
94421,398821.0,,nan,
94421,399199.0,,nan,
93564,394093.0,,nan,
93564,394107.0,,nan,
93564,394218.0,98674.0,This you need to decide.,329512.0
93564,394218.0,98663.0,Thank you! And do we need to remove the entire row having NULL in category_ode and raised_funding_usd? TIA.,304813.0
93564,394588.0,,nan,
93564,395053.0,98889.0,Replace the NaN values with 0 before calculating the average,304812.0
92605,390145.0,97332.0,There are 10000 + rows and it is not possible to see each row.,310419.0
92605,390220.0,,nan,
92605,390712.0,,nan,
92605,390488.0,99483.0,I think the code that provided seems to check whether there are any companies which are present in rounds data frame but not in companies. Instead of this is there anyway to check whether there are encoding issues present in any column in a dataframe?,310481.0
92812,391053.0,,nan,
92812,391212.0,,nan,
92812,392133.0,,nan,
94440,397978.0,99587.0,I m showing both of them,311861.0
94440,397978.0,99586.0,"Thanks. Do we need to consider the entire dataset of , say Venture or just total and % values?",314511.0
94443,397919.0,,nan,
94443,397951.0,,nan,
94443,397917.0,,nan,
94443,398424.0,,nan,
92691,390473.0,97472.0,okay.. Thanks Maddula,305845.0
92691,390501.0,97697.0,"You're working with Pandas(Python) on group study, I don't think MySQL applies here.",318381.0
92691,391339.0,,nan,
94449,397947.0,99571.0,Thanks for clarifying...,303673.0
94449,398006.0,,nan,
94449,398017.0,,nan,
94449,398038.0,,nan,
94449,398064.0,,nan,
94449,398067.0,,nan,
94449,398281.0,,nan,
93533,393817.0,,nan,
94452,398176.0,,nan,
94452,398099.0,99594.0,Its basically the same result that you got from CheckPoint 3.,307176.0
94452,398065.0,,nan,
94452,397975.0,,nan,
94452,398003.0,,nan,
94452,398013.0,,nan,
94452,398037.0,,nan,
94452,398955.0,,nan,
94452,399096.0,,nan,
92866,391078.0,97622.0,What about the tableau plots then?,310511.0
92866,391078.0,97624.0,they haven't asked us for that - that just goes into the ppt/presentation,300694.0
92866,391078.0,97646.0,Thats not cool. I didnt plot the data just to be used as bunch of jpegs.,310511.0
92866,391078.0,97687.0,"You can submit your Tableau plots, It will also be evaluated.",329512.0
92866,391078.0,97688.0,Tableau is optional.,329512.0
92866,391078.0,97691.0,Its optional or it will be evaluated?,310511.0
92866,391078.0,98421.0,"Rajarshi, Tableau plots go into the ppt so pretty sure they will be evaluated too.",318381.0
92866,391078.0,98522.0,Tableau plots will not be evaluated.,329512.0
92866,391078.0,98513.0,"Avneesh, I think Rajarshi is asking if he submitted the optional Tableau plots then whether or not they will be evaluated; I don't think he is asking about the plots in the ppt that are generated via Tableau and pasted as images; my common sense thinking would say that the optional tableau plots will not be evaluated .. as they are not required to be submitted and not part of the assessment grading",300694.0
94460,398167.0,,nan,
94460,398049.0,,nan,
94460,398108.0,,nan,
94470,398073.0,99593.0,"ok, so an API exists to export the dataframe the way we read the csv. It does not rely on if it is a python notebook.",318007.0
94470,398075.0,,nan,
92946,391411.0,97723.0,Thanks it worked,311952.0
92946,391415.0,97724.0,Thanks for help,311952.0
92946,395029.0,,nan,
94483,398153.0,,nan,
94471,398077.0,,nan,
94471,398079.0,,nan,
94462,398082.0,,nan,
94462,398069.0,,nan,
94462,398107.0,99605.0,You will have to.,329512.0
94462,398107.0,99602.0,The question in case study is A plot showing the number of investments in the top 3 sectors of the top 3 countries on one chart (for the chosen investment type FT). the question has not asked to provide 5 m$ and 15 M $ range in Tableau so should we consider the range in Tableau or not ?,307843.0
94462,399006.0,,nan,
94481,398140.0,,nan,
94473,398104.0,,nan,
94477,398114.0,,nan,
94493,398189.0,,nan,
94493,398190.0,,nan,
94487,398179.0,99619.0,Yeah i have found the top 9 countries already which are having highest raised amount in the chosen investment type. My question is now do we need to create a subset from master frame for these 9 countries with the chosen investment type?,318579.0
94487,398179.0,99620.0,3 Subsets D1 D2 and D3,329512.0
94487,398179.0,99621.0,Then what does top9 dataframe contains?,318579.0
94487,398179.0,99624.0,I have found top9 countries already. So do i need to create a dataframe named Top9 with these 9 countries,318579.0
94487,398179.0,99627.0,"Thank you. i have read the below line and got confused actually. ""For the chosen investment type, make a data frame named top9 with the top nine countries """,318579.0
94487,398179.0,99625.0,"No need just report it in your notebook, after that find the top 3 english speaking countries maually and then filter the data based on it to create D1, D2 and D3",329512.0
94497,398200.0,,nan,
94474,398102.0,99598.0,"Thats good question, you will get this answer once the solution is published.",329512.0
94474,398102.0,99596.0,"Think has a case that when we have to tell CEO in which top sectors he can invest , we will tell that invest in others. Isn't it unspecified as other contains a lot more sector.",318426.0
94474,398103.0,99597.0,"And we will report that ""other"" is among top investing sector.Does it make sense as it contains many more sectors included in it.",318426.0
94496,398198.0,,nan,
94496,398197.0,,nan,
94478,398123.0,,nan,
94484,398159.0,,nan,
94484,398150.0,,nan,
94500,398207.0,,nan,
94500,398208.0,99630.0,Can you please let me know the section where it's mention,306243.0
94501,398211.0,99631.0,"Yes thats what I did, but at checkpoint5 I can see top3 english speaking countries have changed while inputting 5-15M range.",313767.0
94501,398211.0,99635.0,"Cool , thanks!",313767.0
94501,398211.0,99633.0,Once you have chossed the top 3 english speaking countries without using the band then you may go for the analysis without worring about it.,329512.0
94479,398127.0,,nan,
94517,398257.0,,nan,
94517,398253.0,99653.0,"As I said, if you are able to add it, it's good, if not then just report it in your excel files, No grading will be done based on that. The answer and the code is important",329512.0
94517,398253.0,99652.0,The three data frames should contain: 1. All the columns of the master_frame along with the primary sector and the main sector 2. The total number (or count) of investments for each main sector in a separate column 3. The total amount invested in each main sector in a separate column My question is about the second bullet point. A column is to be introduced in the dataframes right?,318007.0
94489,398180.0,,nan,
94489,398183.0,,nan,
94489,399768.0,,nan,
94475,398105.0,,nan,
94495,398196.0,,nan,
94495,398195.0,,nan,
94510,398236.0,99642.0,yes i have filtered my data...but still getting the same answer,305650.0
94510,398236.0,99644.0,"Please check your analysis, I can't answer this. Discuss with your team. But yes this can't happen.",329512.0
94510,398236.0,99645.0,If you need to consider both 5 and 15 both values for filtering ?,305650.0
94510,398236.0,99647.0,Yes this is within range.,329512.0
94510,398236.0,99648.0,Also use count of investments and not sum of investments to choose the top.,329512.0
94510,398236.0,99649.0,Please answer,305650.0
94510,398236.0,99650.0,I have considered within range but if we need to consider 5 and 15 both values included for the analysis,305650.0
94510,398236.0,99651.0,You need to include the 5 and 15 in the range. Also your your question the answer is use count of investments and not sum of investments to choose the top,329512.0
94510,398236.0,99682.0,"Consider it, No issue.",329512.0
94510,398236.0,99661.0,If we have to consider others in the sector analysis ? I am getting others as the top sector for D1 frame? Do I consider others or ignore it and take other sector as the top one,305650.0
94502,398212.0,99634.0,but last time for movies assignment this one worked for me.,310598.0
94502,398212.0,99636.0,"The issue is you are doing this to entire column not one row, so the error is asking to input iloc as row or column, here the code I provided uses lambda that is for all rows.",329512.0
94502,398212.0,99638.0,"even though category_list is an object data type,it's throwing error with float does not have split attribute",310598.0
94502,398212.0,99640.0,Can you remove nulls and drop duplicate for it.,329512.0
94502,398212.0,99643.0,thanks it worked,310598.0
94491,398187.0,99622.0,if you remove the column which will effect the analysis that would be wrong as the data is neede to do analysis and if you remove everything you will have very small data,318017.0
94520,398260.0,,nan,
94520,398262.0,,nan,
94520,398709.0,,nan,
94520,398818.0,,nan,
98911,420809.0,,nan,
98911,420827.0,,nan,
94503,398216.0,,nan,
94521,398266.0,99657.0,"But after grouping how we can decide which is best , like which get max number of funding or max amount in funding?",320073.0
94521,398266.0,99685.0,max funding,318084.0
94521,398381.0,99726.0,"This can't happen, you need to recheck your analysis then.",329512.0
94521,398381.0,99692.0,"If there are more than 1 company with the same number of investments, then can we use funding to identify the top company?",318084.0
94521,398381.0,99740.0,"We have to apply 5M to 15M filter on the countries dataframe(D1,D2 and D3)..... right? Can you confirm this?",318084.0
94521,398381.0,99772.0,"Yes, that's true.",329512.0
94521,398381.0,99781.0,"Filter on venture-->Choose the top3 English speaking counties--> Filter data for three, country D1, D2 and D3--> Applyt the filter of 5 to 15M--> And then do the analysis.",329512.0
94521,398381.0,99777.0,"Filter on venture, funding between 5M to 15M and the top sector. After applying the above fillers, if we calculate the company which received the max number of investments, I'm getting more than 1 company.. Verified in Excel also",318084.0
94521,399022.0,,nan,
94524,398280.0,99670.0,Both,308964.0
94524,398280.0,99674.0,ok.thanks,308964.0
94524,398280.0,99671.0,For dataframe I have mentioned the syntax. And for whole notebook I don't think or I am unaware of any syntax which does this.,317991.0
94524,398376.0,,nan,
94523,398277.0,,nan,
94523,398279.0,,nan,
94523,398339.0,,nan,
94523,398728.0,,nan,
94523,398812.0,,nan,
94526,398297.0,,nan,
94526,398310.0,,nan,
94526,398322.0,,nan,
94522,398272.0,99669.0,UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 25: invalid start byte,315856.0
94522,398378.0,,nan,
94516,398250.0,99654.0,if you say we have to see the count then it cant be hightest investment atleast the question should be potrayed in a proper way if one company has got small investment 30 times and a company has 25 big investment what to go for by reading highest,318017.0
94516,398250.0,99655.0,Number is what is asked even in the excel file. This is how the question is framed. Sorry for confusion.,329512.0
94504,398218.0,,nan,
94504,398217.0,,nan,
94499,398204.0,,nan,
94499,398205.0,99629.0,"ok then that means we can first put a filter on most suitable investment type , then make three different DF based on country",315560.0
94494,398193.0,,nan,
94494,398191.0,,nan,
94494,398192.0,,nan,
94527,398298.0,,nan,
94527,398385.0,,nan,
94505,398222.0,,nan,
94482,398151.0,,nan,
94482,398149.0,,nan,
94134,396556.0,99543.0,"As there is a pattern, reg expressions can be used to replace 0 with ‘na’",314511.0
94134,396577.0,,nan,
94134,396563.0,,nan,
94134,396559.0,,nan,
94134,396562.0,,nan,
94134,396741.0,,nan,
94134,396786.0,,nan,
94134,396974.0,,nan,
94506,398227.0,,nan,
94506,398224.0,,nan,
94532,398335.0,,nan,
94532,399000.0,,nan,
94532,398334.0,,nan,
94532,398349.0,,nan,
94498,398202.0,99658.0,"Hi Deval, can we remove rounding code which are null ?",303674.0
94498,398201.0,,nan,
94488,398175.0,99617.0,"Yes, it will be but we will check if the answer is in range.",329512.0
94488,398175.0,99616.0,"Okay, Thanks. I asked this because answer will be varied based on this",315423.0
94508,398225.0,99659.0,num has been used for finding the top sector and then top company has to extracted for that particular sector.,300721.0
94508,398225.0,99646.0,You are using number or sum? You need to use number wise highest.,329512.0
94508,398225.0,99637.0,We are getting the companies which received the same amount of funding which is max 15M.,320073.0
94508,398225.0,99639.0,Oh which the data is not filtered for 5-15M range.,329512.0
94508,398225.0,99641.0,Even after applying filter I am getting this . Checking this on D1 dataframe.,300721.0
94508,398225.0,99773.0,"No, 5 and 15 is included.",329512.0
94508,398225.0,99743.0,"""Also, the range of funding preferred by Spark Funds is 5 to 15 million USD."" Does this statement means we should exclude boundary values that is 5M and 15M?",300721.0
92781,390771.0,97554.0,PDF to country code needs be derived or this can be done manually just by observing ? Or we have to load pdf data as well,317689.0
92781,390771.0,97761.0,I also did the same,301113.0
92781,390771.0,97759.0,"I created a df manually using the PDF (using manual copy/paste, not pdfreader), online CSV file for country codes, and excel",300694.0
92781,391200.0,,nan,
92932,391338.0,,nan,
92932,391381.0,,nan,
92932,391359.0,97807.0,"agreed,... we can check the country by googling the code but i feel the mapping should've been provided",305845.0
92970,391544.0,97774.0,"Try adding encoding after filepath as follows: import pandas as pd data = pd.read_csv('file_name.csv', encoding='utf-8')",317991.0
92970,391544.0,97775.0,"import pandas as pd rounds2 = pd.read_csv('rounds2.csv', encoding='utf-8') rounds2 I tried still showing as unicodedecodeerror",316399.0
92970,391544.0,97776.0,Did you try this also encoding='iso-8859-1' ?,317991.0
92970,391544.0,97777.0,but it is showing as file not found,316399.0
92970,391544.0,97778.0,"Try this code import numpy as np import pandas as pd rounds2 = pd.read_csv(r""C:/Users/Kja technical1/Desktop/rounds2.csv"", sep = ""\t"", encoding = ""ISO-8859-1"")",317991.0
92970,391544.0,97780.0,Thanks finally was able to get,316399.0
92970,391544.0,97781.0,Great. Which code worked ?,317991.0
92970,391544.0,97787.0,the above which you sent me to try ISO-8859-1,316399.0
92970,391544.0,97796.0,"Actually I am not expert into this encoding stuff but I do hit and trial among the common encoding like ""ISO-8859-1"", ""utf-8"" etc.",317991.0
92970,391544.0,97789.0,"can you please explain me on this, how do I need to arrive at this? I mean encoding",316399.0
92970,391528.0,97770.0,try r before path as follows: rounds2 = pd.read_csv(r'C:/Users/Kja technical1/Desktop/rounds2.csv') Let me know if that works or not ?,317991.0
92970,391528.0,97768.0,I tried but getting as follows: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 25: invalid start byte,316399.0
92970,391578.0,97795.0,Thanks for the clarification,316399.0
92970,391661.0,97818.0,thanks,316399.0
92970,391757.0,,nan,
92970,395309.0,,nan,
92970,391936.0,99153.0,Google: removing-special-characters-in-a-pandas-dataframe. The answer is in StackOverflow,329512.0
92970,391936.0,99133.0,How to choose the right encoder ?,307491.0
94528,398315.0,,nan,
94528,398306.0,,nan,
94537,398373.0,99684.0,"Yep, I got that. Thanks",318329.0
94537,398367.0,,nan,
93599,394331.0,98688.0,"But this : print(len(rounds2.loc[~rounds2['company_permalink'].isin(companies['permalink']), :])) is not 0 right? Google foe correct encoding, Try my hint",329512.0
93599,394331.0,98684.0,"rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] I dont see any Non English characters.",310522.0
93599,394331.0,98689.0,Yeah. It is not zero.,310522.0
93599,394331.0,98690.0,Then you solved the issue I guess.,329512.0
93599,394331.0,98692.0,Then you need to check for some other encoding. Use the Hint I have provided and google.,329512.0
93599,394331.0,98691.0,"I had already used rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] before calculating that len part. It is showing that some characters are not in english, especially in the last rows of dataframe",318335.0
94539,398543.0,,nan,
94539,398390.0,99689.0,"Did you try appending ""/1000000"" at the end of the command?",318762.0
94539,398390.0,99688.0,"df.sort_values(by = (['columname']), ascending = False) ----------- how can i apply divide by 1000000 in this command",315560.0
94539,398399.0,,nan,
94539,398710.0,,nan,
94539,399401.0,,nan,
94539,399042.0,,nan,
94515,398247.0,,nan,
92607,390303.0,,nan,
92607,390495.0,99155.0,"Once you have merged the data, you can proceed for Data Cleaning.",329512.0
92607,390495.0,99143.0,We have to data cleaning before finding the mean()?,318846.0
92607,391341.0,,nan,
92607,392092.0,,nan,
92607,392091.0,,nan,
93617,394890.0,,nan,
93617,394496.0,99345.0,Hi vinay how do we calculate avg for specific type funding?,308639.0
93617,394483.0,,nan,
93617,394551.0,,nan,
93617,394660.0,,nan,
93617,396922.0,,nan,
93208,392435.0,98155.0,but how a box plot will show fraction of total investments? it will just compare these 4 funding types.,311686.0
93208,392435.0,98177.0,"Yes, that what is needed. The box plot will help you to choose the suitable representative value of the funding amount for each type of investment. Then you can either choose the mean or the median based on the box plot results. This will also show which is the most suitable funding type.",329512.0
93208,392435.0,98179.0,Yes exactly right. Box plot will not show fraction of total investments. I don't understand how this has been marked as the correct answer??,300694.0
93208,392435.0,98181.0,yes. but it will not show contribution of these types as fraction of total which is asked in the task.,311686.0
93208,392435.0,98182.0,"Yes, that's right. For that you can use Pie chart.",329512.0
93208,392435.0,98297.0,Yes.,329512.0
93208,392435.0,98288.0,So a pie chart + box plot? Treemap can solve both the purpose mentioned in task simultaneously.,311686.0
93208,392471.0,,nan,
93208,395605.0,99017.0,you can see the comments on the 1st answer as to what was discussed and agreed upon :),311686.0
93208,395605.0,99019.0,"I'm confused reading those comments.. so, are going with a Tree?? or combination of pie and box??",316349.0
93208,395605.0,99020.0,pie and tree.. however yet to confirm with my group,311686.0
93208,395605.0,99021.0,how do you represent two plots in the same page!!,316349.0
93208,395605.0,99024.0,Ok. thanks! i got confused since you wrote 'and' :),316349.0
93208,395605.0,99023.0,no.. no.. will use one only after discussing with my group,311686.0
93208,396377.0,,nan,
94514,398243.0,,nan,
94563,398555.0,99711.0,if the next level sum is going to happen as per a different column then surely yes. but if you are using the same column in groupby which you used for 1st sum then you won't get a different result.,311686.0
94563,398555.0,99709.0,"but what if i want to further use the group by sum result in other places, then definately having this result as new column would definitely help",306005.0
94563,398551.0,99706.0,"actually ask is, i have grouped result with me where column1 has both sum and count as aggregation result. i cann't find option to perform further aggregation on the result since both sum and count result are under column1",306005.0
94563,398551.0,99710.0,I am not getting it...how can you have two aggregations under one column?,304814.0
94563,398551.0,99771.0,"Conceptually what you are mentioning is not making much sense. You can either take sum(population) by location or count(population) by location. How can you have both sum, and count in one column?",304814.0
94563,398551.0,99712.0,"say, i am grouping by location and performing aggregation of population column with functions sum and count the my group by result would be having location ---- population where population would further be having sum and count result so what if i want to have these sum and count result in separate column rather in multi index form",306005.0
94563,398827.0,99734.0,"used the aggregation result by multi index approach, and able to get desired output",306005.0
94563,398827.0,99758.0,"Sorry, I did nt get you..is this worked or not",317811.0
94563,399213.0,,nan,
94563,399400.0,,nan,
94563,399399.0,,nan,
94583,398646.0,99724.0,You are right. The course wasn't a mandatory one. I read somewhere that the plots won't be evaluated. Thats why they probably dont want us to submit the twb/twbx file.,310511.0
94583,398646.0,99722.0,"but then its their assumption that everyone has completed their preparatory, but it is not the case, they should have putted basic tableau practice in main course prior to put in case study which is graded one.",306005.0
94583,398687.0,,nan,
94583,398810.0,99733.0,"Its not difficult, just go through the tableau video will take hardly 2-3 hours of you an you are good to plot the graph.",317811.0
94583,398810.0,99732.0,"true but i was concern about using tableau, as not all are familiar with drawing those graph in tableau",306005.0
94583,399062.0,,nan,
94583,399222.0,,nan,
94574,398638.0,,nan,
94574,398571.0,99718.0,Thanks,308442.0
94574,398654.0,,nan,
94586,398636.0,99720.0,thnks Rajarshi,307843.0
94586,398806.0,,nan,
94586,399583.0,,nan,
93137,392100.0,,nan,
94576,398817.0,,nan,
94576,398652.0,99721.0,Thanks Rajarshi,308437.0
94587,398643.0,,nan,
94587,398685.0,,nan,
94587,398708.0,,nan,
94587,399223.0,,nan,
94589,398641.0,,nan,
94589,398683.0,,nan,
94589,398823.0,,nan,
94589,399270.0,,nan,
94593,398661.0,,nan,
94593,398678.0,,nan,
94593,398680.0,,nan,
94593,398699.0,,nan,
94593,399058.0,,nan,
94612,399267.0,,nan,
94612,398844.0,99736.0,fro the first plot i am using the merged dataframe.....how do i go about plotting it?,319846.0
94612,398867.0,,nan,
94612,398984.0,,nan,
94612,399370.0,,nan,
94612,399635.0,,nan,
94615,399261.0,,nan,
94615,398879.0,,nan,
94604,399268.0,,nan,
94604,398744.0,,nan,
94604,398768.0,,nan,
94604,398805.0,,nan,
94604,398957.0,,nan,
94604,399057.0,,nan,
94604,399198.0,,nan,
94604,399567.0,,nan,
94597,398692.0,,nan,
94597,398992.0,,nan,
94607,398820.0,,nan,
94624,398981.0,99749.0,changinng to xlsx is not working .will check the link provided.thanks,300687.0
94625,399259.0,,nan,
94625,398925.0,,nan,
94625,399193.0,,nan,
94609,398847.0,,nan,
94609,398819.0,,nan,
94609,398959.0,,nan,
94609,399207.0,,nan,
94626,398973.0,,nan,
94626,399050.0,,nan,
94626,399191.0,,nan,
94626,399316.0,,nan,
94550,398487.0,,nan,
94550,398454.0,,nan,
94550,398554.0,,nan,
94550,398542.0,,nan,
94628,398943.0,,nan,
94628,398944.0,,nan,
94628,399257.0,99786.0,great point!,318458.0
94634,399044.0,,nan,
94634,399010.0,,nan,
94634,399053.0,,nan,
94634,399181.0,,nan,
94634,399242.0,,nan,
94634,399274.0,,nan,
94634,399365.0,,nan,
94634,399719.0,,nan,
94581,398617.0,,nan,
94581,398649.0,99727.0,Why angel is not required in the plot 1? We considered angel in the analysis to find the fund type right? Should we not include it?,301644.0
94581,398649.0,99774.0,Yesm you may include it. It's not a issue.,329512.0
94581,398649.0,99744.0,I think it is missed to mention in the requirement. We should include angel also as it was part of our analysis. TAs can you confirm on this!,318350.0
94581,398814.0,,nan,
93366,392902.0,98402.0,What kind of data cleansing is required to get the count of observations,306725.0
93366,393011.0,,nan,
94643,399085.0,,nan,
94643,399145.0,,nan,
94643,399189.0,,nan,
94643,399195.0,,nan,
94643,399241.0,,nan,
94643,399301.0,,nan,
94643,399360.0,,nan,
93399,393207.0,,nan,
93399,393139.0,,nan,
93399,393593.0,98977.0,"Can this added as a pinned discussion. I think everyone would have this confusion, as it does not make sense to add the columns in D1,D2 and D3 data frames.",306725.0
93399,395443.0,,nan,
93399,396381.0,,nan,
93399,396676.0,99607.0,"Yes, we can ignore this this as suggested by TA",301113.0
93571,394134.0,,nan,
93571,394149.0,,nan,
93571,394194.0,,nan,
93571,397096.0,,nan,
93387,393052.0,,nan,
93387,393210.0,98512.0,if you read my OP carefully I have explained HOW and WHERE it DOES say to clear the non-english speaking countries for checkpoint 2. I have even put it in bold so it is hard to miss ;-),300694.0
93387,393580.0,,nan,
93471,393504.0,,nan,
93471,393518.0,,nan,
93582,394503.0,,nan,
93582,394208.0,,nan,
93582,394207.0,98677.0,Thanks a lot,308437.0
93582,394207.0,99014.0,Naveen - Thanks for clarifying,301644.0
93582,394216.0,,nan,
93582,394555.0,,nan,
93582,394744.0,,nan,
93580,394204.0,,nan,
93604,394385.0,98695.0,"For e.g if you consider entry for '/ORGANIZATION/1006-TV' in rounds2 data frame , we have 3 entries with different case (lower/upper) combination , although if we compare all column values for these 3 entries all the values are different (like funcding round type,funding round code etc).So can you please provide the clarification whether these rows should be considered as duplicate based on the value of only 'company_permalink' column or they should be treated as unique.",304696.0
93604,394385.0,99028.0,"But if you convert them to lower case, the two columns become unhashable lists. How do you deal with that when merging the dataframes.? I keep getting the error unhasheble : list...",316416.0
93621,394495.0,,nan,
93621,394557.0,,nan,
93621,395030.0,,nan,
98590,419595.0,,nan,
98590,419602.0,,nan,
98590,419714.0,,nan,
98590,419778.0,,nan,
98590,419945.0,,nan,
97285,416339.0,,nan,
97285,416341.0,,nan,
98829,420810.0,,nan,
98829,420746.0,,nan,
99443,425947.0,,nan,
99443,425287.0,,nan,
98599,419680.0,,nan,
98599,419713.0,,nan,
98599,419777.0,,nan,
98599,419943.0,,nan,
106770,461884.0,,nan,
106770,461260.0,110399.0,Doesn’t matter if you drop all ten variables to handle VIF you will left with only 4-5 variables,300735.0
106770,462314.0,,nan,
106234,458643.0,109659.0,"But in this way I will end up making 50 new vars that doesn’t make sense ..what about where one var is having 2 values can it given 1 and 0, and in other scenario where var has more than 2 values making dummy variable will make sense ..",300735.0
106234,458643.0,109727.0,yes you utilize dummy wherever variable have more than 2 different values.,301648.0
106234,458837.0,109861.0,"Let's take one example here that I can club cars model like BMW, Jaguar, Porsche together and I can flag them high-end and in a similar way, I can create medium and low right? so i will end up with High Medium and Low brand type...",300735.0
106234,458837.0,109868.0,rest use pd.get_dummies(),318344.0
106234,458837.0,109867.0,correct,318344.0
116351,503121.0,,nan,
141342,610487.0,135377.0,did that already.. there are around 21000 uNique uses.. is it still more?,316349.0
141342,610487.0,135411.0,Go with Google colab then. If you are still facing the issue.,428646.0
141342,610487.0,135652.0,"You can restrict users as well. Basically it is doing a matrix multiplication, so with high number of users the dimension of matrix becomes large and it is computationally more intensive.",317689.0
141342,611195.0,,nan,
141342,610679.0,,nan,
141069,,,nan,
141368,610676.0,135487.0,"After Visualization Plot is denser, I have tried using count plot but not able to see it properly, Please suggest me something?",315423.0
141368,610676.0,135488.0,you can use 2-3 plots.. histogram being one,311686.0
141368,610676.0,135414.0,thanks a lot!,315423.0
141368,612110.0,,nan,
141368,612349.0,,nan,
141144,609599.0,,nan,
141144,610210.0,,nan,
141144,609556.0,135231.0,"I get the same error - after dropping the duplicates in first 2 columns : df = df.drop_duplicates(['beer_beerid','review_profilename']) df_movie_features = train.pivot( index='beer_beerid', columns='review_profilename', values='review_overall' ).fillna(0) This gives value error - ValueError: Index contains duplicate entries, cannot reshape",319759.0
141144,609556.0,135457.0,check dropna. drop_duplicates use subset option to keep first or lat of these https://stackoverflow.com/questions/32093829/pythonpandas-removing-duplicates-based-on-two-columns-keeping-row-with-max-va,312019.0
141144,611395.0,,nan,
141144,612352.0,,nan,
141475,611389.0,,nan,
141475,611826.0,,nan,
141475,612107.0,,nan,
141475,611150.0,,nan,
141475,612325.0,,nan,
141302,610491.0,135376.0,Intuition,310974.0
141302,610491.0,135569.0,This is done in order to avoid records (for beer_id(s) and review_profilename(s)) where a beer only got very few i.e. 1-5 reviews or a review_profilename only gave very few reviews i.e. 1-10 as this data will be very insignificant to decide the behavior. It is more good to say that such a sample data is not good enough to say something about the entire population.,317811.0
141302,610418.0,,nan,
141302,612106.0,,nan,
141434,611857.0,,nan,
141434,611078.0,135518.0,while using user_correlation matrix i have build a dataframe in which i have 10 rows and 75000 columns. if i apply correlation Matrix the system hangs. Any idea how to solve,306243.0
141434,610873.0,135499.0,"Sorry, I misunderstood the question!",311466.0
141434,610970.0,135517.0,while using user_correlation matrix i have build a dataframe in which ihave 10 rows and 75000 columns. if i apply correlation Matrix the system hangs. Any idea how to solve,306243.0
141436,610861.0,135433.0,Any idea why we are facing this issue and how to fix it?,311254.0
141436,610998.0,135477.0,Okay thanks :),311466.0
141436,610998.0,135576.0,You need to remove all such beer for which the rating count is very less. Take a cutoff based on your understanding.,428646.0
141172,610040.0,,nan,
141172,609803.0,,nan,
141172,609771.0,,nan,
141172,610891.0,,nan,
141172,609744.0,135455.0,"refer this link https://stackoverflow.com/questions/32093829/pythonpandas-removing-duplicates-based-on-two-columns-keeping-row-with-max-va general syntax is as below we should remove dropna and all rows duplicates drop_duplicates - syntax can check. pivot index,columns should not have duplicate. or direct pivot_table",312019.0
141172,609780.0,,nan,
140445,610982.0,,nan,
140445,606936.0,,nan,
140507,609951.0,,nan,
140507,608418.0,,nan,
140507,607160.0,,nan,
140507,607151.0,,nan,
140507,608914.0,,nan,
141454,,,nan,
141463,611149.0,,nan,
141463,611250.0,135577.0,Row count will be different but column count should be the same. Please check again I think you are missing something.,428646.0
141463,611854.0,,nan,
141463,612104.0,,nan,
141463,612159.0,,nan,
141463,611068.0,,nan,
141463,611267.0,,nan,
140376,606961.0,134998.0,"hi, do you mean that we have visualize Avg.Rating got by each beer in the 1st question? If yes then how to handle the fact that we might have 1500+ unique beer ID even if after taking beers with atleast N number of views? Plotting this won't make any sense. Requesting some TA as well to respond.",311686.0
140376,606961.0,135074.0,"Waiting for some one to respond. TAs, pls. help!!!",311686.0
140376,608670.0,,nan,
140376,610086.0,,nan,
140658,608730.0,135136.0,Is the process of creating real rating matrix mention in the upgrad excercise?,310511.0
140658,608730.0,135354.0,@TA please provide clarity on this ?,305650.0
140711,610037.0,,nan,
140711,608140.0,,nan,
140711,608327.0,135022.0,"In the example provided in module they have duplicate data and still used pivot. I want to understand how it works, why that code worked but it is not working.",320103.0
140711,608327.0,135029.0,Combination of columns should be unique. one column having duplicates does not matter. As mentioned earlier index in pivot means combination of index and column and that combination should be unique. if not unique we need to remove the duplicates (not individual duplicates but the combination)and then do pivot.,311254.0
140711,608327.0,135035.0,any example or link to understand above will be useful.,320103.0
140711,608327.0,135044.0,example: col a col b 1 2 2 3 1 2 so first and third row are duplicates .We need to remove such rows.,311254.0
140711,608327.0,135121.0,same issue for me too. https://learn.upgrad.com/v/course/208/question/140824,312019.0
140711,608327.0,135308.0,i have used pivot_table(). It works,305650.0
140711,609129.0,,nan,
140824,609378.0,,nan,
140824,608402.0,135041.0,Thanks for the info. But when i remove duplicates of review_profilename the observations coming to 21k from 4lakhs . is this ok to go head ? pls suggest,312019.0
140824,608402.0,135042.0,no you don't have to remove duplicates from individual columns. check my comment earlier...combination of index and column...in pivot comand what is your index and column. combine those to find duplicates and remove them. once that is done apply pivot,311254.0
140824,608402.0,135120.0,"I am sorry. Its not clear for me still. Because index='id', (user_id) has duplicates. Because of this train.pivot( throws above error. pivot needs index column(user id), columns(beer id), and values column has score. Now pls clarify how to take care of this user_id duplicate error. Thanks in advance",312019.0
140824,608402.0,135049.0,example: col a col b 1 2 2 3 1 2 so first and third row are duplicates .We need to remove such rows.,311254.0
140824,609118.0,,nan,
141335,610411.0,135367.0,Yes I did,310974.0
141335,610411.0,135353.0,"Have you created the Pivot table without converting the ""review_profilename"" column. into integer.",306243.0
140937,,,nan,
140946,609090.0,135255.0,"Ok, but what are these then: 3) The average number of ratings given to the beers 4) The average number of ratings given by the users These are just numbers each, how can I plot a number. If it is just the plot of the count of ratings grouped by beer/user, then it makes sense. But then, what is average here?",310974.0
140946,609090.0,135313.0,ok then what is there to visualise here,310974.0
140946,609090.0,135312.0,My understanding: 3) The average number of ratings given to the beers - mean of (count of rating grouped by beer) 4) The average number of ratings given by the users : mean of (count of rating grouped by users),316036.0
140946,610128.0,,nan,
140946,610561.0,,nan,
140959,609265.0,135249.0,values='review_overall' ).fillna(0) try this way,312019.0
140959,611158.0,,nan,
140959,610349.0,,nan,
140959,609369.0,135195.0,I have done that too. so first time the code executed successfully. The next time i tried rerunning it is throwing memory error . I restarted my system and then the code executed again. Whenever i am trying to rerun i am facing this memory issue,311254.0
140959,609369.0,135197.0,This is because the memory is occupied with the large data frames hence its running out of memory. You can monitor your RAM usage under task manager performance. Once i did User based recommendation i removed the dataframe from the memory which is not used further. Just small trick. You can use below code to clear import gc del [data_frame] gc.collect(),315028.0
140959,609369.0,135223.0,I did not get this point. could you please elaborate. I choose reviews only if the total count is greater than 10. what is total count ?,312019.0
140959,609369.0,135210.0,i had tried that too but of no help. Finally using google colab. Thanks for the suggestions though:),311254.0
140959,609130.0,,nan,
141257,,,nan,
141272,610420.0,135381.0,"Ram,,,,would you able to complete assignment without using ""Real rating matrix"" ?",305650.0
141272,610420.0,135458.0,I did,310974.0
141322,610413.0,,nan,
141322,611022.0,,nan,
141322,612108.0,,nan,
141312,610416.0,135358.0,How did you arrive at least N number of reviews?,307495.0
141312,610416.0,135366.0,Intuition,310974.0
141312,610416.0,135383.0,"Yaa got it. I did only review rating less removal, duplication. Will reduce further based on review sum for beer_id or by user name. Thanks",312019.0
141491,611196.0,135495.0,i did that but still same error. any idea,306243.0
141491,611196.0,135519.0,Solved using X.fillna(0),306243.0
141491,611258.0,,nan,
141491,612157.0,,nan,
141491,611843.0,,nan,
141491,611253.0,135590.0,Thanks!,315423.0
141811,613033.0,,nan,
141811,612567.0,,nan,
141505,611841.0,135581.0,absolutely correct. I was using 0.19.1 and issue got resolved after upgrading to latest version.,312479.0
141505,613112.0,,nan,
141505,611256.0,,nan,
141505,613097.0,,nan,
141505,612163.0,,nan,
141505,612164.0,,nan,
141505,612165.0,,nan,
141505,612718.0,,nan,
141518,611387.0,,nan,
141518,611314.0,,nan,
141525,611394.0,,nan,
141525,611554.0,135547.0,"As per my understanding right now, I think the difference is fine. It depends on the user beer id combination. Probably in test those beer ids are not present.",311254.0
141525,611560.0,,nan,
141525,611853.0,135600.0,"Can you please elaborate? I have a data frame after data cleaning. I am not transforming. Now, I am splitting into train and test.",301643.0
141525,611853.0,135603.0,"Split the data before using pivot on the data. In that case, the number of columns for both train and test should be the same.",428646.0
141525,611853.0,135613.0,"Thanks Sumit. Yes, it worked",301643.0
141525,611853.0,135727.0,"@Sumit Shukla, ""Split the data before using pivot on the data. In that case, the number of columns for both train and test should be the same."" -> train and test split and then pivot. How do we get same number of columns? Did u mean ""Pivot and then split the data to test and train"" ?",310467.0
141525,611853.0,135742.0,No first split the data and then pivot it.,428646.0
141525,611853.0,135773.0,It will not be same if the split is done after the pivot transformation.,428646.0
141525,611853.0,135749.0,"Then as per our discussion forum answers (verified) till now, number of columns will not be same.",310467.0
141572,611594.0,,nan,
141572,613093.0,,nan,
141572,611670.0,,nan,
141572,612771.0,,nan,
141574,611862.0,,nan,
141574,611662.0,135559.0,So does it means I have to use 10 x 72k columns ?,311466.0
141574,611662.0,135700.0,TA please verify,318455.0
141574,611662.0,135699.0,Its hanging while running code. How to resolve this?,320103.0
141941,613046.0,135762.0,use filter,300694.0
141941,613046.0,135759.0,"But how to get for particular users like cokes, etc.",314730.0
141941,613031.0,,nan,
141941,613243.0,,nan,
141933,613690.0,,nan,
141935,613154.0,,nan,
141935,613302.0,,nan,
141935,613989.0,,nan,
141935,612985.0,,nan,
141935,613082.0,,nan,
141935,613247.0,,nan,
141649,612098.0,,nan,
141649,612344.0,,nan,
141649,612736.0,,nan,
141785,612466.0,,nan,
141785,613091.0,,nan,
141666,612323.0,,nan,
141666,612112.0,,nan,
141666,612320.0,,nan,
141587,611715.0,,nan,
141587,611834.0,,nan,
141587,611742.0,,nan,
141587,612327.0,,nan,
141587,611865.0,135614.0,still i did't get the values,310624.0
141587,611865.0,135660.0,Did you have reduced the data by choosing the N?,428646.0
141587,611865.0,135712.0,You need to first remove those beers for which the number of ratings is significantly low. Just try to get the count of rating for each beer and then try to figure out the optimal N that you should take. After this remove all the beers that have the number of rating less than N.,428646.0
141587,611865.0,135679.0,no how to do it ?,310624.0
141601,611829.0,,nan,
141601,611659.0,135554.0,Thanks,318598.0
141706,612716.0,,nan,
141263,610087.0,135432.0,Thanks!,311254.0
141538,611838.0,,nan,
141538,611665.0,,nan,
141359,610678.0,135394.0,ok thanks,318005.0
141829,612577.0,135724.0,Hi facing the same issue.. how did u resolve it,308644.0
141829,612781.0,,nan,
141829,612563.0,,nan,
141829,613159.0,,nan,
141791,612527.0,,nan,
141681,613085.0,,nan,
141681,612291.0,,nan,
141681,612337.0,135655.0,This hasn't been specified to choose 10 best values for each of the 10 beers. TA please conform it !,311466.0
141681,612337.0,135670.0,I'm not saying to list down all the columns. we can just take 1-10 rows and columns and show the correlation between them. we don't need to take best out the whole dataset it hasn't been specific anywhere.,311466.0
141681,612337.0,135668.0,"The 10 best values is just an indication to get a square matrix. As number of columns is large, it is pointless to list down all the columns, the heatmap is virtually unreadable if you intend to plot all the values",309211.0
141681,612400.0,,nan,
141681,612730.0,,nan,
141681,613036.0,,nan,
141877,613687.0,,nan,
141877,613807.0,,nan,
141877,614187.0,,nan,
