question_id,answer_id,answer_text,answer_owner
117508,507134.0,This is how you interpret the coefficients http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients,310974.0
117508,507207.0,"Feature selection should be based on the value of adujusted r square,AIC and BIC not based on the coeffiecient of the feature . Please have a look to the feature selection module: https://learn.upgrad.com/v/course/208/session/31156/segment/164598 Also we can use: VIF (Variance Inflation Factor) and variable selection using RFE (recursive feature elimination)",318476.0
117508,507587.0,"For this ridge,lasso assignment is it required to use rfe(vif) ?",312019.0
117508,507911.0,"In the array generated by Lasso, you will find many zeros which means those features are eliminated. Rest of them remain for which you will see the corresponding co-efficients. The values closer to zero (either negative or positive) have less influence on the target variable compared to bigger value. Big negative coefficient value has a big negative impact and big positive coefficient will have big positive impact on target variable. Model evaluation shall be based on adjusted R2, AIC, BIC and Cp;",301121.0
117508,508003.0,"I think we need to use Regularization, therefore Lasso. And for model evaluation we can use adjusted R square, AIC, BIC",301643.0
119445,,nan,
117938,508577.0,"Hi, Keep it numerical, As it is ordinal so no need to treat as categorical",310974.0
117938,508692.0,convert from int to object and then you can use dummy,317514.0
118156,509430.0,"Scaling data should not cause issue even if you have float. If you're getting a warning, you can either ignore it. What exact issue are you getting?",318438.0
118156,509858.0,"There is no need to change this to Integer and the scaling does not introduce any error. Having said that, if you examine the data, all rows have zero after the decimal and it would not harm if you choose to change this to integer.",301121.0
117836,508110.0,if thr null values for a variable is very high then drop those variable and if the value are less then either impute them or delete the null rows not the complete variable.,318017.0
117836,508120.0,"Electrical : Yes, drop the rows as Null records are very less LotFrontage : If you see then Lot Frontage is around 1% of Lot Area for non null values, so for all Null LotFrontage you can calculate it as 1% of respective Lot Area MasVnrTpe : These are not Null, these are None so needs to be treated as categorical value MasVnrArea : Yes, drop the rows as you will loose very less records GarageYrBlt : Can be dropped as this his highly corelated with YearBuilt, and we have anouther column containing number of Garages, so this coulm does not add any value Let me know if this helps",304814.0
117836,509013.0,Assign some dummy variable like 'X' for NA values for categorical columns. Since deleting rows might result data loss and more over here its 'NA' which expected value in some case its not equivalent null.,301118.0
117836,509514.0,"I suggest to see https://learn.upgrad.com/v/course/208/question/117895 Also, pelase see the data dictionary before deleting any row or column.",312479.0
118417,510725.0,Ideally scaling needs to be done after the train and test split so that your test data does not influence the scaling of train data in any way. The same scaling(mean and variance in case of standard scaler) needs to be applied to the test data. Test data needs to be hidden from you when creating the model. Hence first split then scale.,318762.0
118417,510731.0,"There could be certain specific usecases where you would prefer doing scaling before slit. For instance, with minmax scaler, your training data will be scaled between 0 and 1 but your test data could get scaled between say -1 and 2. If you want to have your test data to be scaled strictly between 0 and 1, you would scale before split.",318762.0
118417,510968.0,"@ Uddhav Phondba Your both Answers are contradicting to each other, I actually didn't get what you were intended to tell.",310507.0
118417,510992.0,@Sai Prasanna First answer talks about the standard recommended approach. Second answer talks about a rarely adopted approach for very specific usecases.,318762.0
118417,511816.0,TA can u pls verify this answer n provide some clarity,308437.0
117880,508256.0,"Not necessarily. The target is to achieve the best test ot validation success rates, whatever metric you are using depending upon the case at hand. In case your training aaccuracy is 100% leading to a test accuracy of ssay 90%. It will be preferable to having a model with training and test accuracy as 80%. Though that is just a rule of thumb and not a given as it depends on the computational power required to implement the model as well.",304281.0
117518,507125.0,You mean optimal values? That is fine since the cost function is different for Ridge and Lasso.,310974.0
117518,507204.0,Yes.Its fine because the Hyperparamter alpha try to reduce the equation E(Theta)+alpha*reglualization(theta) and as the regularization terms are different we migth get different alpha value,318476.0
117605,507310.0,"Hey Srinivasan, I think you can go ahead and impute those values with the mean of YrBlt column.",301655.0
117605,508047.0,Can anyone plz suggest what can be done with this column?,301114.0
117605,507324.0,"But these are the recods which do not have anay Garage ( i.e. No garag). If we impute mean, then it might lead to incorect information. I am stumped what to do here.",304814.0
117605,508104.0,"Drop it as this column is same as year built, and we have another column containing number of garages, so Garage year built does not add any value",304814.0
117605,507836.0,I have imputed it with an invalid numeric value. For eg with 2020. So when I am calculating the age of garage it comes to -1 (an invalid value). I am not sure whether this approach is correct or not?,304319.0
117605,507937.0,I think we should convert this to age(how old is garage) from currnet year. For those non-existence values should be made a high number becuase the age should have a negative affect on the price.,306248.0
117605,507943.0,"We should convert into age and then bin the values as New, Mid-aged, Old, Very.Old and None",306248.0
117605,507944.0,"I think the year NA values can be imputed with the mode value i.e. maximum occuring value in the data set. However, I am not sure how to handle certain places where it doesn't practically make any sense. e.g. if there is no garage in house, how to assume the year if there is no garage. Not sure how to make the model realize this situation.",311729.0
117605,508165.0,I think we should consider the year as categorical and bin the values because price could be estimated based on the age of the building.,318804.0
117605,508736.0,"Hello All, This is what I have done and probably this would make sense. For all three year fields, I had created 3 numerical feature with the calculation ""2019 - feature value"" This will populate the new columns with ""age' of garage, house, remodadd etc. I dropped all those original columns after which I had imputed null fields with mean of calculated age . This way, actual ""year"" may not make sense but their actual age would and it is meaningful impute this way.",301121.0
117605,510261.0,"The column can be dropped. Looking at the YearBuilt and GarageYrBlt, only 290 records have a different value",312259.0
117950,508648.0,you need to import 'metrics' from sklearn it seems.,311686.0
117950,508685.0,"or if you have already declared ""from sklearn.metrics import r2_score"" then you can change the code to print(r2_score(y_true=y_test, y_pred=y_test_pred)) alternatively declare ""from sklearn import metrics""",317514.0
118441,510518.0,Are you using it before initializing?,318329.0
118441,510521.0,No,303228.0
118441,510546.0,it is not a key word so you need to intialize it. you must have not initialized it and have been calling it.,302738.0
117468,506919.0,,319759.0
117468,506955.0,"Please check your dataframe, will be having some categorical variable. which might be creating the problem.",308965.0
117468,507049.0,Solved..Thanks,319759.0
117468,507044.0,"to select column containing only numerical Variables, you can use one of following : num_col= df.describe.columns or Numeric_df=df.select_dtypes(include=['float64', 'int64']) Then proceed with Scaling: df[num_col]=df[num_col].fit_transform(df[num_col]",317984.0
117267,505708.0,A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.,310467.0
117267,505786.0,"Hi Madhusudhan, You can get more details from below link apart from what Keerthi mentioned. https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c",308673.0
118448,510569.0,"did you perform outlier treatment? i was facing the same problem, outlier treatment worked for me.",302738.0
118448,510728.0,Make sure to perform scaling on both predictor and independent variables.,310974.0
117276,505920.0,There is a way to make python not treat NA as nulls https://stackoverflow.com/questions/33952142/prevent-pandas-from-interpreting-na-as-nan-in-a-string,310974.0
117276,505834.0,"If you think NA values are signigicant and can not delete them, impute/replace it with appropriate values.",307495.0
117276,507459.0,"So we should we import all the values from the csv file as it is like using ""housing = pd.read_csv(""train.csv"",keep_default_na=False)"" as Ram mentioned in the above comments? These should not be misinterpreted as NaN ? I have found some 5 columns have genuine NAN values which needs to be treated.",301114.0
117276,505767.0,"If the values are significant and relevent, we can impute them. See if we can assign the value based on the existing values(mean, mode etc). If not, we can assign a different value. ( for example 'none' or 'no' if it is a categorical field).",310467.0
117276,506086.0,"If its categorical case We can fill with Unknown After making dummy variables remove this Unkown column. Because we dont' know the default option If the Unknown is more , we can remove and should mention its important.",312019.0
117276,506109.0,"We can impute the value by ""NotApplicable"" perhaps which sort of makes more sense.",307176.0
118433,510484.0,"in the data dictionary NA values for basment are present where there is No Basement. i dont know if its the right approach or not but what i did was i imputed those NA values by No Basement. similar for the Garage type columns, i replaced NA by No Garage.",302738.0
118086,509885.0,"Hi, Please go through the below link https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/",344894.0
118367,510237.0,"In general Lasso will remove multicollinearity by making the coeff 0. https://waterprogramming.wordpress.com/2017/02/22/dealing-with-multicollinearity-a-brief-overview-and-introduction-to-tolerant-methods/ Just curious: How are you checking the multicollinearity of the features post Lasso regularizaton? You can also increase the alpha value to impose stricter penalty on features, but make sure this does not impact your overall model score.",318438.0
118367,510236.0,How did you confirm that it has multi collinearity? VIF?,318329.0
118100,509018.0,"hi, there might be several reasons for this. will be difficult to tell exactly. but pls check following aspects: 1. scaling is being done after split 2. correct features are created in data preparation step 3. you can also try without scaling target as was shown in the example. btw how are you measuring 'accuracy' here? I understood about R2 but what are you using for accuracy?",311686.0
118650,511814.0,please refer the following link for TA verified answer for a similar question: https://learn.upgrad.com/v/course/208/question/118417,302738.0
118650,511901.0,"Actually its a never ending question, and when you search for the answer you can notice that there is no correct approach amongst both. I also went through different links and found that we can't say which one is correct but we can say scaling after test-train split is recommended. A reasonable answer amongst all the link is given by Ian Lo in below link https://www.quora.com/Should-scaling-be-done-on-both-training-data-and-test-data-for-machine-learning-Can-one-do-scaling-on-only-the-training-data Also visit below link, it has good explanation https://sebastianraschka.com/faq/docs/scale-training-test.html Below links suggest for scaling after test-train split. https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data https://stats.stackexchange.com/questions/267012/difference-between-preprocessing-train-and-test-set-before-and-after-splitting So at last we can say that scaling after test-train split is recommended. Hope this will help.",317991.0
118102,509309.0,Its better to retain them and leave it to feature selection algo's (RFE/Lasso) than we removing them without knowing the impact.,318329.0
118102,509864.0,"I am not sure if you are getting dummies of categorical features one by one, Easier way to do this: Filter the categorical variables based on datatype as object Group this on single dataframe df_cat Execute the ""get dummies"" command. df_dummies = pd.get_dummies(df_cat, drop_first=True) Later you can concatenate with one command after dropping the original fields Lasso will do the rest by dropping unnecessary ones.",301121.0
118103,509021.0,"hi, they can be used as numerical directly. plot a scatter plot between SalePrice and these columns and you will see the trends there suggesting that they can be used as numerical directly.",311686.0
117317,,nan,
117322,506080.0,"For Ridge, we should do RFE. For Lasso, we don't need to.",310974.0
117322,506951.0,"In Lasso, number of feature which will be having a non-zero coefficient will be more. so how should we select the best feature bring it down to 10 to 15 ?",308965.0
117745,507839.0,Did not get your question???,304319.0
117745,507898.0,"Evaluation of any model shall be based on Adjusted R2, AIC, BIC and Cp or cross validation prediction error.",301121.0
118074,509039.0,"Hi, Lasso+GridSearhCV is equivalent to LassoCV. Please see the example in below link https://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html",344894.0
118074,509109.0,"Thanks! I did go through that example. However, when I try it on my dataset, the alpha value that I get from GridSearchCV is different that when I use LassoCV. Is this normal? If so, which one should I pick, the best alpha identified by LassoCV or Lasso+GridSearchCV.",318438.0
118044,508795.0,"Please refer - https://learn.upgrad.com/v/course/208/question/117882 . Hope, it will answer your query. Thanks.",311502.0
117336,506124.0,yes. SalePrice column is there. this is the target variable.,311686.0
117760,508374.0,"Look at TA verified answer from this link: https://learn.upgrad.com/v/course/208/question/117896/answer/508302 As per TA, You can use RegscorePy. Please go through the link below https://pypi.org/project/RegscorePy/",309211.0
117760,507896.0,"Evaluation of any model shall be based on Adjusted R2, AIC, BIC and Cp or cross validation prediction error. For feature elimination, by its nature Lasso is capable of eliminating features by forcing the coefficients to zero.",301121.0
117760,508129.0,Try this after you perform regression y_pred = model.predict(X_test) resid = y_test - y_pred sse = sum(resid**2) k= # of variables AIC= 2k - 2ln(sse),304814.0
117760,508131.0,Further guidance would be appreciated :-),304814.0
117760,508193.0,https://www.researchgate.net/post/What_is_the_AIC_formula,312019.0
117760,508372.0,Use sklearn LassoLarsIC https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html,318438.0
117760,509856.0,"An interesting article on similar lines: https://stackoverflow.com/questions/39920862/model-selection-for-gaussianmixture-by-using-gridsearch Copy pasting from the above post: "" Yes, but GridSearchCV is specifically designed for cross validation. Cross validation is an alternative to the BIC/AIC, and so using the BIC/AIC with GridSearchCV doesn't really make sense. One should use one or the other. """,318438.0
118134,509409.0,"Even a single null value in the column of the dataset may throw that error. Make sure you remove all the NaN values from the dataframe , before proceeding for calculations.",318557.0
118134,509179.0,please remove the null values from your data as pairplot cant convert null so do a proper data cleaning.,318017.0
118134,509660.0,"Y ou need to be fairly methodical to take care of all the null values either by dropping or by imputing as the case may be, I did the following way. I would suggest you do the following:steps Drop those columns which have very high number of many null values Use the value_count() for those having null columns Impute null columns with maximum occuring ones Impute the numerical columns with mean preferably. Please do take care of the years by converting to age and delete original year columns Now go for pair plot,",301121.0
118134,509999.0,"If you are replacing numeric values with mean or median then use () while replacing it. Exa... replace(np.nan,house['LotFrontage'].median() You might wriiten as - replace(np.nan,house['LotFrontage'].median",304812.0
118082,509884.0,"Hi, If you find there are many outliers then take the log of that feature.",344894.0
117774,507892.0,"It is not a good idea to replace NAN with zero, as zero could be a data by itself.",301121.0
117774,508126.0,"Nope, please dont do that.",304814.0
117774,508155.0,"Go through the Data Description, you will get to know if NAN should be replaced with Zero or not.",318804.0
117359,509946.0,"with long list of variables after creating dummies for each feature, it is impracticle to perform manual regression models. Guess it is better to use Ridge and Lasso directly",312199.0
117359,506890.0,you can do rfe as its not possible to proceed with one by one selection you can also make model using lasso which itself works as a variable selection technique.,318017.0
117359,506957.0,"In Lasso, variable selection will not be coming around 10 to 15 feature. If we large number of variable and we are selecting the optimal hyperparamter. then How should we filter further after lasso variable selection ?",308965.0
118161,509303.0,"We need to try a range of values before selecting the one for Lasso/Ridge. We would be doing a grid search with K-fold validation. So, whatever value we choose from the results obtained, we should be good with that.",318329.0
118492,510963.0,"Outright avoiding outlier treatment would not be good for your model nor would doing outlier treatment for all the variables would be ideal in such a case where we have less data points. Rather, Do it for some selected variables such that you are left with at least 70% of the original dataset.",302738.0
118492,510952.0,"490 records are too little. You should not do outlier treatment for so many features; Some of the features look numerical but they are actual categorical and do not do outlier treatment for them,",301121.0
118172,509682.0,This might help you https://medium.com/ai%C2%B3-theory-practice-business/three-effective-feature-selection-strategies-e1f86f331fb1,310974.0
117277,505918.0,They are definitely not nulls. They hold value in the analysis and should not be imputed or treated as nulls.,310974.0
117277,505769.0,"If the values are significant and relevent, we can impute them. See if we can assign the value based on the existing values(mean, mode etc). If not, we can assign a different value. ( for example 'none' or 'no' if it is a categorical field).",310467.0
117277,506123.0,"hi, while importing these values are getting converted as NaN. but during data cleaning step, we need to replace such NaNs with some value like 'None' or 'Not Present'. we will have to identify such features from data dictionary where NA has this kind of meaning and replace NaNs of those features.",311686.0
117277,506142.0,After dummy creation there are around 250 columns that we are getting....so do we need to do RFE and reduce it to around 15-20 variables and then do lasso / ridge? please suggest,320103.0
117816,508124.0,"If you see then Lot Frontage is around 1% of Lot Area for non null values, so for all Null LotFrontage you can calculate it as 1% of respective Lot Area. You anyways would end up dropping this column later as this is corelated with LotArea",304814.0
117816,508097.0,"I don't think 0 is the right value to use as the Linear feet of street connected to property can't be 0. Try finding other ways of imputing like IQR, Mean etc",310974.0
117816,508753.0,You can impute with the mean,318756.0
117816,508744.0,"Domain knowledge comes in very handy while imputing any value. Pay attention to data dictionary , derive other statistic such as quartiles, median, mean for the column of interest and you will get a decent clue on which value to impute",309211.0
117816,508789.0,Replacing with 0 will mean that the property is 0 feet with the street and that will have an impact on the housing price model. We can replace the nan values with the median of lot frontage of the lotConfig for that property.,315028.0
118785,512709.0,Considered only values of the hyper parameter that are less than 200.,310974.0
118785,512836.0,"Initially we are trying out various values of alphas for modelling and then we need to tune it to find the optimal value. The condition is used to restrict the values of hyper parameter uptil a certain limit such as the trend is clearly seen in the graph (negative mean Vs alpha). If you remove this condition, you will see the graph obtained will not be much readable to predict optimal alpha because of massive range on y-axis (in that case y can be represented in log scale so that it fits in 2D graph)",316147.0
118785,512858.0,"Hi, Cv_results = pd.dataframe (model_cv.cv_results_) model_cv.cv_results_ &gt;&gt;&gt; will gives you results based on hyperparaemeter. pd.dataframe (model_cv.cv_results_) &gt;&gt; Here you are converting model_cv.cv_results_ to pandas dataFrame Cv_results= cv_results[cv_results[param_alpha]&lt;=200] &lt;&lt; Here you are choosing dataframe of results which has param_alpha &lt; 200",344894.0
117695,507767.0,There are too many variables. Not sure how to handle the outlier treatment. Same doubt. To see and decide correlations table or heatmap also difficult.,312019.0
117695,507773.0,Are you removing the outlier after sandazation ?,318476.0
117695,507835.0,Remove the outliers for only those features which do not show a gradual increase of values. You need to see the values at various percentiles and decide on the columns on which outlier removal to be performed. TA can verify this.,304319.0
117695,507940.0,"Since the number of records are quiet less and the underlying variables after creating dummies would be large, I believe the rows should not be dropped. However, the outlier values can be replaced to the cut-off values in each variable.",311729.0
118214,509420.0,1) Lasso will give you a list of features which are relevant (coeff is not 0). You can use these filters to create a ridge model. Make sure to use the same features for the test set as well 2) Importance of features is directly related to the coeff. Higher the coeff the higher the importance of that feature and vice versa,318438.0
118214,509669.0,"Hi Aaditya I understand you would like to relate the coefficients and corresponding feature name. You will be able to get all those columns and corresponding coefficients using the following commands cols = X.columns cols = cols.insert(0, ""constant"") list(zip(cols, model_parameters)) If the coefficient is high either in positive or high in negative, both cases have high impact on target variable. features with coefficient close to zero will have less impact on target variable. features with zero coefficient has no impact on target variable.",301121.0
118236,509627.0,"R-squared improved when I left the outlier treatment for lasso while I got a lower score when I did treat outliers manually. So, I assume lasso will take outliers into account.",318329.0
118236,509672.0,"Doesn't matter which model you build, it is akways recommended to do outlier treatment. It is not something you turn on or off for model accuracy.",310974.0
118011,508695.0,"Hi, Your graph is showing positive 'Negative Mean Absolute Error', It should be negative. Based on R2 Ridge regression is fine but Lasso regression looks like overfitting Thanks",344894.0
118243,509625.0,"Create a series out of coefficients and sort the series. Using the index of the values, you could map to original columns.",318329.0
118243,509817.0,"I understand you would like to relate the coefficients and corresponding feature name. You will be able to get all those columns and corresponding coefficients using the following commands cols = X.columns cols = cols.insert(0, ""constant"") list(zip(cols, model_parameters)) I hope this helps",301121.0
118332,510124.0,MAE should be preferred over MSE when ouliers are present in the data. MAE is less sensitive to outliers than MSE. Below image shows MAE is steady while RMSE increases as the variance associated with the frequency distribution of error magnitudes increases.,318730.0
118332,510150.0,"What if there is no outlier, or I have preprocessed my data to take care of it? What is the business significance and how do you exaplain what a MAE of 'X' means over a RMSE of 'Y'.",318438.0
118332,510205.0,"1) If there are completely no outliers present in the data or if all of the errors have the same magnitude, then RMSE=MAE. Otherwise, in all cases RMSE will always be larger than MAE. 2) From business perpective I may not be able to provide any concrete examples, but, if you want to penalize the model more when there is atleast one huge error (i.e. variation in errors is high) then pick RMSE, otherwise MAE.",318730.0
118299,510905.0,"Hi, Please follow the below link https://datascience.stackexchange.com/questions/11928/valueerror-input-contains-nan-infinity-or-a-value-too-large-for-dtypefloat32 https://stackoverflow.com/questions/34779961/scikit-learn-error-in-fitting-model-input-contains-nan-infinity-or-a-value",344894.0
118462,510724.0,You come up with the best linear model based on the optimal value of lambda.,310974.0
118462,510807.0,It's rather a regularized regression. Though you could try changing some of the features with polynomial transofrmations and do a generalized regression.,318329.0
118249,510223.0,"You can use below line of code. It will only show columns that have null values. print(round(100*(housing_df.loc[:, housing_df.isnull().any()].isnull().sum())/len(housing_df.index), 2))",314621.0
118249,509657.0,"round(Df.isnull().sum()/len(Df.index),2).sort_values(ascending=False) You can use this",311254.0
118249,509749.0,"Do you mean you can't see all the columns in Jupyter notebook? You can use : pd.set_option('max_columns', None) and this will display all the columns",318438.0
118249,510510.0,"Use the following commands: pd.set_option('display.max_colwidth', -1) pd.set_option('display.max_rows', 999)",312953.0
118080,508921.0,"Each Feature can be modelled differently. i.e. a*x1 +b*(x2*2)+c*(sqrt(x3)) Above equestion will be Linear equestion. If you passing your features direct without doing feature transformer, then equestion will become polynomial.",318438.0
118258,509803.0,Lasso Regression will force the coefficients of non-relevant variables to zerio and hence they will turn ineffective in the model. I hope this clarifies.,301121.0
118273,510820.0,"Hi, It is expected from current dataset. You need to find R2 on train &amp; test data.",344894.0
118570,511581.0,No we are not supposed to scale dependent variable only the numerical independent variables need to be scaled. refer the following link for TA verified answer: https://learn.upgrad.com/v/course/208/question/118277,302738.0
118277,509828.0,"SalePrice is Target variable and you donot scale it, You can transform the target variable",317514.0
118277,510062.0,I think the jury is split on whether we should or should not scale the output variable. https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re https://www.quora.com/Do-I-have-to-normalize-the-output-variables-also-when-Im-normalizing-input-variables-for-gradient-descent Maybe you can check if scaling impacts the overall model accuracy significantly.,318438.0
118277,510258.0,"I did a small test to check this. I sclaed the output variable as well and when I ran the Lasso model on it, the alpha values became very small, but, the acuracy/score on both the training and test data remained almost same . Also, RMSE remains same/similar to without scaling, and the coefficients appear to be same (I did not go in much detail, just visual inspection). So appears as if the scaling of dependent variable does not impact the model performance.",318438.0
117882,508267.0,"You only need to do feature selection, hence use the lasso. model. Use ridge only to find the optimal value of lambda as required in the second part of the assignment.",304281.0
117882,509652.0,"Same here, Are we considering the Lasso for identifying the optimal value of lambda and corresponding scores As per the statement above, are we considering only for feature selection. I think the feature set obtain in Ridge (based on the coefficient values) will be different from that one produced from Lasso. TA, Please clarify.",311115.0
117882,508793.0,"I am not sure I undertstand why we need to serialize the 2 models. The intent of both models is to simplfy/regularize a linear model. In Lasso, the unimportant features will be eiminated, whereas in RIdge their coeff would be close to 0. If we feed the input of one to another how can be compare the two?",318438.0
117899,508305.0,You can have a look at the answer provided in the following link. Essentially it is trial and error. https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression,304281.0
118644,511905.0,"Hey Madhusudhan, The lasso method requires initial standardization of the regressors so that the penalization scheme is fair to all regressors. For categorical regressors, one codes the regressor with dummy variables and then standardizes the dummy variables. Refer below link for more clarity - https://www.quora.com/How-bad-is-it-to-standardize-dummy-variables Hope this helps.",302742.0
118292,509993.0,"Alpha is same as lambda. In equations and theory the regularization parameter is mentioned as lambda, while in its sklearn implementation, it is referred to as alpha. https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html Loss objective -&gt; (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1 alpha : float, optional Constant that multiplies the L1 term.",318438.0
118175,509361.0,"Lasso regularization is supposed to be robust to outliers by minimizing the error inspite of outliers being present in dataset. Unless the outliers are way outside of the dataset, Lasso should work well and this should reflect in model performance.",301644.0
118175,509302.0,"Outliers would be applicable for Numerical features while dummies are effective on categorical. So, it doesn't matter but do it before dummies.",318329.0
117849,508337.0,TA has answered for ths quextion; Lasso takes care of this. https://learn.upgrad.com/v/course/208/question/117892,301121.0
117849,508611.0,"Need some more inputs on this. Please check if the following two points are correct. In lasso and ridge, 1.We check the linearity of the coefficients/weights/ model parameters with the output variable. 2. We do not check the linearity of the features/fields/columns with output variable. Multicollinearity/VIF check the correlation between the features. But here we need to check the collinearity between model parameters/coefficients. Hence we do not have to check VIF. Please confirm if this is correct.",310467.0
117849,508927.0,Ridge and Lasso take care of multicolinearity. In case of Ridge it distribute the weights to all features trying to keep overall sum minimum. In Lasso it chooses 1 random feature having multicolinearity and makes weights as zero.,317689.0
118302,510002.0,It's good to have high R squared value. are you talking about overfitting?,318329.0
118302,510014.0,We need to scale the test data before checking the R2. Can you double check wheter X_test scaled.,307495.0
118302,510477.0,scaling does not affect r2 scores. only the coeffecients change,318438.0
118302,510720.0,Are you talking about r2_score for train or test? If high for both then good... If not then recheck your model...,318479.0
117853,508274.0,I would suggest you do Lasso instead of RFE as Lasso would eliminate using zero coefficients You can use Ridge on that.,301121.0
117853,508339.0,Just saw that TA has answered to a similar query. https://learn.upgrad.com/v/course/208/question/117882,301121.0
117854,,nan,
120162,520976.0,Take a sample of 20% of data from the train.csv data set. You can use train_test_split.,310974.0
120162,520983.0,We should not make any modifications in the excel. All modifications should be done after loading data into python.,310467.0
120162,521336.0,Use slicing of the dataframe and save in some other dataframe with 20% of the main data.,318585.0
120162,521581.0,Take a sample of data once it loaded into dataframe. So you have to read all the data from csv file and then make a sample out of it.,317412.0
117489,507012.0,"You can use this command to get all the columns with type ""object"". It is not exactly what you need but can reduce the columns df.select_dtypes(include=['object'])",312758.0
118820,512863.0,"Alpha can range from 0 to infinity. There is no guideline to what an optimal value is. It depends on case to case basis, hence it's called a hyperparameter, a value that you decide to meet your business objectives. For a range of alphas, you determine the train and test score (neg mean absolute error or neg mean square error). The value of alpha at which the test score is minimum should be the optimal value. At large alphas, lot of coefficieny will be zero, and your model will be too simple (high bias), where as at low values of alpha, there will be many featues (high variance). Objective is to balance the bias and variance",318438.0
118820,512862.0,"Hi, 1. how to select range of alpha? what is the guideline? You can start alpha minimum value like 0.0001 and keep increasing with some more difference like 0.1, 1, 2, 5, 10, 20, 50 ,100, 1000 Please follow the below link for more details https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models 2. how to chose optimal value of alpha? if i keep increasing value of alpha for lasso, the no of coefficients with zero keep increasing, so when to stop? Here you need to plot graph between alpha and your error term or R2 , and then you will decide what is the optimal value of alpha.",344894.0
117868,508500.0,it is similar to previous regression assignment just see what needs to be done for variable selection,317982.0
117868,508917.0,Same Question... its little perplexing at the moment. It is different from Previous Assignments.,300734.0
117868,509173.0,Data Cleaning:Check out for NA values. For eg:PoolQC has NA values. NA here means not null. It means no pool.So fill NA with none/no pool. Look out for other such columns. Once done treat the categorical values with pd.dummies Perform scaling if required Split into train and test Apply lasso regression which will help in feature selection . Then using these features use the ridge regression.,301114.0
117185,505495.0,Ridge and Lasso are alternatives to plain MLR. They address issues such as multicollinearity and a huge number of variables in the dataset. So yes as Anshul pointed model building is to be done using - Ridge and Lasso.,311857.0
117185,505171.0,"I am no expert here and have some other doubt, but I dont think we need to preform MLR here as we need to perform Regularized Regression here. After doing Reg Regression, these alogorithm will let us know the suitable features as well.",304814.0
117185,506097.0,"I believe you can either chose Ridge or Lasso to complete the assignment. They didnt ask specifically to build model using both. If I were you, I'd use Lasso as it also helps you select the best set of features.",307176.0
117185,506959.0,We have to use both Ridge and Lasso as per the assignment. In the second part of assignment we have question relation to hyperparameter of ridge and Lasso.,308965.0
117185,507047.0,"I believe MLR using statsmodels.api for important Feature selection and Lasso, Ridge for regularization.",317984.0
117495,507136.0,This has been discussed at length in the course videos. You use cross validation to tune the hyper parameter whcih is lambda for regression.,310974.0
117495,507210.0,In the model below we have a working code to how to get the alpha values uisng Girdsearch: https://learn.upgrad.com/v/course/208/session/31156/segment/164587,318476.0
117495,507784.0,http://wavedatalab.github.io/machinelearningwithr/post4.html should help this link.,312019.0
118154,510810.0,"With real life data practically never being normally distrubuted, I think this should have been covered to some extent in the lectures. Reading more about it, it seems that we cannot apply log transformation to all cases (negative or zero values). log1p, BoxCox transformation seems to be more robust. Will try to apply it to the housing dataset to see if it improves the model",318438.0
118154,510798.0,"Hi, 1) What are the common transformations? We generally do log transformation on the skewed dataset 2) If we transform the training dataset, should the same transformation be applied to the test dataset? Or should ensure that the test dataset is actually skewed before applying the transformation? Yes, If we transform the training dataset, then you also need to do the same transformation on test data set https://stats.stackexchange.com/questions/267078/why-is-skewed-data-not-preferred-for-modelling https://becominghuman.ai/how-to-deal-with-skewed-dataset-in-machine-learning-afd2928011cc",344894.0
118271,509835.0,You are using indepenent variables to predict dependent variable. You are getting all the indepenent variables to the same scale so that you can determine the impact of independent variable on dependent variable while keeping all other independent variable constant. No need to scale dependent variable or y variable,317514.0
118271,509930.0,You can also visit below link for TA verified answer. In that it is mentioned that you do not need to scale dependant variable i.e. 'Sale Price' variable. https://learn.upgrad.com/v/course/208/question/118277,317991.0
118291,510006.0,use MinMax if you want to do scaling at once as it doesn't affect categorical,318329.0
118291,509915.0,Categorical variables will have only dummies created for them. Once the dummies are created there is no scaling required. Numeric variables will only be scaled.,310505.0
118291,509928.0,Categorical variables will be converted to dummy variables. And we don't scale dummy variables. So we only have to scale numeric variables. Hope this will help.,317991.0
118333,510139.0,you can check the r2 for the lasso and ridge regression to check the correctness of the model.,318017.0
118333,510196.0,"For both Ridge and Lasso, find the value of lambda for which the mean absolute error is minimum. For that lambda you can find the r2 for Train and Test data",317514.0
118333,510214.0,"An interesting article on similar lines: https://stackoverflow.com/questions/39920862/model-selection-for-gaussianmixture-by-using-gridsearch Copy pasting from the above post: "" Yes, but GridSearchCV is specifically designed for cross validation. Cross validation is an alternative to the BIC/AIC, and so using the BIC/AIC with GridSearchCV doesn't really make sense. One should use one or the other. "" in case you are using GridSearchCV, you can take the best score from the model to be the determining factor",318438.0
118361,510210.0,"Between Lasso and Ridge, which one would you choose and your reason for doing so.",318438.0
117878,508259.0,Lasso and Ridge regression are similar to simple linear regression other than the regularisation. So the evaluation will be the same as for simple linear regression.,304281.0
117881,508263.0,It depends on how you plan to solve the problem. You can solve it by breaking it into three columns or treat it as a single column.,304281.0
117512,507131.0,Scaling neither increases nor decreases the coefficients. Effect on the coefficients is more of a after effect of scaling the features.,310974.0
117512,507598.0,"Please refer to the link given below https://learn.upgrad.com/v/course/208/session/24636/segment/126227 As far as the coefficients of features are concerned, please note It is important to note that scaling just affects the coefficients and none of the other parameters like t-statistic, F-statistic, p-values, R-squared, etc. How it affects, Increase or decrease depends on various factors.",301121.0
118028,508745.0,"Yes, loop it... Something like this Col=[col1,col2,....] For i in col: Df[i]= whatever ...... or please follow the below link https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame",304814.0
117891,508285.0,It could be due to the data cleaning steps that have probably left one of the features which is extremely strong and hence is dominating the results.,304281.0
117891,508289.0,"You may need to adjust the parameter values, start with a very small value scale and try again.",306725.0
117891,510448.0,Hi Mantha I am also getting same kind. Couldnt figure it out? Did you sort out the issue. I havent done outlier treatment and I have dropped Garage-Yrblt. May be these things are affecting. What about you?,308638.0
117892,508287.0,Multicollinearity among variables in a Lasso is not expected and it is indeed expected to take care of it automatically. Please have a look at the model in case you are facing the issue.,304281.0
118219,509425.0,"try to visualize these features and see if they have an impact on the SalePrice. If not, I'd recommend you still let them be there and let the model either drop them or give them a very low score.",318438.0
118219,509809.0,"This is what I have done and probably this would make sense. For all three year fields, I had created 3 numerical feature with the calculation ""2019 - feature value"" This will populate the new columns with ""age' of garage, house, remodadd etc. I dropped all those original year columns after which I had imputed null fields with mean of calculated age . This way, actual ""year"" may not make sense but their actual age would and it is meaningful impute this way.",301121.0
118219,509912.0,Try not to drop the year columns. The model after applying the regularization methods will determine if it is required or not.,317460.0
118219,510021.0,Take the diff of year built and year sold and create new featueebsay aging on sold. This will definitely has impact on pricing. Similarly u can do for other year columns,315679.0
118360,510204.0,"The question is about, which regularization will you choose. There is few difference in L1 and L2, it might perform differently. Which regularization will fit in which scenarios.",317990.0
117895,508301.0,"Make sure that you go through the data dictionary and see what the missing values actually mean. For example, the value for the number of swimming pools in a flat might be missing. Though that might imply that the flat has no swimming pools, rather than a case of missing value. So make sure that you check the data dictionary for the same. Furthermore, since it is a pretty small dataset with only 1460 observations, try to keep as many as you can.",304281.0
117896,508302.0,You can use RegscorePy. Please go through the link below https://pypi.org/project/RegscorePy/,304281.0
118643,512094.0,TA can u pls help me in dis,308437.0
118643,512095.0,Lasso is computationally expensive. How many alphas/CV folds are you trying to test on? try to reduce those numbers and get some output and then re-train in the vicinity of the alpha you get. Try restarting you jupyter kernel as well (in case not tried already),318438.0
117900,508307.0,The same question has been answered at https://learn.upgrad.com/v/course/208/question/117899,304281.0
117900,508584.0,Cross validation ofcourse.,310974.0
117918,508446.0,Here is the link in discussion forum on deciding the number of folds. https://learn.upgrad.com/v/course/208/question/115855,301121.0
119240,515250.0,Optimal lambda /alpha can be found using GridSearchCV. Both the CO2 and Car pricing assignment have covered this.,310467.0
119240,515133.0,alpha is the same as lambda. that part is covered in the mentioned exersises you can go through them also please refer the following links for TA varified answer for similar type of query: https://learn.upgrad.com/v/course/208/question/118292 https://learn.upgrad.com/v/course/208/question/117899,302738.0
119240,515523.0,"thanks , didn't knew alpha is same as lambda. for the 2nd question which lambda to choose n why. ? Do we go be lasso 100 ?",308495.0
119240,516289.0,"we hv to plot graphs of mean test score for various alphas and see the elbow of that curve,i.e the point after which score starts decreasing gradually",318005.0
117603,508089.0,Fine. But how find the final features using lasso or ridge for the best SalePrice. Because we are considering all column features here.,312019.0
117603,507250.0,I guess you need to do RFE before Ridge regression. And Lasso will itself take care of feature slecetion. Then you can select the best features using the coefficients. There should be a consistency between the coeff obtained for ridge and lasso.,304319.0
117603,507328.0,"That is where TA need to come in. We are clear about lasso as it takes care of the feature elimination by making the coefficients to Zero. For Ridge, do we need RFE I am not sure as Righe also is part of Embedded elminiation technique. Even though Ridge does not make coefficients Zero, but it reduces it towards Zero, so CAN we eliminate features which are clsoe or Zero? If we do RFE before Ridge, then what is the benefit of Ridge? I mean why can not we directly do RFE and get over with it? ( Just like we did in Multiple linear regression)",304814.0
117603,507580.0,I think for Ridge and Lasso not required to do RFE. Is its correct ? Then how to know what best features came by using rdige and lasso. based on Car+Price+Prediction+-+Ridge+and+Lasso+Regression.ipynb and generalised_regression+(2).ipynb we can build both ridge and lasso. but how to know what are the best features these models selected for solving. we needs to give these features as answers.,312019.0
117603,507901.0,"Lasso does the function of eliminating the features by forcing the coefficients of features to ""zero"" where as Ridge regression cannot do that.",301121.0
117603,508135.0,"Take top 10 variables with highest coefficients.Higher the coefficient, higher the impact on Price.",304814.0
118031,509879.0,"Hi, it will be equal to the number of features(columns) in X_train.",344894.0
118131,509114.0,i believe it depends on the variables if the variable seems to have outliers then remove the outliers.,318017.0
118131,509115.0,"I believe so. Outliers generally impact most of the models we have studied so far. https://www.reddit.com/r/AskStatistics/comments/4o42u2/is_lasso_robust_to_outliers/ Not sure, how much of an impact it would have on the relatively small dataset we have.",318438.0
118131,509367.0,"Lasso regularization is supposed to be robust to outliers by minimizing the error term inspite of outliers being present in dataset. Unless the outliers are way outside of the dataset, Lasso should work well and this should reflect in model performance. If not then outlier treatment is needed.",301644.0
118131,509499.0,"If you closely study the dataset, you can see few outliers. I believe, we should remove them.",312479.0
118131,509647.0,Outliers should be removed. Lasso is sensitive to outliers.,311254.0
118410,510315.0,A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.,318730.0
118410,510347.0,L1 stands for LASSO &amp;L2 stands for Ridge regression.,320689.0
118410,510726.0,L1 is Lasso and L2 is Ridge regression,306010.0
117888,508283.0,"Ok I think I found answer by googling. I was looking forward to know what's the ""case"" and the ""preferences"", it just cannot be something random; and there is definitely a logical reason to select the same. Am not sure if it is present in any of lecture notes, unable to recollect from the lectures. found explanation here : https://datascience.stackexchange.com/questions/43972/when-should-i-use-standardscaler-and-when-minmaxscaler StandardScaler is useful for the features that follow a Normal distribution MinMaxScaler may be used when the upper and lower boundaries are well known from domain knowledge (e.g. pixel intensities that go from 0 to 255 in the RGB color range)",309211.0
117888,508281.0,You can use any of the scaling functions depending upon the case and your preferences.,304281.0
118473,510902.0,"Yes go ahead, lasso will take care of the feature selection. You might have converted few variables from numerical type to categorical type that's why such a big number. In my case also, I have had 469 variables after dummy variable creations and lasso finally filtered it down to 124 variables.",318730.0
118473,511109.0,I think somehow pandas took some of the numeric variables also as categorical and created dummies. Check that once otherwise you shouldn't be getting these many.,310974.0
118293,509989.0,"LotFrontage is the linear feet of street connected to a house. Since most of the houses are similar in construction, you can impute it with the mean. I would refrain from dropping the column as it seems to be an important feature. Let the model decide how it weighs compared to other features.",318438.0
118293,510003.0,I did drop the rows because 1000 rows are retained after doing so and 1000 is reasonable I felt,318329.0
118293,510030.0,"LotFrontage: Linear feet of street connected to property. LotFrontage for a particular house would be similar to other houses in the same Neighborhood so you can impute the houses based on this assumption. To achive this you can groupby on the basis of 'Neighborhood', take mean/median of the groups and then impute the missing values.",318730.0
118293,510015.0,I vote for #2 option.,307495.0
118293,510221.0,The assignment only states that the property listed are from Australia. There are no zone or any other information to impute the value. Value ranges from 21 to 313. Given less than 20% data getting droped I shall go with droping thr rows with null value,317514.0
118293,513943.0,"Generally Lotfrontage depends on the Lot area, bigger Lot area mean larger lot frontage, i used transform to map the lotfrontage to lotarea for NaN values of Lotfrontage available, this reduced NA values by around 36, then using binning and groupby, over Lotarea and Lot frontage could managee to reduce the NaN values by 200",308495.0
117731,507941.0,"We should bin the values New, Mid-aged, Old, Very.Old and None",306248.0
117731,508132.0,"Can be dropped as this his highly corelated with YearBuilt, and we have anouther column containing number of Garages, so this column does not add any value",304814.0
117731,508380.0,You can assume that the garage was built in the same year as the house.,318438.0
117731,508258.0,"Hey Ruchita, I go with following thumb rule: 1) If NA percentage is &gt; 50% then more likely the column is less useful in analysis and I'd prefer to drop it 2) If NA percentage is about 5- 6% it is worthwhile considering dropping such rows as long as long as within acceptable % of drops. 3) I think it is a good idea to drop the NA values where pct NA values is about 5 - 6%, it does not cause a dent in the analysis. once done, the column can be used to compute age of garage Just my analysis and hope it helps. Thanks!",309211.0
118354,510345.0,,310509.0
118354,510273.0,"Check that the features/columns you are trying to scale are indeed numeric. don't pass the entire dataframe, just the columns that are numeric",318438.0
119355,516017.0,"You have to refer to the plot of Alpha against Negative Mean Absolute Error and find out that point where error starts improving for TEST data after which error increases. Corresponding Alpha value of that particular point is the optimal value of Lambda. In the upgrad lecture, it was 15 for Ridge and 100 for Lasso, just as an exmaple.",301121.0
119355,516165.0,"Cross-validation is an estimate of the expected generalization error for each λ and λ can sensibly be chosen as the minimizer of this estimate. The cv.glmnet function returns two values of λ. The minimizer, lambda.min , and the always larger lambda.1se , which is a heuristic choice of λ producing a less complex model, for which the performance in terms of estimated expected generalization error is within one standard error of the minimum. For more details, please refer: https://stats.stackexchange.com/questions/26528/how-to-estimate-shrinkage-parameter-in-lasso-or-ridge-regression-with-50k-varia",311117.0
90603,,nan,
90026,374853.0,These are the additional resources and not mandatory...U can go through the links and can gain extra knowledge,318358.0
90026,374833.0,No. Those are for addition learning experience.,318368.0
90026,374834.0,"Hi Shashank, As all the modules are well structured and included in this course with very minute details required to be a data scientist. Its always better to go through all the modules provided by Upgrad so that at the end of the day you will be benefited by it. A suggestion as a peer please don't skip anything. I hope this helps!! Happy Learning",300688.0
90026,374836.0,"Hi Shashank, They are additional resources and not compulsory but if you want to gain more knowledge, those resources would be very helpful.",300733.0
92342,388966.0,Yes. Same issue is with me as well. 5.7 MySQL doesnt support windowing functions that would be limitation of this version. Looks like we need to update to 64 bit OS with MySQL 8.0.012 version.,312623.0
92342,389194.0,"5.7 MySQL doesnt provide feature like windowing functions. Therefore, you need MYSQL version 8",317811.0
93232,392434.0,Commas are always used to separate different column otherwise query will fail. Semicolon is used to end the query ( though not mandatory),304814.0
93232,392400.0,"generally, commas are used when you specify different column names in one function.",319721.0
92390,389121.0,"ca you please share the snapshot, also if there are commands in the previous lines?",304813.0
92390,389547.0,Window functions will not work in lower versions. Please install mysql8.0..Before you install the new one. Please uninstall the older version.,318846.0
92390,389326.0,"That is Server version issues, use the updated server",318732.0
92390,389127.0,"Ok. try one thing. remove all commands above your query, and run your query once, and let me know what comes.",304813.0
92390,389131.0,"I assume you have tried to run your query by emptying the entire sheet. If there is no syntax error anywhere in the sheet, then below is true: This is a query with windows and hence supported only for servers 8+. Workbench will show errors for anything below that.",304813.0
92390,389104.0,"I dont think, pay column is there in employee table",317811.0
92390,389126.0,,306729.0
92390,389218.0,"Try following. It is working for me. There is no pay column. Replace it with salary. select ssn, concat(fname , "" "" ,lname) as emp_name , dno , salary , sum(salary) over (partition by dno order by ssn ROWS UNBOUNDED PRECEDING ) as cummulative_total , sum(salary) over (partition by dno order by ssn ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as one_above_and_one_below from employee;",317689.0
92390,389256.0,Windo functions are not supported in lower versions of mysql. You can use the latest one,319006.0
92390,389859.0,Check your SQL version.Window function are supported in the higer version of mysql.,318476.0
91846,384819.0,The same query is working fine for me. Post a screenshot of your workbench to check further.,313826.0
91846,384852.0,Screenshot of the workbench,311741.0
91846,384859.0,,311741.0
91846,384868.0,,311741.0
91846,385103.0,changed,311741.0
91846,385129.0,"Try: select e.ssn, e.salary,ROW_NUMBER() OVER ( order by e.salary ) as 'row_num' from employee as e;",313826.0
91846,385390.0,"select e.ssn, e.salary,ROW_NUMBER() OVER (partition by ssn order by e.salary ) as 'row_number' from employee as e; // You need to provide partition by. It is important that we provide this. The error seems to be because of this",301555.0
91846,386643.0,This doesn't seem to be syntax issue. MySQL 8.0 supports windows function. Check whether you have 8.0 and above installed at your end.,316147.0
91846,387048.0,"It worked for me, there is a bug in MySQL workbench we use. if things are not working fine and if you are sure about your syntax, try running it over MySQL shell. If you still wanna run only in MySQL workbench close it and re-open application. it works like swift. Hope it helps !",306735.0
91470,382703.0,"Yes. More than one window can be defined , but we need to specify the window keyword only once and each of the named windows should be seperted by a comma. Modifying your query as: select Dno, fname, salary, rank() over e as ranking,dense_rank() over e as denserank, salary,sum(salary) over w as cumilativesalary,avg(salary) over w as avgsalarydeptwise from employee window w as (partition by dno order by dno) , e as (partition by dno order by salary); Checkout the below link for more details on do's and dont's on named windows: https://dev.mysql.com/doc/refman/8.0/en/window-functions-named-windows.html",313826.0
91470,382761.0,"We can use 2 named functions at the same time. I tried below example and it worked. select ssn, salary,dno, row_number() over d as 'row_number_salary', row_number() over w as 'row_number_dno', rank() over w as 'rank', dense_rank() over w as 'dense_rank' from employee window w as (order by dno), d as (order by salary);",318328.0
92080,386796.0,"You can also change the data type to bigInt and it will solve your problem, it's not a good practice to keep integers as strings unless needed. :) ALTER TABLE T_PERSON MODIFY mobile_no BIGINT;",318017.0
92080,386854.0,For out of range value try changing datatype of the field. If it is integer make it bigint or string.,317689.0
92080,387061.0,change the data type of the column. just right click on the table name from left side bar and click on alter.. it will show you new wundow with allthe details.. and change it from there..,318802.0
92080,386901.0,"Yes Change the Column DATATYPE or extend the capacity of datatype to Float(20,6) to resolve this error.",317811.0
91541,383097.0,"The lesser the birth year, the elder the person will be . for eg, someone born in 1900 and someone born in 2000. the person born in 1900 would be elder than the one born in 2000. ie, min of (1900,2000) will give the elder person . similarly, for youngest we would require max(bdate) hope that helps.",317998.0
91541,383172.0,"Aah! this is little confusing but anyways, we've to find the maximum age which will be maximum when your bdate is minimum as compared to the date from which you want to find the age.. concentrate on the formula which gives you this result.. the bdate is second variable in below formula negative; age = currentdate - bdate so, age will be maximum when your bdate is as minimum as possible.. hope this helps..",316349.0
91541,383186.0,"Suppose we have two persons A and B ,A was born in the year 1995 and B in the Year 2005 if we want to find out who is the eldest -Obviously A (born in 1995) the Min from both the years (1995,2005) ,using Max would give us year 2005 which is youngest.",319056.0
91541,383238.0,"Eldest will be the one who born earlier. e.g. A born in year 2018 and birthyear=2018 B born in year 2017 and birthyear=2017 To find the eldest mean who born earlier, the year will be lesser If you find max(birthyear) it will give you 2018, but 2018 is not eldest. WE SHOULD USE MIN(birthyear), it will give you 2017 which is eldest.",317811.0
91541,383332.0,Max would give you the person born latest. max of 1965 and 1990 would be 1990. but you want the person who is oldest i.e min of both the years.,317689.0
91541,383368.0,"To put it in simple terms think of date values as any other number. For example you can try the following in Microsoft excel: Try inserting any 2 dates in 2 different cells respectively for example 29-01-2018 and 01-01-2018 (Now if you change the format of the cell from date to number or General, excel would show the following) When you use the MAX or MIN function the backend calculations are actually based on the numerical equivalent of the dates. Hence whenever you need to find the oldest date/earliest date/max age you'll have to use MIN which returns the min value (for the numerical equivalent ). The smilar logic applies to SQL. Hope that helps. P.S. - In excel the numerical equivalent of the date starts from 01-Jan-1900 which is 1",318021.0
91541,389515.0,"max(bdate) will provide you the most recent date, that will not solve your purpose. so use the min(bdate)",312357.0
89830,373409.0,Please go through the below link to understand logic used in the code mentioned ny you https://mysqlserverteam.com/mysql-8-0-2-introducing-window-functions/ I hope it will help.,317991.0
89830,380947.0,"Here, the use case could be to find the second highest salary is each department. Another usecase : in a students data base, we could want the first, second and third ranked students in each department.",300717.0
91567,383327.0,You can call the tables in the store procs. instead of passing them as input. Let me know your specific use case where you wish to pass a table as input rather than use it in query.,317689.0
91567,383326.0,We'll have to use Dynmaic SQL in this case. Following links might help : https://stackoverflow.com/questions/26231880/mysql-store-procedure-get-table-name-as-parameter https://stackoverflow.com/questions/6609778/mysql-stored-procedure-dont-take-table-name-as-parameter,311686.0
91567,383393.0,"There are two tables - table1 and table2. for each row in table 1, I have to compare data from two columns from table 1 and then extract a sub-set of the data and update it in table2. That is why I wanted to understand how to refer to the columns in table1 in the stored procedure. Any pointers on that will be helpful",318084.0
91567,383579.0,The tables within the same database schema are accessible inside UDF or stored procedure and don't need to be explicitely passed as input parameters. You only need to pass the parameters that you need inside the UDF or stored procedure.,300717.0
94059,396390.0,It means that the window starts from the 1st row till the current row.,312758.0
94059,396399.0,it means that the row mentioned will be considered like if u say 19 rows unbound preceding it will have 20 rows,318017.0
94059,396480.0,Unbounded preceeding means to consider all rows before the current row no matter how much they are.,317811.0
91693,384044.0,Dynamic frames are within windows which can be used for function calculation and there are some restrictions in using it.,310974.0
91693,389328.0,Thanks Ram,310009.0
92111,387228.0,The companydb is the same database which was used fr the prepratory session. Check below link for the script to create the companydb. One of the videos also explains the procedure to run this script. https://learn.upgrad.com/v/course/208/session/15786/segment/79808,313826.0
92111,387430.0,"Yes, The companydb databse is the same database used by Prof Prooja, and same can be found in Prepartory Course, Intro to SQL Module",317811.0
91246,381338.0,for me this works completely fine without wny errors.. as for as the error message is concerned it says error in cpde near '() as total salary etc..' that doesn't seem tp have any issues.. juat for in case if resolves the issue; try to switch off the safe mode amd then reopen or reconnect to see if this helps.. i faced the problems because of safe mode on.. my codes were correct still it was throwing errors solely because of safe mode.. try switching it off..,316349.0
91246,381372.0,"put a comma after (dept_salary,)",304692.0
91246,381384.0,"There is a comma missing. It should be sum(salary) over (partition by dno) as dep_salary,",317689.0
91246,381390.0,"Check your DB name, is it companydb or company_db",300717.0
91246,381780.0,"Hi All, This is not the code issue. MYSQL lower version doesn't support ""Windows function"". Here is the reference link for the same. Please use Latest version(mysql 8.0) if anyone facing the above issue. MySQL 8 supports window function. For reference: dev.mysql.com/doc/refman/8.0/en/window-functions.html – gvgvgvijayan",318846.0
91246,383241.0,"Hi Rajani You have written the wrong command in the second last line thats why u getting the error from company_db.employee. You have to write from employee which is table name not companydb which wrong. Try below code. Hope this will help you. select ssn, concat(fname,'',lname)as emp_name, dno, salary, sum(salary) over() as total_salary, sum(salary) over (partition by dno) as dep_salary from employee order by dno;",308639.0
91246,384495.0,"Not a code issue ,window function like rank() ,dense_rank() not work on mysql 6.0 etc., it will work on mysql 8.0 (latest) uninstall alll previous mysql from control panel ,remove all mysql directories from program files,program data,program files(x86) and then reinstall latest mysql server https://dev.mysql.com/downloads/windows/installer/8.0.html (download the second one from list)",318005.0
91246,384496.0,"note : window function will not work on phpmyadmin from xampp,lampp etc . It will work on sql developer .",318005.0
91879,388588.0,We can create a function exploting the primary key(roll number in this case) that will return the sum of the differences. Or we can use a nested query to accomplish the same task. I however am not too sure about a frame that can do this but i can neither confirm nor reject it as of now.,304281.0
92777,391998.0,"Hey, you need to install a MySQL version 8.0 or above. If you are unable to do it in ubuntu, try dual booting your system.",319721.0
91978,385921.0,"It is completely based on the nth value that you're passing.. In the video professor passed the nth value as 2.. and hence the ssn with second highest salary was copied and pasted to all the ssn in nth value (second_value) column.. now, here since the command that we wrote says rows unbounded precedding , the second_value will be applied only to the 2nd and later all the ssn's but not the 1st ssn.. This is the reason why you'll see the first ssn as NULL. I hope this helps..:)",316349.0
91978,385916.0,"For the nth value query, the value of n was 2, which means second value. In that case, the first value will be NULL. Whereas for the first_value query, the first value is populated, so no NULL",318084.0
91986,389427.0,Can you be little specfic. I am umable tounderstand what you want to ask. Try pasting th queries/Screenshot,301555.0
91248,381370.0,https://dev.mysql.com/downloads/mysql/ --&gt; download link https://www.youtube.com/watch?v=Ddx13KlW8yQ --&gt; video how to download,304692.0
91881,385251.0,i think you don't need @ while creating a variable using Declare. pls see this link : https://stackoverflow.com/questions/11754781/how-to-declare-a-variable-in-mysql hope my understanding is correct. requesting someone to comment or verify.,311686.0
91881,385267.0,"Ensure you have changed the default statement delimiter by including "" delimiter $$ "" at the top of the function/stored procedure failing which you will get syntax error.",313826.0
92340,389001.0,your syntax looks correct.. i guess your previous query/statement has colon ( ;) missing..,316349.0
92340,389196.0,You syntax is correct. This is seriously bud in MYSQL that it working very poorly and behaving like a unprofessional software in version8.,317811.0
92340,390112.0,Your query looks correct. Can you try again after restarting,301555.0
92340,390248.0,"Ok, Then it is because of Version. Use MysQL 8 and 64 bit. if you do not have then it is an issue.",301555.0
92067,386924.0,This version of MYSQL has lot of bugs and does not work fine even for correct queries sometime. Just try reopening the mysql and execute you command . It will work fine.,317811.0
92067,386767.0,"The code looks syntactically correct to me. Could you please close the mysql workbench, reopen it and try to execute it. It happened a few times for me. Reopening and re-executing it solved it for me.",318084.0
92067,386825.0,"There are some bugs in the verison of MYSQL workbench we use. Please try running the same command via MySQL shell or as Premnath said you can also close the SQL workbench, reopen and try it again. You're query did work for me no issues with your code.",306735.0
92594,390226.0,Windows functions are only introduced and supported in MYSQL 8.0,317811.0
92594,390056.0,why dont you install version 8.0.13 - I believe that is available for win32,300694.0
92594,390091.0,Window functions are supported for MySQL workbench 8 and above. Try installing that for 32 bit.,317689.0
92615,391307.0,"The SQL standard imposes a constraint on window functions that they cannot be used in ""UPDATE"" or ""DELETE"" statements to update rows. Using such functions in a subquery of these statements (to select rows) is permitted. MySQL does not support these window function features: 1) DISTINCT syntax for aggregate window functions. 2) Nested window functions. 3) Dynamic frame - Dynamic frames are within windows and those can be used for function calculation but there are some restrictions in using it.",302742.0
93286,392627.0,"She is talking about employee data which is there in employee table. Employee table is a part of companydb. You can download ""companydb"" from below location https://learn.upgrad.com/v/course/208/session/15786/segment/79810 At the bottom of page in above link under the heading "" Schema Creation File"". Hope this will help.",317991.0
93197,392282.0,"Hey, you need to have a version above 8.0. If not you cannot use window functions. and that;s why the error, I guess.",319721.0
93358,392882.0,Check your connection once,310974.0
93358,392910.0,Another queries are working,320687.0
93358,394270.0,"Please check your connection as the query is syntactically fine, though it makes little logical sense.",304281.0
93358,395431.0,"Are you able to run other queries? Please check. Also, check how many values does your employee table have.",319721.0
93358,395438.0,"SET global max_execution_time = 300000; perform this query, it sets you max execution time to 5 mins and execute the query.",319721.0
94708,399989.0,Thanks for sharing! It's really good!,319721.0
91690,383923.0,"it let's SQL interpreter know that the if statement block is finished each language has its own rules - so this is how SQL works; python differently, etc",300694.0
91690,384222.0,end if : It will terminate the if else statement. For more understanding refer below link: https://dev.mysql.com/doc/refman/8.0/en/if.html,310419.0
91690,385064.0,endif: it will teriminate the if else statement,306996.0
91690,385355.0,It is used to terminate the statement.,318802.0
91690,390265.0,"This tells to the interpreter the ending of IF CONDITIONS. Similar to BEGIN and END statments used in Funciton/Procedure, the set of related IF statements (more than one IF with ELSE or ELSEIF condition) ending is conveyed to interpretor.",318454.0
91899,385243.0,i think you don't need @ while creating a variable using Declare. pls see this link : https://stackoverflow.com/questions/11754781/how-to-declare-a-variable-in-mysql hope my understanding is correct. requesting someone to comment or verify.,311686.0
91899,385257.0,"Ensure you have changed the default statement delimiter by including "" delimiter $$ "" at the top of the function/stored procedure failing which you will get syntax error.",313826.0
91899,386313.0,set @var=1;,318005.0
90061,374975.0,http://www.mysqltutorial.org/mysql-stored-procedures-return-multiple-values/,317982.0
90061,374979.0,https://stackoverflow.com/questions/20039398/show-message-in-stored-procedure way to print statement..,317982.0
90061,375233.0,"Please find example below mysql&gt; CREATE PROCEDURE `getMonthlyTotalScore`(IN ninjaId int, IN month int, IN year int) -&gt; BEGIN -&gt; DECLARE startDate DATE; -&gt; DECLARE endDate DATE; -&gt; DECLARE maxDay INTEGER; -&gt; -&gt; SELECT CONCAT(year,'-',month,'-01'); # NOTE THIS -&gt; -&gt; -&gt; END; | Query OK, 0 rows affected (0.00 sec) mysql&gt; call getMonthlyTotalScore(1,5,2012); -&gt; | +------------------------------+ | CONCAT(year,'-',month,'-01') | +------------------------------+ | 2012-5-01 | +------------------------------+ 1 row in set (0.00 sec) Query OK, 0 rows affected (0.00 sec)",318368.0
90061,378328.0,"Simple! create hi_upgrad(in s char(10), out var_out char(20)) select concat(s,'there','!') into var_out; call hi_upgrad('Upgrad',@var_out); ---&gt;after execution out string will be stored in var_out. select @var_out; ---&gt; printing the output. Hope this helps!",304813.0
91428,382288.0,"I got the answer , It means that we are return output of function with char size 50 .",301108.0
91428,382340.0,And 'Deterministic' means that it always produces the same result for the same input parameters,310511.0
91428,382587.0,"When the value is too long for the data type being converted to. Because the uniqueidentifier type is limited to char(50) characters, the characters that exceed that length are truncated.",313200.0
91428,383350.0,Is there a possibility of different output with same input?,318436.0
91450,,nan,
91507,382903.0,"Hi, Try entering the space between delimiter and ';' like ""delimiter ;"" worked for me :) hope it helps..",305129.0
91507,382968.0,"There are a couple of thing sthat need to be changed: By the looks of it, the table employee does not have a column by name snn . My guess is that the name of the column is ssn . You need to check and make changes in the procedure'e where clause . You need to p ut a space between delimiter and ' ; '.",313826.0
91526,383245.0,"Use Select before Concat then only it will return the result. SELECT concat(""Hello"", s, '!');",317811.0
91526,382996.0,thanks Samyuktha,300687.0
91526,382993.0,"select concat(""hello"",s,""!"");",301115.0
93005,391718.0,"if a date is an INPUT and where clause is used then it should return single output, please check seperately the DML commands outside the function, it should zero it to the error point.",301115.0
93005,391717.0,Is it HOLD across all the dates?? can you please try passing the date which has BUY or SELL signal in babaj1? if it is still resulting in same signal then may be there is some issue in your UDF.. if you can paste your UDF code here that will help on knowing the issue further..,316349.0
93005,391863.0,DML standlone gives correct result. The isse is with UDf only I guess.,308673.0
91549,383128.0,"the returns statement specifies the datatype of the return value. the return statement specifies the actual value to be returned by the fucntion. hence there is only one return. the other one is returns , which specifies the datatype to be returned. further, the deterministic keyword specifies that for the same input the resulting output would be same. also, char (20) is the size of the input. char (50) denotes the size of the output which can be greater than the input since we are concatenating more strings to the input. so, the size of the output has to be more. hope that helps.",317998.0
91549,403586.0,Returns is a function.. it returns one value and char 20 or 50 is basically used to define inout and output.. data type needs to be mentioned,318791.0
91549,383144.0,"Hi Deva, returns char(50) deterministic - Means the datatype of the return value return concat('Hello, ', s, '!') - return value. This is actually what you are returning. Also, there is no difference between the input and output of the function here. Both are char. The difference is between the length/sixe of the input and output. Although the datatype can also be different. The output of a function is not dependent in any way on the input. Hope this helps.",310511.0
91565,383268.0,"Hi, its an Alias used to make our query simple and short, mostly used in case of Inner Join and Multi Join.",305129.0
91565,383250.0,It's just an Alias name like how you give for an column.,306735.0
91565,383328.0,this is an alias given to the query in the bracket. this is generally used while we are joining multiple tables.,317689.0
92105,387157.0,"If a function is created once, you dont have to create it another time. If you have made changes to the function, please delete the existing function and then re-create the function. It is advisable to use the below statement before creating the function. DROP FUNCTION if exists &lt;&lt;function name&gt;&gt;",318084.0
92105,387313.0,"Yo need to drop the already existing function using below command: Drop Function if exists &lt;existing function name&gt; After executing above command, you can recreate the function",319006.0
92105,387526.0,First exceute the command DROP Function hello3 then again crate your function. It will work fine. This is happening because you have alreadye xceuted the command successfully to create function.,317811.0
92105,388611.0,Change your code to CREATE OR REPLACE FUNCTION,301555.0
92105,389114.0,"WORKBENCH has some BUG and it wont refresh all time..and not show or auto refresh the MySQL Objects/Components post to their creation. So incase you have created the Objects then right click """"refresh all"" in schema / tables list and see whether the COMMIT auto trigger enabled. Add &amp; Trigger DROP stmt before recreating an Object is handy to perform a full run in script at anytime. For functions, Drop Function if exists &lt;function name&gt;;",318454.0
91667,383791.0,"@ is used to refer to variables. For more details on different kinds of variables, please refer to the below link https://stackoverflow.com/questions/11754781/how-to-declare-a-variable-in-mysql",318084.0
91667,383933.0,@ is used to define variables in SQL - here is the manual page from mysql: https://dev.mysql.com/doc/refman/8.0/en/user-variables.html,300694.0
91667,383831.0,"Hello Chetan, You can store a value in a user-defined variable in one statement and refer to it later in another statement. This enables you to pass values from one statement to another. User variables are written as @ var_name , where the variable name var_name consists of alphanumeric characters, . , _ , and $ . A user variable name can contain other characters if you quote it as a string or identifier (for example, @'my-var' , @""my-var"" , or @`my-var` ). One way to set a user-defined variable is by issuing a SET statement: SET @ var_name = expr [, @ var_name = expr ] ... How to use MySQL user-defined variables in SQL statements. http://www.mysqltutorial.org/mysql-variables/",320195.0
91667,387888.0,"hi Chetan though it is declared in procedure those varibles are likely declared local inside the procedure. while calling that procedure to get values outside procedure , we are again declaring variables using @infront. For simplicity, both call statement and procedure variables are declared in same name. Try some other variable name while calling procedure like below.. Hope this clarifies Eg. create procedure employee_salary ( in n char(9), out annual_salary float(10, 2), out all_project_pay float(8,2) . . end $$ delimiter ; set @abc:='123456789'; call employee_salary (@abc, @x, @y); select @x, @y;",318454.0
91666,384058.0,"It all depends on how you define the procedure. If you define an o/p variable, it gives o/p otherwise just runs it. Unlike UDFs, stored procedures are not mandated to return anything.",310974.0
91975,385922.0,"Deterministic Keyword basically means that for the same input, the value returned by the function will remain the same. Refer to the below link for more details: https://stackoverflow.com/questions/7946553/deterministic-function-in-mysql",318084.0
91975,385936.0,"It is used to "" produces the same result for the same input parameters "" Below a quick comparison between deterministic and nondeterministics in MySQL; DETERMINISTIC NONDETERMINISTIC Returns same result for same input. Returns different result for same input. deterministic giving significant time for execution if it is giving same result. Executing method definition again and again for same input. Take more execution time compare than determisitic When using nondeterministic for deterministic type of functions will take unwanted execution time. When using deterministic for nondeterministic methods might return wrong results. Because unwantly executing again and again for the same output. Because not executing for getting different outside at all time for the same input.",316349.0
91745,384069.0,"@ is used to define variables in SQL. For more details on different types of variables and how it should be defined, refer to the below link https://stackoverflow.com/questions/11754781/how-to-declare-a-variable-in-mysql",318084.0
91747,384173.0,"Yes, @Means declared variable i.e. either use declare keyword or prefix variable by @",317811.0
91747,384181.0,"DECLARE - statement declares local variables within stored programs. In Stored procedures, we will pass the arguments like sp(a, @b) . Here, value a is input and the output of the stored procedure is returned through the variable b. to define the return values in stored procedure, the notation @ is used. Source - https://dev.mysql.com/doc/refman/8.0/en/local-variable-scope.html http://www.mysqltutorial.org/stored-procedures-parameters.aspx Thanks.",311502.0
92226,388467.0,start with Delimiter $$,316349.0
92226,388512.0,The DELIMITER statement changes the standard delimiter which is semicolon ( ;) to another. we want to pass the stored procedure to the server as a whole rather than letting mysql tool to interpret each statement at a time. so we have to define delimiter $$ at the start of the procedure. Hope this helps you.,319006.0
92226,388509.0,"delimiter $$ create procedure hello (in s char(20)) begin select concat(""hello"",s,""!""); end $$ delimiter;",317811.0
92226,388696.0,start with the delimiter $$,318476.0
92226,389566.0,delimiter $$ is missing,317979.0
92226,389727.0,start with $$ its missing,312357.0
91317,381616.0,"Hi Girish, A function is considered “deterministic” if it always produces the same result for the same input parameters. Regards, Rajarshi.",310511.0
91317,381615.0,"it is explained in the mysql manual A routine is considered “deterministic” if it always produces the same result for the same input parameters, and “not deterministic” otherwise. If neither DETERMINISTIC nor NOT DETERMINISTIC is given in the routine definition, the default is NOT DETERMINISTIC . To declare that a function is deterministic, you must specify DETERMINISTIC explicitly. Assessment of the nature of a routine is based on the “honesty” of the creator: MySQL does not check that a routine declared DETERMINISTIC is free of statements that produce nondeterministic results. However, misdeclaring a routine might affect results or affect performance. Declaring a nondeterministic routine as DETERMINISTIC might lead to unexpected results by causing the optimizer to make incorrect execution plan choices. Declaring a deterministic routine as NONDETERMINISTIC might diminish performance by causing available optimizations not to be used. So basically you define a functions as deterministic if the output is always the same for the same input; mis-declaring it can cause the mysql query optmiser (it is sort of an engine that decides what is the best way to retrieve data from the table and join it together to present the final output of rows and columns - it makes more sense when you have a lot of data and tables and a complex query.",300694.0
91317,381636.0,"Hi, Functions can be either deterministic or nondeterministic. A deterministic function always returns the same results if given the same input values. A nondeterministic function may return different results every time it is called, even when the same input values are provided. Detailed more about procedure and deterministic and nondeterministic reference.- https://dev.mysql.com/doc/refman/8.0/en/create-procedure.html",305129.0
91317,382211.0,Determinstic results same output for same input. Deterministic giving significant time for execution if it is giving same result.,314183.0
91317,382580.0,"When you create a function, by default it is NOT DETERMINISTIC, which means that for each execution the code will be executed again. In the other hand, DETERMINISTIC will check if the parameters are the same and if is that condition comes true, will return the same result as before without executing again the code. The DETERMINISTIC property was added two things: 1.Protect consistency of called stored procedures that are stored in binary logs 2.Save developers the headache of going back and adding the property",313200.0
91822,384711.0,"UDF, StoredProcedures, columns, tables, triggers are all persistently stored in the database and are available until they are dropped. A server restart will not have any impact",300694.0
91822,384622.0,Yes if you created it than it would be stored in the database just like the tables and store procs.,317689.0
91822,384650.0,"UDF is an stored object, can be retrieved any number of times From server once it's stored in server successfully",301115.0
91822,389715.0,yes you can The main benefit of UDF is that we are not just limited to SQL provided functions. We can write our own functions to meet our specific needs or to simplify complex SQL codes,312357.0
92065,386784.0,"Char(20) - 20 refers to the length of the input variable. char(50) - 50 refers to the length of the output variable. char and varchar datatypes are similar in terms of usage. They differ in the way it is stored in the memory. For more details, refer to the below link https://dev.mysql.com/doc/refman/5.7/en/char.html",318084.0
92065,386931.0,They generally mentioned char(20) and char(50). You can see what is the max length of value that is in the column and can choose as per that.,317811.0
92065,389713.0,"CHAR is fixed length and VARCHAR is variable length. ... VARCHAR stores variable-length character strings and is the most common string data type. It can require less storage space than fixed-length types, because it uses only as much space as it needs",312357.0
93103,393566.0,"Into helps you to define the columns you want to modify. If you want to display only one of these modified columns and perform few operations only on few of the columns, to view the data in a particular way we use select/",319721.0
92609,390194.0,significance of mentioning deterministic is for same input values the results will be same.. the statement defines the output to he returned as character limit of 50 ..,316349.0
92609,390210.0,"Deterministic says that for a particular input, the output returned by function will always be same.",317811.0
92609,390327.0,"I think the below article would clear your doubts a little just like it did for me. The crux of the matter is that there may be some functions like rand() and other date functions which can provide different output everytime we run them, these would be your Non-Deterministic functions. The others, which would always give you same values, like arithematic functions, these are Deterministic functions. https://docs.microsoft.com/en-us/sql/relational-databases/user-defined-functions/deterministic-and-nondeterministic-functions?view=sql-server-2017",318397.0
92609,391024.0,"Functions can be either deterministic or nondeterministic. A deterministic function always returns the same results if given the same input values. A nondeterministic function may return different results every time it is called, even when the same input values are provided. I refered the below links : http://www.mysqltutorial.org/mysql-stored-function/ http://www.knowledgewalls.com/johnpeter/books/mysql/deterministic-vs-nondeterministic",304021.0
92570,389941.0,Restart the workbench..,305129.0
92570,389958.0,that works but there should be some other way which doesn't need workbench restart.,316323.0
92570,390022.0,Apart from restarting workbench I found an alternate way mentioned on stackoverflow. Please go through it. Below is the link: https://stackoverflow.com/questions/28038580/how-to-hide-view-result-grid-in-mysql-workbench Hope this will help.,317991.0
92570,390063.0,as mentioned in a previous answer by me - I just run the 'execution path' query and the result grid ALWAYS reveals itself - no need to restart,300694.0
92570,391276.0,"You can even click on the button in the figure,",319721.0
92570,391554.0,click on query&gt;explain current statement to get the output grid,321850.0
92780,390741.0,I guess for aggregate functions you need database column and table reference.. I cannot recollect how to add two vaues without a column/table reference in mysql workbench.. Let's hear from others if it is possible..,316349.0
92780,390785.0,you can directly write select 7 + 8. Generally sum function takes a column name which we need to aggregate over some group.,317689.0
92780,390791.0,also be aware that it might depend on which database you are working on so in MySQL dbs you can write select without 'from' statement but in Oracle you need to have from statement so select 7+8 will not work in Oracle dbs it will have to be 'select 7+8 from dual',300694.0
92975,391568.0,"You can go through below link to get more insight about 'into' https://dev.mysql.com/doc/refman/8.0/en/select-into.html http://www.mysqltutorial.org/mysql-select-into-variable/ 'As' is used just to rename (alias) table, column or other query result. Below link has more insight. http://www.mysqltutorial.org/mysql-alias/ Hope this will help.",317991.0
92975,391565.0,AS is used to denote alias and INTO is used to insert values into that variable/table.,318084.0
92975,391657.0,The SQL SELECT INTO statement is used to select data from a SQL database table and to insert it to a different table at the same time. SELECT * INTO Customers_copy FROM Customers You can read more in detail here http://www.sql-tutorial.net/SQL-SELECT-INTO.asp,317845.0
91648,383926.0,"you might need to use substitution? e.g. @dynamic_SQL= concat('select ',var1,'from ....'); prepare statement1 from @dynamic_SQL execute statement1",300694.0
91648,383949.0,The prepare and execute method works with stored procedure but not with functions.,313826.0
91648,385368.0,Dynamic queries in MYSQL works in Stored Procedure and not with function,301555.0
93338,392795.0,"You have already created bajaj2 table with signal's, just use it in UDF to get the signal value by giving input as date.",306735.0
93338,392766.0,Are you trying to solve the last task? You need to use only Bajaj2 table for it. Your function should take date as input and return the signal of that date from Bajaj2 table.,311686.0
93338,392771.0,"Hey, you aren't supposed to display your code.",319721.0
93340,392767.0,"Hey, your snapshot isn't uploaded. Please upload it again.",319721.0
93340,392770.0,Please share the error.,306735.0
93340,392773.0,There is a default thousand-row limit in MySQL-Workbench. The SELECT query will return results but UPDATE will fail if the number of records to be updated exceeds one thousand. https://dev.mysql.com/doc/workbench/en/wb-preferences-sql-editor.html Refer to the link above.,319721.0
93340,392783.0,Take a snap of your error save it as image file and using the below share it.,306735.0
93407,393431.0,The reason for null value would be the date format in table and in the udf is not matching.,310629.0
93407,393130.0,"Check your logic, please. You can discuss what is the logic you applied.",319721.0
93407,393131.0,is it returning null for all the inputs that you are giving? one issue might be the difference in format of date that you are passing as input and format of date column in the table.,311686.0
93407,393212.0,The signal value is assigned to a declared varchar variable and date is also data type varchar to avoid. The return value is being selected from bajaj2 table. Function call query is select function_name(date).,301644.0
93354,392884.0,It's easy. Try to do it. I'm sure you will be able to crack it. Use the example in the course.,310974.0
91974,386823.0,https://stackoverflow.com/questions/1009954/mysql-variable-vs-variable-whats-the-difference userdefined variables are something defined by User and contains @. Local variables are normal variables without @ Go through above link for First Reply.,301555.0
91974,390256.0,"Hi userdefined variables are contains @ infront. Local variables are without @. Like Procedure or Function creation if you declare a variable in CREATE PROCEDURE or CREATE FUNCTION statement., they can be accessed without @ if you declare any user defined variable inside PROC / FUNC using SET stamt, then you can still access them outside the PROC/FUNC also. They are available till the session. Using @@ you can access the Server variables like autocommit, back_log etc. select @ABC; -- User defined select @@autocommit; -- Predefined server variables",318454.0
93512,393704.0,delimiter $$ CREATE FUNCTION Functionname (inputvariableName DataType) returns datatype deterministic BEGIN return(statement); END $$ delimiter ; at end delimiter and ; has a space in between,317811.0
93512,393911.0,You need to put it in the beginning as well in the end inorder for mysql to understand this.,317689.0
93512,393706.0,"Hello Sharad, It should be like below: DELIMITER $$ create function project_pay_calc( pno int, num_of_hours float(4,2)) returns float(8,2) deterministic begin declare project_pay_per_hour float(8,2); if (pno &gt; 0 and pno &lt;=5 ) then set project_pay_per_hour = 1000; elseif (pno &gt; 5 and pno &lt;= 10) then set project_pay_per_hour = 2000; else set project_pay_per_hour = 3000; end if; return (project_pay_per_hour * num_of_hours); end $$ DELIMITER ;",320195.0
92329,388803.0,The companydb is the same database which was used for the prepratory session. Check below link for the script to create the companydb. One of the videos also explains the procedure to run this script. https://learn.upgrad.com/v/course/208/session/15786/segment/79808,313826.0
92329,388810.0,Please download company db provided at bottom of page link given below: https://learn.upgrad.com/v/course/208/session/15786/segment/79808,317811.0
92329,388858.0,You can download the company DB schema from below link https://cdn.upgrad.com/UpGrad/temp/3e2fab1d-df58-41a9-b42f-88d5218b0414/company_handout.pdf,317845.0
92329,388911.0,Download from https://learn.upgrad.com/v/course/208/session/19878/segment/101121,317689.0
92329,389118.0,"you can use Companydb instead of Companydb_pp; the Script file given in the session training material uses wrong Schema COmpanydb_pp. Also there is only one table which is not present in Companydb but that is used in the script and session where COmpanydb_pp is used. And, no questions refer/uses that particular table from companydb_pp so you can use companydb instead safely",318454.0
92329,389154.0,"choose relevant name.create schema ,,,by create database dbname;",318005.0
92329,393046.0,"Depends on what name you give while creating the database in your local instance, you need to change in the command provided.",313228.0
90766,378213.0,"First you would open the MySQL script file and once done, you can either run all the statements at once or run one by one.",318329.0
90766,381218.0,"Hi Subhashis, When you double click on the MySQL script file it should open up the file in MySQL workbench. In case you dont have MySQL workbench you may need to go through the SQL course in pre-Launch for instaructions to install the MySQL workbench. File should open in an unconnected tab by default. Select the reconnect icon to connect to the database: You should now be connected to the database and execute your SQL queries eaither using the execute icon as used in the videos or using Ctrl+Enter. Hope this helps.",306725.0
91086,380455.0,"I was checking in Google and found that, For MySql the query parser internally converts it to Left Join. So basically if we write directly left join one step less parsing will be equired by the MySql engine. This is true for MySQL at least. https://dev.mysql.com/doc/refman/8.0/en/outer-join-simplification.html",318554.0
91086,380585.0,This is not just for MySQL but for other RDBMS as well like Oracle.,310974.0
91086,381223.0,"Hi Parul, Lets say you have two tables Table1 and Table2. When you write a right join as below Table1 RIGHT JOIN Table2 on (...) database would internally convert the above statement into below Table2 LEFT JOIN Table1 on (...) so when we user right join, database has to perform an addition step internally, this can be avoided if we use LEFT JOIN only. Hope this helps .",306725.0
94000,396099.0,yes both the codes will return the sane results.. but the second Query is optimized for good performance..,316349.0
94000,396135.0,"You can go through below link to get more insight about "" Subquery vs Joins"" https://stackoverflow.com/questions/2577174/join-vs-sub-query https://www.quora.com/Which-is-faster-joins-or-subqueries Hope this will help.",317991.0
94000,396105.0,"Hi Premanth , Yes , second query is optimized one and recommended structure as well. First query unnecessary using alias to select from another select statment. We can use 'as' alias if we want to output the column-names differently. The below structure is only useful when you want to filter the rows based on computations on same table (like filtering rows with row_number() ) select &lt;columns&gt; from (select &lt;columns&gt; from table) Other than the standard strucure and optimized code , no difference between the above queries. Thanks.",305652.0
90767,388914.0,Suggest to create table name without space. should not do any harm. In this case name table as bajajauto when you import csv.,302740.0
90767,378297.0,"In the MySQL Workbench Navigator, in the SCHEMAS section, see if you are able to view the schema that you have created. If it is found in the list, right click on the schema name and select ""Set as Default schema"". You should be able to query your table then. Alternatively, you can execute statement ""USE &lt;your schemaname&gt;"" in new SQL tab window.",302740.0
90767,379088.0,showing below error.,300690.0
90767,386136.0,"Try using '' for table name. Since table name is space delimited , the engine may not be able to understand the table say: select * from 'table name'",316147.0
91892,385145.0,It means to group by the first column. You can do the same with ORDER BY. In above query group by 1 refers to the first column in select statement which is ssn,307495.0
91892,385264.0,"The columns of the result set generated by a query can also be identified by a numeric value representing their position in the result set. For ex. if a result set has the columns ""Name, Id, SSN, BDATE"", then columns are numerically represent ed as: Name --&gt; 1 , Id --&gt;2 and so on. In the given query, group by 1 means group by the first column in the result set i.e., ssn .",313826.0
91892,385303.0,when using values like these it means the column name used in select statements,318017.0
91892,392129.0,It means grouping by the 1t column,318082.0
92554,389902.0,The catch here is the rows get filtered while evaluaitng the join condition before even they are fetched from disk. The select clause is the last one to be executed.,310974.0
92554,390852.0,"In SQL execution order first the base table is formed ,this includes 'from' and 'joins' then comes where clause followed by group by, having, select. (In short 'where' is applied after 'from' and 'join') In the example shown in the video, in the unoptimized query first employee and department tables are joined and then the where filters are applied, but in the optimized query : joining and filtering are happening simultaneously, this saves a lot of time.",316255.0
92554,390859.0,"After giving a second thought on the example, the inner queries are executed first! :) (In the example given in the video the join is happening on the pre filtered tables) The better example here would be : select e.fname,d.relationship from employee e inner join dependent d on e.ssn=d.essn and e.dno=4 and relationship='spouse'",316255.0
92554,391273.0,"Here it's not just about returning a certain number of rows, it's also about verifying conditions on these rows. If each table has 10 rows, and if where clauses are used inside the inner join of the two tables, then checks for the number of rows in both the tables are done, that is 20. But if where is used outside, the check is done only 10 times.",319721.0
92620,390499.0,On is used to specify the column on which you are joining two datasets. Where as where clause is used to filter data. Where can be used when there is only 1 table only. Whereas for joining you would need 2 tables and a common column on which you can join.,317689.0
92620,390345.0,"When you are making joins and If you dont put filter condition on ""ON"" and use where your join will not work correctly.",317811.0
92620,390239.0,where is a filter clause which shows records pertaining to your where clause condition.. whereas 'on' is a condition on which you're joining two tables.. 'on' checks for the matching records in the two tables that you're joining. hope this helps in understanding the difference,316349.0
92620,390867.0,"You can use where condition after on condition but the where condition is executed after the join condition.But if you add the filter along with the on condition the operations are done simultaneously, which would save some time!",316255.0
92620,391286.0,https://learn.upgrad.com/v/course/208/question/92554 you can refer to the answers to this question as well.,319721.0
92620,391512.0,You can also check the excution time in mysql workbench for both the queries. I am sure second query where clause on filtering after join would execute in less time when compared to previous query.,301118.0
92620,391695.0,"There will be a difference in the way it functions. When you use and: then the tables are joined based on all the three given conditions. When you use where: the tables are first joined. Then the where conditions are verified on the joined table with only one condition. Here, you get 7 rows when you apply the inner join on the first condition. Then the two conditions of relationship and dno are verified on all the 7 rows.",319721.0
92620,391531.0,"Hi Guys, Just to clarify. The question is not about use case of on and where. It is about using them in case of filtering while joining. While joining we are joining on three parameters in the case of example provided above. What if I joined on a single parameter i.e e.ssn = d.essn and use where condition on the rest of the two. Is there any advantage of using on in place of where. I could have used on e.ssn = d.essn where e.dno=4 and d.relationship= 'Spouse'; in place of on e.ssn = d.essn and e.dno=4 and d.relationship= 'Spouse'; What is the difference in working of on and where clause here?",318576.0
92435,389391.0,Yes it is fine to edit it. You can change it to the schema name which you have already created.,317689.0
91418,382279.0,kindly select the proper command you might be running query which is running on some previous command,318017.0
91418,382302.0,"Are you trying to add a column after dno column, then can you please try the below query ? ALTER TABLE `DS_SEP_18`.`employee` ADD COLUMN `des` VARCHAR(45) DEFAULT NULL AFTER `dno`; Thanks",311502.0
92355,,nan,
92358,388923.0,Check this Link https://www.w3schools.com/sql/sql_foreignkey.asp,304814.0
92358,389120.0,SQL query to add foreign key person table PersonID LastName FirstName Age 1 Hansen Ola 30 2 Svendson Tove 23 3 Pettersen Kari 20 Orders table OrderID OrderNumber PersonID 1 77895 3 2 44678 3 3 22456 2 4 24562 1 to add FK key ALTER TABLE Orders ADD CONSTRAINT FK_PersonOrder FOREIGN KEY (PersonID) REFERENCES Persons(PersonID);,317845.0
92358,389520.0,A FOREIGN KEY is a key used to link two tables together. A FOREIGN KEY is a field (or collection of fields) in one table that refers to the PRIMARY KEY in another table,312357.0
92358,389662.0,The foreign key constraint specified at the table level must have the same number of reference columns as that in the constraint column list. The data type of each reference column must also be the same in the column list. Make sure to Grant permission and then add the constraints and references.,318789.0
91025,379931.0,"it is a 'self join' so you are saying the super_ssn column is a foreign key column and it references back to the 'employee' table with column 'ssn'; so values in employee.super_ssn_column ""must"" match values in the employee.ssn column",300694.0
91025,379928.0,super_ssn is the column which we are going to use as a foreign key on the employee table ssn it means that the value of ssn in employee will be stored in super_ssn column of this table,317982.0
91025,380072.0,This is referencial integrity constraint : super_ssn is the suprvisor's ssn and every supervisor is an employee as well. So there cannot be a super_ssn that is not in employee ssn.,300717.0
91025,380133.0,"add constraint fk_super_ssn foreign key (super_ssn) references employee(ssn); add constraint - command for adding foreign key fk_super_ssn - name of the constraint added foreign key - type of constarint added super_ssn - foreign key is added on which column references employee(ssn); foreign key added, points to which column of which table. Here the foreign key on employee.super_ssn points to ssn column of the sam table employee. Thus it is a self join.",310511.0
91025,380592.0,It is self join case. Super_ssn to ssn. The join should work.,301555.0
91025,383023.0,"if ""add constraint"" is used for adding foreign keys, then what's the need to have special command called ""foreign key"" to identify the contraint type? does it mean that ""add constraint"" can be used for adding other types of constraints as well and not just limited to foreign key?",310509.0
91576,383736.0,Seems that you have not selected any database. First run the command use databasename; and Then use below command alter table employee add des varchar(50) default 'manager';,317811.0
91576,383323.0,try the below statement: alter table employee add column des varchar(50) default 'manager';,318084.0
91576,383405.0,"Hi, its working on my system, can you please specify what exact error you are facing?",305129.0
91576,385392.0,"In alter table statement, use companydb.employee for the tablename and check. Somehow the error is on the database, and may be even if you have done that, the sql is not considering it.",301555.0
92075,388609.0,You can use INSTR function too. That will give you the position.,301555.0
92075,386975.0,"SET @values = 'cat#dog#horse#parrot#gecko'; SELECT SUBSTRING_INDEX(SUBSTRING_INDEX(@values,'#', 4 ),'#',-1); which ever element you want, replace 4 by n( its # index)",317811.0
92075,386813.0,"SUBSTRING_INDEX will not give you second ast word alone. You would need to one more function called INSTR() along with it. Keep exploring ! INSTR() givees you back the position, based on the position value use the substring_index() to get what you want. https://www.w3schools.com/sql/func_mysql_instr.asp Hope it helps !",306735.0
91495,385437.0,"Yes as in the drop, it needs to explicitly tell whether you want to drop column, you want to drop constraint on column. Therefore, in the drop command column keyword is required",317811.0
91495,382803.0,"The syntax for adding a column is as follows: ALTER TABLE table ADD [COLUMN] column_name column_definition [FIRST|AFTER existing_column]; You can mention [COLUMN] keyword however its is optional. In case of drop column, you have to mention the [COLUMN] keyword as the ALTER TABLE command needs to know whether you are dropping a column or a constraint. However in case off add column, ALTER TABLE command assumes by default you are adding a column unless you mention ADD CONSTRAINT.",318085.0
91314,381623.0,"You are right, you cannot drop the primary keys without dropping the foreign keys. Can you please share the options available or link to the question. I will duble check and then if required we can report it so, that it gets modified/fixed.",301555.0
91314,381629.0,Thanks Anshul for the clarification. Please find attached the screenshot of the question with the various options. Link of the topic: https://learn.upgrad.com/v/course/208/session/19878/segment/101124,313826.0
91314,381627.0,Thas correct. You will only be allowed to drop a primary after dropping any foreign keys that reference the primary key.,310511.0
91525,383246.0,"Yes, you need to drop the foregin key constraint from other table. And then rename the columnname and again create a foregin key constraint refrencing using the new column name.",317811.0
91525,382983.0,"Droping the constraint, then rename the column ,add the constraint back is the way.",301115.0
91525,382986.0,"I dont think that would be an issue, as long as the data in both the columns is consistent and you change the name of the foreign key with the name of the new column.",317998.0
91525,383014.0,"Typically you can't do that when you rename because you are altering the strucure of tht database design. All databases(mysql,oracle) will throw an error if you have created an relationship b/n two tables. You will need to drop the constraint rename the column and re-create the constraint. Pls have a look at this post which says for MySQL. https://stackoverflow.com/questions/2014498/renaming-foreign-key-columns-in-mysql",306735.0
91525,384032.0,"For any constraint created between the tables, you need to first drop the constraint before you do any Definition change. Remember to see the constraint if you are using Alter command on any field.",315028.0
91536,383132.0,Abhishek it works for me.,306735.0
91536,383056.0,Use STR_TO_DATE function for mysql,306735.0
91536,383051.0,"Same question...the below is not clear to me - ""To take advantage of the date functionality, the date field needs to be formatted appropriately. This often involves some text manipulation, followed by a CAST""",310509.0
91536,383064.0,it did work for me.,306735.0
91536,383053.0,"STR_TO_DATE should solve the porpuse, provided you give the second parameter correctly. You can refer to this for the correct syntax of the 2nd paramter for the str_to_date function. https://dev.mysql.com/doc/refman/5.5/en/date-and-time-functions.html#function_date-format hope that helps.",317998.0
91536,383121.0,"Hello, Not working for me :( Also can we apply it to whole column intead of writing date manually?",305129.0
91536,383125.0,Tried but still not getting it...,305129.0
91536,383322.0,"there is a difference in using %m and %M in str_to_date function. if you use %M it takes into full month value i.e. January, March etc instead of Mar, Jan. For more details please refer to this function's documentation.",317689.0
91536,383242.0,"SELECT str_to_date('2018-DECEMBER-05', '%Y-%M-%d') Use this.",317811.0
91536,384037.0,use STR_TO_DATE funcion of mysql to convert the date in required format,301108.0
91536,384375.0,"SELECT str_to_date('2018-OCTOBER-18', '%Y-%M-%d') from dual. This should work",318080.0
93016,391764.0,"Yes, it can be done using alter table command with change as follows: For ex: If you have table department and it has mgr_ssn as column name and char(9) as data-type. Now if you want to change mgr_ssn to mssn and datatype char(9) to int(10) in a single command you can follows below syntax: alter table department change mgr_ssn mssn int(10); Hope this will help.",317991.0
93016,391814.0,"To change the data type of a column in a table, use the following syntax: ALTER TABLE table_name CHANGE COLUMN column_name datatype ;",319056.0
91535,383244.0,"No, You need a alter table command as you are modifyings schema. It is a DDL COMMAND.",317811.0
91535,383061.0,"You cannot explicity set up foreign key when you do a join but still you can do join b/n two tables eventough you dont have a common identifer using SET operators. UNION, UNION ALL",306735.0
91575,383733.0,First drop foregin key constraint on the table and then drop primary key constraint from main table,317811.0
91575,383336.0,"it appears that your foreign key wasnt set properly and hence it is giving an error while dropping it. so, try and set the foreign key properly then try to drop it and see if it works.",317998.0
91575,383416.0,"Hello Ankur, Please go through below link for better understanding. https://learn.upgrad.com/v/course/208/question/91314",320195.0
91575,383524.0,"You can not drop ""primary key"" without dropping foreign key used in the table. This is not possible even you disable the foreign key. So, please try to drop foreign key first. For more details please refer below link: https://stackoverflow.com/questions/31423145/cannot-change-primary-key-because-of-incorrectly-formed-foreign-key-constraint",318328.0
92858,391031.0,What error you are getting?,305129.0
92858,391034.0,Showing this error:,317558.0
92858,391039.0,"DROP TABLE IF EXISTS emp; As Abhishek has pointed out that emp table already exists, run the above command as well before running your emp table creation command.",301652.0
92858,391047.0,This is because the table must have already been created. U must have executed this statement more than once. In order to view the table columns you can run the following query: desc emp; Hope this helps.,310511.0
92858,391055.0,"create table emp select fname, minit, lname, ssn, bdate from employee; Remove as",317811.0
92858,391109.0,You have to use create table command but remove as from there,314183.0
92858,391141.0,"drop table emp; create table if not exists emp as select fname, minit, lname, ssn, bdate from employee; select * from emp;",304813.0
92858,391189.0,Table emp already exists.. Either drop table emp or create table with new name,303673.0
92858,391672.0,drop the emp table and then create it,300735.0
92860,391049.0,"Mysql doesbt generally allow column aliases to have spaces in their names. Nor does it allow you to name objects (tables,columns,etc) after keywords. To avoid this, you can put the name within tildas. Eg create table `bajaj auto`( ... With this being said, your query will create a table with data from name, bdate and ssn columns of e2. Hope this explains.",310511.0
92860,391054.0,Dont use as CREATE TABLE EMPLOYEE1 SELECT * FROM EMPLOYEE; ABOVE IS SINGLE QUERY,317811.0
92860,391052.0,I think you want to create a table e3 which has same structure as e2. If that is the case then you can use below queries /* This will create a copy without data create table e3 like e2 /* This will create a copy with data create table e3 select * from e2,301643.0
92860,391037.0,"As you can see Full, Date , Select are keywords in SQL (shown in green font in the pic), i.e. they have special meaning. So you cannot use them as column names, it's forbidden. But if you still want to, you can. Just enclose the forbidden column name in backtick (left of 1 key on keyboard). Spaces as well, aren't allowed in Column names. But you can still get away by using back tick. e.g. CREATE TABLE `Mercedes Motors`( ` Date ` date, `Opening Price` float(15,4), `No. of Trades` int(15), `Total Turnover (Rs.)` float(20,4), ` % Deli. Qty to Traded Qty` varchar(20), `Spread High-Low` float(10,2), );",301652.0
92401,389145.0,super_ssn - foreign key of the table being altered ssn - primary key of the table being referred The above example is an interesting case as the primary and foreign key both exist in the same table. i.e. super_ssn refers ssn column of the same table.,318085.0
92401,389170.0,it need not be bold or italics. your MySQL will work as expected with normal text as well. it is used to emphasize the columns being used as keys. it increases the readability of the code and make it easier for others to understand your code. they can get the important information easily if such practices are followed.,311686.0
92401,389189.0,super_ssn - name of the column marked as foreign key in the table being altered ssn - name of the column which is primary key in main table table being referred,317811.0
92401,389249.0,"These columns - super_ssn, ssn are from the same table - employee . ssn is the primary key in this table and super_ssn is the foreign key which refers the field ssn . Foreign keys can only be created by referencing the primary key or unique key fields. In this scenarios the self join can be used to find out the details of the super_ssn record for the given ssn record. You can visualize ssn record as your own record and super_ssn as your supervisor's record",319006.0
92401,389517.0,super_ssn - is foreign key of the table being altered ssn - primary key of the table is referred,312357.0
92401,389665.0,"Foreign key is used for referntial integrity and it needs to refer the primary key (in another table). A foreign key can be used as a primary key if the table is used connected by a one-one relationship. In this case, both foreign key and primary key are present. While referring to the primay key (ssn), super_ssn is the foreign key that is being altered.",318789.0
91697,384043.0,Never tried but that would be a lot of maintenance. Why would you want to do that?,310974.0
91697,384166.0,No,301555.0
91956,385653.0,"Yes for next statement to be considered separate from first, you need to mention ; at end of query. Let say you are giving this sql file to be run in one go, then you need too end each query by ;",317811.0
91956,385746.0,"you will get error when you try to run above queries as script i.e if you want to run above two queries at one go, you will get an error and only on of the query will be executed. But you can run above two queries ,one by one at a time, i.e sql engine will not throw any error when it runs query one by one. In Short - "";"" (semicolon) is NOT MANDATORY to run any query. But to run two queries at one go, Statement must be separated by semicolon.",312746.0
91956,385672.0,End your select * from employee using semicolon ; or when you are running your alter command select only the alter statement and run.,306735.0
91335,381736.0,The companydb.sql has all the create/insert table commands. You just need to open in the workbench and run it have the tables created.,310974.0
91335,381752.0,Open your script files as Sql script file from the Files menu and that's how you'll see all the tables and everything.. now make sure you're making it active by double clicking on the company_db schema and then write your codes and run it..,316349.0
91335,381755.0,The instructions on importing the companydb have been explained in the prepratory course on sql. Checkout the link below which has videos explaining the same https://learn.upgrad.com/v/course/208/session/15786/segment/79810,313826.0
91335,382252.0,Please refer to the SQL course in the prep section (course1) . there is instruction to download companyDB file. it is a series of sql statments which you have to run in your my sql instance to create companydb and its data. --Rajesh,300708.0
91335,382292.0,"nice and clear instructions have been given in the document just follow that, you will be through.",311219.0
91335,381777.0,Create a New Schema( Company_DB ) then select that schema from left side panel. Select Company_DB from left hand side panel Select File and Run Script---- select the company_db from your saved location Run the script but make sure to select Default Schema Name as company_db in the run script box Click on run After that if it don't appear(the tables) in the Schema panel in left side of Workbench then restart your Workbench it the table should appear over there in left hand side Schema section,306010.0
91335,382505.0,"Thanks everyone for responding, issue is resolved after watching the video again.",306011.0
91335,382583.0,clear instructions are given in the documents just follow,306996.0
92123,387408.0,"Yes, First In the table where it is referred as a foreign key constraint. drop the foregin key constraint for this column from table. Then Alter the main table change the column name and again create foreign key constraint for the column with the new name in another table.",317811.0
92123,387385.0,"Hi Venkakta, The foreign key constraing imposes referencial integrity. Having said that the above operation will not be allowed unless and until the foreign key is dropped first",319006.0
92123,389522.0,yes t can be but for that you have to drop the foreign key then only you can go ahead use alter to main data set,312357.0
92131,387491.0,"Hi, in order to import the csv files as tables, you have to first create a db with any name for eg Assignment, after that on tables do a right click and click on table data import wizard and browse the path for that.. after that dont do anything just click on next and continue it till the data is imported successfully. Let me know if it helps..",305129.0
92131,387503.0,"Hi, dont try to change the date datatype format while importing the csv. First import the file and later on you can use. Str_to_date(date,'%d-%M-%Y') function to change the date column from str to date format... Hope it helps..",305129.0
92131,387458.0,"yes Prateek, you need to iMport the file as text datatype and then you can update the same through; Str_to_date() function..",316349.0
92131,387510.0,"Instead of wizard, refer below code taken from https://stackoverflow.com/questions/8163079/importing-a-csv-to-mysql-with-different-date-format LOAD DATA INFILE 'file_name.csv' INTO TABLE table_name FIELDS TERMINATED BY ';' LINES TERMINATED BY '\n' ( id , column2 , column3 , @ date_time_variable ) -- read one of the field to variable SET date_time_column = STR_TO_DATE (@ date_time_variable , '%d-%b-%Y' ); -",317811.0
92131,388627.0,Either of the above method should work,301555.0
92131,389409.0,All this issue is resolved now. i imported the Date as text and later converted to Date using Str_to_date function as suggested by other colleagues.,316036.0
92131,391702.0,"Log in to your database using SQL Server Management Studio. Right click the database and select Tasks -&gt; Import Data... Click the Next &gt; button. For Data Source, select Flat File Source . Then use the Browse button to select the CSV file. Spend some time configuring the data import before clicking the Next &gt; button. For Destination, select the correct database provider (e.g. for SQL Server 2012, you can use SQL Server Native Client 11.0). Enter the Server name ; check Use SQL Server Authentication , enter the User name , Password , and Database before clicking the Next &gt; button. In the Select Source Tables and Views window, you can Edit Mappings before clicking the Next &gt; button. Check Run immediately and click the Next &gt; button. Click the Finish button to run the package.",312357.0
91786,384396.0,Are you using the data type for lname as varch ? It should be varchar(length of value) instead of varch.,311502.0
91786,384451.0,Can you post your create table script. Check if you are using correct data type names in the script. It should be varchar.,310511.0
91786,384918.0,I am using below statement of alter it is giving error in syntax in MYSQL work bench. alter table actor add column desc varchar(20);,301108.0
91786,384454.0,Instead of varch you have to use varchar() and in bracket you have to write length of value. Like lname varchar (50),310419.0
91240,381430.0,"it would help if you gave more info, screenshots, etc - most of us might not remember the question, options, etc",300694.0
91240,381583.0,The below code is used to alter the structure of table. Here you are adding 'col' column into employee table with default value as ' upgrad ' i.e all the value of 'col' coulmn will have value='upgrad' Alter table employee Add col varchar(15) Default ‘upgrad’ ; Below code is used to select rows from a table. Select * from employee Where col = ‘upgrad’ ; Here you are select all the records where value in 'col' column is 'upgrad',317991.0
91123,381604.0,The Youngest one will be first i.e. the year 2018 followed by 2017 followed by 2016 and so on.,301555.0
91123,380678.0,The youngest will come at the first . You have given descending order. Hope this helps.,317149.0
91123,380707.0,In Mysql by default ordering takes place in ascending order. Select * from employee Order by extract(year from bdate) desc; So here you are ordering by desc i.e descending order. Just remove 'desc' as below Select * from employee Order by extract(year from bdate) ; Hope this will help,317991.0
91123,380716.0,"Thanks Suja and Vipul, but this is a quiz question and I cannot change desc to asce. The answer seems to be wrong. Did you also find this issue with the quiz question?",300717.0
91123,380861.0,the query will show the youngest first,300694.0
91123,380890.0,it will result in year wise descending order so the answer will be youngest first. if someone is born in 2018 and sm1 in 2017 then 18 one will be first row,317982.0
91123,380963.0,The youngest one will come first.,314183.0
91123,381180.0,use asc in place of desc as it will only give the youngest first,318017.0
91123,389524.0,chotu aka youngest will come first,312357.0
91019,379901.0,des is which column are you talking about company db ?,318017.0
91019,379926.0,"yes you can disable 'safe' mode in preferences of MySQL workbench and then restart the program then you will be able to run delete and update statements without having a 'where' clause Edit -&gt; Preferences -&gt; ""SQL Editor"" -&gt; Other section at the bottom",300694.0
91019,380012.0,Just execute SET SQL_SAFE_UPDATES = 0; before doing an update employee,300717.0
91019,380409.0,It looks like your MySql session has the safe-updates option set. This means that you can't update or delete records without specifying a key (ex. primary key ) in the where clause. Try: SET SQL_SAFE_UPDATES = 0; Or you can modify your query to follow the rule (use primary key in where clause ).,318368.0
91019,380591.0,Simply execute this SET SQL_SAFE_UPDATES = 0;,301555.0
91019,389137.0,set safe_sql_updates=0; this is ur safest bet,305655.0
91019,382263.0,"In Mysql workbench go to edit-&gt;preferences-&gt;sql editor, there is check box which you will have to uncheck --Rajesh",300708.0
91855,384999.0,"There is a built in function. Try to google it, you'll find it easily.",318084.0
91855,385098.0,There are multiple functions available which can do the trick. It depends on the datatype of the column. If it is in string format than you can use str_to_date(),317689.0
91321,381609.0,"The foreign keys for this table have been defined on primary keys of other tables. fk_essn --&gt; references employee.ssn fk_pno --&gt; reference project.pnumber. The correct alter table commands in this case are alter table works_on add PRIMARY KEY (`essn`,`pno`); alter table works_on add CONSTRAINT fk_essn FOREIGN KEY(`essn`) REFERENCES `employee` (`ssn`); alter table works_on add CONSTRAINT fk_pno FOREIGN KEY(`pno`) REFERENCES `project` (`pnumber`); Hope this helps.",313826.0
91321,381624.0,"Hi Sravani, alter table works_on add CONSTRAINT fk_essn FOREIGN KEY(`essn`) REFERENCES `employee` (`essn`); In the above statement, the various parts means the following: fk_essn : constraint name FOREIGN KEY : type of constaint essn : the column on which the constraint is created employee : the table to which the foreign key references. In this case, its the self/same table (essn) : the column of the table to which the foreign key column references. So ideallly, the foreign key column cannot be same as the column that it references. It should be the primary key or part of the primary key, if the primary key is composite. So the correct statement will be: add CONSTRAINT fk_essn FOREIGN KEY(`essn`) REFERENCES `employee` (`ssn`); Hope this helps.",310511.0
91321,381655.0,"Hi Sravani, the foreign key wont be a primary key for some column in table. What I meant was foreign key cannot reference the same column as the one on which it is made. It has to reference a primary key on the same table(self join) or primary key of some other table.",310511.0
92178,388381.0,You can use below one: https://cdn.upgrad.com/UpGrad/temp/546f50fd-8523-47ff-8f53-b3e4e844f903/companydb+(2).sql,319006.0
91254,381356.0,Found the error... Thankyou...,300698.0
91254,381422.0,you can just delete the entire question using the ... on the top right - if you wish,300694.0
91254,381436.0,"primary_key keyword has syntax eroor, it should be ""primary key"" . Also avoid keywords like ""nothing"" as column label while creating a table.",318770.0
92051,386534.0,"as per my understanding, safe mode won't allow you to delete or update the table content.. and hence you set iit up to 0 or else you can turn it off permanently from the edit &gt;&gt; preferences path",316349.0
92051,386549.0,It means that you are trying to modify without a where clause on a table in which keys/ relationships have been defined. Using the statement SET SQL_SAFE_UPDATES=0 removes the constraint. Under Edit -&gt; Preferences -&gt; SQL Editor -&gt; SQL Editor you will find the following option : remove Forbid UPDATE and DELETE statements without a WHERE clause (safe updates) Unselecting this option will also remove safe mode. Hope this helps,317149.0
92051,386865.0,If doesn't allow you to delete rows without a where clause. Under Edit -&gt; Preferences -&gt; SQL Editor -&gt; SQL Editor you will find the following option : remove Forbid UPDATE and DELETE statements without a WHERE clause (safe updates) Unselecting this option will also remove safe mode. Refer here for more details: https://stackoverflow.com/questions/11448068/mysql-error-code-1175-during-update-in-mysql-workbench,317689.0
92051,388958.0,"If you are encountering the above error, it's because your MySQL connection is running in Safe Updates mode. This helps prevent user from overwriting large amounts of data accidentally. In fact, if we had forgotten to include the WHERE clause we would have updated every single record in the table!",312892.0
91242,381280.0,The query clause ‘order by salary desc’ orders the salary in descending order. Can you tell em what wrong do you see in this ?,317460.0
91242,381282.0,"The question asks us to run the following commands: Alter table employee Modify salary varchar(10); Alter table employee Change salary pay varchar(10); Select salary from employee Order by salary desc; After execution of the first two queries, the column salary with type float(10,4) has been changed to pay with type varchar(10) . Now if we run the query: "" Select salary from employee Order by salary desc; "" , it will result in an error as there is no column by name ' salary ' in the employee table. Hope this clarifies.",313826.0
91895,385302.0,change the safe updates from edit &gt; preference &gt; edit queries and uncheck safe update,318017.0
91895,385213.0,"Execute the below query before the update query SET SQL_SAFE_UPDATES=0; Alternatively, to permanemtly disable the safe updates option navigate to ""Edit--&gt;Preferences--&gt;SQL Editor--&gt;Other"" and uncheck the option against ""Safe Updates"".",313826.0
91568,383737.0,"ssn as 'Select' , Here 'Select' is just alias or name of the column in the result table",317811.0
91568,383272.0,"Hi, in this example they have just created the new table using the existing table, so `select` is just an Alias used for SSN, if you will run this query and select * from e3;(in below example i have used e4) result will be as below--",305129.0
91568,383333.0,"That part was trying to show how a keyword of MySQL can be used as column name if needed. In this example we intended to change the column name of 'ssn' to 'Select' in our new table. If we put Select without backticks in this case (i.e. ssn as Select) , we will get error as Select is a keyword in MySQL.",311686.0
92743,390650.0,fk_super_ssn is just the name of the foreign key constraing you are creating. You must specify the column name inside foreign key() construct to let mysql know which column(s) you want it to consider as foreign key.,310974.0
92516,389714.0,"This is correct:- substring_index(recipe_name, ' ', 1) --Delimeter must be space. What you have written:- substring_index(recipe_name, '', 1)",301652.0
92516,389848.0,Looks like you did not add the space while mentioning the delimiter. It should be ' ' and not ''.,318554.0
92516,389712.0,"hi SUBSTRING_INDEX(str, delim, count) just ad a space in your delim ' ' and try hope it helps",319056.0
92516,389718.0,"substring_index(recipe_name, ' ', 1) as upd_reciper_name # Delimeter must be space.",301652.0
92516,389776.0,"You need to specify delim. Syntax as follows: SUBSTRING_INDEX( str , delim , count ) Returns the substring from string str before count occurrences of the delimiter delim . If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. mysql&gt; SELECT SUBSTRING_INDEX('www.mysql.com', '.' , 2); -&gt; 'www.mysql' mysql&gt; SELECT SUBSTRING_INDEX('www.mysql.com', '.' , -2); -&gt; 'mysql.com' Hope this will help",317991.0
92516,389893.0,"Usage : The SUBSTRING_INDEX() function returns a substring of a string before a specified number of delimiter occurs. Syntax : SUBSTRING_INDEX( string , delimiter , number ) string Required. The original string delimiter Required. The delimiter to search for number Required. The number of times to search for the delimiter. Can be both a positive or negative number. If it is a positive number, this function returns all to the left of the delimiter . If it is a negative number, this function returns all to the right of the delimiter",318732.0
92516,389901.0,you should add space in delimiter. eg ' ',306012.0
92516,390045.0,Use proper space between single quotes delimiter,317811.0
93162,392262.0,The default functioning of MySQL will allocate a non-null field as the Primary Key.,304281.0
92670,390416.0,I used this before any alter or update commands --&gt; set SQL_SAFE_UPDATES = 0; Maybe this will work.,301652.0
92670,390750.0,Update : After considering many options in one of the forum it was mentioned that this is due to a bug in the program so I've re-installed my sql server and now it seems working fine.,318370.0
92670,391700.0,"It is a technical glitch, some sort of commands don't run hence SQL require update so update restart it will start running Cheers Happy Upgrading :)",312357.0
93006,391688.0,"Check there syntax there need , for every statement Kindly go throught the Date manipulation session where it is clearly explained https://learn.upgrad.com/v/course/208/session/19878/segment/101126",303673.0
93006,391715.0,"you need to put comma after bdate, year and month.. and not day.. if this still results in error.. then strange.. in either case can you please paste your error here which will help in knowing the problem further..",316349.0
93006,391809.0,this is the error I am getting,317558.0
91853,384946.0,"Yes it is possible using the where clause you can condition even you can use stored procedures for the same. Eg. below for reference adding new row in Emplyee table for new employe code and updating new sal for existing employe. procedure insert_row(new_emp,new_sal) select count(1) from employee where employee_number = new_emp if count =0 Then insert row; else do not insert; update employee table with new sal; end procedure; Hope it helps",307843.0
91853,385105.0,"You can do inserts and updates based on certain conditions. For example you would want to insert on the records which have order_amount &gt; 10000 into some premium customer table. So, this can be done by adding a simple where condition in the insert statement. General format: INSERT INTO table-name (column-names) SELECT column-names FROM table-name_2 WHERE condition",317689.0
91853,384957.0,"I'm expecting this a general question; though it is possible but this is not basically expected to be done in SQL since SQL is a storage database which has only few basic clauses, select, from, where, groupby, having, order by etc. as a data analyst we are just expected to extract or load data out of database and do all the calculations and computations in analytics tools like python, R, Tableau and other.. python/R has got more ability to mold your data into the form that you want.. SQL, you can add columns and do any complex calculations in the select clause and store it temporarily in some table.. or use scripts to do this for you..",316349.0
91431,382331.0,dept is the column name,318017.0
91431,382329.0,it looks like there is already a column called 'dno' and hence when you are trying to add it again..it is giving error.,317998.0
93401,393209.0,"update tablename set columnameinwhichvaluetobeinserted = substring(columnfromwhichvaluetobetaken, ) pass required parameters in substring.",317811.0
93570,394132.0,I got it. Just some syntactical error that was causing this problem. Thank you for the help.,303085.0
93570,394122.0,"Make sure you selected datatypes correctly for column names. Try for WAP flaot (10,2)",313200.0
93570,394151.0,You can load only relevant column by load data infile to avoid unnecessary data handling,319869.0
102148,,nan,
102142,437117.0,"the select part should be in brackets please try this create table e2 as (select concat(fname, minit, lname) as 'Name' , ssn, bdate from emp ); Syntax CREATE TABLE new_table AS (SELECT * FROM old_table); Reference:https://www.techonthenet.com/sql/tables/create_table2.php",317822.0
102142,437071.0,"I do not see any error in your syntax. Only possibility which I can think of, the table e2 already being present and you are trying to create it again. Try the below code: ------------------------------------------------------------------------------------------ drop table if exists e2 ; create table e2 as select concat(fname, minit, lname) as 'Name' , ssn, bdate from emp; ------------------------------------------------------------------------------------------ If that doesn't work, can you share the error you see?",311160.0
102145,437109.0,"A foreign key always requires an index. Without an index enforcing the constraint would require a full table scan on the referenced table for every inserted or updated key in the referencing table. And that would have an unacceptable performance impact. This has the following 2 consequences: When creating a foreign key, the database checks if an index exists. If not an index will be created. By default, it will have the same name as the constraint. When there is only one index that can be used for the foreign key, it can't be dropped. If you really wan't to drop it, you either have to drop the foreign key constraint or to create another index for it first.",317822.0
123479,538391.0,Can you please share which question you are referring to?,304281.0
92448,389374.0,"Yup, exactly!",319357.0
92448,389382.0,yes in a manner of speaking.,317689.0
92448,389414.0,yes you are right..,319869.0
92448,389725.0,You are correct,314048.0
92448,389810.0,yes you are right . but tableau has flexibility it can treat automatically,314612.0
92448,390039.0,"Yes! It's almost same . Tableau divides the data in two main types: dimensions and measures. Dimensions are usually those fields that cannot be aggregated; measures, as its name suggests, are those fields that can be measured , aggregated, or used for mathematical operations. Dimension fields are usually used for row or column headings; measures are usually used for plotting or giving values to the sizes of markers. When you import the data for the first time, Tableau determines whether to consider a field as a dimension or a measure. So we can say, Dimensions are equivalent to Dimensions in Tableau and Facts are equivalent to Measures in Tableau.",319006.0
92448,392840.0,"Yes, indeed",313228.0
90160,375573.0,"I'm making an attempt to answer your question to the best of my knowledge 1. Are transactional databases and data wareshouses powered by the same data management technologies? The underlying database in Transactional and datawareshouses could be same tool (Oracle, HANA etc). However, the instances will be different. Datawarehouse and Transactional systems will usually not share the same database. The database instance will be different but the underlying tool could be same or different depending on how it is implemented in an organization. 2. As data scientists/ML engg. will we work with transactional db's or datawarehouses? As data scientists/ML engg, in most cases, the data from transaction databases will be extracted into either datawarehouse or to a data lake (which is a centralized place for storing all the required data for analysis/analytics). In cases, where the organization wants to separate Business Intelligence from Big Data analytics, the data in datawarehouse will be used for BI reporting and data lake will be used for performing data analytics (Machine learning and data science). In the case where there is a separate data lake, the data from data warehouses will also feed into data lake. 3. How is an organization's data engineering team related to transactional db's and data warehouses? Data engineering is most cases will be entrusted with the task of setting up the environment for data science/machine learning. By setting up, I mean deciding on which technology stack to use (Cloudera distribution/ HortonWorks/AWS/Azure/Google Cloud etc.....) and also will be responsible for data pipelining. By data pipelining, I mean whatever data is required for accomplishing a specific machine learning/Data science use case, the Data engineering team will decide on what technology to use (SAP SDI/SLI/Data services, Informatica etc) to bring the data to data lake. Hope this helps.",318084.0
90160,375616.0,"Hey Pranesh, Please go through the below link for more clarification on transactional database and data warehouse. 1) You question is little confusing. I understand that you are asking whether both can be done in MySQL. Yes, both can be managed under MySQL. Transactional database is update, drop, alter command whereas Data warehouse is mostly select command. 2) As data mining engineer , you will be working mostly with historical data. So , it is data warehousing. 3) Transactional database supports transaction-oriented applications in a 3-tier architecture. OLTP administers day to day transaction of an organization. Data Warehouse provides analysis of data for business decisions. This systems allow users to analyze database information from multiple database systems at one time. So, an organisation can use both as per their requirement. Broadly data warehouse is used by CTO/CEO of company to take decisive steps. For more information, you can check this site : https://www.guru99.com/oltp-vs-olap.html",319696.0
90160,376101.0,"Also, why are data warehouses denormalized?",306733.0
90160,377612.0,"Hey Pranesh, I don't want to take credits to already answered question on Quora, here is the link and I find some explanations meaningful and helpful: https://www.quora.com/Is-data-warehouse-normalized-or-denormalized-Why I do want to point out my experience though since working with application database for more than 12 years. Basically, the more you normalize the data , the more time it takes to write complex joins to pull off some meaningful derivative out of the data. Data warehouses on the other hand maintain simple table structure (mostly I worked with big fat tables) but have easier/less complex queries to derive meaningful information. This is because of the way you leverage Data warehouse hints (and the query writing style for DWH differ from the trasactional DB queries) to pull that information from big fat tables (without hints, the pull takes as long and is almost the same throughput as the code written using complex queires on highly normal data pull from transaction DB's) Regards, Harsha",309211.0
90160,382379.0,"Denormalization -makes query processing faster on a multiple join queries ,but it comes with a cost of storage",319056.0
90160,382413.0,"Hey Pranesh Let me take one quesition at a time. 1. Are transactional databases and data warehouses powered by the same data management technologies? Answer : Yes . This is because it is very easy to take a dump of all the from the main DB and store it in the Data warehouse. 2. As data scientists / ML engg. will we work with transactional db's or data warehouses? Answer : In comapnies, BI/DS team have deducated server with data ware house. This is maninly because your have to run many quesries on real time. So if BI team and Tch team share same DB, It will load the DB with many queries. So always a slave DB is provided to BI team where they can run as many queries they like without overloading the main DB or transcational DB. 3. How is an organization's data engineering team related to transactional db's and data warehouses? Answer: BI team works with management with data report and helping them to take managemt product descions based on the report , While tech team executes the descions by implementing it in code.",303082.0
91382,388985.0,"As here date provides extra information about the fact variable here , hence it is a dimension",318427.0
91382,382081.0,Date should ideally be a dimension as we compare facts based on dimensions and over here we generally would compare values based on dates.,317460.0
91382,382084.0,Please go through below links. You will get some clarification whether Date can be factor dimension? http://radacad.com/do-you-need-a-date-dimension https://www.quora.com/Why-should-I-add-a-separate-time-or-date-dimension-in-the-construction-of-a-data-warehouse-in-lieu-of-SQL-date-and-time-types-in-the-fact-table Hope this will help.,317991.0
91382,382108.0,Generally we want to measure some value against date to check the trend. Say we want how temperature is changing over past 1 year. So we have temperature(measure numerical value) for each date (dimension - categorical value) Now we want this temperature measure for each date which is dimension. And we can roll up at dimension(quarterly/monthly/weekly average or total).,318554.0
91382,382163.0,Date is an Dimension table. Fact tables stores measures.,314197.0
91382,382197.0,"Date is dimension because you can measure/ aggregate your facts based on dates. For example you want to see product sales by category. So, category is dimension. Similarly if you want to see product sales for a week or month (i.e using date as dimension). Using date as dimension you can derive various aspects of it i.e day of the month, weekday, quarter etc. which are helpful while doing the aggregations of the facts.",317689.0
91382,382227.0,Date is a fact.,308774.0
91382,382267.0,"A date is a Dimension and its list of attributes would be like month, year, holidayFlag etc.,",318732.0
91382,382290.0,it is dimension,311219.0
91382,382304.0,"Depends on where the date is located. - If it is an attribute describing main data, i.e. time stamp of observation. It is in fact table. - If it is in a table which has various attributes for date and time, i.e. year, month etc. It is in Dimention table",318436.0
91382,382058.0,"Facts consists of measurements (metrics / facts) Dimension categorizes facts and measures. It is referenced by fact tables using keys. Thus, date is a dimension",314048.0
91382,382596.0,Dimension,306009.0
91382,382423.0,"Date is fact because it contains the numeric values like measurement, month,avg, year where as dimension is the categorical variable which shows the decription of the facts. Generally fact is refered to as numeric values where as dimension referres to as catagorical values.",308639.0
91382,383000.0,"In a typical Data ware house system , DATE will be defined as an Dimension and using the date dimension table all the fact table data are extracted to apply business rules/logic. When you create a simple table fields, its up to you how you define the date it may be a simple column where you used to do a greater than condition to derive a data. If you're considering buliding up a DATAWAREHOUSE DATE MUST be kept as a Dimension table.",306735.0
91382,383177.0,Apart from measures everything including flags refers to Dimensions So Dates are always Dimensions,304693.0
91382,386788.0,"DATE is a most common table that we would be ideally relate as dimension. Sometimes, could be used to generate fields in Facts as well. For example, you have Order Date and Delivery Date. The difference of these would give you the days for completing of order which could be number type attribute in our Fact table",308960.0
92452,389411.0,Yes. Cross mean it should earlier be smaller and should have become larger. For more details refer my answer at following. https://learn.upgrad.com/v/course/208/question/92449/answer/389384/comment/97113,317689.0
91436,382335.0,You can use the Table Import Wizard on the left navigation panel of the workbench to do this. I am assuming you were trying to do this with the load statement.,310511.0
91436,382336.0,"Create a schema and import it using the table wizard. It should work. If you tried this and still getting error, please share the error message.",318084.0
91467,382998.0,1. go to control panel uninstall all the installed components. Restart your system once clear your temporary files. 2.Install visual studio 2015+ version and install .NET framework 2.5+ and then install SQL installer. it should work fine.,306735.0
91467,390734.0,All the above recommended steps followed but installation of SQL still fails as pr the processs mentioned in the installation guide from Upgrad/IIITB. Is there an alternate freeware which will help me perform the assignment through SQL queries?,301644.0
91467,383259.0,1. Update your windows from windows update. Do it 2-3 times to ensure everything even th optional updates are installed. 2. Uninstall any version of MySQL installed. 3. Restart 4. Download and install Visual C++ Redistributable file for 32 and 64 bit both. 5. Download MySQL Community edition from below link.Download the MSI installer one. https://dev.mysql.com/downloads/file/?id=476477 6. Execute the installer. Setup the password and also do not forget to add one user with admin cred just to ensure. This once installed will already have your world database.,301555.0
91501,382828.0,well it is like asking for microsoft Office :-) Microsoft Visio can do ERDs and there are LOTs of free ones - just search on download dot come for a free one with high reviews,300694.0
91501,383004.0,"If you have schema setup already in place, the relation ship b/n tables are automatically shown cuz all the foreign keys,primary keys are defined. Oracle SQL developer - ORACLE MySQL Workbench - MYSQL",306735.0
91517,382978.0,"to add further, database can be any OLTP,OLAP or for any purpose but data warehouse has characteristics and a purpose, it needs to be designed meaningfully, just a collection of database cannot become a data warehouse",301115.0
91517,382966.0,"Like the the professor had explained, you are using the term database incorretly. Datawarehouse is also a database. what you are trying to refer to is transactional database . (used for daily transactions..such as login, purchase etc) and that is different from data warehouse. although both are subsets of database. the difference is in terms of volatily, time variance, subjectivity. (as the professor pointed out) so, No, we cannot consider data warehouse as a collection of transactional databases. hope that helps.",317998.0
91517,382995.0,"several Datamarts together forms up DATAWAREHOUSE Several Databases together forms up DATAMARTS. As you said, if you have multiple databases you can connect them and generate business results. (OLTP) When it comes to DATAWAREHOUSE we typically have hell lot of data and we need to process more data to take a meaningful business decision.",306735.0
91517,392189.0,"There is a difference in architecture in transactional database and datawarehouse database and they follow different structure so collection of transactional database cannot be called as datawarehouse however collection of data marts can be called as datawarehouse and these data marts can represents the particular section of the company like finance, marketing, hr etc. Also there is no point saying collection of transactional database as datawarehouse as transactional database is volatile and does not store historical data which is necessary for analysis.",320685.0
92533,389812.0,I hope you followed the following format ( This was my understanding). Customer Table Name City Locality Company Table Order Number Mode of Transport Date Time Date Table Day of the date Is Holiday? Coupon of the Day,310522.0
92533,390504.0,"Hey Dinesh, edit answer option is available for you incase you want to modify your answer. Just submit your answer and move ahead. Hope that Helps.",302742.0
92533,392898.0,"You can list like this, not complete though, Company_Dimension CompanyID Company_Region Company_Started Company_Last_Fund_Recvd Investor_Dimension InvestorID InvestorName InvestorRegion InvestedDate Funding_Dimension FundingID FundingType FundingDate FundAmount",313228.0
92534,391258.0,"thanks Sushmitha , let me check",319969.0
92534,391083.0,"If the csv file is huge, it will take a lot of time for it to get loaded in MySQL. And you can do so, by using the UI, by opening, Table Data Import Wizard. check this link, there are ways to do it from the command line too which are stated in this. http://blog.devart.com/how-to-import-a-csv-file-through-the-command-line.html",319721.0
90756,378198.0,go through this video : https://youtu.be/zMIxLCZcfhc,318017.0
90756,378205.0,Facts and Facts table are different. Facts are numeric attributes while Facts table has facts and attribute corresponding to the dimension tables.,318329.0
90756,378285.0,"Professor has coverred this concept of Fact table in the first video where he mentioned that Facts table is including Facts (numeric attributes) and dimensions.. Dimensions provides the life for the numeric attributes.. So yes as Nagaraju stated, 'Facts table has facts and attribute correcponding to dimension tables.'",316349.0
90756,378432.0,"A fact table stores quantitative information for analysis and is often denormalized. A fact table works with dimension tables and it holds the data to be analyzed and a dimension table stores data about the ways in which the data can be analyzed. Thus, a fact table consists of two types of columns. The foreign keys column allows to join with dimension tables and the measure columns contain the data that is being analyzed. Facts don't contain all the columns as numerical. Take an example of DWH. You will have 1 product dimension which will contains all the attributes of the product i.e. name, desc, etc. Whereas in the order fact you will have the quantity ordered with product key, customer_key, region_key. These keys will be foreign key to dimension tables product, customer, region etc. So, in a nutshell fact would contain attributes over which you can do some aggregations and grouping to get some insight. For example if you want to get the orders per region from above case you would aggregate over order fact and group by region For more info refer http://www.zentut.com/data-warehouse/fact-table/",317689.0
90756,381419.0,"Facts as a numerical Attribute is an easy example for us to understand the functionality of the fact table. There are different types of fact tables Transactional Fact Snapshot Accumulating The above facts also come in different flavors Additive Semi Additive Non Additive Factless Facts you will get a clear understanding as you go deeper into Dimensional Modelling. Fact alone doesnt make any sense unless we can define what it represents ""For example XYZ company has sold 50 Pen drives 10-Jan-2018"" Basically Fact table attributes represents the above statement in tabular format. And these attributes are Dimensions of the fact or also known as the grain of the fact table. Grain at a high level is defined as the lowest unit representation of the fact table. --Rajesh R",300708.0
91504,383277.0,"My take on this is that if the datawareshouse is built ground up on a database like say MySQL, then these kind of things have to be manually done. In case of the sophisticated datawarehouses like SAP BW, these are taken care of by the framework. i.e, SAP has built in functions to handle them and will provide you a wrapper to associate the relevant attributes and metrics into one logically partitioned area. Hope this helps.",318084.0
91504,383348.0,Aggregate Tables are nothing but Materialized Views in DWH. You need to write SQL and create it and then it gets refreshed periodically. CUBES can be created by tools like MS SSRS &amp; SSAS.,301555.0
91504,384820.0,"Thanks for the Answers. I understand it can be achieved through using materialized views, but the periodic refreshment (for type1 and type 2 dimensions and associated calculated attributes) do we normally use any specific tools like (ETL tools) or using stored procedures. ? I assume, since the inserts or updates or not so frequent, it should be through a bulk process through ETL tool. Please correct me if I am wrong or is any other ways to achieve this?",311115.0
90346,376638.0,"As far as what I understood, facts and fact tables are different. Facts are numerical attributes as you said while fact tables have key attributes such needs dimensional tables for additional data associated with it.",318329.0
90346,376642.0,Please go through below links for Facts vs Fact tables. https://www.1keydata.com/datawarehousing/fact-table-types.html http://www.zentut.com/data-warehouse/fact-table/ I hope it will clarify your doubt.,317991.0
90346,376654.0,"Simply put, facts-tables are the schemas which has facts(numerical) recorded in it along with other attributes which refers to deminsion tables to that they can be analysed better.",306248.0
90346,376652.0,"A fact table stores quantitative information for analysis and is often denormalized. A fact table works with dimension tables and it holds the data to be analyzed and a dimension table stores data about the ways in which the data can be analyzed. Thus, a fact table consists of two types of columns. The foreign keys column allows to join with dimension tables and the measure columns contain the data that is being analyzed. Facts don't contain all the columns as numerical. Take an example of DWH. You will have 1 product dimension which will contains all the attributes of the product i.e. name, desc, etc. Whereas in the order fact you will have the quantity ordered with product key, customer_key, region_key. These keys will be foreign key to dimension tables product, customer, region etc. So, in a nutshell fact would contain attributes over which you can do some aggregations and grouping to get some insight. For example if you want to get the orders per region from above case you would aggregate over order fact and group by region For more info refer http://www.zentut.com/data-warehouse/fact-table/",317689.0
90346,380485.0,"Fact Facts are the measurements/metrics or facts from your business process. For a Sales business process, a measurement would be quarterly sales number Fact Table A fact table is a primary table in a dimensional model. A Fact Table contains Measurements/facts Foreign key to dimension table",306242.0
91594,383459.0,"Fact table is essentially 1 table in star schema that is created first which contains columns acted as measures which can are foreign keys by for dimensions tables. So, based of fact tables dimensions tables are created. From your question I can understand that does columns of fact table taken from different tables or where do they come from. They are born from questions and analysis requirements of business.",318328.0
91594,383474.0,A fact table typically has two types of columns: those that contain facts and those that are a foreign key to dimension tables. The primary key of a fact table is usually a composite key that is made up of all of its foreign keys. You can go through below link to get more insight about fact table. https://en.wikipedia.org/wiki/Fact_table Hope this will help.,317991.0
91594,383577.0,"Facts or fact columns are generally numerical and some mathematical operations can be performed (sum, avaerage etc.) where as dimensions are meta data that add more meaning or life to the facts.Eg. Salary, Hours are facts, bdate, ssn, pname are dimensions. In a given schema (depending on the actual schema an its design), facts can be spread across multiple tables and not necessarily restricted to one table alone.",300717.0
91594,383598.0,"We group data we have on hand into the facts and dimensions and derive at some dimensions from existing ones. Facts can be in multiple table. In the upGrad schema that was explained, there are mulitple fact tables - Lead table, Application table, Payment table, touchpoint table, Conversion table that are connected to different dimension tables.",301654.0
91594,383728.0,Facts are part of the transaction table and using which later dimensions tables are created,317811.0
91594,383973.0,"Fact tables have keys from dimension tables, i.e facts have columns which act as foreign key to Dimension table, so its ALWAYS DImension table is created first and then the fact tables. This is based on my exp.",301555.0
91594,384829.0,"A fact table can have columns which is a combination of other tables apart from columns which are inherent to itself. As prof. RC explained, a fact table in itself cannot help in drawing insights on the dataset, it is the metadata which are part of the dimension tables that would help us to do analysis and decision making. A fact table will have columns which are foreign keys to the dimension tables. Typically, a combination of columns will make a primary key in a fact table.",314730.0
91638,388989.0,"Hi As it is a numeric value , hence it is a fact.",318427.0
91638,383686.0,"Hi, The transaction amount is Fact. In order to better understand the difference between Fact and Dimension can refer below link.. https://stackoverflow.com/questions/20036905/difference-between-fact-table-and-dimension-table Hope it helps :)",305129.0
91638,383730.0,"Numeric Values are facts. Transaction Charge amount is fact and also it does not stets any catergory in itself, can only be used to do mathematical operations. Henc It is a Fact",317811.0
91638,388483.0,Facts are nothing but the measures in terms of Tableau. The concept remains same just that the trems keep changing!,316255.0
91638,389474.0,Transcation charge amount is always a Fact (Measure column in a fact table),304693.0
91625,383626.0,"Schema diagram(Upgrad) there shows the tables and their relationships(observe each table is combination of dimensions and some facts(optional)) Now when we create a star schema(based on the business requirement) which will have fact table sorrounded by dimension tables (using the tables mentioned in Upgrad schema) and load data from the base tables. Only in few cases we can identify a table which has more facts is a prospective fact table,but in real time scenarios like in Upgrad schema based on scenario we will identify which table has most of the facts required becomes fact table and other dimensions required will be dimension table.",301115.0
91625,383984.0,"Simple, Fact tables mostly has columns that are foreign key to dimension tables. They also have columns like sales, profit etc which are measures. In Dimensions you will only/mostly have only attributes. No foreign keys , only primary keys and no measures like sales, profits",301555.0
91625,389478.0,DImension Tables will have a Primary Key which belongs to that dimension with some descriptive columns where as a Fact table will have Measure columns and all the foreign keys (Primary Keys of dimensions) belongs to that star schema. So we need to check the table structure first to conclude it as a Dimension / Fact,304693.0
90508,377548.0,Please go through below links to import csv file in mysql: http://www.mysqltutorial.org/import-csv-file-mysql-table/ https://dev.mysql.com/doc/workbench/en/wb-admin-export-import-table.html Hope this helps.,317991.0
90508,380840.0,http://www.mysqltutorial.org/import-csv-file-mysql-table/ This has very good procedure to code to import csv in mysql. Hope it may help. Which i tried.,312019.0
90508,377638.0,"Hi Darshana, I hope you covered preparatory training in SQL during the start of this course (pre-prep). Please go through ""MySQL reference Book"" Section 4.3 within Basics of SQL page : https://learn.upgrad.com/v/course/208/session/15786/segment/79809 Here is the direct link: https://cdn.upgrad.com/UpGrad/temp/1f7cfd18-a25c-42b5-aa55-04377c2c8eb6/mysql-tutorial-excerpt-5.1-en.pdf Note that the complete syntax is not in the reference as that was depicted for simple example. Complete command is here from MySQL reference manual. I have provided example below on how to use it. syntax: LOAD DATA [LOW_PRIORITY] [LOCAL] INFILE 'file_name.txt' [REPLACE | IGNORE] INTO TABLE tbl_name [FIELDS [TERMINATED BY '\t'] [OPTIONALLY] ENCLOSED BY ''] [ESCAPED BY '\\' ]] [LINES TERMINATED BY '\n'] [IGNORE number LINES] [(col_name,...)] example: mysql&gt; LOAD DATA INFILE 'companies.csv' INTO TABLE companies FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n'; Note that the table being loaded to should have same number of columns as the csv you are trying to load . If not, use this syntax: mysql&gt; LOAD DATA INFILE 'companies.csv' INTO TABLE companies (col1,col2,...) FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n'; If delimiter changes, replace the comma with the delimiter being used. Thank you, Harsha",309211.0
91217,381107.0,I think there are various kinds of updates 1. update of schema 2. update of existing data 3. adding of new data to some variation over time ////// 1. and 3. are ofcourse acceptable (1. - within limits because we have to consider how to ensure integrity of underlying old data) 2. is to be avoided and only done if something was wrong with the data; unlike in transactional database where values are constantly updated,300694.0
91217,381110.0,might be useful for your query https://www.oreilly.com/library/view/data-warehousing-fundamentals/9780471412540/9780471412540_updates_to_the_dimension_tables.html,317982.0
91217,381121.0,"Addition to above links, you can also go through below links to get more insight about ' Update to Dimension table"" http://www.ques10.com/p/17034/write-short-note-on-updates-to-dimension-tables/ https://en.wikipedia.org/wiki/Slowly_changing_dimension Hope this will help",317991.0
91217,381185.0,yes you can update the schema and tables once created data warehousing is a growing thing you should always try to improve the data warehouse once created you cant just depend on it so it is very helpful to update the fact and dimension table suppose i have a dimension table specifying the return of orders you can modify it by adding a new field that item damaged to find out are the damaged item reaching your customers,318017.0
91217,381413.0,"Usual practise is not to update the DW tables. But that often is not the case with any DW systems. you will end up processing historical data, missing data. Correction of some type of data etc every time. Usual Cases we encounter when we have to update DW tables Change in Dimension like Person's name, or person address (again depends Type 1 or Type 2 depending on Business Use case. Usually handled ETL) Conversion of Type 1 table to Type 2 Slowly arriving dimensions or Fact reload Business use case changes partial load DW restructuring --Rajesh R",300708.0
91847,384874.0,Major difference between OLTP and OLAP : OLTP is optimized to write every user transaction in real time (there could be millions of transactions written one at a time) while OLAP is designed for extremely few write operations. OLAP databases may be updated maximum once/ few times per day. But in terms of consumption of the loaded data - OLTP read queries are very light / very less complex while OLAP reads are subject oriented/ multi-dimensional/ integrated in nature. OLAP queries could potentially go against large datasets. Due to differences in read and write operations you would obviously design the databases in completely different way. On your point on ETL - that is again a batch load process...very unlikely to happen every time there is a new transaction. Of course there are some exception scenarios...where OLTP can validate a record against a DWH in very specific cases (Master Data Management scenarios) before commiting a transaction.,306250.0
91847,386899.0,"Reporting tools can be used on OLTP systems, but is not advisibile. Traditionally, in OLTP systems we place table level locks to avoid simultanous updations on same record. likewise, trasactions are updated very frequently. in usch circustance, accessing the data from the same table for reportig would impact either reportig or impact updations of Transactions. So, conventionally the OLTP systems are not used for reporting. Likewise, OLTP systems only store very recent information and hisotrical reporting using OLTP system does not give the required details. Samething goes with using Server side transactions on top of OLAP systems. They are different breeds of horses with different purpose.",308960.0
91847,385418.0,"OLTP is like the database where you are performing daily business transactions and It is having limited number of tables or says like Fact Table will be here. using OLTP database and its archieved data, Datawarehouse will be created by creating dimension tables using the OLTP fact tables by analysing the subject which data is required and what should be the dimension to fulfil the study of business case",317811.0
91126,380944.0,"Yes good to keep month and other date attributes in date dimension. Try to avoid the function to derive during runtime. This is the design followed in star schema. Join the fact usinf date dimension id and use the date attributes as month, year , qurter for rollup/aggregating the data",315679.0
91126,380757.0,that is one of the differences between transactional and OLAP db; in OLAP db such details are STORED in the db in dimension tables; whereas in transactional db and in simple reporting we calculate on the fly,300694.0
91126,380681.0,yes. it can be. but if you need some intensive reports on regular basis with month as one parameter/criteria then imagine the computing time it will take every time to find out month from date column. It might be trivial for medium size data but for heavy databases it can be significant. also it will come handy when you are sure that you want only month for any analysis. you don't then need to invoke other information stored in date column at all.,311686.0
91126,380696.0,"In real time projects,we usually don't store month of all the date columns(order date, invoice date,shipment date),only based on the(month end Invoice reports) reports which significanyly uses month column (i.e Invoice date) in the report we store the month of that Particu;ar date(Invoice date) Most of the advanced DB,reporting tools which handles huge data have bultin functions to convert dates to months even for millions of data,we go for indexing for other king of data like order number,line item",301115.0
91126,380740.0,"Yes, You can derive it from the date column, but if you have them as additional columns it will reduce the load on the CPU when doing reporting and calculation on arriving at the month column can be avoided.",301113.0
91126,381026.0,"This table handles the time dimension. It contains five attributes besides the primary key. The lowest-level data is sales by date ( action_date ). The action_week attribute is the number of the week in that year (i.e. the first week in January would be given the number 1; the last week in December would get the number 52, etc.) The actual_month and actual_year attributes store the calendar month and year when the sale occurred. These can be extracted from the action_date attribute. The action_weekday attribute stores the name of the day when the sale took place. http://www.vertabelo.com/blog/technical-articles/data-warehouse-modeling-the-star-schema",314183.0
91126,381402.0,"In Datawarehousing, we are dealing with a large volume of data. It wouldnt be practically a good approach to derive data on the fly as it will impact performance and also enable ease of use of the DW system esp in reporting tools like tableau etc. That is the reason why columns are derived in ETL and captured. Also majority of the data will be in denormalized form. --Rajesh",300708.0
91126,381274.0,"There are a few things to consider here. The volume of data that you are working on may be huge and performing a calculation to derive a new dimension(in this case month from date ) would mean that there is a lot of resource utilization and risk of performance degradation. Further, we are not sure of the kind of queries that may be coming from an analysis perspective. Today, the analysis may revolve around sale by month, tomorrow it could be sales by quarter or sale on holiday. The possibilities are endless. Having a variety of dimensions on date would give us the power to address these concerns.",313826.0
91126,383633.0,"Hi Guys, A bit confused by looking at the answers here. I could see everyone's comment is about performance to extract the 'month' from 'date' column. But in real time if the data is really huge to extract the month value, then the table is divided into partitions per month wise or quarter wise to improve the performance of the queries. If really extraction of month takes time, I believe apart from select query everything takes time on the same table. Happy to be corrected",312093.0
91858,385097.0,Both fast constellation and snowflake schemas are extension to star schema only. When you link multiple facts tables with each other than you get fast constellation. When the dimension itself have hierarchy than you have your snowflake schema. Refer to following link for more detail. https://www.guru99.com/star-snowflake-data-warehousing.html,317689.0
91249,381431.0,Database vs Datawarehouse - Database is a container which holds the data whereas DW is an implementation on how you define the container to hold the data. There are different ways you can design the container As a online transaction processing system(OLTP) or As an online Analytical processing system (OLAP) OLTP is a system which is defined when there are lot of writes. The goal here is capture and save information at a faster rate. on the other hand OLAP is mostly used for representing aggregated data. Goal in OLAP system is faster read with aggregation. Only ETL program is allowed to write to DW and is usually loaded in batch. --Rajesh,300708.0
91249,381382.0,Data warehouse is designed as on a database. It has tables views which is normal concept in a database. i.e Data warehouse is essentially a database only. The question should be OLTP vs OLAP. In OLTP systems where transactions are stored they are designed and optimized to insert and update records in parallel with low latency. Where as in DWH insert and updates happens in bulk and they are generally used to query data for analysis. in order to do the analysis the dimensional modelling is done. Facts and dimensions are designed to get insights from data.,317689.0
91361,,nan,
91046,380104.0,"we need to do dimensional modelling, which means what dimensions need to be grouped to a dimensional table which makes it meaningful, most of the time we draw the idea of which dimensions to be grouped from business.",301115.0
91046,380126.0,"No, they are not. We have to manually create both the fact and the dimension tables by modelling.",310511.0
91046,380497.0,We need to create the Dimensions table to do annalysis with the facts table,308782.0
91046,380502.0,"It is during the design phase when you plan to create dimensions and facts (star schema) based on business requirements and source system analysis. You can correlate with ER diagram/datamodel when modelling transaction systems. To elaborate if you are doing business intelligence for a chain of retail stores, the following could be possibly classifed as dimensions and facts. Dimensions: Store Location, Time, Procuct, Customer, Vendor, etc to name a few Facts: Sales, Inventory, Stock, etc",318002.0
91046,380704.0,"No , It is Designing Stage. We have Design Dimension Tables based on business needs.",318732.0
91046,380738.0,"No, You need to design the dimesions by understanding the OLTP database.",301113.0
91046,380760.0,"In datawarehousing, the accuracy in creating your Dimensional Modeling determines the success of your data warehouse implementation. Creation of fact and dimension table is a part of Dimensional Modeling. It has certian steps as follows: Identify Business Process Identify Grain (level of detail) Identify Dimensions Identify Facts Build Star Please go through below link for more detailed explanation https://www.guru99.com/dimensional-model-data-warehouse.html Hope this will help.",317991.0
91046,380793.0,"FACT and Dimention tables are created by Analyst based on the data nature, and requirement. From datawarehouse ETL team creates datamarts (can say sub-datawarehouse) with FACT and dimenstion tables from other data/tables collected in DW . The core data which depends on other data behaviour are kept in FACT table. the data stored in Dimention table are helps to see the FACT data to slice and dice to analyse the FACT tables data. FACT Tables are quite similar to Master table data in Technlogy.. they store the parameters. All decision making or analysis support data are stored in DIMENTION , from where an User can look and feel the core data stored in FACT.",318454.0
91046,381019.0,A dimension table contains dimensions of a fact.They are joined to fact table via a foreign key. Dimension tables are de-normalized tables.The Dimension Attributes are the various columns in a dimension table. Dimensions offers descriptive characteristics of the facts with the help of their attributes. No set limit set for given for number of dimensions. The dimension can also contain one or more hierarchical relationships. Dimention tables are created by Analyst with the help of Fact table.,314183.0
91046,381175.0,you have to create it depending on the requirement and buisness needs it is done after consulting with the buisness that what are there requirements and what should be thing that they want to check or improve upon,318017.0
91046,381407.0,"In DW there is nothing which is autogenerated. you have to design each and every table. That is why dimensional modelling is done. Once you have your Dimensions and Facts, you will start looking into how to load these tables. This is where ETL comes into picture. ETL stands for Extract, transform and Load. There is also a concept of ELT, which stands for Extract, Load and Transform. Which is typically used in processing real time systems or clickstream data. Here the the goal is to load the data in system. Consumers of the data will apply the transformation part. -Rajesh R",300708.0
92156,387820.0,It worked for me - ALTER TABLE `DS_SEP_18`.`employee` ADD COLUMN name VARCHAR(50) NULL; remove quotes for the column name.,311502.0
92156,387814.0,Could you check if this helps: https://stackoverflow.com/questions/50440247/mysql-8-drop-or-alter-causes-error-3664-hy000-failed-to-set-sdi?rq=1,319006.0
92156,387896.0,"Remove Quotes from column Name, DataBase name and table name. Quotes are not required.",317811.0
92156,388067.0,This is a issue with table name space. Your table is not registered in table namespace propelry. This is a bug in myql 8 version. For more info - https://github.com/mysql/mysql-server/commit/261981bdf42c110f08f98ad2cf84ce6fdef1949e,312746.0
93288,392702.0,"Please remove multiple version on visual studio, .net frame and re-try installing the mysql workbench. it should work.",306735.0
93288,392618.0,Pre requirements for installing MYSQL 1) a) .net framework b) Visual C++ Kindly go through below link... It will be helpful https://cdn.upgrad.com/UpGrad/temp/89ddbb36-9d58-4db6-92b9-d0bebd4d6862/MySQL+Installation+Guide+(Windows).pdf,303673.0
93288,392703.0,"You will need to remove all program files and registry files and reinstall fresh. Uninstalling the installer community is challenging. Download IObit Uninstaller to remove the installer community completely before the reinstall. Try to reinstall in a new drive in the system (D: drive instead of C:). Remember to use same password as before if it asks for only password, else create new username and password.",301644.0
93237,392432.0,"Since you have segregated the table and only have permalink as the common attribute in both tables, I think only it can be used to join back (link) the two tables.",304281.0
93237,392447.0,"I'm sure that you're aware that a star schema has a fact table and dimensions table. For the particulars you mentioned above, the fact table can have the first table. And there can be dimension tables related to this fact table. Like you mentioned, table 2 can be related to the permalink column. If you want to add other dimension tables, you can relate the same using respective columns with the help of the foreign key concept.",319721.0
91508,382875.0,"hi, I think facts columns are not id and reference columns. facts columns are those which contain some factual and numeric information. for example price of house is fact column whereas id of that house is not fact column. suppose you are storing location of house, city etc then these will be dimensions.",311686.0
91508,382879.0,"I am reframing my question. Is it like apart from all facts columns , all other columns (excluding id and reference columns) in a table become dimension candidates?",304398.0
91508,382976.0,"In regard to what the professor taught, there were only 2 types of classification, facts and dimensions. so, anything that is not a fact becomes a dimension, since if it were not a fact or wasnt describing or providing additional information regarding a fact (as the dimensions do), there wouldnt be any need to keep it in the data warehouse schema. hope that helps.",317998.0
91508,382988.0,"well,apart from facts rest becomes dimensions,but in real time scenario challenge is few dimensions need to be categorised as facts(like order numbers). secondly, grouping the appropriate dimensions into a dim table 😀",301115.0
91508,383349.0,"Dimensions - Are mostly the smallest unitof tables having the information about a topic. They simply have information about the topic. Facts - They have key columns to connect to dimensions from more then one dimension and measures like sales, profit etc. Example - COnsider a CAR Company. Dimensions will be Car_Model, Car_Type, Car_Colors etc and the Fact could be Car_Sales. Car_Sales will have keys from other tables to kind of connect them(join) and get meaningful info.",301555.0
91425,382277.0,No it's not wrong. The right option answer is available in the options.,310974.0
91425,382278.0,i cant see the question,318017.0
91425,382356.0,"You need to sum the house numbers, which are available in the Address field. Right answer is available in the options.",306725.0
91425,382522.0,"I have answered it, correct option is listed",314092.0
91425,382661.0,Correct option is there in the list. use Sum() function on house no.,311004.0
91425,382743.0,To get answer use sum(substring_index(......)),300690.0
91425,386134.0,I got the sum as one of the options listed . Please recheck,319759.0
91425,389015.0,Delete all the tables. and then again run the SQL script provided before the graded questions. Then it will show the correct answer.,304319.0
89461,371188.0,"I think, you have to use the same Company Schema which was shared in the pre-paratoory course. https://learn.upgrad.com/v/course/208/session/15786/segment/79808 Thanks",311502.0
92477,389530.0,"Hi Prashanth, Error code refers to problem with foriegn key constraints, please check if there is any issue to reference contraints with this command. Thanks.",305652.0
92477,389541.0,Not sure how to the problem with foreign key constraints. I remove the schema and added it again.,312376.0
92477,389579.0,Drop the constraints and try running this statement again.,317689.0
89520,371635.0,"The SQL graded questions are not on the company DB dataset. They just gave a relationship diagram between various tables and we need to answer from that. And, there is no change in the company db dataset.",310974.0
89520,377090.0,The Database is provided on the Graded Questions -II Segment. That should be providing you with the correct answers. Please let me know if you face any other problem.,304281.0
92195,388336.0,There is no issue with this. It works fine if the function is defined correctly. Cross check once again.,310974.0
92195,388360.0,you'll get the exact answer in the option.. just make sure that the ssn number of supervisor is passed correctly and also the subquery is returning the result as expected.. otherwise the code is not that complex and quite similar to previous question on average distance..,316349.0
92195,388857.0,"Just read the question carefully and implement it same way as it is mentioned in question, it will work. I was facing the same issue earlier.",314621.0
92195,389201.0,Sorry... Thanks for the help. I forget to update the discussion forum. I have found out the and able to fix it a few days back.,311115.0
91494,382788.0,"Hi Rajesh, There is a note on how to calculate the age, just above the question. Please find below screenshot of the same.",313826.0
91494,382815.0,you just need to take year difference. Don't take exact days difference. Taking exact difference might be creating an issue as the question asks sum to be calculated based on year difference only.,317689.0
91494,382837.0,this has already been asked before - you shouldn't use the datedif function; and if you do you need to round the answer in such a way that only the year of the dob is considered,300694.0
91494,388230.0,Here its geeting me the null values. correct me if I wrong. alter table employee add age int; update employee set age = 2018 - year(bdate); select sum(age) from employee;,308639.0
91494,384477.0,"I completely agree, the date provided 30 September 2018 really confusd the things. This information was not used in evaluating the solution. I also used the date in the same manner and was unable to get the right result even after two attempts. Such confusing information if not supposed to be used should not be provided.",311729.0
91494,385445.0,It is clearly mentioned in the question that USE ONLY BIRTHYEAR AND CURRENT YEAR TO GET THE AGE DIFFERENCE INSETAD OF USING ALL DATE MONTH AND YEAR.,317811.0
89627,372002.0,Which graded question section are you referrring to ?,318358.0
89627,373300.0,"If you are talking about the options, notice there is a slight difference in date and day. If you are talking about the result, then yes the default data consists of only month one day while you could insert more data to verify if your query is working fine which I don't think is necessary.",318329.0
91531,383022.0,"1.Try clearing temporary files go to RUN -&gt; type %temp% clear all the files the temp directory, there may be files which might not get deleted just leave it. restart the system and try installing again. 2.do the above step, re-download the package and try installing.",306735.0
91531,383074.0,May be your mysql installer file is not 100% downloaded,318429.0
91531,383833.0,I have downloaded it 4-5 times still not able to install it.,300719.0
93689,394876.0,"Since the super_ssn is NULL for ssn=888665555 , the person with this ssn is the CEO.",313826.0
93689,395444.0,use super_ssn is null to calculate distance from CEO's house. whether super_ssn is not null to calcuate average distance from CEO's house.,308634.0
93689,395039.0,you need create a UDF in such a way which takes input as ssn of CEO fpr which superssn is null and have a subquery which deducts the house distance of each employee from that of CEOs ssn and take average of it..,316349.0
92538,389828.0,"This is how you can use update command with substring of an column. alter table employee add column hno int; update employee set hno = substring(address, 1, 4);",310522.0
92538,389843.0,"Update syntax is UPDATE [LOW_PRIORITY] [IGNORE] table_reference SET assignment_list [WHERE where_condition ] [ORDER BY ...] [LIMIT row_count ] Now in assignment list, we should use col1=expression, col2=expression etc. Expressions can be written using literal values, column values, NULL , built-in functions, stored functions, user-defined functions, and operators. Hence you can use substring or any other function. UPDATE TABLENAME SET COLNAME_X = SUBSTRING(COLNAME_Y, START_POSITION, LENGTH)",318554.0
91546,383115.0,"Hi, In this all the employees have the same address , except the house nos are different.. here the question is trying to say that for example if a employee has a address ""2 Fondren, Houston, TX"" and the other employee has address ""38 Fondren, Houston, TX"", so in this case the distance between the two is 38-2=36 units, so its same for all. so we have to take the sum for all the house nos of all employees. Hope it makes sense now :) Happy learning :)",305129.0
91546,383599.0,"Hello Maya Question has sample house numbers mentioned not the original data. It will be confusing if you look for the sample data in the table. Here, house address is same for all the employees expect house numbers are different. We need to work with the data which shows the difference between house numbers. Hope this helps.",305652.0
91546,383707.0,Thank you all .my logic was same but i was using the companydb dataset and not the given dataset hence was getting the incorrect answer unfortunately.,300687.0
91929,388287.0,I would suggest create a new schema and run sql script provided against this schema so that the tables are created for this new database.,314730.0
91929,385481.0,"if you don't want to drop previous table, alternatively you can rename old employees table. then the new table creation won't be problem. syntax is alter table oldtablename rename to newtablename",318554.0
91929,385429.0,By dropping employee table in your database or execute in different schema.,319869.0
91929,385439.0,"Hello, It means that the table which you are trying to create is already there. In order to create again you need to drop this, in order to do so, you can either write command to drop the table. or directly go to the left of workbench where all database is shown and under that find your table and right click on the table name and click on drop the table.. after that you can run the command again.. let me know if this clearify your doubt..",305129.0
91929,385663.0,DROP TABLE EMPLOYEE THEN EXCUTE THE SQL FILE AGAIN OR EXCUTE THE SQL FILE UNER A NEWLY CREATED SCHEMA.,317811.0
91929,386112.0,"If you don't want to reference back to the tables and data you already have on the existing companydb and do a complete refresh of the data, one option is to drop the companydb and it will drop the db with all the tables in it. Then you can recreate the database schema and then run the sql given which will create the tables and insert the rows.",301654.0
91929,391282.0,You can just create a new schema and run this file on that schema. Create schema your_schema_name and then run this SQL on your newly created schema.,306247.0
93045,391804.0,Read the section before the graded questions. It talks about what you should consider as units.,310974.0
91007,379934.0,there is data I believe it also happens to be same as the first sql file - I got all correct answers based on that assumption,300694.0
91007,380063.0,"For me, I could answer all the SQL graded questions based on campanydb (2).sql file.",318329.0
91007,381206.0,"nce u open th efile,, u will see they have given the scriptof create tables and also added values to it. just copy paste them in connected sql and run the querry. the tables will be created",308495.0
91577,383487.0,Above query means employee whose birthday on January 1 comes first and the employee whose birthday on January 2 comes second.,310419.0
91577,383347.0,i believe Jan 1 employees should be displayed before Jan 2. i dont see any other interpretation of what is stated. how are you interpreting it as the opposite?,317998.0
91577,383401.0,"Hi, this question is simply asking that anyone born on January 1 should be listed first before the person born on Jan 2.",305129.0
91577,384142.0,"Hope the above confirmed answers ""answers"" your query",301555.0
91577,383743.0,"It says like this if there are four people along with their brithdate as below: A January 2 992 B Januaray 1 1992 C March 3 2008 D February 21, 2002 Result should be sorted in ascending order: A January 1 B Januaray 2 C February 21 D March 3",317811.0
91614,383539.0,What is the error that you are getting??,313826.0
91614,383593.0,"Try this substring_index(address,"" "",1)",305129.0
91614,383677.0,"Hello, Must be issue with substring syntax , please make sure that column name is not enclosed in quotations and delimter for space is "" "". Example: substring_index(address,"" "",1)",305652.0
91614,383746.0,"Two Errors in your query are:- The second parameter of substring function should be ' ' instead of () set `hno` , remove quotes from hno, it should be set hno=",317811.0
91614,384366.0,"Why do you require ""WHERE"" clause in this Query. Try without it.",311741.0
91963,386375.0,yes but ths answer is being calculated using 21....its also appearing as 21 in the create table section,310509.0
91963,386793.0,"It is 291. There is no 21. If you find somewhere mentioned as 21, by mistake, report it and we will fix it. As far as i remember, i did not find 21 anywhere",301555.0
91963,386506.0,"oh! this is a simple concept, your original table has address as 291.. and if you're extracting only numbers using substring then the result should be 291 only! try pasting the address in excel or somewhere, it should be 291 only..!",316349.0
91963,385749.0,i cannot see any house number as 21 but it is infact 291 only.!!,316349.0
91963,389757.0,I am getting 21,303082.0
91701,383969.0,"The day when 20DMA &gt; 50DMA, signal should be BUY, after that 20DMA will continue to be higher than 50DMA, but signal should be ""HOLD"" in that case. The day when 20DMA &lt; 50DMA, signal shoule be SELL, after that 20DMA will continue to be lower than 50DMA, but the signal should be ""HOLD"". Hope it is clear now.",318084.0
91700,383972.0,Please refer to the below link https://stackoverflow.com/questions/11448068/mysql-error-code-1175-during-update-in-mysql-workbench step by step instruction on how to change the setting. Hope this helps.,318084.0
91700,383981.0,"Safe mode in MqSQL is not allowing you to do this action. This was a trouble for me too. Simple, turn it off through; Use the path for turning it off (on the top menu, next to File) &gt;&gt; EDIT &gt;&gt; Preferences &gt;&gt; SQL Editor &gt;&gt; in others section, uncheck the 'Safe Updates' checkbox.. reconnect to your database and this should help in getting rid of this..",316349.0
91700,389012.0,Go to Edit-&gt;Preferences-&gt;SQL Editor. Uncheck the safe option and then reconnect to the database.,304319.0
92154,389754.0,SET SQL_SAFE_UPDATES = 0,303082.0
92154,387731.0,For removing this error follow below link may be this will help you: https://stackoverflow.com/questions/11448068/mysql-error-code-1175-during-update-in-mysql-workbench,310419.0
92154,387764.0,tHis was already answered.. either you permanently put the safe mode off or through query;,316349.0
92154,387901.0,SET SQL_SAFE_UPDATES = 0 ; Execute above command and then run your query. it will work.,317811.0
92154,389010.0,Even I got this error. Go to Edit-&gt;Preferences-&gt;SQL Editor. And uncheck the safe option. Reconnect to the database.,304319.0
92154,389103.0,"EIther "" SET SQL_SAFE_UPDATES = 0; "" or ""uncheckc the safe option in Edit --&gt; Preferences --&gt; SQL Editor"" works. when you apply ""in a table , Update a column/field by getting value of another field from same table "" we likely wont use proper WHERE CLAUSE then this error occurs though the QUERY satisifies or no ERRORs. after setting above change, the WHERE CLAUSE check is not considered by the query evaluator",318454.0
92154,391278.0,Just uncheck this and reconnect your db.,306247.0
92154,392008.0,SET SQL_SAFE_UPDATES = 0 or from Edit-&gt;preferences It is good idea to turn it back on though,311803.0
91751,384170.0,"Change The Below Descripted Query to Just SQL Command, You will get the result : Find AverageOf(AbsoluteOf(HNO Subtract (Find HNO of employee whose superssn is null))) of all the employees whose superssn is not null.",317811.0
91751,384217.0,I have just checked the employee table. It had house number of Jennifer Wallace as 291 instead of 21. That was the cause for the wrong output. Thank you Guys :),312507.0
91751,384098.0,"you should get the exact answer. i can read that you've taken absolute difference, but make sure you've taken the average of absolute difference. other thing make sure that you're passing the ssn of CEo whose superssn is null.. this gives the exact answer that is there in option.. try to check your nested subquery..",316349.0
91994,385952.0,"It can be done using a single query. Since this is a graded question, I'm not commenting any further",318084.0
91994,386065.0,"It can be done in single query, you can use window functions concepts and achive the result. Thanks.",311502.0
91673,383856.0,"Safe mode in MqSQL is not allowing you to do this action. This was a trouble for me too. Simple, turn it off through; Use the path for turning it off (on the top menu, next to File) &gt;&gt; EDIT &gt;&gt; Preferences &gt;&gt; SQL Editor &gt;&gt; in others section, uncheck the 'Safe Updates' checkbox.. reconnect to your database and this should help in getting rid of this..",316349.0
91673,383934.0,this has already been discussed before in the forum you can either do it via preferences as you mentioned OR you can update preferences via SQL I personally did it via Edit -&gt; Preferences,300694.0
92009,386050.0,"the column address is kept in single quotes. Please remove the quotes. Please use it as hno=substring_index(address,' ',1) instead of hno=substring_index('address',' ',1). Thanks.",311502.0
92009,386440.0,"Your like condition is not right. Please note there is space between Fondren and Houston. and they are separated by "",""",312746.0
92009,386844.0,Can you reply here with the statement you exeuting. The Screenshot is very tiny and may be minute errors miht get missed and we are finding it difficult to find the reason of your error,301555.0
92009,389942.0,"Try giving the where clause which is true for all the values in table... I've faced similar issue and solved it after using where clause try by using this at the end of code where dno in (1,4,5)",303673.0
91348,381919.0,two imp things which are to be careful about in this question; 1. Make sure you're taking the absolute difference 2. Make sure you're excluding the number itself,316349.0
91348,381837.0,Please check if you are excluding the house number of the employee (for which calculating the average for) from the calculation of average.,317689.0
91348,383211.0,"I encountered the same the problem, initially. But later when I evaluated my query,I found out the problem. 1) First simple query to find out just the supervisor 2) Use the query to exclude the supervisor from reference dataset 3) Find out requisite information and make sure that you use Absolute Values (in some cases the distance difference is negative)",319866.0
91228,381398.0,"Hi Rashmi, In SQL aggegate operations like SUM, AVG, MIN, MAX etc are performaned on all rows of data, you dont need to write loops. In this question, you need to first find the house number for ssn '123456789' and then the distance of this house from other houses in a column. Once you have the values in a column, you can use AVG function on the column to get the required value. HINT: Use subquery.",306725.0
91228,381546.0,"thanks u could do it. only mistake, i did was not using absolute function, as the difference in distance was negative also, hence the sum of all difference didnot match the option in the question.",308495.0
91235,381418.0,"If I remember correctly, all employees were born in Jan or Feb? and the qn said age as of Sept 2018? - so therefore I can't see how you can get different answers whether you use datediff or extract year -&gt; as long as you floor the resulting age instead of round. So if I am 23.7 years old ... my age is still 23, until I turn 24.",300694.0
91235,381547.0,"true, for indivdual age not true for sum of age as given in the quiz datediff() gives 4 years more over year() for simple reason it addups multiple such .7 to give over 1200 days equivalent to 4 years.",308495.0
91235,381603.0,"that's because datedif is not working on the AGE, it is working out the date difference .. this question asks to work on age. so if datedif is being used .. it is not even doing a sum of age .. : age date dif 35 35.7 23 23.9 total: 58 59.6 so as you can see sum of age is actually 58 but if you use date_dif then you will get either 59 or 60 -&gt; which is wrong so what I said is true for individual as well as group age - and this is further verified by noting which answer is correct in the quiz",300694.0
94291,397162.0,thank you...can we update and convert entire coloumn of the table...if it is...the same syntax applies..if not please let me knwo the syntax,320606.0
94291,397133.0,"STR_TO_DATE(DATE, FORMAT) FORMAT CAN BE '%d-%M-%Y' Pls modfiy format as per below: Format Description %a Abbreviated weekday name (Sun to Sat) %b Abbreviated month name (Jan to Dec) %c Numeric month name (0 to 12) %D Day of the month as a numeric value, followed by suffix (1st, 2nd, 3rd, ...) %d Day of the month as a numeric value (01 to 31) %e Day of the month as a numeric value (0 to 31) %f Microseconds (000000 to 999999) %H Hour (00 to 23) %h Hour (00 to 12) %I Hour (00 to 12) %i Minutes (00 to 59) %j Day of the year (001 to 366) %k Hour (0 to 23) %l Hour (1 to 12) %M Month name in full (January to December) %m Month name as a numeric value (00 to 12) %p AM or PM %r Time in 12 hour AM or PM format (hh:mm:ss AM/PM) %S Seconds (00 to 59) %s Seconds (00 to 59) %T Time in 24 hour format (hh:mm:ss) %U Week where Sunday is the first day of the week (00 to 53) %u Week where Monday is the first day of the week (00 to 53) %V Week where Sunday is the first day of the week (01 to 53). Used with %X %v Week where Monday is the first day of the week (01 to 53). Used with %X %W Weekday name in full (Sunday to Saturday) %w Day of the week where Sunday=0 and Saturday=6 %X Year for the week where Sunday is the first day of the week. Used with %V %x Year for the week where Monday is the first day of the week. Used with %V %Y Year as a numeric, 4-digit value %y Year as a numeric, 2-digit value",317811.0
93329,392800.0,"Nope, you cant use it in windowing fucntion. You can use it in proc's, SQL's, CASE, IF etc.",306735.0
93329,392774.0,Interesting question. But I think it is not possible. Pls see the link pasted below : http://www.mysqltutorial.org/mysql-window-functions/ It gives complete list of such functions which can be used as Window function. It doesn't mention anywhere that UDF can also be used.,311686.0
92518,389726.0,Please mention the steps you followed.,304813.0
92518,389847.0,"It is possible your data is not committed. Please try to commit the data. Create Alter Modify the se commands are autocommited. But UPDATE is not autocommit by default, unless you enable it. So try to enable autocommit or use commit command explicitly. Believe that will resolve the issue.",318554.0
92518,389787.0,"The table's last row is null because of the following reason. Usually, when viewing the table data in something like Workbench or SQLYog, it will display the rows of data, plus a blank row at the end that is used for adding new data.",310522.0
92518,391044.0,"Hey Venkatesh, I ran your code. It worked fine and I did not have any null values displayed in the name column. As Indranil mentioned, your data might not be committed. Please check that.",319721.0
93453,393480.0,"Hello Sheith, There is no pass key for database file in Graded Questions II for SQL.",320195.0
93453,393614.0,No such pass key required for database file in Graded Questions II for SQL,307015.0
93453,393551.0,never came across any such thing while I was doing the graded questions ii for sql,300694.0
93423,393311.0,I hope you already have hno asked in question no 2. to calculate the avg distance apply avg function on difference of hno and (hno of emp whose ssn is '123456789'). donot apply avg function on the emp whose ssn is '123456789' .,312746.0
93423,393312.0,You need to remove the house number of the given ssn n then find the average of all other ssns. hope this helps,307710.0
93423,393567.0,you take the numbers in front of each address and also remember that distance does not have negative or positive polarity - it is always positive. So make sure you take this into consideration.,300694.0
92413,389211.0,"not sure if any, there will be I've come across some date related differences since i was playing more with it.. but here you can simply subtract the intended column/rows in select clause through manually specifying them.. (A-B) as difference",316349.0
92413,389257.0,"No. Theres none. For most of the arithmetic operations, the operands '+,-,*,/' work. Summation is a function available because finding sum of a list of values is a fairly common activity in data retrieval process. However, we dont usually want to find the difference of a list of values.. Hope this values.",310511.0
92574,389954.0,"Hi Venkatesh, A self-join is a query in which a table is joined (compared) to itself. Self-joins are used to compare values in a column with other values in the same column in the same table. To write the query, select from the same table listed twice with different aliases, set up the comparison, and eliminate cases where a particular value would be equal to itself. Example : https://www.w3schools.com/sql/sql_join_self.asp https://www.geeksforgeeks.org/sql-join-cartesian-join-self-join/",320195.0
92574,390018.0,You can go through below link to get detailed explanation in Self Join. http://www.mysqltutorial.org/mysql-self-join/ https://stackoverflow.com/questions/16701920/mysql-how-to-use-self-join Hope this will help.,317991.0
92574,390010.0,"Self join is join when you use same table with different alias. Ex select a.name, b.age From tbl a, tbl b Where a. id = b.id",315679.0
92574,390357.0,"Hi Venkatesh, For self join you can consider the most basic example like an employee table consisting the details about the employee in a single table. In order to identify the managers of the employees we use the concept of self join as the manager of the company is also the employee of the oraganization. Use the below link for a better understanding : https://blog.sqlauthority.com/2016/03/27/self-join-explain-example-interview-question-week-064/",300688.0
92574,390023.0,"When you join a table to itself on columns with common values, you can picture how each record is related to one another. This is known as self-join. Self-join is normally used to represent hierarchical relationship or tree structure in a table. Syntax : SELECT column1, column2,... FROM table AS A (LEFT | INNER) JOIN table AS B ON join_condition",319006.0
92412,389206.0,Found the mistake. Thank you!,310522.0
92412,389208.0,"There is no column named hno provided in the sql script. First you need to create this column based on address of the employee. Create a new column. You can extract house number from address column using following: LEFT(address,LOCATE(' ',address) - 1) --This function will take all the charecters before first space occurence. So it will return a string. Covert it into int. Assign this values from above function to the new column.",317689.0
92412,389205.0,sql is not able to understand the hno (first) column to be referred from which table.. at the end make sure you're adding the from clause; select avg......))) from employee;,316349.0
92412,389454.0,Did you add hno in table. ALTER TABLE TABLENAME ADD HNO datatype,317811.0
92412,389456.0,Query is wrong. It should be select avg(abs( hno -(select hno from employee where super_ssn IS NULL ))) for all employees where super_ssn IS NULL Bold part is missing,317811.0
92412,389752.0,There is no column named hno provided in the sql script. First you need to create this column based on address of the employee. alter table table_name add hno datatype,303082.0
92906,391185.0,This worked for me---- DELIMITER $$ CREATE FUNCTION return_signal (d DATE) &lt;function body&gt; RETURN &lt;variable&gt;; END$$ omit DELIMETER; at the end.,301652.0
92906,391223.0,There should be a space between delimiter and ; . like delimiter ; instead of delimiter;,311502.0
92906,391300.0,DELIMITER; should be DELIMITER ; If you are using $$ then DELIMITER $$ Hope this helps,317149.0
92908,391252.0,You can go through below link to get detailed explanation in Self Join. http://www.mysqltutorial.org/mysql-self-join/ https://stackoverflow.com/questions/16701920/mysql-how-to-use-self-join Hope this will help.,317991.0
92908,391210.0,"You're expected to join the employee table with itself to find out the Supervisor name and birthdate (fname, bdate corresponding to Superssn) for which ssn of employee is 666884444.. you just have to join employee with itself and extract the right columns in select clause to get the ans.. (anyways getting the answer is easy, look at the complete data and it makes clear what should be the ans)",316349.0
92908,392094.0,"this question is more of a puzzle than coding, i mean just look at the question and find a relationship between ssn and superssn of employees. no need of writing a code , you will find an answer.However , you can perform an inner join function by just assuming the table employee is divided in two parts , having said that only self join method can teach how to partition and assume a single table in two partss.hope this would help.",315560.0
92750,390646.0,"The answer is available in the options. Take a break and try again after some time, you will get it :)",310974.0
92750,390706.0,Check the explanation of calculation here https://learn.upgrad.com/v/course/208/question/92449,317689.0
92750,390992.0,"This question is similar to the 2nd question in that section. Just the difference is you need to add ""Department"" as a condition in the where clause. Try and i am sure you will figure it out.",316202.0
92809,390864.0,After toggling this error should resolve. Otherwise try to restart your sql server.,317689.0
92809,390874.0,You can go through below link for solution of the error mentioned by you https://stackoverflow.com/questions/11448068/mysql-error-code-1175-during-update-in-mysql-workbench Hope this will help.,317991.0
92809,390903.0,"update employee set hno = substring_index(address, ' ', 1) Before above syntax write and the below statement and again run your update employee query set sql_safe_updates=0;",310419.0
92809,390922.0,you need to reconnect to Database after applying changes,318804.0
92809,391058.0,first excute below query set sql_safe_updates=0; Then you can do any dml commands successfully.,317811.0
92809,391419.0,"Can be solved in two ways Either, execute the command set sql_safe_updates=0; Or, Go to Edit --&gt; Preferences Click ""SQL Editor"" tab and uncheck ""Safe Updates"" check box Query --&gt; Reconnect to Server // logout and then login Now execute your SQL query. Cheers!",318080.0
92809,391808.0,simple trick Set SQL_Safe_Updates=0; your problem will be solved,312892.0
93189,392317.0,Window is used to extract an aggregate function (arithmetic or non-arithmetic). Salary/age is not an aggregate function. Its value for a single row has no dependence on its value for the second row.,304281.0
93189,392252.0,"select fname,salary,age,dno, sum(salary) over() as totalsalary, salary/age over() as rankwise from employee order by rankwise desc; what is worng in this code",315560.0
93189,392453.0,"The ""salary/age over() as rankwise"" is not having any aggregate function, scondly the sum(salary) over () would provide Sum of Salary for all the rows.The requirement is to get salary/age and rank rows as per value you get from salary/age calculation.Hope this helps!!",303240.0
93368,392948.0,"Try this:- select fname , ssn , salary ,age , rank() over (order by abs(salray/age)) as 'ranking' from employee ; Changes I made:- 1. removed comma after alias 'ranking' 2. added comma after column age 3. added semi-colon after table-name employee;",301652.0
93368,392954.0,"Try this:- select fname , ssn , salary ,age, rank() over (order by abs(salary/age)) as 'ranking' from employee; Changes I made:- 1. Removed comma after alias 'ranking' 2. Added comma after column age 3. Added semi-colon after table employee; 4. Changed salray to salary in window order by ...So, your logic was correct, only some syntactical errors were made.",301652.0
93415,393191.0,"There is two operation happening here : 1) First part :select the hno for the employee where ssn ='123456789' 2) Second part :calaculating the avg distance of employee where ssn != '123456789' ,meaning selecting all the employee other than ssn ='123456789' Hope this helps.",318476.0
93415,394240.0,The query is running perfectly fine in my system and providing the correct answer. Please check your schema and make sure all the relevant rows are populated.,304281.0
93413,393196.0,"In the above sql code three things are happening: 1) Add a new column name hno to the employee table : alter table employee add hno int; update employee 2) updating the hno column value set hno = substring_index(address, ' ', 1); 3) selecting name and hno of the employee by ranking over the hno",318476.0
93413,393314.0,To answer your question of any easier method of writing this.. No. you got to do all these 3 steps one by one,307710.0
93434,393306.0,I am getting 483 and this option is not in answer list,302735.0
93434,393337.0,What is the error saying? Is it syntax error ? Maybe change age to float data type instead of int or check the open/close braces in the set age code.,301644.0
93434,393348.0,Taking the Year value from date value will give you more accurate result. Example : set age = 2018 - year(yourdate);,318476.0
93434,393565.0,you are not clear with what is the problem? Is it a syntax error that youa re getting or are you asking about a logical error? if it is logical error then it is beause you are using datediff incorrectly - you are adding up fractions of ages as well whereas age doesn't have a fractional element (when some asks you your age you dont say 'my age is 23.25 years old' ; you say 'my age is 23'),300694.0
93434,393680.0,Use DIV instead of '/' and 365 days instead of 365.5,319384.0
93434,393726.0,"Age is int and the result of set age = datediff('2018-09-30', employee.bdate) / 365.5 wll be floating type Please change the datatype from int to float",317811.0
93577,394174.0,"Hello Bhagyashree, You are doing something wrong while calculating 50_MA.",320195.0
93577,394281.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date. window function should cover ROWS BETWEEN 19 PRECEDING AND CURRENT ROW ROWS BETWEEN 49 PRECEDING AND CURRENT ROW,317811.0
83382,342423.0,Thanks!,310585.0
83382,342414.0,"There are two ways- 1-By default, Excel searches the formulas instead of the values. If you tried Find without changing Formulas to Values. you might get this issue. Jst change ""Look in""options to values from formula.Please find the attached screen-shot. OR 2- Dont use ""Accounting"" option format for those cells which are having this issue. use ""General"" instead.",312746.0
82117,335500.0,I believe you will have +ve values for profit &amp; -ve value for loss. Add a conditional formatting for +ve values with 'green' &amp; then another conditional formatting for -ve values with 'red'.,301276.0
82117,335417.0,I believe you can do this by using Conditional Formatting -&gt; New Rule. option and make your required rule to visualize the data as per expectation. You can use Cell colour option to highlight your cell in respective colours.,301648.0
82117,335456.0,This is the assignment in excel in first part...,308437.0
82117,335527.0,This is what I did:,310974.0
82117,336036.0,In conditional formatting 'color scale' option can be used to do that.,300690.0
82117,336389.0,"if you have a range or something like between or less thanor so values as profit then you can use the cell value contains option in conditional formatting for eg:100 and above as profit you can select call value equal and greater=100 and format the colour to green add new rule selecting manage rules and repeat the above and select less than 100 as red cell value contains has lot of optionseg:between,not between,equal to,so on and so forth",300687.0
82117,336393.0,select 'color scale' conditional formatting select format all cells based on values format style select 2 color style select the colour minimum as red and maximum as green and type as number option,300687.0
82117,336995.0,you can also use calculated field also define a scale fo profit for profit and then se the colour with Marks card for that colour field. Hope it helps!,304813.0
82117,354388.0,"I do not think it is possible to have two graded/shading rules. You can have multiple rules for solid colors but for graded/shading .. I don't think so (as in two rules as per 1. of the qn). The first rule will always kick in and take care of the shading .. the second rule will never be evaluated. On the other hand if the question is asking for a gradient that is static then it can be done with a solid fill using 'fill effects' where you can choose two shades of green for one rule and two shades of red for another rule. Then all values &lt;30,000 will have the SAME green gradient and all value &gt;30,000 will have the SAME red gradient. The shade of red or green will not change depending on value.",300694.0
82117,358013.0,"Hi All, For the second task :- 1)Fill the column in red and green so that the values above a certain number (say 3000) are highlighted in a gradient of red and the ones below the number are highlighted in a gradient of green. It is possible .Say for example from the dataset we have i take the Average Medical Payments one. Just select the column-&gt;Color Scales-&gt;More Rules.In the style drop down select Classic.Select Format only cells that contain cell value greater than.Then Format with Custom Format. Then you will see four tabs.Select Fill tab and choose the red color. Repeat the steps for value less than 3000. Then you are good to go. @ Madhusudhan please confirm whether I got your doubt correctly or I am missing something. Thanks and Regards Somnath",314617.0
82117,359903.0,"Hi Madhusudhan, To do the First one Question:::: 1. Fill the column in any one color (red or green) so that the highest values have the darker shade of the color and the lowest values have the lightest shades of the same color. Sol:: Fill the column with says Green, So that Value ∝ Shade of Green . i.e Higher the value More Green it is Choose the Color Scales from the Data Bar and select the Gradient. You will See the higher values will be dark Green. Check Image below. For Second Ques : Fill the column in red and green so that the values above a certain number (say 3000) are highlighted in a gradient of red and the ones below the number are highlighted in a gradient of green. Soln:: Now, the Same Column Should have 2 different Color (Red and Green) based on 3000 value. IN GREEN &lt;3000&lt; IN RED It Means 3000 is the Mid-Point of the Two Colors. So let's choose the mid value to be 3000. So Choosing 3-Color Scale. Green --- &gt; White ---- &gt; Red. Even you can choose 2-Color Scale. If you want to avoid White for exact 3000. Find below image.",315028.0
82152,335718.0,You can go to the below link and explore. https://www.deskbright.com/excel/excel-resources/,301648.0
82152,341457.0,"Hi Amit, You can check out following important links for learning Excel.. Link For Learning Excel Features www.contextures.com Link For Learning Excel Advanced Formulas www.excelexperts.com Link For Learning in YouTube ExcelIsFun Hope this helps..",310508.0
86347,355794.0,"Yes, we can select Highlight Cells Rules -&gt; Greater Than... under Conditional formatting on the Home tab of the ribbon and enter 3000 in the first field, select custom format and use ""Fill effects"" under Fill tab to choose Red gradient. In a similar manner, we select ""Less Than..."" option under conditional formatting, enter the value 3000 in the first field, select custom format to chose Green gradient. Hope this helps answer your query.",314730.0
86347,358517.0,"yes, this is possible when you explore excel for conditional formatting, as explained above.",317418.0
86347,359858.0,"To do the First one Question:::: 1. Fill the column in any one color (red or green) so that the highest values have the darker shade of the color and the lowest values have the lightest shades of the same color. Sol:: Fill the column with says Green, So that Value ∝ Shade of Green . i.e Higher the value More Green it is Choose the Color Scales from the Data Bar and select the Gradient. You will See the higher values will be dark Green. Check Image below. For Second Ques : Fill the column in red and green so that the values above a certain number (say 3000) are highlighted in a gradient of red and the ones below the number are highlighted in a gradient of green. Soln:: Now, the Same Column Should have 2 different Color (Red and Green) based on 3000 value. IN GREEN &lt;3000&lt; IN RED It Means 3000 is the Mid-Point of the Two Colors. So let's choose the mid value to be 3000. So Choosing 3-Color Scale. Green --- &gt; White ---- &gt; Red. Even you can choose 2-Color Scale. If you want to avoid White for exact 3000. Find below image.",315028.0
83410,342591.0,"Its because of huge difference between your min and max value of that column. (min and max is calculated automatically which you need to change (using formaule) to get better view of data bars. Try below steps :- Select the cells that contain the data bars. On the Ribbon, click the Home tab In the Styles group, click Conditional Formating, and then click Manage Rules. In the list of rules, click your Data Bar rule, then click the Edit Rule button In the ""Edit the Rule Description"" section, the default settings are shown for Minimum and Maximum Click the arrow for Type, under Minimum, to see the other options. use formual and give your formula to calculate min value of that column.",312746.0
82535,338638.0,I had used the color scale from conditionnal formatting.it does work but better option may be the using formula option in type and putting the condition the cell value&lt;0 as red and greater=0 as green would be better but its not working for me,300687.0
82535,338815.0,Its a two step process- 1- Use conditional formattion on the cells which has -ve values (make more loss to red and less loss to green) 2-Use conditional formattion on the cells which has +ve values(make more profit to green and less profit to red),312746.0
82535,338823.0,"Hi Alok, If you don't mind can you please attach screenshots for me to get more clarity.",311160.0
82535,338921.0,,310974.0
82535,339138.0,You can use the Format -&gt;Colour conditional formatting,311004.0
82535,338856.0,you can use below solution:- This queation is quite tricky.. :),312746.0
82535,342078.0,Hi Alok.i have attached the screen shot .can you please advise thanks why the negative values are showing as green .where am i going wrong ?,300687.0
82535,348079.0,"I have used a 3 color graded scale with percentile as the ""Type"" with 10th percentile as red, 50th percentile as white and the 95th percentile as green. Looks ok. That is the only rule that I have applied.",313826.0
83381,342385.0,,310585.0
83381,342427.0,Please share the error you are seeing. I believe what you would like to achieve is find and replace '$',309451.0
83381,342664.0,"Hi Naga Sai, This $ symbol is appearing because the number type is changed to currency. If you want to remove dollar sign, you need to change the number type from the options shown in pic: Instead of ""Accounting"" you can select ""General"".",301559.0
83381,346658.0,"I agree with Ashish . As numbr type is currency, thus $ can't be removed. first need to change type to general , then $ symbol will not be part of coulmn.",311404.0
83381,344295.0,"Hi Naga Sai, Try formatting that cell by pressing Ctrl +1 and selecting symbol as none in currency.. Hope it helps.",301618.0
86041,352525.0,"I have Microsoft Office Professional Plus 2010 installed in my machine and was able to follow all the instructions provided in the course. So, I think you should be fine with using MS Excel 2010. Hope this helps.",313826.0
86041,352544.0,Yes that will perfectly work well,312357.0
86041,352859.0,"Yes, you can use Microsoft 2010 version of Office suite. Few differences you will see in the shortcut keys. For example, typing Alt + I and Alt + D in latest versions like Office 365 will show a message ""Continue typing the menu key sequence from an earlier version of office or press ESC to cancel."" Which means some of the shortcut keys would have changed since Office 2010 and I believe more features and additonal excel functions would have been added in the newer versions of Office.",314730.0
86041,352992.0,You can absolutely use Excel 2010 for data analysis. The higher version will have additional features that you might not use for now. Below is the link where you can find the difference between 2013 and 2010. As long as you don't need those extra features you are good to go with 2010. https://www.stl-training.co.uk/versions/office-2013-difference.php,317845.0
87471,361561.0,Goto conditional formatting from Home menu. Select color scales-&gt; more rules. Now you can see no of options to select color and set limit. Enter Number as type and enter your value and select desired color.,318332.0
87471,361594.0,"One way of doing it is to create 2 rules - one for value &lt;= 3000 and another for value &gt; 3000. Conditional Formatting -&gt; High light Cell Rules -&gt; greater than -&gt; enter the value as 3000 and select custom format. Here the fill effects can be choosen as gradient and color green. So this will highlight all the cells with value greater than 3000 with gradient green. This can also be done from more rules -&gt; Format only cells that contain. The condition and formatting can be set here. Similarly, for values less than or equal to 3000, we can set another rule that highlights the cells in gradient red. If you get the blanks cell formatted and want to get rid of it, another rule for cells with blanks and no format will do it( In manage rules, select Stop If True for this rule) . Hope this helps. """,301654.0
87471,364084.0,"Yes , it is possible to do the second task by using 3-Color Scale with following settings: Minimum with color 'Green' Midpoint with Value (say 3000) and color 'White' Maximum with color 'Red' NOTE : The question asks gradient of green and red, and hence the gradient must be spread across all the cells based on their values and not that each cell have the same gradient colouring based on the condition that they are greater than or less than 3000. Thus the value 3,500 and 3,600 should have different shaded of color 'Red' with 3,500 being a lighter shade of 'Red' than that of 3.600 and so on.",317987.0
85740,352405.0,"When you select a cell and click on ""Freeze Panes"" option : All the rows to the top of the selected cell are frozen All the columns to the left of the selected cell are frozen In the above case, you should select cell ""C2"" to freeze first two rows and columns.",313826.0
85740,352148.0,"In order to freez first two column and top row. in Excel sheet go to the cell C2 and view -&gt; freez panes. Also, refer to this link: https://www.excel-easy.com/examples/freeze-panes.html",317845.0
85742,352523.0,"Please see the attached image. I can see an option to change the ""Fiil"" type under which I can choose ""Solid Fill"". PS: I have Microsoft Office Professional Plus 2010 installed in my machine.",313826.0
86201,354456.0,Yes you can filter by using filter by color which helps you to display only coloumns/rows in color.,303673.0
86201,354087.0,You can filter the cells by color and get the count of the cells with that filtered color in the bottom left corner of the sheet as shown in the screenshot below.,314730.0
86201,356108.0,"Excel has no way of gathering that attribute with its built-in functions. If you're willing to use some VB, all your color-related questions are answered here: http://www.cpearson.com/excel/colors.aspx",319721.0
86300,354937.0,Below is the answer posted in Quora.It contains a link for donwloading excel. https://www.quora.com/How-do-you-download-Excel-onto-a-Mac You can get a 1 month trial period .Create an account in Microsoft and you are good to go. Thanks and Regards Somnath,314617.0
86300,356534.0,You can refer the official documentation by microsoft for installing it on mac. Here is the link: https://support.office.com/en-us/article/download-and-install-or-reinstall-office-365-or-office-2016-on-a-pc-or-mac-4414eaaf-0478-48be-9c42-23adc4716658#InstallSteps=Install_on_a_Mac,319721.0
87782,364705.0,"Hey Aman, If you have gone through the segment on conditional formatting, there they used various methods to sort and apply filters on desired values. https://learn.upgrad.com/v/course/208/session/15791/segment/79837 Please go through this module and it'll help you learn about ways of conditional formatting. And if you have any specific questions while applying these ways to the given question, you can get back to us.",319721.0
88142,365118.0,Can you share the snapshot of your selection on the sheet and how the icons look like?,319302.0
88142,365212.0,Is this only for this file or does this happen for any file?,319721.0
88142,365907.0,any excel file,312892.0
88142,366080.0,thanks everyone for your help... MSOffice had to be repaired using the online repair function and it working now,312892.0
95872,409164.0,Thank you for sharing!,319721.0
82270,337771.0,"Yes, it seems there is some noise, and I have mentioned this in the feedback. Although, I was still able to listen to what Anand Sir was trying to say.",301652.0
87109,358787.0,"I understand there is a shortcoming in Mac with the lack of Alt key as opposed to Windows machines where it is a booster and great time saver with efficient shortcut keys. Nevertheless, there are alternatives, check the below links. The main discussion forum from Apple site itself - https://discussions.apple.com/thread/3001929 This link has some accelerator keys that are equivalent to some of the windows Alt key combinations - https://answers.microsoft.com/en-us/mac/forum/macoffice2011-macexcel/lets-talk-about-keyboard-shortcuts-and-menu/4cf2fc34-89c7-4542-aaca-0e80b508e8ef Check the below site if you are ready to spend some money, there is an excel skin which you can stick to your Mac keyboard to explore all the shortcut keys that are equivalent to Windows shortcut keys - https://www.excelskin.com/",314730.0
80126,325207.0,I am assuming Y2 contains 1 So above formula will average out all values of W col. where corresponding X col value is 1. Hope this helps. If not you can share the simple sheet. Thank you!,308439.0
80126,326921.0,"Well No, Column W contain values of 1 and 0's, Column X is the range where as Column Y is a subset of X with only unique values from X and is the matching condition(married-secondary) in this case, but I am still getting 1 as an average. Here is the image from the excel sheet, Its the same test sheet provided in the lecture notes by Anand. Also, here is the formula.",300734.0
80126,327867.0,"I have resolved the Issue, Thanks for the time and effort.",300734.0
76876,307058.0,"Hi Surya, I didn't understand your question completely. Anyways, we have options like ifelse or nested 'if's i.e if inside another if etc to add more conditions/checks. Thus we can combine multiple if, and, or etc in a formula to accomodate all our conditional checks. I hope this answers your question.",301276.0
76876,357940.0,"we can use if logical function if (or (age&gt;60,age&lt;20),""perform a"",""else perform b"")",305129.0
87234,359631.0,You can't do this on a normal excel sheet because it has a hard-coded row limit of 1.048 million rows. However you can use something called a PowerPivot which is available as an add-on in excel. I have only limited exposure to this but it seemed very similar to using a DBMS like Access. Lets see if any of our peers have used PowerPivot. :),319898.0
83571,343674.0,Since there are two correct answers to the question asked. You are supposed to check 2 options to get it correct,310585.0
83571,343675.0,"screenshot clearly shows what are the right answers. It says ""you missed it"" (@first option)",310585.0
83571,343458.0,the question says to opt the the possible functions so in this scenario you need to check both those options whenever you have radio buttion the round one the answer would be one but in case of these check boxes it can be more than one depending the question,300687.0
83571,343433.0,"Please Read the question properly see here question is saying mark all that apply.It will have answer both a and d. Let see how!! 1. if(response = ""Y"",1,0)# it means if the response column value is Y then return it 1 else change with 0. 2. if(respons = ""N"",0,1) # It means the if response column value is N, then return 0 else change with 1. Therfore the jinx from 1 and 2 both is same as in short Y implies 1 and No as 0 when true. else do the vice versa.",307843.0
88278,365267.0,"Click the edge of chart's frame to highlight the chart . On the Home tab, in the Editing group, click Clear &gt; Clear All. Tip: For faster results, you can also press Delete on your keyboard.",319721.0
83172,341536.0,"1- You might have 1 blank row in that sheet. That's why it is not picking up the entire column. 2- To overcome this situation, there are two solution: a- Fill any value in any adjcent column (of vacant row) of that column for which you want to apply autofill. and they double click the Autofill square. OR b- Select all remaining cells of the column and use paste-special option.",312746.0
85619,351454.0,"After a few investigation this was an easier approach i figured. Try the below shortcuts in the sequential manner(let say for Inseting a row; ALT key followed by H then I and R, please make sure you hold on to the ALT key until you reach the end(in this case R) Insertion 1. Insert a row: ALT+H+I+R 2. Insert a column: ALT+H+I+C Deletion 1. Delete a row: ALT+H+D+R 2. Delete a column: ALT+H+D+C Hope this helps or let me know if there are any easier approaches.",312259.0
85619,357937.0,"first select a row above which you want to add a row then follow below steps- Shortcut to select a row- Shift+ space then to add- Control+SHift+ ""+"" and to delete any row select a row using Shift+space then to delete- control+shift+""-""",305129.0
85619,351834.0,"You can try these shourcuts: To insert a new row on the left of a selected cell: Ctrl + "" + "" followed by pressing "" r "" and Enter . To insert a new column on the top of a selected cell: Ctrl + "" + "" followed by pressing "" c "" and Enter . To delete a row on the left of a selected cell: Ctrl + "" - "" followed by pressing "" r "" and Enter . To delete a column on the top of a selected cell: Ctrl + "" - "" followed by pressing "" c "" and Enter .",313826.0
86130,353145.0,I wanted to practice it manually.,318344.0
86130,353464.0,"You could try these methods: 1. Open the csv file in notepad/notepad++. Copy all the content and then paste it ina a excel sheet. Then use ""Text to Columns"" feature to convert. 2. Open a blank worksheet. Click on ""Data --&gt; From Text"". Choose the csv file form its location. This would open up a window where you can convert the imported text.",313826.0
80603,357943.0,"no, there is no need to understand it compuslory- its just to reduce any manual task. or alternate to recording is VBA. also can go through the link below to see the basics of recording macro and understanding it- https://www.teachucomp.com/record-a-macro-in-excel-instructions/",305129.0
80603,328518.0,"Hi Prashant, Its always good to have knowledge of macros. If you are proficient with macros &amp; VBA, you'll be able to automate models and improve overall efficiency for repetitive tasks. Hope this helps.",301618.0
82175,337029.0,Create a pivot table created w.r.t to product subcategory and region. Then sort the profit &amp; you will get the top three values.,301276.0
82175,340047.0,i solved it using pivot table. Getting total profit by each product sub category and by region. and then applying sorting on values region wise. Any thoughts if my ans is wrong?,312746.0
82175,350815.0,"Took a little different approach here. Found out the unique regions and pasted them as transpose. Here's how it looks. If you want to work without pivot tables, maybe tryout SUMIFS or AVERAGEIFS maybe",318381.0
82175,350838.0,"I have taken the following approach: 1. Concatenated the ""Region"" and ""Product Sub-Category"" to get ""Region_Product Sub-Category"" field. 2. SUMIF() on Profit and Sales fields based on ""Region_Product Sub-Category"" --&gt; This would give us the ""Sum_Profit"" and ""Sum_Sales"" fields for each product sub-category for each region. 3. Created a new field ""Profit Margin"" by dividing ""Sum_Profit"" by ""Sum_Sales"". Represent this in percentage format. 4. Custom Sort for each region for ""Top 3"" based on ""Profit Margin"". Used the same data to get the Pivot Tables. First Pivot Table for highlighting the ""Top 3"" product sub-cat in each reagion. And the second, to give an overall picture of profitablity. Please find attached the screenshots. Please let me know your thoughts.",313826.0
82637,338799.0,Check in the options ribbon hope it works,300687.0
82637,339268.0,try this hope it works goto file menu and options then to customise ribbon select main tab filter then slicer slect thenew group tab and add option,300687.0
82723,339964.0,"Yes, you can record in the form of macros for this we want Developer tab on the ribbon most don't get as default. For that goto files-&gt;Optionss-&gt;Customize Ribbon and then tick on developer tab and save it. Then Click on Developer tab -&gt; used relative references -&gt; record macro. You can place a shortcut for your remembrance and then your recording will be started for the perfect visualization understanding I suggest a video https://www.youtube.com/watch?v=ltcpaHdXUrU",304692.0
83469,342931.0,"This is what I did, let me know if you need explanation after viewing this",310974.0
83469,350839.0,Answered a similar question. Please check the link below. https://learn.upgrad.com/v/course/208/question/82175,313826.0
86283,356243.0,Check this:,310974.0
86283,357809.0,How do we derive chairs and tables are brought together or any two sub categories are brought together from this data?,305842.0
86283,367655.0,"If you do this analysis for actual value of profit insead of profit margin, you will find quite different results. In that analysis, Tables generate least profit while chairs have high profits. Then the approach is to not just look at a single category and form hypotheses as this could result in wrong insights. ex: For some regions chairs provide more profits than tables have losses. So stopping tables as product for such regions has to be carefully examined on overall company level. Would it impact chairs sale? if yes, how much. Does it make business sense in the case these are always bought together? For this you have to form groups of category and then look at the data.",317995.0
120683,524554.0,I don't see any harm if it's commited as private kernel.,318329.0
119216,,nan,
120782,524176.0,can you have recall values? I am only calulating Accuracy value (this is also only what the Kaggle page asks for - Accuracy),300694.0
120782,524545.0,You can use metrics.classification_report to get the detailed metrics per label and the average as well. I did not get the same values but they are somewhat closer.,318329.0
120681,,nan,
119669,519514.0,"Since charcter or digit recognition highly non-linear, hyperparameter C alone is not sufficient and it should be accompanied by gamma too. After the usual EDA etc, you must use RBF kernel since it is highly non-linear. Using cross validation, preferably 5, optimal values of C and gamma shall be found using GridSearchCV. This will do.",301121.0
119669,519513.0,"Hi, We'll divide the analysis into the following parts: Data understanding and cleaning Data preparation for model building Building an SVM model - hyperparameter tuning, model evaluation etc (Use Grid Search Cross-Validation)",344894.0
119669,521024.0,"You can check the accuracy on all 3 types of kernel - linear, poly and rbf. Whichever gives the higher accuracy, you can tune both the hyperparameters - C and gamma using GridSearch CV on that particular kernel.",304319.0
120668,523594.0,Extract only 10 to 20% of total input data - which comes to 4200 to 8400 rows. And then split traing and test on the extracted data.,319759.0
120668,523461.0,"1. Sub-sample 10-20% data from the training.csv file. 2. Split the sub-sampled dataset into train and test datasets in some ratio (say 80:20 , 70:30 , etc). 3. Build model on the sub-sampled train dataset by trying out various kernels and hyperparameters. 4. Once the final model is generated, predict outcome for the sub-sampled test dataset to see that the accuracy is as per the expectation set for the assignment. 5. Optional Step : Using the final model, predict outcome for the test.csv dataset. Save the index and the predicted value to a csv file. Submit the csv file on Kaggle which in-turn will generate accuracy for your predictions by comparing with the actual values. Check that the accuracy provided by Kaggle is in line with teh accuracy that was obtained for the sub-sampled test dataset. Hope this helps.",313826.0
120650,523074.0,You can train 20% of the data from train.csv and predict the labels for the 28k records present in the test.csv. you can ignore the other 80% of the data from train.csv,311160.0
120650,524882.0,"Create an extra column in excel using RANDBETWEEN(1,n) split data into 10 groups or any groups Try to run model train separetly for atleast of 3 groups and check wheather the model is accurate or not. You can drop the column post loading data into data frame If you need very specific and dynamic grouping - like lets say groups on 0-9 labels please view below url https://exceljet.net/formula/randomly-assign-people-to-groups",309212.0
120650,523097.0,You can go with any of the above approaches you have suggested. But final test score would based on test.csv by submitting it to Kaggle. Using 20% of data for training is a suggestion given to reduce the run-times as most of us are running the notebooks on their PCs with limited computational power.,317689.0
120650,523462.0,"1. Sub-sample 10-20% data from the training.csv file. 2. Split the sub-sampled dataset into train and test datasets in some ratio (say 80:20 , 70:30 , etc). 3. Build model on the sub-sampled train dataset by trying out various kernels and hyperparameters. 4. Once the final model is generated, predict outcome for the sub-sampled test dataset to see that the accuracy is as per the expectation set for the assignment. 5. Optional Step : Using the final model, predict outcome for the test.csv dataset. Save the index and the predicted value to a csv file. Submit the csv file on Kaggle which in-turn will generate accuracy for your predictions by comparing with the actual values. Check that the accuracy provided by Kaggle is in line with teh accuracy that was obtained for the sub-sampled test dataset. Hope this helps.",313826.0
120650,523064.0,"From 42 K data, you can choose 20% random data. On this 20% data(say 8.4K), do a train test split. For e.g if you are taking train test ratio as 80: 20, we will get 6720 records in train dataset and 1680 records in test dataset. Please refer the TA verified answers for similar question in the below link. https://learn.upgrad.com/v/course/208/question/120581",310467.0
120930,524982.0,"Hi Rajat, Please have a look at this page for the instructions to submit your csv file:- http://joshlawman.com/submit-a-prediction-to-kaggle-for-the-first-time/",344353.0
120930,525443.0,simply create a CSV file with two columns -&gt; ImageId and Label then output your predictions to the CSV file (I used the row number as ImageId) then simply upload the file to Kaggle - and you will get a score soon enough,300694.0
120519,523521.0,i'm sure they are not evaluating these models on their PCs,312199.0
120519,523027.0,"since this cannot answered by students, I thought TAs would respond to this.",313515.0
119739,519480.0,"Hi, If you are using 20% data for training , and your system has 8GB then it will not take more than 30 min. Please do not make large list of hyperparameter.",344894.0
119739,518559.0,"Not much cane be done for this. This is a computationally intensive assignment. Best you can do is put it on Colab/Kaggle to utilize the GPU backend, keep the cross validation folds to 3, and fewer hyperparameter values.",318438.0
119739,519872.0,"Please use the parameter n_jobs=-1 in GridSearchCV. This will take the data 8 way parallel processing. I was able to get the same working on my Macbook within 18 minutes: Fitting 5 folds for each of 9 candidates, totalling 45 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 45 out of 45 | elapsed: 18.4min finished By default this parameter is set to None and you are not utilizing your parallel processing. Try this, it will run even faster on Colab",309211.0
121000,525523.0,I checked gamma values are same,308495.0
121000,525530.0,comment ply.ylim() .. and execute the code block again.,312479.0
121000,525518.0,"check if the gamma values that you used to create the model are same as the ones that are used in the plot. For e.g gamma for building the model=0.05. But while plotting you might have used a different gamma , say 0.01.",310467.0
121000,525617.0,Remove the ylim statement,300694.0
121000,525890.0,Either remove ylim statement or increase the limit interval. y-axis values must be outside of mentioned limit 0.60 to 1.0 cause of which not showing up in your graph,316147.0
121000,525995.0,"thanks it worked now, i missed using scaled data before",308495.0
121000,525964.0,rather removing ylim increase the range . But i would recomend try increasing accuracy from 90% to at least 93% what type of scaling you have done better use standard scaler.,311386.0
120001,520047.0,"Yes you can, using PyDrive. You could follow couple of other ways as well -&gt; https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92",318329.0
120004,520052.0,yes there is a stop button in Jupyter notebook to interrupt the kernal,301648.0
120004,520074.0,"Try running it on Google colab platform, it will be faster there.",310974.0
119218,515190.0,"Depending on your laptop/desktop configuration, it could take hours. Better put your code on Kaggle/Colab and utilize the GPU backend. See: https://learn.upgrad.com/v/course/208/question/118686 and https://learn.upgrad.com/v/course/208/question/118903",318438.0
119218,525894.0,it took 10 minutes on my laptop - which is not a short time by any means but I suppose that is the price to pay for large datasets (I only used 20% of data) and complex models,300694.0
120125,520748.0,Facing same issue in colab also. its running for infinite time.,311404.0
120125,520858.0,"It is expected. ""Also, running a GridSearchCV() may take hours if you use a large value of k (fold-CV) such as 10 and a wide range of hyperparameters; k = 5 should be sufficient."" Focus on above statement, and try to execute code with less number of hyperparameter, it should reduce the execution time. Hope this will help.",317991.0
120125,524617.0,"it depends on your computer, amount of data, complexity of model make sure you are using parameter to use all the CPUs of your computer, and not just one CPU. With 20% of data, 5 folds and non-linear model my i5 8th Gen laptop is taking 10 mins (16GB RAM)",300694.0
120125,521340.0,"With 5 folds and 20% of the data, it is taking about 20 minutes on my laptop but i have used scaling also.",318585.0
118497,510989.0,But I see the submission page and upload deadline. Confusing it is. TA has to confirm what it means.,318329.0
118497,512011.0,"Where do you see it is non-graded? May be I am missing something or they have removed the non-graded text. ""For submissions obtained within one week after the deadline, there will be a 30% penalty. Submissions beyond one week after the deadline will not be accepted."" : This is what submission page says. So it should be graded.",318458.0
118497,512516.0,"Hi Rajarshri, SVM assignment is definitely a graded one. There was a slight mistake in text which we have corrected. Sorry if it created any confusion",301619.0
118497,525891.0,They have fixed this now,300694.0
119768,519407.0,"In the sessions of kernels, we have got to know about two hyperparameters SVM can have if you are using a kernel. Gamma and C. Use GridSearchCV for range of values of gamma and C and find the best combination for highest accuracy.",318329.0
120435,522011.0,"Please checkout my response for a similar query https://learn.upgrad.com/v/course/208/question/120278 IMO, we need not di de-skewing, recentering or any such image processing techniques for this assignment. We could have a similar approach as provided in the letter recognition exercise.",313826.0
120435,522208.0,"So, I will use Letter Recognition as reference , but if I could then I will try to include digit recognition approach.",315560.0
120732,523670.0,try using n_jobs=-1 in the gridsearchcv oe execute in google colab. also reduce the number of parameters in c and gamma,311254.0
120732,525320.0,"sklearn documentation: GridSearchCV( estimator , param_grid , scoring=None , fit_params=None , n_jobs=None , iid=’warn’ , refit=True , cv=’warn’ , verbose=0 , pre_dispatch=‘2*n_jobs’ , error_score=’raise-deprecating’ , return_train_score=’warn’ ) If you have chosen 20% of training data you should be getting approx 8400 records. this you will split to training and testing data in 80:20, 70:30 or any suitable ratio. use the GridSearchCv and add in the parameter n_jobs=-1 for increasing the computation time. you can pass some 3 different C and gamma values. with this I got the results in approx 20-25 mins because u have reduced the records and used n_jobs as -1. I have used kfolds as 5",301114.0
119336,516168.0,"Yes. Without going through the data cleaning steps, we would not be able to know about the data sanctity.",311117.0
119336,516486.0,Data Cleaning is an important step before you proceed with any model building. It needs to be done everytime in order to ensure the data sanity. Data Cleaning Step should not be skipped.,311254.0
119336,519873.0,Perform EDA on the data and you will be able to decide on how the cleaning needs to be done on the dataset,309211.0
121050,525838.0,"You can use the below code to get 20% of data from train dataset. train.sample(frac=0.2,random_state=100)",310467.0
121050,526205.0,"Hi, X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42",344894.0
119809,,nan,
119197,515202.0,"No, it is not necessary to use GridSearchCV if you know the opimal values for different hyperparameters. GridSearchCV just provides a clean and concise wrapper. Whatever GridSearch is doing, can be done by hand, although it would be prone to error (copy-paste) . Morevoer doing it manually won't save you much time either. This dataset will take a lot ot time, and there is not much anyone can do about it. Chose you hyperparameters wisely and try to run on Kaggle/Colab. Also, use a subset of data, not the entire dataset",318438.0
119197,515276.0,"I am assuming you are just picking the hyperparameters at random (making intelligent guesses).What's the basis of your guess? How do you ascertain that the value your are thinking should be the best is actually the best? If your hyperparameters meet the model selection and evaluation critera, you can continue with them. For me, using 3 fold valiation on 20% of dataset took ~55-60min on Colab.",318438.0
119197,516826.0,"Hi, You need to do GridSearch. You can pass 3 values for each parameter. LIke 'C':[1, 10, 100]. It will take time while building model, but you will be able to choose the right hyperparameter. Thanks",344894.0
119197,519870.0,"You can also use a parameter n_jobs=-1 in GridSearchCV. This will take the data 8 way parallel processing. I was able to get the same working on my Macbook within 18 minutes: Fitting 5 folds for each of 9 candidates, totalling 45 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 45 out of 45 | elapsed: 18.4min finished By default this parameter is set to None and you are not utilizing your parallel processing. Try this, it will run even faster on Colab",309211.0
120449,522021.0,Checkout a discussion on a similar query: https://learn.upgrad.com/v/course/208/question/120184/answer/521145/comment/119708,313826.0
120449,521998.0,"For this assignment, usage of test.csv file is not needed at all. You need to sub-sample 10-20% data from the train.csv file and then use this sub-sampled data to split into train and test sets,scale, build the model and perform evaluation on the sub-sampled test dataset. For Kaggle contest, you can use your final model and the test.csv",311254.0
120780,524023.0,test.csv file is not needed in SVM assignment. Choose any 10 -20% of sample data from the train.csv file. Then sub sample this data to train and test sample. Model should be trained on this subsampled train data. Final modle evaluation should be done on the subsampled test data. Test.csv file is required for kaggle submission. You can use the model trained on subsampled train data to predict the accuracy for kaggle data.,310467.0
120780,524181.0,To get an accuracy score for the test.csv file you will have to use your model to predict the digit for each of the row of data in the test.csv file; then that prediction along with also the ImageId column (basically the row number) you can submit online to Kaggle -&gt; they will provide you with an accuracy score. if you are ofcourse looking to just find the accuracy score on your local computer then your train/test split of train.csv will give you a good answer as we have lots of data rows to play with (48k) - and we are only taking 20% of that initially.,300694.0
120780,524547.0,"You cannot calculate the accuracy without the labels being present. Since, test.csv doesn't have labels, you cannot calculate accuracy using it. It is meant for kaggle for your model to be ranked.",318329.0
120062,520403.0,"Yes, scale the training/test features (not the dependent feature)",318438.0
120062,520419.0,Yes you must otherwise you will get crazy results from the model. I've experienced this :),310974.0
120062,520475.0,yes scaling is necessary.,301648.0
120062,521554.0,"yes,scaling is necessary for both training /testing features",306996.0
120062,521616.0,Scaling as shown in lecture notes will do?,307710.0
120062,530650.0,"but here is my question, the independant variables are already on a similiar scale (pixels) then why we need to scale it?",302739.0
119836,519394.0,"You can use opencv or PIL to read the image, convert to numpy array and treat it as a row.",318329.0
120249,521374.0,"Similar kind of question has been asked previously, and it has been answered and verified by TA. Below is the link of it. https://learn.upgrad.com/v/course/208/question/120184 Hope this will help.",317991.0
120461,522816.0,"Hi, Please follow the below link https://stackoverflow.com/questions/50319617/opencv-error-cv2-cvtcolor",344894.0
120258,522035.0,"Hi, Please do not take the range of C values. Try tree values for C as well like 0.01, 1,100 etc. You do not need GPU. If you are using 8 GB system then it will not take more than 30 min.",344894.0
120259,521802.0,You can try something like this to check for a space in the column name : df.columns[df.columns.str.contains(' ')] Please note that we are passing a space as the pattern to the contains method.,313826.0
120259,521682.0,List the column names. In general the pattern of first few columns would give an indication if there is any space in the column name.,311857.0
120259,523078.0,You can check the datatypes of the columns. Space would make the type as object instead of int,311160.0
119860,519599.0,Yes you are right,310974.0
119860,519640.0,Are we taking 20% data in train.csv only ?. The test.csv contains 28000 rows. For validation on test do we need to split test.csv as well?,311115.0
119860,519703.0,As per my understanding now we have two files train and test so from test you need take 20 percent of test as test data.And for train you need to take 20 percent from train dataset. Till now we were have one file so we were Splitting single file wherein now we are provided with train and test as two separate file. Please correct me if my understanding is wrong,300687.0
120581,523595.0,Extract only 10 to 20% of total input data - which comes to 4200 to 8400 rows. And then split train and test on the extracted data.,319759.0
120581,524557.0,It means you can extract just 20% of the whole data for train and test. Then that 20% you can use a 70/30 or 80/20 split for train/test,300694.0
120581,522630.0,Please refer to the following discussion threads and your doubts will be cleared. Essentially pick a 20% random but fixed data by setting random_state and use that as though that is the data you have for you to build the model. There are different methods to take this 20% out of the entire data. https://learn.upgrad.com/v/course/208/question/120263 https://learn.upgrad.com/v/course/208/question/120162 https://learn.upgrad.com/v/course/208/question/119860,301121.0
120244,521461.0,"Try a non-linear kernel with default values to see if there is an improvement in the accuracy. Accordingly, you can decide whether to use linear kr non-linear kernel. Please also try Gridsearch on other hyperparameter",313826.0
120244,522753.0,I took 10% of data. 4.2k observations. linear - 89% rbf - 91%(with best c and gamma) This case faster than used 20% data. We should do some percentage base modelling with accuracy ? not sure.,312019.0
119870,519631.0,"it is ok if you didn't have any null values or another basic randomness in a dataset. Just we need to follow the cleaning process I mean we need to check is there any null value, if yes then do treatment otherwise pass on.",301648.0
120270,522070.0,"Hi, Please follow below link https://mahotas.readthedocs.io/en/latest/install.html",344894.0
120657,523817.0,"Hi, Please follow the below link https://stackoverflow.com/questions/24121018/sklearn-gridsearch-how-to-print-out-progress-during-the-execution",344894.0
121113,526191.0,"You can take Amazon server or Microsoft Azure platform ( free account for 12 months) to practice ML. If I am not wrong, you can install/un-install softwares also.",312479.0
121113,526378.0,try google colab. it will give you the same jupyter environment as is shown in the videos and is a cloud based execution environment.,305839.0
121113,531611.0,my Dell laptop is 2010 XPS15 I3 4GB model with WIN 7 . Before start of this course i upgraded ram to 8 gb and its working fine.,306245.0
119658,522577.0,This helps Ashish. Thank you,316215.0
119658,518275.0,you have to divide your train data into test and train from there you can use test data as it contains label.,318429.0
119658,518327.0,"The Test dataset provided (without the label) can be thought of as the ""Real Test Data"" which will be used after you have finalized the model. For the purpose of building the model, as suggested by Rajat, you need to take the Train dataset and split it into trwin and test datasets.",313826.0
119658,518398.0,"thanks, Rajat and Vinay. Well Noted",310463.0
119658,521280.0,For the assignment there are two sets provided Train and test. You build the model on the train set taking 20% sample and apply the same to the test set. Test set in the given assignment is different from the test set that we create using train test split function. The asignemnet has 2 csv files- 1 train.csv and 1 test.csv,310629.0
121125,526322.0,"I think it would depend on how you identify the classes i.e.,dot = -1 and * = 1 or dot = 1 and * = -1. However, a verticle line at x=3 also seems to be a very good candidiate for this classification.",313826.0
121125,526198.0,"I think its B. because the support vectors seem equidistant from both the hyperplanes, then it comes to simplicity. and as Occam's razor rule says 'when in dilemma, choose the simpler model', we would check for simplicity. and as in 'B' the hyperplane passes through origin, it is simpler than 'A'. so in my opinion it should be B.",302738.0
121125,526491.0,"Hyperplane B equation is y=x which is a simple equation , computationally will be lighter as the coefficients are 1 and no constant and its easier to preent and maintain , thereby implying a simple model. As we should chose a simpler model based on Occam`s razor rule , hyperplane B should be chosen. Equation for hyperplane A is y= -x + 6... as you can see the coefficient here is -1 and constant is 6 , this will always conume more computation memory than hyperplane B.",318340.0
121125,526735.0,TA can you please confirm the answer ?,312479.0
121125,527035.0,"Both are technically same if you compare performance wise. Both points are equidistant(perpendicular distance) from the hyperplane in both figures. The only difference is the equation. Equation for A is: y=-x + 6 and equation for B is: y=x. If you go by complexity of the equation, I'd say B is more simpler than A.",307176.0
121125,527097.0,"Although both are same as we decide on hyperplane but equation y = x is more easy to compute. Hence, B is simpler than A.",317689.0
121125,527684.0,B for being less complex &amp; simple,306009.0
118977,514026.0,"True. We can't use test.csv to test the model, rather just predict the results. You can take the training data and split it to train and test/validation and once satisfied predict on the test.csv data.",318438.0
118977,519561.0,"Yes, I had similar kind of doubt. I guess if we upload our code to Kaggle it gives us the result ( I think in Kaggle intentionally they dont give us the y_test column as it is meant to check our model without revealing the exact answer), but it is not applicable to our assignment as we need label column",314092.0
118977,519714.0,"Yes, as Ashish has mentioned we need to train 20 percent of data of train and splitting it to 70:30. Test.csv should be used only for predicting the results.",300687.0
119885,519862.0,"Hi, No need to use PCA. It will work well on original data.",344894.0
119885,524619.0,theortically you can but it won't improve the results much (see blogs on the net that do this),300694.0
119886,519722.0,"Couple of things, Confusion matrix does not have to do anything with either scaling or being 2x2. MNIST case is not binary, i.e. there are 10 classes (0-10) and hence the confusion matrix will be a 10x10 matrix. One row for each class. Try to use the classification_report (sklean.metrics.classification_report). This will give you the essential metrics for each class. try pasting your code for the error you are getting",318438.0
119886,519943.0,For MNIST dataset the confusion matrix will give a 10x10 matrix. You will have to sum all the diagonal elements. A good alternative is sklean.metrics.classification_report,306248.0
119886,520420.0,I am getting a 10X10 matrix as well as the accuracy. Not able to calculate the Precesion and Recall and that's where the above error is poppping up. Tried both linear and non-linear approach,314313.0
119886,520450.0,"The above error is coming, as its expecting the target(label) to be Binary. In our case it's not. Please check the below link for more info: https://stackoverflow.com/questions/45890328/sklearn-metrics-for-multiclass-classification",314565.0
120909,524903.0,"Hi Manohar, Performing data quality checks is more important in this case. Check the range of values, see if there are any missing values, see if there any negative values. In terms of visualisation, try visualising randomly the pixel values for each digit.",344353.0
120910,524907.0,"Hi Vipul, Yes the correlation heatmap is required for the assignment.",344353.0
120910,525597.0,Do we need o delete the columns which are highly correlated?,304319.0
119525,517343.0,"Yes, your understanding is correct. Because running SVM on 42000 is computationally heavy task thats why it is suggested to use 10-20% of data.",317991.0
119525,517332.0,yes. your understanding is correct. we need to consider only about 20% of the training data set to train our model.,311686.0
119525,517573.0,How to select the 20% sub-sample as training set from the entire set given?,310974.0
119525,519330.0,"As the number of rows is 42000, you can select random number of rows count between 4200 to 8400.",301113.0
118686,513175.0,"It basically depends upon the processing power you have, try google colab with the GPU. It will solve the issue.",301619.0
118686,513192.0,"That's why I was asking for benchmark. Have you guys tried it out on different compute options with vaying data volumes and how long it generally takes? For the above dataset, I tried it on both Colab and Kaggle, and training time is easily into couple of hrs (need to time it accurately). But is this somethign that is expected or is there any mechanism to tune this?",318438.0
118686,513469.0,Yes. Been training the model with 3cv and only 4X3 parameters yet not able to complete the modeling process on Kaggle though i got 96% accuracy with a random set of parameters,317984.0
118686,513486.0,try using colab with gpu. it took me around 55mins for 3cv,318438.0
118686,519874.0,"You can also use a parameter n_jobs=-1 in GridSearchCV. This will take the data 8 way parallel processing. I was able to get the same working on my Macbook within 18 minutes: Fitting 5 folds for each of 9 candidates, totalling 45 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 45 out of 45 | elapsed: 18.4min finished By default this parameter is set to None and you are not utilizing your parallel processing. Try this, it will run even faster on Colab",309211.0
118686,519901.0,you guys have been talking in mins/hrs of processing time.... So no way u can get output within seconds or atleast in a couple of mins?,308437.0
120913,525408.0,Wish this was made clear in the problem statement. Less ambiguity the better,312199.0
120913,524912.0,Randomly take some percentage of data provided to you along with labels and perform the test on it. The test.csv is for participating in Kaggle competition and is not required for the assignment.,344353.0
120913,524918.0,Ok. The problem statement says to use test.csv for testing. That is why I had this doubt.,315471.0
120184,521206.0,"Not easy way, but if you really need to, can pick a row from test data, plot the pixel and compare with predicted value",317514.0
120184,521145.0,"For this assignment, you need not use the test.csv file at all. Instead, you need to sub-sample 10-20% data from the training.csv file , then use this sub-sampled data as the data that will be used for all the further analyses. In that, you need to again split this data into train and test sets, build model and perform evaluation on the sub-sampled test dataset. However, if you intend to submit for Kaggle, then you can use your final model to predict the label for the data in test.csv and create a submission file containing the imageid and the predicted label. Then you can submit it on the Kaggle platform, which will evaluate it with and provide you a score. Hope this helps.",313826.0
120594,522745.0,"Correct. Not sure accuray will improve, or SVM takes care internally. How to get columns which are having only one value ?",312019.0
120594,522802.0,"No, you should not drop these columns. These are images, whose pixels have been converted into grayscale values. The columns are 0 because they are the edges of the 28x28 matrix, where there is nothing written (whitespace).",318438.0
120594,523098.0,You dont need to drop these values. SVM will automatically take care of these values.,317689.0
120594,523129.0,"0 is not a null value or an insignificant value that can be dropped. Number reflects the portion of light/ ink that the pixel covers. Use it in the SVM analysis like taught in lectures, it will be better for results otherwise you'll end up losing data.",318335.0
120594,524594.0,I completely agree with @Ruchita to drop off the columns with 0 variance for the training our model. If it comes to creating images out of the pixels then we should not drop. Dropping these columns won't give any accuracy improvement but yes the computation time would improve.,318355.0
120594,524555.0,"I am also of the thinking that it should be able to be dropped, but won't be dropping it, because there is no variability in those columns",300694.0
120535,522398.0,"Hi, Those are not 'zero' in the sense of Unavailablity of data. Those are proper values with a meaning that for that particular pixel the 'ink' value is zero (which can take any value in the range of zero to 255). No column needs to be dropped. Will also suggest you to read the problem statement and the Kaggle data page once again.",311686.0
120922,524945.0,"Hi Aditya, I am assuming this for participating in the Kaggle competition. You can upload the csv to check your rank. If this is for the assignment, then this file is not required.",344353.0
120539,522463.0,"Yes except label, scaling can be done on the rest of the variables.",311254.0
120539,522634.0,"Label column is the one that contains the ""digits"" 0 to 9 and associated data are available in rest of the colums. Hence you should never scale the column ""Label"" as it is unlike other models. Basically they are not the numbers meant for the purpose of calculations but for character recognition. I hope the reasoning is clear.",301121.0
120539,522738.0,Normally sacling to be done only on independent variables. Here output/dependent variable is label. So only X data columns to be scaled. Y is to be predicted,312019.0
120539,523103.0,"Label is y (predicted variable). We generally dont scale predicted values as we need to predict the actual values. Here, these are numbers which bears significance and can be treated as charecters as we want to predict whether a 0 in image is actually classified as 0 or not. Here 1 and 7 would be much closer than 1 and 2 or 3. Which is why we will not use scaling.",317689.0
120539,523139.0,Scaling is required as the value difference between different pixels is in order of 3 digits when you see average values.,318335.0
120539,523351.0,"After splitting into X and y, you can only apply scaling on X dataset as it does not contain label column.",318448.0
120539,524561.0,yes you can scale the non-label columns,300694.0
120539,526346.0,We can scale the columns except labels,305650.0
120925,524962.0,"Hi Aditya, SVM is senisitve to correlated features. The correlated features does affect the output of the SVM algorithm. hence it is expected for this assignment.",344353.0
120925,525629.0,Do we need to drop the correlated pixels from the data set?,304319.0
120263,521524.0,But I was going through old discussions where ppl took only 20% of 42000 = 8400 and then did the train_test_split with 30% and 70%.. Im confused now which is correct?,307710.0
120263,521539.0,"Yes. But they got this 20% using train_test_split only which you mentioned about. Or you can use df.sample command. Once you get this 20%, you can treat this 20% as your entire dataset and do the normal processes which might again include a train_test_split.",311686.0
120263,521518.0,"hi, your first understanding is correct i.e. 'the train.csv should be used as a dataset and train_test_split, we have to split 20% and 80%.'. Then we use that 20% data to train our model using CV.",311686.0
120263,522255.0,Extract only 10 to 20% of total input data - which comes to 4200 to 8400 rows. And then split traing and test on the extracted data.,319759.0
120522,522400.0,"hi, if you are talking about the graphs that we plot then those are between 'K Fold mean train score' and 'K Fold mean test score'. Not sure if I understood your query correctly.",311686.0
120522,522479.0,"Once you run the GridSearchCV for various hyperparameter values and some K-fold (say 5 fold), you need to choose the best set of hyperparameters and rebuild a final model to fit the entire train data. Next, predict the labels for test data and then calculate the accuracy using actual-labels and predicted-labels for the test data.",313826.0
119987,519959.0,I think the correlation evaluation is not required in this assignment. Because we are dealing with different type of data i.e. image data.,317991.0
120764,523978.0,use average= 'micro' as a parameter. hope it helps,314313.0
120764,524186.0,"yes because of the way you are trying to calc precision, recall, etc you can try the suggestions above or you can ask yourself why you need to calc more than 'accuracy' score. Kaggle competition, for example, provides accuracy score. Calculating Accuracy score works fine using metrics.accuracy_score with default parameters",300694.0
120764,523903.0,you need to define the parameter ' average ' as it is a multilabel target not binary. you can refer the following links for more clarification: https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary?noredirect=1&amp;lq=1 https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html,302738.0
120940,525264.0,the probability in SVC is by default set to 'False'. you need not explicitly set it to false. you can refer following link for further information: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html,302738.0
118629,511722.0,"Yes, the reason they are asking us to subsample is due to intensive computational needs. Running the SVM algortihm on ~42k data points with multiple combinations of C, kernel and gamma can easily take hours.. For the sake of this exercise, you can take 20% (~8k) data sets and play around with the hyperparameters. Similarly to test, you can take a sub sample of the test set. Once you are happy with the results, you can train/test it on the entire set.",318438.0
118805,513122.0,"Yes, it is computationally expensive. Reading more about SVM, they don't seem to the choice of algorithms for large datasets. Try running your code on Kaggle or colab, which offer GPU backends. Even on that it's going to take atleast a couple of hrs to train. I had to scale down to 3 CV folds and all the features.",318438.0
118805,518117.0,"well, I managed to run 5 folds on 3 gammas and 4 C on the data in 20 mins...that's not bad !! But my graph looked pretty skewed...",304814.0
118805,518113.0,"Have any one of you identified the way to run the CV? I am just not able to perform the grid search and fit the model. If I however chose random values of C and Gamma, then atleast model fits :-)",304814.0
119956,519894.0,"After posting the question, I came across other posts with a similar question. The common answer was to use train_test split, which does sound feasible for the given problem. In my search, i came across the below and it provides a different approach: https://maxhalford.github.io/blog/subsampling-a-training-set-to-match-a-test-set---part-1/ Can some of you advise which might be the adequate approach?",319302.0
119956,519941.0,"samples=data.sample(frac=0.2, random_state=100)",306248.0
119656,525635.0,Do we need to submit Jupyter notebook in zipped folder ?,314621.0
119656,518252.0,TA intervention is required in this query. I think it's mistake.,318429.0
119656,519475.0,"Hi, You need to submit One Jupyter notebook",344894.0
120949,,nan,
120954,525272.0,Please refer to a similar discussion: https://learn.upgrad.com/v/course/208/question/120278,313826.0
120954,525368.0,No image processing is not needed in this assignment.,317991.0
120954,525441.0,it is not NEEDED - but if one WANTS to .. then why not ;-),300694.0
120956,525265.0,the features in the svm assignment are basically pixels. so you do not need to do feature reduction.,302738.0
120956,525439.0,this has been discussed previously -&gt; yes you can do dimensionality reduction (as opposed to feature reduction) -&gt; but it is not needed and probably won't make much difference (as discussed in several blogs),300694.0
120956,525606.0,"It is not required to do feature reduction. It is because, we are working on the samples which are collected by the digitizer and pen. If you remove any feature from sample, it means that you would be erasing the ink and it would be a problem to recognise the handwriting correctly. Here, even a single pixel value matters.",312479.0
120338,521788.0,"The sub-sample of 20% will have to be taken from the ""training.csv"" file itself, using one or the other techniques (like train_test_split(), sample() or any other method). The process of sub-sampling has to be done programatically within your notebook using python code itself. Please ensure that the code written picks up the same set of records everytime ( for example using random_state parameter in train_tes_split() method), thus reproducing the same set of results everytime the notebook is run. Hence, I believe there is no need to include the sub-sample file as part of the submission. Regarding PDF, the TS has clarified that there is no such requirement and the only file that needs to be submitted in the jupyter notebook. Please refere the duscussion https://learn.upgrad.com/v/course/208/question/119656 Hope this helps.",313826.0
120338,524436.0,"As long as the training.csv file is used for sampling, that should be fine. I think we need not use random_state in the portion taken from the remaining 80% that is used for testing purpose. That way, we can validate the accuracy of our model as well.",314730.0
120338,524635.0,"We do not have to change anything in training.csv file, only we have to extract 20% data from it and work on the same. We can either use slicing or random_state to fetch 20% of data.",318448.0
120139,520977.0,Why do you think this matrix is bad? Looks ok to me.,310974.0
120139,520991.0,"In general, in a confusion matrix, the predicted classes are compared with the actual classes. Each row of the matrix represents the results of prediction for the corresponding class at that row, while each column represents the actual class. The diagonal cells show the number of correct classifi_x000c_cations, while the off diagonal cells represent the misclassifi_x000c_ed predictions. For example, the trained classifier that you built correctly predicted 787 cases of digit 0, 927 cases of digit 1 and so on,",310467.0
120139,524616.0,"Matrix looks fine as it is not a binary classification, but multi-class classification",300694.0
120928,524977.0,"Hi Rajat, If you want, you can increase the size of the test data. Testing is not computationally intensive. So you can go ahead and increase the test data size if you feel so.",344353.0
120959,525297.0,you can refer following link for TA verified answer for similar kind of query: https://learn.upgrad.com/v/course/208/question/120924,302738.0
120959,525324.0,"sklearn documentation: GridSearchCV( estimator , param_grid , scoring=None , fit_params=None , n_jobs=None , iid=’warn’ , refit=True , cv=’warn’ , verbose=0 , pre_dispatch=‘2*n_jobs’ , error_score=’raise-deprecating’ , return_train_score=’warn’ ) If you have chosen 20% of training data you should be getting approx 8400 records. this you will split to training and testing data in 80:20, 70:30 or any suitable ratio. use the GridSearchCv and add in the parameter n_jobs=-1 for increasing the computation time. you can pass some 3 different C and gamma values. with this I got the results in approx 20-25 mins because u have reduced the records and used n_jobs as -1. I have used kfolds as 5",301114.0
120959,525440.0,"use n_jobs = -1 for the grid search - this will ensure ALL your CPUs are used for processing then it depends on how powerful your PC is, how much data you used (I used 20%), how complex your modelling is Mine took arnd 10 mins on i5 8th gen 12gb ram",300694.0
119626,519321.0,"You can copy the 10-20% rows to new CSV file and work on that, because they are asking us to not work on 100% data as it might take longer time to do processing",301113.0
119626,518097.0,"Use test_train_split to divide the original data set into two data set. Just dont use Y here for now. Something like : A, B=train_test_split(train, test_size = 0.2, random_state = 4) Where train is your orginial train data frame containing 42,000 records, and B would contain 20% of 42000 records. Just make sure that you get enough split for each digit in your new train data set (B)",304814.0
119626,518179.0,"You can subsample any percentage of data using test_train_split. Sample Below # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 4)",301648.0
119626,518341.0,Check out the below stack overflow discussion on stratified sampling of MNIST Dataset https://stackoverflow.com/q/29438265,313826.0
119626,519226.0,"Randomly select index , assign length as approx 20% of total shape.",317990.0
119626,519266.0,"This can be achieved if we split the data and make the train_size in train_test_split as 0.2 Since the model would be built using results from training data and having 42000 data points would make it computationally intense, it is recommended to have training data as 20% of 42000 and test data as the remaining data points.",306726.0
119626,519918.0,"digits_train = digits_train.sample(frac=0.20, random_state=99)",304812.0
120600,522722.0,"The number of candidiates the GridSearchCV runs can be found by multiplying the hyperparameters that are being passed. For example, if you are passing hyper_params = [ {'gamma': [,1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]}] , then you are passing 3 values for gamma and 4 values for C, which will be equal to 3*4 = 12 Candidiates. On top of this, if you are mentioning the number of folds as 5, then you will be running 5-fold models for each of the combination of above hyper-parameters. So in all, it will run 12*5 = 60 fits of the model chosen.",313826.0
120600,523099.0,Total fits would be total number of different model combination tried by GridSearch. k * number of gamma values * number of C values. Which in your case is 5 * 3 * 4 = 60.,317689.0
119665,518345.0,"That's right. This is a standard dataset, but for computational effectiveness, we can carve out ~20% of the data and perform all the required steps (EDA, data cleaning, imputing etc) just on this subset.",318438.0
119665,519334.0,"Yes, you can take around 4200 to 8400 rows from the file and do a EDA on that",301113.0
119665,519796.0,"I have one doubt, do we have to carry this 20% data approach for test data as well?",300721.0
122032,533139.0,"I had faced the same issue, when i missed to scale. Scaling had fixed the issue.",319302.0
122032,530504.0,Make sure you scaled the variables before applying classifier.,310974.0
122032,530683.0,"I believe you forgot to apply scaling before model building. Apply scaling, then use model",305650.0
120809,524244.0,"Data understanding and basic EDA are necessary. Check summary,columns,for Nan values and so on. Even try with heat map and atleast add one or two sentences on plots obtained For ex,heat plot is difficult in this case to draw interpretations as there 785 varibales and so on.",308638.0
120809,524163.0,"I have done EDA - in my opinion, if you look at the CRISP-DM model, you ALWAYS have to do EDA. In one of the assignments I did only partial EDA and lost marks. Now I NEVER forget to do FULL EDA ;-)",300694.0
120809,524269.0,"As mentioned by others EDA are necessary , as it shows the graphical data undertanding . For this data set you can do Bar plot or Count plot too .",311861.0
120809,524544.0,"You could do NULL value exploration, check the number of images per label you have, df.info, df.describe",318329.0
120809,524556.0,"Yes, EDA is an important part of any machine learning model building process to understand the data so you should perform EDA like check duplicates, null values and visualize the data, etc",320687.0
120809,524665.0,"Yes, EDA is must according to the CRISP-CRISP-DM workflow",311466.0
120430,521869.0,"Since the join is based on index, check if index of both dataframes are having similar values. It might not match, resulting in NaN values. If this is the reason, you can use the approach given in the below link to fix this. https://stackoverflow.com/questions/44119437/pandas-join-function-resulting-in-nan-values",310467.0
120262,522072.0,"HI, There was only 16 features but if see here you have 784 features(columns ). It increased your vector dim.",344894.0
120262,522484.0,"But now one more doubt...In real world, there will be thousands of features and millions of data points. In such a case, can a data engineer/scientist wait for hours for a Gridsearch result? How is this issue overcome practically?",308437.0
120278,521794.0,"Since these techniques have not been covered as part of the course and also there is no mention of these in the evaluation rubrics, my intuition is that we need not do any image processing for this assignment. However, there is no restriction and these steps can be included as additional steps for own learning.",313826.0
120278,522063.0,"Since TA has verified the answer I am assuming wether we write the code for image processing (deskewing the image, re-center of image etc) for this assignment or not it will not be included for evaluation (or marks calculation) and no marks will be deducted if we don't include it.",317991.0
121051,525839.0,"You can use the below code to get 20% of data from train dataset. train.sample(frac=0.2,random_state=100)",310467.0
121051,525840.0,"You Need to take 20% sample data from Train datasets .. You can use below snippet. sample(frac=#required Fraction,random_state=100) You can give your required fraction of total data accordingly.",301648.0
121051,525871.0,you can ues either the sample function and pass a percentage (e.g. 0.2 for 20%) or you can just take the first n rows of the data frame using slicing and/or iloc,300694.0
121051,526208.0,"Hi, X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)",344894.0
119824,519331.0,using predict on test data,318429.0
119824,519400.0,You can measure the metrics if you have the labelled data. Predict using x_test and measure with y_test and predicted.,318329.0
119824,520283.0,But how will the accuracy be measured. For measuring accuracy we need both predicted data and actual data for comparison?,311729.0
119824,521687.0,"the train data needs to be split into 20, 80. Use the 80% data for measuring accuracy. If you try to use the test.csv you won't be able to measure accuracy as it does not have labels.",311857.0
120050,520352.0,Scale before making the split of test and train data and don't apply scaling to target (Dependent variable ) i.e. label,311466.0
120050,520324.0,It is not required and also it is not recommended to scale target (dependent) variable i.e. 'Label' in this case.,317991.0
119869,519617.0,"if the datasets are available online, you can use wget/curl to download them into the workspace if it's local, you can try creating a github repo and clone it on colab",318329.0
119869,519674.0,"Hi Keerthi, This is also how you can import : 1) At first upload the files to google drive 2) Use this command to mount the folder in your workbook: from google.colab import drive drive.mount('/content/gdrive') 3) When you execute the code, it shows up authorization which requires you to click on URL google provides and copy paste in authorization box 4) You're all set. reference the files you have uploaded using command such as follows: pd.read_csv(""/content/gdrive/My Drive/Colab Notebooks/train_100.csv"") Hope it helps. Thanks!",309211.0
119869,519717.0,You can also integrate Kaggle with Colab. https://stackoverflow.com/questions/49310470/using-kaggle-datasets-in-google-colab,318438.0
120247,521367.0,This method may not give you an equal distribution of all classes. You can't say that the first 8400 rows only can be used. It can be any of the 8400 rows. But the distribution has to be equitable among classes. You need to sub-sample using train test split.,310505.0
120247,521370.0,"u mean i need to take input data as it is and then while splitting into train and test, use 20% factor for train data?",308437.0
120247,522258.0,You can take any 8400 rows and check if the data is biased towards one of the digits. using: sns.countplot(df['label']) plt.show() if the data is unbiased - you can go ahead with the selected rows,319759.0
120921,524938.0,"Hi Denny, Yes, you can choose a different value which does not overfit the data and which generalizes well.",344353.0
120693,523472.0,"For this assignment, usage of test.csv file is not needed at all. You need to sub-sample 10-20% data from the train.csv file and then use this sub-sampled data to split into train and test sets,scale, build the model and perform evaluation on the sub-sampled test dataset. For Kaggle contest, you can use your final model and the test.csv",311254.0
120693,523402.0,"Yes Pooja, even I have the same doubt Can anyone please help us clearing our doubt ?",301655.0
120693,523756.0,"Yes we have to use test.csv According to the below mentioned sentences what I understood is, 10-20% of train.csv for training the model and 10-20% of test.csv to test the model. I this case we need not split the data, insead we need to prepare (scaling etc) both the subsets "" Important Note: Since the training dataset is quite large (42,000 labelled images), it would take a lot of time for training an SVM on the full MNIST data, so you can sub-sample the data for training (10-20% of the data should be enough to achieve decent accuracy). Also, running a GridSearchCV() may take hours if you use a large value of k (fold-CV) such as 10 and a wide range of hyperparameters; k = 5 should be sufficient."" ""You can download the dataset from Kaggle here - please use train.csv to train the model and test.csv to evaluate the results. """,316255.0
120693,523967.0,Yes we have to use test.csv to predict digits by using the svm model which we have build,311466.0
120689,523593.0,A sandclock on the jupyter notebook window can help,319759.0
120689,523387.0,In my case it took 50 mins of timespan to process 15% dataset ( i.e around 6300 rows).,311004.0
120689,523705.0,Use the timeit module. It will help enormously.,314048.0
120689,524192.0,gridsearch automatically outputs status updates. Here are two lines from mine: [Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers. [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 7.1min [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 10.0min finished,300694.0
120689,524693.0,"I agree with all the earler answers; to arrive at an accurate estimate, here's my steps: Say 3 alpha and 3 Gamma parameter with 5 folds, the code will train the model 3*3*5 = 45 passes ; if you are not using parallel option(n_jobs=-1), this would take sequentially 3.2 minutes for a single pass and about 147 minutes for all the passes [ 2.45 hours] ; however if I use n_jobs=-1, since I have octa-core, the single pass takes about 3.2/8 = 0.4 minutes i.e for 45 passes it takes roughly 18 minutes. If my machine was a quad core (instead of octa core), it would take 3.2/4 = 0.8 minutes i.e for 45 passes it takes 36 minutes. Now instead of 5 folds I change it to 10 folds, the total computation would be for 3*3*10 = 90 passes and it would roughly take 36 minutes for Octa core and 72 minutes for quad core So in short, number of passes you choose depends on number of hyper params and the folds you choose and you can roughly arrive at total time taken by just choosing 1 hyperparameter each and not select any folds (default is 3 folds ), so you can estimate the time taken for a single hyper parameters (1 gamma and 1 C value) in 3*1*1 = 3 passes and extrapolate the time taken on your machine using gridsearchCV for the same single hyperparamater, use the n_jobs=1 and you will certainly see that job processes the passes in parallel Hope the estimation technique I mentioned above helps. You do the math for your scenario and can precisely estimate total time that's taken by your Gridsearch. Also please use this on 20% of the subset. [even with single hyperparameter gamma / alpha, if you try to run this on entire dataset, the computation will take really longer and you will additionally need to factor that in as well; the above time that I indicated was on 20% subset and not the entire dataset ] Thank you, Harsha",309211.0
120926,524971.0,"Hi Raveena, No need of doing it twice. You can either normalise or scale and no need of doing both.",344353.0
120995,525485.0,heatmap is a part of eda to find correlation only,318017.0
120995,525498.0,"As Nitesh mentioned, it dooes not make any sense in plotting the heatmap since its a 800*800 cell martix. It does not provide any meaningful insights. But since TA mentioned that we need to plot heatmap, I would like to understand how to interpret such a cell matrix, Also, what if we find correlated fields. Should we drop them?",310467.0
120995,526457.0,"I agree with Keerthi and Nitesh.Since there are 785 variables, drawing interpretations viacorrelation using heat map is very difficult. EDA is very essential for any data analysis as we know.TA should give few key points how EDA can be done in this case(When more than 500 variables involved) apart from checking for NA values and other basic EDA.",308638.0
120995,526697.0,"Hi, In this assignment no need to check the correlation between pixels.",344894.0
121041,526160.0,"yes, zip notebook and upload.",312479.0
121041,525872.0,in previous reply TA already mentioned it is only python file but I played it safe and uploaded it as zip file as instructed on submission page,300694.0
121041,526031.0,Make sense. I also uploaded in zip.,317991.0
100197,430938.0,"AFter the deadline is over , the correct approach for assignment and case study solution is published in the module. Please cehck the module post second deadline. You can also check with the mentor for the same.",318340.0
90919,379089.0,The idea here is to compare three actors in terms of means of following parameters for each of them ' num_critic_for_reviews' and ' num_user_for_review'. The 4 steps mentioned there are self-explanatory. Any specific doubts you have?,311686.0
90919,379802.0,"Merge the three dataframe into one say df_4 using append. Then df4_grpbya = df_4.groupby('colname') Then df4_grpbya[['colatoaggrgate','colbtoaggrgate']].aggfunc()",317811.0
90922,379083.0,"There are multiple ways to extract top 250 movies. You can use the method 'nlargest' of Data Frame. But make sure that you use it with a clause meeting the requirement that ' num_voted_users is greater than 25,000' For Rank colum: Sort the new Data Frame created if not already sorted and then use position based indexing to create a new column rank which contains the rank from 1 to 250.You can use an .iloc() here",311686.0
90922,379396.0,Use label based index on 'num_voted_users' column and assign it to a data frame. Sort this data frame by 'imdb_score' column decendingly and assign it to mentioned data frame. Use reset_index function to reset the index and create new column 'Rank' which takes in the index + 1 value to Rank column.,305843.0
90922,379652.0,First generate the 250 top IMDB score movies. After that create rank column using range(1 to 250 increments of 1). Then use set_index to bring rank as first column.,301644.0
90920,,nan,
90926,379069.0,what issue you facing?,309451.0
90926,379076.0,able to filter for other two actors but i am not getting any movies but i can see movies in the df,314678.0
90926,379093.0,selected the same but not able to filter,314678.0
90926,379091.0,Make sure you are using following text for filtering : 'Leonardo DiCaprio'. The problem you are facing might be beacuse of some wrong text being passed.,311686.0
90926,379279.0,"Hi, Try using the filter for the exact name 'Leonardo DiCaprio'. Apart from that, there seems to be no other reason for not being able to filter the rows. We would not be able to help with codes since this is an assignment question.",317987.0
90926,379423.0,Make sure that the column name and values is correct and you assign values to your df after operation and everything else shold be fine.,318368.0
90926,379794.0,"Please check the dataframe to make sure you haven't dropped any additional rows while doing the previous steps. Because, if you are able to filter for other 2 actors, this should not be any issue in doing the filter for Leonardo DiCaprio.",316202.0
90926,380285.0,Read the question ..it is given by which text we have to search,318426.0
92440,389361.0,yes i guess that woukd be a logical assumption since BUY and SELL has got no significance in such cases.. i assumed the same.. let's hear from a TA if this is not correct..,316349.0
92440,389370.0,It is mentioned in the problem set that the moving average should cross each other to generate the signal.If it has not crossed it is hold.,310629.0
92440,389389.0,Only in case of crossing point it need to change. Rest all cases would corresponds to HOLD.,317689.0
92440,389412.0,"for first 19 and 49 rows, what should be signal value... Null or hold??",318319.0
92440,389627.0,'Cross Over' means it should move up or down and not be equal.,300717.0
92440,389696.0,Yes it is required to show HoLD signal as mentioned in problem statement,319869.0
92440,390064.0,"Only crossing points will be sell or buy signal, rest all will be hold",317811.0
92440,390286.0,"Only first CROSS points need to be highlighed as BUY or SELL . I think, if both values are equal for consequitive rows, then the first CROSS point should highlight BUY/SELL but the followng consequitive points should say HOLD",318454.0
90159,375571.0,"No, I have got the correct answer.",318368.0
90159,375838.0,I got the correct answer. Make sure if you have grouped it according to the presented instructions.,311727.0
90159,376024.0,Even I have got correct answer. Please check the groupby part and also if you have picked the correct columns.,300708.0
90159,376125.0,No. Getting the correct answer. Please check your group by on the correct columns and also need to sort in correct order to get the answer.,311115.0
90159,376538.0,You will get the right answer. Run your analysis by sorting. Use proper dataframe.,317269.0
90159,376727.0,"Hi I get ""Family + Sci-Fi "" on the raw data set (without making any chnages to the original csv data) other wise i get a different answer.",306725.0
90159,377631.0,I also got the correct answer.Please check your dataframe.,314183.0
90390,376889.0,I have used rank function and assigned rank in ascending order of imbd_score.,318368.0
90390,376914.0,"In my opinion, since we are sorting only based on one attribute, we have to provide same rank if that attribute has the same value.",313826.0
90390,376954.0,We can give the rank based on the order in which they appear. I think we have to assign rank from 1 to 250 hence same scored movies also need to have different ranks.,317689.0
90167,375612.0,"Create separate df for each grouping, dont assign it to Combined df.",318368.0
90167,377103.0,"I think it should be pretty clear for you after ""because if I assign grouping object to combined dataframe, contents of combined dataframe change after performing the operation like mean "" this observation. As the assignment is graded, and you'd be evaluated on the aspects like this, you are encouraged to take a call by yourself. Also, refrain from asking direct questions from the assignment, rather ask a general conceptual question.",319721.0
90929,379217.0,"There is no such path mentioned by the team. You could read the file by placing it in Jupyter installation folder. In that way, it is enough if you just use the filename in read_csv statement.",318084.0
90929,379275.0,"Hi Parinita, This URL https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0 is actually a download link to a CSV file. You can open the URL in your browser and it will download a CSV file. If you wish to work on the same dataset, you can do it in 2 ways: Use the same URL in your Jupyter 'ipynb' file as they have used in the console. Download the CSV file by opening URL in the browser, place it in the same folder where you have 'ipynb' file, use the method read_csv to load data.",317987.0
90168,375687.0,"Let say in a sample dataset of 100 people residing in a city, there is column age and it has 100 rows/records and 40 rows are blank in this column having no data in cells. If we need to analyse that what is the average age of the people residing in this city and usually what we will do we will take sum of the values in this column and divide by 100. It will distract our data analysis as there are around 40 people for whom data was missing but still be divided by 100. To make our data analysis more accurate, Data Cleaning comes into picture. We should impute blank values with the mean values of age column to proceed our data analysis.",317811.0
90168,375645.0,"Columns which are having &gt; 5% null values. So, drop all rows having those columns as null.",317689.0
90168,375630.0,When you inspect the the df. You will see there are column which has more than 5% null values. You need to remove all the rows where those column contains null values.,318368.0
90168,375648.0,"In the dataframe that you are working on, there may be some columns which will have a large percentage of missing values. In my view, we can deal with such columns in the following ways: Drop the entire column only if that column is insignificant to the analysis that you are doing. Impute the missing values. Here, you are setting some value to the missing vlaue based on the other values that are present in this column. For imputing, you have to be fairly confident that the value that you are imputing would certainly fall in the range of expected values and also that you are not introducing any bias which can affect your analysis. In case you notice that by imputing you are introducing huge bias that will certainly affect the outcome of the analysis, then you are left with th option to delete such rows where the column is having null values. It is a trade-off that has to be weighed in, as deleting rows would mean that you are reducing the size of the dataset that you will be using for further analysis.",313826.0
90935,379249.0,"Not sure why would you have No Name or a column without a header here. You are doing a groupby by column c and applying aggregation function (in this case, mean()) on some field of the dataframe.",302740.0
90935,379799.0,"Step 1: a= b.groupby('c')['columnnametoaggregate'].mean() Step 2: Now to sort this you need name of aggregate column. res = pd.DataFrame(a).sort_values(by='columnnametoaggregate', ascending=?).head(n) res",317811.0
90935,379261.0,"Hi Ayushman, it's a good question. I came across the following solution to the problem we have: .agg({'ColumnName':'mean'}) instead of .mean() Thus, your new code: a=b.groupby('c').somevalue.agg({'ColumnName':'mean'}) I found it useful since we can have further filtering and sorting on the newly formed column if they have the column name. Reference: https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/",317987.0
90935,380410.0,You can access with same column name if you wish to,311803.0
90171,376547.0,You question is valid. But i think more important are the codes after database loading.,317269.0
90171,375680.0,It should not be an issue. But generally we should keep all such files in the same folder where our ipynb file is located to access them without any other explicit path being passed.,311686.0
90171,375659.0,"Hi Praneeth, I don't think read_csv path location will create the issue. To be on safe side, keep the path location as general, as that was specified in examples, so that it doesn't creates confusion for the evaluator.",318429.0
90171,377025.0,Since the csv path is relative and not absolute in the code. It wouldn't matter as long as the csv file is in the same directory as the .ipynb file.,318085.0
90171,377105.0,"Your code will be evaluated in another system, and when being evaluated, the path will be edited by the grader. Don't worry about the path, it will be taken care of by the graders. Hope you enjoyed the assignment.",319721.0
90943,379213.0,Maybe this column is already dropped. Please reload the file and execute the statements again.,318084.0
90943,379787.0,You are doing right way. Just eecute the command again where you are intially importing the csv file.,317811.0
90943,379215.0,"got the issue, it was because i did used header=None while reading the csv file, hence header were turned into first row. which caused the drop command unable to find that column.",306005.0
90943,389138.0,"Not sure if this is late but you can use similar to this movies = movies.drop('COLUMN', axis=1)",312732.0
90329,376543.0,I would say you are going in the right direction. Please think/ play around with your code. But yes there are different ways to do this. If you want to approach with your method than chk stack and pandas documentation for use of drop.,317269.0
90329,376471.0,There is no way of doing this. You have to explicitly provide the column names. I spent a good 2-3 hours trying to find if a condition can be given but didn't find any :),310974.0
90329,376500.0,i m not getting your question but see this https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html it might help.,302738.0
90329,376524.0,"There is one way find all the rows which have null value in frame lets name it ROWFRAME then using this frame put your condition in a loop such that for column in range(count(clumn) of all ROWFRAME : Here you write a condition to test .loc[""column[i]].sum == 90 # It checks one by one column as Panda Series it loops for first column as series and return True if found NAN else return False if( found True t hen drop the column and break out from the loop) else loop throuh all the series of panda till the end and if not even a single condition true means no drop is required. Similary it will loop all through the column. Hope it helps!!",307843.0
90329,376664.0,"Guys, this is so simple. Use numpy arrays to evaluavte the conditions over the column for all trues and then use invert operator.",306248.0
90324,376433.0,I Did it for the movies data frame as top_250 date frame was only for subtask 3.4 context,300694.0
90324,376446.0,It should be on complete movies df.,318368.0
90324,376556.0,Please try on both. It has to be done on movies. You might not be able to extract required output easily if you use IMDB. But the ouput does show in both dataframes.,317269.0
90324,376670.0,movies,306248.0
90324,376672.0,It has to be from the movies dataframe.,314547.0
90324,376921.0,movies dataframe,310511.0
90324,377339.0,Why movies dataframe? then what is a purpose of cleaning the data? In my opinion it should be IMDb_Top_250,306011.0
90324,378831.0,"we have been said to group dataframe as per directorname so for that we shuld go for movie so that we hv complete picture ,,,, filtering had been done in case of imdbtop250 for num of voted users &gt;25000, only confusion is that is we can apply extra filtering measures like that for this subtask (i think we should) but problem statement dont tell it .",318005.0
90324,378433.0,"its movies dataframe, I had similiear doubt but check point 4 will answer this question",317577.0
90165,375603.0,Lambda function worked fine for me. There might be syntax error in your code.,318368.0
90165,375606.0,i used simple df['col_name'] = df['col_name']/1000000 and it worked. :),308495.0
90165,375628.0,"You can use a lambda function to convert $ to million. 1 million =1000000 I will say one example lets divide a list by 5 myList = [10,20,30,40,50,60,70,80,90] newList = list(map(lambda x: x/5, myList)) print(newList) I hope the example helps",317845.0
90165,375934.0,You can directly use df['ab'] = df['ab'] / 1000000,318358.0
90165,376028.0,you can use apply function of DataFrames df['column'].apply(lambda x: x/1000000) or you can use directly df['column']/1000000,300708.0
90165,379245.0,"We can achive this without using lamda, simple pandas col airthmatic should do like we did before for creating new column Profit ( Gross - Budget), this this case col operation will be on same columns",317577.0
90877,378930.0,yes it should be Leonardo DiCaprio instead of Leo Caprio.,300726.0
90877,378793.0,"Yup, it will be Leonardo. The checkpoint 6 also says Leonardo.",314547.0
90877,378801.0,Leo_Caprio is just the varible that they have asked us to use for including all the movies in which Leonardo DiCaprio is the lead.,313826.0
90877,378989.0,"Yes, Assignment mentions names also in the description.",318368.0
90877,378815.0,"Its already mentioned in the question. Create three new dataframes namely, Meryl_Streep , Leo_Caprio , and Brad_Pitt which contain the movies in which the actors: 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' are the lead actors. Use only the actor_1_name column for extraction. Also, make sure that you use the names 'Meryl Streep', 'Leonardo DiCaprio', and 'Brad Pitt' for the said extraction.",317991.0
90877,379657.0,Leo Caprio is the dataframe that we need to create for Leonardo DiCaprio.,319302.0
90877,379785.0,"Leo Caprio is the data frame name that we need to create for actor Leonardo DiCaprio. After performing the operation of finding the highest mean of critic review and user review, Leonardo DiCaprio will be the at the top. This is what checkpoint 6 says.",316202.0
90646,378063.0,"I think they are the same as Dataframes are basically a collection of Series. In the above case, this is a single Series dataframe.",318085.0
90646,378086.0,Please refer to the following link: https://stackoverflow.com/questions/26047209/what-is-the-difference-between-a-pandas-series-and-a-single-column-dataframe,306040.0
90638,378065.0,"That's dependent on you and you'll be scored for that part based on what all functions you used to inspect your dataframe. Recall your lectures from the session ""Getting and Cleaning Data"" and use the appropriate functions.",306040.0
90638,377955.0,anything u feel like some are included in q itself.. rest you can do research from Google and try,317982.0
90638,377981.0,As many as you can get from google. More the parameters more insight you get about dataframe.,317991.0
90142,375484.0,What do you mean by lenght? Rows or columns? Also how to you want to append them? Vertically or Horizontally?,318368.0
90142,375489.0,If you just want to merge three dataframe with different length you might find below link useful: https://stackoverflow.com/questions/33086881/merge-two-python-pandas-data-frames-of-different-length-but-keep-all-rows-in-out,317991.0
90142,375485.0,use append() as below to concatenate all the rows df1.appen(df2).append(df3),318804.0
90142,375519.0,"Append rows of other to the end of this frame, returning a new object. Columns not in this frame are added as new columns. df 1 = pd . DataFrame ([[ 1 , 2 ], [ 3 , 4 ]], columns = list ( 'AB' )) df 1 A B 0 1 2 1 3 4 df2 = pd . DataFrame ([[ 5 , 6 ], [ 7 , 8 ]], columns = list ( 'AB' )) df 1. append ( df2 ) #It appends the two Data frame df2 being appended at the end of df1 A B 0 1 2 1 3 4 0 5 6 1 7 8 Hope it helps !!",307843.0
90403,376988.0,"Since this is an assignment question any comment on this will reveal solution itself which is not advisable by TA. But to help you out I can give you suggestion - you have ""Movie+Assignment+Data"" excel file. You can apply all the steps which you have followed while doing it in python to this excel file to cross check your solution. I hope this will help.",317991.0
90403,377238.0,We have the explanation under 3_Handling_Missing_Data.ipynb. Please go through it once you should find the answer you are looking for I too came across this question :),300727.0
90854,378691.0,Read question again and again and then follow your intution. Ranking before sorting does not make sense :P,318770.0
90854,378701.0,"New column is always added to right most in DF. You can follow pattern below to change the position. Below code will bring mean to front. df = df[['mean', 4,3,2,1]] Rank function can be applied both before and after. That will depend on method parameter of the rank.",318368.0
90854,378733.0,"If you use rank function with respect to a certain column, it will assign same rank to the rows with same value. Hence you did not get 250 at the end in rank column. In this case you could add a RANK column with RANGE (1,251).(for this you have to sort the columns first and than add the new RANK column) To make Rank column to appear first, you can set RANK as index using set_index function. Hope it was helpful",318009.0
90854,378850.0,First sort and then add the column 'Rank'. Yo can simply use range() to generate numbers 1-250. df['col_name']=list of values,304319.0
90056,374948.0,It is advisable to do so. however if any task is being performed in a single line of code then you can skip that as question's comment will explain it. if a task is being performed in multiple steps then write your own comments.,311686.0
90056,374944.0,I'm doing and I advice you to do as well since it enhances the readability. Good luck.,310974.0
90056,375046.0,"In the solution stub we do have some comments in place. Do, we need to add more. For me it seems like the comment lines might exceed the coding blocks.",317689.0
90056,374949.0,"Its better to give our own comments wherever required in .ipynb file. Since it will enhance the readability as well as it depict the thought process applied by you on code. But be careful also not to over done the commenting, because it will make you code messy.",317991.0
90056,375252.0,"Putting comments will increase the readability of the code. It will help others to quickly understand the logic behind the code you have written. Also, it will help even you to infer the logic you have written when looked at the same code after sometime.",314730.0
90056,375419.0,"Comments should be given for each task that you perform when doing the assigment. Generally it imroves the qaulity of the code as well as let the reveiwer knows what you want to perform exactly with the code. Also in Pythn notebooks, generally you may also want to record your obeservation for eample duplicates were found and it was deleted. When you go down to Advanced topics in Machine Learning and run alogorithms, your obeservations are must. Hence the comments and observations goes hand in hand as per the task and activities allocated to us.",310482.0
90056,375334.0,"Look at comment coding as your summary. Which will come in handy when you are reviewing that section weeks or months from now. Although the coding you are going to do here, mostly like you will not look upon, but this practice and habit will come to your rescue in real life situation. Don't worry about the coding size, as the comments are not executable pieces, but rather a reflection and understanding the madness and mayhem your code will do as it runs :-)",312199.0
90056,375424.0,"Comments should be given for each task that you perform when doing the assigment. Generally it improves the quality of the code as well as let the reveiwer knows what you want to perform exactly with the code. Also in Python notebooks, generally you may also want to record your observations for eample duplicates were found and it was deleted. When you go down to Advanced topics in Machine Learning and run alogorithms, your obeservations are must as observations differs when running models. Hence the comments and observations goes hand in hand as per the task and activities allocated to us.",310482.0
90056,375625.0,"A very important aspect of coding is maintainability. Not just in terms of fixing bugs but also why some code was writte, When your is visited by some other developer(even by you), one should be able to read the code and comment and should be able to figure out why certain changes were done.",301649.0
90056,375980.0,"Not just from the assignment point of view, you will need to comment your code because when you take up these files later on, it will help you understand the code you have written.",317149.0
90663,378115.0,It is not recommended that you directly ask for an optimal approach here as it is a graded component. Your approach seems fine. There's just one small clarification. You don't need to use the rank() function anywhere. You just need to create a new column and assign the ranks to movies in the range 1 to 250 as it is.,306040.0
90663,378035.0,You can try this: Sorting and filtering can happen in a single statement. Store this is as a dataset Use rank function to add the rank field to the above dataset Hope that helps!,318085.0
90621,377959.0,I found below link useful for your question. Please go through it: http://py-tut.blogspot.com/2016/11/pandas-concat-and-append.html,317991.0
90621,378001.0,"Append is always get added at the last of dat free example df = 1 2 3. if you append 4 5 6 then it will be appended at the 2nd row of df syntax df.append(df2). 2. Concat panda along only one axix i.e either row or column example S1 = df(a , b) S2= df( c,d) then pd.concat( S1,S2) will result in a,b,c,d in one single column by default. 3. join method is string method whic return a string concatenation with iteration example,2,3,4 S2= a,b,c,d s1.join(S2) return 1abcd2abc3ab4a 4.merge it merge two dataframe with option left right on more like SQL operation of joings table think of every thing what you can perform to join two table is similar here",307843.0
90655,378037.0,numpy is primarily used for numeric arrays and series. pandas is used for dealing with tabular data (It is called dataframe in pandas terminology). A data frame is nothing but a collection of attributes and measures. It is similar to a relational database table.,318084.0
90655,378099.0,"NumPy basically deals with numeric arrays of any dimension whereas Pandas is used for working with dataframes or series. So if you see some sort of a list or matrix of numbers, you need to use NumPy and whenever you spot some tabular data, you need to work with Pandas.",306040.0
90655,378113.0,"As a rule of thumb, whenever you come across a case of a 2 dimensional csv or excel with headers kind of sheet data in tabular form, you should use pandas.For arrays or sequences of any dimenson, use numpy. However, there may be little exceptions to the above based on some indifferent requirement. The main motive is to analyze the data in best possible manner that suits you.",311729.0
90655,388737.0,"Numpy as names says it's Numeric arrays. Pandas mostly deals with Dataframes(Table) or Series(Column) So if you are dealing with lists, tuples, matrixes, n-dimensional arrays then go ahead and use NumPy. Where as if you spot some tabular data then need to go with Pandas. Hope this helps! Thank you.",318080.0
90656,378052.0,You can go through this link https://www.geeksforgeeks.org/monkey-patching-in-python-dynamic-behavior/,317991.0
90120,375322.0,I believe that wouldn't be a problem at all as the assignments are going to be reviewed manually. We can add comments to increase the readability.,310974.0
90120,375338.0,Yeah you can add additional comments and print as assignment wouldn't be auto checked by computer. It will be evaluated by the faculties.,317689.0
90120,375365.0,"Assignments will be manually verified by the faculty members. Try to provide information on why you have choosed to go with any specifc approach in the comments. Try to be more explanatory, which will give you adiditional marks as well.",318368.0
90218,376424.0,you may use loc or iloc to filter first 250 after sorting,300718.0
90218,377022.0,"You can leverage the .iloc function to the dataset after sorting your data. Although .loc function is a good option, but I prefer i.loc as it helps you to extract values on the basis of index rather than the values.",318085.0
90218,375836.0,imdb_score is one of the columns. we have to use this column and extract top 250 movies in a new data frame. the name to be given to this new data frame is mentioned in the task question.,311686.0
90670,378015.0,use drop_duplicate option.,318084.0
90670,378046.0,"It is an assignment related question. So I would advise , go through the question , do some reasearch on google as how can that be done and try first. If any error comes you can post on discussion forum, everyone will be happy to help you. I believe It is the best way to learn .",317991.0
90670,378132.0,Please don't ask direct questions related to the assignment on the discussion forum as it is a graded component. It would be better that you revisit the lectures or use Google if you're stuck somewhere. The assignment is a learning opportunity for you; please don't waste it.,306040.0
90625,377996.0,No. You just need to submit the ipynb file.,306040.0
90626,378002.0,"No, all the movies will have different ranks. Just create a new column in the dataframe and assign the values 1 to 250 to it. :)",306040.0
90581,377872.0,Yes ofcourse movies.,310974.0
90581,377936.0,Drop the duplicates inplace so that the changes are reflected in your original dataframe.,306040.0
90639,378042.0,No. It's not. It'll be done manually. Don't worry about additional statements.,319721.0
90741,378164.0,Do re-check all the previous tasks if all checkpoints have cleared.,318329.0
90741,378201.0,you might have missed something please check all the parts correctly,318017.0
90741,378222.0,"I all faced this issues, so i started solving the problem from first. so you will come to know where you have done the mistake. i think you have deleted the duplicate rows. please check once.",300726.0
90741,378479.0,You can open the csv in excel and do the same in excel to verify where your code is going wrong. Dataset is not too huge,317514.0
90741,379000.0,"While deleting the rows with null values, maybe you have deleted some extra rows. Please recheck the entire code. And keep inspecting the data after every step.",304319.0
90741,380080.0,"Check if you have deleted some extra rows during the data cleaning, Even if you haven't, don't worry. Just write the command for dropping the duplicates as the syntax carried marks. If your top10 dataframe matches the expected solution, you'll get full marks nonetheless.",306040.0
90224,375834.0,only ipynb file is needed as per the instructions given.,311686.0
90224,375850.0,We need to upload our ipynb file which the TAs are going to run against the movie dataset and validate. They will have the dataset. No need to upload.,310974.0
90224,376025.0,you need to add only your notebook file ipynb.,300708.0
90224,376089.0,Only ipynb file is needed.,301648.0
90224,377021.0,"Since the evaluation is manual, it is implied that the faculty would have a copy of the dataset. Just the Python notebook without compressing should suffice in my opinion.",318085.0
91020,379895.0,close the kernal and open the workbook again you can try saving it with a new name also,318017.0
91020,379896.0,"Hi Mahendra, Your questionis not verry clear. Are you not able to write any more code in the jupyter notebook after task 3.3? or are the commands not running successfully starting 3.3? Please rephrase your question withh clear details.",313826.0
91020,380228.0,Ensure you download the solution before you do any changes. to token,311803.0
91020,379925.0,your token might have expired - look in the command prompt window for the new token and enter that into the browser and your sessions should be back to normal and you should be able to save again. Happened to me as well.,300694.0
91024,379922.0,well try it .. but I don't think so,300694.0
91024,379962.0,"Hello Payel, Please find more information on spliting the dataframe. It will help you in section 3.6. Series.str. split ( pat=None , n=-1 , expand=False ) https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html",320195.0
91024,380166.0,Why using lambda ..simply split and assign new column with 1st element then assign second column with second element ..then check for null values and assign again,319869.0
91024,380010.0,"You can use split function inside your lambda function, df[your_1st_column] = df[movies].apply(your logic to split the string using lambda or whichever approach suits you) Same logic you can apply for the creation of 2nd column I hope this helps you out. for more information on split() function , refer this : https://www.geeksforgeeks.org/python-string-split/",301655.0
91024,380226.0,lambda is a good idea.. I used it but you may have to do it twice considering there is one more condition to be checked :),311803.0
91024,380883.0,"I have just used the split() method on string using below logic.... movies['genres1'] = movies['genres'].apply(lambda x: x.split('|', 1)[0] if ('|' in x) else x) movies['genres2'] = movies['genres'].apply(lambda x: x.split('|', 2)[1] if ('|' in x) else x)",319319.0
91024,381241.0,try this df['col1'] = df['colx'].str.split('|').str[0] df['col2'] = df['colx'].str.split('|').str[1],316323.0
90717,378071.0,yes create a new df were language != English,301115.0
90717,378098.0,it means you need to select all rows without english as for this data set English is Primary language as majority of data set approx 3000 rows are with English language,319869.0
90717,378627.0,yes you need to exrtact no english movies from top 250 movies by using (language != English) and create a new DF.,300726.0
90628,377965.0,This movie dataset assignment carries 150 points. https://learn.upgrad.com/v/course/208/session/19876/segment/101224,317991.0
90628,378007.0,150 points,319721.0
90629,377943.0,just check the number of rows you have in movies DF in Subtask 2.6 and devide it with the number of rows in movies DF which you got in the begining( just after reading the csv file) . you will get the proportion of the rows retained.,320073.0
90629,377952.0,We shouldn't check with hardcode values rt...any solution thru fuctions/methods?,318846.0
90629,378848.0,I uess it is just a checkpoint. It has to be done manually. No need to code for the same.,304319.0
90629,377989.0,It seems we need to pass the hardcoded value in this case.. confirmed by Upgrad team TA,300727.0
90629,378488.0,You store the maxrows when you first read the file in a variable and then use that variable latter on,317514.0
90630,377933.0,The file location will be taken care by the Upgrad team don't worry.. For assignments you can use your local path.,300727.0
90630,377957.0,You can use the file name relative to your . pynb file means give the path by assuming that d ata file is also in the same folder as the .pynb file.,320073.0
90062,374977.0,Use the one that are taught in the course. Those are more than enough.,310974.0
90062,374973.0,You can print all possible parameters.Try to Google and find what all params are useful for data analysis. it will help you more than directly telling you what is required 😊 There are many things for sure,317982.0
90062,375041.0,You can type the ones which are already shown in the notebook for these lectures.,317689.0
90062,376027.0,have used following describe() info columns shape,300708.0
90062,376563.0,There is a file. use that for reference,317269.0
90062,377755.0,"It is mentioned in the question what to print. use commands for info, columns, shape, describe",301644.0
90636,377950.0,yeah just execute the command or mention if u r not the reason why you have skipped it,317982.0
90636,378054.0,Just write the code even though it doesn't change anything. Some of the subtasks in the assignment might be present there just to check your syntax knowledge.,306040.0
90636,378161.0,"Even if it does not return any output, we should run the code to validate/check. In terms of this assignment, the checkpoints/requirments are mentioned. It is a good practise for own future analysis where we might write/run codes to check and validate as well.",319302.0
90616,377961.0,Hi. Please refer to the following link: https://stackoverflow.com/questions/42654934/need-of-using-r-before-path-name-while-reading-a-csv-file-with-pandas/42655010,306040.0
90653,377971.0,Correct.,320073.0
90653,377977.0,"check point 1:You might have noticed that we still have around 77% of the rows! that's after filtering,calculation should match the check point 1",301115.0
91041,380061.0,Use lambda fucntion to do it. df.apply(lambda x: x+1) hope this helps !,306735.0
91041,380056.0,You can just do movies[column name]=movies[column name]/1000,317845.0
91041,380053.0,"You can directly divide as follows: df['col1'] / 10 #to divide by 10 To assign it back to the same column, you could do: df['col1'] = df['col1'] / 10",313826.0
91041,380043.0,"You can follow below sytax: import pandas as pd test = pd.DataFrame([[4, 9],] * 3, columns=['A', 'B']) test A B 0 4 9 1 4 9 2 4 9 test['B'] = test['B'].apply(lambda x: x*5) test A B 0 4 45 1 4 45 2 4 45 For division you can use / in place of * in lambda function. Hope this will help.",317991.0
91041,380157.0,just divide the column it will impact each n every element,319869.0
90588,377885.0,Concatenation will create new Data frame but append will add to the same Data frame.,301120.0
90588,377928.0,"It depends on the requirement. Since Concat gives the flexibility to join based on the axis( all rows or all columns) Append is the specific case(axis=0, join='outer') of concat Source: https://stackoverflow.com/questions/15819050/pandas-dataframe-concat-vs-append",317991.0
90588,377946.0,"Hi, please check both the FAQs on the following page and you might be clear regarding the use of each of merge, concat, and append. https://learn.upgrad.com/v/course/208/session/19863/segment/101062",306040.0
90442,377172.0,Since we can't mention the actual code. The approach would be something lilke this: Apply the dataframe group function on the dataframe. Ensure the right column is selected as the groupby column Apply the mean and sort function cumulatively on the above result set seperately for critic and audience reviews seperately. You would get means for audience and critic reviews as a result Hope that helps.,318085.0
90442,377170.0,Got it... thanks:-),300726.0
90447,,nan,
91039,380112.0,"how about using NAN and assigning non Nan values back to data frame, check pandas notebooks",301115.0
91039,380045.0,Can you tell what exact error is coming.? No need to post code.,317991.0
91039,380163.0,Check videos of concerned session or check the syntax explanation for np isnan,319869.0
91039,380220.0,There is an example in the lecture as well,311803.0
90221,375829.0,Use absolute path and put the file in the same folder. Then you can simple load the file with the filename without path.,318368.0
90221,376562.0,Keep things simple. But there is no guideline i have come across. But your codes will only work if your database is properly loaded. So i guess no worries.,317269.0
89903,374050.0,"For dataset provided, if the language is not ""English"", it is considered as Foreign.",318368.0
89903,374097.0,"Ar you sure? Because there are other languages also there in the data set like Japanese and all. If i consider your criteria then "" Veer-Zaara "" is not considered as foreign film, and this is one of the check point in the assignment!",311741.0
89903,374142.0,"Other than English Language, any other language is considered as the Foreign Language for the given dataset",318804.0
89903,374241.0,use language != English as all other languages are considered as foreign.,318017.0
89903,375311.0,"The dataset is Hollywood centric and is constructed based on the primacy of English language films. So any non-English film including Indian, Asian and European films would be considered as foreign films as they are made outside the Hollywood film fraternity. So you need to choose Non-English as your filter for this question.",318085.0
90659,378036.0,When you want to append rows below another row use append and use concat for one dimensional concat of string syntax for append S1.append(S2),307843.0
90659,378029.0,"I think below explaination will help in understanding the use of one over other. They are all fairly similar in that they all facilitate combining two or more dataframes together, but each seem to be more appropriate at certain combining tasks. Append combines the rows of one dataframe to another dataframe. Concat can take a group of 2+ dataframes and combines the dataframes via the rows or columns. I tend to use this method fairly often and is the most efficient way to merge multiple dataframes, e.g. pd.concat([df1, df2, df3, ...]) Merge allows for more SQL-like merging of two dataframes, specifically merging on similar values in a column found in two dataframes. you can defined which type of join you want and also on what column.",320073.0
90659,378053.0,"well, based on stackover flow forum ,it depends on the scenarios and code location, overall ,append() can also be used to perform concat",301115.0
90557,377744.0,"in context of what? There are many places files can be uploaded in upgrad if in the context of graded assignment, then I imagine there is just one file - the ipynb file so no need to rar",300694.0
90557,377748.0,"If the query is regarding the Python Assignment, then you need to upload only the .ipynb containing the solutions.",313826.0
90557,377767.0,Submit the completed ‘ipynb’ file through the submission link below it's mentioned in assignment section,317982.0
90557,378133.0,Just sbmit the Python Jupyter Notebook file,301555.0
90557,378534.0,If there are more than 1 files than you are requested to zip it. In that case try .zip format rather than rar. For single file you can go with ipynb,318009.0
90557,378652.0,"The instructions on the Assignment: Submission page does suggest that if there are multiple files, we can zip them together and submit: Here are the steps that you must follow during submitting any assignment- If there are multiple files then collect all the files and compress them together in a zip file. However, for the current assignment as there is only one Jupyter notebook, we can perhaps submit without the need to zip it.",319302.0
90593,377908.0,"np.where(condition,x,y) it will return x if condition is true. It will return y if condition is false.",317689.0
90593,377929.0,"That's right, Arihant.",319721.0
90594,377951.0,"What is the dimension of [[a,b,c], [aa,bb,cc]] - 2D, right? 2x3 If I the elements, a,b,c,aa,bb,cc are like, [[k,j], [kk,jj]] - this is again 2D right? 2x2 Now there is a 2D element in a 2D array, now that makes it multidimensional. 2,3,",319721.0
90595,377956.0,1. You need to implement vectors and reduce the time. 2. The expected problem statement needs to be solved.,319721.0
90560,377738.0,Please check out the answer for a similar question here: https://learn.upgrad.com/v/course/208/question/90559,313826.0
90560,377743.0,"You can use syntax as follows: df.drop(['A' , 'B' , 'C' ] , axis=1, inplace=True) Hope it will help.",317991.0
90560,377745.0,"You can try like this, passing column names in a list, df = df.drop(['BuildingArea','YearBuilt','CouncilArea'], axis=1)",318328.0
90560,377800.0,"The syntax to drop multiple columns is quite similar to the way in which a single column is dropped. For multiple columns we pass the list of column names to be dropped as the argument, instead of a single column name. Syntax for dropping single column: df = df.drop('Col_1', axis=1) Syntax for dropping multiple columns: df = df.drop(['Col_1', 'Col_2', 'Col_3'], axis=1) Or cols_to_drop = ['Col_1', 'Col_2', 'Col_3'] df = df.drop(cols_to_drop, axis=1)",318334.0
90560,377803.0,"You can drop the column from the data frame in different ways Link of the CSV for reference https://s3- eu -west-1.amazonaws.com/shanebucket/downloads/FAO+database.csv # Delete the ""Area"" column from the dataframe data = data.drop(""Area"", axis=1) # alternatively, delete columns using the columns parameter of drop data = data.drop(columns=""area"") # Delete the Area column from the dataframe in place # Note that the original 'data' object is changed when inplace=True data.drop(""Area"", axis=1, inplace=True). # Delete multiple columns from the dataframe data = data.drop([""Y2001"", ""Y2002"", ""Y2003""], axis=1)",317845.0
90560,377861.0,df = df.drop(columns=[list of columns]),318329.0
90560,377915.0,"In syntax, parameter 1 = columns as list and parameter 2 should be (axis=1)",320103.0
90560,377925.0,"df.drop(columns=[list of columns], axis=1, inplace=true)",316041.0
90560,378532.0,"In any of the Pandas command if you use a LIST i.e. squared brackets '[ ]', it will consider more than one variable. Be it 'drop','sort' or any other functions, use squared brackets to add multiple variables",318009.0
90666,378045.0,"group by can handle multiple columns where as mean cannot handle multiple columns in a single statement of(group by,mean,sort)",301115.0
90666,378025.0,"Yes you can. Logically the output of one function is an input for the second function, so applying 'group by, sort and mean' in one statement should work",318085.0
90666,378058.0,yes you can use ..,319869.0
90666,378119.0,"Yes, you can always cascade the functions together in Python. :)",306040.0
90666,378097.0,"yes you can. however it has to be in order of first groupby, then mean and lastly sort to make it meaningful.",311729.0
90666,378068.0,It is very common scenrio to have some aggregated data (mean and group by) and which is sorted. It can be done in pandas in 1 statement. df.groupby('column_name')[['column_name2']].mean().sort_values(by='column_name2') Here data is grouped by column_name mean of column_name2 and is sorted in ascending order of mean of column_name2,317689.0
90666,377994.0,Yes you can use.,313826.0
90666,378004.0,"yes, lets take dataframe df. df.groupby('movie')[['ratings']].mean().sort_values(by='ratings', ascending=False)",318319.0
90607,377982.0,"Can you specify what sub-task are you referring to? Understand that, the data which is cleaned, only should be used for the analysis.",319721.0
90611,377911.0,Do we need to find this particular title or just inspecting in the dataframe is OK ?,317689.0
90611,377923.0,"If it is present in the dataframe, then you don't need to worry. You'll get full marks for it. You don't need to explicitly use the said conditional indexing to show the movie is present in the dataframe. The checkpoints are just present to give you a sense that you're going in the right direction. :)",302827.0
90611,377927.0,"If it is present in the dataframe, then you don't need to worry. You'll get full marks for it. You don't need to explicitly use the said conditional indexing to show the movie is present in the dataframe. The checkpoints are just present to give you a sense that you're going in the right direction. :)",306040.0
90612,377916.0,I think you have to submit the .ipynb file only. No need to send the data set . How do I submit the assignment? Upload your ‘ipynb’ solution file in the ‘submission’ section on the next page. this was there in the instruction.under Assignment: Problem Statement section.,320073.0
90612,377954.0,You don't need to submit the dataset as we already have that with us. Just submit the ipynb file and we'll take care fo everything else. :),306040.0
91074,380321.0,"Hello Abhishek, 1) Why you are using columns here? 2) Please check tutorial video for defining list. 3)You can print info. 4) for round, Please refer tutorial video one more time. It is very simple. You can do it.",320195.0
91074,380456.0,"hey Abhishek, you don't need to specify the key word 'column =' because that is taken care of when you pass the second argument as axis=1 so just use the code as; movies.drop(['columname1','columnname2','columnname3'], axis=1) that's it.. and the columnd will be removed.. hope this helps..",316349.0
91074,380510.0,"The column 'actor_1_facebook_likes' has been defined twice in the list of columns and hence you must have got an error. There is no harm in mentioning the columns parameter , infact it is a good practice to pass arguments by reference as this increases the readibility of the code. The rest of the code looks fine.",313826.0
91074,380721.0,When I tried to run your code I got an error stating that drop() got an unexpected keyword argument 'columns' That means you don't need to mentioned 'column' in drop() if there are more than 1 column. Beacuse when I ran your code by removing 'columns' from drop() it works perfectly. So just remove 'columns' from drop(). Hope this will help.,317991.0
91074,380767.0,"Your code is absolutely fine as you have mentioned same column name two times. you will get an error. to avoid that you can use df.drop(columns=['column_1','column_2'],axis=1,inplace=True,errors='ignore') if you pass errors='ignore', your code will not throw an error for missing column",317845.0
91074,380770.0,"It is the problem of pandas version. It is clearly mention in documentation that 'columns' parameter has been newly added in version 0.21.0. Check the version of pandas installed in your python notebook as follows: import pandas as pd print(pd.__name__, pd.__version__) If it is less than 0.21.0, 'columns' will not work and you have to remove it from drop(). For verification you can go through below link: https://github.com/pandas-dev/pandas/issues/19078 Hope it will help.",317991.0
90615,377953.0,You can check below link to get insight into this https://datascience.stackexchange.com/questions/37878/difference-between-isna-and-isnull-in-pandas/37879,317991.0
90722,378106.0,If it's not specified you can drop them altogether,319721.0
90722,378219.0,"Drop all the column, which are specified in the problem statment.",300726.0
89938,374460.0,I think its part of assignment and we should not discuss this,318358.0
90723,378111.0,"You might notice that two of the columns viz. num_critic_for_reviews and actor_1_name have small percentages of NaN values left. You can let these columns as it is for now. Check the number and percentage of the rows retained after completing all the tasks above. After this, Get the percentage of retained rows by dividing the current number of rows with initial number of rows",319721.0
90723,378110.0,its a check point for retained rows after cleaning data. check notebooks for the syntax,301115.0
90721,378102.0,convert the budget into millions and update the df $ is not present in data frame,301115.0
90721,378112.0,do we need to change the datatype??,311004.0
90721,379254.0,"i believe, this mean converting the value present (assumption: which is in USD) to convert into million USD. say if column has value 2700000 then it became 2.700000. this is my understanding.",306005.0
90724,378116.0,"It's already mentioned, right? Please elaborate your doubt and be specific.",319721.0
90724,379402.0,"As mentioned in the subtask, create 3 data frames for 3 actors. Use concat or merge or append to combine all the 3 data frames created. Group the combined data frame on column 'actor_1_name'. Find the mean on two columns individually and verify your answer by checking to the check point.",305843.0
90725,378114.0,mean on seperate columns on the grouped data frame,301115.0
90725,378217.0,Mean on critic reviews and Audience reviews column together for combined data frame.,300726.0
90726,378224.0,"profit = gross -budget, then sort with profit column.",300726.0
90726,378117.0,profit = gross -budget then sort and pick top10,301115.0
90727,378128.0,"Hey, I understand that you have doubts regarding the problem statement. But, asking for a brief explanation for every segment of the problem statement is not going to help you. So, first go through the entire statement, and try to understand it. If you have any specific doubts then, please post them. Again, what in the problem statement in section 3.5 do you not understand?",319721.0
90727,378257.0,"The objective of the assignment is to perform data cleaning and analysis for the excel/csv file which has 5000+ rows of records using Python Jupyter notebook. Instead of using excel to filter, sort, group, the entire operation will be performed using Python Pandas and Numpy libraries from Jupyter notebook. The learnings from Python for Data Science will be applicable here as both Pandas and Numpy libraries are needed. So start the assignment once you complete Python for DS course.",301644.0
90727,378227.0,"Find the best directors with director_name column, take the mean of IMDB score and sort them.you will find ur answer.",300726.0
90822,378522.0,The assignment notebook and the related dataset are present at the bottom of this link: https://learn.upgrad.com/v/course/208/session/19876/segment/101117 The questions are present in the assignment notebook itself.,313826.0
90822,378523.0,If you're looking for Python assignment; it is a zipped file placed in the 'Assignment: Problem Statement' section of Python Assignment Module. You need to download both the data excel file and the Python jupyter file. And then open it in the Jupyter Notebook. Complete your assignment in the same Jupyter file. Hope this helps. :),316349.0
89934,374404.0,I don't think it would be automated since detailed feedback is going to be provided on the submitted file but it doesn't harm to put those extra print statements for readability.,310974.0
89934,374408.0,800+ people in the course. I don't think they will evaluate it manually.,314547.0
89934,374825.0,"It is manual evaluation of Assignments and Case Studies. You can add your logic or understanding or anything else you want, just ensure to comment it and your end result is not damaged or altered.",301555.0
89934,376029.0,possibly there would be some auto code review tool which reads our file and prints out the score.,300708.0
89934,377002.0,The answer would be verified manually by the faculty staff. You can also add print statements to add clarity and explanation to your code. I had a smilar query to yours.The below link is the confirmation of the same. https://learn.upgrad.com/v/course/208/question/90120,318085.0
90956,379833.0,Ensure that both the operands are of the same data type.,304319.0
90956,379363.0,"Can you please post the question. Just one thing makes sure before you are doing any mathematical operation ensure to check the data types of both the operands. type(operand) you can use, to ensure you are not trying to add float and string value. eg , 100+""%"" would not work",318554.0
90956,379394.0,Make sure you are converting the unit of both columns[gross and budget] before doing mathematical operation.,305843.0
90956,379420.0,"Error says, you are trying to add two values, in which 1 value is string and 1 is float. Try printing column type of dataframe and check why values are string. You can convert string to int or float using. df['x].astype(float)",318368.0
90956,379485.0,"Here Two things needs to be taken care: At task 2.3, you have removed rows with missing vales in gross column and budget column i.e. having &gt; 5% missing values Then At Task 3.1 you have converted units if above two are correct, then simply profit will be as :- df['c']=df['a']-df['b']",317811.0
90633,377932.0,I think we need to retain the genres column as there is not specific instruction that it needs to be discarded.,313826.0
90633,378033.0,Retain them.,319721.0
90598,377970.0,"Make sure that you are doing this after dropping unnecessary columns mentioned. The other columns are significant. So, you better not drop these rows. If it's specifically mentioned, then you can drop them.",319721.0
90598,377902.0,"In my point of view there are two possibility: 1. If that 'insignificant' column is used to perform data analysis then you should consider that column and consider it as not duplicate. 2. If that column is not considered for data analysis, you can remove that column and then you can consider rows as duplicates. It all depends on the need of data analysis.",317991.0
89022,369204.0,Problem statement is defined in the very same page from where you downloaded the files: https://learn.upgrad.com/v/course/208/session/19876/segment/101117,310974.0
89022,369393.0,The tasks to be performed are given in the 'ipynb' file only as 'Markdown' of Comments.,311686.0
89022,377756.0,Load both python file and the csv file into Jupyter notebook. The questions for assignment are given in the python file. Open that and work out each question. The file mentioned where to write the code for each question.,301644.0
90250,376026.0,you need to open the file in jupyter. In your command prompt type jupyter notebook . It will show you a webpage (your local host). Open that link. you can open your ipynb files in the web page.,300708.0
90250,375983.0,That file is a .ipynb file that can be open using juptyer notebook. Try to open it in jupyter notebook. Hope it helps.,317991.0
90250,376051.0,Open Jupyter notebook. If path is set properly then from command prompt type jupyter notebook and it will open a webbrowser. There you can browse the ipynb file for the assignment. And then you can work on that.,318554.0
90250,376018.0,Guessing here - It could be that the default programme is set to word for ipynb file in your system.,317149.0
90250,377014.0,"Here are the steps to open the iPython (i.e. ipynb) notebook file: Launch Jupyter Notebook application Once started, your browser would be redirected to ""http://localhost:8888~"" Browse towards the location where you have saved this file Select the file and your notebook would be opened in a new broswe tab Hope that answers your question.",318085.0
90608,377983.0,Overwrite them.,319721.0
89611,372039.0,"Hi Anuj, u can run this code on python termial to convert to .py file jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb check if that hepls u out :)",317822.0
90103,375201.0,Yes. It result should be descending order of imdb_score and rank for highes imdb_score should be 1 and so on...,318368.0
90816,378495.0,you can use sort_values(by='column_to_sort') method to sort the values.,318329.0
90816,378530.0,Thank You.,309452.0
90816,378511.0,you can also apply nlargest on mean dataset,317514.0
90816,378970.0,You can cascade the mean function with sort_values() function . Eg ...mean().sort_values(ascending=False),304319.0
90816,379109.0,"I am not getting Damien Chazelle. mean my codes are wrong. Presently, used groupby for director name. However, while finding out top 10 directors, used movie data and not grouped data by director name. Please let me know where i am wrong.",311006.0
90816,378504.0,you can continue your code with sort code.. but just adding a dot and sort_value followed by your arguments inside the sort.. or else you can name a datafrane and then can do sorting on the new dataframe.. either way sorting will give you the same results..,316349.0
90966,379578.0,"We need to download the .ipyb file provided in the assignment module and write our commands in the same file. Once you are done, upload the same file in the platform. Good Luck..",316202.0
90966,379468.0,We need to submit the same .ipyb file provided along the assignment.,318368.0
90966,379461.0,answer in the same script file and submit the same when you're done.. :),316349.0
90966,379473.0,Ww have to write all commands in same .ipyb file provided in the assignment and then submit it,317811.0
90966,379684.0,same jupyter notebook that we downloaded is to be used for the submission with our reapective codes.,319302.0
90966,379933.0,"Before submitting the file make sure you download the complete coded file from Jupyter notebook as File --&gt; Download as option and choose .ipynb extension file as that will be updated file. The initial file in your folder which you uploaded before start of the assignment does not get updated as you save the file while coding, so we need to download post the coding is done. Thanks",310210.0
90830,378580.0,1. You need to drop duplicate rows entirely which are in an exact match. 2. Then you need to repeat subtask again to fetch the top row.,301648.0
90830,378540.0,"You need to drop duplicate values not just based on the profit column but the entire data row, meaning if all the column values for a particular row match that of another row/rows, then you need to drop the duplicate rows. Hope this makes it clear.",310511.0
90830,379247.0,"After Top10 movies are encountered, there lies a duplicated profit value . On Inspecting the profit value again , and all of its columns , every other values are similar. So that particular row itself is duplicated. Remove that using your logic. Even that logic can be specific ,may be like using only 'profit ' subset.",318557.0
90830,379044.0,I dropped the duplicates from the Top10 dataframe itself.. after this it worked. assuming this is the way to go,312199.0
90830,378942.0,i have gone thru the steps of drop_duplicates. I confirmed that numbers of rows are reduced after running this command and still see duplicates in the top10.,312199.0
90830,378972.0,You need to drop the entire duplicate row which can be done using drop_duplicates(). Then inspect the top 10 rows. Check that the duplicate row which was present earlier does not exist anymore.,304319.0
90830,379046.0,drop duplicates and re-assign data to Top 10,312199.0
90830,379642.0,"We need to drop rows which are duplicated for all columns. Selecting specific columns would lead to skewed result, e.g. name of movies - there are multiple movies with same name, director - directors have directed multiple movies, profit - movies can have same or similar profits, etc.",319302.0
90830,378568.0,"Clear all the duplicates from the datafram. You can use the drop duplicate function for the same. After dropping all the duplicates, your number of rows will reduce. Perform the top 10 code for getting new list of movies in top 10",318009.0
90973,379573.0,A KeyError exception is raised when the operation that you are performing is looking for a key (in this case the column 'genres') and it is not found.,313826.0
90973,379559.0,You need to split the values in column genres into a 2 new columns genre_1 and genre_2. Please make sure you understand this before proceeding with the the split. Good luck.,310974.0
90973,379575.0,"Can you explain what was your query for which you are getting this error. Because ""genres"" is available in the data set and we need to split it in to 2 new columns (genre 1 and genre 2) based on the seperator character mentined.",316202.0
90973,379775.0,You are getting this error because in the current data frame column 'genres' is not available. Try printing the data frame and check if you have genres column. you can split function to convert one column to many columns. You can read more at https://apassionatechie.wordpress.com/2018/02/24/how-do-i-split-a-string-into-several-columns-in-a-dataframe-with-pandas-python/,317845.0
90973,379776.0,How are you accessing the column 'genres'? You can access it using df['genres'] or df.genres.,304319.0
90973,380857.0,"I have just used the split() method on string using below logic.... movies['genres1'] = movies['genres'].apply(lambda x: x.split('|', 1)[0] if ('|' in x) else x) movies['genres2'] = movies['genres'].apply(lambda x: x.split('|', 2)[1] if ('|' in x) else x)",319319.0
90973,379795.0,"1. Below command will create two new columns col1 and col2 in dataframe_1 and fill these column's values from splitting the values from unplittedcolumn ie. mention the name of column and str.split will help here. dataframe_1[[col_1','col_2']]=dataframe['unplittedcolumn'].str.split('delimiter', n=1, expand=True) 2. After this you will see that first genre is in col1 but rest of the genres is in col2. 3. Now you need to keep only second genre in col_2 column for each movie, Again use above command dataframe_1['col_2']=movies['col_2'].str.split('delimiter', n=0, expand=True) 4. For movies having only one genre (i.e. having col_2 value missing), # for these movies, imputing col_1 in col_2 using df.loc and selecting rows inside where col_2 is null for col_2 column and impute col_1",317811.0
90976,,nan,
90627,378410.0,sorry. my earlier comments addressed wrongly to Ashish ; It should be for Rushi. Apologies !,311115.0
90627,377945.0,"In the second syntax df.loc[pd.isnull(df['fruit']),'fruit'] = 'mango' , u are locating exactly the place and rewriting it with mango,so the data type of other columns not changed where as in the first one df['fruit'].isnull() returns the row index which are null ,for the rowindex which are returned using df[rowindex] u are assing mango so for that row all columns are assigned as mango(which is object type) Hope this helps.",301115.0
90627,378023.0,"Hi Ashish, Even after running the syntax shown I have the null values if u observe.. only the language column is getting updated to 'English' and Ican also see other foreign languages when used that syntax.. I'm still not clear with your answer. Please clarify",318013.0
90627,377948.0,"1. df['fruit'].isnull() will return boolean values for each element in the'fruit' column. True for a null value and False for a non-null value. &gt;&gt;&gt; df[df['fruit'].isnull()]='mango' --&gt; This will assign 'mango' to all the columns of the data frame where the row of the 'fruit' column was a null value. Other columns which had float values will now also have a string object as it's element. Thus, the datatype of the whole column implicitly changes to object. 2. &gt;&gt;&gt; df.loc[pd.isnull(df['fruit']),'fruit'] = 'mango' --&gt; In this case you have written 'fruit' inside the square brackets. So this assignmnt task will only deal with 'fruit' column. This will assign 'mango' to the 'fruit' column only , where the row of the 'fruit' column was a null value. Notice in the screenshot, which I have provided. Column 'A' had elements of mixed datatypes, namely- int, string. So everything was implicitly converted to string. This is because each column of a DataFrame is a Series. And Series contains elements of same Datatype. Also, the columns having string values will be displayed as dtypes: object, when df.info() is used.",301652.0
90627,378408.0,"Hi Ashish, Even though your language count showing correctly( after replacing isnull() values) - In this case, you are supposed to replace only the language column having 'NaN' values, and not all the columns for the entire row with 'English'. The command movies[movies['language'].isnull()] = ""English"" will replace all the columns in that row with a value 'English'. You should be using 'loc' command to replace only the particular column value 'language'. TIP:: Rerun from the beginning and check for the row indexes 4958,4110 and 3086 and see the results.",311115.0
90622,377986.0,"Can you please elaborate your question? And also share a screenshot of the code and the error, if possible.",306040.0
90845,378634.0,Check your output. It was mentioned to extract first 2 genres from the column and store in new columns. But from your output I see that the first genre was correct but the genre_2 output is not.It should have only one value.,318804.0
90845,378643.0,just split the generic column by using str.split('|') .after split assign the first value to genre1 and second to gener2. then you can see some rows have NAN vauels in gener2. for that u have to replace genre_1 vaules to genre_2. hope this helps you.,300726.0
90845,379025.0,"After splitting, assign the frst value to genre1. Check whether the second value exists or not. If it does not exist copy the first value to genre2 also, otherwise assign the second value to genre2. This can be done using the apply(lambda function) on the series obtained after splitting 'genres'",304319.0
90847,378649.0,you need to drop the rows which have Null percentage greater than 5. go throught the perivous output and see which all columns have null value more then 5%. then drop those rows.,300726.0
90847,378640.0,First you need to findout the columns having Null percentage greater than 5. Then Fetch the row indexes for all the rows having null values in those columns. Then drop the rows using the row index. Hope this helps,318804.0
90847,378669.0,"As stated, on inspection you might notice that some columns have large percentage (greater than 5%) of Null values. Drop all the rows which have Null values for such columns. So, all the rows in the columns with &gt;5% nan values need to be dropped. I would suggest to go through 3rd video of below link for more clarity. https://learn.upgrad.com/v/course/208/session/19864/segment/101071",317991.0
90960,379389.0,As per assignment we need to fill the genere2 with Genere1 in case genere1 is null. For that use of fillna Can be used and to find Null is Isnan Can be used,307843.0
90960,379415.0,Think of problem as assigning values to null values in a column from another columns data.,318368.0
90960,379453.0,"If you've splitted the genre into genre1 and genre2; crack a logic to impute the null values in genre2 with genre1 and this is already covered in session.. Just a hint, your left hand side should drill down the rows to show only the missing values rows in genre2 and the Right hand side should contain a code to impute the values from the genre1.. hope this helps..",316349.0
90960,379492.0,"yes. Finally figured it out. My challenge was TWO fold 1. syntax of the function calls 2. Jupyter kernel, had to stop and restart to clear all the cache built up for it to behave. Thank you all for your help.",312199.0
90960,379471.0,"1. Below command will create two new columns col1 and col2 in dataframe_1 and fill these column's values from splitting the values from unplittedcolumn ie. mention the name of column and str.split will help here. dataframe_1[[col_1','col_2']]=dataframe['unplittedcolumn'].str.split('delimiter', n=1, expand=True) 2. After this you will see that first genre is in col1 but rest of the genres is in col2. 3. Now you need to keep only second genre in col_2 column for each movie, Again use above command dataframe_1['col_2']=movies['col_2'].str.split('delimiter', n=0, expand=True) 4. For movies having only one genre (i.e. having col_2 value missing), # for these movies, imputing col_1 in col_2 using df.loc and selecting rows inside where col_2 is null for col_2 column and impute col_1",317811.0
90748,,nan,
90963,379412.0,Print dataframe column datatypes using df.dtypes You can only apply function isnan on native datatypes but not on object types. You can cast / change type of the object data to Float or Int or any other data type.,318368.0
90963,379475.0,np.isnan is used by importing numpy as np.,317811.0
90963,379583.0,"Please use ""isnan"" with numpy. e:g - np.isnan() . Based on your question, you might of used only isnan. Let us know if it works.",316202.0
90963,380450.0,np.isnan would work on native types but not on objects. Try doing it using it some other way if you are trying to delete or change certain rows containing Strings. You can also try casting.,318499.0
90962,379414.0,"Yes, Mean only works on numeric data types. Once you do the mean along column, it will give you mean of all the numeric columns data.",318368.0
90962,379478.0,"Yes, It will show on the columns on which group by is done plus the column on which you are applying aggregate functions after group by",317811.0
90962,379586.0,"In 3.5, you would have grouped it by the required column as per the question and would have applied the mean function to the column as per the question. Only those will be shown in the new dataframe.",316202.0
90751,378175.0,You can retain only the rows which are non null by checking the sum along axis =1 with 0 and store them in as a df. df= df[df.isnull().sum(axis=1) == 0],318329.0
90751,378228.0,"the problem statment its says :-rows might have greater than five NaN values so you have to drop only those row, not all rows which have null value.",300726.0
90751,378288.0,For removing the rows from your dataframe; 1. You need to filter rows with greater than 5 null values thrrough using a condition with '&gt;' sign in your code 2. Then you check the statistics of the data to see if the less than 5 null value rows can be imputed with some number like mean or anything 3. Still for some null value rows you categorize the data and then imoute it again in the missing value columns these are the procedural steps which helps in getting rid of missing value rows. Code you can refer in the Jupyter file. Try to relate it will the steps and it should be easy in understanding...,316349.0
90751,378307.0,"Another way to look at this problem is that it is asking us to retain only those rows which are having less than or equal to 5 null values . A similar example has been explained in "" 3_Handling_Missing_Data "" notebook under "" Module 3 : Python for Data Science --&gt; Session 4 : Getting and Cleaning Data "".",313826.0
90751,378995.0,Retain only the rows which have less than 5 null values. You can do it by cascading the isnull() and sum() function. Refer to the Session : Gettinng and Cleaning Data,304319.0
90753,,nan,
89640,372301.0,"That should not be the case ideally, you should get some rows as the following sub tasks also need to be completed. Refer to the checkpoints for more clarity.",310974.0
89640,372318.0,I think it is possible to have such a case. Even then you should write the code to perform the mentioned tasks and move ahead accordingly. A similar situation might not be there for another dataset so handling such a situation is necessary.,311686.0
89640,373446.0,"Hi Ranip, I guess you have deleted all the rows whose contains more the 5NAN.. But question says you need to delete all the rows whose columns has more than 5% NaN.",311004.0
90632,377937.0,It should be movies..,300727.0
90647,378050.0,You don't have to print anything when you group the dataframe by 'actor_1_name',319721.0
89841,374346.0,Checkpoint 1 of 77% cleared even though sub task 2.4 returned 0 rows for me. Anything wrong here?,310974.0
89841,373457.0,"Hi Babita, Output for the Subtask 2.3 shoul be correct to achieved the results of subsequent tasks.",311004.0
89841,373483.0,"Hi Pooja, Thanks. passed the firsr check point percentage of 77%.",307494.0
89841,377447.0,Even I got the similar result after Checkpoint 1 of 77% passed for me. I think that's correct,318329.0
90776,378280.0,"Use the Range concept to rank your dataframe starting from 1.. Now, here since you already know the number of rows; you can directly specify the second argument that range takes or else you could use 'len(dataframe.index) which will find the last row and impute the Ranks in the sequence..",316349.0
90776,378276.0,it is a assignment question so i cant answer directly but you can do one thing you can setup a index column for it,318017.0
90776,378501.0,"if you have the array sorted,select the datadrame, rank has a method first - first: ranks assigned in order they appear in the array",317514.0
90776,378624.0,first sort the imdb_score score column and extract top 250 movies. Then you create new rank column by using rank function on imdb-score column of top 250 movies.,300726.0
90776,378983.0,dataframe[new col]=the values you need to assign. You can use range function to do so.,304319.0
90654,378094.0,"You can refer to the following link: https://www.pythonforbeginners.com/basics/list-comprehensions-in-python And you can, anyways, always check out the official Python documentation: https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions",306040.0
92490,389599.0,The file you are trying to open is a .ipynb file but you are opening it in Adobe Reader.I think that is the problem. You can open this file in notepad or launch jupyter notebook and then open the assiggnment file.,317460.0
92490,390945.0,"Hey Vimalan, you can also open the file from the Jupyter Notebook. You launch the Notebook and open the assignment file from there.",319721.0
90794,378425.0,"Formula is correct. If it is giving zero means , It depends on the dataset what you are applying on.",318804.0
90794,378370.0,I tried your code. It worked as expected.,301652.0
90794,379033.0,"You can simply use : df['x']=round(df['x']/1000000,2) Maybe you are executing the same cell multiple times. So the apply function is getting applied multiple times which is why the values are becoming 0. Run all the above cells again and then run this cell only once. It should work.",304319.0
90792,378360.0,"Can't directly answer that but whether we have it or not, don't worry about it. Just, write code for removal if there are any.",310974.0
90792,378364.0,yeah.. its weird .. mt code doesn't find any data set matching the criteria. thanks though for your response. wanted to verify if I'm on the right path. Cheers!!!,312199.0
90792,378388.0,Since it is a question related to assignment I can't directly answer. But what I would suggest that go ahead and write code for next two subtask i.e Subtask 2.5 and 2.6. At the end there is checkpoint as follows: Checkpoint 1: You might have noticed that we still have around 77% of the rows! If you are getting this checkpoint your solution for 2.4 is correct. Hope this helps.,317991.0
90792,379792.0,"No there are no such rows, better write code to check the count of such rows, if exists then only drop else not required",317811.0
90798,378394.0,You can do it simply by subtracting two column for the movies dataframe and put the result into another new column.,301648.0
90798,378500.0,"Yep, Subtract the 'budget' column from 'gross' and store it in new column 'profit'",318329.0
90798,378420.0,"yes ,find the difference of two columns and assign it to new column",318804.0
90357,376671.0,Just sort them based in IMDB first and then assign rank. Nothing as such mentioned about movies with same IMDB score.,314547.0
90357,376692.0,we're just expected to rank the movies based on the scores.. here we'll be using the logic of adding a new column which follows the sequence starting with 1 till the last index of the dataframe..,316349.0
90357,376722.0,All the Movies with same imdb_score should have same rank with min value assigned I closely observed Upgrad's learderboard ranks and they follow the same pattern and hence I assume we should follow their methodology,306248.0
90357,376794.0,I think that if the scores are same then they have to be assigned the same rank.,313826.0
90357,376867.0,You can simply choose to assign rank in ascending order first come first serve basis.,318368.0
90357,377807.0,"Here, you are asked to extract top 250 number of movies according to the IMBD score. But if you assign the same rank to movies with the same score, then more than 250 movies will have ranks from 1-250. So, it's better when we have no two movies with the same rank.",319721.0
90775,378278.0,you can split a single column into multiple column based on the delimiter,318017.0
90775,378283.0,I don't remember if this was covered in the module anywhere trule speaking. But i used google to see how a column content can be splitted into multiple columns and then we can work on the other part as Proffesor has taught us..,316349.0
90775,378620.0,just split the generic column by using str.split('|') .after split assign the first value to genre1 and second to gener2,300726.0
90775,379016.0,"Split the column using split() with the delimiter as '|'. Assign the first value to a new col 'genre1'. Check for whether the second value exists or not. If it is there, assign it to 'genre2', otherwise copy the 1st value to 'genre2'. This can be done using apply(lambda function).",304319.0
90805,378419.0,"Yes , you need to execute from the start else you will not get the result what you are expecting",318804.0
90805,378418.0,select the last cell where u finished and select the option execute above all,301115.0
90805,378609.0,The assignment notebook will be automatically saved. just select the last cell where u have last saved and select the option execute above all.,300726.0
90805,378978.0,You need to execute the entire code. Go to the last cell and select the option execute all the above cells.,304319.0
90805,378574.0,"The notebook you are working on is automatically saved in the default directory (where your jupyter is installed). Hence even if you close your file, the code will be in place when you open next day. Once you open a notebook again, you need to rerun all the cells. You can press 'shift+entre' on each line or can click on the last line from where you wanna start working and click 'execute all above.",318009.0
89881,373940.0,Yes. 237000000 in million would be 237.,318368.0
89881,373877.0,Yes. (assuming 237000000 is in $),317998.0
89411,370910.0,"Yes, you will have to write code as required, add comments if any and can submit same file along with its dependencies like data file in this case as a zip.",318329.0
90035,374947.0,"Check this https://stackoverflow.com/questions/18640305/how-do-i-keep-track-of-pip-installed-packages-in-an-anaconda-conda-environment. It helped me before. Typically, if pip is provided by conda env in your laptop, you shouldn't see this problem. But if you have pip installed standalone, you see this, The above url tries to fix it.",310974.0
89418,370969.0,No. Its strictly Python3,306248.0
89418,370977.0,"No, since the syntax of several functions is different in Python 3 from Python 2.",314547.0
90030,374867.0,I am able to download both the files just now. Refresh the page and see if it helps.,313826.0
90030,375047.0,Try refreshing the page. Sometimes it is an issue. If it persists try clearing your cache or open it in incognito window.,317689.0
90030,375269.0,You can also try changing your browser. Works on Chrome.,318499.0
90289,376476.0,"I think the Rank function works differently, right? it doesn't rank sequentially basically the Rank column is just a column containing sequential numbers when the dataframe is sorted by IMDB ratings in descending fashion. think of a way to just add a new column to the dataframe and assign sequential numbers to the column 1 to 250",300694.0
90289,376299.0,"Hello Umesh, IMDb_score is one of the columns. You have to use this column and extract top 250 movies in a new data frame. You need to give name for new data drame as per given in the task question.",320195.0
90289,376305.0,You can use rank function of dataframe. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rank.html,318368.0
90289,376677.0,Create a new column and assign rank to it.,314547.0
90289,376701.0,Use pandas rank function https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rank.html,306248.0
90289,377019.0,"Pandas hasn an inbuilt function to assign rank to a dataframe with regards to a defined criteria. DataFrame.rank(axis=0, method='average', numeric_only=None, na_option='keep', ascending=True, pct=False)[source] You can use parameters like axis, ascending or method to determine the ranking functionality for your dataframe",318085.0
89885,373919.0,What is the exact error you're seeing?,310974.0
89885,374351.0,identify those column and using dropna you can proceed by passing in subset columns which have value less than equal to 5%.,318426.0
89885,373967.0,check the code output in bits and pieces you will understand better where it is going wrong.,318804.0
90421,377061.0,"to remove duplicate data apply the below command : df.drop_duplicates(keep = False , inplace =True) this command helps in removing duplicates from and dataframe and inplace will change the database itself so tgat changes that you make remains there in future also.",318017.0
90421,377072.0,You might be not assigning update DF to original df after removing df. df = df.drop_duplicates(),318368.0
90421,377086.0,"Hi, the movies.drop_duplicates() if you dont pass any command in the brackets - it will not delete the duplicates instead it will show you a copy without duplicates, and if you will show movies dataframe again- duplicates will still exist. Instead try using inplace=True - which will delete the duplicates permanently. for more reference- https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html Hope this will help..",305129.0
90421,377988.0,May be you are not assigning the values to original data frame after executing duplicates command,319869.0
90421,378126.0,I wasn't getting duplicates in top 10 profit movies. Don't know where I missed. Suggessions will be appreciated,304693.0
90652,378060.0,"https://www.geeksforgeeks.org/reduce-in-python/ https://www.youtube.com/watch?v=oritw8qAR1U Please go through he links above, if you have any specific questions then, please ask.",319721.0
90231,376139.0,How do i replace the Null values with first values df.col2[df.col2 == 'NaN] = df.col1 Below is not working Need some help,308495.0
90231,375878.0,"You check check for null values in the second entry and if second entry is null, replace it with first value.",318368.0
90231,376099.0,thanks... can u also share how to extract 2nd element in a strign,308495.0
90231,376120.0,"Thanks, i think i got it for second df['Second'] = df['Sentence'].astype(str).str.split().str[1] and for 1st df['first'] = df['Sentence'].astype(str).str.split().str[0]",308495.0
90231,376146.0,"Below worked df.Col2 = np.where(df.Col2.isnull(), df.Col2, df.Col1) Hope it helps others",308495.0
90419,377033.0,"yes, you're correct.. head will just pick the top 10 rows and show you.. if your intent is different then you should use the appropriate code.. that's all i can say.. since this is assignment question, cannot answer it completely.. :) hope it helps..",316349.0
90419,377036.0,No Problem Hemant.. I got my answer :),300727.0
90419,377031.0,I hope head picks up the first 10 roes in the dataframe.. irrespective of the order weather they are top in the list or not right ? Do we need to apply any sort functinallity here to fetch the top 10 ?,300727.0
90419,377027.0,use .head() concept on your dataframe with number in parenthesis.. this should extract the top specified numbered in parenthesis results for you..,316349.0
90419,377073.0,You can use head function. It accepts number of rows to be returned from the top. df = df.head(number),318368.0
90419,377265.0,first sort then head(10),306248.0
90419,377449.0,You can use slicing and dicing as df[0:10],318329.0
90419,377894.0,first sort the values using dataframename.sort_values() and then use head(10) to get the top 10 values,306737.0
90419,377661.0,we can use head() df=df.head(),306996.0
90419,377888.0,just in case. nlargest(10) also works,317269.0
90419,377964.0,Using Head Concept is easy to use than slicing ..,319869.0
90495,,nan,
91987,385903.0,"To my knowledge, if you have dropped any columns in a dataframe with inplace = True, then there is no way to undo that operation except to reload the data again and start all over again.",318084.0
91987,386164.0,Oh Okay! But I have not used inplace=True anywhere in my code.,316255.0
91987,386604.0,"You don't have roll back option in Df, u need to be careful before you do so. it's always good practice to have a copy of DS or DF before you start work on it.",306735.0
90565,,nan,
91993,385974.0,Thanks! I am keeping one and dropping the other,320603.0
91993,385964.0,they've not mentioned it specifically.. but better to retain one of the duplicates.. usually we keep the first row..,316349.0
91993,385958.0,Keep the first value and drop the 2nd one.,318084.0
91993,387935.0,Keep the original row i.e. first row and delete all other duplicate rows.,317811.0
90591,377893.0,To detect NaN values numpy uses np.isnan(). To detect NaN values pandas uses pd.isnull().,312019.0
90591,377914.0,"https://datascience.stackexchange.com/questions/37878/difference-between-isna-and-isnull-in-pandas/37879 Hey Ruchita, their utility is the actually the same. It's explained in the link above in detail. Please refer to it.",319721.0
90600,378405.0,https://www.dataquest.io/blog/large_files/pandas-cheat-sheet.pdf,301644.0
90600,378261.0,Looks like we need to compile one!! Good way to learn and consolidate.,301644.0
90600,377899.0,https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf,318429.0
90600,377912.0,Above cheatsheet I already have. but mainly I need Pandas.,318319.0
90600,377917.0,https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf,317991.0
90600,377939.0,https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf,316041.0
90600,377975.0,,305652.0
90599,377906.0,http://interactivepython.org/runestone/static/CS152f17/Lists/AppendversusConcatenate.html refer this link.,318340.0
90599,377935.0,"In terms of pandas dataframe It depends on the requirement. Since Concat gives the flexibility to join based on the axis( all rows or all columns) Append is the specific case(axis=0, join='outer') of concat Source: https://stackoverflow.com/questions/15819050/pandas-dataframe-concat-vs-append",317991.0
90599,380092.0,Hi. Please refer to the following StackOverflow answer: https://stackoverflow.com/questions/15819050/pandas-dataframe-concat-vs-append,306040.0
90609,377998.0,"First, inspect the rows with &gt;5% NaN values. Then drop those with high NaN values. As mentioned, The rows for which the sum of Null is less than five are retained. So, see if you have those only.",319721.0
90609,383503.0,i am stuck with exactly the same issue. Subtask 2.4 is to no effect,303230.0
90620,377979.0,You just need to create a rank column in the sorted IMDb_Top_250 dataframe and assign values from 1 to 250.,306040.0
90620,378476.0,The question says add a Rank column containing the values 1 to 250 ..hence we cannot assign movie with same score with same rank. Rank function hs method first: that assigns ranks in order they appear in the array,317514.0
90620,378510.0,You can create a separate Rank dataframe and concatenate it with the sorted IMDb_Top_250 dataframe.,318335.0
90650,378090.0,Don't worry about the file location. It is obviously better that you keep the csv file in the same location as your notebook as it would be easier for you as well as for us. :),306040.0
90649,378087.0,It is always better that you submit the assignment with well-written comments. There's also a small weightage assigned to this portion.,306040.0
90645,377995.0,Correct,318398.0
90645,378076.0,You need to extract all the columns for all the three actors.,306040.0
90651,378051.0,"If you can specifically ask me what is your question on pivot tables, i'll be able to help. You can go through the video again and come back with questions.",319721.0
90580,377873.0,Can you please explain in some simple terms.,315560.0
90580,377863.0,"The problem comes when we give path as follows: "" C:\Users\ 302001419\.......\Movies+Assignment+Data.csv"" Here, \U starts an eight-character Unicode escape, such as '\U00014321`. In above code, the escape is followed by the character 's', which is invalid. So we either need to duplicate all backslashes, or prefix the string with r (to produce a raw string). Source: - https://stackoverflow.com/questions/1347791/unicode-error-unicodeescape-codec-cant-decode-bytes-cannot-open-text-file",317991.0
90580,377930.0,Please refer to the following link: https://stackoverflow.com/questions/42654934/need-of-using-r-before-path-name-while-reading-a-csv-file-with-pandas/42655010,306040.0
90580,377934.0,"In Python, backslash is used to signify special characters. e.g. ""hello\nworld"" -- the ""\n"" means a newline. Try printing it once in the Jupyter notebook. Path names on Windows tend to have backslashes in them. But we want them to mean actual backslashes, not special characters. r stands for ""raw"" and will cause backslashes in the string to be interpreted as actual backslashes rather than special characters. e.g. r""hello\nworld"" literally means the characters ""hello\nworld"". Again, try printing it. More info is in the Python docs, it's a good idea to search them for questions like these. https://docs.python.org/3/tutorial/introduction.html#strings",305334.0
90580,377962.0,"generally backward slash('\') is used to escape the special characters like quotations, backaward slash and also for some special uses like ""\n for next line"", ""\t for tab space"", etc So if you use '\' in your path like C:\Users\ 302001419\Downloads\XYZ, Python will think that there is some action to be performed when it sees backward slash('\').. If you want, you can give your path as C:\\Users\\ 302001419\\Downloads\\XYZ but it would always be tedious using double slash('\\') everytime. So for simplicity people generally tend to use the raw path which can be done by usng rawstring 'r' before your path like r' C:\Users\ 302001419\Downloads\XYZ'",318013.0
90658,378044.0,You can use Pandas inbuilt rank function to determine the rank values for a given dataframe. Refer to the following documentation for more details: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rank.html,318085.0
90658,378077.0,"Then you can restrict the range to (1,251)",319721.0
90658,378082.0,Can we do it without rank function as most of us are suggesting this function but i want another approach,319869.0
90658,378104.0,"Here, you are supposed to, Use position based indexing to get the first 250 rows in the sorted dataframe and Create a new column rank which contains the rank from 1 to 250 You can use an .iloc() here",319721.0
90658,378456.0,If you are interested in doing without rank then you can try reset_index() to reset the index. Assign df.['Rank'] = df.index + 1,317514.0
90658,378515.0,"Just change the index of the IMDb_Top_250 dataframe before concatenating. There is an index mismatching between your Rank DF and IMDb_Top_250. If the index for both will be same, then the concatenate function will not give NULL values. I also got null values in the beginning, but when I changed the index, I got the answer as desired.",318335.0
90658,378528.0,"you can create a new column with range function. df.['Rank'] = range(1,251)",318009.0
90658,378904.0,There are other ways too. Maybe a little more complicated but one could take in another column to figure out which movie should rank higher given two movies that have the same rating.,319357.0
90658,379315.0,"use df['expected column']=range(1,n)",318340.0
90610,378003.0,"https://stackoverflow.com/questions/8949252/python-attribute-error-nonetype-object-has-no-attribute-something Give me a line of your code, which doesn't reveal the answer for the assignment. Or modify it. Then we can solve the error. Look at the link above too.",319721.0
90617,377968.0,"Even if you choose to drop using just the 'movie_title' column, it might not be correct. Because a lot of movies do have the same title. So you need to drop the duplicate rows for which all the values in all the columns are the same.",306040.0
90662,378109.0,"Yes, you need to use the dataframe after you've handled all the Null values and cleaned your dataframe accordingly. Also, there is a subtask wherein you need to drop some duplicate values, so just make sure that change is reflected in your dataframe.",306040.0
90662,378233.0,"yes, you have to use the same Df for all, but for some subtask you have to use the one which you have newly craeted.",300726.0
90605,377940.0,"Yes, we need the calculate the percentage of rows reatined after perfoming row manipulations: Percentage would be like this this = 100*(No:records in dataframe/Original number of rows in dataframe)",318085.0
90669,378021.0,you can store the result in new DataFrame before dropping ...if you encountering this problem in assignmnet because of wrong deletion of data go Cell/Kernal in jupyter and clear output and run specific cell again or can run all cell from start of file,319869.0
90669,378127.0,"Unfortunately, there isn't any way to do that. You need to run your code again from the beginning. Beginning as in the part where you've read the original dataframe. If you manually don't wish to do it, you can try either of the two options: 1. Select the 'Kernel' option from the menu and select 'Restart &amp; Run All"" 2. Or as Vinay mentioned, click on the cell and select ""Run All Above""",306040.0
90669,378089.0,"Rows are dropped from the dataframes and not the underlying souce like csv file or the database or web etc. (assuming that you haven't updated the data i updatable source like csv) So you can start performing the same sequnce of operations again till the point you dropped the rows to get the same. As of now, I believe most of the questions or practice command seqnces are not big, so it should not be an issue. However, you can learn from this and can make a practice to make copy the data each time you drop any column or delete rows.",311729.0
90669,378012.0,"No, there is no way of reverting back the dropped columns/rows. You need to re-import the dataset again and repeat the previous data operations",318085.0
90669,378006.0,There is no way to do that. You have to reload the file to get the complete data and then perform operations on top it.,318084.0
90669,378014.0,"As of my understanding best way is to run the code from the section you created the DF, from whcih you dropped the columns. means regenerate the DataFrame again.",320073.0
90669,378019.0,Many ways you can do but the best and easy way start compiling from the first statement which import your data then follow up next all statement and compile. Since dataframe we are making from having the source is excel so always u can perform this approach,307843.0
90669,378034.0,Dont know of a way to get it back. But if you start from the top pressing Shift + Enter. Reach until that point.,317269.0
90669,378056.0,"Once deleted we cannot get them back unless we reload the dataframe again. One trick I use : Click on ""Cell"" --&gt; ""Run All Above"". This way, all the cells (Code and Markdown) till that point get executed and you can proceed further.",313826.0
90669,378238.0,Deleted rows or columns cannot be recovered from a dataframe. We can always create a copy of the dataframe and apply the changes on the copy and refer to the original as and when required.,319302.0
90669,379580.0,"Reload the DS, no other Go. I always used to make a copy of it before i start working on a DS. It helps !",306735.0
90669,379530.0,"Another way is to take a temporary backup (copy) of the dataframe, try your operations on this and once you are happy with the results, you can do this on the original dataframe. Else one needs to redo all the previous steps.",300717.0
90669,379779.0,"Data dropped cannot be recovered in flow of code, unless you have a backup :D It is still easier to do so in Jupyter notebook, but in real projects, it may not be possible. After delete only a subset of data goes to the next level.",318078.0
90641,378074.0,Please check the Rubrics table in the following link: https://learn.upgrad.com/v/course/208/session/19876/segment/101118,306040.0
90644,378081.0,Just use the sort function on the said dataframe as you normally would and assign the dataframe to it. Something like: df = df.sort_values(by = 'some_column'),306040.0
92008,386118.0,"Looks correct to me, and I tried but not getting any error",318554.0
92008,388690.0,Check your parenthesis,319721.0
92008,388830.0,"Hi Saurabh, I tried the same code as you have shown which can be seen in the following screenshot. It would be easy to spot error if you can share the whole code written in this block of Jupyter Notebook.",318355.0
90637,377967.0,you can create a list of columns based on the condition given and use that list in the subsequent steps to drop the rows. I don't feel like giving the code for the same will be any help. you have to figured that out.,320073.0
90637,378059.0,"Just check which columns have a high NaN percentage and for such columns, drop all the NaN rows. You have done similar tasks in the session 'Getting and Cleaning' data several times.",306040.0
90637,377958.0,do we have any way for this to be done dynamically.. I couldn't find ..,317982.0
90637,378000.0,create a function input the condition for removing the droping the rows.,301115.0
90637,378150.0,we can do with negation option with numpy np option isnan. we can get some idea with the melbourne.csv case too.,312019.0
90637,379174.0,"Inspect the Dataframe , by using column wise null percentages. Identify the column having more than 5.00 % null values , and remove those columns from dataframe manually. Already some columns have been removed , mentioned as unnecessary columns.",318557.0
92000,386026.0,"Movies dataframe will have duplicates, it will not give accurate results Top_Foreign_Lang_Film - this data frame has all non-english movies, some details still be missing. You have to use the IMDb_top_250 movies.",311502.0
92000,387572.0,Use initial datafame MOVIES.,317811.0
92000,386290.0,You can do it on the movies without duplicate or initial movies dataframe.,317689.0
92000,388688.0,"You need to use the dataframe which has been updated the latest, that is after dropping.",319721.0
90631,378026.0,"You might've noticed that there are duplicate values in the top 10 profit list, after subtask 3.2. Then you need to remove these rows with duplicate values and find the top 10 movies again. Like you did in subtask 3.2. The second task means, 3.2 here",319721.0
90596,377913.0,"1. No, you did not do wrong. Actually in the given movie dataset movies column has the entry ending with Â. And we just read that data into dataframe using read_csv(), so whatever is there in dataset it is showing in dataframe. 2. In jupyter notebook, inside the cell the statement gets executed sequentially i.e one by one so when you try to execute 2 statment in single cell then 2nd stmt will get executed at last and you will get output related to 2nd stmt.",317991.0
90640,378070.0,"Unless it is specified that you only need to retain certain columns, please retain all the columns.",306040.0
91018,379892.0,"The groupby() function returns a DataFrameGroupBy object. Only upon applying an aggregration function, it would retun a series. Suggest you to apply the aggregration function you are looking for and then proceed further from there.",313826.0
91018,379927.0,"Groupby only works when we apply aggregation on it such as mean, sum and as mentioned by Vinay it returns DataFrameGroupBy object. Once the aggregation is performed it will return a series.",310210.0
90545,377672.0,"File not found error comes when file does not exist at the specified location. From the above code I can assume you have put MovieData.csv file at desktop. But path for this file will not be simple ""desktop/MovieData.csv"". Here is the step to follow to get file location: 1. Go to the file MovieData.csv reside on desktop. 2. Right click on file --&gt; Click on properties. 3. Check for 'Location' --&gt; you will find something like C:\Users\.......\Desktop . This will be the actual location of your file. So the file location for read_csv will be ""C:\Users\....\Desktop\MovieData.csv"" Hope this will help.",317991.0
90545,377681.0,"Vipul has provided all the required steps.From my experience i can add one more thing it does work without the encoding and adding the seperator will aslo help. df = pd.read_csv(""desktop/xyzData.csv"", sep="","") as it is csv for text file the seperator will be ""/t"" and so on.",300687.0
90648,377973.0,"As direct code cannot be written, here is the approach 1. split the genres in the panda series 2.create two columns and assign the values 3.as for the condition genre 2 may be blank ,so fill the null values of genre2 with genre 1 4. now after this steps,see if we can reduce the code",301115.0
90648,378030.0,You aren't supposed to ask answers for graded questions directly. We'll be happy to provide any conceptual help.,319721.0
90643,377984.0,repeat Subtask 3.2 afetr removing duplicates i.e Extract the top ten profiting movies,301115.0
90643,378075.0,"Hi TA, Please clarify this question. Very important.",306248.0
90643,378084.0,Repeat Subtask 3.2 means that you need to find the top 10 profiting movies as it wasn't correctly captured it the said subtask due to one duplicate value. You obviously need not do the first two steps of subtask 3.2. Just extract the top 10 movies after removing the duplicates and you should be good to go. :),306040.0
90643,379747.0,"Removed duplicates at Subtask 3.2, as the original dataframe has duplicates and it is necessary to remove duplicates at first place.",306731.0
89670,372269.0,"If the file is in the same directory as the Jupyter installation folder, just giving the file name will work.",318084.0
89670,372355.0,"No need to give any folder structure if its in the same directory. That is called relative path and works fine in all the cases. Other option is to have absolute path, which is path based on your folder structure, which will also work.",318368.0
89670,372737.0,"In case of asignment, we will ensure that the files are being pointed correctly, meaning that could imply that slight file path being modified during evaluation and that should be fine. But try to create a final solution where your data files and notebook is in same folder and directly accessible",301555.0
91014,379899.0,b[b &gt; 5],313826.0
91014,380003.0,"Hi, As told by Vinay, the solution would be: SeriesName [ filter condition ] This in your case, b[ b&gt;5 ]",317987.0
90322,376416.0,"TLDR - group all the movies by genre1+genre2 and find the mean using the 'gross' column for each grouping; get the top 5 grossing grouped categories and put in a new DF Long answer: The genre column is seperated by pipes '|'. so for example: Action|Adventure|Fantasy|Sci-Fi has 4 genres - Action, Adventure, Fantasy, Sci-Fi 1. so extract the first two genre for each movie - in Action|Adventure|Fantasy|Sci-Fi it would be 'Action' (genre_1) and 'Adventure' (genre_2); some only have one genre (e.g. data row 6 in the csv - only has 'Documentary') so in this case both genre_1 and genre_2 will have the same value - say, Documentary as in example of row 6 2. group the dataframe using these two values with genre_1 being primary and genre_2 being secondary- grouping sort of creates a subset of the data; so here are some examples genre genre_1 genre_2 grouping(genre_1+genre_2) ======= ======= ====== ===== Animation|Drama|Family|Fantasy Animation Drama Animation+Drama Action|Adventure|Sci-Fi Action Adventure Action+Adventure Adventure|Action|Drama|Romance Adventure Action Adventure+Action Adventure|Family|Fantasy Adventure Family Adventure+Family Adventure|Family|Fantasy Adventure Family Adventure+Family Note1: grouping is just shown above for illustration purposes, and it is represented differently inside Python Note2: notice that row 1 and row 2 are different groupings and not the same 3. then for each grouping (e.g Action+Adventure) find the mean of the 'gross' column for all of them in that group. So if there are 5 movies that have Action|Adventure as the first two genres then find the mean of their gross column 4. sort the mean gross value for the groupings by descending order and get the top 5 and store in a dataframe called PopGenre",300694.0
90322,376559.0,First you need to split genres into two : genre_1 and genre_2. Then you need to merge these two with movies. Drop genres. Than do the regular extraction.,317269.0
90322,376676.0,1. Create 2 new cloumns genere_1 &amp; genere_2 by splitting the genere cloumn 2. Do a group by on genere_1 &amp; genere_2 3. Apply a mean function on the group and produce the results,306248.0
91236,381242.0,"If there are multiple files then all together can be uploaded through a single ZIP folder. Otherwise, you can upload a single IPYNB file.",311502.0
91236,381245.0,"Please cehck the below link: https://learn.upgrad.com/v/course/208/session/19876/segment/101117 Under the "" How do I submit the assignment? "" section it is mentioned that we need to submit the .ipynb solutuon file.",313826.0
91236,381347.0,Yes you can submit zip file but make sure that it has the jupyter notebook ie .ipynb as grading would be done based on .ipynb file.,317460.0
91236,382082.0,Ok. Thanks for the updates:-),320008.0
91236,381414.0,"For python assignment you should only submit the ipynb file;students are not allowed to edit the csv file why would you want to submit more than just the ipynb file? it is not needed .. so, keep it simple :-)",300694.0
90618,377974.0,"We cannot tell you the answer since it's a graded component. But try splitting the genre string. Notice that all the genres for a movie are separated by a pipe (|) operator. Can you maybe use this somehow to extract the genres as a list of strings? Recall the initial lectures in the Introduction to Python module. Or maybe, do a simple Google search. :)",306040.0
90618,377987.0,Since we can't mention the exact code. You can try the following approach: Use Pandas inbuilt string function to split the value into a list of genres Use indexing to identify the right values ( in our case the 1st and 2nd values (if applicable)) Also be mindful to handle scenarios in case of single genre values. You need to ensure the second genre list has the value of the first genre Hope that helps,318085.0
90894,378885.0,"they said genre1 as primary column,genre2 as secondary column, for the dataframe movie by segment",301115.0
90894,378927.0,"use group by function like this groupby(['primary ', 'secondary ']) on movies DF. hope this will help you.",300726.0
90894,378984.0,Primary and secondary mean grouping on multiple columns. First grouping on first group and second grouping on secondary column. Simply apply grouping on multiple items with primary as first value and secondary as second value.,318368.0
90949,379268.0,what is the issue. did you load it?,318009.0
90949,379286.0,Please elaborate on the issue so that we can help.,313691.0
90949,379291.0,I downloaded the file and tried opening it.i have jupyter in my lap but even then it's asking for an app to open where I couldn't find jupyter. So it automatically opens in a word doc. how to open it in jupyter,315831.0
90949,379331.0,"You can copy the jupyter file in the computer file system for instance, keep the file under the user directory C:\Users\Amani\Documents\Python Scripts\jupyternotebook Now run the jupyter notebook from Anaconda Navigator. now traverse to the exact file path using jupyter and open the file. Hope that helps",317845.0
90949,379332.0,you first need to run the jupyter on your local machine then open the web browser which will run some command in command prompt and enter the local host url (http://localhost:8888/tree) and then from there you need to go to the location of the file and open it from there it will work try it .,318017.0
90949,379505.0,"or the other way round you can try opening the anaconda prompt and once the anaconda prompt opens, type the following command in it : jupyter notebook this command will open the jupyter notebook for you locally I hope this helps, if everything doesn't turn out good",301655.0
90949,379494.0,"1. Before starting below steps. Create a folder on desktop named say UpGrad. Inside this folder paste both the downloaded ipynb and csv files as given for assignment. then, 1. Go to Windows/Start Icon on Laptop. Look for Anaconda3. 2. 2. Click on Anaconda3 and it will show you a sub program Anaconda Navigator. 3. 3. Click on Anaconda Navigator. Allow 2-3 minutes It will be launched and you will see a round green icon on your taskbar for anaconda navigator. 4. Anaconda Navigator is opened. Here you will see Jupyter notebook on second number in the icons provided. Click on Launch provided in jupyter notebook section. 5. It will open the jupyter notebook in microsoft edge(e icon browser or internet explorer or see if any other browser). No you can see in the folders here, Desktop is shown, Click on Desktop, Then You will be able to see inside the folders that are on your desktop. You can see UpGrad folder as we created same at first step. Click on UpGrad. Inside this you will se assigment's ipynb file. Click on that it will be opnened. And Now you just need to use pd.read_csv(""filenameonlyofcsvfile"") to access the csv file",317811.0
90904,378922.0,not appearing,307843.0
90904,378925.0,the logic you have used is wrong. you have to extract top foreign language films from 'IMDb_Top_250'.. by using this logic language != 'English'; then you can find Veer-Zaara' in the list.,300726.0
90904,378936.0,The movie 'Veer-Zaara' is present in the original dataset provided. Please check out the screenshot below You can also verify this by checking for the movie in the csv file by opening it in excel.,313826.0
90904,379010.0,There is a type mistake in your code. You have written 'Veer ZaaraA' instead of 'Veer Zaara'. This is why you cannot find it in your dataset.,304319.0
90904,379148.0,I have the same problem. No issues with the syntax or the code but not seeing veer zaara in the list. I know it exists in the csv file but not in my Top_Foreign_Lang_Film :-(,312199.0
90904,379301.0,"Hi, Can you please try to match it exactly with 'Veer-Zaara' once and not the one you see in excel (it is different when we read using excel and Pandas, maybe encoding)? I do see this exact name in the Pandas dataframe. If the issue persists, download the CSV file again and replace the old CSV you have.",317987.0
90904,379771.0,"There is an extra /xa0 that is being stored internally in the dataframe. That's why we write code to see if it's present in the dataframe, it shows nothing. If you check like this, you could see the results. movies[movies.movie_tile == 'Veer-Zaara/xa0']",318329.0
90904,379803.0,yes,317811.0
90904,379909.0,The movie appears towards the end of the dataframe. Check manually towards the end.,316416.0
90904,379982.0,"you could also use other paramters to look for Veer-Zaara, like language = 'Hindi' and director='Yash Chopra' or actor_1_name='Shah Rukh Khan', this should also give you result, or you can simply list all the movies which has actor_1_name='Shah Rukh Khan'.",300734.0
90904,380019.0,Getting Veer-Zaara in the python DF,318827.0
90904,380038.0,Thank you so much guys for all the input. Rectified the error and now 'Veer-Zaara' is appearing. Thank you once again.,317991.0
90904,380044.0,Note the Case sensitivity. Also note the checkpoint 3 is not asking to list Veer-Zaara but to indicate that in data analysis this is one of the films in the resulting data frame,311803.0
90904,380136.0,The movie appears towards the end of the dataframe. Please check again.,308442.0
90905,378943.0,"I would suggest you to restrict your analysis to the problem statement mentioned in the assignment, although you may feel that the same analysis could be done in a better way by taking a few more additional steps.",313826.0
90905,378954.0,Do No use any alternate methods in the assigmnet even if you feel its is easy and effiecient. stick on the problem statment mentioned in the assignment-beacuse the score is depended on this.,300726.0
90905,379648.0,The question asks for ranking (from 1 to 250) the 250 top IMDB score movies only nothing else is needed.,301644.0
90905,380142.0,Hi you can use alternative method for ranking but don't think any parameters to rank like IMDb or anything else Create a series for 250 rows as dataframe has 250 rows only ..keep in mind start and stop while making a loop to create series,319869.0
90951,379323.0,"Deval, how you are using then ?",306735.0
90951,379321.0,you must not follow this as you will have to manually add the data each time you want any change to happen for example you made some change to the data and want to use the data that was present originally you will have to uplaod data each time.,318017.0
90951,379322.0,It's not required to upload the CSV file to Jupyter local. You can keep the file anywhere in the file system and give the path. for example: import pandas as pd movies = pd.read_csv('C:\\DataFolder\\upgrad\\Movie+Assignment+Data.csv') print(movies),317845.0
90951,379328.0,i am doing this using the read_csv command for uploading the csv present in the base location.,318017.0
90951,379353.0,used read_csv and provide your local path,318340.0
90951,379486.0,Use read_csv and follow same pattern what the professor has used to read melbourne.csv in cleaning data part,317811.0
90927,379172.0,"Hello Nagaraju, This is an assigment question so we can not discuss further more on this. What approach I need to follow after sorting values on imdb_score to only fetch the number of votes for that particular film are &gt; 25000. = Your approach is correct.",320195.0
90927,379097.0,"I am assuming, I only need to use label positioning on columns and not to do two different operations and combine them in loc function. Please correct me. What approach I need to follow after sorting values on imdb_score to only fetch the number of votes for that particular film are &gt; 25000.",305843.0
91259,381435.0,"Open the file in Jupyter - there are cells marked where you enter your code once you are done, you go back to the upgrad learning platform graded assignment module and on the last page there is a submission page - you submit just the ipynb file",300694.0
91259,381425.0,Type your answeres in the same 'Movie Assignment.ipynb’ file. And once you're done with your assignensubmit the same file (only Movie Assignment.ipynb’ file) i the Assignment submission session. Deadline is over though for Python assignment.,316349.0
90664,378047.0,"It is an assignment related question. So I would advise , go through the question , do some reasearch on google as how can that be done and try first. If any error comes you can post on discussion forum, everyone will be happy to help you. I believe It is the best way to learn .",317991.0
90664,378048.0,"Create a new dataframe IMDb_Top_250 ,after sorting the movies df based on condition",301115.0
90664,378118.0,You don't need to extract anything from that dataframe. Just use the head() function if you wish to inspect the dataframe or rather you can simply see the whole dataframe by just typing IMDb_Top_250 at the end of the cell.,306040.0
90664,378024.0,"for extracting the top 250, please use the .head() function",318084.0
90664,378079.0,Head function is the best..for any number of desired rows but only for sorted data,319869.0
90671,378011.0,"I think you have to assign English for the missing values, please use the fillna method.",318084.0
90671,378013.0,Use square brackets [ ] with .loc and not parenthesis.,313826.0
90671,378073.0,Refer Fillna method in UPgrad videos,319869.0
90671,378055.0,"You can apply square brackets instead of parenthesis in loc function, like this: df.loc['Column'] = 'Value' OR you can use the fillna method to replace null values with English. Hope that helps",318085.0
90671,378135.0,"It seems that you're using round brackets instead or square ones. Also, if someone asks such questions, please guide them in the right direction rather than posting the final command/answer. It would be really helpful for them as well as others. :)",306040.0
90671,378131.0,"df[""language""].fillna(""english"",inplace=True)",306996.0
90675,378066.0,please go through the notebooks for calculating columns percentage which has null values 1. identify the columns have large percentage (greater than 5%) 2 .go through the notebooks for dropping rows for the identified columns,301115.0
90675,378062.0,I would suggest to go through 3rd video of below link. You will easily understand what needs to be deleted. https://learn.upgrad.com/v/course/208/session/19864/segment/101071,317991.0
90675,378067.0,First select on which column you need to operate apply function which select all missing values rows and drop,319869.0
90675,378072.0,"As stated, on inspection you might notice that some columns have large percentage (greater than 5%) of Null values. Drop all the rows which have Null values for such columns. So, all the rows in the columns with &gt;5% nan values need to be dropped. Foe example if you got, 'language' and 'budget' columnslarge number of NaN values, drop all the rows with NaNs at this column",319721.0
92474,389509.0,"Hi Rohit, Yes , your understanding is correct. When ever values cross each other , Sell / buy will occur , other days signal will remain as Hold.",305652.0
92474,389523.0,Refer here for more details. https://learn.upgrad.com/v/course/208/question/92449/answer/389384/comment/97113,317689.0
91088,380480.0,"Whenever there is a clash in imdb_score , it's always better to sort with a secondary variable, in this case num_voted_users . i.e. A movie with imdb score of 7.9 with 40,000 votes is anytime better than a movie with imdb score of 7.9 with 30,000 votes.",318004.0
91088,380586.0,"do sort, &gt; value, too 250 records.",306735.0
91088,380505.0,"Since there is no special mention of sorting on a secondary variable if the 'imbd_score' is same, it is better to not do any additional sorting and just go with the order of occurence in the dataframe. This is also significant, since there are many attributes like 'num_voted_user' , 'num_critic_users', 'num_user_for_reviews' or even derived fields based on these/other attributes ,which could all be candidiates for secondary level of sorting and hence deciding on which of these attributes can be considered as secondary attribute without understaning the drive behind the analysis would be going in the wrong dorection. Hence, I believe we should stick with sorting only based only 'imdb_score' and rank be decide on the order of coourence in the dataframe.",313826.0
92522,389818.0,"there are lot of movies for which there is only one genre. In such cases, if you use split function, then genre2 would be null ( obviously). So for al such records where there genre2 is null, genr1 needs to be cpied ovr to genre2.",304814.0
92522,389846.0,"As per the question and as Anshul mentioned for movies where there is only one genre, same genre should be reflected in genre_1 and genre_2 . As the picture you shared is not opening we cannnot validate. But Please validate from your side. And after that if you are still not satisfied there is a option for reevaluation. WHere you are seeing the detailed score.",318554.0
92522,391205.0,This the thing that i did for the task asked by them.,318461.0
92522,391033.0,"The dataframe given for everyone is the same. And if you performed all the other sub-tasks correct, your dataframe will have data points with only one genre. So, for those with only one genre, the second of the two new columns, (genre1 and genre2) you created will have a null value because there is no second genre for these. Hence you are asked to update the null value with the first and the only genre the data point has. If you don't do so, like it was mentioned, partial marks will be awarded. Becuase the results are dependent on this.",319721.0
92522,391201.0,"movies[""genre_1""] = movies[""genres""].apply(lambda x: (x.split(""|"")[:2])[0]) movies[""genre_2""] = movies[""genres""].apply(lambda x: (x.split(""|"")[:2])[-1]) i did that tasking using above statement so that it automatically do that. I check the following statement after this one i didn't get any null values in genre_2 movies.isnull().sum() it gives following output. It doesn't contain any null values director_name 0 num_critic_for_reviews 1 gross 0 genres 0 actor_1_name 3 movie_title 0 num_voted_users 0 num_user_for_reviews 0 language 0 budget 0 title_year 0 imdb_score 0 movie_facebook_likes 0 profit 0 genre_1 0 genre_2 0",318461.0
92945,391435.0,First talk about the problem to your student mentor then if he said to apply for re-evaluation then go for it.,310419.0
92945,391499.0,reason might be that you have not defined inplace=True (as False is default) -&gt; so effectively you have not dropped the duplicates unless you assigned the output - which doesn't look like you have.,300694.0
92945,392553.0,"Sorry to hear that. But it should be, movies.drop_duplicates(subset = None, keep = 'first', inplace = True) Reason being, your code movies.drop_duplicates() doesn't store the data back to the dataframe as you have not explicitly set inplace=True and hence it takes the default value which is false. You might either had to do "" movies = movies.drop_duplicates()"" or “movies.drop_duplicates(subset = None, keep = 'first', inplace = True)"" Additionally FYI. subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns keep : {‘first’, ‘last’, False}, default ‘first’ first : Drop duplicates except for the first occurrence. last : Drop duplicates except for the last occurrence. False : Drop all duplicates. inplace : boolean, default False Whether to drop duplicates in place or to return a copy Thank you. Hope this helps in future :)",318080.0
134042,583323.0,"Yes, that's the whole idea behind data locality concept.",428646.0
133169,580655.0,"Once the connection is established between cluster and developer machine through Yarn Resources Manager, All the tasks are monitored by it, So there is always the resources manager to look at all the transactions.",428646.0
133312,,nan,
132441,578611.0,"In realtime situations spark is run on Hadoop clusters, which are often very good hardware with high config and are designed to run such jobs, Almost all the vendors be it cloud or on premise, use very good configuration with the clusters which have good amount of memory (RAM) and high collective CPU. In these clusters spark jobs are run which evaluated lazily. In lazy evaluation manner spark reads the data in memory (RAM) and does the processing in memory thus reducing the disk read writes which is done by hadoop. Hence it is much faster. But it would require good config cluster to do this. For more information go through the spark documentation which explains how this processing happpens in detail.",317689.0
132441,578225.0,"Hi Chetan, Spark leverage memory for faster processing. If this is required more elaborated, let me know, I'll post it here itself. First of all, as far as, running Spark job on local machines is concerned, we Only run it with limited data which can fit into the local machine memory Run it for testing whether we're getting the desired results or not By desired results, I mean, when we use some transformation, we must validate the transformation on few records If we keep checking the results by executing our spark job on the huge set of data (which would only be done by running the job in cluster as the data is in the cluster), it would definitely consume much more time rather than we, testing on the local In other words, we do not run on the whole data like 16GB which is stored in the cluster because the Data locality principle still holds here i.e. the code needs to go where the data is stored &amp; not the data needs to come to where the code is stored. Hope this helps!!",318355.0
132441,578250.0,"The local machine is for developer to check the coding syntax and quickly prototype the solution on the sample data. If your data is huge you need to sumbit your pyspark or Scala job in cluster mode or client mode to take advantage of the in memory ram a cluster have ,your local processing depends on the number of core u have ram memory.",318476.0
132441,578295.0,"Take a random sample of your data, such as the first 1,000 or 100,000 rows. Use this smaller sample to work on all of your data.You may also consider performing a sensitivity analysis of the amount of data used to fit one algorithm and moreover data locality principal and your local system ram will play a mojor role in processing here.",318427.0
132441,578312.0,"1-Local machines are used only for POC. They would not be used on production like system. 2- If you develop a spark program, Testing on cluster is time consuming some time . so we test it on local machine to check compile time and run time error on small dataset. 3- In memory will play huge role once your data resides on hadoop cluster not on local machines. Hopes this helps.",312746.0
132441,578309.0,"Apache Spark is 100 times faster than Hadoop MapReduce and comfortable APIs, some think this could be the end of Hadoop MapReduce. Apache Spark processes data in-memory while Hadoop MapReduce persists back to the disk after a map or reduce action, so Spark should outperform Hadoop MapReduce. Nonetheless, Spark needs a lot of memory. Much like standard DBs, it loads a process into memory and keeps it there until further notice, for the sake of caching. If Spark runs on Hadoop YARN with other resource-demanding services, or if the data is too big to fit entirely into the memory, then there could be major performance degradations for Spark. MapReduce, however, kills its processes as soon as a job is done, so it can easily run alongside other services with minor performance differences. Spark has the upper hand as long as we’re talking about iterative computations that need to pass over the same data many times. But when it comes to one-pass ETL-like jobs, for example, data transformation or data integration then MapReduce is the deal—this is what it was designed for.",314183.0
132558,578528.0,"Hi Shubham, Let's first understand batch processing: Data resides in the cluster We execute a job The job runs for a few seconds/minutes depending upon the amount of data The output is generated &amp; is used somewhere In case of Stream processing: Some data is already present in the cluster Also, the data is coming at real time In this case the job keeps on running &amp; as soon as the real time data comes in, the output is generated within a few seconds The major difference between the two is that in case of real time analysis, we need to see the output of our processing as soon as the new data comes in Let's understand further with an example: Batch processing I would say, all the queries which we ran during our HIVE assignment comes under the Batch processing i.e. the data we're analyzing is static &amp; not changing every second. Realtime processing Let's take the example of e-commerce, suppose you're purchasing some thing from e-commerce site like Flipkart Suppose a sale is going on The company target is that, if a product has been added to a users cart &amp; the user leaves the application for some reason. The application should notify the user that the product would be out of stock very soon. Now, a real time processing has to be set up. The process would be: The process has already analyzed the huge data that is already present somewhere Now, as soon as the sale goes live, the count of the available products would decrease massively The new data comes in and is analyzed at a rapid rate &amp; the output is generated which tells the another API to notify the users that the product in their cart is getting out of stock very soon This way, as you see, we don't have much time to batch process &amp; then generate the output. There has to be some processing which takes bare minimum time, so that we can meet one such target as stated above which realtime processing performs beautifully! Other examples include: Twitter sentiment analysis using SPARK Recommendation systems ex. Netflix, e-commerce For some more use cases, please visit: https://www.qubole.com/blog/apache-spark-use-cases/ which is a company by Joydeep Sen Sharma who's inventor of Hive. If you want to go in furthere detail, pleasee visit: https://data-flair.training/blogs/batch-processing-vs-real-time-processing/ Hope this helps!",318355.0
132558,578607.0,"For a simple understanding batch means data load is done in batches. Generally in traditional datawarehousing systems this is done in nightly manner in form of nightly ETL jobs or spark jobs. This data is mostly available till day-1 of the analysis. For example you want to check the sales year to date (YTD) today againast same period last year. So, if you want to this analysis in batch processing systems this data would often be available till the minimum batch window which can be daily, hourly etc. Whereas in realtime systems data is ingested and processed as soon as it is being generated. The most common use case for this is banking and financial transactions (fraud detection, stock markets). These applications need to be designed with very low latency.",317689.0
132558,579554.0,"Batch processing is where the processing happens of blocks of data that have already been stored over a period of time. For example, processing all the transaction that have been performed by a major financial firm in a week. This data contains millions of records for a day that can be stored as a file or record etc. This particular file will undergo processing at the end of the day for various analysis that firm wants to do. Obviously it will take large amount of time for that file to be processed. Stream processing is a golden key if you want analytics results in real time. Stream processing allows us to process data in real time as they arrive and quickly detect conditions within small time period from the point of receiving the data. Stream processing allows you to feed data into analytics tools as soon as they get generated and get instant analytics results. There are multiple open source stream processing platforms such as Apache Kafka, Apache Flink, Apache Storm, Apache Samza, etc. Please refer the below link: https://medium.com/@gowthamy/big-data-battle-batch-processing-vs-stream-processing-5d94600d8103",314183.0
138142,597410.0,"Yes underlying structure of spark is rdd. So, even if you read data in dataframe under the hood it is stored as RDD.",317689.0
138142,595845.0,Yes. RDDs are the main abstractions provided by Spark and all input data is converted to RDD before processing.,313826.0
134184,584426.0,"In older version of Spark there was different contexts that was entrypoints to the different api (sparkcontext for the core api, sql context for the spark-sql api, streaming context for the Dstream api etc...) this was source of confusion for the developer and was a point of optimization for the spark team, so in the most recent version of spark there is only one entrypoint (the spark session) and from this you can get the various other entrypoint (the spark context , the streaming context , etc ....)",316147.0
134184,583465.0,"Spark session is a unified entry point of a spark application from Spark 2.0. It provides a way to interact with various spark’s functionality with a lesser number of constructs. Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session. Prior Spark 2.0, Spark Context was the entry point of any spark application and used to access all spark features and needed a sparkConf which had all the cluster configs and parameters to create a Spark Context object. We could primarily create just RDDs using Spark Context and we had to create specific spark contexts for any other spark interactions. For SQL SQLContext, hive HiveContext, streaming Streaming Application. In a nutshell, Spark session is a combination of all these different contexts. Internally, Spark session creates a new SparkContext for all the operations and also all the above-mentioned contexts can be accessed using the SparkSession object.",317689.0
134184,583466.0,"Prior to 2.0, SparkContext was the entry point for spark jobs and Starting from Apache Spark 2.0, Spark Session is the new entry point for Spark applications. Basically, RDD was one of the main APIs then, and it was created and manipulated using Spark Context. For every other APIs, different contexts were required - For SQL, SQL Context was required; For Streaming, Streaming Context was required; For Hive, Hive Context was required. Although, from 2.0, RDD along with DataSet and its subset DataFrame APIs are becoming the standard APIs and are a basic unit of data abstraction in Spark. All of the user defined code will be written and evaluated against the DataSet and DataFrame APIs as well as RDD. Hence, there is a need for a new entry point build for handling these new APIs, which is why Spark Session has been introduced. Spark Session also includes all the APIs available in different contexts - Spark Context, SQL Context, Streaming Context, Hive Context. can refer this for more information: https://www.quora.com/What-is-the-difference-between-spark-context-and-spark-session",302738.0
133074,580323.0,Spark Session is capable of creating a spark context object also but the reverse is not possible. Spark Session is now the updated session with Spark 2.0,428646.0
134229,583585.0,Check out below stackoverflow link on similar query https://stackoverflow.com/questions/35243744/get-specific-row-from-spark-dataframe,313826.0
133076,580311.0,"In case if there are any Spark Sessions open, then close them using spark.close() and then close the notebook(s) as well. Then re-open the notebook and try creating a new session. Apparently, we are allowed to have only two open spark sessions and any further attempts to start spark sessions would be unsuccessful. In case if issue still persists, then raise a ticket with Corestack.",313826.0
133076,580708.0,Try to login again and close all other running notebooks. You can check it here also: https://learn.upgrad.com/v/course/208/question/132863,318448.0
133045,580184.0,We would be using Jupyter Notebook for PySpark. That is present within the Lab Access. Data science with python as shown in the video is not there for us.,311254.0
133045,580203.0,"As mentioned by Shayari, for spark module Jupyter Notebook is used with pyspark, and we also have to use it. First image you uploaded comes when you login into corestack platform. Once you click on Upgrad Big data - lab access details you will see below screen Click on Jupyter Notebook icon, use your credentials (username, password) and you are go to go to work as mentioned on video. Hope this will help.",317991.0
132963,579819.0,"When Spark reads the data it tries to infer the schema by default unless you give schema separately. And to infer schema Spark sample few records and try to infer the data type. With sampling Spark can not tell if the column is nullable or not. So it takes it as Nullable = True which is safer. Cause later when doing some operations where full scan of the data is required if Spark finds some field is null it would not cause any exception. Also if we pass schema we need to set inferschema flag to False. Regarding metadata: It is just adding some metadata, for example when we use create table column we can give the column description aslo. Which some tool can show. So that the end user will have an idea what that column is about. Similarly here we are giving the metadata so that later part if someone want to check what Count column really is they can see it.",318554.0
136381,589969.0,Transformations are functions which produces new RDD using existing RDDs. They have lazy execution. They only get executed when an action is called. Refer to list of transformations supported here. You an also write custom udf and they can be used as transformations. https://spark.apache.org/docs/2.4.3/rdd-programming-guide.html#transformations You can read more about spark transformation at spark documentation.,317689.0
136381,589827.0,"Spark Transformation is a function that produces new RDD from the existing RDDs. Transformations are lazy in nature i.e., they get execute when we call an action. They are not executed immediately. Two most basic type of transformations is a map(), filter().",301121.0
134860,585749.0,The error is indicating that the spark context has stopped. Run the code to create the spark session again and then try importing the csv file.,313826.0
134860,585748.0,Create a Spark Session to load the Dataframe. Spark Context is for RDDs.,428646.0
134860,585750.0,It seems your SparkContext has been stopped and because of this you are getting this error. Run your code to initialize SparkContext again and then try to load your data. Alternatively you can restart notebook by logout and login again in the notebook Or You can use Kernel -&gt; Restart and Run All or Restart and Clear output without logout from jupyter notebook.,317991.0
134860,585727.0,"--------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) &lt;ipython-input-78-e747497d7440&gt; in &lt;module&gt;() ----&gt; 1 air = spark.read.format(""csv"").option(""header"", ""true"").load(""/common_folder/heart.csv"") 2 air /usr/local/spark2.4/python/pyspark/sql/readwriter.py in load(self, path, format, schema, **options) 164 self.options(**options) 165 if isinstance(path, basestring): --&gt; 166 return self._df(self._jreader.load(path)) 167 elif path is not None: 168 if type(path) != list: /usr/local/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args) 1255 answer = self.gateway_client.send_command(command) 1256 return_value = get_return_value( -&gt; 1257 answer, self.gateway_client, self.target_id, self.name) 1258 1259 for temp_arg in temp_args: /usr/local/spark2.4/python/pyspark/sql/utils.py in deco(*a, **kw) 61 def deco(*a, **kw): 62 try: ---&gt; 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e: 65 s = e.java_exception.toString() /usr/local/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name) 326 raise Py4JJavaError( 327 ""An error occurred while calling {0}{1}{2}.\n"". --&gt; 328 format(target_id, ""."", name), value) 329 else: 330 raise Py4JError( Py4JJavaError: An error occurred while calling o855.load. : java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext. This stopped SparkContext was created at:",310179.0
132035,577034.0,"Hi Vipul, DAG is basically used for optimization which generally cannot be handled by MapReduce programs. DAG scheduler breaks the operation into stages(such as Map, Shuffle , Reduce). These stages are then submitted to the task scheduler. Each stage can have different number of tasks depending on the size. The task at each stage is combined to be sent to the next stage. For eg. Tasks at the Map stage are combined based on key to be submitted to the shuffle stage Below link will help you understand better https://stackoverflow.com/questions/25836316/how-dag-works-under-the-covers-in-rdd",308673.0
132131,577313.0,"Check your code, variable splits is missing or not been executed. Check if you have executed all line of code.",428646.0
133119,,nan,
133120,580756.0,"1-It's up to you what the name you want to use. This app name helps to debug your job. Whatever the app name you will give that will refelt on spark UI. if you dont provide app name, a random app name will be given by spark. so it will be difficult to find your running job if you dont provide the app name. 2- Perfoamance is not releated with app name.",312746.0
133120,580622.0,"appName is the method of class SparkSession.Builder It Sets a name for the application, which will be shown in the Spark web UI. So whatever name you want to show in web UI you can specify in appName. Source: https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/SparkSession.Builder.html Hope this helps.",317991.0
132782,579172.0,"Before running SQL query you need to create the view of the Spark Dataframe Create it, the issue will get resolved. Refer to the Notebooks provided.",428646.0
132789,579201.0,"Upload the notebook to your jupyter console. When you will launch your jupyter environment, it will show a tree with all available notebooks, now there on top right you will see a upload option, click on it and upload all the provided notebooks.",428646.0
132796,579237.0,Did you created the Spark Session? Please follow all the steps.,428646.0
132769,579117.0,Use | not &amp; df2= df.filter((df.ArrDelay&gt;=0 | df.DepDelay==0)),428646.0
132769,579118.0,Tried that as well. Same issue,305653.0
132769,579122.0,"from pyspark.sql.functions import col df.filter((col(""ArrDelay"") &gt;= 0) &amp; (col(""DepDelay"") ==0))",428646.0
133024,580207.0,"If the problem is not resolved already, then try to logout from everything (Jupyter Notebook, Corestack platform) after saving your work. And then again log back in. It worked for me sometime.",317991.0
133024,580315.0,Were you able to resolve the issue? I am facing it too.,305655.0
133024,580321.0,"Hey Rajarshi, Please write to Corestack about the issue. It looks like a error due to lack of resources.",428646.0
133024,580407.0,Were you able to resolve the issue?,318335.0
133024,580093.0,This seems to be an issue related to the environment (lack of resources). Try again to establish the session . You may have to try multiple times. Similar query: https://learn.upgrad.com/v/course/208/question/132960,313826.0
132823,579565.0,"You can follow below link, it contain ways to add column to a spark dataframe https://stackoverflow.com/questions/33681487/how-do-i-add-a-new-column-to-a-spark-dataframe-using-pyspark Hope this will help.",317991.0
132823,579936.0,"No need to add this to the original data, just perform your analysis using this only.",428646.0
132823,580190.0,"No need to add review_length. just execute the above line review_length= spark.sql('SELECT helpful, overall, reviewText, reviewTime, summary, asin, LENGTH(reviewText) AS reviewLength FROM review') then use review_length.show. This will give dataset result for further analysis",301114.0
132823,579313.0,"df.withColumn(col_name,col)",308673.0
132823,580597.0,I get max length of review_length as zero - while in the table I cud see values for it. How do i resolve this,319759.0
132827,579421.0,"Hi Jyotishri, If the reviewer name is considered to be unique Then we might actually loose out the reviewers whose name was same but were actually two different individuals giving the review But if we take the reviwerId to be unique Even if we have two reviewers with same name, we'll not loose them out &amp; which should be the ideal case So, when the same reviewer has different ID, but in real-life scenarios two or more reviewers might have same name but must have different reviwer ID. Hope this helps!",318355.0
132854,579469.0,Both the cases are with respect to Originating airpot. We need to check the arrival and departure of flights at the origin airport,308673.0
132863,580705.0,"If it is still not solved, please follow the steps below: 1. There are some spark sessions still running due to which we are not able to create a new session, so we need to kill all other sessions. 2. Login to hue, go to Job browser tab at the top right corner beside S3 and hdfs browser. Check what all sessions are running and kill them by pressing the kill button. Then you will be able to create a new spark session.",318448.0
132863,579847.0,"A walkaroung for the same is, Check if the spark session is in active byexcute only spark after importing.It should give the version and details. After that at the end os the execution type spark.stop() and then again execute ""spark"" before moving to another notebook to make sure this one closed.",318386.0
132863,584456.0,I do perform a restart and it works,301646.0
132863,579495.0,1.Close all other notebooks and shutdown them. You need to go to the running tab provided in the jupyter notebook tree and then shutdown all the running notebooks. 2. Always close the spark session by using spark.close() command. 3. Try writing to CorseStack if this fails.,428646.0
132863,579467.0,"It takes few minutes to load. If you face the issue again, try restartig CoreStack again",308673.0
132863,579576.0,It takes some minutes to load the files.,311004.0
132885,579562.0,"Similar sort of problem has been addressed in below link. https://learn.upgrad.com/v/course/208/question/132863 And you can also try to logout and login again to Jupiter notebook. It happened with me also, above steps did help. Hope this will hepl.",317991.0
132885,580713.0,"Try killing all the Spark sessions which are running : Login to hue, go to Job browser tab at the top right corner beside S3 and hdfs browser. Check what all sessions are running and kill them by pressing the kill button. Then you will be able to create a new spark session.",318448.0
132885,579620.0,Please try logging in again!,318427.0
132285,577633.0,"Spark Data frames ideally looks same but are different from Pandas Data frame. The way you access data from a Pandas DF is different from Spark DF. Yes, you need to learn this new API, but Spark has an advantage. You can use and implement your SQL knowledge on top of Spark DF by creating a view. In that way, it would be easy for you to complete most of your task regarding data analysis.",428646.0
132658,580514.0,"First of all, the file /common_folder/Advertising.csv does not have TARGET label to split into features and Label column. Are we missing something here? Are we generating Label column before running the LR? Please clarify.",311115.0
132658,578745.0,Refer to this:https://towardsdatascience.com/building-a-linear-regression-with-pyspark-and-mllib-d065c3ba246a,428646.0
133101,580700.0,"I am able to solve it, we need to kill the running sessions.",318448.0
133101,580440.0,"As per some of the comments on this discussion https://learn.upgrad.com/v/course/208/question/133024 , restarting the laptop helped in resolvign the issue. Try if this helps in your case.",313826.0
133101,580437.0,It looks like a error due to lack of resources. Write to Corestack about the issue as directed by TA in below link: https://learn.upgrad.com/v/course/208/question/133024,317991.0
133294,580760.0,"You can use .option(""header"",""true) along with the load option for reading a csv file. Please see below for sample code This has been covered in some of the notebooks, suggest you to revisit . df = spark.read.format(""csv"").option(""header"", ""true"").option(""inferSchema"", ""true"").load(""&lt;csv file path/csv-fule.csv"") You could also use spark.read.csv(&lt;csv name&gt;, header = ""true"")",313826.0
133299,580777.0,"Once logged in to jupyter, check if there are any notebooks already running by clicking on the ""Running"" tab. Shutdown if you find any. Then try launching a fresh notebook. If issue still exists, then suggest you to raise it with corestack.",313826.0
133299,580774.0,I think there is no exact solution for this error: 1. You have to keep trying by logout and login again after some break. 2. Also shutdown all the running notebooks. 3. If problem persist for long time then you can write to corestack / upgrad team. I also faced same problem and logout &amp; login after sometime solved the problem.,317991.0
132811,579299.0,Can you try this by creating a spark context? I think that will work.,428646.0
132811,579310.0,"Hi Sumit, Its still not working. Here's the screenshot of the same. I'm able to understand from here is that we need to call range() function of Python which would give us a collection to pass on to the parallelize method to create RDD. But what's happening here is that range() functino of PySPARK is getting called &amp; range() function in PySPARK return a dataframe. That's why its giving this error.",318355.0
133493,581443.0,"There could be possibility of two things that leads to the error you are getting: 1. You have created the SparkSession with different name than ""spark""...--- in this case make sure you use the same name which you used to create SparkSession in all the code. 2. If the name is same then you might have not executed the code which contains defination of ""spark"" --- In this case make sure to execute the code having defination before referencing it. i.e. execute the code first which contains ""spark"" defination and then execute the code of review_length.",317991.0
133493,581333.0,You need to create a spark session before creating data frames and tables. Here spark is the name of the spark session created and please refer to given notebooks on how to declare this.,317689.0
133493,581199.0,"Here, "" spark"" should be the name of the spark session that you would have created at the beginning. Probably you have created the session with some other name and that needs to be used here. Replace spark with the SparkSession name that you have created.",313826.0
132418,578092.0,use read.csv not read.load,428646.0
132418,578113.0,"If you are using load then before that use .format(""csv"") ie; spark.read.format(""csv"").load(path)",318554.0
132383,578010.0,"Hi Rex, This is a syntax error. Using spark = SparkSession.builder.appName('SparkML').getOrCreate() would resolve the error. Hope this helps!",318355.0
132421,578106.0,share your code.,428646.0
132421,578111.0,Try using df.columns[0] instead of df.columns(0),318762.0
132421,578109.0,"Believe you are using pyspark. Here, df.columns is a list. So you should use df.columns[0] to get the first element. In case of scala the syntax is (0) to get the first element.",318554.0
132421,578134.0,"Hi Naga, The simplest solution to rename a column is df.withColumnRenamed('age', 'age2') where 'age' is the original column name &amp; 'age2' is the new column name. I strongly recommend to check any function you need in the pySpark documentation: https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html Hope this helps!",318355.0
132421,578107.0,"You nedd to use the colum name not the index withColumnRenamed(df, “existingColName”, “newColName”)",308673.0
134584,584662.0,"Group By just groups based on the required criteria specified on column value or any other. Aggregate functions are used along with groupby to aggregate the values like SUM, AVG, COUNT etc.",301121.0
134584,584664.0,"Aggregated functions and Group by clause goes hand in hand when you want to view data in terms of an atrribute or multiple attributes For example, select count(*) , attribute X from table t group by attribute X How can I see results after groupby function without any aggregated functions? Use window functions instead of group by for the above question.",311254.0
134584,585453.0,Groupby orders the data based on specified criteria while aggregate does the math part of it like taking the count or sum or average of the ordered dataframe.,319759.0
134584,584754.0,"Group by and agg functions works together. And for your query ""groupby function without any aggregated functions"", have a look into below link for the explanation: https://stackoverflow.com/questions/20074562/group-by-without-aggregate-function",317991.0
132960,579884.0,"I also had faced the same issue. After checking the log file which gets created with each connection, realized that the issue was happening due to unavailability of resources. Suggest you to try creating the session by re-running the command again. You may have to try multiple times before it executes successfully.",313826.0
132960,580117.0,"Restart the Jupyter server, then you will be able to run the commanda without error. click on Quit and then start on next screen.",317412.0
132434,578228.0,"Hi Bindu, Try using this from pyspark.sql.functions import min, max df.agg(min(df.age), max(df.age)).show() Hope this helps!",318355.0
132434,578306.0,"While importing in the data use header =True df = spark.read.csv('housing.csv', header='true')",428646.0
132434,578567.0,@sumit shukla - let me know where is the answer for this..thanks,319759.0
132973,579814.0,"After taking group by and aggregate you can use: .sort(col(""avg(ArrDelay)"").desc())",428646.0
132879,579497.0,Refer to this: https://learn.upgrad.com/v/course/208/question/132863,428646.0
132984,579891.0,"Yes, the answer coming matches one of the option given in the question.",317991.0
132497,578333.0,"DF = spark.read.format(""csv"").load(""/path of the CSV file"") DF_1 = DF.select(""col1"",""col2"",""col3"",""col9"",""col10"").show()",428646.0
132675,578806.0,"df.columns ['TV', 'Radio', 'Newspaper', 'Sales'] the list of columns are as above",319759.0
132675,578803.0,"The error is basically indicating that There is no column named ""Features"" yet you are trying to access it. The available fields are: TV, Radio, Newspaper, Sales' according to the error message. So, I would suggest to recheck if the column names that you have and whether they have been renamed them successfully or not. Also check for minute mis-spellings. Hope that helps.",317998.0
132675,578807.0,"I am not sure where am I trying to access a field called ""Feature""",319759.0
132675,578856.0,"I haven't reached till this module, but I think you need to split the dataframe into 'label' and 'features', then provide these to lr.fit(). Check out the pyspark documentation for usage details.",313826.0
132675,580507.0,"In the Linear Regression, the file /common_folder/Advertising.csv DOES NOT have the TARGET variable (LABEL) column exists, to split in to feature and label column. Are we generating this column or it is missing from the file?",311115.0
132675,579614.0,I used these 2 additional parameters and it helped me resolve the issue. 1. setFeaturesCol --- Pass the features column to this as a parameter 2. setLabelCol --- Pass the column that is used as a label to this parameter.,303670.0
133689,581963.0,"Two types of Apache Spark RDD operations are- Transformations and Actions. A Transformation is a function that produces new RDD from the existing RDDs but when we want to work with the actual dataset, at that point Action is performed. When the action is triggered after the result, new RDD is not formed like transformation. source: https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/ You can refer to the following for more insight on what constitutes action and transformation and how they work: (The first image itself clearly demonstrates it, do go through it.) https://medium.com/@aristo_alex/how-apache-sparks-transformations-and-action-works-ceb0d03b00d0 Hope that helps.",317998.0
132633,578660.0,29-40 40-50 50-60 60-70 It will not include the last bucket. Read the official documentation here,428646.0
132992,580188.0,"It means you have not sorted the data in descending order. They have given the query as review_length= spark.sql('SELECT helpful, overall, reviewText, reviewTime, summary, asin, LENGTH(reviewText) AS reviewLength FROM review') You can alter the above query by adding order by descending on reviewText. Then execute this review_length and use review_length.show(). This will arrange the dataset in descending order. The fourth from the top would be the one with reviewlength 29087.",301114.0
132992,579929.0,Check the row where the reviewLenght==29087 and answer accordingly.,428646.0
132640,578686.0,"Intercept is nothing but the value of b0(beta-0). It indicates the valie of the dependent variable y when all the predictor vatiable(s) are equal to zero. Foe Simple Linear Regression, the equation is given by y = b0 + b1x where b0 is intercept and b1 is slope.",313826.0
133393,580986.0,"You can change the appname before creating the Spark Session. Once the seesion is created, you cannot change it.",308673.0
132577,584458.0,for cloudera it has to be /usr/lib/,301646.0
132577,578577.0,"In a CDH deployment, SPARK_HOME defaults to /usr/lib/spark in package installations and /opt/cloudera/parcels/CDH/lib/spark in parcel installations. In a Cloudera Manager deployment, the shells are also available from /usr/bin .",308673.0
133304,580846.0,"Can use below syntax to find coefficient and intercept:- print(""Coefficients: "" + str(lr_model.coefficients)) print(""Intercept: "" + str(lr_model.intercept))",320103.0
133304,580808.0,Use the .coefficients and .intercept attributes. Please find below the official documentation. Go to linear regression and click o python for example implementation. https://spark.apache.org/docs/2.1.1/ml-classification-regression.html#linear-regression,313826.0
100939,433647.0,"I think only alpha and beta values are changed .The sample size ,standard deviation remains same.",308638.0
100939,433665.0,You don't have to bother about those values. Use only the alpha and beta values to explain the scenarios that have been asked.,313517.0
100947,433619.0,Check this post:https://learn.upgrad.com/v/course/208/question/99284,313517.0
100949,,nan,
100953,433658.0,"You are doing the hypothesis test using 2 sets of alpha and beta. In one case you have alpha =0.05 and beta=0.45. In the other case, you have alpha = 0.15 and beta=0.15 Your task is to explain a situation where either method would be more preferred than the other. So basically explain a situation where conducting the hypothesis test using alpha=0.05 and beta=0.45 is better than conducting the same test using alpha =0.15 and beta=0.15. Similarly, explain a situation where conducting the hypothesis test using alpha=0.15 and beta=0.15 would be better than conducting the same test using alpha=0.05 and beta=0.45",313517.0
100953,434173.0,This helped me further understand the concept and answer the question - https://web.ma.utexas.edu/users/mks/statmistakes/errortypes.html,312376.0
100957,433678.0,"No, you don't need to do any sample test using XLSTAT. Just write the procedure that you need to follow while doing A/B testing. Check this post as well:https://learn.upgrad.com/v/course/208/question/100575",313517.0
100957,433931.0,"Hi Yatheendra, The TA has already answered the question very well. I have checked the link which the TA has provided. To add on to this : you may refer the A/B testing session under Session 3 of Hypothesis Testing to get a complete understanding and also the steps involved in the this testing.",318355.0
101029,434038.0,You need to calculate the probability of drug to not produce the satisfactory result and probability of drug to produce a satisfactory result.,320687.0
101029,434148.0,First you need to propose a suitable probability distribution than find out the probability of p(&lt;=3).,314621.0
101029,434313.0,based on previous samples will give the ratio or percentage to be used in distirbution and as earlier reply mentioned it needs to be atmost 3 i.e x&lt;=3,311386.0
101029,434361.0,First : Proposed Distribution Second :Based on the Proposed Distribution find the probability of success and fail. Thrid: Find the probablity for the event i.e P(X atmost 3 i.e &lt;=3),318476.0
101029,435415.0,"Given a small sample of 10 drugs, you are required to find the theoretical probability that at most, 3 drugs are not able to do a satisfactory job. a.) Propose the type of probability distribution that would accurately portray the above scenario, and list out the three conditions that this distribution follows. b.) Calculate the required probability.",301618.0
101034,434032.0,Yes we have to submit the assingmnet in PDF only.,308639.0
101034,434054.0,Yes assignment has to be submitted in single file and that too in pdf format. You can also find sample solution file in below link https://learn.upgrad.com/v/course/208/session/18671/segment/95306 Hope this helps.,317991.0
101034,434146.0,Yes it is already mentioned in assignment.,314621.0
101034,438355.0,yes we don't have any other tool use there,304693.0
100132,429112.0,Basically we have 2 scenarios. with different values of type 1 error and type 2 error. Having low error is good. We have to think about situations related to drug when which error is more critical. We need to interpret by examples in what kind of situations type 1 error should be more small that is 0.05.,317990.0
100132,431467.0,Please watch the video where the two errors are explained with the help of an example of criminal case.,304319.0
99559,425888.0,Same doubt.,305845.0
99559,425890.0,"hi, this statement is basically saying that the probability of a success(satisfactory result) is 4 times than a failure(no satisfactory result) in a single event i.e. for a single drug. considering that total probability(either success or failure) is 1, you can find the probability of success and failure individually for further use. hope this helps.",311686.0
99559,426446.0,"If p is the probability for satisfactory result and p1 is the p is the probability for non satisfactory result, then p +p1 =1 4x + x = 1 x= 0.2 p = 4*0,2 = 0.8 and p1 = 0.2",319319.0
100425,431935.0,Please go through the below links: https://learn.upgrad.com/v/course/208/question/100271 https://learn.upgrad.com/v/course/208/question/100194,311160.0
100344,431551.0,"I think it should be the general steps that we follow for A/B testing. It should not be specific to any tools, since there are many tools available in market.",310467.0
100344,431795.0,You just need to give the basic procedure we follow while conducting A/B testing. Doesn't have to be specific for a particular tool,313517.0
100344,431903.0,I guess it should be a general procedure used to perform AB testing for ad campaign.,311004.0
100344,432149.0,"Hi Dheeraj, You may refer to the module: A/B Testing Demonstration under Module 7 Session 3 Hope this helps!",318355.0
100956,434138.0,What about null and alternate hypothesis? Would that remain same in both the scenario?,314621.0
100956,433674.0,Do let me know if you have any other doubts.,313517.0
100956,433918.0,Thank you so much for posting this! :),318355.0
100956,434171.0,This helped me further understand and answer the question - https://web.ma.utexas.edu/users/mks/statmistakes/errortypes.html,312376.0
100956,434709.0,"Hey Mahima/Other - please help, sorry I'm still confused here; Do we have to consider the two conditions as; Type I error &gt;&gt; which method first/second to be preferred? Type II error &gt;&gt; which method first/second to be preferred? Because I cannot imagine of a condition where first sampling method preferring over second! Am i interpretting it correctly? or wrong??",316349.0
100956,436188.0,"Hi Hemant, I think this post by Mahima serves the purpose itself. To understand the question better I'll recommend you focusing upon: So basically explain a situation where conducting the hypothesis test using alpha=0.05 and beta=0.45 is better than conducting the same test using alpha =0.15 and beta=0.15. Similarly, explain a situation where conducting the hypothesis test using alpha=0.15 and beta=0.15 would be better than conducting the same test using alpha=0.05 and beta=0.45 which is an excerpt from the post. Hope this helps!",318355.0
101113,434657.0,It has to be to buy the drug and we need to describe why A/B testing is useful amd the steps involved to find the same.,311160.0
101113,434671.0,People should buy the drugs based on tagline and not just open the online ad; Basically it is conversion of those visits into actual transactions,301121.0
101113,434885.0,Its actual conversion - track visiting webpage ( based on a particular tagline) till purchase. Mention the steps how this can be achived using A/B testing.,306009.0
101113,438352.0,yes AB testing means that,304693.0
101131,,nan,
101137,434944.0,You can do below activity First build the hypothesis ( H0 &amp; H1 ) Test the hypothesis using the Critical Value method Then test hypothesis using the p value method. So we need to do it by both method,311861.0
101137,435483.0,"Hi Ankur, Below are the steps:- 1)You need to first frame the null and alternate hypothesis. 2)Make specific conclusions with both the methods about your hypothesis.(i.e p-value and Critical Value Method) Hope this solve your query. Thanks and Regards Somnath",314617.0
101137,438351.0,both are same but two methods of solving,304693.0
101147,435660.0,H0 is &lt;= 200 and H1 &gt;200 We are expected to solve this using critical value method and p-value method. I hope I have answered your question.,301121.0
100194,430089.0,"We have been taught about few probability distribution methods and in the process, we were explained about the conditions on which the methods have been developed. Accordingly, we need to propose the type of probability distribution which is best suited for the given scenario and list out its conditions.",311160.0
100194,431417.0,One way to know which method to use is ; to just count how many given preconditions are there in the question and then estimate with these number of conditions which particular method can be used . Hint : there are not many things given .,315560.0
100194,432854.0,"Based on the count of possible outcomes and chance of occurence of each outcome in each trail we can decide upon the type of distribution. Go through the lectures on probability lectures,You will get to know easily",308638.0
100218,430481.0,It refers to probabality of drug passing the quality assurance check,317996.0
100218,431212.0,"you can use this information to calculate the probability of succefful and unsuccesful trials by using the appropriate law ( addition, multiplication, etc.)",310509.0
100218,431460.0,probability of a drug to produce satisfactory results is 4 times the probability of the drug to not produce satisfactory results.,304319.0
100218,431476.0,"The statement is a bit confusing. If the probability of drug not working is p, then probability of drug to be working is 4 times more (4 times in additional than one already exsting time) i.e. 4p+p = 5p. The other way of considering the same statement is quiet simple. If the probability of drug not working is p, then the probability of drug working is 4p (most people assume this in English). The same analogue of confusion English can be realised from the below link from stack exchange : https://english.stackexchange.com/questions/7894/x-times-as-many-as-or-x-times-more-than I would like to request the concerned evaluators to clear the doubt what the unusually used English is creating. In real world, while working, we have the liberty to clear the doubts from the analysts or concerned person. We should not be evaluated on the del meaning English to impact the results completely.",311729.0
100218,431477.0,the drugs will produce a satisfactory result its chacnes are 4 times as compared to not giving acceptable or good results. Probabilty of satisfactory to not satisfactory is 4:1 .,315560.0
100218,432187.0,It simply states that the probability of drugs producing satisfactory result =4/5 and probability of drugs not producing satisfactory result =1/5 during the quality assurance of previous batch of products.,320685.0
100262,431063.0,You should define conditions in which each of them are suitable. They are not asking why you prefer over the other but to justify each in different conditions.,310974.0
100788,432978.0,Yes that can be done too as per the guildline.,318476.0
100788,432941.0,rather use word for your work its better,318017.0
100788,433073.0,"You can take a mixed approch, whereever you need to write formulas or calculate do it on paper and take a snap and copy into the word document",306011.0
100788,433220.0,If your handwriting is good such that people can read and understand then why not ? If you really do so I appreciate it. I have forgot to even sign well or write my name after spending more than 20 years on keyboard. I guess you are fresher 😀😀,310501.0
100788,433405.0,"Yes, if you think that is helpful you certainly can do it! I was hesitant to writing the same in Word and wanted to take that approach earlier but after I tried Word I felt it was extremely convenient to use equations in Word. These buttons have everything you need:",309211.0
100788,433423.0,Yes you can but it would be more presentable if you combine excel and word.,300706.0
100788,434149.0,You can solve problems which involve calculation in paper and rest you can write down in word.,314621.0
100788,438359.0,yes please do what ever you can write and attach pics for formulas,304693.0
100871,433232.0,in such cases you can take the sample standard deviation as Population standard deviation in real life cases you wont get population standard deviations. The approach you are thinking is correct :),318017.0
100871,433235.0,That is the standard deviation of the sample. sigma/root(n) gives the standard error of the sampling distribution.,303229.0
100871,433239.0,Got the right explanation. Thank you guys.,318435.0
100871,433644.0,Standard deviation of sample is given.We can use it in finding standard error ie s/root(n),308638.0
100871,434145.0,Yes the given standard deviation is for sample.,314621.0
100871,433921.0,"Definitely yes, the standard deviation given in the question is of the sample only. Let's suppose if the standard deviation given in the question is of the population. This means if they can find out the standard deviation of the population then they can find out the mean of the population as well. Then, there is no good in finding out the range in which the population mean would lie. Hope this helps!",318355.0
99627,431924.0,"A different sampling procedure, I believe, means that the level of significance (α) for the hypothesis testing is chosen as 0.15 (instead on 0.05). As α and β are connected, this choice leads to to a new β i.e. 0.15",305653.0
99627,426634.0,Sample size and sample variation are the two of many sample parameters that impact the value of alpha and beta. Please refer the below link for the answer to the above question. https://learn.upgrad.com/v/course/208/question/99628,310467.0
99627,426449.0,Same doubt,319319.0
99627,427250.0,Sample size is same in both cases only sampling technique used is different.,318426.0
99627,427728.0,"Hi In the question it is told that Now, a different sampling procedure is proposed- what is it 's meaning.",320689.0
99628,426606.0,Which method. Two hypothesis testing method or two scenarios with different alpha and beta values. TA please explain.,319319.0
99628,426475.0,I think sample size and standard deviation can be different for the second sampling procedure. However TA can verify.,310467.0
99628,426476.0,"As per me selecting <span style=""font-size:13.5pt;line-height:107%; font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:Arial;mso-bidi-theme-font:minor-bidi;mso-ansi-language: EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA"">significance level 0.05 or 0.15. Which is better option keeing in mind the same test condition.",319319.0
99628,426495.0,It doesn't matter if the sample size and standard deviation are same or different. Based on Alpha and Beta values we need to explain under what conditions would either method be more preferred than the other. TA please verify,310467.0
99628,426535.0,"Yes, Keerthi AK's answer above is what you need to do. Based on the given alpha and beta values only, you have to explain the conditions under which either method would be more preferred than the other. You have to use your knowledge of errors and the given values to justify the situations which you're proposing in your answer where one method is better than the other. So basically, explain at least one situation where the first method is better than the other. Similarly, explain the opposite scenario. In both the cases, interpret the given error values only to justify your answer of why the first method or the second method is better than the other.",313517.0
99628,426621.0,"Let me rephrase the question again. So basically you are conducting the hypothesis test using two different sampling procedures. In the first procedure, when you conduct the hypothesis test, you get one set of alpha and beta. Similarly in the other procedure, when you conduct the same hypothesis test, you get a different set of alpha and beta values. Now, your task is to explain a situation where the first procedure should be preferred over the other one while conducting the hypothesis test. And similarly, explain a situation where the second procedure is more preferable. If it is still not clear, please ask again.",313517.0
99628,431162.0,So in this scenario the α = 0.15 and β = 0.85 ? Yes ? Didn't got the whole question correctly !,311466.0
99628,432103.0,"So, here we need to explain using which condition is better 1st condition : alpha = 0.05 and beta = 0.45 (OR) 2nd condition : alpha = beta = 0.15 only one condition should be high-lightened correct? or do we need to explain the other condition also?",312756.0
99628,432436.0,"Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of a and ß are controlled at 0.15 each It means both α = 0.15 and β = 0.15 ?",312019.0
99628,432764.0,Hint: Check https://www.khanacademy.org/math/ap-statistics/tests-significance-ap/error-probabilities-power/v/introduction-to-type-i-and-type-ii-errors for better clarity. It helps.,310501.0
99628,432950.0,"So, the question is all about which scenario is better 1st one or the 2nd one? Based on our selection, we need to justify our answer theoretically? 1st scenario : 0.05 &amp; 0.45 2nd scenario: 0.15 &amp; 0.15",312093.0
99584,426229.0,"It's hard to remember the basics of probability after you've gone through all the sessions :) I would suggest you to go through the basics of probability module again and revise the list of distributions. You will get it. And for the hint, it is asking for P(X&lt;=3)",310974.0
99584,426674.0,You need to calculate all the probability of getting x &lt;= 3. Nice explanation for the same on below link. https://mathbits.com/MathBits/TISection/Statistics2/binomialAtMost.htm,318368.0
99584,432653.0,How do you get the p for this?,307710.0
100536,432159.0,"Both, i.e, We need to explain, in which scenario 1st condition is better than the 2nd. Also, in which scenario 2nd condtion is better than the 1st.",311160.0
100536,432775.0,Both the scenario needs to explained. More Details can be found from: https://learn.upgrad.com/v/course/208/question/99628,318476.0
100536,433411.0,You need to pick the both the controlled parameters and explain in which situation you use alpha (and why not beta) and in which situation you will use beta (and why not alpha),309211.0
100271,431062.0,"If you find the probability distribution type, you will realize the conditions they are asking about. Revise the course once. You will get it.",310974.0
100271,431415.0,One way to know which method to use is ; to just count how many given preconditions are there in the question and then estimate with these number of conditions which particular method can be used . Hint : there are not many things given .,315560.0
100273,431769.0,"you better refer the sample document that they provided download, By looking you will understand , How limited words you can explain.",305804.0
100273,431074.0,You have to limit your answer to 200-300 words per question that's it.,301655.0
100273,431061.0,"No, it doesn't include blank lines and images. As they said, words :)",310974.0
100604,432558.0,Recollect the probability distirbution method and the rules you have studied and try to find P(X&lt;=3),314197.0
100604,432607.0,Go through the probability distribution modules and the rules are given in it ! and you have to find probability that at most 3 drugs are not able to do a satisfactory job. i.e.(PX&lt;=3) !,311466.0
100604,432660.0,"I can make out the n, r values...but what about p? I feel its 0.5..but someone mentioned it will be 0.2. Can you explain how?",307710.0
100410,431720.0,"I believe it should be fine as long as the corret answer is conveyed. However, I guess it is better if we just follow the instructions as it might help the evaluators to look for the same in all the markings. Rest TA can confirm.",311160.0
100410,431721.0,Yes. These things depends on one comfort. I myself have used equations in MS word to write formuales.,318429.0
100410,431792.0,"Yes, you can use the Equation editor if you want. It depends on you to choose either method.",313517.0
100410,431869.0,"As far as evaluator can understand and you can express, you can use any of both as per your choice. Pen-Paper-Image is mentioned for people who dont feel handy with MS Word especially when it comes to writing mathematical equation. With this they can also complete assignment without any overburden of work.",318770.0
100341,431723.0,We need to explain only the properties of the formula we are using and the assumptions we are making if any excluding the words required to solve the question. I hope I answered your query.,318429.0
100341,432102.0,There is specific concept which we ned to use to solve the problem and the concept has certain properties whihc we need to narrate,310629.0
100341,432865.0,"We need to explain the properties of the method/thereom we are using with respect to the question.Go through the lectures of confidence limits,you ll come to know",308638.0
100575,432632.0,Do we have to do it by making an excel and then carry out the A/B testing or do we have to explain only the steps i.e how we do A/B testing according to our scenerio here ! T/A could you confirm it please ?,311466.0
100575,432349.0,"In the question it is mentioned about two taglines which were proposed for the campaign, and the team is currently divided on which option to use.Explain why and how A/B testing can be used to decide which option is more effective. Give a stepwise procedure for the test that needs to be conducted. You need to explain A/B testing as per the case mentioned in the question.",311254.0
100575,433302.0,Why:- Give reason why A/B testing should b used Steps:- Give steps to conduct A/B testing like dividing traffic/population in two parts etc.,318377.0
100488,432070.0,"Given the values of alpha and beta, you need to justify which one is suitable/favorable for which conditions,",310974.0
100488,432069.0,My understanding is that you are given set of values for α and β which gives the probability of Type 1 and Type 2 error. You are not happy with the outcomes due to these error and hence you are changing the sampling procedure to get new set of values for α and β. Which values are preferable under what condition?,317514.0
100488,435882.0,"Hi Susmita, According to question we are required to make out an conclusion for both procedures :- 1)One in which we have alpha = 0.05 and beta as 0.45. 2)One in which the value of apha is 0.05 and beta are controlled at 0.15 each. And we have to frame an example related to drug example given in the question,in which these two processes would give us favourable outcomes Thanks and Regards Somnath",314617.0
100591,432439.0,Thanks Amith. Still my doubt is finding the Z value. We should find for bothe the sides. 98.5 and -98.5 like above example ? or direct 97% z table ?,312019.0
100591,432423.0,"<p class=""MsoNormal"" style=""mso-margin-top-alt:auto;margin-bottom:0in;margin-bottom: .0001pt;line-height:normal""><span style=""font-size:12.0pt;font-family:&quot;Times New Roman&quot;,serif; mso-fareast-font-family:&quot;Times New Roman&quot;"">to summarise, have a sample with sample size n, mean X-bar and standard deviation S. Now, the y% confidence interval (i.e. the confidence interval corresponding to y% confidence level) <p class=""MsoNormal"" style=""mso-margin-top-alt:auto;mso-margin-bottom-alt: auto;text-align:center;line-height:normal"" align=""center""><span style=""font-size:12.0pt; font-family:&quot;Times New Roman&quot;,serif;mso-fareast-font-family:&quot;Times New Roman&quot;"">Confidence interval = <v:shape id=""Picture_x0020_5"" o:spid=""_x0000_i1026"" type=""#_x0000_t75"" style='width:286.5pt; height:45pt;visibility:visible;mso-wrap-style:square'> <v:imagedata src=""file:///C:/Users/IBM_AD~1/AppData/Local/Temp/msohtmlclip1/01/clip_image001.png"" o:title=""""> X-bar - Z*S/Square Root N,X-bar + Z*S/Square root N where, Z* is the Z-score associated with a y% confidence level . In other words, the population mean and sample mean differ by a margin of error given by <v:shape id=""Picture_x0020_6"" o:spid=""_x0000_i1025"" type=""#_x0000_t75"" alt=""\frac{Z^{*}S}{\sqrt{n}}"" style='width:24.75pt;height:31.5pt;visibility:visible;mso-wrap-style:square'> <v:imagedata src=""file:///C:/Users/IBM_AD~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.gif"" o:title=""sqrt{n}}""> .Z*S/Sqare Root N",307843.0
100591,432540.0,When you could inference or it is given a case of 2-tail and significance level is specifient then only use this approach.,307843.0
100591,433409.0,Entirely dependent on whether it is a single tailed test or a two tailed test. Whatever Alternate hypothesis you choose suggests whether you should consider single tailed test (LCV/UCV) or a two-tailed test. (LCV and UCV) I had to go through these two videos in the following segment a couple of times to get the grasp of the concept. hope it helps you : https://learn.upgrad.com/v/course/208/session/18006/segment/91603,309211.0
100618,432622.0,"I don't think so, rest TA can confirm.",318429.0
100618,432665.0,Use the drug case only.,313517.0
100594,432490.0,you can use either of the two methods both methods gives the same result,318017.0
100594,432557.0,"You have to calculate the required range using both critical value method and the p-value method, because it is clearly mentioned in the questions that we have to find the probability using both the methods",301655.0
100594,432630.0,In Question 3 : We have to do 2 hypothesis testing methods to make your decision. In Hypotheis testing there two testing ! 1) Z-testing a) Critical Value Method b) P-value Method 2) T-testing I would suggest that you can use both testing method on from Z-testing and other one is T-testing !,311466.0
100594,432763.0,Hope this point for 3rd question only right ?,312019.0
100757,432768.0,I think. It means the α = 0.15 and β = 0.15,318451.0
100757,432772.0,Previously the value of alpha was 0.05 and beta was .45. Now they are controlled to 0.15 for both alpha and beta. We need to indentify which scenario when the above two condition added more benefits to the hypothesis testing.,318476.0
100757,432856.0,the values of α and β are 0.15 each,308638.0
100757,433224.0,Probability of Type 1 error is denoted by Alpha. Probability of Type 2 error is denoted by Beta. And both this are controlled to be 0.15 for this scenario.,310501.0
100633,432677.0,"Given the same sample data (size, mean, and standard deviation) of the previous question, test the claim that the newer batch produces a satisfactory result and passes the quality assurance test. https://learn.upgrad.com/v/course/208/session/18006/segment/91600 and other lectures",310508.0
100633,432664.0,in the question you have to check whether the sample present in question 2 will pass the test or not according to the condition given,318017.0
100633,432704.0,"Hi, 1) Here you need to check if to reject the null hypothesis or not , also showing all the calculations. 2) you need to do this using two hypothesis testing methods. size, mean, and standard deviation you have to take from previous question.. Hope it is clear..",305129.0
100633,432773.0,Approch is as below: 1) Formulate your Hypotheses(H0 and H1) 2)Test the Hypothese using the Critical Value menthod 3) Test the Hypothese using the p value menthod,318476.0
100633,432938.0,You have to check whether to reject the null hypothesis or not based on the statistic information given in the previous question by critical value as well as p-value method.,300706.0
100633,432867.0,"Hi Formulate null and alternate hypothesis based on first part of the statement ,use the sample statistics given in previous question such as sample size,standard deviation and apply the procedure of testing of hypothesis to check to reject null hypothesis or not .Apply the critical value and p value method to do so",308638.0
100795,433000.0,"p(0,1,2,3) as you have p(x) &lt;=3",318017.0
100795,433088.0,"p(0,1,2,3) as have condition p(x&lt;=3)",306996.0
100795,433105.0,You have to find P(X&lt;=3) = P(0)+P(1)+P(2)+P(3) as per condition given in the problem.,311466.0
141282,610701.0,"Hi, Please treat the null values, your data contains null values.",344894.0
141282,610574.0,fill nan by 0 using fillna(0),318005.0
91362,381865.0,"pay attention to the word MAY . incase of excel, locally stored files MAY or MAY NOT have historical data. but, Data Persistence is a feature of data warehouse. ie, It has to be persistent with historical data and hence will have them in their records. hope that helps.",317998.0
91362,381874.0,"well,data warehouse data ( historical and current) is always stored in tables and schemas,regarding the question MS Excel files, any business always stores the data after initial process of cleaning and format which obeys to their metadata.so that it can consumed any time for analysis,Excel data may get corrupted and cannot be plugged on the fly for the analysis if we open it after 4 yrs(assume)",301115.0
91362,382203.0,Generally you would want to store historical data in the warehouse. You can maintain history of the records by creating type1 and type2 tables. Read more here: http://datawarehouse4u.info/SCD-Slowly-Changing-Dimensions.html,317689.0
88607,366822.0,Yes it is highly recommended to go through all of these modules not just for graded assignments but for whole course as well because these are the base for this whole course and you might have to revisit them over and over again. Furthermore going through these modules will not cause any harm but improve your overall grip in DS field.,318495.0
88607,366793.0,It's totally based on your knowledge and experience of particular technology. As i am new to Python but has some experience in SQL so I will go through the Python module before attempting the Graded question related to python..,320073.0
88607,366979.0,It is highly recommended to enhance your knowledge. It will make you prepare for the advanced topic.,301648.0
88607,367054.0,If you have a basic understanding of it and have some prior hands-on experience then it is not required but if you are new to programming world you must definitely complete those as it will help you understand the future modules.,316133.0
88607,367356.0,"Yes, it is. I completed the Python Graded Qs and the questions did refer to the pre-course. However, if you are already versed with the topics then perhaps you can proceed without completing them.",319302.0
88607,403904.0,"It depends on your comfort level. If you are already well versed in that particular tool, then you can attempt the graded questions without going through the pre-course. Although it is advisable that before attempting the graded questions of a particular topic, you should go through that module.",304319.0
87455,361687.0,"even if answer is right, it would be good to see how upgrad answer looks like",300694.0
87455,363933.0,"Hey Manoj, your assignment will be evaluated and you will get to know where you committed mistakes and where you can improve it along with what all you got right. You will definitely improve yourself with the comments provided.",319721.0
87455,363995.0,"Yes, Right. Thanks",320008.0
85717,352120.0,"Hi Aman &amp; Rajat, Thanks a lot for the inputs and deatils i will go through the above links:-)",320008.0
85717,352009.0,"Proper course material along with a lot of reference material is already given by UpGrad itself. However, if you still wish, you can try 'tutorialspoint' to have a quick introduction to the topics.",317987.0
85717,352023.0,I'm sharing short video playlists which will be very beneficial for quickly learning the basics. Python Basics Coure: https://www.youtube.com/watch?v=pgBdFgLngkk&amp;list=PLxxA5z-8B2xm1yUDAh2_pXGWBTePjCa3n Python OOPs Tutorial: https://www.youtube.com/watch?v=ZDa-Z5JzLYM&amp;list=PL-osiE80TeTsqhIuOqKhwlXsIBIdSeYtc SQL tutorials: https://www.youtube.com/watch?v=fA0jpjwi4J8&amp;index=3&amp;list=PL-osiE80TeTsKOdPrKeSOp4rN3mza8VHN I Think this is pretty much that will cover all the basics in minimal time. For Advanced level like for developing software you can refer below link but i guess it's not needed here. Python Advanced(recommended for application development only): https://www.youtube.com/watch?v=oVp1vrfL_w4&amp;list=PLQVvvaa0QuDe8XSftW-RAxdo6OmaeL85M Hope this helps :),318495.0
85717,352486.0,"Hello Manoj, all the pre-requisites are covered as an introductory course to the main course. It's alright if you are new to coding or if you haven't had any experience with databases before. Course1, Introduction to Data Management, is a prep course to get you acquainted with different applications and languages we use in data sciences. Do this thoroughly you'll be good.",319721.0
85741,352478.0,"No, there are no graded questions for Tableau available in my dashboard as well.",311502.0
85741,352529.0,I believe the Graded Assessments will be available once the course starts on 30th Sep '18.,313826.0
85741,365535.0,"No, i too got the same message that ""will be available when cohort begins"". so i think maybe after sep30 will get the graded questions",300733.0
95957,406945.0,is it a question :p,318017.0
95957,406987.0,Yes. It is a question with approach for getting the right outcome in Tableau.,310508.0
95600,404553.0,BF - Balls Faced SR - Strike rate Pos - Batting position,306248.0
95980,407289.0,"The intention here is to find the ""total runs"" scored in all matches where he was out ""caught"" . this has to be analysed for each opposition and the country against which the maximum runs have been scored has to be found out. I hope it's clear.. I agree the question has a certain amount of ambiguity.",319898.0
95980,407371.0,"Here ""maximum"" implies total runs.",313826.0
88354,366515.0,https://kb.tableau.com/articles/issue/error-cannot-load-packaged-workbook-tableau-reader please follow the steps mentioned in the link above and try opening it,319721.0
80289,325442.0,"Box Plot Simplified Step 1: Take all the observations along y axis Step 2: Count the number of observations and draw line at the middle point i.e if there are 10 obs then middle is between 5&amp;6, if there are 11 obs then middle is 6 th one Step 3: Now similarly take the middle of upper half and draw the line; Take the middle of lower half and draw the line; Step 4: Draw Whiskers joining the box to the highest point and the lowest point Naming conventions (refer image below): Q1 = 1 st Quartile Q2 = 2 nd Quartile Q3-Q1 = Inter quartile range",304694.0
80289,345214.0,"The answer for the box plot question can be either of two - 1- house maid , enterpreneur 2- housemaid, housemaid. Below is my solution graph -",312746.0
84691,346956.0,Just drop response dimension on Marks card. It will automatically create the bin for you.,312746.0
84691,346957.0,Response on the colour side right?,305129.0
84691,347235.0,Try this and note the comment for histograms:,310974.0
84691,347541.0,"Sorry but its not working for me, no option is highlighting other then the bar chart and stacked bar chart :(",305129.0
84691,347547.0,"Instead of dragging the measure, try double clicking on it and then select histogram",310974.0
88499,366328.0,"I believe it scans the complete data set from the source and plots the graph with the count depending on the bins which it creates. We are still computing quantity, and the fact that there can't be a quantity field unless an actual order. We hypothetically are relatiing to orders. From the graph in example, there are 899 records with quantity value as 1 and so technically, there are 899 orders.",316253.0
88499,406483.0,"It automatically computes the bins and visualize according to that in the most readable format. However, you also assig Bins manually.",315560.0
86351,356240.0,"It was clear when I watched it, please re-try",310974.0
86351,358292.0,Yes I am also facing same issue..it was showing same screen,300716.0
86351,359961.0,"Please copy paste the links below if it it is not pointing you correctly; let me know in case of any questions ""Box-Plots I"" link takes you to the following video: https://www.tableau.com/learn/tutorials/on-demand/reference-lines ""Box-plots II"" link takes you to the following video: https://www.tableau.com/learn/tutorials/on-demand/box-plots",309211.0
86351,361860.0,"Thanks Harshendra, I copied the link from your answer and pasted in the browser, it worked then.",318397.0
85326,350093.0,Please follow below thread :- https://learn.upgrad.com/v/course/208/question/80289,312746.0
85326,351018.0,"If your question is, ..",319721.0
85326,351037.0,"To understand box plots, we need to udnerstand percentiles. Please refer to the following video incase to better understand percentiles. https://www.youtube.com/watch?v=mDJvDRvvDXo Let us now understand about box plots with an example of an entrace exam to a University. There are 100 applicants to applu for a scholarship/ but the university has only 100 seats. The University gives addmission to the best 100 of the 1000. So the top 10% of the applicants, regardless of their scores go",319721.0
85326,351063.0,"To understand box plots, we need to understand percentiles and quartiles. Please refer to the following video to better understand them. https://www.youtube.com/watch?v=mDJvDRvvDXo Now let's try to understand box plots. The above figure will help us understand box plots. We see a line inside the box denotes the median value: 50% of the data lies both above and below that value. The boundaries of the box denote the 25th and the 75th percentile, with the whiskers, 10th and 95th percentile, denoting the lowest and the highest values in the data set. To get a better idea on what these percentiles are, consider a Scholarship exam of a University as an example. 1000 students applied for the exam and the exam is conducted for 100 marks (with only multiples of 1 as the possible score. There are no decimal scores.) And if any student scores 100 marks then he/she will be given a scholarship of 5 lakhs by the government and if they score 42 they won't be given any scholarship . These people are not given scholarships by the university. The rest are given scholarships ranging from 1lakh to 10thousand by the university. 10-25 percentile: 10,000 scholarship 25-50 percentile: 30,000 scholarship 50-75 percentile: 60,000 scholarship 75-95 percentile: 1,00,000 scholarship Look at the figure and observe the marks associated with different percentiles. Please understand that the marks are only for your reference. 99 marks - 95 percentile means that there are 95% of 1000 that is 950 peopel who scored below 99 marks. Which means, there are 50 people who had a score of 100 and 50 people will be given 5 lakhs scholarship. 85 marks - 75 percentile means that there are 75% of 1000 that is, 750 people who scored below 85 marks. So, there are 250 people with a score of more than 85. From, 75-95 percentile: 1,00,000 scholarship, we can say that 200 people whose score is between 99 and 85 will get a 1,00,000 scholarship. Similarly, we can say that 250 people(75-50 percentile) whose scores are between 85 and 78 marks will get a scholarship of 60,000. 250 people who scored between 78 and 65 (50 - 25 percentile) will get a scholarship of 30,000. 150 people scored between 65 and 42. They will get a scholarship of 10,000 from the university. We can observe that all these can be easily represented by a box plot. Now, what about those students who won't be given a scholarship by the university? For that we have whiskers. In the example above whiskers are at 95 percentile and 10 percentile. The top 50 students and the bottom 100 students won't be given any scholarships. And they are called outliers. Outliers are the data points which have absurdly high or low values, compared to the values of the rest of the observations. It might not be the way in this case.",319721.0
87537,364194.0,"Hey Sham, please ensure that you downloaded Tableau Desktop. And try re-installing it again, please.",319721.0
84096,346003.0,Check this :,310974.0
84096,346029.0,Here we need to choose age as histogram and the response as the categorical beacuse based on the response of the yes\no this can be resolved and marked with color and label it to get the result.,307843.0
84096,346007.0,you can see shaded part in orange color for response= yes. 1033 people having age between 30-35 respnded yes which is maximum.,312746.0
84096,346327.0,age group 30-35.,300690.0
84096,359955.0,"As you can see, 30-35 has the largest percentage hence that should be the choice",309211.0
84096,355153.0,"I manually edited the bin size on the Age(bin) to exactly match 5 and found that the histogram that came up was slightly better. Also, the records selected met the boundary conditions perfectly unlike the histogram created with automatic bin size i.e., bin 30 has records where in age is between 30 and 34 only. Please check out the screenshots below. Open for comments / feedbacks.",313826.0
84095,346008.0,you can see the X axis where i change default Major tick marks (bin width) from 10k to 18k. here is how to change the bin width :- 1- Right click on X axis 2- click on edit axis property. 3- change major tick marks from automatic to fixed. and provide the value 18k. Final histogram will be as below :-,312746.0
84095,346332.0,54K-72K,300690.0
84095,346005.0,"Check this To tweak the width, follow the below steps:",310974.0
87477,361855.0,"Hi Ravi, You can create a box plot like in the screenshot below: Once you create the box plots and hover over the individual circles, you will be able to identify the jobs and the respective balances, thus identifying which one are closest to median for each response.",318397.0
87477,404501.0,"i tried to do the same thing, but am getting strange distribution as attached. please help.",310509.0
87477,363516.0,First take the avg balance in th row. Add a refernce line and choose box plot with whiskers extending to maximum extent. Choose colour however you like. Then add the response to the details in marks card. and mark it as a dimesnsion by right clicking and selecting the dimension parameter. Now add the job to the colours in the marks card. Add the Y dimension from data pane to columns. Now you will have two box plots. Check the jobs which are closest to the median.,318576.0
87570,364213.0,"Use different types of representation for different dimensions. One dimension can be a fill representation, another can be a shape. And overlap them, then. The process to do this is explained in the video below, check it out. https://www.youtube.com/watch?v=bwUj2NZHTC4",319721.0
87687,364110.0,It will be clear once we go through the statistics part of the course where we understand more on the measures of central tendency.,310974.0
87687,364284.0,"Hey Bhaswati, can you please be more specific on where you couldn't follow the lecture? It's always better if we are sure about what we know and what we need to improve on, before moving on with learning. Try going through the segment on box-plot again, and post your questions here. We'll be happy to help with all those. It'll be better for other learners too.",319721.0
87688,364288.0,"Hey Bhaswati, Can you please refer to the question below, https://learn.upgrad.com/v/course/208/question/85326 To understand box-plots better, some knowledge about percentiles is needed. I detailed the particulars with an example in the comment above, please refer to it.",319721.0
88235,365214.0,https://learn.upgrad.com/v/course/208/question/87477 Please refer to this question. It's a similar questiona and it'll help you.,319721.0
88242,365476.0,"I believe you have created the box plot correctly. The question asks answers for NO and YES. Maybe you did opposite mistakenly. (YES and NO) Another posibility is that the values for entrepreneur and house maid are very similarly close to the median which might have led to incorrect reading of the plot. I would recommend considering values for this particular example. For NO : Median: 1424, Entrepreneur: 1495, House Maid: 1354; Difference is least for House Maid . For YES : Median: 1806, Entrepreneur: 1819, House Maid:1793: Same difference for both. Pick any one . In options we dont have HOUSE MAID, HOUSE MAID as option. So HOUSE MAID, ENTREPRENEUR is the answer. This is what I did. Willing to know if anyone did it better which gave the answer just by observing the box plot.",315471.0
95587,404564.0,I am not sure which question is this..but You have selected the stacked bar chart.Box plot gives the below graph,300687.0
96147,409148.0,hi please go through the below link it might help you in understanding . https://www.google.co.in/amp/s/www.geeksforgeeks.org/univariate-bivariate-and-multivariate-data-and-its-analysis/amp/,318017.0
96147,409166.0,"Hey Abhishek, you can contact your student mentor regarding the same.",319721.0
87268,359668.0,"Right click on the column isn't showing delete/drop option? My license got expired, otherwise, would have testetd and shown you.",310974.0
87268,361229.0,"I don't think Tableau allows the deletion of any column or data, otherwise that would mean editing the data source itself. We can hide the data if weed don't need it. There are 2 types of data connection: Live - here we are always connected to the data source and get live updates(deleting column not possible) Extract - here we don't have a live connection but the extract needs to be refreshed to receives updates from originl data souces( deleting column still not possible) The splitted columns are created for easier visualization but are not part of the original data source",304815.0
87120,358767.0,I believe the license key will be provided once the program starts.,313826.0
87120,359802.0,"You will be receiving Tableau keys once the program starts. For now(prep stage) you can refer to ""Tableau Public"" for getting a hang of Tableau. Hope it helps.",301618.0
87120,361216.0,"yes, you will get the licence for an year once the program starts.",312479.0
87120,363823.0,Thanks for the confirmation. I have the same question.,310518.0
88594,366859.0,"Hello Gaurav, Have you: 1. Ensured that the install file is not corrupted 2. Tried running the installation as administrator 3. Ensured your machine meets the minimum system requirements ( Download Now | Tableau Public ) Kindly go through above point. Let me know if you face any issue !",308966.0
84696,346991.0,2018.2.0 is what I'm using,310974.0
96355,,nan,
88484,366332.0,https://www.tableau.com/products/desktop/download,301649.0
88484,366857.0,"Hello Pratik, here is the Link to download Tableau for 32 bit system : https://downloads.tableau.com/esdalt/10.3.1/TableauDesktop-32bit-10-3-1.exe Happy learning!",308966.0
81490,331971.0,"My issue is resolved now. Please follow the link https://kb.tableau.com/articles/issue/verifying-tableau-message-and-install-does-not-complete. Warm Regards, Rajesh R",300708.0
81490,361223.0,I did not see any issue on Mac. I have installed it on Mac High Sierra.,312479.0
81490,366363.0,Prefectly installed on Mac OS Mojave as well !!,318004.0
77909,312100.0,when u will go to home page then from drop down u can select and click course 2.u will see the course details of tableau.as shown below,305847.0
77909,312094.0,"Hi Ravindra, You will find the Tableau in Course 2 ( Statistics &amp; Exploratory Data Analysis). Here is the link to the Module: https://learn.upgrad.com/v/course/208/module/5341 Please let us know if you still can't find it.",301621.0
77909,312389.0,Follow below steps: program -&gt; courses -&gt; select couse 2 Now you can find Tableau course.,304397.0
77909,313620.0,thanks a lot !!!!,300698.0
84927,347950.0,Try the solution provided here https://kb.tableau.com/articles/issue/Cannot-Install-Tableau-Desktop-Error-0x80070666,310974.0
84927,347989.0,"Thanks Ram, it worked...",306011.0
88689,367236.0,"It will be given when we start course 3, because from course 3 Tableau would be required. till then it is not needed. that is what my mentor said.",317998.0
88689,367306.0,You can use Tableau public for the module. it is free and without any trial period. it has all the functionality you need to complete the module. you cam download it from below link: https://public.tableau.com/en-us/s/,320073.0
88689,367440.0,Tableau public can be used if ur license is expired. you can download and use it . upgrad will provide the license when we will need it in Course 3 probably. refer link. https://public.tableau.com/en-us/s/,305847.0
88689,367733.0,I have three suggestions: 1. Tableau public 2.use Virtual Machine and reinstall OS and tableau for every 15 Days 3.In AWS(amazon web services) cloud and get Virtual machine to practice which will cost some money per hour of Practice. I recommend you Second option. But Prof.RC mentioned that once Program starts we would get the Tableau Student license.,306008.0
88689,367888.0,you can also use web version of Tableau.,300708.0
88689,368092.0,"Guys/Gals, U can apply for Tablue Stuent edition from the following link: -U can obatin a one yesr valid key for both mac and win. -U can submit ur own ID card or ur siblings or ur anyone u know studying in a collage or school - the verification process is very easy and quick ,and works . - https://www.tableau.com/academic/students",317822.0
82446,339792.0,Any version upto 10.4 support windows 32 bit.I think after that from 10.5 only support 64 bit.,306038.0
82446,338670.0,Are you using latest tableaur of 32 bit version ? That looks the only possibility other than moving to 64 bit machine and using what has been suggested.,301557.0
88586,367062.0,Tableau activation key is not sent by UpGrad yet and you don't need it to complete 30th assignment.,310974.0
87752,364298.0,"Hey, Lalitha, can you give the details of the error? What is the error? If you share more details, we'll be able to help.",319721.0
95421,,nan,
91068,380488.0,Go ahead and download the free Tableau Public (Desktop) version. https://public.tableau.com/en-us/s/download This should be enough for now.,318004.0
91068,380492.0,You can work on public edition of tableau. It's completely free. Just some pointers to remember Not all file types are supported You cannot save anything on your computer. You can save your projects only on tableau public site which can be shared publicly.,317995.0
86060,355826.0,Yes. my desktop is 64 bit.,311117.0
86060,352952.0,Can you get a screenshot of the issue?,310974.0
86060,353371.0,"Hi, can you verify if your laptop is 64 bit or 32 bit one. Also can you share a screenshot of the issue ?",301618.0
86060,355829.0,Thanx for reply. PFA sreenshot below.,311117.0
86060,355937.0,Please check the link below for some workarounds as suggested in Tableau's official website for this particular error and see if your issue gets resolved. https://kb.tableau.com/articles/issue/error-0x80070643-fatal-error-during-installation,313517.0
86243,354517.0,"You can go here - https://www.tableau.com/learn/training for Training videos. To see this videos, you have to create an account. Happy learning!",311502.0
86977,358214.0,"Why, it's logical right to join different data sources having a common variable. What is the confusion?",310974.0
86977,358881.0,"I agree with you Puneet and have reported it as a mistake to staff. You CAN join across multiple files/data sources in Tableau and I have ofcourse tested it (I am on 2018.2) IMHO, the use of blending is for doing UNIONS as opposed to joins - ie vertical (adding more rows) vs horiontal (adding more columns) expansion of data from Tableau website: "" Beginning with Tableau version 10.0, if the tables you need to analyze are stored in different databases, or workbooks (for Excel), or directories (for text), use the following procedure to combine tables using a cross-database join. """,300694.0
86977,358389.0,"Hi Puneet, Data Merging and Data blending are two different processes in Tableau. The question is asking when ""Merging"" will not work. Since the files belong to different data source, data blending will work here and not merging.Blending comes into picture when cross-database join is not a feasible option: Go through the below links for brief explanation on Data Blending and cross-database joins: https://onlinehelp.tableau.com/current/pro/desktop/en-us/multiple_connections.html https://onlinehelp.tableau.com/current/pro/desktop/en-us/joining_tables.html#About Functionality in newer version of Tableau: https://www.clearpeaks.com/cross-database-join-tableau-10/",305845.0
86917,357770.0,"Please check this - https://www.tableau.com/support/releases/desktop/10.4.10. Not sure, if you can install on 32 bit machine. You can refer : https://onlinehelp.tableau.com/current/desktopdeploy/en-us/desktop_deploy_download_and_install.htm Thank you.",311502.0
86917,358334.0,"Use the below link to download the new version of Tableau desktop. http://www.tableau.com/new-features/10.0 This has many new features like joins between multiple types of data sources, improvements in visualization etc. You can also check system requirements: https://www.tableau.com/products/desktop/download#system-requirements They mentioned Windows 7 or newer (64 bit) machine is required. Try it out.",319721.0
86917,358421.0,Thanks guys! Found the solution.,312376.0
87388,363727.0,"Cube is another type of data source, with predefined relational hierarchies. So, while analysing this data, we will have to follow similar steps though we are using Excel or Tableau and have a similar kind of analysis. Please refer to the link below to know further about using Cube data sources in Tableau. https://onlinehelp.tableau.com/current/pro/desktop/en-us/cubes.html One more link for excel: https://docs.microsoft.com/en-us/system-center/scsm/olap-cube-excel?view=sc-sm-1807",319721.0
87489,362005.0,"As per my understanding : A workbook can have connections with multiple data sources exists in different environment. For example, a database (SQL Server) reside in Windows environment and another database (MySQL) resides in Linux environment. Now, if you want to make a join (inner, outer, left etc.) between tables stored in these two databases, then you can go ahead with the ""Add Connection"" to make connections with these databases. These connection strings will be stored in a same workbook. Also, you can fetch the records from these databases to apply analytics. The ""new data source"" option gives you a liberty to connect with a new data source completely. For example, earlier you were doing analytics for the data stored in Amzon RDS and now you want to do the analytics for the data stored in SQL Server. The tables/records/data are completly independent from each of the databases. In this case, you can switch from a data source to another database at anytime.",312479.0
87489,363869.0,"A Data Connection represents the database server information required to log in (connect) to a database, and it also contains the relation for the underlying data. The relation may be a single table, a multi-table join, or a Custom SQL query that yields a result set. A Data Source represents the view of this data connection as it appears in the 'Data' pane on the left side of the Desktop product interface. This includes any changes you make to the fields, such as their caption, aliases, geographic role, default aggregation, etc. Additionally, a Data Source represents fields which do not exist in the underlying Data Connection, such as calculated fields, ad-hoc groups, sets, and Tableau-generated fields such as 'Measure Names', 'Measure Values', 'Latitude (generated)', and 'Longitude (generated)'. Collectively, this is sometimes referred to as the data model.",305845.0
87563,362870.0,"I am not sure if there is provision to delete columns/rows once we have pulled the data into tableau. Alternatively, we can hide/delete the unwanted columns/rows in the source excel sheet and then pull it intp tableau.",313826.0
87502,362000.0,"Tableau's definition of dimensions and measures is as below. Dimensions are those which cannot be aggregated or do some mathematical operations like sum, average, etc. Measures are those were we can perform mathematical operations. Yes, you can perform date related calculations on DOJ but they cannot be aggregated to draw any conclusions on the data source. Hence, they are dimensions.",314730.0
87700,364068.0,"Initial data: Since all the rows do not have an ""_"", we cannot do a ""Split"" and will have to do a ""Custom Split"" for all the rows. After custom split we have: Now, in the second column, all the rows have a ""-"" and we can split by that character. After split: Finally we can hide columns that we do not need (and also rename for more clarity). The rows which do not have any employment type are to be considered are permanent employees.",313826.0
87700,365032.0,"First we have to do a custom split on ""_"" which divides the column into two portion at the place of ""_"". note : after custom split we might get blank data in some columns since they are permanent employee ( they don't have additional identifiers i,p etc; lets name this column identifiers). then a split on ""-"" gives the split values of location and employee number. now for the final result. those columns which has blank values in identifiers are the permanent employees",311032.0
87441,361688.0,"make sure you have the latest version of Tableau and that you have enough system resources (RAM, etc) - it can take a while on slower computers",300694.0
87732,364180.0,Please check the following link from the Tableau official website for some workarounds and see if it resolves your issue https://kb.tableau.com/articles/issue/error-0x80070643-fatal-error-during-installation,313517.0
87732,364181.0,Please check the following link from the Tableau official website for some workarounds and see if it resolves your issue https://kb.tableau.com/articles/issue/error-0x80070643-fatal-error-during-installation,319721.0
87760,,nan,
86969,358073.0,Tableau desktop version is only supported by Windows and Mac. You can install the virtual box in Ubuntu machine and setup windows guest OS. You can install Tableau desktop without any issue in virtual box guest OS https://tecadmin.net/install-oracle-virtualbox-on-ubuntu/ https://itsfoss.com/install-windows-10-virtualbox-linux/,317845.0
89352,370519.0,"No, you cannot install Tableau Desktop on Ubuntu, it's only available for Windows and Mac however, you can install Tableau Server on Linux system https://www.tableau.com/products/server",317845.0
89352,370521.0,Currently Tableau is only supported in windows and iOS only. https://community.tableau.com/thread/241689,313691.0
89352,372306.0,Thank you that was helpful.,315661.0
88308,365466.0,"The video player does not seem to have the option to modify the playback speed. Also, they have provided a link to download the .mp4 video file on each of the videos, but they did not work for me. However, from the below link you can download all the videos module-wise. You can then play the videos in your favorite player and modify the playback speed as required. https://www.tableau.com/download-on-demand-training The drawback is that the file size are huge and also all the videos from a module will be downloaded which means that a lot of videos that are not covered as part of this course will also get downloaded",313826.0
88308,366170.0,"You should ideally be able to increase the speed to 1.25x or 1.5x. Refer to the image below and as you can see the speed video speed options are next to ""Auto"".",319721.0
94164,396709.0,You'll need to import the excel file or any other files that was used for creating this charts.. Try adding the; Sample - Superstore file and also the bank marketing files.. you can download it from the same module..,316349.0
95527,405263.0,"(-2,2) would mean that the range of observations is within 2 standard deviations of the sample mean. Similarly, (-1,1) would mean that it is within 1 standard deviation. You can learn more about them here:https://www.tableau.com/learn/tutorials/on-demand/reference-lines",313517.0
96426,411186.0,"Try using only one software at a time. In case you are working on either of the ones mentioned above alongside viewing the lectures on your browser, make sure that all other unnecessary applications are closed. It may also happen due to a lot of process-intensive tasks running simultaneously. Try closing them manually to improve the performance of your system.",313517.0
95617,404709.0,"Yes, that will be beneficial but fo you go through with the data then I think you can find out the significance of a column.",301648.0
95617,404741.0,"I agree..this is certainly good to have.. But in the current form, this dataset actually mirrors the some of the real life problems faced by analysts in this field. we have a huge dataset and (sometimes) we have our goals worded out. But in most cases I don't think we will be provided with a data dictionary.. that part will have to be done the hard way itself.. Maybe the course is designed in such a way to provide us a feel of working with raw data.",319898.0
95617,405525.0,"It gets easier, definitely when we have a data dictionary but like Ranjith correctly pointed out, in real life cases, we wouldn't have it.",319721.0
95617,405748.0,"think when we import the data, we have most of the data exaplined by the columns apparent name. if not we could raise a question with the TA.",312731.0
94614,399532.0,"Can you please elaborate your question. Are you asking how to use a column that has been created using ""Split"" function. If Yes, it can be used the same way as you use for other columns. But, please explain more about your question and what are you looking for?",316202.0
95279,402920.0,average salary will give the distribution of of avg salary over the months of a year whereas salary will tell about the salary for that particular month so we will get only one answer,318017.0
95279,402968.0,"Average salary is like 1+5+6/3 i.e. 1,5,6 are salaries and Salary is 1,6,8,9",315560.0
95279,403163.0,Try doing the below and calculate manually to understand the formula:,310974.0
95283,402991.0,"The plots will be available based on the measure and dimension you are trying to plot ie..the ones you have in your rows/columns. If you hover over the plots in the ""show me"" tab, it will give you a short info as to what combination of dimension and measure fits best.",309451.0
95283,403003.0,You should have appropriate number of measures and dimensions in order to plot the graphs. When you hover on different graphs it shows how many measures and dimensions are required for analysis.,317689.0
95283,403086.0,"Also, please check if tableau has rightly distributed the columns between Dimensions and measures. Tableau classifies the columns as measures as soon as it sees the numbers in the columns. So, you might want to shuffle them based on our analysis on the data set.",311160.0
95289,403054.0,"Being in analytics, i would say its must to use. it is very helpful to talk about your findings to senior management.",300735.0
95289,403155.0,Better to go with Virtual Machine and have it installed. I believe Tableau is required in the assignments and case studies in this course.,310974.0
95289,403432.0,yes it's needed specially for the data visualization and analysis of results like in presentation etc. it's also a good way to validate your results using plots from other sources like python.,317982.0
95289,403961.0,"I dont think it is optional. It needs to covered, it is also mentioned in the brochure. Better to cover it, the tutorial vidoes of Tableau on its website are helpful. And in my experience yes it DOES make the system slower in combination with SQL.",310508.0
95289,406190.0,"Update: Tableau Online works just fine for the assignment questions. I used Tableau Online with the free trial to complete the assignment questions. So, I don't think that there's a pressing need to install the desktop version of Tableau. Hope this helps!",306733.0
95289,406245.0,oh great.. 👍,310508.0
95729,405420.0,"What is the error that you see? If you aren't able to view the UI at all, uninstall and go back to the previous version.",310974.0
95729,405506.0,"Try to repair the software in following way: Click Start , and then click Control Panel . Under Programs, click Uninstall a program. you will also get repair option click on that . I also faced this issue but above process fixed it for me.",320073.0
95729,406267.0,Try reinstalling the software and check if it resolves the problem,313517.0
95743,405548.0,I beleive while installing MYSQL for Upgrad couse/assignements we installed the server also on localhost. If you also have done the same please try localhost as server.,316255.0
95743,405546.0,try the below link it might help : https://serverfault.com/questions/443887/cant-connect-to-local-mysql-using-ip-address-but-can-connect-from-remote-serve,318017.0
95743,405656.0,you can use MySQL ODBC driver to connect to Tableau. here is the link to install and configure https://dev.mysql.com/doc/connector-odbc/en/connector-odbc-installation-binary-windows-installer.html,317845.0
95760,405531.0,You can go through documentation of tableau for your query. Below is the link https://onlinehelp.tableau.com/current/pro/desktop/en-us/split.htm Hope this will help,317991.0
95760,405541.0,custom split will have a default split method wheras in split you can make the split according to your need,318017.0
95760,405713.0,Split funcction splits the fields sutomatically based on the common operators Tableau detects. Custom Split lets us to specify the operator for which the split should happen.,316202.0
95760,407208.0,You can use split or custom split options in Tableau to separate the values based on a separator or a repeated pattern of values present in each row of the field. The new fields created by the split or custom split are added to the data source as calculated fields.,314183.0
98674,420158.0,"Hello Naren, You can install from below link. https://www.tableau.com/support/releases/desktop/10.3.1#esdalt",320195.0
98674,420205.0,"You need to install Tableau Desktop. You can find it at - https://www.tableau.com/products/desktop You can try it for free for 1 month. After that you will need a licencing key. Upgrad must have already emailed you licencing key. Search for ""Tableau Key for Program"" on your registered email. It has the key &amp; also link to download the software .",318458.0
88560,366527.0,Great link.. These below links can also be useful for the above: https://www.evolytics.com/blog/tableau-fundamentals-dimension-vs-measure/ https://tekslate.com/dimension-vs-measure-in-tableau/amp/,318495.0
88560,366620.0,"In simple terms they are as belows Dimensions -Values such as categories, date, names, etc Measure - are the matrices/values which we want to analyses, let say the no. of qty, profile, sales, etc",312259.0
88560,366766.0,"Dimensions are like variables, measure is the value of these variables in as-is form or modified form",301644.0
88560,366823.0,"Dimensions: Dimensions are categorical variables. Each dimension provides more information about every data point/ row in the dataset. For Example: Gender, Name of Company you are working in, place of Birth, etc. We cannot apply numerical functions like - Avergae, Minimum, Maximum on these kind of variables. Measure: Measures on the other hand are numerical variables. They provide insights on entire dataset especially when we use aggregate functions like - Average, Minimum, Maximum. For Example: Your salary, No. of years of job experience, etc. Data is analysed &amp; visualised based on Dimensions &amp; Measures. A measure can be plotted against dimensions to obtain valuable inferences. Example: Understanding the trend in data using bar plot (Average salary VS Gender). Here average salary is numerical data or measure that is being analysed against gender(Male, Female) which is a dimension.",300730.0
88560,366902.0,"On a short note to memorize difference between Dimension and Measure is : Dimension: It is anything on which we can't apply aggregate functions(avg(), max(),count() etc.) like: Emp. Id (bit confusing for me sometime it is numerical, but it's providing details of Employee so we can't apply aggregate functions on it), name, date, place etc. Measure: Any variable on which we can apply aggregate functions, means numerical values. I hope you can memorize it without confusing too much.",318319.0
88560,366860.0,"In a nutshell, Measures are Quantitative attributes of data. Eg profit, sales. &amp; Dimensions are Qualitative attributes of data. Eg Age, origin.",310511.0
88560,366978.0,"Dimension - categorical value, eg: name of person, city, Measures - numerical value, eg: sales, profits, age",308437.0
88560,367120.0,"A dimension is the lens through which you are looking at your data. It is the way you “categorize” the data. The most common dimension is, arguably, time. For example, if I am looking at sales by month in 2018, then the dimension is the months of the year 2018. The measure is the easy part. This is the numerical values that quantify the data set that you are digging into to understand better. So if you are looking at sales by employee, then “sales” is the measure and employees are the dimensions.",317845.0
88560,367286.0,"Dimension : Something which you can count on fingers. They are categorial data. Eg. gender, no of years. Measure: which cannot be counted on fingers. They are quantiative values.",311119.0
88560,367324.0,"Dimension tells the quality of a measure, hence dimensions are qualitative and discrete/categorical values. Measures are something which can be measured, hence measures are quantitive and continues in nature. Example : Trend of sales in last 4 years, here we sum the sales (sum because data might be at day/month or quarter level) and group by year - we apply a mathematical function to a measure and group it with a dimension. Measures are always numbers and Dimensions are usually alphanumeric (combination of character with number or only characters). Important : Sometimes dimensions can also be numbers example role number of students, here you just think if you want to do sum or average of role number, No!! that do not make sense right? hence role number is a dimension :)",316255.0
88560,367304.0,"Meaasure According to Tableau’s Knowledge Base, a measure is a field that is a dependent variable; that is, its value is a function of one or more dimensions. Tableau treats any field containing numeric (quantitative) information as a measure. Dimension According to Tableau’s Knowledge Base, a dimension is a field that can be considered an independent variable. By default, Tableau treats any field containing qualitative, categorical information as a dimension.",319319.0
88560,367436.0,"in simple words, Measures are numerical values that mathematical functions work on. For example, employee salary column is a measure because you can find out a total or average the data. Dimensions are qualitative and do not total a sum. For example, employee name, location, or date are dimensions .",300687.0
88560,367529.0,"Dimension are categorical parameters whereas measures are quantitative values from a data set. e.g. There is a data set given for students having student id, student name, student marks. In this data set, student id and student name are dimensions/categorical parameters as we dont think there is need of any mathematical operation to be done on these columns i.e. sum of student names, sum of student ids is not valid. Student Marks is a measure/quantitative value on which we can perform mathematical operations like average of marks, maximum of marks, minimum of marks etc.",317811.0
88560,367648.0,"Measures are measurable/quantitative fields. We can perform mathematical operations on measures. E.g. Salary, spend. It is dependent on the context for e.g salary is dependent on the name of the employee, department. Dimensions are independent, non measurable fields e.g. Name, Region etc. We slice and dice measures based on dimensions.",301643.0
88560,367662.0,"Dimensions are the ones where you can categorize things such as gender, wiining a match(which can be binary) etc. Measures are majorly the numerical values to which you perform arithmetic operations. Tableau automatically differentiate the two from your data source. But sometimes when you need one variable categorized as measure to be dimension or vice versa you can do so, by dragging the variable and dropping into the other section",318159.0
88560,367786.0,Dimensions are the categorical/ Non numeric data based on which you analyze numerical data. This numerical data is also called as Measure. i.e. Sales by Employee. In this case Employee is non numeric/ dimension data by which we are calculating sales which is numberic or measure data.,318368.0
88560,367709.0,"Dimension - Dimension is a Catagarical Data. Dimensions can be both discrete and continuous. The dimensions are defined as names, dates, IDs, geographical info) Measure - Measure is a quantitative data. Measure will be aggregated (summed, averaged, etc)",314183.0
88560,368064.0,"DImension can not be aggregated while measure can be mesured ,aggregated and used for mathematical operations.",320635.0
88560,368591.0,"Let me answer this question with one example. Suppose there is a requirement to figure out the balance in each of branch of each state of a bank. so final output data will look like: State BranchName Balance Here state and branch are dimensions and balance is the measures so data is summarized at the state and branch level and the sum is applied at balance. Dimensions are variable at which data is summarized and measure is the variable at which function as sum, avg, a count is applied as shown in the above example...in data analysis, we always try to put all the variable in this format so it can be easily analysed with tools like Excel, Tableau etc.",300735.0
88560,368830.0,"Hi Rahul, Hope you are doing great! Simply put, if you are looking at the data of your organization and its employees in a detailed manner, wherein let's say you have a big Excel file with all details of all employees, then all of these fields in your Excel are dimensions, as they are providing specific and non aggregated information to you. On the contrary, let's say, your superior(s) in the company are frequently asking you to produce statistics like count of male employees, count of female employees, count of employees on bench, etc. In this case, the output Excel, for example, which contains aggregated numbers, would have columns which are measures. They provide aggregated information and helps to summarize the data. Hope this helps. BR, Suddhasatwa",310217.0
87766,364356.0,"I see that the "" Response "" field is appearing under the ""Measures"" category. Whereas when I pull the csv data into tableau, the ""Response"" field is appearing under the ""Dimension"" category for me. The original bank marketing csv that has been provided for this tutorial has values ""yes"" / ""no"" for the ""Response"" field and hence should appear under the ""Dimension"". So, looks like either you have converted the ""Response"" values to ""1"" / ""0"" or you are using some other similar data source wherein the ""Response"" has values ""1"" / ""0"". Suggest you to download the csv file again and try.",313826.0
88336,365708.0,Checkout the below link. See if this helps resolve the issue https://kb.tableau.com/articles/issue/error-could-not-find-the-referenced-file-replace-it-with-another-file,313826.0
88336,366062.0,"You need to download the Sample Super store file available in the introduction Session of Tableau module Even i faced the same kind of problem, once you download the file you can perform all kind of visualization analysis.",303673.0
88428,365953.0,You can right click on the response and convert it to dimension.,318370.0
88428,365901.0,Please find the answer,319444.0
88428,365892.0,"The bank marketing dataset for the tableau course is available on https://learn.upgrad.com/v/course/208/session/15814/segment/79967 Suggest you download the file in a new location, open the downloaded file in the excel and check fo rthe values under the field "" response "". The values should be "" yes "" or "" no "". Alternatively you could edit the file that you are currently using to change the value in the "" response "" field to have values "" yes "" or "" no "".",313826.0
88428,366139.0,"Hello Raja, Please refer below image. Click on dimension for salary.",320195.0
88428,366149.0,Still scatter plot option is not coming.,320195.0
88428,366185.0,Below are the steps I followed: - Column - Salary - as Dimension - Row - Avg Balance - as Measure - Analysis dropdown - aggregate measures - checked - Marks selection - Selected Circle - Marks - Response on color - Marks - Response on label,319302.0
88428,366380.0,The dataset wont open in mac when I try to open the BankMarketing.csv data set ? any suggestions ?,315242.0
88428,366787.0,"Hi Nishan, The question is a bit confusing so making it clear : Ques: For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. So we need to Find Salary for which ------ &gt;&gt; Average Balance corresponding to YES (Response) &gt; Average Balance corresponding to NO (Response) We have Salary column --&gt; Drag it to Columns so that it becomes the axis. It will select SUM(Salary) by default, click on the right small arrow on this SUM and select dimension so that Salary becomes the axis. Bring Balance to ROWS now, Change SUM(Balance) to AVG(Balance) , by again clicking on the right side and selecting measure to be Average. Now we have both Salary and Average Balance, now to see the pattern on two different type for the dimension (Response). Bring Response to color, it will be differentiated into two lines. One for Yes and other for NO. Note: For Scatter you can choose Marks to Shape. I am attaching the screenshot. Hope that helps! Rahul",315028.0
88428,368116.0,thanks for explaining the question!,312376.0
88428,403522.0,"we need to bring response on Label also , so that it clearly shows yes/No responses with graph.",313200.0
88438,365984.0,"Hi Arjun, If you mean that you want your Sum(measures) to appear as Avg(measures), it can be done by either of the following. 1. Just type in manually 'Avg' instead of 'Sum' by double clicking on the on the tab of ur measure. 2. Alternatively, from the pull down menu on the Sum(measure) bar select 'measure' and then 'average'. Now Sum(measure) will appear as Avg(measure).",315471.0
88477,366085.0,closed and re-opened tableau - issue resolved,316323.0
88477,368384.0,whenever such sort of thing happens restart tableau. I had faced similar things,317982.0
88477,400496.0,"Tableau takes all sales occurrences and creates an average for all of them. So automatic AVG aggregation in Tableau will do SUM (Sales) / COUNT (Sales). If we wish to calculate an average per day (or weekday) we do need to make an extra calculated field since we want to have AVG aggregation as SUM (Sales) / COUNT (Days). How do we do this? First, create a calculated field below, this will get us to a total daily sales. This calculation uses Level of Detail expressions which basically allow us to set the specific level of detail to run the aggregations, regardless of the filters or other levels of detail in the view. Then we can use this total daily calculation to add to our visualization that has Weekday or Order Date in it and change the aggregation to an AVG to see the desired results. Below shows how results vary from Tableau’s default AVG Sales calculation and our new AVG Daily Sales. Tuesday, for example, had only one sales occurrence so the numbers match up but as you can see other days produce totally different results due to multiple sales events. And now you know. Happy analysis.",303082.0
93748,395098.0,"Contact your mentor, he might be able to help.",313691.0
93748,395611.0,"A product key might already have been shared with you. But if you have formatted your system, after instaling tableau with this key and try reinstalling after formatting using the same key, there will be an issue. So, as JaiKrishna mentioned, please contact your mentor, they will help you out.",319721.0
88527,366382.0,I also have the same problem. Have you found out what solves it.,315242.0
88527,378311.0,type AVG(SALARY) in the Rows shelf. It will give you the average.,322683.0
88527,367294.0,"From the image you have posted, I can see that you have imported the data dictionary file. The data dictionary file is for your reference for the bank marketing data set. Download the Bank marketing file from this link: https://cdn.upgrad.com/UpGrad/temp/0f0e2afb-2b0b-4db9-8be4-0e2e59f836eb/bank-marketing.csv. Import this file into tableau as a text file not as an excel file.",313691.0
87501,,nan,
82562,338518.0,"I had the same doubt; check this out-&gt; https://learn.upgrad.com/v/course/208/question/80903 A few things I would like to add to Pulkit's answer:- 1. In the Marks Card on the left, you need to change &lt; Automatic &gt; to &lt; Shape &gt;. (so that instead of lines ( see picture-&gt; my old solution ), points appear and the graph looks like a scatterplot.) 2. I recommend you watch this video as well-&gt; https://www.tableau.com/learn/tutorials/on-demand/understanding-pill-types 3. Finally, when you click on the Show Me option on the top right, you'll notice that Scatter Plot option is not enabled. So, although the graph looks like a scatter plot, Tableau doesn't recognize it as one.",301652.0
82562,338678.0,Thanks for the solution.,304812.0
82562,343996.0,I got the solution as below method :-,312746.0
82562,345099.0,See below the scatter plot for both yes and no in same graph but in different pane it shows 8000 salry for avg balance is lower for the value yes than no.,307843.0
82562,365492.0,"For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. I hope this gives clearly that 8000 salary has only ""Yes"". (lower than ""no"" avg(balance) Steps:1) use Salary as Dimension&amp;Discrete Step2) Use Avg(Balance) as Measure in rows Step3) Use Response in colors and label as well. step4 ) After u select all the these variables --- tableau uses its own plot ---there after try to use Packed Bubbles plot. Clearly differeniate the values by labeling the response. This helps to clear our Question.",318846.0
87558,362862.0,"As per my analysis, the answer is March . The answer has been accepted as correct. Please see the attached image of the visualization that I have done. Suggestions welcome on any better way of visualizing.",313826.0
87558,363612.0,I did it like this. Pulled M onth and R esponse dimension in Label markscard. Got the answer by getting the darkest blue shade for a Yes in March. Values can be identified from shades by hovering over the rectangles. March was accepted as correct answer for me too.,315471.0
87558,366458.0,See this diagram. I have already filtered response = yes and also labeled Avg of salary. It is clearly showing Avg salary of March is highest.,311404.0
87558,369470.0,Yes answer is March. Since it required only Highest Salary for response = yes. I removed avg Balance and filtered Response to Yes only. Chart attached. Step1: Drop Salary on Size in Marks card. Step 2: Do Average of Salary. Step3: Drop Months to Marks Card. Step4: Drop Resposnse to Color in Marks Card. Step5 : Right click response and do filter on Yes. It is done.,318440.0
83563,343745.0,"You are right, explore more on this.",301557.0
83563,343749.0,"Yes Maya.. You are right.. Tableau connects to delimited text files (*.txt, *.csv, *.tab, *.tsv). using Text File option of New Data Source. It will automatically detects the separator and splits the data accordingly. BTW, on a lighter note - Process to bring the data into Tableau is called ""Import"" not ""export"".",312746.0
83563,343861.0,"Try to use ""Cleaned with Data interpreter"" option once..",312746.0
83563,355891.0,"Hi Maya, Just open the CSV file and resave it as an Excel workbook and then export the Excel file. Will work for sure. Or you can use the text file option, it will detect the delimiter(', ' in this case). Let me know if you are still facing the issue. Thanks.",315028.0
83596,344048.0,you can find it near right side of the chart. Dont try to find out it in the right most of your laptop's screen. e.g. Before applying dual axis (you can find the dotted line near red oval ) after applying dual axis :-,312746.0
86443,356140.0,"Hi Ruchik, The question is a bit confusing so making it clear : Ques: For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. So we need to Find Salary for which ------ &gt;&gt; Average Balance corresponding to YES (Response) &gt; Average Balance corresponding to NO (Response) We have Salary column --&gt; Drag it to Columns so that it becomes the axis. It will select SUM(Salary) by default, click on the right small arrow on this SUM and select dimension so that Salary becomes the axis. Bring Balance to ROWS now, Change SUM(Balance) to AVG(Balance) , by again clicking on the right side and selecting measure to be Average. Now we have both Salary and Average Balance, now to see the pattern on two different type for the dimension (Response). Bring Response to color, it will be differentiated into two lines. One for Yes and other for NO. Note: For Scatter you can choose Marks to Shape. I am attaching the screenshot. Hope that helps! Rahul",315028.0
80782,,nan,
88816,368411.0,good info...thank you,320606.0
82801,339565.0,The same question has been answered previously. Visit this link-&gt; https://learn.upgrad.com/v/course/208/question/82562,301652.0
82801,339979.0,Doing that I am getting as follows.How do I solve from here?,300690.0
82801,340156.0,"1. Change aggregation on Balance from Sum to Average . To do so, right-click on the Balance pill and select --&gt; Measure -&gt; Average . 2. Right-click on the Salary pill and select --&gt; Dimension .",301652.0
82801,340475.0,Thanks.got it.,300690.0
82801,359919.0,"You can have it as Rows(Average Balance) and Columns (Salary); For Salary the Dimension has to be turned on. Scatter plots/dots without connecting lines proved useful to me. You can see that only for one value i.e. Salary 8000, you have response 'yes' lower than response 'no'; for all other values you see that response 'no' is lower than response 'yes'",309211.0
82801,356010.0,"Hi, The question is a bit confusing so making it clear : Ques: For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. So we need to Find Salary for which ------ &gt;&gt; Average Balance corresponding to YES (Response) &gt; Average Balance corresponding to NO (Response) We have Salary column --&gt; Drag it to Columns so that it becomes the axis. It will select SUM(Salary) by default, click on the right small arrow on this SUM and select dimension so that Salary becomes the axis. Bring Balance to ROWS now, Change SUM(Balance) to AVG(Balance) , by again clicking on the right side and selecting measure to be Average. Now we have both Salary and Average Balance, now to see the pattern on two different type for the dimension (Response). Bring Response to color, it will be differentiated into two lines. One for Yes and other for NO. Note: For Scatter you can choose Marks to Shape. I am attaching the screenshot. Hope that helps! Rahul",315028.0
87467,361562.0,Could you check your tableau version? Make sure you are using latest tableau version.,318332.0
87467,364804.0,I am getting the same error! its the latest version that I have 2018-2-0. Do you have any version which is more newer than this???,317334.0
80903,329303.0,Recommend you to go thru the session again and attempt these to better your understanding.,301557.0
80903,331845.0,"Hi I will try to answer the questions to the best of my ability. 1. For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. Ans: We are basically required to find the Average Balance, so i will want to do a Average aggregation on my balance measure. As the question is for what salary, there shall not be any aggregate function on Salary and we would need to take salary as a dimension: You can see only for 8000 salary the average of yes is less than no. 2. What exactly is the use of Attribute and Dimension option (from drop-down menu of a pill )? Ans: As you saw in the above answer, Salary (which by default was selected as measure by tableau) was required to be converted into a Dimension to get the individual values. A quick cheat cheat for tableau, any column in Blue is Dimension/Discrete , any column in Green is Measure/Continues. 3. What happens when we select a Measure pill as a Dimension? (See Picture: Salary selected as a Dimension , although it's a Measure ) Ans: Selecting a measure as dimesnsion will ignore any aggregate function on that measure and you will get individual values from the column. However you will still see the column in Green as the axis on which the column is ploted is still continues. To convert it into a pure dimension, you would also need to select Discrete after electing Dimension. Hope this helps",306725.0
80903,365491.0,"For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. I hope this gives clearly that 8000 salary has only ""Yes"". (lower than ""no"" avg(balance) Steps:1) use Salary as Dimension&amp;Discrete Step2) Use Avg(Balance) as Measure in rows Step3) Use Response in colors and label as well. step4 ) After u select all the these variables --- tableau uses its own plot ---there after try to use Packed Bubbles plot. Clearly differeniate the values by labeling the response. This helps to clear our Question.",318846.0
80903,365299.0,I think a key thing to note is that the above graph is a 'circle views'- same as line graph but using circles to mark discrete points it is not actually a scatter plot because the scatter plot option is disabled: See attached screenshot - correct me if I am wrong Below is an actual scatter plot - using Balance (not avg Balance - I don't think it is possible to have a scatter plot with avg balance since avg is calculated over each discrete salary point):,300694.0
86522,356532.0,"Tablueau will be available to you once you complete these 5 modules and after the course is launched. Before the course starts these 5 moduels are like prep modules which would cover the pre-requisites of the course. Don't worry, you will get Tableau modules.",319721.0
86522,357071.0,Tableau is available as Course 2 under Courses in the top left corner of the Program tab. I could see there are quite a few questions being asked in the discussion forum now but I am sure we can ask questions about Tableau as it is part of the preparatory program.,314730.0
86522,357108.0,Tableau in the course moudule 2,318476.0
86522,357766.0,"Hi Denny, I have already answered this question, please visit this link: https://learn.upgrad.com/v/course/208/question/85253",318355.0
86522,358177.0,"Hi, Its another course available as Statistics and Exploratory data analytics once you click on course 1 button.. select the second option..",305129.0
86522,358267.0,Its under course 2... and you can start in parallel.,317418.0
88881,368688.0,thanks !,302739.0
88881,368410.0,"Pull the Salary measure on the Column shelf. By default, it would come as SUM(Salary Right-click on the pill and choose "" Dimension "" Pull the Balance measure on the Rows shelf. By default, it would come as SUM(Balance) Right-click on the pill , then click on Measure and choose Average . On the Marks Card , change the graph type from Automatic to Circle Pull the Response Dimension on the Color Marks Card . Pull the Salary Measure on the Label Marks Card . Right click on the Salary pill on the Marks card, then choose Dimension . Hope this helps.",313826.0
89104,369441.0,"Hello Vinay, Yes i had viewed that answer and if we follow steps of that answer it comes out to be using Gantt Plot and not scatter plot. I completely understood your answer. Just wanted to know if scatter plot can also be used for this questions since they have asked to use it. Thanks, Nirav",318440.0
89104,369436.0,Check out answer to a similar query: https://learn.upgrad.com/v/course/208/question/88787/answer/367881,313826.0
89104,369449.0,"Are you referring to the fact that once you have done the visulization, if you check under the ""Show Me"" tab, then it says ""Gantt Plot""? If that is the case, then please note that under the ""Show Me"" tab, Tableau suggests the possible best-fit plot that can be done based on the data that you have selected. Also, some of the other plots that can be done with the selected data are available for selection and the ones which are not applicable are greyed out. The method provided in the answer is an alternative way of getting the scatter plot. Hope this answers your query.",313826.0
87699,364689.0,"Hey Prashant, you have used AVG(Salary) while calculating the avg salary right? If you do so, it should give you a consistant value each time youcalculate it.",319721.0
87608,363609.0,"Alternatively, you can follow these steps: 1. Pull Job dimension in column shelf. 2. Pull Duration measure in row shelf which will show as sum(Duration). 3. Click the drop down for the sum(Duration) in row shelf and select Measure(Average) option. It will now appear as avg(Duration) . 4. Pull the R esponse dimension in the color markscard. You can observe in the chart that Blue collar Jobs have the maximum average duration.",315471.0
87608,363581.0,Pull the Duration measure to the Row shelf . Change the measure from Sum to Avg (Right Click on the pill then choose Measure followed by Average). Pull the Response dimension to the Row shelf . Pull the Job dimension to the Column shelf . Pull the Response dimension on the Color Marks Card . Pull the Duration measure on the Label Marks Card . Change the measure from Sum to Avg (Right Click on the pill then choose Measure followed by Average). The resulting Barc Chart would provide the average duration taken for yes/no response for each of the job category. You would be able to identify that Blue-collar job category is the answer in this case.,313826.0
87719,364224.0,the key to solving this is identifying that we need to selectively aggregate BALANCE. this can be achieved by considering Salary as a dimension. Rahul Kumar has provided a detailed answer regarding this in response to a q from Ruchik Mehta. please search for it,319898.0
87764,365204.0,"You need to first connect to the superstore data source to work on it. For that, you need to go to the Connect tab and click on excel. Then navigate to the directory where you downloaded this file and click on the file. Have you fowllowed these steps? If you haven't please do. Once you did this, you can select the sheets you want to work on.",319721.0
89344,370505.0,"Sets are exactly what we learnt in our high school maths. We even learnt in Python as a data structure. They can be used in Tableau. You can create sets on dimensions and measures and apply set calculations on them. Parameters are simple variables with values which can be used inside calculated fields, filters, etc. Hope this helps.",310511.0
89344,370908.0,Thanks a lot for answering my question,308442.0
89344,370541.0,Parameters as the name suggest used to parametrize some values while you do some calculations. These parameters are passed by the user who access the report on tableau web or tableau desktop. For example you want to show date range or show some charts in Yearly/Monthly/Weekly format. Than parameters would come in handy. These parameters would than be passed from filters and added into calculation.,317689.0
88239,365210.0,That's really cool. Thanks for sharing.,319721.0
88239,365707.0,thanks for sharing,311004.0
88314,365838.0,"Hi Darshan, It is in csv format. It should open in excel but if it still not opening then please change form .csv to .xls and try.",320195.0
88314,366171.0,"https://community.tableau.com/thread/195972 please refer to the thread in the link above. If your error is the form of the error mentioned, as suggested you can do this, On the Connect page, under Connect, click Text File. Select the file you want to connect to, and then click Open.Alternatively, to connect to the text file using the Microsoft Jet-based connection, in the Open dialog box, click the Open drop-down menu, and then select Open with Legacy Connection. The data source page appears.",319721.0
88314,366488.0,"Dear Nishan, The file which got downloaded was bargraphs.twb and I'm Having issues opeining it. any suggestion. Thanks and Regards Darshan",311032.0
88314,366519.0,"Is the error like, the requested workbook doesn't exist in Tableau?",319721.0
88296,365362.0,"You will find the dataset under the section ""Guidelines for In-Module Questions:"" on the link attached below: https://learn.upgrad.com/v/course/208/session/15814/segment/79967",313826.0
88296,365524.0,Thanks,319759.0
95514,405232.0,"In this case, the Prof was trying to give a general idea about the nature of how sales and profit vary for each category. As you can see in the video, the point representing the category Furniture has high sales and low profit whereas the point representing Technology has both high sales and high profit. Even though it is for the aggregate value, it is not necessarily wrong to generalise the way Prof has explained.",313517.0
88787,367828.0,Salary is continious variable since salary be 1000.01 rs for someone it can1000.20 rs ets so it cant be fixed and avg is something we can arrived so choose as per this convinience.,307843.0
88787,368779.0,Thank you very much Vinay for all your prompt response. :-) This is really very useful.,311729.0
88787,367881.0,"Pull the Salary measure on the Column shelf. By default, it would come as SUM(Salary Right-click on the pill and choose "" Dimension "" Pull the Balance measure on the Rows shelf. By default, it would come as SUM(Balance) Right-click on the pill , then click on Measure and choose Average . On the Marks Card , change the graph type from Automatic to Circle Pull the Response Dimension on the Color Marks Card . Pull the Salary Measure on the Label Marks Card . Right click on the Salary pill on the Marks card, then choose Dimension . Hope this helps.",313826.0
95801,405907.0,You are connecting to a csv file. Please use connect to text file option.,318084.0
95801,405908.0,"To open csv files you need to choose the file type as "" Text File "".",313826.0
95801,406126.0,If you are not sure which option to choose.. you can click on more and then click on you file.. tableau will automatically detect and uploads the file..,305129.0
95542,404131.0,"Yes, even i get this message. It is a version issue. However, i was able to access the files by ignoring the messages. Click OK and access these files.",316202.0
95916,406667.0,Please check the follwing post on a similar query https://learn.upgrad.com/v/course/208/question/88787,313826.0
94486,398258.0,"Thanks Shalini. You are right ,w e have to show avg and fraction. My question is regarding fraction calculation. For fraction, we need to know the total: Total is the sum of raised amount in of 5-15 Mil range for all venture types or only 3 venture types?",303666.0
94486,398182.0,"They want both average and fraction. It must be clear seeing the plot that the best funding type is venture between 5 and 15Mil, Also compute percentage of the total investment for the particular fund type (i.e. venture seed and private equity).",300748.0
95631,404989.0,may be by uninstalling and installing it again will solve the issue. Also delete the temp files and cache memory.,318429.0
96429,410780.0,"Can you share a screenshot of the error you are getting. Also, make sure you are importing the files using ""Text File"" option.",313826.0
96429,411188.0,"Either convert the files to Excel format or choose ""Connect to a text file"" option and then open the csv file.",313517.0
95977,407201.0,"1. Drag salary as column and change it into a dimension. 2. Drag Balance into row and select AVG. 3. Drag Response to Color in Marks card. 4. In the marks card, change to line/circle. Now note, for which salary yes circle is lower than no circle.",304319.0
95977,407374.0,Check out the below link on similar query https://learn.upgrad.com/v/course/ 208 /question/88787,313826.0
95261,,nan,
95678,405080.0,There's a problem in the version that is distributed with conda. Either ignore the warning or install the latest version. You can even follow the below link to modify a single line to get rid of the warning https://github.com/mwaskom/seaborn/issues/1392,318329.0
95754,,nan,
95759,405543.0,can you be more elaborative about the question ? i cant see any image if you have shared some .,318017.0
95759,405555.0,"Wonderful, after reading about it I understood that the Y axis in seaborn distplot is not signifying the ' Probablity ' but rather the density .. how dense your values are for the bin.. so, you might want to imagine it as the 'Area under Curve'.. Please see if the following stackoverflow helps you in understanding it.. https://stackoverflow.com/questions/51666784/what-is-y-axis-in-seaborn-distplot https://stackoverflow.com/questions/32274865/seaborn-distplot-y-axis-normalisation-wrong-ticklabels",316349.0
95759,413015.0,"I would also look at increasing the upper bound on the Y axis. You can do this: sns.distplot(df['column'][:200], rug=True).set(ylim=(0, UPPER BOUND )) eg: if your upper bound is something like this: sns.distplot(df['column'], rug=True).set(ylim=(0, 200 )) Try changing it to say 1000 (imagining that your data contains 1000 that is out of bounds due to the plot's default YLIM sns.distplot(df['column'], rug=True).set(ylim=(0, 1000 )) Reference: https://seaborn.pydata.org/introduction.html?",309211.0
96733,412873.0,"Hey Prakash, You can use Tabcmd for generating a PDF of Tableau view and emailing. For clarity, you can go through the Tableau community link mentioned below: Convert Dashboards to PDF and sent by email - https://community.tableau.com/thread/241723 https://www.youtube.com/watch?v=kryMpDU7V7g Hope this helps",302742.0
102759,439944.0,"Well Seaborn is built on top of matplotlib it provides additional features than matplotlib. There are few cases where i found matplotlib is bit diffcult to plot and i find seaborn being easy. Of course one has to learn matplotlib and then proceed with seaborn to understand the visualization better. Start with Matplotlib if someone is new to this field, probably when we get a hang of it we can gradually start incorporating seaborn plots for creating beautiful graphics as it allows to customize the plots.",301114.0
102759,439478.0,"I would agree with the point that mpl is better than sns in terms customization.Though we have to write few lines of code more but No Pain ,No Gain ,right? sns is more attractive (visually) but mpl has customization prowness with it. Im a beginner in DS field and when I want to plot graphs I would prefer mpl as it gives more customization because we can control more no of variables while plotting a graph. sns is what I would call built-in function type command.As it has less no.of variables to control,so we dont have to write numerous lines of code and hence a 100% preferred lib for beginners. For a beginner sns is good to get the feel of DataViz ,but as the learning curve progresses they will feel comfortable with mpl.(sns is still in developing phase I suppose) For quick Visualisation ,sns is good but mpl will always be better when it comes to real graph plotting. As a beginner I would like to first be comfortable using mpl and then moving on to sns",318386.0
102759,447469.0,After using since last 3 months I found seaborn very handy and useful in comparison to Matplot.,300735.0
97326,416552.0,"Hey Bhargav, In case you want to update your mini profile(Profile book) details, you'll have to fill the template all over again before deadline. As per my understanding your latest record will be used for career service. Please clarify this from your Student Mentor.",302742.0
95705,405326.0,please provide the link of your LinkedIn profile not your name like www.linkedln.-------,318017.0
95705,405450.0,pls add https:// to your actual link will solve this problem Ex my link is www.linkedin.com/in/neerajakshulu-vijjini-b27b8730 need to update https://www.linkedin.com/in/neerajakshulu-vijjini-b27b8730,315455.0
97385,416824.0,"As far as I remember it was asked to mention PG (Post Graduate) degree if any. Since this course is Post Graduate Diploma, it can not be considered as PG degree. So cannot use this PGDDS course as PG. Rest TA can confirm.",317991.0
94973,401150.0,"Yes, I am also seeing only career development essentials in Course 4.",308438.0
94973,402005.0,"Yes, everyone is now able to see only career development essentials in course 4 as this is buffer week and i think from this monday we will get other modules.",320685.0
94973,401442.0,"Hi Surendra, The subsequent modules of career development essentials will be released with a designed timeline, as you progress in the program. You can refer to this link https://learn.upgrad.com/v/course/208/session/17450/segment/89013 to have better clarity of the timeline. Hope this helps !",301619.0
94973,402738.0,The next moduls are unlocked today. Upgrad has sent out email and notification on the upgrad app as well.,310508.0
94993,401348.0,you can select anyone based on the way you want your profile to look and you just have to fill your basic and educational details in that book,318017.0
94993,401519.0,"Upgrad is going to help to build the profile, also I've seen that they have shown some templete in one of the video. I think it is a good idea to use the same templet, student mentor can help get the format.",306011.0
96197,409280.0,yes you can update it till 2nd December go ahead and edit and place a new version,318017.0
98926,421136.0,"Check with your mentor, right person to confirm.",300691.0
98926,420920.0,it will be there in the career section and if you again try to fill it will ask you if you are want to put a new version,318017.0
98926,421038.0,"The deadline for completing that is Dec 2. If you have completed it, you should have received a message from Upgrad that it is completed.",318084.0
95771,405628.0,please check the url again it should work,318017.0
95771,405761.0,"Hi Udipta, pls add https:// to your actual link and it shall work",301619.0
102467,438348.0,please try it using a desktop or laptop resume is not getting uploaded with mobile app,318017.0
102467,438585.0,"Same with me, did you resolve it?",310974.0
127346,567504.0,Can you help me with the page link. Answer below.,428646.0
88605,366769.0,Never Mind i got my anser in the solution sheet. :),306725.0
88605,384553.0,"select count(name) as 'number',countrycode from city group by countrycode order by number desc ;",300684.0
88605,368387.0,"Hi Pulkit , there is one more feature to restrict number of records using the "" LIMIT 1 "" you can restrict the output to only one record select b.name, count(1) from world.city a, world.country b where a.countryCode = b.code group by b.name order by 2 desc LIMIT 1;",318454.0
88722,367465.0,the link is already provided by upgrad. kindly refer to article. https://learn.upgrad.com/v/course/208/session/15788/segment/79827,305847.0
88722,367438.0,Please follow the steps mention on the link: https://dev.mysql.com/doc/world-setup/en/world-setup-installation.html,317845.0
88722,367540.0,"1. Go to https://dev.mysql.com/doc/index-other.html and then go to:- ""Example Databases"" section 2. Then click on zip given in row of world database to download it. 3. Once downloaded, unzip it an place its extracted folder in C drive 4. Then open the directory on your pc C:\Program Files\MySQL\MySQL Server 8.0\bin\ 4. Then open command prompt in adminstrator mode by typing cmd in the location address bar of this directory 5. Then paste or type below command in command prompt. mysql -u root -p 6. enter the password. 7. Then type the below command in command prompt (Replace the C:/temp/world.sql with the path of extracted folder of the dpwnloaded world database. SOURCE C:/temp/world.sql; It will create the schema and says rows affected in command prompt. 8. Then Open MySQL and refresh the schemas, now world databse will be shown for you.",317811.0
88722,367614.0,"I assume you already have MySQL server installed on your local machine, you can run single command on linux system to import mysql db. MyBook-AJ:~ arpitj$ mysql -h 127.0.0.1 -u username -p databasename &lt; /path/to/database.sql",306248.0
88722,367439.0,"Ideally, the world database should come as part of the standard MySQL Installation. In case it is not available, please follow the instructions provided on the below link https://learn.upgrad.com/v/course/208/session/15788/segment/79827",313826.0
88568,366583.0,select name from country where population = (select max(population) from country); A nested query needs to be written to get the maximum population first from country table.,304696.0
88568,366566.0,"Your query will always return the same value in 2nd column i.e (maximum population) but 1st column will be giving you all the country names in the country table. So yes code is not giving whats expected(i.e only the country with max population) because it gives me right population value but wrong Country name. You can try : select Name, population from country where population &gt; max(population);",318495.0
88568,366818.0,you can use this:- select * from country where population=(select max(population) from country );,312746.0
88568,366870.0,"Ideally, whenever you are using an aggregate function along with 1 or more columns in the select clause in a query, you have to use a group by clause on the 1 or more columns that you selected. It is used when you want to find max(of an attribute) from various records per group as grouped by an attribute Otherwise you can use a subquery like : select name from country where population = (select max(population) from country);",310511.0
88568,366959.0,Using subquery you can find the maximum population and with the maximum population corresponding country details can be retrieved. select * from country where population = (select max(population) from country) But you can also make use of order by + limit cluases . sort the country rows by population in descending order so that maximum population country row will be on top and then use limit function to get the top most row. select * from country order by population desc limit 1;,305652.0
88568,367417.0,"You can simple write a the select query and order it by population and limit 1 SELECT Name, population from country ORDER BY population desc LIMIT 1",306248.0
88568,367339.0,You must fetch maximum population first then apply your condition look return me that country name which is maxpopulation..in your query it is like .look give me name of country from that table and along with also calculate population . While writing translate it first as we derive while going for coding .set approach first.good luck,319869.0
90522,377639.0,"1. Select count (*) - gives the total number of rows. 2. select count(&lt;column name&gt;) - counts total number of rows in that column which are not null. If you want to count Number of rows with null values, then just subtract query 2 from query 1. i.e Select count (*) - select count(&lt;column name&gt;) . Does that answer your question?",318328.0
90522,378600.0,"https://blogs.technet.microsoft.com /benjamin/2013/12/27/sql-tip-counting-null-values/ here's a blog, which would help you. Please go through this, and try it.",319721.0
92779,390951.0,"After installing SQL as per the instructions provided, you have to access the local host connections by providing the root password that you would have set while installing. Are you getting the error while accessing the local host. Can you paste the screenshor of the error message.",316202.0
92779,391159.0,"I could install successfully soon after posting the question. Many thanks for the responses. I still dont know how it worked or why it failed all these past days, but atleast for now it is working. Keeping my fingers crossed till assignment submission!",301644.0
92779,391091.0,"You probably set your password differently or some mistake has happened while setting it. You can reset the password.You can use the link below to see the procedure, https://www.top-password.com/knowledge/change-sql-server-password.html",319721.0
93258,392495.0,Self join...you have to subtract 1 from pk and treat it as new pk. I can not post the query here,304814.0
93258,392494.0,use lag function or a self join,318017.0
92287,388617.0,Any specific issue? This should not happen. Try uninstalling all the components and re-installing the package.,311686.0
92287,388677.0,check if all the previous required things are present for me one of the module was missing,318017.0
92287,388695.0,Uninstall any older version of the product. Ensure you meet the pre-requisites. (Microsoft .NET Framework) (Visual C++ Redistributable for Visual Studio) Document link on how to install MySQL Workbench for Windows https://dev.mysql.com/doc/workbench/en/wb-installing-windows.html Link to download MySQL Installer https://dev.mysql.com/downloads/installer/,319721.0
92287,389252.0,While installing workbench and server products don't show up.Is it anything to do with my system being a 32 bit system?,300706.0
92287,389407.0,"Hey Gaurav, I was also facing the same issue because of 32bit OS. I upgraded my OS from 32bit to 64bit, installed .net framework , Visual c++ and then installed MySQL. Its working fine now. Please try upgrading. Hope it helps :)",302742.0
92136,387512.0,usually it comes by default when you install the workbench.. but if not then you can download it and run the same there are instructions and links to download the world database within the page;,316349.0
92136,387538.0,You dont need to install database separately. Just install the MSQL as mentioned in the instructions and then open MYSQL Wrkbench and there you can find worlddatabase as shown in image below.,317811.0
92136,388713.0,The solution file for the practice questions module is given. https://learn.upgrad.com/v/course/208/session/15788/segment/79827 The download link is there at the end of the page.,319721.0
92136,393351.0,world schema is already mentioned in mysql jiust download the world.sql file from SQL practice module and then open it on Mysql,307496.0
92782,390759.0,"Which dll file, if it is missing in google you may find the missing dll file and keep it in the required path.",318554.0
92782,391094.0,"If you're facing problems with the questions on world database, can you download the world database again and try? Delete the existing world schema before downloading it again.",319721.0
93455,393478.0,"Hello Naren, Please go through below link for details installation steps of MYSQL. https://learn.upgrad.com/v/course/208/session/15786/segment/79809 Hope It will help.",320195.0
93455,393436.0,in Prep course - Basics of SQL SESSION 1,307710.0
93141,392113.0,"Hey Dinesh, you need to import the tables from the files which are provided to you into a schema you create. This can be done by right-clicking on the tables option and selecting the import wizard. You can also refer to the answers to the question below. https://learn.upgrad.com/v/course/208/question/90110",319721.0
92816,390918.0,"Use datediff function.For this function both inputs should be of date or datetime format. Ex: SELECT DATEDIFF('2008-05-17 11:31:31','2008-04-28'); It will give you 19. For more info refer: https://stackoverflow.com/questions/2490173/how-to-get-the-number-of-days-of-difference-between-two-dates-on-mysql",317689.0
92816,390939.0,"Use the Datediff function to get the difference. E:G SELECT DATEDIFF(day, '2010-01-22 15:29:55.090', '2010-01-22 15:30:09.153') You can replace the day in the query with any units you want as a difference. e:g you can give mi for minutes. Refer to the below link for more details: https://stackoverflow.com/questions/2116540/difference-of-two-date-time-in-sql-server",316202.0
92816,391007.0,"You can use DATEDIFF() to find the diffrenace between two dates. SELECT DATEDIFF(""2017-06-25"", ""2017-06-15""); Please refer the below links for more details. https://www.w3schools.com/sql/func_mysql_datediff.asp https://www.w3resource.com/mysql/date-and-time-functions/mysql-datediff-function.php",317845.0
84651,346807.0,"It works even without the . operator for the columns which are unique across the joined tables. But, as a best practice, we should qualify the columns with . operator.",310974.0
84651,346924.0,When we write queries on joinng of large tables we do not know which columns names may collide. So its best pracitse to use table name along with column to avoid any run time surprises on production.,312746.0
84651,346895.0,"When the name of one or more columns that is part of your select query is same across the tables that you are joining, then it becomes imperative to specify the column with the dot notation, so as to tell MySQL which table the column belongs to. Otherwise, we get error ""Error Code: 1052. Column 'essn' in field list is ambiguous"" As mentioned by Ram, it is a good practice to always mention the selected columns by the dot notation to avoid such error and for clarity.",313826.0
92444,389381.0,"Double does not take null values, try changing datatype to varchar. That should help",319357.0
92444,389388.0,Change the datatype to varchar i.e. string which can take null values as well.,317689.0
87266,359641.0,"See the above it is joining of Employee and dept then with Project , it will give all the results employee deparment equlas with project department so all the records will be display but in reality if you see the Works on Table not all emplyooee works on all department. Example if you take Employee ,Department,Project of For John result will display That John works for project 1,2,3 which is Wrong. Now Consider Employee and Works on Table you will find John Works on Project 1 and 2 . Therefore answer Emplyee and Works on Table joining is correct it sepicify which employee works for which projec where as (Employee,department,Project list all employee working for the department and hence it will give those project for which even employee is not working in excatly.)",307843.0
87266,359637.0,"The requirement here is to list the employee's ssn, lname and name of the project that the employee is working on. The works_on table has the detail on the project that each employee is working on. Wheareas, Project table lists all the projects for a department. Not all employee in that department will work on every project in that department. So when employee, department and Project tables are joined, the output would show the employee details, the department of the employee and all the projects in that department. which will include the projects that the employee is not working on. Let's consider employee 123456789 in department 5. When joining employee , works_on we get the detail that this employee works for Pno 1 and 2 .Joining this with project table gives the project name as ProductX and ProductY. or the same employee, joining employee and department would return department 5 and pno 1,2,3. Joining this with project table will return ProductX,ProductY and ProductZ which is incorrect since employee 1234546789 doesn't work on ProductZ. So works_on table is the correct table to use since it has the employee - project relationship.",301654.0
91374,382016.0,"Easier option is to uninstall mysql and re-install it. Below is the option to reset the root password: Stop the MySQL server through services option in Windows. Create a new .txt file - reset.txt with the below contents USE MYSQL; UPDATE mysql.user SET Password = PASSWORD('newpassword') where User = 'root'; FLUSH privileges; Execute the below statement mysqld --defaults-file=""Path of my.ini file"" --initfile = ""path of reset.txt"" Kill the mysqld process from task manager and then restart mysql server from the services option in windows.",318084.0
91374,382029.0,The link below has detailed steps on how to reset the password . https://dev.mysql.com/doc/mysql-windows-excerpt/5.7/en/resetting-permissions-windows.html Hope this helps.,317149.0
91374,382048.0,https://learn.upgrad.com/v/course/208/question/91374 you can check this question as well,319721.0
92388,389105.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
92388,389219.0,Nulls can be ignored for first 50 days or simply handled by using a case statement.,317689.0
92388,389172.0,you can ignore the values being populated there as per the instructions if not getting null values. or use CASE as suggested by Ankit to handle it.,311686.0
88346,365697.0,"Please ignore the notification, I received a message saying on same. The remaining R &amp; SQL modules will open only after 29th September.",301644.0
88346,365618.0,"No, you will be given the assignmnet once the course gets launched. If you've completed the SQL course, then you're good to go.",319721.0
84650,346808.0,"You won't get meaningful data if you join employee &amp; project table directly even though they have a common column for the ask ""Display the ssn, lname, name of project of all the employees"". You must join the workson as well. Focus on ""meaningful"". Try joining employee and project tables and see if they give meaningful data or not.",310974.0
84650,346934.0,"There are following things which needs to be understnd- 1- whenever we need to join two tables, we should join it on primary key- foreign key relationship. 2- Even if you can join employee table and project table by joining dno and dnum, you might get repetetive result since they do not have foreign key relationship. To overcome this issue, Works_on table is used.",312746.0
84205,346376.0,"If you want to run multiple sql queries at once, then "";"" is must. It acts as separator if there are more than one sql statement.",312746.0
84205,346615.0,depends upon interest. generally data scientist will work with data managements where sql is necessary,314612.0
84205,346760.0,For single SQL query run ; is optional whereas if you want to run a SQL script with multiple queries ; is mandatory.,304696.0
84205,348565.0,"The practice of using a semicolon to terminate statements is standard and in fact is a requirement in several other database platforms. SQL Server requires the semicolon only in particular cases—but in cases where a semicolon is not required, using one doesn’t cause problems. When a semicolon is required and is not specified, the error message SQL Server produces is not always very clear.",304813.0
84205,346942.0,"Case1: in SQL ; (represents end of the statement) is optional but this has very important role to play. Lets take an example(This was obsered one of the senario when we worked) Statement1 Statement2 Statement3 when you execute this, this will execute parlelly (may impact your output or performance if any iner dependency in statements) Case2: Statement1 Statement2; Statement3 when you use ; it executes statement1 &amp; 2 as one block and than statement3. I my senario i wrote a querry where i need Statement1 &amp; Statement2 output to execute statement3. when i was executing case1 taken long time to execute where as case2 executed with-in min time. This is my observation and sujjestion received from one of expert while working",315455.0
84206,346374.0,Kindly paste your sql script. then we can analyse the issue.,312746.0
84206,346447.0,"The SQL Query for Example 29 is SELECT fname, salary FROM employee WHERE salary &gt;= (SELECT avg(salary) FROM employee); Same can be written using having clause - SELECT fname, salary FROM employee having salary &gt;= (SELECT avg(salary) FROM employee); Check this if you are looking for this. Otherwise, let us know the query which you are trying. So that, this can be corrected.",311502.0
84244,346485.0,Just now answered it in this thread https://learn.upgrad.com/v/course/208/question/84249. Please have a look.,310974.0
84249,346484.0,"Tip It'll be easy to understand if you run each sql independently and analyse the o/p Inner most sql given below gvies the list of projects with their corresponding count of employees: select pno, count(*) nemps from works_on group by pno ) tempproj The middle one given below selects the number which is the least count of employees in all projects: (select min(nemps) from ( The outermost sql given below gives again the list of projects with the number of employees but this time the projects which has least number of employees: select pno, count(*) numemps from works_on group by pno having numemps =",310974.0
88434,365891.0,"The keyword "" FROM "" is missing in the subquery. Try running: select e1.fname,e1.lname,e1.salary,e1.dno from employee e1 where e1.salary &gt;=(select avg(salary) from employee e2 where e2.dno= e1.dno);",313826.0
88434,366161.0,"If you are interested to find another solution to this instead of doing a correlated sub-query, check below: select e.fname, e.salary, temp.Dept_Average from employee e inner join (select dno, avg(salary) ""Dept_Average"" from employee group by dno) temp on e.dno = temp.dno where e.salary &gt;= temp.Dept_Average;",310974.0
88476,366132.0,"I believe you are trying to find out employees that have salaries less than the average salary in their departments. You will need to use joins to find out the required result. 1. First create a table which calculates average salary per department. 2. Then join this table with employee table with joining key as 'dno' . 3. And finally put your desired condition of salary being less than the average salary of the particular department. Try the below query: Select a.fname, a.dno, a.salary, b.avg_salary from employee a inner join (select dno, avg(salary) as avg_salary from employee group by dno ) b on a.dno=b.dno where a.salary&lt;=b.avg_salary Remove the where condition and you will get all rows. With where condition you will get the desired result.",315471.0
88476,366159.0,"Is this what you arr tyring to do? Q: Display the fname and salary of employees whose salary is more than the average salary of all the employees A: select fname, salary from employee where salary &lt;= (select avg(salary) from employee);",310974.0
88476,366513.0,"https://learn.upgrad.com/v/course/208/question/84819 If you're trying to do it by having clause, there's a similar question please refer to that.",319721.0
88476,366927.0,"Simply giving avg(salary) in having clause wont fetch you any value. If you want to fetch any value(aggregate or simple a column value), it has to be part of the select clause of a query. Try the below query instead: Select emp_name, dno, salary from employee emp, (select dno, avg(salary) avg_sal from employee group by dno) dept where emp.dno=dept.dno and emp.salary &lt; dept.avg_sal Above query will give you employees whos salary is less than the average salary of the department to which they belong.",310511.0
88645,367056.0,You can refer this for good explanation https://stackoverflow.com/questions/49768987/what-is-the-difference-of-using-having-vs-a-subquery,310974.0
88645,367327.0,Simple Thumb rule is to go for Having clause in case of Aggregated values instead of ordinary read additional topics on MySQL.org,319869.0
88645,367090.0,"""Having"" amd ""Sub Queries"" have different Use-Cases. The are used in different situations. For eg, if you had a table (storing students details) with 2 columns one for student's name and the other for his/her marks in an examination. Now, if use the aggregaotr ""avg()"" on the marks column, you will get the avg makrs of the entire class (all the students in the table). Now, you can use ""Having"" to limit your selection to only those students who have scored more than the avg marks. But, if you needed to limit your selection of rows (just like we did above) but NOT on a aggregated value, you will then be using sub query. (wont be needed in this example cuz the table is very simple). But, in general, when you aggregate values of a column and want to limit your selection, you would need ""having"" other wise to limit the selections you can use sub queries.",317998.0
88490,366136.0,Try restarting the MySQL Workbench. I had the same problem and after restart was able to see the result grid.,313826.0
88490,366516.0,"https://stackoverflow.com/questions/50159740/how-to-find-result-grid-in-mysql-workbench Here's a question similar to yours, hope it helps.",319721.0
84819,347560.0,SELECT * FROM employee WHERE salary &lt;= ( SELECT avg(salary) FROM employee);,311041.0
84819,347539.0,". select salary,name from employess where salary &lt;= (select avg(salary) from employess) /* It will display the avg salary of each employee*/",307843.0
84819,347540.0,"You don't need a having clause in this as there is no group by to aggregate any column here. select fname, salary from employee where salary &lt;= (select avg(salary) from employee);",310974.0
84819,347738.0,HAVING clause will be used along with GROUP BY which again used to generated output by grouping based on parameter / values. In this senario (as per your question) we need to use nested querries as mention below select * from employee where salary &lt;= (select avg(salary) from employee) in this SQL will generate overall Avg salary frist using (select avg(salary) from employee) then will generate list of employees whoes salary is &lt;= Avg Salary. I hope you got it.,315455.0
84819,350035.0,Please refer to the below code: SELECT * FROM EMPLOYEE WHERE SALARY &lt;= ( SELECT AVG(SALARY) FROM EMPLOYEE) ;,318455.0
84819,361529.0,"Having clause applies on a aggregated level on each group when you are quering your table with group by clause. Where clause applies on a each row. If you have used group by query with both where and having clause, all rows will be filtered first using where clause as it is aplied on each row and then having clause applied on each group. Remember where come first.",318332.0
85009,348269.0,"You have put a semi colon at the end of the group by clause and then proceeded to write the having clause, hence the error.",313826.0
85009,350042.0,remove the semicolon from the grouping it will work because semicolon is used to indicate the end of the query or we can use /,313833.0
85009,350032.0,"You have used ; for both last and secnd last line. Just remove it from the second last line, the code will get executed.",318455.0
85009,348386.0,"There is semicon after the group by clause on country so the query stops here and again there is semicon after having clause condition, when you run the query the statement having(count *) will give you error becaue the statement just ends after the first semicolon and hence the having clause gives error as it is not being interpreted as correct syantax.",307843.0
85009,349432.0,"multiple semicolons is the issue. i suggest having a mysql installation on your laptop where you can create the schema, tables and data first, post which, you can first run the query locally to test if everything works, post which it should be put into the LMS for submission. hope this helps.",310217.0
85009,349557.0,The semicolon after group by country is the error. Using semicolon teeminates the query. Therefore you now have 2 queries: 1st up to group by clause.. 2nd having clause query which is syntactically wrong. Hope this helps.,318495.0
85327,350098.0,You can use temp table join. make temp from your above inner select query. select * from employee a full outer join (select avg(salary) as avg_salary from emp ) b group by employee_id having a.salary &gt; b.avg_salary,304813.0
85327,350100.0,"Ans of your question is :- select fname, salary from employee group by fname ,salary having salary &gt;= (select avg(salary) from employee); But I am curious why do you want to use ""HAVING"" clause? We should avoid group by and having clause since these creates overhead on performance.",312746.0
85327,351144.0,"Having clause is used to restrict the result returned by group by clause, whenever you use group by clause you get subset and to further filter on the subset you can use having clause, in short having clause can be used to filter on the subsets returned by group by clause, In the requirement above having will return the result but you can do without it as well. When you want to use having clause, you do a nested query: select * from employee having salary &lt; (select avg(salary) from employee); The stated is one of the ways. However, there are different ways to achieve the same results.",319721.0
93257,392490.0,"A LEFT [OUTER] JOIN can be faster than an equivalent subquery because the server might be able to optimize it better—a fact that is not specific to MySQL Server alone. So subqueries can be slower than LEFT [OUTER] JOINS , but in my opinion their strength is slightly higher readability.",318017.0
93257,392491.0,"In most cases JOIN s are faster than sub-queries and it is very rare for a sub-query to be faster. In JOIN s RDBMS can create an execution plan that is better for your query and can predict what data should be loaded to be processed and save time, unlike the sub-query where it will run all the queries and load all their data to do the processing. You can go through below link to get more insight into it. https://stackoverflow.com/questions/2577174/join-vs-sub-query",317991.0
85328,350089.0,"select A.EMPLOYEE_ID, B.count(*) from EMP A LEFT OUTER JOIN PROJECTS B ON A.EMPLOYEE_ID = B.EMPPLOYEE_ID GROUP BY EMPLOYEE_ID AS the key was not given, I have assumed both tables have EMPLOYEE_ID as common key, in EMP table is Primary key in PROJECTS tableit is foreign key which references to Primary key of EMP table. I have used left outer join because it will also show number of projects for an employee as Zero id the employee does not have any entry on PROJECTS table.",304813.0
85328,350106.0,Output of the above question is employee name (or ssn) along with no of project he is working on.,312746.0
85328,351193.0,"Try doing this and you can see the output by yourself. select essn ,count(pno) from works on group by essn; select essn, count(*) from works on group by essn; Both will give the answer. Since we are grouping based on essn so count(pno) and count(*) has the same effect.",319721.0
85328,356307.0,"SELECT ESSN, COUNT(PNO) AS ""PROJ_COUNT"" FROM WORKS_ON GROUP BY ESSN;",318370.0
85350,350287.0,Details of the question &amp; video?,310974.0
85350,356293.0,"Q1 : Display the SSN of employees sorted by their Salary in Ascending order. We are retrieving SSN and Salary from Employee table by sorting the salary column. If we don't use the orderby clause we get the output which is not in proper order, thats why we are using orderby clause. Q2 : Sort the works_on table based on Pno and Hours All the random pNo will get arranged in ascending order if we use single column in order by clause. * If we don't specify the sorting order the system takes ascending as default. What happens if we do order by on two columns. In the previous scenario the pno is only thing it got organized but in this scenario the hours also got organized according to the pno. Hope it helps",318370.0
86441,356367.0,"Hi Chetan, Correlated subquery can be confusing at times , please find the below explanation for sub query vs correlated subquery. Subquery: Subquery is independent of outer query i.e. subquery alone can produce output. For example : below is the sub query to show the names of employees whose salary is more than the average salary of all the employees select fname,lname,salary,dno from employee e1 where e1.salary &gt; (select avg(e2.salary) from employee e2); Here if you execute subquery i.e. (select avg(e2.salary) from employee e2) it will produce output as 35125. so while executing query , it internally behaves like below: select fname,lname,salary,dno from employee e1 where e1.salary &gt; 35125 ; Correllated subquery: Correllated subquery dependent on outer query i.e. correllated subquery alone can not produce output. Example: Query to get the names of employees whose salary is greater than the respective departments average salary Here query should calculate the average salary for the department that employee belongs to i.e. correlated subquery should be executed for each row of outer query (for each employee). select fname,lname,salary,dno from employee e1 where e1.salary &gt; (select avg(e2.salary) from employee e2 where e2.dno= ? ) In the above query employee salary should be greater then the average salary of the department . So which department average salary we required ? , the employee for which we are executing the query at the moment so complete query would be: select fname,lname,salary,dno from employee e1 where e1.salary &gt; (select avg(e2.salary) from employee e2 where e2.dno= e1.dno ) internally average of department salary is calculated for current executing employee and average salary is then matched with the current executing employee salary . If the current executing employee salary is more , then the employee row will made it to the final output otherwise not. Hope this is helpul :)",305652.0
86441,356078.0,"Lets say you have two table named as employee and Department . Now Employee must work in some department let say there are 3 department_no (10,20,30) in department Table information. Lets say you want to select the department_no 10 column from the Department table, use below query. select department_no dno from department where department_no Dept = 10 Note : Here dno is alias name of department_no column and Dept is alias name of Department Table. Lets say you want the employee name,depatment_name and department_no of employeess work in department number 10. Also Assume Department Table column {Department_num,Department_name} Employee Table columne are {emp_no,deartment_no,emp_name} Then select e.emp_name,d.department_no,d.department_name from employee e,department d where e.deartment_no = d.department_no/ *here we are joining e as employee and d as department for department column*/ and d.deptartment_no = 10",307843.0
86441,366163.0,"If you are confused with the correlated sub query and want to get the same result, you can check below: select e.fname, e.salary, temp.Dept_Average from employee e inner join (select dno, avg(salary) ""Dept_Average"" from employee group by dno) temp on e.dno = temp.dno where e.salary &gt;= temp.Dept_Average;",310974.0
85345,350203.0,"Hi Darshna, By including distinct in the query, below would be the output: fname ------------------- John Joyce Ramesh Jennifer Ahmad Alicia",311160.0
85345,350316.0,SELECT fname FROM employee WHERE ssn IN (SELECT ssn FROM works_on WHERE hours>=20),307176.0
85345,350548.0,"Darshana note the question is asking with condition on hours. We apply conditins using the where clause i.e Where (It is keyword means only one meaning and apply to get filter) There are 2 methods to solve this try to run the query and let me know in case you face any isse please post. 1 SELECT fname FROM employee WHERE ssn IN (SELECT ssn FROM works_on WHERE hours&gt;=20) 2 select e.fname from employee e,works_on wo where e.ssn = wo.ssn and wo.hours &gt;=20",307843.0
85345,351194.0,"Try incorporating this in your code and you can see the output by yourself. select fname from (select fname, ssn from employee having ssn in (select essn from works_on where hours &gt;= 20) order by fname) as subq;",319721.0
85345,359061.0,select distinct fname from employee inner join works_on on employee.ssn = works_on.essn where hours &gt;= 20 order by fname;,317269.0
85345,352029.0,This is the expected output: fname ------------------ John Joyce Ramesh Jennifer Ahmad Alicia,318495.0
85345,366166.0,"The first answer to this question by given by Rakesh doesn't take into account the total hours worked by an employee on all projects hence may not return appropriate results. The below query calculates the total hours worked by employee on all projects and then filters the employees: select fname from employee where ssn in (select essn from (select essn, sum(hours) as hours_worked from works_on group by essn having hours_worked&gt;=20) temp);",310974.0
83295,342015.0,"As per the class we should use more joins rather than nestted joins or subquery. Below is example 1st one as nested and 2nd one done using join both will result out same i.e list of employess working on depatment number 5. Where employee delatails is in EMP table and department information is in dept table. select e.* from emp e where dept_no = (select dept_no from dept where dept_no = 5) Same we write in join queries. 2. select e* from emp e,dept d where e.dept_no = d.dept_no",307843.0
83295,366169.0,"-- 40. Co-related subquery -- Details of employees whose salary is greater than the respective department's average salary select e.fname, e.salary, temp.Dept_Average from employee e inner join (select dno, avg(salary) ""Dept_Average"" from employee group by dno) temp on e.dno = temp.dno; (vs) select e.fname, e.salary, temp.Dept_Average from employee e inner join (select dno, avg(salary) ""Dept_Average"" from employee group by dno) temp on e.dno = temp.dno where e.salary &gt;= temp.Dept_Average;",310974.0
83295,342163.0,"1- In most cases JOIN s are faster than sub-queries and it is very rare for a sub-query to be faster. In JOIN s RDBMS can create an execution plan that is better for your query and can predict what data should be loaded to be processed and save time, unlike the sub-query where it will run all the queries and load all their data to do the processing. The good thing in sub-queries is that they are more readable than JOIN s: that's why most new SQL people prefer them; it is the easy way; but when it comes to performance, JOINS are better in most cases even though they are not hard to read too. 2- Use EXPLAIN to see how your database executes the query on your data. e.g. In My opinion - Try to avoid sub-query if you can.",312746.0
81635,332756.0,"My Suggestion to check the referential integrety of each column it seems dno is from department table,dunm from employee table. My answer is hypothetical based since you habe not provided the table structure. also Dno is Primary Key in department table same time Dnane is normal column in departmentment table with no constraint and dnum is foreign key in emp table. Therefore the conclusion is dno being primary key there is no need to groupby on dno because dno being primary key it is expected that each row is unique so the query gives result even in case for null department name. Same time dname is normal column and it can have repeated department name or null values for example supose dno10 and 20 have same department name and one department name is yet to hence group by working on dno when selecting dname so it is not allowing for primary key in select statement.",307843.0
81635,339438.0,Your query is failing as : dname exists in the department table as there is only one entry each for dno and dname. So group by is occuring on department table.so count will always be one. wheraas if u group by dno. the group by is executing on the employee table where there are more than one employee for a particular dno.,304319.0
81635,333339.0,"Thanks Amit for the explanation. Also, I've got response in the above stack overflow question and that's even more detailed. ================================================================= My select dno ... group by dname started working after I did below: ALTER TABLE department ADD CONSTRAINT dname_unique unique(dname); ALTER TABLE department modify dname varchar(30) not null; ================================================================= The following rules apply for the columns (other than the group by column) to be used in select list: Case 1: Select involving single table 1) group by 1 col The group by column has to be unique &amp; not null or the primary key column of the corresponding table 2) group by col1, col2 ... Atleast one group by column should be unique not null or primary key column of the corresponding table Case 2: Select involving multiple tables 1) group by 1 col Only columns from the same table can be used in select list if the group by column is unique &amp; not null or the primary key column of that corresponding table 2) group by col1, col2 ... a) Only columns from the same table as one of the group by columns can be used in select list if that group by column is unique &amp; not null or the primary key of that corresponding table Ex: ssn even though primary key of employee table, it can be used to select any column in any table select mgr_start_date from department, employee ... group by ssn Success b) Any column of table can be used if one of the group by colums is the primary of any table. This doesn't apply to the group by primary key column is part of the join condition unless the other group by column is a unique not null column of the other table Ex: Dnumber even though primary key column of department table, it can't be used in conjunction with any non unique null column of employee table to be able to select any column from any table. Whereas, Dnumber can be used in conjunction with lname (if it is unique not null) to be able to select any column from any table select ssn from department, employee ... group by dnumber Error select ssn from department, employee ... group by dnumber, dno Error select ssn from department, employee ... group by dnumber, lname (if lname is unique &amp; not null) Success I dont' know if I made it more complex than before :)",310974.0
83312,342156.0,"1- your query is returing duplicate result. it should be :- select distinct fname,ssn from employee e inner join works_on w on e.ssn=w.essn where hours &gt;=20; 2- Nested query for above case would be - select fname,ssn from employee e where e.ssn in (select essn from works_on where hours &gt;=20); My Suggestion if you are going to write queries for PROD environment :- Try to avoid nested queries as much as possible :- Since they are difficult to understand and maintain in longer run. Support guys will find their hard time if you are using nested queries frequently. Only use when they are must.",312746.0
83312,349313.0,"For one employee, there are multiple rows in the works_on table. So I used sum() to get the total hours grouping by ssn and then filtering the rows with &gt;= 20 hrs using having clause. select sum(w.hours) as totalhrs,e.fname,e.ssn from works_on w , employee e where w.essn = e.ssn group by w.essn having totalhrs &gt;= 20",301654.0
83312,366168.0,"Alternate solution: select fname from employee where ssn in (select essn from (select essn, sum(hours) as hours_worked from works_on group by essn having hours_worked&gt;=20) temp);",310974.0
91448,382940.0,"Hey, Parul, can you try installing another version of sql and try. Even I'm not sure, what's going wrong with your code. Will get back to you if I can identify what the error is.",319721.0
91448,383732.0,"I got the problem, the thing is MySQL has included support for window function with its newest version that is MySQL 8, MySQL's earlier versions will always return a syntax error with over (). Please update your version of MySQL to 8 and the error will be gone.",300734.0
91448,382463.0,copy the whole query and run in a new tab sometimes the above query effect the below query while you run,318017.0
91448,382425.0,"I dont see anything wrong with the syntax. Please copy paste the same SQL statements in a new SQL script window and execute it. If that doesnt work, can you share the exact error message.",318084.0
91448,382494.0,There seems to be no error in the syntax. Are you still facing the error?,317149.0
91448,382525.0,"I also don't see any syntax issue but below is what i got from google for error code : 1064. Why do i get a query syntax error 1064 from MySQL when the syntax seems correct? On certain MySQL commands, I'm getting a 1064 Error. You will get a MySQL query syntax error number 1064 when you incorrectly use a reserved word in your query such as ""when"" or ""order"". MySQL 4 has additional reserved words that you cannot use and the full list of them is at",318328.0
91448,382526.0,"The syntax looks fine, should have executed successfully. Try manually writing the entire query and then see if this executes.",313826.0
91448,382540.0,"syntax looks absolutely correct. I have faced this issue sometimes but mostly when I did not have semicolon in previous command, see below: Shows error as my previous command did not have semicolon: After semicolon reinstated in previous command the error disappears:",309211.0
91448,382751.0,"I think there are some unwanted invisible characters, they might get copied when you copy syntax, I have faced such issues in past. Please try to remove spaces and new line from your syntax and start adding them back, if there are any invisible characters they will be gone, another way is to paste the entire query command on notepad++ and enable symbols Menu View → Show Symbol → *Show All Characters` or Menu View → Show Symbol → Show White Space and TAB or if you are using sublime please follow this link https://tosbourn.com/how-to-turn-on-invisible-characters-in-sublime-text-2/",300734.0
92362,388982.0,There will be missing data which can be a sinal data as well.SO you will get wrong outputs.Its better to load the file instead of using wizard,317982.0
92362,389111.0,"while importing set `cols` = IF(@cola = '', NULL, @cola); It will import null rows and all correct records will imported. DO this for both column where in dataset null exists",317811.0
92362,388993.0,"it affects you results.. i completed the assignment and then realised through a comment from discussion forum only that one row was excluded.. and when i included that row and ran the queries again it added one more BUY signal to the dataset.. so yes, it affects the assignment grades if my results are correct.. anyways you can assign the datatype as TEXT for both this deliberable quantity and other column (i forgot the name) and use import table wizard.. it should bring all the 889 rows.. the 31st Aug 2015 row has null values which has a valid close Price reading and hence it affects your results..",316349.0
92362,389220.0,Try importing the null column value rows by handling the datatype appropriately (strings can take null value) when you define create table script. Post that use load data infile as per the standard way you will be able to load all data.,317689.0
92523,390076.0,"Hi Ankit, its not a syntax error. I am not able to fgiure out ..its come with error-3644..",300735.0
92523,390068.0,alter table tablename add column columnname datatype;,317811.0
92523,389822.0,They syntax to add column is Alter table table name add column column name In your query the column key word is missing Alter table employee add column hno int;,300687.0
92523,389814.0,Please have a look at the below link. Hope this helps you. https://stackoverflow.com/questions/50440247/mysql-8-drop-or-alter-causes-error-3664-hy000-failed-to-set-sdi,310522.0
92523,389800.0,please share the snapshot? also the table def.,304813.0
92523,390309.0,"Hey, Nitin, you missed, to add the word column. As Maya and Ankit correctly pointed out, the correct syntax would be, alter table employee add column hno int;",319721.0
92523,390460.0,alter table employee add column hno int; still i am getting the same error. if incase i am creating a new table employee1 from employee than i am able to add a column. not able to understand why its not working with employee table ..,300735.0
92523,390479.0,"HI Nitin, the problem is due to there is a spatial index on the column in that table. alternate is, drop the table and recreate it by running the scripts again and post to that Alter Table.",318454.0
86976,358074.0,select * from employee where salary &gt; ( select avg(salary) from employee ) group by dno The subquery returns the average salary of all the employees in the employee table. The value returned by the subquery is then used in the where clause of the main query to filter out only those records where salary is greater than the average salary.,313826.0
86976,358247.0,select * From employee where salary ge ( select avg(salary) from employee) group by dno;,314612.0
86976,358284.0,select * from employee where salary &gt; ( select avg(salary) from employee ) group by dno,317418.0
86976,358908.0,if you mean to return all the employees having salary greater than avg salary in their respective department then you can use co related sub query select * from employees e where e.salary &gt; (select max(salary) from employees s where e.dno = s.dno);,320685.0
86976,359013.0,"In the question, if mentioned ""average salary"" is avg(salary) of all the employees of employee table then "" select * from employee where salary &gt; ( select avg(salary) from employee ); But, if the mentioned ""average salary"" is avg(salary) of each department of employee table then what is the query?",318328.0
86976,364680.0,"This query will return the count of employees in each department whose salary is greater than the department average. select dno, count(*) from employee as e where salary &gt;= ( select avg ( salary ) from employee as ie where e.dno = ie.dno ) group by e.dno ; The inner query will check all the departments whose number is equal to the department number of outer query and calculate the average. Note that this is happening before aggregation. Now the outer query is comparing the salary of each employee with the department average and aggregating the counts of such employees by grouping them on the basis of department number.",318576.0
86727,357825.0,"Hey, You have done your part in prepartory session.rest will start after 30 sep dont worry for that...!!",305847.0
86727,357958.0,"hi, i guess that two hours includes instructions, videos if available any also average pace to complete the questions..",305129.0
86727,358052.0,Hey Prashanth. Don't worry. You didn't miss anything. You did complete your part. Just wait for the program to get unlocked you'll get to know about further sessions.,319721.0
90980,379655.0,You simply need to apply the same procudure syntax and the logic as was taught in the course.,310974.0
90980,379817.0,"we're just expected to convert the UDF to Procedure.. results are same as UDF I used it as below; DELIMITER $$ create procedure hello (in s char(20)) begin select concat('Hello',s,'!'); end $$ DELIMITER;",316349.0
87693,364033.0,"Hi Kunal, Now i am able to see and also going through SQL session... No issues now. Thanks",320008.0
87693,364004.0,"Hi Manoj, Could you please let me know where exactly are you facing an issue. Also let me know on which platform are you accessing the video.",301618.0
87693,364003.0,"Hey, I got it ....Now i am able see SQA Session program and all others also. Thanks",320008.0
87721,364345.0,"Hi Arjun, I can run the same query successfully.",317418.0
87721,364290.0,"In MySQL Workbench, when you are writing comments using double dashes ( -- ) , then the double dash should be followed by atleast one whitespace for the server to recognise it as comment. Insert a space after the double dasj and the query should run successfully. Please check below link on ways to write comments https://dev.mysql.com/doc/refman/8.0/en/comments.html To view error details on the workbench, enable the output area panel by clicking on ""View--&gt;Panels--&gt;Show Output Area""",313826.0
87710,364222.0,Ok. Thanks for the Update.,320008.0
87710,364108.0,"Everyone gets the same thing, we need to wait for it.",310974.0
92289,388716.0,"Hi, Check if mysql service is running or not. Enter ""services.msc"" on the Start menu search box. Find MySQL service under Name column, for example, MySQL56.",319006.0
92289,388719.0,"Hey, please go through the last video of the session mentioned, https://learn.upgrad.com/v/course/208/session/15786/segment/79809 The professor shows a step by step procedure to connect the data to sql right",319721.0
92289,388770.0,in MySQL workbench click on navigator-&gt; instance-&gt; startup/shutdown. make sure the server is up and running. if not start the server and try again.,306735.0
92289,388912.0,Restart your mysql instance. If the server is not running try restarting it. Check here navigator-&gt; instance-&gt; startup/shutdown,317689.0
92289,389125.0,1. Check the companydb schema name and the submitted schema name as there is a typo with _pp in company db 2. check ur instance is working for other schema .. if not restart the workbench 3. if still same issue then stop &amp; restart service. but this is not necessary since the services wont get discturb for less conection and query trigger unless server leve issues,318454.0
92289,389332.0,"Sometimes it is disconnecting, after adding company try to refresh and check or use short cut (ctrl+U) to connect to dataBase",318732.0
92289,389373.0,"Even i faced the same issue, but for me it was due to new server connection was not created and after creating the new server connection i was automatically able to access the companydb schema",310611.0
92289,389343.0,restart the services.. that would help,311219.0
91002,380448.0,An easier way would be to use Brew. Brew is a quick installer that makes installing a lot of open source apps on your Mac very easy. Follow this link for the steps: https://gist.github.com/nrollr/3f57fc15ded7dddddcc4e82fe137b58e,318499.0
91002,379829.0,"1. Download the dmg file from here: https://dev.mysql.com/downloads/mysql/ You'll also need workbench, which you can find here: https://dev.mysql.com/downloads/workbench/ 2. You don't need to worry about your system requirements if your macbook has an intel chipset, at least for the kind of workloads that are involved in the course. As long as your macbook is less than 15 years old you should be ok. 3. Don't worry you just need to install MySQL, you'll be fine.",319357.0
91002,380689.0,Thanks everyone. Installed MySQL .Then installed MySQL workbench which is a visual tool for database.,308636.0
91002,380508.0,1. You can get installation steps from below link: https://dev.mysql.com/doc/mysql-osx-excerpt/5.7/en/osx-installation.html Also you can go through the link below if you find any difficulty during installation. https://sequelpro.com/docs/ref/mysql/install-on-osx 2. Please go through following link for supported platform by MySQL database https://www.mysql.com/support/supportedplatforms/database.html 3. MySQL is a Database (i.e software) that you need to install. Whereas SQL is a Query language. You can use SQL in any database. Basically it is developed to interact with database. Hope this helps.,317991.0
91580,385411.0,at the time of importwhen using load command. SET DATE COLUMN by Tell the import wizard using STR_TO_DATE what is the format of the date in the column in csv file. i.e. It is like 17-August-2018 means date format is '%e-%M-%Y',317811.0
91580,383378.0,You can change it anytime if you want to. Once you proceed further with the assignment you'll understand whether to change the format or not!!!,318084.0
91580,383532.0,Its possible to change this at any moment (i.e. during/after import). Here is a thought - you can change the value using string manipulation and conversion functions to convert this into the desired format. Hope that helps!,318085.0
91587,383403.0,Please go to View --&gt; Panels --&gt; View Output Area. This will show you the output panel. Thanks.,311502.0
91587,383406.0,Restart Workbench and execute again. Worked for me.,313826.0
91587,383415.0,"Hello Neelam, Restart SQL workbench and try.",320195.0
87647,363798.0,The query to find the employee with the highest salary would be: select * from employee where salary = (select max(salary) from employee); To find female employee with the highest salary: select * from employee where salary = (select max(salary) from employee where sex = 'F');,313826.0
87647,366366.0,"usually query execution sequence is where clause, aggregate function and lastly the record''s select executes. due to max function above query forced to select only one record ...so the execution engine chooses the first record of the select query result. hope this clarifies.",318454.0
88138,364924.0,"Implicit conversion from VARCHAR to NUMERIC happens for aggregrate function like avg(), sum() etc. which expect a numeric value. If the data contained in the VARCHAR field is such that it cannot be converted to a numeric, then it gets converted to zero. For ex.: If the VARCHAR Field contains the strung '1234', then it gets converted to number 1234. However, if the VARCHAR field contains the string 'ABC123' , then it gets converted to 0.",313826.0
91597,383929.0,The other way is to execute an execution plan -&gt; this will always show the results window,300694.0
91597,383966.0,"Workbench 8.0.12 seems to have a bug and is fixed on Sep 20 in 8.0.13 that is yet to be released. https://bugs.mysql.com/bug.php?id=91265 When does this happen? Whenever a statement like stored procedure or alter table ddl statement that returns no output is executed, the results grid is hidden automatically as there is nothing to display. This hidden grid does not pop out again automatically when a select query is run. So, there are a couple of ways to get away with this. Maintain different connections for stored procedures with not output is to be executed and other normal queries. Click on Query menu -&gt; Explain Current Plan -&gt; Results grid every time an output is to be seen. You may hate doing this again and again. Shortcut Ctrl + Alt + X also does not seem to work very well. You may want to try if interested and check if you are lucky. Save youe queries. Close all the query tabs that are open. Close and open your workbench. Just be cautious that may may call a stored procedure that does not return a value. you will encounter the bug again.",318007.0
88256,365178.0,"Hi Sampatt, It will be available on 30th september",319444.0
88256,365339.0,Thank you very much,318159.0
88256,365784.0,it will open in 3oth september 2018,318461.0
92798,390820.0,"Hi Snehal, The assignment is not looking for any specific analysis or any analysis from 'Stock market' point of view. It just wants to see our creativity/capability in terms of data interpretation. You can cover different aspects in your analysis using prices/signals of different stocks and thus finding good observations. You can also pick any specific stock and get some insights using its data only. You can also talk about some overall generic points as well as some specific stock together. All that is needed is some insights using the data available with us. Of course the constraint is 250 words. Hope this helps.",311686.0
92798,390807.0,Refer to following question. I have explained this in detail how 20 day MA and 50 day MA crossing works. https://learn.upgrad.com/v/course/208/question/92449,317689.0
92798,391097.0,"Hey Snehal, don't worry about the intricacies or more like the story you need to frame for the analysis as it can be very subjective. We need you to give us your insights from the analysis you carried out. Base any statements you make on the data of the stocks from the facts obtained from your analysis.",319721.0
90498,377518.0,"A mysql query normally consists of an SQL statement followed by a semicolon. Adding a semicolon after ""use company_db"" statement should solve the issue. Like this: use company_db; select * from bank_marketing;",318085.0
90568,378748.0,"Right, but you need to update the table. Please refer to the link below, the explaination is pretty good. https://stackoverflow.com/questions/27376152/how-to-add-a-column-to-a-table-from-another-table-in-mysql#comment60033927_27376207",319721.0
90568,377805.0,you've to use ALTER TABLE &lt;table name&gt; ADD &lt;column name&gt; &lt;data type&gt;;,316349.0
90568,377918.0,"You have to use the alter table functionality in MySQL. The syntax for it is: ALTER TABLE table_name ADD column_name datatype ; As an example: The following SQL commands adds an ""Address"" column to the ""Employee_Info"" table: ALTER TABLE Employee_Info ADD Address varchar( 255 );",305334.0
90568,378172.0,when am doing this it add Address column Null values. It doesnt give the address as given in employee,308495.0
92117,388640.0,"No, they don't have common attributes. You can check the image above.",319721.0
92117,388667.0,"Hey you're right. But what he means here is, we cannot join, the employee table and the project table using the attribute, dnumber.",319721.0
91196,380946.0,every database has there own syntax we need to use the ones wrt mysql,317982.0
91196,380950.0,"Concept wise SQL is almost same for every databases. The only thing is that each databases has some different syntax. But that can be managed easily by checking syntax on google. I worked on many databases like Oracle, MySQL, Teradata. And believe me it won't create any problem. You just need to do little bit of research on net for syntax.",317991.0
91196,381049.0,i too working on sqldeveloper from oracle.,318005.0
91196,381115.0,"ofcourse SQL is similar at the absolutel lowest level. Having been someone that has used MSSQL and then Oracle - I can tell you that there are enough differences that you should use the same SQL implementation that your client uses. In the case of our studies, from personal experience so far (I have finished the SQL modules and SQL graded assignment), I can only highly highly recommend that you use MySQL - that too the MySQL from Oracle (and not something like a fork like Mariadb) The syntax for MSSQL is slightly different to MySQL - so when you hand in your assignment and/or work with others in a group your MSSQL SQL might give errors when others try to use it on MySQL and it might become a big problem at that point. So a suggestion might be to do all your coding in your comfortable MSSQL but then run it in MySQL before handing it in to make sure it performs the same.",300694.0
91196,382210.0,So here is an important question. can i complete my assignment in MSSQL rather than MySQL? Will I be awarded marks for that? Or Do I need to check for MySQL code conversion before submitting my assignment?,307176.0
91196,381386.0,"SQL vs MySQL S QL stands for S tructured Q uery L anguage. It’s a standard language for accessing and manipulating databases. MySQL is a database management system, like SQL Server, Oracle, Informix, Postgres, etc. MySQL is a RDMS (Relational Database Management System). When considering a utility for data management the two most popular choices are MySQL and SQL Server. Both are efficient at keeping your data organized and readily available through a user interface. Both technologies have the concept of schema (that is table storage) for data storage. SQL is a language. Specifically, the “Structured Query Language” Now it would be better if we start differentiating the topic as difference between SQL server and MySQL and take them point by point. Now going to differences: 1) MYSQL available for free since it is open source, But SQL is not open source. 2) MYSQL offers only updateable views, SQL offers indexed views which are much more powerful, performing wise. 3) MYSQL doesn't support XML, Where as XML supports it. 4) Auto tuning is not supported in MYQL, But can in SQL. 5) User defined functions are not supported in MYSQL, But supported in SQL. 6) Transaction support is very much limited in MYSQL, But extensively and fully offered in SQL. 7) Stored procedures and full joins not offered in MYSQL, But offered in SQL. 8) Cursor feature is not there in MYSQL but is there in SQL 9) Job scheduling and profiling not available in MYSQL, But available is SQL.",306242.0
92015,386110.0,"Both are same, it is just matter of convenience Extract you can use to get other units also from a date",318554.0
92015,387569.0,Both are same and returns same result i.e. year,317811.0
92015,387380.0,"1. year function will return the year part of the specified date. For example - Table tblEmployee as JoiningDate as a date field having the value '2007-09-11'. Then output of query ""Select year(JoininDate) from tblEmployee"" will be - 2007. 2. extract function has a lot of flavors to it. extract(part FROM date) will allow us to extract the part of the date which could in any of below forms: day, month, year, quarter, week, hour, minute, second, and many more..",319006.0
92015,389928.0,Both are same,312357.0
91117,380637.0,"Hi Bhanu, You will have to do this in 2 steps: 1. Write an alter statement to add a new column to the table. alter table tablename add total varchar(20); 2. Run an update query on the entire table to set the summation of 2 columns and store it in total update tablename set total = column1+column2 Hope this helps.",310511.0
91117,380666.0,ALTER TABLE test ADD COLUMN sum int; UPDATE test SET sum = colA + colB,317149.0
91131,380717.0,Yes it is possible. So in your case the understanding would be product info being used in sales and store supply both..,305129.0
91131,380734.0,Yes Suja. You can do a dimensional modelling wherein the product information will be stored in a facts table and other attributes of product like sales and store supply can be stored in dimension tables around the product table. Only thing you have to concern yourself with is product table should have primary key/foreign key relation to each of its facts tables. Hope this helps.,310511.0
91131,380866.0,"I think it was mentioned in the lecture nicely - facts table are the numbers, data and dimension tables are the story around the facts without one the other would be non-sensical so yes definitely you can have fact table based on products and then dimensions based on sales and supply; it is as per business needs",300694.0
91131,382597.0,"A Dimension table contain descriptive (detailed) data and a fact table contain measurements (facts) which are used to evaluate a business, all numeric are not a facts but the numeric which are key performance indicators is known as facts.Fact table contain summarized data. The main logical difference is that we use connected lookup while loading dimensions and unconnected look up while loading facts.",313200.0
91835,384760.0,"eg: in an exam there are two students holds same marks and if you provide rank to them using rank(), students holding same position will have same rank say 2,2 but when it comes to next student mark it will NOT be 3. It will be 4, it just skips the 3rd grade and issues the next consecutive grade. the dense_rank() on the other hand gives you the immediate position. It still gives you grade 3 position.",306735.0
91835,384722.0,"the image on this page explains this quite well -&gt; https://sqlandplsql.com/2017/02/27/difference-between-rank-and-dense_rank-oracle/ Basically the difference between rank and dense_rank is that rank takes into consideration if multiple rows have the same ranking, and then skips to higher number to signify that multiple rows had the same ranking; whereas dense_rank does not skip Both have their use depending on business requirements so in above example since two rows have the value '10000' then the 9th row is given the rank 9 but the dense_rank 8",300694.0
91837,384728.0,it depends on how the stored procedure is written in Oracle vs MySQL. But in my opinion this question is irrelevant as we don't have any exercise in this program to migrate a stored procedure from Oracle to MySQL; and at the same time we are all asked to install and use MySQL A URL for comparison of differences in stored procedures between MySQL and Oracle: https://docs.oracle.com/cd/E12151_01/doc.150/e12155/triggers_proc_mysql.htm#g1049668,300694.0
91842,384773.0,Can you show the example on which you worked on without using delimiter ?,317991.0
91842,384792.0,"Reproducing from mysql official tutorial page: https://dev.mysql.com/doc/refman/8.0/en/stored-programs-defining.html ""By default, mysql itself recognizes the semicolon as a statement delimiter, so you must redefine the delimiter temporarily to cause mysql to pass the entire stored program definition to the server."" So, essentially when you are writing stored procedure which involve more than one statement , then you need to set the statement delimiter to something else like 'DELIMITER $$'. However, if the stored procedure is having only one statement , then there is no need to set delimiter to something else. Hope this clarifies.",313826.0
91842,384750.0,"Not really it depends on how the data is. if your data is clumsy and u need to get granularity, you may need to use fuctions to do it and there comes these operations. But as you said even without delimiter also Stored procedure works fine.",306735.0
91842,389931.0,"Are you sure is it working for all while Delimiters other than the default are ; typically used when defining functions, stored procedures, and triggers wherein you must define multiple statements.",312357.0
91870,385206.0,"hi, in this course I don't think it has been explicitly shown how to load CSV in MySQL. the assignment wants us to explore and learn how to do so. here is a very good page explaining the process : http://www.mysqltutorial.org/import-csv-file-mysql-table/ hope it helps.",311686.0
91871,385091.0,There are multiple formats in which you can load data from file. You can use table data import wizard. This is more UI based and not adivsable. Or you can use load data infile. This can be used after you have created your table structure. Refer to following for more details: https://stackoverflow.com/questions/14127529/mysql-import-data-from-csv-using-load-data-infile,317689.0
91871,385205.0,"Yeah, you do exercises using csv files in the advanced SQL module, starting from this https://learn.upgrad.com/v/course/208/session/19882/segment/101150",319721.0
91871,390827.0,There are 3 ways to load data in MySQL in easy way. 1. mysql workbench import wizard. 2. load command 3. mysqlimport utility,310482.0
91320,381647.0,"For this, you have to take the summation of all the houseno differences wrt to his house no. Also, please note that you need to convert any -ve difference to positive before calculating the summation. Thus 4-2 = 2 4-6 is also =2 since house difference basically is the absolute value of the difference.",310511.0
91320,381610.0,"if i remember correctly, you just take the numeric part of the address of each employee and you sum the values - one of the options is the correct resulting sum",300694.0
91320,381630.0,"Hi, in this question all the addresses of all employees are same except the house no. so in order to get the sum- you need to take the sum of the house nos of each employees you need to take out the numerical value out. Please let me know if it is clear now..",305129.0
91320,381745.0,Thnx you all!,318585.0
91295,381576.0,"While mentioning the column names, we need to enclose them in a pair of backticks(``) . The backtick ` character is on the bottom of the 'Esc' key on the keyboard. The modified command is as below: alter table employee add constraint fk_dno foreign key(`dno`) references department(`dnumber`);",313826.0
91311,381553.0,"The foreign key created, should always reference a primary from the same table or some other atble. Now here your foreign key is trying to reference the column essn from same table. But you have already dropped the primary key from the tbale. So Mysql wont allow you to create a foreign key. Hope this helps.",310511.0
91311,381570.0,"The foreign keys for this table have been defined on primary keys of other tables. fk_essn --&gt; references employee.ssn fk_pno --&gt; reference project.pnumber. The correct alter table commands in this case are alter table works_on add PRIMARY KEY (`essn`,`pno`); alter table works_on add CONSTRAINT fk_essn FOREIGN KEY(`essn`) REFERENCES `employee` (`ssn`); alter table works_on add CONSTRAINT fk_pno FOREIGN KEY(`pno`) REFERENCES `project` (`pnumber`); Hope this helps.",313826.0
93082,391889.0,use update query.. you'll neee to store your select query results as temporary table and then extract it out to update the exist table..,316349.0
93082,391918.0,First get that result set value in a local variable. then use update statment like update &lt;tableName&gt; set &lt;column_name&gt;=@variable_name.,312746.0
93082,392181.0,You can also self-join the table with the results obtained.,319721.0
92868,391110.0,"you can use group by with having. select dno,count(*), salary, avg(salary) from employees group by dno (check for correct name) having salary &gt; avg(salary);",304813.0
92868,391133.0,"Here is correct one, select e1.dno, e1.salary, count(dno) over (partition by e1.dno) as count_in_dept, avg_in_dept from (select dno, salary, avg(salary) over (partition by dno) as avg_in_dept from employee) e1 where e1.salary &gt; e1.avg_in_dept;",304813.0
92769,391089.0,Please share the error you got and the code you used. We can help you resolve it then.,319721.0
92769,390724.0,Can you please share what error you are getting ?,317991.0
92769,390686.0,"Hi , Please use the below command to add primary key to existing table: alter table &lt;table_name&gt; add primary key (&lt;column_name&gt;); It sould work fine unless there are any duplicate records in the table. Thanks,",305652.0
92769,391514.0,Your table must have duplicate keys.,305846.0
93422,393307.0,19 preceding n 49 preceding,307710.0
93422,393313.0,Use below condition in your over clause - 1-ROWS BETWEEN 19 PRECEDING AND CURRENT ROW 2- ROWS BETWEEN 49 PRECEDING AND CURRENT ROW,312746.0
93422,393339.0,try using over with order by Date and preceding 19 and 49 rows together.,301644.0
93422,393730.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date. window function should cover ROWS BETWEEN 19 PRECEDING AND CURRENT ROW ROWS BETWEEN 49 PRECEDING AND CURRENT ROW,317811.0
93422,393760.0,Thanks everyone. I have restart the workbench and the values were appearing as expected. Not sure what was the issue though. Appreciate your comments! :),318080.0
93488,393595.0,",",319721.0
92622,,nan,
92531,389820.0,This link is really a answer to your doubt. Hope this helps. https://stackoverflow.com/questions/23515347/how-can-i-fix-mysql-error-1064,310522.0
92531,389845.0,Just a thought I believe when you are using window function and giving column alias please try to enclose then in single quote. sum(salary) over () as 'total_salary' I was getting this error. See the reference: https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html,318554.0
92531,389837.0,"Sri Krishna Adusumalli , 1) Please update you server and work with the latest version . 2) I have executed your query and it's workign fine. select ssn , concat(fname, ' ', lname) as emp_name , dno , salary , sum(salary) over () as total_salary , sum(salary) over (partition by dno) as dep_salary from employee order by dno ;",306729.0
92531,390511.0,I agree with Pavankumar Harathi here. The same code you mentioned got executed for me without any errors. I am using the 8.0 version. Please update your version accordingly.,319721.0
92745,390609.0,"I believe that all the table and column names should be the same as given in the assignment, or the marks will be deducted as mentioned in the grading criteria.",303229.0
92745,390687.0,"Hi, Yes , it is advisable to create the column names as mentioned in the assignment. Please refer the below threads: https://learn.upgrad.com/v/course/208/question/91600 https://learn.upgrad.com/v/course/208/question/92567 Thanks.",305652.0
92745,391025.0,"Yes, I suggest you to retain the column name as per the question. Because, your query will be executed with the tables from their end, and changing the column name might be an issue.",316202.0
92749,390647.0,Refer this https://stackoverflow.com/questions/7946553/deterministic-function-in-mysql,310974.0
92749,390995.0,"For more details on the usage if Deterministic function, refer to the below link as well: https://www.sqlite.org/deterministic.html",316202.0
92749,391379.0,"If you don't add deterministic after the function, sql will by default take it as a deterministic function. Sql does not verify whether the function is a deterministic or not, it will execute considering it a deterministic function but might cause isssues if the function results in different outputs.",318397.0
92723,390558.0,it will give you the number of employees in a project number having only the project number where the count of employee is least the inner most query will give you the list of pno and number of employees in it for ex : pno nempl 2 20 4 30 5 32 6 24 and after that the outer query will fetch the minimum employee pno that will be 20 for pno 2 and the outermost query will givr you output as pno nempl 2 20 hope this help,318017.0
92723,390555.0,"Hi THis Query helps to find the PNOs which is/are with LEAST number of employees. 1 First see the SELECT QUERY and SUB-QUERY details there. 2. if you run the outer most SELECT query like below, you can get the result as explained below. select pno, count(*) numemps from works_on group by pno PNO NUMEMPS 1 2 2 3 3 2 10 3 20 3 30 3 3. Since our required result is to get """"minimum employee working PNO"""" we should find what is the least employee count in the company per PNO Least employee count can be found by MIN function. so using set of Queries using , GROUP BY PNO get the COUNTs and pass the result to another SELECT query where finding MINIMUM employee count of a PNO using below SUB-QUERY select min(nemps) from ( select pno, count(*) nemps from works_on group by pno ) tempproj 4. the SUB-QUERY detailed MINIMUM employee count which is 2 is passed to the outer SELECT query (step-2), through HAVING clause. so the OUTER query with GROUP BY is now with a condition HAVING = 2 EMPLOYEE this satisfies to two rows PNO NUMEMPS 1 2 3 2",318454.0
92616,390798.0,"Shital perfectly answered your questions regarding the same table which has mutiple alias. To answer your question on defining the alias before it is actually being used, we need to understand how the codes are executed in SQL. We have to remember that, we are defining the entire query line by line. It might seem to us that we used the alias which before even defining it because we wrote it late. But the SQL interpreter will take the entire query at once, and processes it according to the way syntax is defined to work and not line by line, like in a chronological manner. The way we write our code, with names defined later that they are being used, makes complete sense to SQL. As they are all executed together.",319721.0
92616,390229.0,"You can if you are using the table multiple times in different clauses and/or sub-queries like SELECT SeniorManager = sm.displayName, Manager = m.displayName, Worker = w.displayName FROM people AS w LEFT OUTER JOIN people AS m ON m.id = w.bossID LEFT OUTER JOIN people AS sm ON sm.id = m.bossID But you can not give the same use of a table more than one alias - the following for instance is invalid syntax: SELECT a1.name, a2.name FROM people AS a1 AND a2 Also, you can not (in most DBMSs) use a table by both its real name and alias, so the following is invalid too: SELECT people.name, a1.name FROM people AS a1 If ause of an object is given an alias you must refer to that instance by that alias even if the real name is unambiguous within the query. Please refer the below link : https://dba.stackexchange.com/questions/39323/multiple-aliases-in-an-sql-query",319006.0
93222,392411.0,You can not use the alias in where clause as it is computed before the select clause. You can read more about it at the link below. https://www.itprotoday.com/development-techniques-and-management/tip-apply-and-reuse-column-aliases,304281.0
93224,392379.0,"If inline queries contain joins and then we compare join vs sub-queries then In most cases JOIN s are faster than sub-queries and it is very rare for a sub-query to be faster. In JOIN s RDBMS can create an execution plan that is better for your query and can predict what data should be loaded to be processed and save time, unlike the sub-query where it will run all the queries and load all their data to do the processing. You can go through below link to get more insight into it. https://stackoverflow.com/questions/2577174/join-vs-sub-query",317991.0
91874,384997.0,Yes. Thats right. It is only for bajaj stock,318084.0
91874,385031.0,"Yes, you are correct. Only for Bajaj stock",301643.0
94707,,nan,
95375,403533.0,i guess they'd be uploading it in the sample solution that they provide after declaring the results for a particular assignment.,302738.0
95375,403970.0,"Agree to the point you have raised: It would have been great if the expectations for this questions were made clear i.e, observation of performance of individual stock based on buy/sell etc, same for all stocks, performance comparison of the same stock for two different years. We would have well prepared to look at the outcome data and give insights.",310508.0
95375,403749.0,"A good analogy that helps you write your summary well is by imagining you are reporting the results from the analysis to your clients or colleagues. Here, you are expected to understand the problem statement and devise a solution to address this problem statement. This solution should be backed by facts and you get these facts from the analysis you performed. It's not just facts which are important, your answer should be understood by the other person very well, for that, it should be well structured. So, the summary needs to have the inferences regarding the analysis you did and the results you obtained.",319721.0
93251,392474.0,hey you need to use windows functions here. I hope you did so.,319721.0
93251,392524.0,"Please raise an issue ticket and share the code there with me, so that I can help. Can't tell you here as it is a graded component.",304281.0
103619,444889.0,"Except saving little typing, natural join is never encouraged, the reason being, with 'inner join' you will have better control on your results. In real SQL world, one hardly uses 'Natural Join', atleast I never used.",316889.0
103619,444511.0,"Hi Ashish, Kindly look at the below link: https://stackoverflow.com/questions/8696383/difference-between-natural-join-and-inner-join",311160.0
103619,444598.0,"INNER JOIN joins two columns on the basis of the column name provided in the ON clause, whereas NATURAL JOIN joins two tables where column name and data types are same.",301655.0
103619,444853.0,"If you know when there are fwo same column names on different tables then you can use Natural Join. Imp thing to remember : The SQL NATURAL JOIN is a type of EQUI JOIN and is structured in such a way that, columns with the same name of associated tables will appear once only.",301646.0
103619,445029.0,A NATURAL JOIN is a JOIN operation that creates an implicit join clause for you based on the common columns in the two tables being joined. Common columns are columns that have the same name in both tables. One significant difference between INNER JOIN and NATURAL JOIN is the number of columns returned. Consider: TableA TableB +------------+--------+ +--------------------+ |Column1 | Column2 | |Column1 | Column3 | +-----------------------+. +--------------------+ | 1. | 2 |. | 1. | 3 | +------------+----------+ +---------+----------+ The INNER JOIN of TableA and TableB on Column1 will return SELECT * FROM TableA INNER JOIN TableB USING (Column1); SELECT * FROM TableA INNER JOIN TableB ON TableA.Column1 = TableB.Column1; +-------+--------+--------+-----------+ | a.Col1 | a.Col2 | b.Col1 |b.Column3| +--------+--------+--------+-----------+ | 1. | 2 | 1 | 3. | +--------+--------+---------+---------+ The NATURAL JOIN of TableA and TableB on Column1 will return: SELECT * FROM TableA NATURAL JOIN TableB +------------+----------+----------+ |Column1 | Column2 | Column3 | +------------+-----------+----------+ | 1. | 2 | 3. | +------------+----------+----------+ The repeated column is avoided. Don't use natural joins. Don't bother learning natural joins. They are an abomination. Why? The join conditions are based on columns with the same names . Natural joins do not even take declared foreign key relationships into account. This can be quite dangerous.,317412.0
103619,446350.0,Nutural join and inner joins are almost same but only one diffrence is that Natural join does not need a condition. Example: SELECT * FROM table1 NATURAL JOIN table2 USING (column),300735.0
91375,382020.0,"Go to View --&gt; Panels --&gt; View Output Area. If it is not helpful, can you share the error screenshot ?",311502.0
91375,382022.0,"1. I had the same issue . Restarting workbench solved it. 2. If it is still not coming , check output window for errors in the query . 3. If there are no errors, the result window could be behind, the output window. Hide the output window and try. Hope this helps.",317149.0
91375,382015.0,restart .. if it is still not coming then check your query,317982.0
91375,382059.0,"Here is a simple technique to get back the results grid in MySql WorkBench. Place the text cursor on a query and use the ""EXPLAIN command"" for the statement under the cursor. That will open the Visual Explain window. In that window you can find Results Grid. Click on that Results Grid. You can see the result grid again. Let me know if that works or not.",317991.0
78633,318394.0,"Hi Krishnam, I would suggest to use schema name before table name while querying. E.g. - SELECT * FROM sakila.category; The above query will show you result grid as well. Whereas, if you directly use table_name w/o schema_name it will just show output tab. Let me know if this works.",304694.0
78633,317815.0,Same thing happened to me. I just closed the editor and reopened it. Worked fine.,303670.0
78633,350743.0,"When we do all setup. and In MySql work bench, running script for table create it wont refresh. and doing some select command result grid is not displaying. These things will get fixed, once we close and reopen the MySQL Work bench. Seems there is a bug in this 8.0.12 MySQL work bench version. thankyou",312019.0
78633,332616.0,Apparently its a bug which is fixed in https://dev.mysql.com/downloads/workbench/ . It worked for me. Just replace the existing one. No need to unistall the previous one.,310506.0
87131,358856.0,"Hi Damini, Once you create compnaydb schema in MYSQL workbentch. Please download DDL script for schema creation from the bottom of this page and run it in your MySQL Workbench. How to run it ? 1) first select compnaydb schemas. 2)Go for file option which is available at the top. 3)select Run SQL script. 4)select the download files(company_db) from your system and open it. 5)select default schema name as companydb which you have created in the starting. 6) click on RUN.",320195.0
84701,347137.0,"These are three default schemas provided in MySQL. In the course as well, we will be looking into data from these data base, but there we also have provided insert statements for those who are not able to find these database. MySQL by default provide some DB for the user to practice and test. These are those such DB.",301555.0
80584,327241.0,Order of execution is as follows: 1. FROM 2. WHERE 3. GROUP BY 4. HAVING 5. ORDER BY 6. LIMIT,301652.0
80584,327269.0,"Note: Not every query needs to have all the parts we listed below, but this is the order by which SQL executes in most of the databases - 1. FROM and JOIN's: to determine total working set of the data 2. WHERE: to select the rows that pass the conditions 3. GROUP BY: to group the data based on the common values of the column specified in GROUP BY 4. HAVING: to discard the grouped rows that don'tsatisfy the constraints 5.SELECT 6. DISTINCT: Of the remaining rows, rows with duplicate values in the column marked as DISTINCT will be discarded. 7. ORDER BY: rows are sorted 8. LIMIT/OFFSET: rows that fall outside the range, specified by LIMIT or OFFSET, are discarded",304694.0
80584,328098.0,"In Sql or any other databse, the order of the exceution is the crux. If this rule is violated then you may end up facing errors in it. So as Kiran and Ashish said it is not mandatory to use all the keyword in a query. It is just that if we are using it then it should have in below order. Select ""column/s"" from ""table_name"" Where ""n no. of conditions"" Group by ""name of column/s"" Having ""aggregate condition"" Order By ""Asc or Desc"" Limit",300686.0
82297,341873.0,In the reference book provided by Prof.RC there are few steps to be followed to establish a connection with the server. please check it. hope it helps,310585.0
82297,337187.0,"During installation of Mysql you should have given the passowrd, same password you should enter. For that open MySql command line it will ask you the password there you enter the same password hope it will work. If you forget the password which you have givien during the installation you can reterive by using the you tube video (search how to reset password of mysql)",307843.0
82297,337202.0,"The below steps is what i recommend for Windows. Try to follow similarly for MAC. Let me know, if you are successful. 1. Update your windows from windows update. Do it 2-3 times to ensure everything even th optional updates are installed. 2. Uninstall any version of MySQL installed. 3. Restart 4. Download and install Visual C++ Redistributable file for 32 and 64 bit both. 5. Download MySQL Community edition from below link.Download the MSI installer one. https://dev.mysql.com/downloads/file/?id=476477 6. Execute the installer. Setup the password and also do not forget to add one user with admin cred just to ensure. This once installed will already have your world database.",301555.0
81437,,nan,
78954,319027.0,Click to Mysql Installer- Community(you can see in start menu).Reconfigure the Mysql Server.Reinstall it you will get the option of setting the password for the Mysql with two option in which Cached sha2 password is the recommended one .Dont choose that instead choose the other one.,301651.0
78954,319248.0,https://stackoverflow.com/questions/49194719/authentication-plugin-caching-sha2-password-cannot-be-loaded Check the above link if still facing issues.,301555.0
78954,376171.0,"The other one os older version, and not updated",308495.0
77554,310144.0,To find the least number of employees in a department you can use the count() function in sql,300688.0
77554,311947.0,"To find the least number of employees in a project, you can use the min() function along with count() in SQL, please find the query below for your reference: SELECT MIN(employeecount) FROM (SELECT Dno, COUNT(Dno) as employeecount FROM employee GROUP BY Dno);",305656.0
77554,316736.0,"List the number of employess in each account sorted from high to low. Here, Id is primary key. SELECT COUNT(Id), ACCOUNT_NAME FROM DB_NAME.COMPANY_TABLE GROUP BY ACCOUNT_NAME ORDER BY COUNT(Id) DESC",304397.0
77554,366172.0,"This is what I did: select pno, count(*) from works_on group by pno having count(*)=(select min(least) from (select count(*) as ""least"" from works_on group by pno) temp);",310974.0
77554,336958.0,"To find the project which has least number of employees select pno, count(*) numemps from works_on group by pno having numemps = (select min(nemps) from ( select pno, count(*) nemps from works_on group by pno ) tempproj ); tempproj is the temp view which is created with pno and the no, of emps in each project. Select min(nemps) finds out the min no of emps out of the above view. The outermost select hen prints the pno and the count of emps whose count is equal to the min count found in the above select.",304319.0
81607,332410.0,"Vikas, it's always easy for others to help you in resolution if you provide the error details. I think in the document UpGrad provided for MySQL installation on windows this issue is covered. Please re-check or provide more details of the issue.",310974.0
81607,332973.0,"As part of this course, what I think is that you need to install only MySQL Server and MySQL workbench. In later stages if you want to connect Python to MySQL via APIs then you need to install the MySQL Python connector. Apart from that I think you may not need any other developer tools for the same. Hence do not make the installation complex with Visual Studio C++ etc. If you are really coding with .NET and JSON, then only you need the Visual Studio and MySQL developer tools. Thanks and Regards Vijay K",310482.0
81607,332428.0,"Method1 ====== Install this update for Visual C++ 2013 https://support.microsoft.com/en-us/help/3179560/update-for-visual-c-2013-and-visual-c-redistributable-package Then retry the installation of MySQL Server . Method 2 ============== Also check that you have installed Visual Studio 2013 redistributable version 12.0.40660.0. If you are installing MySQL Server x64 you will need a 64 bits redistributable otherwise if you are installing MySQL Server x86 you require a 32 bits redistributable, below are the links where you can get the latest Visual Studio 2013 redistributable versions. Visual C++ 2013 Redistributable 64 bits: http://download.microsoft.com/download/0/5/6/056dcda9-d667-4e27-8001-8a0c6971d6b1/vcredist_x64.exe Visual C++ 2013 Redistributable 32 bits: http://download.microsoft.com/download/0/5/6/056dcda9-d667-4e27-8001-8a0c6971d6b1/vcredist_x86.exe Hope either or steps resolve your issues.",307843.0
81607,333018.0,"Hello All, I was able to resolve the issue of the Visual Studio. Installed Visual Studio 2017 .NET Development and also installed the python 3.6.6 64 bit. Also installed the Visual C++ 2013 Redistributable 64 bits and Visual C++ 2015 Redistributable 64 bits. Irrespective of all this MYSQL Server 8.0.11 is failing. All other installations as a part of package including MYSQL workbench are being installed fine and Pythin connector also works fine. Checked the install log file to look for error pertaining to MYSQL Server but don't see any error message. Also this installation I am doing on Windows 10. Still struglling with the issue. Any recommendations would be helpful. Thanks and Regards Vikas Lalchandani",310210.0
81607,335798.0,I was facing similar issues. But now solved. Update your windows from windows update. Ensure that there are no more updates (Very important). Uninstall all the software which has got installed by mysql installer (This you can know by date and time). Also uninstall Visual C++ redistributables. Restart the computer. You need not install Visual C++ redistributable separately. It will get installed automatically. Now run Mysql installer. Click on execute option after it shows warning that u need to manually update some software.,304319.0
81607,333101.0,Follow the below steps. 1. Update your windows from windows update. Do it 2-3 times to ensure everything even th optional updates are installed. 2. Uninstall any version of MySQL installed. 3. Restart 4. Download and install Visual C++ Redistributable file for 32 and 64 bit both. 5. Download MySQL Community edition from below link.Download the MSI installer one. https://dev.mysql.com/downloads/file/?id=476477 6. Execute the installer. Setup the password and also do not forget to add one user with admin cred just to ensure. This once installed will already have your world database.,301555.0
87202,359151.0,Restart the workbench and check again. It worked for me after restart.,313826.0
87202,359680.0,"Hey damini, this happened to a couple of students too. It is known that there is some bug with a particular sql version. Check out the link below. https://bugs.mysql.com/bug.php?id=91152",319721.0
87202,364802.0,"In case the results don't show up, ""Action Output"" screen tells you what's happening If your SQL syntax is wrong, no results are seen, for example result grid disappears when you type in wrong syntax such as shown below; please note that ""Action Output"" screen shows you what's the error and you can decide your remedy. Once you correct and then execute the SQL by clicking the following button once towards the end of the code, you will see the result come through:",309211.0
79199,321989.0,Execute the below statement sudo dpkg-reconfigure mysql-server-5.5 The below link could be helpful https://stackoverflow.com/questions/21944936/error-1045-28000-access-denied-for-user-rootlocalhost-using-password-y,301555.0
79199,329058.0,Check this Video and proceed. https://www.youtube.com/watch?v=dyc5b3yT2tI,301555.0
79199,388631.0,Hey My group mate,312357.0
81690,332837.0,"For me, even in Windows, the Result Grid suddenly used to disappear. Other people ( https://learn.upgrad.com/v/course/208/question/81165 ) have expressed this issue before, and yes, restarting seems to be the only solution.",301652.0
81690,333052.0,I have the same configuration as yours. I faced the issue once at the beginning.Restart solved it. Its working fine till now. It happened to be a bug it the earlier version of workbench..but read that they have fixed it in the latest version-8.0.12,309451.0
84133,346420.0,"The difference between the two files is - mysql-installer-web-community is a Online installer, which needs internet connectivity to connect to the online server to access all the installations files and install the mySQL software. mysql-installer-community is offline installer file with all features enabled in a single file. In production environement, most of the backend servers are not exposed to Internet. So, in such cases, mysql-installer-community file will be useful as it comes with all required features. Thank you.",311502.0
84133,346133.0,"Even I had the same issue. Please select the option encircled in red to get ""No Thanks just start my download"" hyperlink. Hope it helps.",310585.0
87213,359263.0,The addresses that has Houston in them doesn't end with Houston,310974.0
87213,359839.0,"When we use ‘%Houston’ it only looks for addresses ending with ‘Houston’, as the % wild card is used only at the beginning of the word. Not all addresses will always have Houston at the end. Using ‘%Houston%’, the query will return all instances of address which have ‘Houston’ in it, disregarding where it is placed in the actual address.",319302.0
87213,364793.0,"As a good practice, code can be re-written as follows: select ssn from employee where lower(address) LIKE lower('%houston%'); Benefits: Using a case match function such as lower(address) will get the letters to match in the event address in database gets changed, you don't need to re-write your query ; with the current coding, in the event the address changes to say HOUSTON or houston, your query will return no results",309211.0
84931,347948.0,"In the sql course, this is explained using a document. I installed on my MAC using the same:",310974.0
84931,347997.0,"Simplest Way to install mysql is to use Homebrew. Install Homebrew Installing Homebrew is effortless, open Terminal and enter : $ /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" Note: Homebrew will download and install Command Line Tools for Xcode 8.0 as part of the installation process. I nstall MySQL Enter the following command : $ brew install mysql it will install stable 8.0.12. That is ok to practice all the problems Open Terminal and execute the following command to set the root password: mysqladmin -u root password 'yourpassword' To install Wokrbench :- Use the pdf provided by Upgrad. That is more than sufficient.",312746.0
84931,351229.0,Please check this link..Information worthy https://razorsql.com/articles/mysql_mac_os_x.html,314617.0
84931,349946.0,"Install and start MySQL using homebrew using dollowing steps: Ensure you have homebrew installed on your Mac: /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" Install MySql: brew install mysql Start the MySql server: brew services start mysql Configure your MySql installation with a root password: mysql_secure_installation Use the following configuration settings: VALIDATE PASSWORD PLUGIN: No Enter your new password (this will be for user: root) Confirm your password Remove anonymous users: Yes Disallow root login remotely: Yes Remove test database and access to it: Yes Reload privilege tables now: Yes Test that MySql is installed and configured: mysql -u root -p Enter your password and you should see a mysql prompt Exit mysql : exit (optional) Install LaunchRocket to easily manage MySql server: brew cask install launchrocket Navigate to System Preferences on your Mac and click LaunchRocket Click Scan Homebrew Install Homebrew Workbench: Download Workbench from following location : https://dev.mysql.com/downloads/workbench/ Use the instructions from Upgrad's pdf and video to connect to MySQL DB you just fired up following the steps above",309211.0
88555,366509.0,You should've set it yourself during the installation.,310974.0
88555,366533.0,I'm not sure if you want to set it up for the first time or you want to reset the password! Ok if you want to set it for the first time then there are 2 options: 1. During installation it will ask for string cryptic password. 2. When you start a new connection at that time also you can set the password. Now in case you want to reset the password then below link might be useful: https://youtu.be/dyc5b3yT2tI,318495.0
88575,366981.0,"I will tell you the concept. Syntax will be different for different languages. run a for loop and iterate for the string characters and check if it contains ""a"" or ""e"". r ex: Like for ex: str[i] and iterate for every value of looping variable 'i'",308437.0
88575,366593.0,thank you.,318016.0
88575,366587.0,"You can do that using the like function but the order in which you use the alphabets will be important too while using the wildcard %. in your case you can use the following SELECT * FROM employee WHERE lname LIKE '%a%e%' OR lname LIKE '%e%a%'; in the above case if you only use '%a%e%' the result would be Wallace and not Zelaya as in Zelaya the alphabet 'e' comes before 'a', hence the need to use OR clase and cover both scenario.",318021.0
88575,366814.0,You could also explore the very powerful REGEX operators and functions. Checkout the below links for more details: https://dev.mysql.com/doc/refman/8.0/en/regexp.html https://www.tutorialspoint.com/mysql/mysql-regexps.htm,313826.0
88575,367296.0,"we can use where lname contains = ""a"" AND ""e"" also .",304338.0
82028,334902.0,When you download using the document of upgrad there are few steps take care of that use 32 bit 2nd click on execute button whenever you found many options execte will automatcally download all VS.,307843.0
82028,335780.0,"Finally got it right. After updating the windows, the installation was successful.",304319.0
82028,336181.0,Follow the below steps and it will be done. 1. Update your windows from windows update. Do it 2-3 times to ensure everything even th optional updates are installed. 2. Uninstall any version of MySQL installed. 3. Restart 4. Download and install Visual C++ Redistributable file for 32 and 64 bit both. 5. Download MySQL Community edition from below link.Download the MSI installer one. https://dev.mysql.com/downloads/file/?id=476477 6. Execute the installer. Setup the password and also do not forget to add one user with admin cred just to ensure. This once installed will already have your world database.,301555.0
80253,325198.0,For the purposes of this course we are using mySQL. Otherwise I am sure you can go to town exploring!!,300748.0
80253,325275.0,Basic SQL concept will be same throughout if you have hold on one SQL language you can manage others too.,300688.0
80253,325303.0,You can concentrate on SQL there is open source tool for the same i.e download the Sql developer and ther you can practice. Try other option after this.,307843.0
80253,353537.0,"Differences between 2 types of SQL are quite less. However, the following articles show differences between the languages. http://mwiki.gichd.org/IM/Difference_MySQL_PostGreSQL https://in.godaddy.com/help/key-differences-between-mysql-vs-postgresql-12392",318370.0
82229,336624.0,"Uninstall the my sql from your system then reinstall as per the guidance provided by upgrad, there are few missising steps in documents just keep in mind whereevr there is execute and next button first click on execute it will first upload and install the missing libraries and Visual studio once all execute is done then only click next. Hope this helps.",307843.0
82229,336629.0,Please confirm the OS you are using?,300734.0
82229,336641.0,,300719.0
82229,337200.0,Follow the below steps and you should be able to install it. 1. Update your windows from windows update. Do it 2-3 times to ensure everything even th optional updates are installed. 2. Uninstall any version of MySQL installed. 3. Restart 4. Download and install Visual C++ Redistributable file for 32 and 64 bit both. 5. Download MySQL Community edition from below link.Download the MSI installer one. https://dev.mysql.com/downloads/file/?id=476477 6. Execute the installer. Setup the password and also do not forget to add one user with admin cred just to ensure. This once installed will already have your world database.,301555.0
91898,385194.0,"I don't think sublime text is useful for our course. Can you give me the link of the video you're talking about. pelase? The SQL code files are given as reference e-books in the end of the page, https://learn.upgrad.com/v/course/208/session/15786/segment/79809 like in this one.",319721.0
91898,388681.0,"Hey, you can just copy paste the sql code given in the e-book from the reference ebook provided to you. It's better to directly have an SQL code for reference than a sublime text code as you can directly execute the commands by yourself while the preofessor is teaching instead of copy pasting it from the sublime text file right?",319721.0
90210,375775.0,You can follow below link based on your operating system. https://dev.mysql.com/doc/refman/8.0/en/installing.html,318368.0
90210,375764.0,https://dev.mysql.com/downloads/file/?id=479196 u can download mysql from here ensure to keep username passowrd somthing u remember Once done you can download MySql workbench which is UI for database .Herte u can write all your queries https://www.tutorialspoint.com/mysql/ for further help,317982.0
90210,375882.0,"Hi Sandhya, You can refer the installation instructions referred in course content itself :-&gt; https://cdn.upgrad.com/UpGrad/temp/89ddbb36-9d58-4db6-92b9-d0bebd4d6862/MySQL+Installation+Guide+(Windows).pdf That's best guide to setup MySQL on windows platform.",318429.0
90210,376643.0,Thanks everyone,317990.0
87420,,nan,
83416,342574.0,"Yes. This syntax is valid. We are supposed to delete foreign key related data from foreign table first if you are actually going to delete the data. Or if you really want to delete the data, first drop the foreign key constraint from employee table, then use above syntax. But I would suggest dont perform this delete task on local. since this table would be used in next section extensively. FYI- Actually, you rarely use hard delete option to delete the records on PROD environment.",312746.0
83416,342686.0,"True. But since there was no mention of deleting the foreign key related data from foreign table, I was confused. Thanks.",307488.0
83416,342865.0,"If you get foreign key errors and the tables are linked, then the correct way of deleting the data and maintaining the integrity is. 1. Delete the data from the child table as per the condition/criteria. 2. Then go and delete the related record from the parent table. This is the way to follow. We should not suppress or disable foreign key constraints or drop foreign keys as it violates the integrity and design of the data model/tables.",310482.0
93261,392496.0,the key should be present for all the rows make sure of that,318017.0
93261,392511.0,the key should be present for all the rows make sure of that but it should be better to join the table using all keys for optimisation,318017.0
93261,392515.0,"Yes, you can, there is no restriction in deciding which column will be the foreign key.",304281.0
91216,381113.0,https://dev.mysql.com/downloads/windows/installer/5.5.html can be downloaded from here,317982.0
91216,381111.0,no it's not needed .. u can download directly,317982.0
91216,381103.0,I don't believe so - any needed APIs/DLLs will be installed automatically can you provide more info - it is hard to help without knowing what error your getting?,300694.0
91216,381124.0,MS-Office is not pre-requisite for installing MySQL. They don't have and interdependency. Can you please share screenshot of any error coming ?,317991.0
91216,381767.0,,314678.0
91216,381765.0,,314678.0
91216,381811.0,"Try following the links below, https://dev.mysql.com/downloads/file/?id=479196 u can download mysql from here ensure to keep username passowrd somthing u remember Once done you can download MySql workbench which is UI for database .Here u can write all your queries https://www.tutorialspoint.com/mysql/ for further help",319721.0
86413,356221.0,"I also tried to explore options to autocomplete in MySQL Workbench 8, but could not find any options. However, I have DBeaver installed and it works like a charm. Link to download DBeaver https://dbeaver.io/download/",313826.0
85269,349681.0,on Mac - go to edit -&gt; autocomplete It might be also similar on windows.,312746.0
81165,329785.0,Try closing the and reopening workbench again. It worked for me.,309451.0
81165,349612.0,"Facing same issue uable to see the Query result, but after closing and reopening workbench again,it works.",312518.0
81165,356569.0,Execute the EXPLAIN command and then go to RESULT GRID,318344.0
81165,386135.0,"Facing same issue, Query result doesn't show up when executing a query after error, but after closing and reopening workbench again,it works. this is very frustation, any alternate solution other than closing and re-opening workbench?",316889.0
85312,349969.0,"Use below code :- select ssn from employee where address like ""%Houston%"";",312746.0
85312,350034.0,select Ssn from employee where address like '%Houston'; if we use this it will check in the following word exists in the starting select Ssn from employee where address like 'Houston%'; if we use this it will check in the following word exists in the ending select Ssn from employee where address like '%Houston%'; if we use this it will check in the following word exists in the front as well as end and in the centre of any word,313833.0
85312,350082.0,It can also be like select Ssn from employee where address in ('Houston');,300690.0
85312,350145.0,"Dear shristhi, if you want that particular Houston you have to write it in double quotes (""%Houston""). when you specify double quotes SQL will consider you want that particular value.",308639.0
85312,350137.0,"select SSN from employee where address like ' Houston' if we want to compare strictly with Houston. if we use % before and after Houston ('%Houston%), it will include any address where Houston is there. eg AHouston , Houston City all will be returned with %",317982.0
85312,350311.0,"SELECT ssn FROM employee WHERE address LIKE ‘%Houston%’ % is a wild card character and when used with LIKE, its position matters. For example, in the above solution: address is a field which contains Houston in a text series of words, so theres no way to tell where exactly Houston would be. So, using ‘%Houston%’ means to search for anything before Houston and anything after Houston. Simply means search for anything which contains Houston. Hope it clears",307176.0
85312,352386.0,Select * from employee where address like '%Houston%';,314617.0
91250,381374.0,Thank you swapnil. That is the same answer provided by mySQL documentation mentioned below: https://dev.mysql.com/doc/mysql-windows-excerpt/5.7/en/resetting-permissions-windows.html It worked for me. Thank you once again.,305843.0
91250,381343.0,Try the steps in the link below : https://support.rackspace.com/how-to/mysql-resetting-a-lost-mysql-root-password/,317460.0
91250,381365.0,"1. Log on to your system as Administrator. 2. Stop the MySQL server if it is running. For a server that is running as a Windows service, go to the Services manager: From the Start menu, select Control Panel, then Administrative Tools, then Services. Find the MySQL service in the list and stop it. If your server is not running as a service, you may need to use the Task Manager to force it to stop. 3. Create a text file containing the password-assignment statement on a single line. Replace the password with the password that you want to use. MySQL 5.7.6 and later: ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass'; 4. Save the file. This example assumes that you name the file C:\mysql-init.txt . 5. Open a console window to get to the command prompt: From the Start menu, select Run, then enter cmd as the command to be run. 6. Start the MySQL server with the special --init-file option (notice that the backslash in the option value is doubled): C:\&gt; cd ""C:\Program Files\MySQL\MySQL Server 5.7\bin"" C:\&gt; mysqld --init-file=C:\\mysql-init.txt If you installed MySQL to a different location, adjust the cd command accordingly. The server executes the contents of the file named by the --init-file option at startup, changing the 'root'@'localhost' account password. To have server output to appear in the console window rather than in a log file, add the --console option to the mysqld command. If you installed MySQL using the MySQL Installation Wizard, you may need to specify a --defaults-file option. For example: C:\&gt; mysqld --defaults-file=""C:\\ProgramData\\MySQL\\MySQL Server 5.7\\my.ini"" --init-file=C:\\mysql-init.txt The appropriate --defaults-file setting can be found using the Services Manager: From the Start menu, selectControl Panel, then Administrative Tools, then Services. Find the MySQL service in the list, right-click it, and choose the Properties option. The Path to executable field contains the --defaults-file setting. 7. After the server has started successfully, delete C:\mysql-init.txt . You should now be able to connect to the MySQL server as root using the new password. Stop the MySQL server and restart it normally. If you run the server as a service, start it from the Windows Services window. If you start the server manually, use whatever command you normally use. If the ALTER USER statement fails to reset the password, try repeating the procedure using the following statements to modify the user table directly: UPDATE mysql.user SET authentication_string = PASSWORD('MyNewPass'), password_expired = 'N' WHERE User = 'root' AND Host = 'localhost'; FLUSH PRIVILEGES;",310522.0
80974,329175.0,"Which binaries of MySQL you downloaded. Did you download it correctly for the platform which you intend to install (For e.g Windows, Linux). That is why you are getting this error.",310482.0
80974,335804.0,I was facing similar issues. But now solved. Update your windows from windows update. Ensure that there are no more updates (Very important). Uninstall all the software which has got installed by mysql installer (This you can know by date and time). Also uninstall Visual C++ redistributables. Restart the computer. You need not install Visual C++ redistributable separately. It will get installed automatically. Now run Mysql installer. Click on execute option after it shows warning that u need to manually update some software.,304319.0
80974,333097.0,Follow the below steps. i have practically installed on Windows 7 - 64 bit and verified the steps works fine. 1. Update your windows from windows update. Do it 2-3 times to ensure everything even th optional updates are installed. 2. Uninstall any version of MySQL installed. 3. Restart 4. Download and install Visual C++ Redistributable file for 32 and 64 bit both. 5. Download MySQL Community edition from below link.Download the MSI installer one. https://dev.mysql.com/downloads/file/?id=476477 6. Execute the installer. Setup the password and also do not forget to add one user with admin cred just to ensure. This once installed will already have your world database.,301555.0
88798,367916.0,Looks like Visual Studio is not installed. Please check the pre requisites section in the installation document,318804.0
88798,367959.0,"if you will the installation document, then it will install without any issues.",318319.0
88798,367970.0,It might be because of installer issue. You have to download MSI Installer that contains all the required file. You can download the installer from below link https://dev.mysql.com/downloads/installer/ Below is the link for installation steps: https://dev.mysql.com/doc/refman/8.0/en/mysql-installer-setup.html,317845.0
88798,368022.0,I faced the same issue. On my 32bit machine MySQL workbench did not install. But on 64bit machine it installed perfectly and running smoothly. Check whether your machine is 32 bit or 64 bit. ? System requirement for MySQL workbench is 64bit architecture. For system requirement refer below link : https://www.mysql.com/support/supportedplatforms/workbench.html,317991.0
88798,373490.0,Thanks for the info. My system is 32-bit and I tried as per installation guide and followed all the steps. I will try again. Should work even with 32-bit.,303227.0
83632,343673.0,"if it is not the first attempt of yours to install mysql there are chances that the rest products are already installed on ur pc. If it's not the first time, check your control panel if the products are already installed",310585.0
83632,345940.0,1.Uninstall already available Visual Studio C++ package from Control Panel. 2. Install Visual Studio C++ again 3. Install SQL workbench or follow the SQL installation process provided .,311119.0
83632,369447.0,"Hi, i am also facing the same issue. ITs not installing MYSQL Shell. I tried uninstalling and installing it many times. my OS is 32 bit version windows 7. I cannot upgrade to 64 bit, if I am not wrong because its 32 bit processor. any help on this would be great. Regards, Prem.",322683.0
83632,344226.0,Is this issue resolved? Even I had some problem with sql installation I had to install.net framework Visual c++ was already installed 2-3 times I tried installing sql and SQL workbench Finally succeeded I can help u if u still have d issue,308437.0
83129,341310.0,"In query- you are selecting ""deparment"" table. It must be ""department"" table. You are making typo in your query. Please correct it.",312746.0
83129,341401.0,As you mention you have companydb created by you so first Check what are the tables being created under this database and as the error says Table not exists so check is the table you are using in your select statement exists in the companydb or not.,307843.0
86478,356311.0,"Limit is used to limit the output of number of rows. Below example suppose Employee table has 20 records, and you want to limit hte output only to 5 recodes,then you can use Limit. Select * from Employee; LIMIT 5; Note: LIMIT is keyword so limit pursore is defined above. Limit is also same as &lt;= rownum explained below i.e select * from employees where rownum &lt;=5;",307843.0
86478,356407.0,"LIMIT restricts the number of rows in a result set. Suppose you have a table called ""employee"" with 8 rows. select * from employee; -- would give all the 8 rows of the employee table select * from employee LIMIT 5 ; -- would give only 5 rows from the employee table.",313826.0
86478,357042.0,"The LIMIT in MySQL will limit the output to the number of rows mentioned. SELECT * FROM myTable LIMIT 1; This will only output the first row. SELECT * FROM myTable LIMIT 10; This will display first10 rows only. You can use OFFSET to skip some rows. SELECT * FROM myTable LIMIT 1 OFFSET 10; This will skip the first 10 rows and show you the 11th row SELECT * FROM myTable LIMIT 10, 1; Does same as previous example You can achieve similar output using TOP or ROWNUM in other SQL implementations. MS SQL , Oracle respectively. https://www.w3schools.com/sql/sql_top.asp https://www.guru99.com/limit.html",319721.0
83282,341921.0,1- use semicolon after use company_db ie( use company_db; ) 2- please look at the screen shot for your ref:-,312746.0
83282,342572.0,"1- open notepad 2- type create commnad e.g. create table works_on( essn char(9), pno smallint, hours float(4,2), constraint pk_works_on PRIMARY KEY (essn, pno) 3- save it with.sql extension 4- run this file using workbench.",312746.0
83282,341937.0,Still it is showing following error.,300690.0
83282,342123.0,Please Provide a table name as per syntax supported by MYSQL . like Bank_Marketing,301648.0
83282,342350.0,No special characters except underscore (_) is allowed in the table or column names. remove the hyphen '-' from the table name which is causing the problem.,304319.0
83282,342404.0,Table exists as bank-marketing in the schema.while typing bank_marketing gives error showing such table does not exist.,300690.0
83282,342442.0,How a table would be created in SQL text file?,300690.0
81083,329258.0,"Essn is a foreign key that refers to Ssn in the employee table. Pno is a foreign key that refers to the Pnumber of the Project Table. Separately Essn and Pno act as foreign key for employee and project table respectively. However PNO and Essn DO NOT form a composite foreign key that refer to employee table, since there is no Pnumber/pno in the employee table.",300748.0
81083,330045.0,There is very simple rule when a column is foreign key. 1. A coulmn which is primary key(i.e unique values and non repeated value) in Table only act as Foreign Key in another table. Eg. SSN number is social Security number which is always unique for all people or employee in Emplyoee table therefore SSN number must be a forengh whereever this colum is used with repeted values. Now suppose in Department table if a employe works in two department then in this case Deptnumber is Primary key but SSN number is foreign key because SSN is primary in Employee table but if same employee work works in 2 dept then SSN number becomes foreign key because SSN is reparted for those employees who work 2 or more dept.,307843.0
81083,332285.0,"This was atricky one from the video, it asked if ESSN and Pno is foreign key to employee table.Many people would have answered option 1. PNo is not a primary key in Employee table so the question was just from Employee table so answerwas ESSN not Pno thoughh it is foreign key but for Project table. Hope this helps.",304813.0
85472,350808.0,Thanks Alok. Third option has worked. But i am not able to see the tables after running script by clicking on TABLES under my schema. still i am getting data when i am executing the DML statements. can u help me on this,300733.0
85472,350800.0,Basic steps which you can try out :- 1- try to close all open application. 2- reopen workbench and import .sql file 3- use open sql file option from file menu to import the sql file in workbench. 4- plz dont save .sql file in c drive.,312746.0
82838,,nan,
82841,339701.0,"Simplest Way to install mysql is to use Homebrew. Install Homebrew Installing Homebrew is effortless, open Terminal and enter : $ /usr/bin/ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"" Note: Homebrew will download and install Command Line Tools for Xcode 8.0 as part of the installation process. I nstall MySQL Enter the following command : $ brew install mysql it will install stable 8.0.12. That is ok to practice all the problems Open Terminal and execute the following command to set the root password: mysqladmin -u root password 'yourpassword' To install Wokrbench :- Use the pdf provided by Upgrad. That is more than sufficient.",312746.0
82841,355975.0,In addition to Alok's solution: After going through the MAC pdf for installation. You can start/stop/restart the sql server in terminal using the following code. &lt;Use any of the last 3 words to start/stop/restart the server&gt; You can change the SQL password in workbench as well. In the left pane of workbench you can see the users and privelages option where you can change the password for SQL server. sudo /usr/local/mysql/support-files/mysql.server start/stop/restart,318370.0
85490,351151.0,If you are facing this issue with all the code then try installing the workbech again.,305845.0
85492,350905.0,it looks like your MySql session has the safe-updates option set. This means that you can't update or delete records without specifying a key (ex. primary key ) in the where clause. Try: SET SQL_SAFE_UPDATES = 0; Or you can modify your query to follow the rule (use primary key in where clause ).,312746.0
85492,351017.0,Delete from employe # It is not good proctice or simple terms you are trying to delete all recods from emp. Note: when there is safe-update-mode you can't delete or update without using a primary key in your clause or conditions . However you can delete all records using below comands. delete from employees where ssn_num in (select ssn_num from employees) # Try this By default mysql comes with safe-updates mode profile setting preference when installed I have verified in some forum discussion and it also safe updatemode in my machine too. You can edit safe mode options by below command. SET SQL_SAFE_UPDATES = 0 ; or,307843.0
85492,351161.0,"First, to resolve the error 1175, SET SQL_SAFE_UPDATES=0; right before your query. But the fname you want to delete might not get deleted as there might be a foreign key constraint showing you the error 1451. So, check for foreign key constraints before you delete anything.",319721.0
85624,351315.0,The Employee Table already exists under the schema you are running the Table creation script. Please delete the Employee Table available under Schema(Or) Database and re-run the script.,311502.0
85624,352026.0,"Hi Arifa, The error itself is self explanatory. The Schema in which you are trying to create the table employee,there exists alreay a table employee. So you can do 2 things: 1)Use a new Schema. 2)Run a SQL query to drop table i.e ""drop table employee;"" and run the script again. Hope this helps :)",318495.0
87371,360821.0,"Hi Ravi, I faced this problem multiple times but not for each question. I think we could provide this feedback to Upgrad which can help them to look into the issue and fix it. Thanks for raising the point in forum.",318328.0
87371,361279.0,Please ensure that you are updated with the latest version of Chrome and have a good internet connection. Also please clear browser cache before viewing.,301618.0
87371,361542.0,"I was facing same issue, I found that there was a problem with my network connection, it was getting disconnected for few seconds and then it was getting reconnected. I switched to stable network, now it works fine. Hope this helps.",318332.0
87371,361572.0,internet connection and speed is not the cause in my case.,312199.0
87371,376863.0,try viewing in incognito mode. or a different browser if internet connection is not the case. I was facing the same issue and tried it in Mozilla instead of chrome and it worked.,308962.0
85715,351930.0,"The error message basically says that the table ""employee"" already exists in the scheme your trying to create this table. Try to close the my sql workbench and reopen or execute the deletion script to remove the table from the current scheme and then the same script.",312259.0
85715,352024.0,"Well the error itself is self explanatory. The Schema in which you are trying to create the table employee,there exists alreay a table employee. So you can do 2 things: 1)Use a new Schema. 2)Run a SQL query to drop table i.e ""drop table employee;"" and run the script again. Hope this helps :)",318495.0
85715,352527.0,"Change the the existing table ""employee"" so that the new import works, if wishes to use same schema . Else use a new schema",303082.0
88897,368457.0,"Hello Adiraju, It is availble in below link: Download the sample superstore dataset from below link: https://learn.upgrad.com/v/course/208/session/15814/segment/79967",320195.0
88897,368461.0,"Hello Jaikrishna, I think so RC sir is using super store data only through out the session. Please correct me if i am wrong. https://learn.upgrad.com/v/course/208/session/15814/segment/79967",320195.0
88897,368458.0,Which video are you referring to ?,313691.0
84052,345871.0,"1- sql is not case senstive. 2- By default, String comparision is also case insensitive. a- select fname,ssn from employee where fname='JOHN'; b- select fname,ssn from employee where fname='John'; Both are same. both query will result same output. 3- If you want to make string comparision to case senstive, use BINARY key word. e.g. select fname,ssn from employee where BINARY fname='JOHN';",312746.0
84052,346006.0,"What the lecturer meant was the sql keywords are case sensitive like select or SELECT, from or FROM etc",310974.0
84052,346267.0,"Data definition language (DDL), Data manipulation language (DML ), Data Control Language (DCL), Transaction Control Statement (TCS), Session Control Statements (SCS) are not case sensitive. Names of Variables/Objects which users create / reads (Strings) are case sensitive.",315455.0
84052,352113.0,"HI Madhusudhan, Below is the link which has a vary good description about this :- https://stackoverflow.com/questions/153944/is-sql-syntax-case-sensitive. (A small chunk from there) The SQL Keywords are case-insensitive ( SELECT , FROM , WHERE , etc), but are often written in all caps. However in some setups table and column names are case-sensitive. MySQL has a configuration option to enable/disable it.",314617.0
84052,348533.0,"names are case sensitives like table name,column namesets but commands are case insensitive update/UPDTAE both work, so be clear if its name then case sensitive while all other are case insensitive. helps??",304813.0
86946,358104.0,"Hi Mantha, Please follow the documentation link provided in the course. For your reference here's the link https://cdn.upgrad.com/UpGrad/temp/89ddbb36-9d58-4db6-92b9-d0bebd4d6862/MySQL+Installation+Guide+(Windows).pdf I followed the steps mentioned in the documentation and its working fine. Please let me know if you need further assistance. Thank you!",318355.0
86946,358272.0,"Hey Mantha, can you please try uninstalling all the components related to MySQL and installing them again?",319721.0
84055,,nan,
93785,395312.0,"Hello Suja, Can you please restart your Laptop and confirm.",320195.0
93785,395341.0,"Did you check if the services related to MySQL are running? For windows, please follow the below instructions if the services are not running: Click on the Cortana search box and type ""Services"". Click on the ""Services"" desktop app. You will be presented with a view displaying the services. Scroll to find services related to ""MySQL"". Check if the ""Status"" is ""Running"". If it is not ""Running"", right-click and ""Start"". Also, change the ""Startup Type"" to ""Automatic"". This can be done by right-clicking, then select properties and choosing ""Automatic"" from the drop-down for ""Startup type"".",313826.0
93785,395494.0,"I guess you'll have to uninstall and reinstall again, unfortunately. I did a search and that's what I found too.",319721.0
93785,395504.0,Update : A reinstall worked but had to give a new port. Apparently 3306 is in use in my system.,317149.0
85655,351882.0,"Hi Aditya, Try to install MySQL notifier using this link . Turn on the MySQL server using the notifier. (Read instructions here ). Try to reconnect. So let me know if the problem persists.",304281.0
86523,357039.0,"Seems that you did not select/assign any key to the table you want to create. Try doing that, you won't get any error. If you don't get the above options in your screen, try fiding for arrows like below. Use those and your options will be dropped down.",319721.0
86523,357760.0,"You are trying to create a table without columns. That is not possible in Relational DBMS. You need to specify at least one column in order to create table. So, Please specify atleast one column. Happy learning!",311502.0
86523,358268.0,Try specify a column t create a table.,317418.0
86110,,nan,
86121,354129.0,"if OS is windows7: ""You can install 4.7.2"" elif OS is Windows8.1 : ""You can install 4.7.2"" elif OS is Windows8 : ""You can install 4.6"" Source : https://docs.microsoft.com/en-us/dotnet/framework/get-started/system-requirements",311502.0
86118,354130.0,"Windows 8 does not supports .Net Framework 4.7.2 It supports the following - .Net Framework 4.5.1, 4.5.2, 4.6 &amp; 4.6.1 Source : https://docs.microsoft.com/en-us/dotnet/framework/get-started/system-requirements",311502.0
86114,353229.0,This is a bug and it fixed on 6.3.10 version. Try that version. https://stackoverflow.com/questions/45967413/results-grid-not-showing-on-mysql-workbench-6-3-9-for-macos-sierra,319721.0
86114,353231.0,Restarting the MySQL Workbench fixed the problem for me whne I was not getting the Results grid after successful execution of a query.,313826.0
86114,356507.0,I faced the same issue and upon restarting the SQL Workbench I was able to view the Results,304389.0
86251,,nan,
86933,357887.0,"Is Python is available on the server where you are installing the mySQL? If not, Please first install the Python and later install the mySQL. During the mySQL installation, it installs all VC++ supporting files as well. Thank you.",311502.0
86933,357954.0,you need to install sql installer - from there you can add the required addins.,305129.0
86261,354491.0,Vist the below link. SQL for linux Also check out the related links at the bottom on this link to define database/schema. Hope this will be helpful :),318495.0
86261,355299.0,Install based on your Linux distribution. Refer to the link below for the different version of Linux. https://dev.mysql.com/doc/refman/5.6/en/linux-installation.html,317845.0
86358,355657.0,"Yes. I believe once the 3rd session on practice questions is also completed, the status would change to 100% complete.",313826.0
91468,382623.0,try to uninstall and reinstall it.,318084.0
91468,382679.0,"If it is password issue, then please reset the password for the root account. You can refer here on the process to be followed to reset the password - https://dev.mysql.com/doc/mysql-windows-excerpt/5.7/en/resetting-permissions-windows.html https://www.youtube.com/watch?v=dyc5b3yT2tI Thanks",311502.0
91468,382724.0,"There are many reason that can lead to this error ""Cannot connect to Database Server"". Below link has many reason and also solution for them, please go trhough it: https://stackoverflow.com/questions/7864276/cannot-connect-to-database-server-mysql-workbench Hope this will help.",317991.0
91468,382758.0,"please confirm if your Database server Daemon is running, a Daemon is equivalent to service with Windows.",300734.0
88882,368690.0,"Hi, Please share the log file also ,we might get some hint from that.",319056.0
88882,369159.0,"Hi mahesh, If you're using the SQL 8.0 version, this might happen as there is some bug in it and hence doesn't work on few systems. So, try using the earlier verison, proably sql 5.7.",319721.0
91397,382531.0,"Thanks, great links. Microsoft typically have Trasact-SQL/T-SQL style of writing SQL code. MySQL is more ANSI-SQL/ ODBC-SQL oriented SQL coding standard. We have other coding styles that are more type-strict such as PostgreSQL",309211.0
87408,360727.0,"After you have answered , you need to scroll down and you will be presented with a ""Continune"" button clicking on which will proceed with rest of the video.",313826.0
87408,361545.0,Try reloading the page if you are not able to see continue button. Make sure your network is stable and with constant speed.,318332.0
92625,390302.0,"Hey, you need to start the server first before making a connection.",310974.0
92625,390810.0,"Please make sure that your server is started. If the server is already started, you will see the button in “MySQL” preference panel as ""Stop MySQL Server"". If this works fine, you can add a MySQL connection using the password. Can you please tell us what is the step you're stuck on?",319721.0
91927,385408.0,looks like result grid is not opening.it happens sometimes. the best way is to close the app completely and start it again. you may refer to the below link https://stackoverflow.com/questions/28038580/how-to-hide-view-result-grid-in-mysql-workbench,318554.0
91927,385376.0,"Are the other results being displayed? If no results are displayed on the grid, then try restarting the workbench.",319721.0
93741,395052.0,JSON is discussed in Module 3 - Python for datasciene (getting and cleaning data from APIs) There is very good explanation for JSON in below link. Have a look into it. https://www.w3schools.com/Js/js_json_intro.asp,303673.0
93741,395054.0,more_horiz JSON is discussed in Module 3 - Python for datasciene (getting data from APIs) There is very good explanation for JSON in below link. Have a look into it. https://www.w3schools.com/Js/js_json_intro.asp,303673.0
92734,390532.0,"MYSQL is different appliction from SQL Server. MySQL is an open source RDBMS, whereas SQL Server is a Microsoft product. So if you system has space few GBs you can install MYSQL 8 or latest.",318454.0
92734,391035.0,You don't have to unistall all the dependencies. You can insytall the MYSQL 8.0 workbench and server.,319721.0
92734,392084.0,"after loading mysql 8 , I hv clicked my downloaded file . Now I hv to solve question No.1 count city etc .. so for that what are the steps to solve ... is there any You tube site etc .. pl. help",319969.0
87655,363963.0,Thanks All for your answers.. I was clear with the Logic but English language of both these options was confusing me! thanks again for the support!!,316349.0
87655,363864.0,"All the records which satisfy both the conditions on either side of OR clause are returned. When you look at a record and if one of the conditions is true for that record, the record is selected.",310974.0
87655,363873.0,"Hi Hemant, the answer you selected satisfies only one clause of the query i.e salary&lt;=25000. However in addition to these, all the female employees will also be selected according to secong clause in OR.",305845.0
87655,363924.0,"In the case of OR operator , atleast one condition from the conditions specified must be satisfied by any record to be in the resultset. Few records will satisfy sex='F' and few records will satisfy salary&lt;=25000 So the final result will be combine of both. All female employee and employee with salary&lt;=25000 You can find more info at https://www.geeksforgeeks.org/sql-and-and-or-operators/",317845.0
87655,369922.0,"Hi, this query returns all the rows which are female and all who has salary less than 25K because we have used a 'OR' condition. if it was an 'AND' it would show all females having salary less than 25K",311032.0
88219,364909.0,"Check if the services related to MySQL are running by folowing the steps below: Click on the Cortana search box and type ""Services"". Click on the ""Services"" desktop app. You will be presented with a view displaying the services. Scroll to find services related to ""MySQL"". Check if the ""Status"" is ""Running"". If it is not ""Running"", right-click and ""Start"". Also, change the ""Startup Type"" to ""Automatic"". This can be done by right-clicking, then select properties and choosing ""Automatic"" from the drop-down for ""Startup type"". After this, try connecting. Hope this helps.",313826.0
88219,367276.0,"@Nakul, the problem is that your server is unable to connect using port 3306. This connection establishment process is performed by Router module that comes inside MySQL installer. You can check its configuration by again executing Downloaded installer and then selecting Repair/Reconfigure option. More you should ensure that from Upgrad portal, you have followed correct installation procedure strictly as per your operating system(""mac"" for Apple Macbook and ""windows"" for Microsoft Windows) .",318770.0
88219,365076.0,"I hope that you installed two pieces of software: (1) MySQL Server and (2) MySQL Workbench. Only then can you use the Workbench and connect to server. That's the first step. Now it seems that your workbench connection requires a mannual setup. It usually doesn't happen. If you tried uninstalling and reinstalling the sever and the workbench already, then you can carry it out manually refer the video below. https://www.youtube.com/watch?v=DCgRF4KOYIY",319721.0
90073,375072.0,simply double click on companydb schema in left section.. this ll make it active and then run your code.. hope this helps..,316349.0
90073,375086.0,Try restarting MySQL Workbench. I had also faced teh same issue and restarting resolved the issue.,313826.0
90073,375094.0,"Line above text ""Action Output"" will have a separator. You need to drag and move out put pane upside. Please find screen shot attached.",318368.0
92086,386873.0,"Yes, It will be fine even if you install MySQL 5.7. The last major version before release of v8 was MYSQL v5 Please refer https://mysqlserverteam.com/inplace-upgrade-from-mysql-5-7-to-mysql-8-0/ for more details.",317811.0
92086,388841.0,Thanks Everyone for the answers...I will try to have the latest version.,312623.0
92086,387302.0,It's always advisable to install latest version as older versions may not support window functions.,319006.0
92086,387049.0,Yes. you can install.. please follow the same process mentioned in sql installation guide in upgrad session.. it will help.,318802.0
90281,376281.0,"Hi Shristi, Follow the below steps and confirm it is working or not. https://learn.upgrad.com/v/course/208/question/82028 1. Update your windows from windows update. Do it 2-3 times to ensure everything even th optional updates are installed. 2. Uninstall any version of MySQL installed. 3. Restart 4. Download and install Visual C++ Redistributable file for 32 and 64 bit both. 5. Download MySQL Community edition from below link.Download the MSI installer one. https://dev.mysql.com/downloads/file/?id=476477 6. Execute the installer. Setup the password and also do not forget to add one user with admin cred just to ensure. This once installed will already have your world database.",320195.0
89700,373701.0,Dot Net framework 4.7.2 is already installed as per the install guide.,318762.0
89700,373598.0,"Make sure you have enough space on you C:\ drive, as the installtion is fully done in C:\ and there is no way to change it. Also make sure you have Dot Net framework 4.5 install, otherwise MYQSQL workbench would fail to install.",319721.0
89700,373705.0,Here is a screenshot of the VC++ redistributables installed,318762.0
89700,373709.0,Here is the screenshot of the error,318762.0
89700,374499.0,"Hi Uddhav, Based on the screen shots and explanation: You have the necessary VC++ installed, however the installation of MySQL fails and complains about them. Per instructions from the installation guide you have installed the Visual Studio 2017 x64. However, you have downloaded and trying to install x86 My SQL. Could you download the Visual Studio 2017 x86, install it and re-run MySQL installer again, the prerequisite check should pass. Let me know how it goes. Cheers!",314048.0
89700,408610.0,@Abhishek Kumar - did my suggestion work for you?,318762.0
90282,376278.0,"Hello Rashmi, Please try to follow below step. It should work. https://learn.upgrad.com/v/course/208/question/82028",320195.0
91630,383634.0,"Hi, Please go thorugh below link. Just had this same problem, i don't know what caused Database initialization failed. But the error was gone after i did a complete uninstall with 'MySQL Installer - Community', restart my pc and then tried to install mysql server. https://stackoverflow.com/questions/51031821/error-during-installation-database-initialization-failed If it dosent resolve this problem then you're using the SQL 8.0 version, this might happen as there is some bug in this version so you can try using the earlier verison sql 5.7.",320195.0
91630,383637.0,Please go thorugh upgrad link as well. https://learn.upgrad.com/v/course/208/question/82229,320195.0
91630,384475.0,"Hi Lalitha, You can try the following steps. Uninstall any older version of the product. Ensure you meet the pre-requisites for: (Microsoft .NET Framework) (Visual C++ Redistributable for Visual Studio) Document link on how to install MySQL Workbench for Windows https://dev.mysql.com/doc/workbench/en/wb-installing-windows.html Link to download MySQL Installer https://dev.mysql.com/downloads/installer/",314048.0
91957,385658.0,"go to control panel to make sure if MySQL is installed or not, if yes try navigating to below path assuming that you have installed it in C: drive. You should be seeing MySQLWorkbench.exe file. C:\Program Files\MySQL\MySQL Workbench 8.0 CE If you dont find install the software as per docs. if you find and still its not wokring properly, then un-install the software redo the whole installation process.",306735.0
91703,384041.0,Make sure the port is open and mysql is able to start the process on that port.,310974.0
92481,389575.0,"In sql if you want to filter on multiple values in is preferred. select * from project where plocation in ('houston','stafford'); Write like above instead of or which you used if you want to include both houston and stafford.",317689.0
92481,389542.0,'OR' in the question indicates that the response needs to include both records. You could try to build your filter condition using 'IN' clause.,306250.0
92481,389552.0,"Yes, you have to mention plocation for both locations. And if there are more filter with plocation you need to specify it every time. But there is another way to perform same task using 'IN' operator. \ You can use below code: select * from project where plocation in ('houston','stafford','Bellaire'); Hope this will help.",317991.0
91985,385896.0,"the correct query is: select * from project where plocation in ('houston','stafford');",318084.0
91985,385907.0,"when you've more than 1 argument for WHERE clause, that has to be defined using 'IN' with parenthesis.. here since you're interested in drilling down your results for project for two particular locations; it has to used as; select * from project where plocation in ('houston','stafford');",316349.0
91985,387578.0,"Instead of where plocation = 'houston' or 'stafford'; use where plocation in ( 'houston' , 'stafford');",317811.0
91992,385944.0,Please use %Houston%. Maybe the address does not end with Houston.,318084.0
91992,386069.0,"You have to use wildcard - '%' before and after the word - Houston like '%Houston%. If you use %Houston, then it will find the word that ends with Houston. whereas in our case, there is a word after Houston. So, you have to use % before and after the word/character. Refer - https://www.w3schools.com/sql/sql_wildcards.asp Thanks.",311502.0
91992,387574.0,where address like '%Houston'; Insetad of this use where address like '%Houston%';,317811.0
95799,405839.0,You have to install the visual studio for installing the MYSQL. following pdf has all the details and process for doing this. https://cdn.upgrad.com/UpGrad/temp/89ddbb36-9d58-4db6-92b9-d0bebd4d6862/MySQL+Installation+Guide+(Windows).pdf,320073.0
91783,384438.0,"This could be related to python installation, you have to install python and set the path in an environment variable. I think you can skip the installation of python connector. you can install pymysql from anaconda navigator and connect to MySql import pymysql # Open database connection db = pymysql.connect(""localhost"",""root"",""Password"",""world"" ) # prepare a cursor object using cursor() method cursor = db.cursor() # execute SQL query using execute() method. cursor.execute(""SELECT VERSION()"") # Fetch a single row using fetchone() method. data = cursor.fetchone() print (""Database version : %s "" % data) # disconnect from server db.close() Hope it helps.",317845.0
91783,384427.0,"Hi Aditya, There could be 2 reasons, 1) There is no proper SQL-Connector for the version of Python you have installed. Check if you have installed proper sql-connecter for your version of python. Proper MySQL connector for python3 can be downloaded from https://dev.mysql.com/downloads/connector/python/ 2) MySQL Connector Python requires python to be in the system’s PATH. Installation fails if python cannot be located. Add PATH variable (Environment variable) to python. My Computer &gt; Properties &gt; Advanced System Settings&gt; Environment Variables &gt; (Python installation folder)",314048.0
92333,388805.0,"Here's one error the possible solutions to some of the errors you might face while importing .csv files into MySQL 8. ERROR 1148 (42000): The used command is not allowed with this MySQL version. Solution: This error occurs because you have a problem in your MySQL configuration. You're running a MySQL installation that has certain privileges missing. You can solve this by following one of three solutions: `1. Change the secure privileges option in the config file. 2. Change the local-infile system variable value. This is off by default and you need to set it to ""on"" to resolve the issue. To check the current status of the variable run: show global variables like ""local_infile""; To change the value to ""on"" if it's off, run: set globals local_infile = ""on""; After this, you should be able to import files into MySQL 8 with the ""load data local infile"" statment.",306733.0
92333,388814.0,"Dear All, I have faced a different error ERROR 1290 The MySQL server is running with the --secure-file-priv option so it cannot execute this statement This err i got when I used Load Data Infile 'file Location' command to load the Data from csv file into tables. Solution to this problem is to set secure-file-priv to an empty string in mysql.cnf file. below the steps to make the changes go to folder location /etc/mysql/mysql.conf.d and find the file mysqld.cnf execute the command gksudo gedit mysqld.cnf in the file add secure-file-priv="""" (no spaces between) restart your server using service mysql stop; service mysql start Execute your load data infile command. Sometimes the mySQL user will not be able to access your folder and it will throw no permissions error. In such cases you can copy the csv files to sudo cp *.csv /var/lib/mysql/Assignment/. Replace the file location path to just file name you want to load. /var/.. is the . def ault location where the mysql searches for the file it needs to load. PS: i have provided the locations as per set up in my laptop. i believe this would remain the same, just check out these aspects if you still getting error. --Rajesh R",300708.0
91167,380849.0,whenever you log out it would need the password to connect to but when you have disconnected the mysql it doesnt mean you are disconnected with the server as well,318017.0
91167,380851.0,i dont rememnber disconnecting from my side...but currently the status shows that Server is Stopped. How to reinstate?,310509.0
91167,380854.0,@Chetan : could you share the screenshot of the error?,318429.0
91167,380868.0,,310509.0
91167,380897.0,"Looks like the database server is not running. Try the below steps: Click on the Cortana search box and type ""Services"". Click on the ""Services"" desktop app. You will be presented with a view displaying the services. Scroll to find services related to ""MySQL"". Check if the ""Status"" is ""Running"". If it is not ""Running"", right-click and ""Start"". Also, change the ""Startup Type"" to ""Automatic"". This can be done by right-clicking, then select properties and choosing ""Automatic"" from the drop-down for ""Startup type"". After this, try connecting. Hope this helps.",313826.0
91167,381522.0,,310509.0
91824,384709.0,"it is fine if you move on I believe. The ODBC - is used if you want to use connectors to your oracle db ;and my sqlrouter is used more for routing traffing, high availability, etc The easiest way to make sure would be to login to your mysql server via workbench, create a db, run some queries - if it all works then you should be fine.",300694.0
91824,384713.0,"hi, for the assignments/cases in this session you can move ahead as you will be able to do all those directly. odbc connector will be needed when you want to integrate some other applications like excel or Tableau to your MySQL database. MySQL router is generally used in enterprises for optimising distributed database access through different applications. however you should try reinstalling those for having complete package in your machine l which will help MySQL to perform more efficiently.",311686.0
91232,382040.0,"Hey, I actually this the other way. I think the videos are pretty neat. The first video introduces us to SQL and gives an overview then it transitions to explain its features more elaborately. And later we go ahead and try to implement these features. If you can let us know why you feel that, it'd be great. Edit: The videos were realigned. And hence the flow looked right,",319721.0
91241,381585.0,"Although it doesn't make much difference whether you use Oracle or MySQL, but I would suggest you to use MySQL because graded question as well as assignment is to be done in MySQL. And MySQL and Oracle has slight different syntax.",317991.0
91241,381438.0,you need to use MySQL - it has a slightly different syntax,300694.0
91241,381279.0,"I believe Yes it can be used. MySql is one of the database that has been recommended by the faculty to use for the course. If someone is comfortable with other DB like oracle or postgres for matter, it can be used.",317460.0
91241,381397.0,I also prefer to work on Oracle. but the graded questions and assignments are in MySQL only. I think we need to work on MySQL for this.,311686.0
91241,381639.0,"We are not supposed to. I had a similar question in the beginning but my SM confirmed. Also, the syntax is slightly different in the 2 databases.",310511.0
91241,381731.0,I would suggest you to use MySQL,314621.0
91241,382291.0,better to use Mysql as upgrad queries are written in Mysql code. but you can practice in oracle for further knowledge,311219.0
91354,381854.0,"RDBMS concepts remains same across all DB tools. The only difference is usage of SYNTAX in the tools. As this course is designed with MySQL. The connection strings used/are going to be used in the Python code is compatible with MySQL. I feel, it is better to get to use with MySQL. Thanks.",311502.0
91354,381851.0,all the assignments and graded questions ahead will be in MySQL only. we have to use that only for this course. however you will not find it much difficult. just need to see the syntax only.,311686.0
91354,381916.0,"Although it doesn't make much difference whether you use Oracle or MySQL, but I would suggest you to use MySQL because graded question as well as assignment is to be done in MySQL. And MySQL and Oracle has slight different syntax",317991.0
91354,383254.0,Thanks everyone for putting your time in resolving my question :),306147.0
91841,384764.0,"yes, it can have as many as databases as they needs depends upon how the company wants it. they may have each database for each environment say Dev,Qa,UAT,PreProd,prod. or even they may have separate database for each sector as well say for employe and payroll will have one DB, where as customer order related details will be in different db. consider Flipkart.",306735.0
91192,380940.0,Please refer the instructions given here - https://learn.upgrad.com/v/course/208/session/15786/segment/79809.,311502.0
91192,380953.0,Did you try to uninstall and reinstall already? in such case pls try this. I found on internet. You will have to delete a hidden folder. https://bugs.mysql.com/bug.php?id=78310,318007.0
92595,390350.0,"Hi Josyula, Make sure you use the same case of the plocation. ex if its SUGARLAND As per the table mentioned by you use S as caps ‘Sugarland’ and then try to execute your code and see. I hope this helps",300688.0
92595,390053.0,I Don't see anything wrong with your SQL in terms of syntax,300694.0
92595,390102.0,"I think you are getting 0(zero) rows as output. this will happen because you are saying that ""show those rows where plocation is both 'bellaire' and 'sugarland' at the same time."" AND means both these conditions need to be true simultaneously for any record to be shown. there is no such record where plocation has both these values simultaneously. by design it can't be. instead you need to use an OR here. however an ideal query will have where clause as 'plocation not equal to houston AND plocation not equal to Stafford' .",311686.0
92595,390118.0,"Try to execute select * from project where plocation = 'bellaire' or plocation = 'sugarland'; If that doesn't work try select * from project where plocation in ( 'bellaire' , 'sugarland'); Let me know if that works or not ?",317991.0
92595,390849.0,"Few things I noticed from your sql query are 1) The question asked you to exclude Houston and Stafford. But you are writing a sql query to include ""Bellaire"" and ""Sugarland"". You will get the desired results this way because your data set is small. Working on hundreds/thousands of rows, you will not be able to manually manuipulate the desired outcome. Hence, try writing a query to exclude Houston and Stafford (and not the other way round). 2) While text matching, UPPERCASE and lowercase matters. Ensure that you type the correct spelling and case. otherwise you may include/exclude undesired records. 3)In your attempt, select * from project where plocation = 'bellaire' and plocation = 'sugarland'; Try to visualize that SQL engine will execute this clause on each row, and will collect the rows in result table(in memory) that satisfy the criteria. Since, ""plocation"" is one single column, it will have one value in each row(either ""Bellaire"" or ""Sugarland"" or ""Houston"" or Stafford""), you condition will not return any results. Condition you provided for a row to qualify in result is ""plocation"" value in the row should be Bellaire, and it should also be Sugarland"". But the fact is ""plocation"" can never be both(Bellaire and Sugarland) in any single row as it can hold only one value. Hope this helps.",304393.0
92595,390757.0,"1. You need to use an or operator becuase using and means there are both Sugarland and Bellarie in plocation as stated by Chandan. 2. The compare method is case sensitive. You need to use a capital 'S' for Sugarland, as correctly pointed out by Prakash. So, the ideal code would be, select * from project where plocation = 'Bellaire' or plocation = 'Sugarland';",319721.0
92601,390152.0,Select ssn will bring only ssn column with all the rows matching to where clause.. now since you're specifying the record to show all the ssns which has houston in the address field it brings all such records from employee.. just check your employee table.. you'll understand this easily..,316349.0
92601,390140.0,Your code will give all the rows with address having 'Houston' in them. What exactly do you want to find out?,310511.0
92601,390108.0,"This is because all the rows in address contains 'Houston'. And you are using wildcard % to both side of Houston, that will fetch all rows that contain 'Houston'. Hope this clarifies your doubt.",317991.0
92601,390224.0,"all are having houston in their address, so all records will be returned.",317811.0
92601,390507.0,Using %% on both sides of the Houston will give all ssn having Houston in their address. It can be at any place. It just do a string comparsion and return all rows having Houston in their address column.,317689.0
92601,390765.0,"If you can see the image below, all the addresses have Houston in them. This code has the same logic you used.",319721.0
92764,390644.0,That is because the tables are already created and can't be created again.,310974.0
92764,390702.0,write create table if not exist. If they are already created and you want to recreate it than first drop them or else use alter command.,317689.0
127477,557706.0,Please go through below link..it has good explanation on boosting. https://towardsdatascience.com/boosting-and-adaboost-clearly-explained-856e21152d3e,317991.0
107032,462279.0,"i wrote this code ..df['CarName'].replace('vw','volksvagen',inplace = True) error is car name not defined",319969.0
107032,462300.0,instead of inplace write regex and try. and if you want to seperate company name and model name try using split().,302738.0
107032,462324.0,First use replace as str.replace undescore with space and then str.split(' ').str[0] .Hope this will work.,319006.0
107032,462442.0,"Try this: X['company'],X['brand'] = X[' name'].str.split(' ',1).str This may help you.",311117.0
107032,463224.0,Its simple go back to your movie assignement which you did yourself in the beginning there you separated genre using split function and then made a new column using get function..here you need to do the same,319869.0
107032,463956.0,Using split function,317979.0
107032,462357.0,"You can use split function. new = data[ ""Name"" ]. str .split( "" "" , n = 1 , expand = True ) # making seperate first name column from new data frame data[ ""First Name"" ] = new[ 0 ] # making seperate last name column from new data frame data[ ""Last Name"" ] = new[ 1 ] https://www.geeksforgeeks.org/python-pandas-split-strings-into-two-list-columns-using-str-split/",317845.0
103689,445199.0,"Hey Prathap, Please check whether you have cleaned you data thoroughly or not. Because, data cleaning is the most crucial part to complete before starting any further analysis.",301655.0
103689,445331.0,"Hi Prathap, Check the y-axis units. It is in percentage for sample solution. Yours is standalone count.",334535.0
102251,437256.0,"Hi Nitesh, Yes, this and related type of analysis is part of the assignment. Please review the thread you referred; it mentions ""You cannot do much about Current as they are in progress""",334535.0
102251,437214.0,You can analyse all the three status as that will give you insights and help you filter out the variables that show a strong relation with deafult. The 5 variable drivers should be on Charged Off status. I think TA ment that in the referred link https://learn.upgrad.com/v/course/208/question/102031,318451.0
102043,436850.0,python is considering every null values cells as NaN not a number.. and not sure how you're calculating the mean on the dataframe with .isin function.. if you're intending to remove all the nulls i.e. NaN; you can simply use .dropna with one of the argument as how = 'all'.. try taking help of google for dropping all the nulls in your dataframe also for additional reference.. hope this helps..,316349.0
102043,437092.0,"Refer to solution of the very first assingment provided- Try out this: df = df.loc[: , df.isnull().mean() &lt; 0.6]",308638.0
102043,437381.0,"Hi, 0.6 means 60% try using 0.06 , which is 6%",302750.0
99955,428428.0,Thank you for the clarification.,311254.0
99955,428415.0,"The problem statement is open-ended and hence if you want you can use hypothesis testing to derive any insight. But the entire case-study needs to be majorly done using the concepts of EDA only and the core part of the problem statement-identifying the factors of default' need to be done with EDA. For any further doubts, check the evaluation rubric to get an idea of what exactly is asked from you. If your analysis adheres to those guidelines and you've performed the necessary steps that are required, any further analysis would be an added bonus and we do encourage you to try it out.",313517.0
100421,432403.0,"Hi Chetan, 'issue_d' column is not holding proper date related data in string format. If you just print the values, you'll see it's in 'Mon-YY' format which AFAIK is not a valid format and hence the error. Here 'Dec-11' means December 2011, we can quickly convert it using our own function using apply on entire series.",334535.0
100421,432406.0,Also additional useful link https://stackoverflow.com/questions/32888124/pandas-out-of-bounds-nanosecond-timestamp-after-offset-rollforward-plus-adding-a,334535.0
100421,437180.0,Is this column required. issue_d for analysis. Not sure. #Suppress Warning import warnings warnings.filterwarnings('ignore'),312019.0
100427,435726.0,"From analysis of the data , idenitify variables which strongly influence the loan default. Company would like to use this data for build their Risk Analytics dashboard.",306009.0
100427,432098.0,The excel has the information on the cases approved by the online institution. We need to analyze the loan portfolio and identify the factors- demographic or behavioral which drives the delinquency on the portfolio . Based on the analysis the institution may make changes to the underwritting parameter.We need to analyze the Loan portfoli provided and segment it based on the parameters provided to asses the paramters whihc has greater impact on the Losses that the institution suffer for non payment of the Loan installments.,310629.0
100399,431632.0,Jupyter Notebook,300717.0
100399,431663.0,you can drop the attributes which will not impact your analysis but you should keep in mind that the data retained after dropping should be more then 77 percent of original data provided.,318017.0
100399,432101.0,You need to drop through python as when the codes are executed by the assessor it will be on the original csv file,310629.0
100399,433227.0,It should be Jupiter notebook.,310501.0
100399,435570.0,You cannot make any changes in the riginal data i.e in the excel file. Delete using python in jupyter notebook.,304319.0
100399,435848.0,All modification will be done on Jupyter Notebook.,310952.0
100399,436247.0,"Drop in Jupyter Notebook only , because if there is any error in your Mater Data Sheet you wont be ab;e to go back and recover the Data. In that case you have to download Data Set again. So , its better to make changes in Jupyter , that way you can always make new ipynb notebook , if anything goes wrong.",315560.0
102430,438124.0,"As the data provided is huge, there are loading issues for 32-bit system. It works well with 64-bit",318804.0
102430,438065.0,your system is behaving weird try loading file again,318017.0
102430,438183.0,"Use ""low_memory"" parameter in read_csv method.",312953.0
102430,438482.0,"Yes, you can use the 'low_memory' parameter or use lambda wherever it can be used, since it helps in reducing the time and space complexity.",301655.0
102430,438618.0,"Hi Sushma, You can use df = pd.read_csv(""your file name"",sep=\"",\"",low_memory=False). The reasoning for this is captured beautifully in this link:- https://stackoverflow.com/questions/24251219/pandas-read-csv-low-memory-and-dtype-options/24251426 Thanks and Regards Somnath",314617.0
101070,435124.0,"Hi Veena, Please check if the file you have downloaded is correct. I reverified it. It contains 39718 rows and issue_d has data from year 2007 to 2011. The problem could also be in Excel setting or your PC Regional Date settings. You can verify the loan.csv file by opening in plan text editor like notepad++. It should show first data row as ""1077430"",1314167,2500,2500,2500,"" 60 months"",""15.27%"",59.83,""C"",""C4"",""Ryder"",""&lt; 1 year"",""RENT"",30000,""Source Verified"",""Dec-11"" .....",334535.0
101070,435545.0,"Do we have to download from the website, or from the UpGrad. As I am getting only 2018 data",317418.0
101070,435550.0,"I am getting the first row as ""1077501"",""1296599"",""5000"",""5000"",""4975"",""36 months"",""10.65%"",""162.87"",""B"",""B2"",""10+ years""....",317418.0
101070,435999.0,"I opened the csv file in Notepad++, I could see only Dec-11 or Jun-10.. etc. Don't see the year in the date format. Is it an issue ?",310518.0
102376,437768.0,You have to consider both Fully Paid and Charged Off. Refer this discussion https://learn.upgrad.com/v/course/208/question/102031,310974.0
102376,437806.0,"well you need to compare charged off vs Fully Paid and identify a pattern, so ignoring fully paid would not be a good idea IMHO",304814.0
102376,437956.0,basic idea to find out which variable is key driver..that only can be identified when you make two different dataframes depending on loan status then compare both for each individual variable,319869.0
102376,438812.0,"No we cannot just use charged off for loan status. In order to understand the default rate , we would need to make a comparison of Charged off candidates with some other status and in this case it should be Fully Paid.",317460.0
102376,446144.0,no we need to consider other status' as well - just considering 'charged off' will not give a good analysis as we need to compare to fully paid loans to see what drives defaulters vs non-defaulters - so we need to compare against two different populations,300694.0
101154,435143.0,"Hi Avneesh, There are only 3 attributes of the 111 attributes which has the _inv suffix. The Data_Dictionary gives description for those. Gramener being peer-to-peer gives loan from investors funds and a minor part(~5%) from self funds. Basically, in your analysis you can determine any difference the ""_inv"" columns makes. But as such this can be a very small part of the actual analysis.",334535.0
101154,436002.0,"LendingClub is a peer to peer community, mostly the funds will be thru Investors &amp; lending it to Borrowers. LendingClub also has share of the amount in the Loan handling. But it always dependents on the Loan approval process &amp; Investors funding strategy. With the data analysis, identified that LendingClub share is just 5% (approx) when compared to Investors funds. Hope this helps.",310518.0
101194,435560.0,There are enough parameters in loan.csv to analyse.,304319.0
101194,435856.0,"Data dictionary is a general file, where you may get more than the required field for Information.",311117.0
101194,435855.0,It's not necessary that all data in the dictionary will be available in loan.csv. There are lots more parameters in loan.csv to analyze.,310952.0
100561,432214.0,"In risk analytics , what is done is monitoring the portfolio in terms of delinquencies and loss rates. Delinquencies is based on number of installments the customer has not paid.In risk analytics we need to continuously monitor the portfolio to identify the segments in the portfolio which are causing the delinquency and losses. for eg income segment- We may identify that lower income segment is contributing higher loss rate and as the income increasing the loss rate decreases. What is loss rate that the lender is willing to take decides the cut off for the income segment. Like less than 10K income segment has a loss rate of 15% and the lender is willing to take a loss of upto 10% , then the lender may tweek the policies on income segment less than 10K- they may cap the multiples to reduced levels or may charge high rate of return to cover the higher loss or they may decide to stop lending to said segment In the EDA project- objective is to identify the segments within the portfolio which would contribute to the higher loss rates as compared to other segments- It could be demographic or behaviorial. You need compared the loss rates for individual segments and subsegments and see if there is a clear demarcation on loss rates for these segments and subsegments.",310629.0
100561,432409.0,"Hi guys, For EDA I understand that the approach may give a change to the final results but the evaluation will always be based on both, i.e. approach and the right answer. For some cases, there is no right answer or wrong answer in those cases approach is evaluated. To answer your question, Try to solve the objective of the case study, i.e. identifying the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default. You may use derived metrics but clearly state your assumptions for doing so also keep in mind that the step should be useful in answering the objective of the case study. Data cleaning is an important step before any analysis, so you are required to do so but keep in mind that your step should be explained properly with the assumptions if taken. In general while evaluating we should be able to understand your approach easily. You are free to use any plots as far as they explain the objective of the case study. Hope this helps",334535.0
101208,435559.0,"Restart Anaconda. If this doesn't help, then you need to reinstall it.",304319.0
101208,436044.0,it happens when there is a lot of processing being done. esp when you are doing high cardinality columns and plotting a graph using them,300708.0
101236,435568.0,"""Purpose"" column conatins the purpose for which the loan has been taken. There is no value as CURRENT. It contains values as car, credit card, house etc.",304319.0
101236,436010.0,"I think you are referring to Loan_Status column in the dataset which has values of 'Fully Paid', 'Current', 'Charged Off'. For your EDA analysis, you can drop 'Current' in Loan Status field as you will perform EDA on 'Charged Off' Loans alone for this case study.",310518.0
101236,436108.0,"yeah sorry, it is the loan_status variable.",317418.0
100349,,nan,
101223,435542.0,"Hi, This is basically saying that the loans which have defaulted are tagged as 'Charged-off' in the banking terminology and thus the data file. The assignment is to identify the driving factors for such 'defaults' by borrowers. Hope I understood your question properly.",311686.0
101223,435910.0,Customers who are defaulted (i.e. failed to pay the installments in due time.) are grouped as Charged-off.,314730.0
101223,436008.0,"in addition to other comments above, I just want to add a point here that if the borrower is keep on failing to pay the installment is more likely to be defaulter in short time. So, delinquent related fields drives important factors (acc_now_delinq,delinq_2yrs,delinq_amnt,mths_since_last_delinq).",310518.0
101220,435546.0,"Hi, You need to upload the file as it is in Python. Then during Data Cleaning steps you need to identify which columns are redundant/not necessary. Post that you can delete such unecessary columns. Hope it helps.",311686.0
101220,435912.0,You need to load the csv file like in the previous Python assignments and clean the data / delete the unrelated coilumns. You should not delete them upfront.,314730.0
101930,436351.0,"From my understanding revolving balance is not particular for the mentioned credit. It takes into account other credits taken by the same borrower too. Because a lot of them have multiple credit line in the column named ""open_acc"" which description according to the Data Dictionary is ""The number of open credit lines in the borrower's credit file""",318451.0
101930,436344.0,"In credit card terms, a revolving balance is the portion of credit card spending that goes unpaid at the end of a billing cycle. The amount can vary, going up or down depending on the amount borrowed and the amount repaid. If you revolve a balance that is, not pay it off at the end of the month then the lender will charge you for the privilege of borrowing their money.",301644.0
101930,436866.0,"Hi Pranav, From the understanding I have is revoloving balance is some thing that was carry forwarded from previous months some thing like credit card clearance. I beilive banks would check this parameter as well before sanctioning the loan, as if this parameter (revolving balance) is more, then his/ her loan might get rejected. This parameter has nothing to do with outstanding principal as both are different all together is what I understood",312093.0
101264,435690.0,loan amount -- amount which customer applued for a loan at bank funded amount -- amount which was funded/provided by bank to the customer,316349.0
101264,435846.0,Loan Amount - Amount which customer Applied for a loan Funded Amount - Amount Received by a customer by bank,310952.0
101264,435743.0,loan amount = requested amount by consumer funded amount = amount given by bank as loan to the consumer. Funded amount may be less than or equal to loan amount.,301644.0
101249,435691.0,"this is the key here Ranjana.. you've to think through to select the columns which will help you solve the business objective.. As far grouping is concerned, i don't we can group any columns but if that helps andbgives you some useful insights, go ahead and group it..",316349.0
101249,435693.0,"You need to assess the various demographic factors which has impact on the loss rate of the institution. lets take an example of income . If we band the income in to group and assess that loss rate against each income band we may see that lower income band would have high loss rates while higher income band may have smaller loss rates. Based on this information the institution may change the lending policies where stringent norms are created for lower income band customers. Similar you need to assess other parameter whihc could have impact on the loss rate , so that necessary chanes can be made in the lending policy in regards to those parameters so that the over rall loss rate is within the risk appetite of the institute",310629.0
101249,435742.0,"My group is from non-financial background, so we started off by searching online and asking bank experts on what type of consumer behaviour they look into before giving out loans. We compiled these external insights and then went ahead exploring the data set given to us. It is a trial and error process as the insights received may not necessarily match with the findings from dataset. But it definitely helped our group get a initial grip of the case study problem. Good luck!",301644.0
101931,436347.0,"Hi Manasa, As part of the assignment please review whether the id and member_id contribute to the analysis. This needs to be done for other columns too. It should not be just because of not having duplicates.",334535.0
101931,436333.0,ID column can act as an index column. You may drop it but will it really add value?,301644.0
101316,435931.0,No. You are not missing anything but some fields mentioned in RejectsStats are available in Loans.csv and some need to be calculated or string manipulated from the fields of loans.csv,301121.0
101316,435997.0,"No. Data dictionary for Loan Stats &amp; Reject Stats provided, but Reject Stats data are not provided as mentioned in the Problem Statement. You can ignore Reject Stats Data Dictionary for this problem statement.",310518.0
101316,436376.0,we can ignore reject stats data dictionary,314678.0
101323,436193.0,Check for the columns which arent useful for both univariate and bivariate analysis.It is beeter to drop unecessary columns and proceed,308638.0
101323,435994.0,"Hi Madhusudhan, Review the column and check if you can group the columns. Generally if there are several columns it is a good idea to group them and assess if then add value to the analysis or not.",334535.0
101938,436342.0,"No its not. The description in the Data Dictonary clearly states that ""Indicates if income was verified by LC, not verified, or if the income source was verified"". By my understanding it maybe if the borrower has submitted their income tax statement/salary slip it may termed as ""Source Verified"" and LC may have some other ways to verify the income from their own which are termed as ""Verified"".",318451.0
101938,436330.0,"In verification_status column, Source Verified and Verified are not same. Source Verified indicates the source of income has been verified. For example, whether the loan applicant is a salaried employee at a certain organization or not is what is meant by source verified. The Verified status is given when the actual salary of the loan applicant from the source of income has been verified by the Bank.",301644.0
101324,436011.0,You can refer this wiki. https://en.wikipedia.org/wiki/Mortgage_underwriting Basics of Mortagage Underwriting,310518.0
101324,436043.0,You can refer below. https://en.wikipedia.org/wiki/Credit_risk https://www.kaggle.com/janiobachmann/lending-club-risk-analysis-and-metrics https://www.investopedia.com/terms/c/creditrisk.asp,314730.0
101325,435963.0,It is mentioned in FAQ that we have to understand the buisness model from the website. It will help in identifying variables required for doing the analysis. Probably it will help gain domain knowledge and we think in terms of investors and borrowers which would provide valuable insights on the final data we arrive at.,306736.0
101336,436045.0,"There is no threshold count. You can remove all the columns having NAN values as they do not have any significance. For columns having few NaN values, I believe you can make a judgement whether to remove the corresponding rows or leave them as is if the percentage of NaN values is less than 5% (Provided these columns have some relevance in your risk analysis).",314730.0
101336,436104.0,"I would suggest to drop all the NA columns as they will not have any impact on your analysis and for the ones having both NA and non-NA values, see if that particular row contains data of any relevance for your analysis then you can keep it. *My suggestion would be to keep it.",305656.0
101336,436360.0,can drop all the columns with more than 6 percent of nan values,314678.0
101336,436518.0,calculate % of nan values present in columns .there is no point having columns have high % of nan values as you can not infer anything on those variables.,319869.0
101351,436166.0,Kindly take a look at the below link: https://learn.upgrad.com/v/course/208/question/101154,311160.0
101351,436281.0,"Hi , Inv means Investors, who are one of the source of funds for the finanace company. The peer to peer can give loan from the investor funds. In order to answer your query, As you see in the data see two columns, funded_ammt - total loan amount funded by the finanance company funded_ammt_inv - the portion of the loan amount given from investor funds So , total_pymnt - Payments received to date for total amount funded total_pymnt_inv - Payments received to date for portion of total amount funded by investors Hope this helps.",302750.0
101351,436516.0,not whole amount funded by finance company portion of amount by investor fund too. that's why total payment is amount recieved by borrower till date in his loan account total_payment_inv is the amount which is going to contribute return in investor portion..,319869.0
101969,436463.0,"Based on each loan application and credit report, every loan is assigned a grade ranging from A1 to E5 with a corresponding interest rate. Check the below link for more info: https://www.lendingclub.com/public/rates-and-fees.action",309451.0
101969,436521.0,"Lending Club or any peer to peer company determines whether the borrower is credit worthy and assigns to its approved loans a credit grade that determines payable interest rate and fees. Grades are like ratings for the borrowers. Any borrower with Rating A is highly likely to get a loan as compared to a borrower with Grade D . Grades are based assigned to borrowers on various metrics like their Credit History , monthly income etc . Hope this clears your doubt.",315560.0
101974,436502.0,As this is openended question we may use hypothesis testisng to derive conclusion but the main part here is to use EDA to derive factors contributing default is mandatory.,320197.0
102281,437384.0,"https://stackoverflow.com/questions/23415500/pandas-plotting-a-stacked-bar-chart test5.plot(kind='bar', stacked=True)",312019.0
102281,437369.0,Figured out myself https://github.com/mwaskom/seaborn/issues/1027,310974.0
101978,436805.0,Broadly dti is the ratio of the monthly EMI to the monthly disclosed income.,304319.0
101978,436455.0,Below is the link which explain dti in simple words. https://www.consumerfinance.gov/ask-cfpb/what-is-a-debt-to-income-ratio-why-is-the-43-debt-to-income-ratio-important-en-1791/ Regarding relationship with other columns is the task we have to perform as part of assignment. Hope this helps.,317991.0
101978,436588.0,"The debt-to-income ratio is one way lenders, measure an individual’s ability to manage monthly payment and repay debts.In lehman terms it is the percentage whihc tells if the borrower will be able to repay monthly installments or not. Higher the dti ,lower is the probability to repay and vice versa . Please, see to the link it will help you understand dti more precisely and easily.: https://www.investopedia.com/terms/d/dti.asp",315560.0
101978,437540.0,"In simple words, it is the percentage of total monthly debt by total monthly gross income which can determine whether a person will be able to repay any loan or not. Lower the DTI, more chances of getting loan.",318448.0
101978,436998.0,"Your debt-to-income ratio (DTI) compares the total amount you owe every month to the total amount you earn. Lenders may consider your debt-to-income ratio in tandem with credit reports and credit scores when weighing credit applications. To calculate your DTI, divide your total recurring monthly debt (such as credit card payments, mortgage, and auto loan) by your gross monthly income (the total amount you make each month before taxes, withholdings, and expenses). For example, if your total monthly debt is $3,000, and your gross monthly income is $6,000, you would divide 3,000 by 6,000 to get .5 or 50%. That's because DTI is considered an indicator of whether you'll be able to repay a loan. If you have a low DTI, meaning you make much more than you owe, you might be better able to repay a new loan. However, if you already have a lot of debt, taking out additional credit might make it difficult for you to meet your financial obligations. Generally, to get a qualified mortgage, your DTI needs to be below 43%. In fact, the lower your DTI the better, and many lenders prefer ratios below 36%.",317822.0
101993,436553.0,"Hi Neerajakshulu, If you are referring to the RejectStats tab in the DataDisctionary, that is additional information. Review the below lines in the problem statement. It will provide more clarity. "" Loan rejected : The company had rejected the loan (because the candidate does not meet their requirements etc.). Since the loan was rejected, there is no transactional history of those applicants with the company and so this data is not available with the company (and thus in this dataset) """,334535.0
101999,436599.0,"Anyhow you are dropping columns which have NaN values, thus no point wasting efforts for those columns. Thus, this should be done to those columns only which you are retaining.",313767.0
101999,437265.0,droping the right nan columns might be included in the data quality evaluation criteria.,317822.0
98697,420295.0,"I believe we have to form the groups again. As far as I remember, it was being discussed during the first goup case study that we can change the group for any future assignments. So looks like we need to submit the group member names again. We must be receiving a mail soon for group name submission. TA's can verify further.",309451.0
98697,420317.0,"I had checked with my student mentor and he said that if we do not have any changes in the group members then the previous group member names will show up by default. It is not reflecting though. There maybe a date after which it will reflect. If you have a change in the group names, then do inform your student mentor else leave as is.",301644.0
98697,426006.0,The groups are now reflecting on the top. received an e-mail today,300729.0
98697,426079.0,"For me its reflected with same old group members.If you want to change the group members, Please check with your mentor",305804.0
98697,427042.0,"For me also, the earlier group members are showing up.",307494.0
102021,436690.0,funded_amnt = total amount funded = funded by investors + loan funded by lending club Funded_amnt_inv = loan funded by investors,320197.0
102021,436705.0,Also check out https://learn.upgrad.com/v/course/208/question/101351,334535.0
102021,437278.0,the loan amount is shared b/w the bank and the investers therefore funded_amnt ( total amount funded) sum of funded by investors and loan funded by lending club,317822.0
102021,437436.0,"Hi Manasa, funded_amnt =""Amount Funded by the Lender i.e bank"" funded_amnt_inv=""Total committed by Investors"" Thanks and Regards Somnath",314617.0
102031,436721.0,It should not be just for charged off data only; Rest of the data shall be also used to get the clues if loan can be sanctioned or not based on various credentials.,301121.0
102031,436800.0,"From my understanding taking all the data will be a good option. Because if you found a particular pattern like if Variable X, Variable Y, Variable Z are present than the person is most likely to default and the loan being charged off. You can verify if that variable you found are also present for other status i.e. Fully Paid and Current. And you found that Variable X is present in other two but still person did not default so you can eliminate that variable from your conclusion. Since it’s not the differentiating factor.",318451.0
102031,436834.0,"In Rubrics, third criteria, i.e. in data analysis, you can see in ""meet expectations"" column that, for making business analytical sense, these variables will be a factor. i.e. for comparison, which grades of peoples have fully paid and in current loan status. You can refer this: https://learn.upgrad.com/v/course/208/session/20136/segment/102524.",311117.0
102031,436863.0,"I think it makes sense to consider only ""Charged off"" data since we are predicting drivers for loan default. it will be good if TAs give their opnion.",313691.0
102031,436993.0,we must consider only Charged if we are predicting for loan defaulters.but u can still do extra comparision to try and get some clues about the data set,317822.0
102031,437034.0,"Team, As mentioned in problem statement, there as 3 statuses Fully paid : Applicant has fully paid the loan (the principal and the interest rate) Current : Applicant is in the process of paying the instalments, i.e. the tenure of the loan is not yet completed. These candidates are not labelled as 'defaulted'. Charged-off : Applicant has not paid the instalments in due time for a long period of time, i.e. he/she has defaulted on the loan You cannot do much about Current as they are in progress. It does not serve much purpose to analyze. Rest statuses need to be analyzed, in itself, as well as percentage of total.",334535.0
102040,436755.0,It means whether you have bankruptcy record in bank or not.,318426.0
102040,436792.0,"pub_rec_bankruptcies - this column indicates the number of times the person has filed for bankruptcy in past. Bankruptcy is a legal status of a person or other entity who cannot repay debts to creditors. In most jurisdictions, bankruptcy is imposed by a court order, often initiated by the debtor. fico ranges high and low - FICO is perhaps the best known credit scoring company. The most well-known FICO scores have a range from 300 to 850 . In theory an 850 would be the highest score you could get.",318451.0
102040,436844.0,"Regarding public record Bankruptcy, can refer the link: https://www.thebalance.com/public-records-and-your-credit-report-960740. Regarding Fico ranges, can refer link: https://www.creditkarma.com/advice/i/good-fico-score-range. It's well defined there.",311117.0
102040,437599.0,FICO Score is just like sibil Score. Where as Bankruptcy score represents,318016.0
102040,437098.0,"Higher scores can help you qualify for a loan or credit card with better terms, like a lower interest rate, which can save you money on interest payments. High credit scores could also make it easier to be approved for an apartment rental and may even lead to lower car insurance premiums. Your credit scores can be an important factor in a lender’s decision-making process, and having higher scores may get you better terms. The base scores range from 300 to 850. FICO breaks down its base credit score ranges based on the FICO® Score 8 credit-scoring model. Poor Fair Good Very Good Excellent FICO® Score 8 300 to 559 580 to 669 670 to 739 740 to 799 800 to 850",317822.0
102100,436973.0,you can use conda update seaborn is the module is already installed this automaticall update the version and the required depencency . You can check the version using conda list,318476.0
102100,436887.0,"Check your version of seaborn, if it is below 0.9 then you need to upgrad to 0.9 version of seaborn. Please go through this link. This might be of help. Try this command conda install -c anaconda seaborn=0.9.0 Also you can follow below links for more information https://stackoverflow.com/questions/51402579/module-seaborn-has-no-attribute-any-graph https://stackoverflow.com/questions/51846948/seaborn-lineplot-module-object-has-no-attribute-lineplot Hope this will help.",317991.0
102054,436840.0,"The factors that ""last payment received, amount of late payment fees received"" will definitely be used as a driving factor, as they will give sight for further sanctioning of the loan to a particular grade of person, and it may be a key factor for any agency. You can additionally refer this for more sight: https://learn.upgrad.com/v/course/208/session/20136/segment/102524.",311117.0
102054,436810.0,I think your question is related to whether we should filter out for current status in the loan_status column and do analysis on that. If this is your question please refer following link. https://learn.upgrad.com/v/course/208/question/102031 I hope your driving variable doubt will get solved after viewing the above link discussion. Please reply if i misunderstood your question.,318451.0
102102,437011.0,Try to find pattern and relate it with other variables which are important for our analysis.,318451.0
102102,437263.0,"we can try find some hidden patterns ,but considering that there are 115 col we have to delete some",317822.0
102105,437009.0,"revol_bal - column indicates the amount that you are liable to pay. It is a culumative amount of all the open credit account you have. revol_util - its a percentage value which tells you how much percent of your balance revolving credit forms to the total credit limit/total credit taken. To calculate overall utilization (all revolving accounts), add up all of the credit limits (total credit limit) and all of the balances (total balance) on your revolving accounts, divide the total balance by total credit limit, and multiply that number by 100. https://finance.yahoo.com/news/credit-101-revolving-utilization-060108050.html",318451.0
102105,436954.0,"A “revolving” credit account is an account where your monthly payment is based on your balance, which can change from month to month. Revolving balances can also be paid in full without incurring finance charges, if paid within the the “grace period.” Revolving utilization, also known as your “debt-to-limit ratio” or “ credit utilization ,” measures the amount of your revolving credit limits that you are currently using. Your revolving utilization is an important part of your credit score . These calculations are made both on individual revolving accounts, and on the total credit limits and balances of all revolving accounts appearing on a credit report. https://blog.credit.com/2013/04/what-is-revolving-utilization-65530/",310974.0
102106,436951.0,"In classical sense, credit line is any source through which the individual can draw money. It could loan as well as a credit card and I believe your understanding is right.",310974.0
102106,437005.0,Yaa your understanding is right for 1) &amp; 2) point earliest_cr_line mean the first credit taken by the borrower as mentioned in the file.,318451.0
102107,436950.0,"Yes data wise, it's a good question but in the purview of this case study, you can ignore these for analysis.",310974.0
102107,436974.0,We can use the loan_status cloumns to figure out the present status of the loan,318476.0
102107,437000.0,out_prncp and out_prncp_inv can be 0 since the borrower has been declared a defaulter it’s a financial practice to take the amount out of the books. As we no more expect that amount to be received so it will serve no good to keep it in outstanding column. If there would have been a column of the amount defaulted it would be considered under it.,318451.0
102108,436948.0,Not necessarily right? Investors money will be re-invested in other loans and so on.,310974.0
102108,436971.0,total_pymnt: Payments received to date for total amount funded total_pymnt_inv Payments received to date for portion of total amount funded by investors total_pymnt is for the amount received still date for the toatl fund but that total fund can come for x number of investors so total_pymnt/x is the amount an investor can expect as a repay which going to be small.,318476.0
102108,436972.0,lc tries to give back the investor money even when the default occurs in the default case all the money they received they would give it to investor only,318017.0
102108,437061.0,total_pymnt - This column indicates the total amount LC recieved from the borrower. Since LC is providing the service of a marketplace where the lenders and borrowers meet and transact. LC charges lender a particular %fee on the loan amount that they collected back from borrowers. total_pymnt_inv - In this column the LC collect payment is subtracted by the %fee LC will charge for their services. After deducting the fee the balance loan amount recieved is directed to the Lenders account.,318451.0
102108,437047.0,"Hi Rashmi, You will notice that for those rows there is a big difference between funded_amnt and funded_amnt_inv. Thus, since only a small part of the loan was funded by investor/s, proportionate part will be refunded back.",334535.0
102109,436946.0,The most recent month LC pulled credit for this loan I think it's the month when the last installment was recevied by LC.,310974.0
102109,436970.0,I simple trems its the most recent month Lending Club pulled/recived the emi for the loan,318476.0
102109,436997.0,Last Date on which LC Pull credit for this loan means the last date when the LC enquired about the loan. This is basic enquiry that LC carries. For more info please refer below link https://www.nfcc.org/blog/pulling-credit-report-soft-vs-hard-credit-inquiry/,318451.0
102109,437097.0,it is the last date when the money was recevied by LC,317822.0
102112,436943.0,"You are free to do so but the evaluation rubric doesn't say that. I would say, save your time by not attempting it and just do the plain old EDA &amp; Visualization to solve the problem. Good Luck.",310974.0
102112,437021.0,Yes. If you have enough time you can do hypothesis testing too. If not you can simply do EDA and the required part as per the problem statement and evaluation rubric.,317991.0
102112,437046.0,It completely depends on the way you structure your analysis. I f your are not making any hypothesis and assumptions and completely doing EDA on the data you do not need it. If you are making a hypothesis and concluding something on that than you have to do hypothesis testing.,318451.0
102112,437096.0,"Yes we did ,but it was is not compulsory",317822.0
102143,437010.0,"When your max population gives you mean, median, mode as X,Y,Z and when you include some entries it get distorted and the resultant value of dispersion are A,B,C then it is a outlier. when that is not true for the maxmium number of enteries in your data.",318451.0
102143,437115.0,unless the outlier distorts ur data so much that there is a considerable difference between ur mean and median and ur box graph of the variable look very bad there is no need to remove them,317822.0
102143,438253.0,"I found an interesting article online. Reference : https://www.theanalysisfactor.com/outliers-to-drop-or-not-to-drop/ [ All of the excerpts are from the website; there is one video where Prof Srinivasaraghavan mentions of log(x) and I think it is helpful, hence mentioned it below for your reference ] 1) If it is obvious that the outlier is due to incorrectly entered or measured data, you should drop the outlier 2) If the outlier does not change the results but does affect assumptions, you may drop the outlier. But note that in a footnote of your paper This is the part I like the most within this online article: [ In one of the lectures, prof Srinivasaraghavan uses log(x) to plot : https://learn.upgrad.com/v/course/208/session/19856/segment/101024 , this is precisely the situation why log(x) is used, see below ] So in those cases where you shouldn’t drop the outlier, what do you do? One option is to try a transformation . Square root and log transformations both pull in high numbers. This can make assumptions work better if the outlier is a dependent variable and can reduce the impact of a single point if the outlier is an independent variable. Another option is to try a different model. This should be done with caution , but it may be that a non-linear model fits better. For example, in example 3, perhaps an exponential curve fits the data with the outlier intact. Whichever approach you take, you need to know your data and your research area well. Try different approaches, and see which make theoretical sense.",309211.0
102151,437108.0,yup,317822.0
102151,437056.0,Thanks for highlighting your concern Aditya.,334535.0
102184,437091.0,"YES. Once data cleaning is done-We are suppose to find the driving factors(driver variables) behind loan default, i.e. the variables which are strong indicators of default can be.This can be identified while doing univariate,segmented univariate and bivariate analysis.",308638.0
102184,437067.0,"Yes. After cleaning the data, you may have left with X number of columns each column can be a variable. Many columns can be clubbed to form one variable. You can derive many variables from the available column. These variable which you derive should show a strong relation with the person being defaulter That you have to prove using Univariate and Segmented Univariate analysis. And as per the condition at least 5 strong and logical variables will lead to getting full marks for that analysis evaluation matric",318451.0
102218,437192.0,To normalize data you can look into calculating percentages within the relevant sample population,300694.0
102218,437212.0,For outliers interquartile deviation will be the best parameter.,318451.0
102218,437395.0,You can chose some other visual if Histogram not working for you ; also check if u can take percentages /ratios wherever possible.,300731.0
102248,437209.0,I don't think so. We can put our hypothesis while presenting in a PPT.,301648.0
102248,437260.0,"i dont think its a compulsary ,we can include it in the ppt if we want to",317822.0
102248,437378.0,This case study is more related to EDA. So if you want to mention about the hypothesis testing it can be done in ppt,318804.0
102235,437298.0,Is these columns are required for analysis ?,312019.0
102235,437191.0,I didn't fully understand your question. But if you are saying that you want to convert the date string into date object format then that's certainly possible by importing datetime and formats %b and %y to convert Jan85 to 1/1/1985,300694.0
102235,437211.0,"Use the following parameters : loan[XXX] = pd.to_datetime(loan[XXX], format='', errors='')",318451.0
102220,437158.0,There is no column for age and DOB column in loan.csv dataset. We have to do loan default assignment based on the data provided. Whatever data is present in loan.csv file we have to take that only and perform over analysis based on problem statement.,317991.0
102220,437244.0,"age and dob coloumn are not present ,but age would have been a intresting data to have, Dob i dont think has as much value,though once could say age is just a derived data from dob",317822.0
102241,437189.0,No. Why do you feel we need to do that?,300694.0
102241,437208.0,No. The data is given only for the Loan which are accepted by the LC. Reject ones are not processsed and hence not recorded.,318451.0
102275,437322.0,"Interesting, since I got more records of 36 months who defaulted when compared to 60 months.",310974.0
102275,437341.0,Based on what you have written then you could say that higher loan term is a driver for people defaulting,300694.0
102275,438717.0,"Hi Aditya, For further analysis you can club loan term with interest rate and see some insights too.Moreover if you carry this step further you do analysis of int_rate with loan_status .There will be a useful variation i.e you can see how loan_term has an effect on interest rates and how interest rates are affecting loan_status. Thanks and Regards Somnath",314617.0
102275,437393.0,"For further analysis, you can use segmented univariate analysis and check how the loan term is affected by other categories.",313517.0
102271,437320.0,"loan_status is the column where you find the details. Value ""Charged Off"" means defaulted.",310974.0
102271,437377.0,Refer the Data dictionary.xlsx file for better understanding of all the columns and refer the problem statement. It is clearly mentioned that Defaulters will be stated as charged off,318804.0
102288,437388.0,Please check this link: https://blog.credit.com/2013/04/what-is-revolving-utilization-65530/,313517.0
101348,436101.0,"Hi Akbar, I suppose there will be some minimal amount of information available for loan to be sanctioned (since loan rejection data is not provided). We have to analyze based on the given data as considering too many variables for the analysis would be an overkill and add to the complexity. Ignoring non required data is also part of analysis",306736.0
101348,436110.0,"Hi Akbar, The given problem statement talks about 3 customer categories - fully paid, current and charged-off respectively. Based on the given data, I would suggest targeting the current customers to record and find out whether they are going to pay or not - high priority, as for the fully paid you can keep them as low priority because they have a good history, charged off are the red flags one where we would require to decide whether to give load at all or not. Also as for the dataset, it contains the past record of the customer in loan payments.",305656.0
101348,436215.0,"This is an EDA assignment not a prediction modeling one. Problem statement clearly says we need to find Consumer Attributes and Loan Attributes that drive default loans. Based on these key variables, give recommendations for future loan predictions. There is no mention of current loan default prediction.",301644.0
101348,436263.0,"I never mentioned that you need to do some kind of predictive modelling but aren't you deciding the loan conditions from historical data, when I say that you need to find out whether they are going to pay or not then I meant to say that using the historical behaviour of the customer by seeing customer attributes we can derive metrics to understand whether he is a right fit to give loan or not and if we are going to give whether we need to change the loan conditions. For example, delinquency refers to the unpaid amount and that mostly would be associated with the current customers then based on amount, we can derive an insight on customer behaviour.",305656.0
101348,436119.0,"Hi Najaf, The problem with that approach is that nowhere in the problem statement does it say that we need to find out if the current loan customers will default or not. It says, and I quote ""If one is able to identify these risky loan applicants, then such loans can be reduced thereby cutting down the amount of credit loss. Identification of such applicants using EDA is the aim of this case study."" Clearly referring to applicants, it seems like we need to make some assumptions as to what data will be taken from applicants.",319357.0
102295,437396.0,"Yes, you certainly can since this entire analysis is on the loans that get defaulted.",313517.0
102297,437399.0,"A credit line is a pool of money available for borrowing. Also known as a line of credit, these loans have a maximum limit, and borrowers have the option of borrowing any amount up to that limit In simplest terms, it is an arrangement between a financial institution, usually a bank, and a customer, that established the maximum amount of a loan that the customer can borrow. There are some good examples given in this link. Please check them : https://www.thebalance.com/what-is-a-credit-line-315586",313517.0
102298,437404.0,I don't think it is required. It is neither a customer qualifier nor a loan qualifier.,310974.0
102298,437437.0,"@mahima, the issue_d column shows data from 2007 to 2011, yet the data dictionary mentions about emp_title as -&gt; Employer Title replaces Employer Name for all loans listed after 9/23/2013 . Is this a generic statement or it has to do something with the analysis, please clarify.",318078.0
102298,437489.0,"Will suggest rather than employee title focus on employee length, which will provide better indication. Employee title you can't convert into derived metrics but emplyee length you can.",313767.0
102298,437406.0,"If you check the data dictionary, it means the job title supplied by the borrower when applying for the loan. It depends on you whether you want to include it in your analysis or not. Just do a few preliminary checks like the number of null values and number of unique emp_titles. That should give you an idea of whether to use this column or not.",313517.0
102298,438486.0,"Everyone can have a different approach for their data cleaning and understanding process, that's why some of the columns might be there in some group's dataset while some of them may not have that particular column in their dataset. It all works on how you are grasping the things. So, there is nothing to worry if the columns of your dataset don't match with others. Just go ahead with your analysis",301655.0
102304,437424.0,For outlier analysis general rule of thumb is that you need to keep your data between 5-95 percentile.,313517.0
102304,437430.0,at 95 perecentile about 1900 values are getting removed whereas at 99 percentile about 360 rows would be removed . Won't above 95 perectile result is loss of relevant data in this case?,300698.0
102304,438722.0,"Hi Ravindra , For this case study it would be better to have a categorization of interest rates as high and low.For categorization we can have an analysis on interest rates by using describe function and we can take the mean as threshold value if difference between 25 percentile ans 75 percentile is not so significant otherwise we can carry forward with median.Then using the specific value ,all values above that can be termed as High and subsquently all values below that can be classified as Low. Thanks and Regards Somnath",314617.0
102304,437435.0,"Yeah, that's true and the reason why this is happening is that the outliers are not well defined in the int_rate case. What I mean to say is that the values are pretty close and large discrepancies aren't easily observed. So generally in such cases, we don't go for outlier analysis. A better idea would be to categorise them into specific buckets (like low, high, etc. )and continue your analysis. For the annual income case, yes you can go for an outlier analysis but again categorization is a much better idea.",313517.0
100746,432971.0,"By analysing the existing applications, we need to come up with driving factors.",310974.0
100746,432992.0,"Hi Shayari, Please note the below lines from the problem statement ""If one is able to identify these risky loan applicants, then such loans can be reduced thereby cutting down the amount of credit loss. Identification of such applicants using EDA is the aim of this case study. In other words, the company wants to understand the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default. The company can utilise this knowledge for its portfolio and risk assessment "" So basically, you need to come up with driving factors basis existing default/non-default cases and use those while processing/approving new loan applications.",334535.0
100746,436176.0,We are required to find driving factors for past data so that the company can focus on those factors for approval of loan for new applicants,308638.0
102330,437586.0,Too many distinct values in the emp_title columns might not be that useful for EDA. But if you still don't want to drop the columns then you create own categorical variable like in above case you can put them under veterans.,318476.0
102330,437779.0,Delete the column,318451.0
102330,437810.0,"emp_title looks like a free text, and might not be helpful for analysis. You can keep it or can even drop this column",304814.0
102330,438182.0,You can consider dropping the column as it doesn't have significance in analysis.,312953.0
102330,438338.0,You can consider dropping the same if each value is unique.,318770.0
102334,437584.0,issue_id will be string format when you intially import it. Below are the ways you can to the conversation: 1. using pd.to_datetime() make sure you give the correct format in which the data is now for this columns or else u end us betting error. 2. if you are only interested in fetching the year or month from this field a normal string split function will also work. Hope this help.,318476.0
102334,437780.0,"Use the following parameters : loan[XXX] = pd.to_datetime(loan[XXX], format='', errors='')",318451.0
102334,437597.0,"Please specify the input format of date in the pd.datatime function. Input data is in format e.g: ""Dec-11"". Input formats are clearly present in below link this can help you to find which format strings to be used. https://docs.python.org/2/library/datetime.html",318328.0
102350,437621.0,Check your seaborn version and update to the latest one.,310974.0
102350,437809.0,"If it doesnt work, then use factorplot.",304814.0
102350,438241.0,"Refer to https://seaborn.pydata.org/whatsnew.html As per website v0.9.0 (July 2018) has what you need. More details below and from the reference web page: API changes A few functions have been renamed or have had changes to their default parameters. The factorplot function has been renamed to catplot() . The new name ditches the original R-inflected terminology to use a name that is more consistent with terminology in pandas and in seaborn itself. This change should hopefully make catplot() easier to discover, and it should make more clear what its role is. factorplot still exists and will pass its arguments through to catplot() with a warning. It may be removed eventually, but the transition will be as gradual as possible. The other reason that the factorplot name was changed was to ease another alteration which is that the default kind in catplot() is now ""strip"" (corresponding to stripplot() ). This plots a categorical scatter plot which is usually a much better place to start and is more consistent with the default in relplot() . The old default style in factorplot ( ""point"" , corresponding to pointplot() ) remains available if you want to show a statistical estimation.",309211.0
102355,437704.0,"Hi, you can use below- loan['int_rate']=loan['int_rate'].str.rstrip('%').astype('float') Hope it helps :)",305129.0
102355,437808.0,Abhishek nailed it. so the idea is to remove the % and change the datatype,304814.0
102369,437709.0,we thought of creating box plot for each numerical attribute which gives you the idea about the spread of data.. based on which you plan the type of plot for those attributes and see what you can infer from it.. that is what we did in univariate and bivariate analysis.. see if this helps you..,316349.0
102369,437807.0,"For Univariate, Identify 6/7 variables and draw the plots to identify the pattern. Once you have important variables, then you , based on your analysis, can come up with combination of Univariate and perform Bi variate analysis",304814.0
102369,437972.0,"To start with Univariate n Segmented analysis n bivariate 1 Identify the variables which may affect loan status 2 And plot frequency distributions for the variables identified n draw inferences. 3 As target variable is loan default ,do segmented analysis using appropriate plots with variables which may influence loan status. Draw inferences from each plot drawn say for example see impact of grade on loan status by plotting appropriate plot 4 For bivariate analysis identify numeric/continuous variables and find correlation matrix and related plots",308638.0
102312,437509.0,You can use pd.fillna method to fill the missing values or can drop that row based on your judgement for that feature values.,318476.0
102312,437811.0,"emp_length is indeed imporant. As Sambit mentioned, you can either impute values or drop such rows if %age is not high. ( I preferred later)",304814.0
88648,367078.0,"In shorter terms .. model is something that represents your data in brief. When you study the data, you infer something that is repeated in pattern or is apparent throughout. You can use that as model. But this is just a bird's eye view of this topic for basic understanding.",308962.0
88648,366993.0,Modelling in CRISP-DM is to search useful patterns. The modeling phase includes four tasks. These are Selecting modeling techniques Designing test(s) Building model(s) Assessing model(s) For detailed information follow the below link: https://www.dummies.com/programming/big-data/phase-4-of-the-crisp-dm-process-model-modeling/,308439.0
88648,367046.0,"In very basic terms, a model is something that represents the current data set and the future data set as well. In mathematical terms, it is a function of the subset of data given as input and outputs another subset (usually a single value) of the same data for the current and future inputs.",310974.0
88648,367115.0,Modelling as per the name suggests- preparing a model for any data.. after a model is prepared we test different scenarios on that particular model.. and use it for our solution..,305129.0
88648,367212.0,"What I understood is -- * a Model is set of methods/algorithms that works on a series of input data and yields result as per the given logic to solve a specific business problem (typically decisive/predictive in nature -- e.g. with our profile, past years results etc. as input, a model can be built to predict who all amongst us are likely to score a CGPA of 3.5 or higher :)). * Models, once built, need to be evaluated on test data and eventually deployed once the results seem to be satisfactory. * Build-Evaluate-Deploy should be iterative as the model effectiveness might fall down over time given the input data varies in nature * Model should be chosen judiciously as different models on same input will work differently.. With flour, sugar, milk etc. as input we can get cake also and biscuits also depending on which machine we feed that to",301116.0
88648,367243.0,"In simple language : 1.Say you have a problem statement and a few inputs or arguments. 2.You are done with understanding the problem statement and you know what is your goal. 3.You also have your inputs ready (after cleaning &amp; analyzing &amp; formatting it). 4.You know you can solve the problem using Python/ R/ Sql/ Excel/ Tableau ( different models to your problem statement ). Now, you have to choose one particular model or method to get the desired output.",307488.0
88648,367225.0,"A few things that I would like to add to the answers written above:- Now that you have cleaned the data, you will want to draw some inferences from it. To do that you will have to create a mathematical model the data. For particular problems only some models are suited. Ex. a) Supervised Learning (If we have labelled data)- Models suited are- Regression, Classification, Multi-Variate Classification, etc. b) Unsupervised Learning (If we have unlabelled data)- Models suited are- Clustering, Anomaly-Detection, etc. Some examples of Modelling tasks: 1. Classification- Let's say we have historical crime data. We want to create a recidivism model, i.e., whether the person will commit crime again after leaving jail. We have previous data about the convict like his/her education, marital status, zip-code, whether parents divorced, etc. and whether commited crime or not (0 or 1). The algorithm of choice is Logistic Regression. 2. Clustering- Let's say we have data about just two parameters- weight and height of the customer. We want to predict what size of T-shirts will fit them (S,M,L,XL). But we don't have any previous data about customers. We want the computer to figure out itself. The algorithm of choice is k-means clustering. Hope it helps :)",301652.0
88648,367027.0,"Ayush, Forget about the modelling for a second and think in terms of simple mathematics. lets say: we have a set of numbers as 2,4,6,8,10. We need a model to describe this data. (Problem statement) 1) first part of modelling: understand the problem domain and select suitable family of model. --&gt;&gt; So what does our understanding say, that this is a set of positive intergers(thats our problem domain) and what family of model will satisy ( model to identify even numbers). 2) appropriate algorithm: we need a algorithm to derive a model that describe the data with min probability of error. Therefore in our case, the algorithm would be like ANY EVEN NUMBER DIVIDED BY 2 WILL GIVE ZERO REMAINDER. (N%2 ==0) 3) after modelling, testing the accuracy of model: now we have created a model (n%2 ==0) we can test this by inserting any number, if its a even number like 24 model will succeed. But if you enter an odd number model will fail because we are only analyzing even number, and 3 will be a errorneous value. For more detailed info you can visit link presented by Mr Manoj Tiwari. Hope This help.",302735.0
88648,367719.0,"I will explain you with an Example Suppose you have a problem statement that Increasing your Company Sales to 20% based on the data that you have consists of Last 5-6 years. This is a scenario of Predictive Analytics. Now first you will choose a technique among different statstical Methodologies. First you will check the consistency by hypothesis Testing Then based on Behaviour of data You will build a model to evaluate which areas to be improved,What are the factors affecting your sales(ANOVA),Forecasting the sales, applying regression Etc....and then you will identify the require factors those are influencing your sales. Now you will test that your model by applying part of the data to evaluate results. If this is not satisfactory,then you will do the modelling from start,Suppose If you miss a factor that is more influencing your sales ,you will add this also and build the model again till you get satisfactory results. I hope you understood the what statistical modelling means.",306008.0
88699,367404.0,"Hi Minerva, A data model is a graphical view of data created for analysis and design purposes. For Example, you have designed the Data Model for Order warehouse in details, so the model will have all the tables, their dependencies, and the relationships. Data Modeling, in this case, is designing data warehouse databases in detail. ------------------------------------------------- While in Data analysis, You create a Model by analyzing the Data So that your model works for your data set and evaluation gives the correct results for your analysis. So this Model is like Black-box, which will give desired output based the different dataset you choose. Modeling, in this case, is creating a model with an appropriate algorithm s to predict(Predictive Analysis), and provide answers to business problems.",315028.0
88699,367432.0,"Data Modelling Data modeling is a set of tools and techniques used to understand and analyse how an organisation should collect, update, and store data. It is a critical skill for the business analyst who is involved with discovering, analysing, and specifying changes to how software systems create and maintain information. Data modeling evaluates how an organization manages data. On a typical software project, you might use techniques in data modeling like an ERD ( entity relationship diagram ), to explore the high-level concepts and how those concepts relate together across the organization’s information systems. DATA WAREHOUSE There are three levels of data modeling, conceptual data model , logical data model , and physical data model . DATA MODELLING &amp; DATA ANALYSIS is two different concepts. DATA ANALYSIS Data analysis is a technique to gain insight into an organisation’s data. A data analyst might have the following responsibilities: To create and analyse important reports (possibly using a third-party reporting, data warehousing, or business intelligence system) to help the business make better decisions. To merge data from multiple data sources together, as part of data mining, so it can be analysed and reported on. To run queries on existing data sources to evaluate analytics and analyse trends. Data analysts will have hands-on access to the organisation’s data repositories and use their technical skills to query and manipulate the data. They may also be skilled in statistical analysis, having a high-level of mathemetical experience.",305847.0
88699,367382.0,"Data Model in Data Analysis A big part of data science modeling involves evaluating a model, for example, making sure that it is robust and therefore reliable. Also, data science modeling is closely linked to creating an information rich feature set. Moreover, it entails a variety of other processes that ensure that the data at hand is harnessed as much as possible. Take a example of Data Modeling suppose you want to create a Model in application which can upload upto 5 GB at an spped of 1GB per minute. Therefore to create the above model you need some varibles likes File size,upload time,filenameetc. Therfore to create this model you should otline or remove all the variable such as data in your data set which are having file size greater than 5 GB, 2nd you will you perform modeling with the relative comparision of file size and upload comination. The model when finalize should be robust and perform our task well when evaluated for file size less than equal to 5GB. Data modeling in DS is exploratory data analysis ( EDA ) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. The model in DS we can see it. Data Model of data warehoulse It is more a procress to filter the data from external source and bring the data in datawarehouse using applying the filter data and remove the errornous data. The data model that created in warehouse mostly created for users to see the data from their dashboard. Data modeling in ware house doesnot have any inferential nor it can predict but its more a business and trnsactional data. Hope it helps !! Thanks",307843.0
88699,367641.0,"Data Modelling in DWH: It is the relational model design i.e. facts and dimension relationships. Whereas in the Data Modelling in the Data Analysis is deriving a function which can satisfy the given data (historical) and which can later on predict the future results. These functions can be multi-dimensional. So, in a nut shell Data Modelling in DWH is designing the tables and their relationships whereas in Data Analysis it is to deriving functions.",317689.0
88699,367543.0,"Let me try to give a very simple answer Data modelling in Data Science is like deriving a function y = f(x) from the historical data set made available to you. As discussed in the lecture here x = Input Variables, y = output variable and f = model or function or black box. Input variables are a column variables. Eg. if you have a data set with variables as -- maternal diet, gestation period, age of mother, smoker, alcoholic, drug addict, weight of baby -- and you are asked to create a model so as to predict the weight of the baby after birth then in this data set maternal diet to drug addict will be the i/p variables x1 to x6 and weight of the baby will be o/p variable y. So using necessary algorithms (based on data) suppose your model (or function f or relationship between I/p and o/p) comes out to be as y = a1x1 + a2x2 + a3x3 + a4x4 + a5x5 + a6x6 + c After other steps (like model optimisation, etc) this model or function will be used to predict the baby birth weight for new mothers coming to hospitals. Data modellng for data warehouse is simply how to store the data so that it can extracted easily for generating reports, MIS, Dashboards, etc. (This is called dimentional modelling where a model is designed to read, summarise, analyse numeric information, etc. A dimentional mode l has various elements like facts(measurements/metrics), dimension(who, where, what of a fact), attributes(the characteristics of dimensions), fact table, dimension table) Please do not confuse dimensional model with relational model . Relationation model, in simple terms is for adding, updating and deleting data in real time. Hope i have clarified your query.",318479.0
88699,367738.0,"Data Modelling in Data Ware Housing: Here you will get data from a Source(it would be from a database or Flatfile) and you will perform ETL and Put that in Info Provider(Info Object,DSO,Infocube..Etc) based on the Charecterstics of data.In General here data modelling in the sense Building the Blocks of Data Warehouse. Data Modelling in Data Science: In general you will get the data from Database/Datawarehouse/Bigdata sourceand you will fit your data into particulal Statistical model to obtain Objectives of your Problem Statement.Here you need to create a statistical model which comprises of different statistical techniques(Descriptive,Presecriptive or predictive based on your Objective).",306008.0
88699,369125.0,"Let me explain with a simple example in our day-to day life like Retail. We have store based data like (adidas),which gives the sales of the store ,when this enters a Datawarehouse it enters as a Datamodel,the way Adidas company stores the data. we may get different data models from different countries/regional head stores sharing different levels of data. the above is all about the datamodels in data warehousing. Now ,Data modelling either for Data analysis/Data science Based on our business requirement ,we need to tweek and torture the data Example for data analysis: Based on each below requirement, u need to combine or join then query the data models in data warehouse. which country has higest sales in what category in last season. Which stores promoted more offers to increase the sales which brands/models are sold quickly in few weeeks of the release. Another kind of data modelling is predictive analysis.(data science) we need to observe the data for patterns so we can prepare for the future. Related example: we observe a particular category(eg:ZAP) of clothing is sold very popularly in few countries ,so for the next season adidas will plan to procure the raw material required for the ZAP production some where near which would lower their production costs. Please let me know if any intrested to know more, The example continues for data mining,data science,master data,",301115.0
88703,367366.0,what do the 2 options state? if you could post it as well,317998.0
88703,367374.0,Suppose we have data we can create multiple models for the data based on different parameters Now we have multiple models and we have to validate the models that we developed. we might see a few models are correct rest all are not relevant to the data set. in 3rd option it says about a model gives an incorrect result for some data set and data is likely to be incorrect. the reason of incorrect result can be the incorrect model (relation between parameters) not the data,317845.0
88703,367481.0,"Thanks Rahul and Amani. I think you are talking about he ""Model evaluation and deployment"" quiz. I am refering to the previous one. Which of the following are examples of a model? <p class=""MsoNormal"" style=""margin-bottom:0in;margin-bottom:.0001pt;line-height: normal""> *Doctor Bill measures the size of 500 cancer tumours and concludes that if the size of tumour is less than 4mm, chances of death are less than 20%. *In the investments example, Spark Funds conclude that Big Data Analytics companies have received the most investments last year *Bill tells Steve that the price of house y (in INR) in Mumbai depends on the floor area x only. *All of the above",311857.0
88703,367489.0,"Hi Pradnya, So, basically in the 1st option Doctor Bill is working on some data (500 cancer tumors) and then comes to a conclusion, (if size is less than 4 then chances are 20%) based on the data. Whereas, in the 3rd option Bill is just telling Steve. He has not worked on any data and not reached any conclusion. It is just his assumption (or could be anything..intuition etc) that price depends on floor area. Hence, option 1, in which Dr. Bill follows the correct DM Framework and goes for model evaluation after model creation and then reaches conclusion, should be the answer since we, as data scientists need to draw conclusions based on data. hope that clarifies.",317998.0
88703,367377.0,"Hi Pradnya, Option 1 Says : Model evaluation is done to verify or validate that the model developed is correct and conforms to reality. &gt;&gt;&gt; Model evaluation is a step where you validate and test your model, after you are done with data preparation and So you go on with different sets of samples to test the model. Option 3 : If model evaluation reveals that the model gives incorrect results for certain data, then the data is likely to be incorrect &gt;&gt;&gt; If the Model Evaluation reveals that the results are incorrect for certain data. Then you remodel it so that you are able to analyze why the model is not working for certain data. And remodeling is a part of a cycle with evaluation. Incorrect results don't mean that the data is incorrect. So before coming to evaluation, you need to make sure that the data preparation has been done with no errors and you have non-erroneous data with you. 1. Correct Data (Data Preparation) --- &gt;&gt; Model Creation--- &gt; Evaluate Model with different data ---&gt; Does it work for all samples??? ---&gt; NO? Reevaluate the model --&gt; If Yes ? then continue for Model deployment.",315028.0
88703,367642.0,"In option 1 we are deriving a result based on the sample dataset where as in option 3 we are deriving anything. As it says in the lecture contents Model takes relevant data as input and gives an output you are interested in. So, in option 3 we are not sure whether any modelling has been done. it seems to be a statement based that price of y (in INR) in Mumbai depends on the floor area x only. No modelling and data is being talked about in option 3.",317689.0
89526,371534.0,what are the options? and question? if you could post them as well.,317998.0
89526,371545.0,"Data Modelling Which of the following are examples of a model? 1. Doctor Bill measures the size of 500 cancer tumours and concludes that if the size of tumour is less than 4mm, chances of death are less than 20%. 2. In the investments example, Spark Funds conclude that Big Data Analytics companies have received the most investments last year 3. Bill tells Steve that the price of house y (in INR) in Mumbai depends on the floor area x only. 4. All of the above The correct answer is Option 1. But I can't understand what is the difference between Options 1 and 2 as both of them are conclusions after analysis of the data.",311745.0
89526,371652.0,"Model in ML is created by learning from past experience and model can determine future outcomes from the past experience training. In Option 1, Doctor has past experience of treating cancer tumours and conclution (Model) is extracted from the data. Now using that model he can predict what are the chances of death based on cancer tumour. In Option 2, There is a conclution that compaines has recived most investment. No model to determine what will happen this year or next year. Because Spark fund has no model to predict that.",318368.0
89526,371568.0,"So, basically in the 1st option Doctor Bill is working on some data (500 cancer tumors) and then comes to a conclusion, (if size is less than 4 then chances are 20%) based on the data. Whereas, in the 3rd option Bill is just telling Steve. He has not worked on any data and not reached any conclusion. It is just his assumption (or could be anything..intuition etc) that price depends on floor area. Hence, option 1, in which Dr. Bill follows the correct DM Framework and goes for model evaluation after model creation and then reaches conclusion, should be the answer since we, as data scientists need to draw conclusions based on data. By the same reasoning, option 2 can also be negated. hope that clarifies.",317998.0
88608,366853.0,"I agree to your point, bookmarking capability is needed for us to be able to bookmark any particular topic for quick refernce. I have not come accross any such functionality built in the course material videos but the webpage can be book marked in your browser for easy reference.",308967.0
88608,366849.0,"Yes, you can bookmark the page in the web browser. Go to the module -&gt; session page and 'cntrl+d' now you can see the option to bookmark Note: since it's require username/password to access the resource, you will have to login first and then click on bookmark link in web browser. Happy learning",317845.0
88608,366827.0,There is no such feature available here on this platform as far as i know. You can bookmark this webpage in your browser if need be!,318495.0
88608,366832.0,"I believe there is no such way in our platform to pin/bookmark any topic. You can't even go forward leaving any topic to study later. Until you finish the current topic you cant't go further , Reason is to maintain the streamline flow of study. You can only bookmark the URL in your browser so that you can access it directly to restudy. Hope that helped!! :) Good luck !",308966.0
88608,367067.0,You can only bookmark the URL in your browser. This option is only available as of now. There is no option to save video in bookmark for any particular course video.,320195.0
88608,367065.0,if in chrome you can either use the STAR button next to the URL of the page or use Ctrl+D to bookmark your page / video. Apart from that I guess there is no option available to bookmark / time stamp a particular moment in video.,308962.0
88608,367218.0,I believe you can use navigator instead of bookmarking any topic to refer later,314936.0
88608,367403.0,In the URL tab there is one * (STAR) click on the same and rename according to your convienence,318455.0
88608,367536.0,"The course has a feature which bookmarks(auto-saves itself) at every point. For example, if you were doing module 1 Python and you were in Tuples topic. You just close your browser and log back in next time, on top you'll see where you left off last and a Continue button will be right beside it. However, I have noticed that Continue button doesnt work the way it is expected to work. If you Click on the Continue button, it will take you to the starting point of the course which makes no sense as you left off at ""Tuples"" and it should take you to ""Tuples"". But, for now if you want to keep a track where you left off in your last session, simply log back into the browser and on top you'll see the name of the topic where you were before. Click on the course and then you can use the navigate feature (left side) and jump directly to the exact topic where you left off. Hope, this helps. I have attached a screenshot to explain myself.",307176.0
88720,367445.0,"Semi-Structured Data The structured data which does not conform with the formal structure of data models in the context of relationships is semi-structured data. Examples could be XMl , JSON, some NoSQL databases like MongoDB which store the data natively in JSON. Unstructured Data All the remaining data having no structure at all fall into this category and according to IDC estimate, it represents a whopping 90% in share. Examples could be sensor data (huge in percentage), social media streams, images, videos, mobile data, etc. you can read more at https://www.datamation.com/big-data/semi-structured-data.html",317845.0
88720,367446.0,"Semi-structured data is information that doesn’t reside in a relational database but that does have some organizational properties that make it easier to analyze. With some process you can store them in relation database (it could be very hard for somme kind of semi structured data), but the semi structure exist to ease space, clarity or compute… Examples of semi-structured : CSV but XML and JSON documents are semi structured documents, NoSQL databases are considered as semi structured. Unstructured data represent around 80% of data. It often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents. Note that while these sorts of files may have an internal structure, they are still considered « unstructured » because the data they contain doesn’t fit neatly in a database.",318804.0
88720,367467.0,"Three concepts come with big data : structured, semi structured and unstructured data. Structured Data For geeks and developpers (not the same things ^^) Structured data is very banal. It concerns all data which can be stored in database SQL in table with rows and columns. They have relationnal key and can be easily mapped into pre-designed fields. Today, those datas are the most processed in development and the simpliest way to manage informations. But structured datas represent only 5 to 10% of all informatics datas. So let’s introduce semi structured data. Semi structured data Semi-structured data is information that doesn’t reside in a relational database but that does have some organizational properties that make it easier to analyze. With some process you can store them in relation database (it could be very hard for somme kind of semi structured data), but the semi structure exist to ease space, clarity or compute… Examples of semi-structured : CSV but XML and JSON documents are semi structured documents, NoSQL databases are considered as semi structured. But as Structured data, semi structured data represents a few parts of data (5 to 10%) so the last data type is the strong one : unstructured data. Unstructured data Unstructured data represent around 80% of data. It often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents. Note that while these sorts of files may have an internal structure, they are still considered « unstructured » because the data they contain doesn’t fit neatly in a database. hope this helps. refer https://jeremyronk.wordpress.com/2014/09/01/structured-semi-structured-and-unstructured-data/ for more details.",305847.0
88720,367507.0,"Hi Lalitha, Structured data: any data which can be stored easily in database SQL format like csv ( means every value is separated by "","" or "";"" or "".""), so we can process and analyse it without much effort. We can load this eype of data easily into database using various ETL tools Semi-structured data: In this only certain value or part are in structered format like xml or json, so for this we can't directly load into most common database like MySql easily using ETL tools. There is certain mechanism by using them we need process it and then only we can load into database. Unstructured data: Any data which we can't process using SQL, like images,videos,email data etc . this type of data we generally processing using NoSQL (“not only SQL”) database. I tried to explain in normal text, so we can understand the difference easily &amp; how can we process it.",318319.0
88720,367434.0,"hi, basically in structured data, all data belongs to a certain fixed format. say for eg, in an excel sheet, if all rows contain data in accordance to the header that is provided, then it is structured. you know what data is expected in each column. this is generally found in relational databases. for semi structured data, not the entire data sheet contains the data in one single fixed format. it might contain a portion of rows that conforms to one format where as, another portion that follows a different format. this is found in xml documents. hope this helps. :)",305839.0
88720,367550.0,"What is Structured Data? Structured data is highly organized data that is easy to search and is predictable. The data is usually in a fixed field or predetermined record and can be related to other data records within its structure. An example of structured data would be a relational database or even the table below first_name last_name order_id order_total Kathleen Jo 123456 12.34 John Doe 098765 98.76 What is Semi Structured Data? Semi structured data does not have the same level of organization and predictability of structured data. The data does not reside in fixed fields or records, but does contain elements that can separate the data into various hiearchies. Examples of semi structured data are: JSON (this is the structure that DataAccess uses by default) XML .csv files tab delimited files [ { first_name : ""Kathleen"" , last_name : ""Jo"" , order_id : ""123456"" , order_total : ""12.34"" }, { first_name : ""John"" , last_name : ""Doe"" , order_id : ""098765"" , order_total : ""98.76 } ] Semi structured data, due to its lack of organization, makes the above harder to accomplish, and requires an ETL into a system such as Hadoop before it can be utilized.",317811.0
88720,367621.0,NUTSHELL explaination,306248.0
88720,367728.0,"here I will give you an example Suppose if we observe Telecom industry: Your telecom provider will maintain the database with your Number,Persondetails,Bill details and call details in Data base in Form of Tables.Telecom companies do the surveys in different areas where they want to improve and store Excel sheets,Google Sheets.This type of data will be stored in Row based or Column based data base(Mostly SQL based).Some data will be stored in Data marts or Data Warehouses for analytics Purpose. Suppose If you have any query,then you will contact Customer care and they will record the call(Audio format),They will ask you screen shot of misplaced bill(Image)..etc.These will be mainatined in NoSQL databases.Some data they will store as Pdfs,Images and dashboards from Thirdparties.This won't have any particular sturcture and stored in File system.This will unstructured data.",306008.0
88720,367848.0,"Big Data is a complex sets of Data. Big Data is a collection of huge volumes of Data. Generally BIg Data consists unstructured Data Structured Data Structured data concerns all data which can be stored in database SQL in table with rows and columns. They have relational key and can be easily mapped into pre-designed fields. Structured data is highly organized information that uploads neatly into a relational database Structured data is relatively simple to enter, store, query, and analyze, but it must be strictly defined in terms of field name and type Unstructured Data Unstructured data may have its own internal structure, but does not conform neatly into a spreadsheet or database. Most business interactions, in fact, are unstructured in nature. Today more than 80% of the data generated is unstructured. The fundamental challenge of unstructured data sources is that they are difficult for nontechnical business users and data analysts alike to unbox, understand, and prepare for analytic use. Semi structured Data Semi-structured data is information that doesn’t reside in a relational database but that does have some organizational properties that make it easier to analyze. Examples of semi-structured : CSV but XML and JSON documents are semi structured documents, NoSQL databases are considered as semi structured. Semi - structured data is a form of structured data that does not conform with the formal structure of data models associated with relational databases or other forms of data tables, but nonetheless contains tags or other markers to separate semantic elements and enforce hierarchies of records and fields within the data Source: Types of Data",318495.0
88720,367885.0,"Data in today's world comes in variety of formats - structured semi structured and unstructured structured data is a well formed data set where you can identify the data attributes. example Rdbms data. all the entities are well defined semi structured is similar to structured, but there can be some entries in the data which has additional attributes or less than the previous one. for example log files from server, the structure might change. usually Rdbms are slightly inefficient in handling such data. hence there are techniques like Json or xml to read the data. nosql dbs are efficient in handling semi structured data. unstructured is kind of data without any said pattern example a comment email text or a tweet or a photo file. data format is so random that a structure would be practically impossible to define. https://jeremyronk.wordpress.com/2014/09/01/structured-semi-structured-and-unstructured-data/ hope this helps",300708.0
88720,368689.0,"To understand it in a simple way i will give some examples Unstructured data - All the website, html files, text files, audios, videos and documents etc which does not have any fixed structure .Note that while these sorts of files may have an internal structure, they are still considered ""unstructured"" because the data they contain doesn't fit neatly in a database. Semistructured data - Semi-structured data might include XML documents ,JSON format, NoSQL databases. Which does have some structure but does not strictly follow it unlike relational data model which have very stingent rules which makes it's data a more structured as compared to semi-structured data.",320685.0
88720,368824.0,"Hey Lalitha, Hope you are doing great and progressing well with the course? Let's take a simple example to understand what is a unstructured data and what is a semi structured data. Let's say you have lots of images stored on your local drive or pen drive or external hard drive. If you want to write a software to analyze these images, which is usually the case when you are analyzing (let's say) data from variety of security cameras for example as part of a Big data analytics project to defect fraud or to identify criminals, the software you write needs to analyze each and every image differently. This is because all the properties of one image is different from the other one. This is a clear case of unstructured data, where you cannot put a schema on top of it. In paralllel to above, let's say, your are also storing a lot of data about the people living in your housing society by means of a survey. The survey is electronic, and record various details of the residents. However, not all fields in the survey form are mandatory fields. There would be users who fill many details, while there would be users, who will only provide least information to maintain privacy. This is called semi structured data, as (if you try and visualize this data as being stored in a table) each row of the data has different set of columns, but can at max have a definitive list of columns only, which helps, and prevents this data from becoming completely unstructured data. Hope this helps. BR, Suddhasatwa",310217.0
88894,368452.0,"Hello Rajani, We Will use querying(SQL), data visualization(tableau) , reporting techniques(Excel,tableau) R &amp; Python language. Below is the list of top 10 of data analytics tools, both open source and paid version, based on their popularity, learning and performance. R Programming. Tableau Public: 3.Python. SAS: Apache Spark. Excel. RapidMiner: KNIME. Some other tools also available in market for data exploring: https://www.softwaretestinghelp.com/data-mining-tools/",320195.0
88894,368456.0,There are plenty of tools for exploring data. I found the below article useful to get information about various tools in the market. https://www.analyticsvidhya.com/blog/2018/05/19-data-science-tools-for-people-dont-understand-coding/ And you can check below link for the most used data science tools http://businessoverbroadway.com/most-used-data-science-tools-and-technologies-in-2017-and-what-to-expect-for-2018,317991.0
88894,368794.0,"I saw people using, mostly ggplot2, matplotlib,seaborn, and Tableau for EDA purpose. In seaborn we use multiple graphs like, kde, violin, boxplots, etc.",304813.0
88894,368702.0,"There are various tools and different region of world use different tools : Like in USA "" R "" is more prominent than others and account for almost 75% while in USSR "" Excel "" is used more and account for 43% market share while SAS is another big competitor in this field which account for more than 30% market share so different tools are used for EDA based upon region and different other factors while on real time projects R and SAS are the leading EDA tools .",320685.0
89143,,nan,
89208,369893.0,"Tools, I can say R, Python, Spark etc. And there are lot of techniques available, like outlier detection, missing value calculation (for numerical by average value, for categorical by most frequent value) if too many missing values in a record, ignore that record, if too many missing values for a feature(column) then checking importance of that column if not too much important then drop that. Dedup or duplicate elimination also another technique. In statistics there are lot of theories available, if someone from statistics backgroud they might put more light.",318554.0
89208,369923.0,"Yes, that's a very interesting question. Most people use Microsoft Excel and Tableau If you are from programming background you will have always an edge of programming knowledge like R, Python, Scala and SQL Lots of tools are available in the market and it totally depends on your need. You can read more at http://bigdata-madesimple.com/top-30-big-data-tools-data-analysis/",317845.0
89208,370847.0,We can analyse any data set by using below tools and language: 1. Python 2. Numpy library 3. Pandas library 4. matplotlib library 5. seaborn library 6. scikit learn library 7. jyputer notebook tool.,304397.0
89206,369882.0,"just highlight the imp points like side heading and make a note as u remember them, and at last, you can find the printed notes",304692.0
89206,369936.0,"I would say the following points: 1. write down the key concept and usages. 2. Underline the important concept and the points that took more time to understand because you will likely to miss/forget those pints after some time 3. Write the limitaion , you can say pros and cons of the concept 4. The snippet of code with the concept so you don't have to search all time on the internet 5. If you have checked any online content bookmark the link so you don't have to search all the time. just visit the link 6. Write the common concept and map them with related points. For example, Excel and tableau have charts, write down the points from each and mark with mapping so you can easily recall them. 7. Revise the concepts periodically Most important Practise, Practise and Practise.",317845.0
89651,372157.0,Outlier values are the ones that lie outside the range for which you have created a chart in tableau and they can be a symbol of defects in your chart,318358.0
89651,372178.0,An outlier is an observation which lies at an abnormal distance from other values. For example below is the data for annual collections by employee Employee Collection A 750 B 780 C 800 D 760 E 10000 Not if you look at the data all the employee has collected amont between 750 to 800 but employee E is an outlier as he has earned 10000. Spotting of detecting those outlier is called splotting outlier.,318368.0
89651,372224.0,"In statistics, an outlier is a data point that differs greatly from other values in a data set. Outliers are important to keep in mind when looking at pools of data because they can sometimes affect how the data is perceived on the whole. If we take salary of 10 people as follows 40000,50000,45000,60000,65000,70000,15000,48000,120000,62000 If we calculate average salary of this dataset it comes 57500. But there are two values 15000 and 120000 which are at significant distance from this average value. And if we remove these two values from calculation average comes as 55000 which shows the real picture of dataset. So outliers plays important role to distract the analysis and most of the times while analysing data we skip those outliers to analyze the data.",317991.0
89651,372333.0,"Adding to the other comments, although the outliers could pose a lot of problems in the analysis we should not remove the outlier values directly and instead try to understand as to why such a highly varying observation got recorded. As such, the outlier(s) could actually be indicative of a new trend (trend spotting) or could help us discover inaccuracies in data capturing etc.",313826.0
89833,373464.0,"Hello Nithya, You can not submit same question again after reaching max(2 or1) attempt limit. You can click on continue to move further in next session or module.",320195.0
89833,373652.0,"Hi, Just click on continue to proceed to next session.",307496.0
89367,370763.0,Really nice topic and thanks for sharing. This shows the benefits of having a diverse team here in Upgrad. I want to point out that CRISP-DM is already well defined and adopted by diffrenet organization and most popular. As per the kdnuggests article 4 problems arise due to the shortcut people adopt and ended up with corrupted version of CRISP-DM. WHere all 4 probles mainly arising due to lack of business and data understanding. And in corrupted CRISP-DM this essential phase is bypassed during the iteration. Also IT(who understands the big data technology) is coming late into the picture in this corrupted framework But original CRISP-DM clearly says these should be part of the iterative process.,318554.0
91010,379839.0,"If your file is in the same path as the Jupyter notebook, u need not give the complete path. Only file name will do.",304319.0
91010,379840.0,"For the Python Assignment, you can just specify the file name and no need to give the path. The path will be edited by the graders if need be.",313826.0
91010,380124.0,"we can not hardcode the path. Just give the file name because we don't know where they will keep the csv and jupyter file. I hope they will keep both files in the same directory, so jupyter file will pick the file with just file name not with absolute file path.",317845.0
91010,380173.0,Just Mention the file name it will read condition is .it must be on same path as jupyter working like same drive of your system,319869.0
90914,379008.0,What was the error? from functools import reduce worked for me.,310974.0
90914,379028.0,from functools import reduce works without any problem. you can refer the link https://docs.python.org/3/library/functools.html if you can copy and paste your sample code it will help to understand the exact problem.,317845.0
90914,379294.0,"Hi, The import statement ' from functools import reduce ' works fine for me too. It would be great if you can share the exact error you are getting. Also please verify if you are not using reduce as a variable/local function name in your code. In such case use: from functools import reduce as functools_reduce and use functools_reduce function in your code.",317987.0
90914,379428.0,"As you are trying to use functools.reduce you can't import reduce function only. You will need to import complete functools library. You an import reduct function only and while using instead of functools.reduce, You can use reduce.",318368.0
90914,380015.0,"Kapil, I am also getting the similar error as Vikash got. Error message: Traceback (most recent call last): File ""/code/source.py3"", line 10, in &lt;module&gt; print(functools.reduce(lambda x,y:x*y,a)) NameError: name 'functools' is not defined",303227.0
90914,380033.0,"Please confirm if functools is installed on your system, if not you can install functools using ""conda install -c conda-forge functools32"". I assume you are using Anaconda if using pip then you can install using ""pip install functools""",300734.0
90914,380690.0,Please upvote and select my answer as the solution. Thanks!,300734.0
90914,379329.0,"Hi here is my code n = int(input()) from functools import reduce if n==0: print(1) else: a= range(1,n+1) print(functools.reduce(lambda x,y:x*y, a)) I am getting error as Traceback (most recent call last): File ""/code/source.py3"", line 11, in &lt;module&gt; print(functools.reduce(lambda x,y:x*y, a)) NameError: name 'functools' is not defined",320257.0
90156,375556.0,"In both the cases, it will throw error i believe: TypeError: 'bool' object is not iterable",310974.0
90156,375559.0,"This are binary operators. There are two use cases. 1) SET In context of set these are union and intersect operations. A = {0, 2, 4, 6, 8}; B = {1, 2, 3, 4, 5}; print(""Union :"", A | B) print(""Intersection :"", A &amp; B) Output : ('Union :', set([0, 1, 2, 3, 4, 5, 6, 8])) ('Intersection :', set([2, 4])) 2) Number &amp; It performs bit by bit AND operation on the two values. Here, binary for 2 is 10, and that for 3 is 11. &amp;-ing them results in 10, which is binary for 2. Similarly, &amp;-ing 011(3) and 100(4) results in 000(0). &gt;&gt;&gt; 2&amp;3 2 &gt;&gt;&gt; 3&amp;4 0 | It performs bit by bit OR on the two values. Here, OR-ing 10(2) and 11(3) results in 11(3). &gt;&gt;&gt; 2|3 3",318368.0
90156,375579.0,"The mentioned operations will work only if the variables a, b and c are of set type. For other types of objects ut will throw error. For set type objects, | is equivalent to union and &amp; is equivalent to intersection .",313826.0
90156,383176.0,"Hi Joy, The above two operators are called ""Set Operators"". Whenever you need to find the common elements(basically the intersection) or all of the elements(union of the sets) between given lists,you use these operators. ""|"" - used for UNION ""&amp;"" - used for INTERSECTION EX. L = input_list[0] ---&gt;&gt; L_set = set(input_list[0]) M = input_list[1] ---&gt;&gt; M_set = set(input_list[1]) With L_set,M_set you can perform different sorts of Set operations. P.S. Whenever you want to use set operators, you have to convert the given lists to sets to perform the operations.",308673.0
90156,375641.0,These operations depend on the data type: in case of boolean data: '|' is equal to OR opeartor '&amp;' is equal to AND opeartor in case of number: Its binary AND and OR operator in case of set data: '|' is equal to Union and '&amp;' is equal to intersection You can read more information here https://www.programiz.com/python-programming/set https://www.tutorialspoint.com/python/bitwise_operators_example.htm,317845.0
88764,367712.0,What do your expect the program to output?,318084.0
88764,367722.0,"Are you trying to check if a string is palindrome, that is if the orginal string and its reverse are equal ? for example the string ""madam"". If you are checking for palindrome, change str1 = "" "" to str1 = """" . There should be no space between the quotes. Then it will work.",313691.0
88764,367730.0,"Can you try this without the sapces ie str1 = """" s = input() s1 = s.lower() def rev(s): str1="""" for i in s1 : str1 = i + str1 return(str1) s3 = rev(s1) print(s1) print(s3) if s1 == s3: print('1') else: print('0')",317460.0
88764,367739.0,I have tried and its working.,313691.0
88764,367824.0,You can try this short code(use of splicing): s=input() print(s) print(s[::-1]) if s==s[::-1]: print('yes') else: print('no') Hope this helps :),318495.0
88764,367743.0,"you should remove the space predefined in your str1 variable. also, take a look at the following way of writing the same function. it uses slicing of an expression in an elegant way and is much faster than the way you are currently doing in. def rev(s): return s[::-1] if(a== rev(b)): print('yes') else: print('no') where a and b are the two strings you want to compare. do note the way the rev function is written. and play around with slicing expression, you will definitely find it interesting. please upvote if you like. thanks.",317998.0
88764,369348.0,"Hey Deval, please understand that it is a graded question and it is unethical to ask for answers regarding this on the discussion forum.",319721.0
90851,378664.0,you can create a set test case would also do the same,318017.0
90851,379192.0,Thank you !,300699.0
90851,378703.0,Creating fix student set is not hardcoding as long as student number is not changing. You can go ahead and create set of all student.,318368.0
90851,378658.0,yes. you have to create a set of all the students for subsequent operations. the question statement mentions student set for 20 students.,311686.0
88727,,nan,
88693,367288.0,Submission will have additional test cases which are run to see if all the possible inputs are being handled by your code correctly or not. You will see those test cases only after the submission is done. This is to avoid any hardcoding.,310974.0
88693,367932.0,"when you submit, it verifies if your coding satisfies some more unknown/hidden test cases.",319912.0
88693,367935.0,submission has hidden test cases which u r code might not get passed your code must be effective and it should work for any input,306737.0
88693,367291.0,"Run Code - If your code has no syntactical errors it will run successfully. Even with logical errors, program will still run. For example, if the Question is print all elements in an input list , and you run, &gt;&gt;&gt; print(""I don't want to print all elements in a list. What'll you do about it? ;) "") . This will run succesfully, provided you don't have any syntax errors in the program. Verify Code - A few test cases are provided in the coding console. And against every test case a particular answer is pre-recorded. When you run your program, your output must match to the answer that is provided in the conslole. Submission - In this case the coding console will have some different, more difficult test cases, that you might not have thought of. For exmaple, let's say you had to write a program to calculate the sum of two numbers and print it as an integer . And in the test case of Verify code, they only provided you with integers, so your code worked. But in the test case of Submission, they provided you with a decimal value i.e. float. Now the program will throw an error because, let's say, initially, you did not write code to convert the sum to an integer. .... So, you will want to explore all the possibilities. What can all the different test cases be? This is why we have been provided with the Run Code. Verify Code is like a second check, whether the program just written works for another person's test case or not.",301652.0
88693,367316.0,Depends on test cases.,317269.0
88693,367342.0,It may have failed in final submission for different set of arguements.It generally not happened. If it has appeared recheck and validate,319869.0
88693,367347.0,"When you verify and run code, it just tests the code against the mentioned input in the console. It might run successfully for that particular input. But when you submit the code, your code is validated against different kind of inputs. Not just the one mentioned in the INPUT section of console. The code to be accepted, has to pass all the sample test cases.",307488.0
88693,367435.0,"Problem Stub - A problem stub (also known as ‘language stub’) is the skeleton code which the teacher has provided for a particular language; you have to solve the coding problem by adding your code to the given skeleton code. Execution Time Limit - Time-taken for the code to execute from start-to-finish for the given input on the server. Note: This time is independent of your internet speed. In case, your code exceeds the time limit for a particular input, you will receive an error message such as - “Time limit exceeded. The execution took more than 5.00s.” Load Language Stub - This is the action which you can find in the overflow menu, on clicking this you will be able to go back to the problem stub. Note: Your current solution code will be replaced by the problem stub. This action will help you to clear your current solution code with the problem stub or the skeleton code. Demystifying Test Cases Test cases are a tool to evaluate your code. A test case determines if the solution code (code written by you) works for a particular scenario or not. In most questions, your code is checked against multiple test cases to check if it meets the requirements specified by the teacher or not. A test case has two major components - an input, i.e. the scenario which your code is being tested for, and an expected output, i.e. the expected results for that particular input. If your code is returning the same output as the expected output given a specific input, then the test case is deemed as “Passed”. Otherwise, it is deemed as “Failed.” Let us consider a problem we saw in the above video, an example of a test case for this problem would be something like this: Input : 2 87 98 87 89 67 56 Expected Output: 2 4 In this example, the test case includes the input of two arrays with lengths 2 and 4 respectively. If the output of your code for these test cases matches the expected output which is 2 and 4, then your code will pass the test case. HOPE THIS HELPS. you can refer this from upgrade site too. https://learn.upgrad.com/v/course/208/session/15859/segment/80224 thanks",305847.0
88693,367640.0,Submission has additional test cases which needs to be passed by the code,306248.0
88693,367964.0,I think it verifies against the sample input that the console has provided and the testcases surrounding it.,314221.0
88693,367872.0,"Basically, Run code checks your code syntactically, not functionally. Verify and Submit checks your code syntactically based on input test cases. Now, the coding questions on the platform have two kinds of test-cases, i.e. sample and non-sample test-cases. The sample test-cases are the ones which are evaluated when you click on 'Verify' and the non-sample test-cases are the ones which are hidden and are evaluated when you click the 'Submit' button. So its possible that your code is syntactically correct and passes the sample test cases, but is failing the non-sample test cases. Hope it is clear now.",310511.0
88693,367967.0,"Verify and Run Code will help you eliminate any syntax error and pass the result for sample test cases. After you submit the code, then corner cases scenarios would be tested to make sure the code is full proof.",304814.0
88693,368499.0,"it means code is working only for few of the test cases , submit checks additional test cases which were not tested during verification",314936.0
88693,368501.0,I identified one more thing after verfying i submitted and there was remark like partially accepted ..what is this either code is correct or not correct like binary system,319869.0
88693,368418.0,i think we should check our code first in juptier and then verify it.,320606.0
88693,368817.0,"Hello Vikas, Hope you are doing great! there would always be hidden test cases elaborated enough to check your submission, which runs in the background and other than your logical representation of the question, it would also check for time taken, memory used, if a particulat function was made use of or not, etc. For this reason, I would suggest to have Anaconda Python distro isntalled on your laptop and practice/solve the questions locally, post which you can put them into the submission window once you have tested them thorughly in local system. As far as I have seen (until now) there seems to be no time limit for hte programming assignments to be submitted in the portal. Hope this helps! BR, Suddhasatwa",310217.0
88693,367305.0,"Tips on how the coding console works : Run Code : On clicking this button, whatever code that has been written in the coding consle gets executed. Here, the only thing that is checked is whether the syntax of the code written is correct or not. No checks ae done on the logic of the code itself. This accepts the input from the ""Input"" tab of the coding console. Verify : On clicking this button, the logic of the code is also tested. If the ""Solution Output"" matches the ""Expected Output for the given problem, then you get a message to indicate successful execution else it mentions a failure. This also accepts the input from the ""Input"" tab of the coding console. You can change the input provided by changing the contents on the ""Input"" tab to cater for various test cases. Submit : On clicking this button, the code is tested against the ""Non-Sample"" test cases. Meaning, some other test cases which are not known upfront to you when you are writing the code. This is done to check if the code is generic enough to handle any type of input to the given problem statement. The non-sample test cases and the ideal solution for the given problem are displayed after : either you have successfully submitted a correct answer or you have exhausted the stipulated number of submissions for the given problem. Take a look at the content on this link tp get a clear understanding of the coding console https://learn.upgrad.com/v/course/208/session/15859/segment/80224",313826.0
89116,369455.0,"Here the problem is in parameter passed in slice() function. Since it is very basic of python, I would suggest you try it at your own by googling it. I can share you the website from where you can take reference. https://www.w3schools.com/python/ref_func_slice.asp",317991.0
89116,369467.0,"Slice function doesn't accept colon, please use comma instead of colon. syntax : slice(start,stop,increment) If you specify only 1 argument then it consider start and increment as None. For example,",318328.0
89116,369530.0,Ok. i got answer while practcing Python code - L[2:] -will return from index 2 . :-) Thanks,320008.0
89116,369457.0,"Here the problem is in parameter passed in slice() function. Since it is very basic of python, I would suggest you try it at your own by googling it. I can share you the website from where you can take reference. https://www.w3schools.com/python/ref_func_slice.asp",317991.0
88853,368253.0,"1. you may not need user defined function for factorial program. You are using it that is okay. 2. what is meaning of ""Initially it accepted and afterward it rejected.Execute code properly.""? Do you mean you used Verify/Run code or Submit button?",318458.0
88853,368271.0,"Please share your code, so that it is easier to debug.",317998.0
88853,368285.0,The question mentioned it has to be written using reduce function. Write it using reduce function instead of writing a recursive UDF.,317689.0
88853,368288.0,To help you better please share screenshot of your code.,317991.0
88853,368311.0,"The question specifically mentions to use the reduce() function. Please revisit the pre-prepratory tutorial on map,filter anad reduce in case of any doubts on how to use the reduce() function. Link for preprepratory tutorial: https://learn.upgrad.com/v/course/208/session/15861/segment/80240",313826.0
88853,368579.0,Hi My question is not answer properly.,320689.0
88853,368968.0,"If your code is right and even then it is not accepting Then the reason could be that the coding section is checked by automated result which works in background and so first check whether your output variable name is same as given in question otherwise they must have something to check whether you have used the method( here reduce function) or keywords as mentioned in the question that's why it is getting accepted first and then gets rejected as your method is different and ask for using reduce function.question Also in the question it is written "" PS:Finding the factorial without using the reduce() function might lead to deduction of marks. "" So , they must have implemented some test to check whether you have used reduce() for finding factorials or not. Hope you got your answer.",320685.0
88853,369430.0,You have to use reduce() to build factorial program. Also there are 2 test cases that needs to be cleared. If your code is correct it will give correct output for both the test cases. Do make sure to take care of outlilers as metioned in code that Factorial(0) is 1. Best of Luck!!!,308439.0
88853,370075.0,"The coding console enables you to do the following things, 1. Check your code for execution. You can do this with the help of 'run' option. 2. Verify the code by yourself with the inputs given. Remember that the inputs are predetermined. 3. Submit your code. When you submit your code, there are test inputs that your code will be tested against. If your code doesn't pass these, that means there's some mistake in your code. And you need to change your code to be right, for it to get successfully submitted. https://learn.upgrad.com/v/course/208/session/15859/segment/80224 Go through the module above, there are details about the coding console.",319721.0
90624,377993.0,"You need to use the original dataframe only. But just make sure that in subtask 3.3, you have dropped the duplicate values inplace so that the changes are reflected in your original dataframe. :)",306040.0
89441,371098.0,"I believe, for the graded questions the check is based on the Solution Output macthing the Expected Output.",313826.0
89441,371105.0,I guess you would have hard-coded the values in your script. Your code should be generic so that it can pass all the test cases.,314547.0
89458,371261.0,The solution looks correct to me. Please share the error you are getting.,309451.0
89458,371183.0,"Please check your syntax once ? Also, Check if you can perform Set Operators on Lists ? If not, how to convert the list to Sets ? Later, apply the Set Operators on those sets.. Thanks.",311502.0
89439,371108.0,Implementing set operators directly on two or more lists is not supported. The List should be converted to a Set like set([]). Source - https://realpython.com/python-sets/,311502.0
89439,371111.0,"Please check the code below, you can see that the logical operators can be operated on the the elements of the list, but not the list. A small example here. a = [1,2,3] b = [1,2,3] c = a[1]&amp;b[2] print(c) d = a&amp;b 2 --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-37-de47248dbcca&gt; in &lt;module&gt;() 3 c = a[1]&amp;b[2] 4 print(c) ----&gt; 5 d = a&amp;b TypeError: unsupported operand type(s) for &amp;: 'list' and 'list'",319721.0
89439,371220.0,You can use set operators on list directly. Please convert your lists to sets and than you can use set operations on resulted set. You can convert list to set by below syntax. your_set = set(your_list_here),318368.0
88783,367809.0,"you can also use both positive and negative indexing in Python, with the last index being ‘-1’. suppose you have string "" I am DS student"" you want to retrieve DS then in Positive index (I am DS) its length as including is 123456 and DS starts from 5 and ends at 6 so you can use [5,6] In Negative Indexing ( (I am DS) you can read as -6 -5 -4 -3 -2 -1 so you can use [-2,-1] as DS starts from -2 and ends at -1. similary you can solve for I love python. Hope this helps.",307843.0
88783,367814.0,"Hi, If you are splitting a string and then needs to join that then use the below syntax: Consider s is the string you want to join then pass: any_variable = """" w = any_variable.join(s) print(w) Hope it resolves your query.....",305656.0
88783,367816.0,""" "".join(list) Use space with join and you might get the desired output as you wish! Hope this helps :)",318495.0
88783,367826.0,"I think you need to put the each words with double quote since double quote itself used for joing. However python has provided a solution for that as given below. subject = ['I', 'Love', 'Pyton', 'and', 'all'] p=', '.join('""{}""'.format(s) for s in subject) print(p) Logic p=', '.join('""{}""'.format(s) for s in subject) Here p is joining each word with double quote using '""{}""'.format(s) here s is first word since there is for loop so next time s is increased by 1 as it will loop all the words in subject list Output : ""I"", ""Love"", ""Pyton"", ""and"", ""all"" Hope it helps.",307843.0
88783,367853.0,"First, you have to split the single string to the list using space "" "" var=""You love Python!"" list=var.split("" "") print(list) O/P ['You', 'love', 'Python!'] Since you have the list now you can join them using join function, you need space between each word so you have to use space to join var=""You love Python!"" list=var.split("" "") print(list) var1="" "".join(list) print(var1) Happy learning :)",317845.0
88783,368966.0,"Use """".join(&lt;list_name&gt;) The double quotes can contain anything like space for space between the elements , no space to concatenate elements without space and any other character which you want to put in between concatenation of elements.",320685.0
88783,367801.0,"You need to join with a space and not an empty string. try "" "".join(my_list) instead of """".join(my_list) ie, you need to put a space in between the double quotation marks. To elaborate more, the join function joins the list or string using the character inside the double quotes. for example, if you have list1=['1','2','3'] and you do s = ""-"".join(list1) print(s) then your output would be 1-2-3 Hope that clarifies.",317998.0
88783,369440.0,"Hey, Deval This is a graded question and you're not supposed to ask solutions for it directly in the discussion forum. That's un ethical.",319721.0
89725,372775.0,"Hello Priya, If your second attempt is still pending then go for it otherwise you can not go for submission as you have reached max. limit. if it is showing rejected then they will not consider your first code..",320195.0
89725,372671.0,"What's the status of the submission? If it's Rejected, then no marks will be awarded.",318329.0
89725,372778.0,"Sad. It lesson for each of us. Submit button just comes very close to the scroll bar, need to be very cautious.",304813.0
89725,373217.0,"If your first submission was correct then you'll be awarded full marks for it. From both the submissions, it picks up the max. marks scored. So don't worry. :)",306040.0
89725,376572.0,How you have handled 0. I am not getting it. I have calculated factorial.,310502.0
89497,371445.0,"When you do a verify of the code, it will test the code over one scenario of input values but when you do a submit of the code it executes the code over multiple scenarios of input values. So check if your code satisfies all the possibilities.",318804.0
89497,371336.0,"There are two sets of test cases. 1) Which gets verified on the client side when you click on verify. 2) Other which gets verified when you submit. In your case, verification level test cases are getting verified. Possible Solutions: 1) Look for any kind of hardcoded values in your code. That will solved your testcase1 problem but will fail on other test cases. 2) After verifying try few more input from the ""Input"" tab and verify you code against those inputs also to make sure your code will work with all test cases possible.",318368.0
89497,371499.0,"Hi. The following happens when you Run, Verify and Submit code: Run - will fail if there are any runtime errors Verify - will fail if your code output doesnt match with the sample test cases (which you can see). Submit - will fail even though run and verify has passed if your code output doesnt match with some non-sample(hidden) test cases. So your code needs to be generic enough to work for any output. Hope this helps.",310511.0
89497,372338.0,do I have to undo the the code to make it generic,312892.0
89497,372520.0,Do not use hardcoded inputs. Rather use variables to accept inputs and then use them in your code.,310511.0
88792,367874.0,3. Students who play exactly two of the sports should be all students playing 2 sports - students playing all 3 sports. print(sorted(((setC&amp;setF)|(setC&amp;setH)|(setH&amp;setF))-(setC&amp;setF&amp;setH))) Hope this helps.,310511.0
88792,367877.0,"In the 3rd one, you missed this case : (H_S&amp;F_S) - C_S So, it should have been print(sorted(list((C_S &amp; H_S - F_S) | (C_S &amp; F_S - H_S) | (H_S&amp;F_S - C_S)))) Hope it helps.",317998.0
88792,368029.0,Refer the explanation in the YouTube link that you will get after your submission. Don't write your code here as it is a Graded question.,314547.0
88792,367922.0,"I won't post exact code as it is a graded question, but I will explain how we can write code to calculate no. of students who play exactly 2 sports. method1 : calculate students who play 2 sports and make one set and then remove those students from it who play all three sports. method2 : calculate separately who play only 2 sports and remove student if any students are there in result set who play third sport and add all 3 separated result. eg. calculate student who play cricket and football and from resulted set remove those student who play hockey also along with cricket and football. Like that you can calculate other 2 games cricket and hockey &amp; football and hockey . I hope you understand.",318319.0
88809,,nan,
88835,368185.0,Use the logic of a&gt;b and swapping while traversing through list.. and I think you have submitted question in wrong category!,318495.0
88835,368198.0,"Since it is a graded question, i wont be writing the code. But share the approach. You can achieve the desired result by the following steps: 1. Traverse the input list once, and count the number of 0s, 1s, and 2s in it. Keep track of those counts in a variable. Say, n1 ,n2, n3 keeps those counts respectively. 2. Traverse the list again and fill the first n1 places with 0, next n2 places with 1 and finally the next n3 places with 2. Through the above the approach we are not using inbuilt sort function, and the complexity would be O(n). Hope that helps.",317998.0
88834,368146.0,"Looks like you are trying to merge 2 lists and generate a sorted list. If that is the intension, then you could use in built-in methods and functions to achieve the same. Try the below code: list1.extend(list2) sorted_list = sorted(list1) print(sorted_list) PS : Please mark the question to the right ""Topic"". I believe these question are from the additional optional module mentionecd at the end of the python graded assignment.",313826.0
88834,368154.0,Problem is there in the below code: while p != x and q != y: Here you are checking whether you have traversed the whole list or not. If you have not traversed whole list if-else condition runs. But at the last iteration of while loop where the code checks that 7 &lt; 8 or not it will return true and the value of 7 is appended to the newly created list and the value of p=4 and q=3 . Now when next iteration of while loop runs (p != x) i.e(4!=4) return false and execution terminates there itself without checking the condition (q != y) and the last element 8 is not traversed. Since when p=4 reached it means element of list1 are traversed and no more element are left in it . You can use one more while loop to append the remaining element of list2. Hope this will help.,317991.0
88834,368721.0,"If you just want to merge the lists and sort, you can try it using the set operator UNION ( | operator) easily. please see the below piece of code list1=[1, 3, 5, 7] list2=[2, 4, 6, 8] merged_list=print(sorted(set(list1) | set(list2))) Output: [1, 2, 3, 4, 5, 6, 7, 8] Correct me If I misunderstood your question. Thank you.",310481.0
88837,368504.0,"Archit, I believe you are correct. The equivalent comprehension would be a list comprehension because the output is a list. If in case the loop had an output which was a dictionary, the equivalent would be a dictionary comprehension.",318397.0
88837,368176.0,"Comprehension will return a single value. List comprehension will return a list of values. I can give an example which will make it more clearer.. a for i in range(0,3) This(comprehension) will give final output as 2.. because values 0 and 1 are not saved anywhere.. But enclosing it in square brackets (list comprehension) will give [0,1,2]",318495.0
88837,368199.0,"Comprehension comes in handy when creating a new built-in data structure like a list, a dictionary or a set from an existing data structure with conditions. Technically Comprehension is the parent of -List comprehension -Dictionary comprehension -Set comprehension Each of the above evaluates comprehension iteratively on each item within the original dataset, once it's conditionally satisfied then each item can be transformed/added to the new dataset I hope that makes sense",317845.0
88837,368098.0,"In the question, they have provided a sample code which implements some logic using nested for loops. They have asked to implement the same logic using list comprenhension. So, you need to convert the nested-for-loop to a list comprenhension such that the end result us the same. Hope this answers your query.",313826.0
89445,371241.0,Completely agree,308437.0
89445,371345.0,i am also agree with you,318461.0
89445,373285.0,"Hello Sushmitha, I completely got your points and agree with you. We can not discuss any graded questions and code in discussion forum. I have one query here. If we will ask some questions for below module. It is coming under the ""Python Graded Questions"". I dont think so below are graded questions. Please confirm. https://learn.upgrad.com/v/course/208/question/89219/answer/369940/comment/93872 Intro to Data Management &gt;&gt;Data Structures and Algorithms in Python",320195.0
89445,380631.0,I don't think so this is graded question. Please confimr should we discussed this module..,308639.0
89445,381278.0,Can we discuss about the variation in codes once the deadline is over ?,306241.0
89749,372803.0,"hey Umesh, unfortunately i cannot see your code but still a quick check; make sure that you're not hardcoding anything. Also, see if your code is handling all the possible inputs..",316349.0
89749,372950.0,"If in this question you are using union, intersection and difference of different sets please make sure not to use hardcording and use below symbols or functions to solve set operations union - A | B or A.union(B) intersection - A &amp; B or A.intersection(B) difference - A - B or A.difference(B)",318358.0
88869,368370.0,"That is because the test case is just one of the cases and is just a TEST case. After submition it tests for the validity of the code by checking other cases as well. it is rejected because it fails to pass those other test cases. Or, if you are talking about before submition, then there could be more than one test case , and it could be that one has passed and the other has not. so, it shows rejected for one case and passed for other case. hope that clarifies further, you can calculate factorial using reduce like this: reduce(lambda a,b : a*b, list1)",317998.0
88869,368376.0,"Because when you verify your code, the system verifies with only 1 or 2 test cases. But when you submit your code, the system verifies it for many test cases. Your answer will only be accepted if your code produces correct result for all test cases. It could be that your code didn't produce the expected output for one of the test cases when you submitted your code. Hence, it was rejected.",308962.0
88869,368711.0,"Hi Atulyan, When you submit the code it checks more test cases than the one in Verify button. In most of the questions, there are special notes mentioned hence you might need to check your code with various combination of inputs before submitting it. And finally it is always better to write a generic code which can give results for any kind of input.",310481.0
88869,369034.0,Thank you everyone for your help... I have succesfully submitted the code,312892.0
88869,369280.0,"You test case are passing only for one particular scenario, i.e. Sample Test Case. If you want to your code to be Accepted, then all the test cases are supposed to be passed. For factorial of the number, maybe you are missing the key condition. i.e. factorial of 0. Give a condition in your code to catch this particular condition.",318780.0
88869,369755.0,"If all the test cases are passing then only it will accept the code. one of them test case will not match, it will reject. If all the test case are passed then it will partially correct if you are not consider the following condition ""Make sure you handle the edge case of zero. As you know, 0! = 1"" .I was facing the problem patially correct now i got the answer.",314183.0
88869,375293.0,"when I wrote the factorial code question ,in the last print row it is showing invalid syntax . what does it means . if entire code write up is okay then the last line should be also ok . not getting ans ... what shld I do now",319969.0
88869,376128.0,can you share the screenshot of the error message,312892.0
90239,375949.0,"Online IDE has three tabsc: ode, input, output import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) Above code read the data from the input tab. You will have one sample input rest all will be hidden. While submitting the code, your code will run against all the hidden test case.",317845.0
90239,375974.0,"From the FAQs: import ast,sys input_str = sys.stdin.read() is used to read the input from the standard input/output stream. It will be present in almost all the coding questions that are present because when you're writing code in any console, you need to read the input from and write the output to the standard I/O stream. Also, note that the variable name in the second line is 'input_str'. This is because all the inputs provided to the standard I/O stream are read as a string and you need to convert them as required. This will also be done for all the codes. For example, the code in this page requires you to read a list. Hence, the following code is used to convert the read input from string to list: input_list = ast.literal_eval(input_str)",309451.0
90107,375197.0,Try a different browser. Preferably firefox.,310974.0
90107,375211.0,after reloading ...now showing the screen ... Thanks ..,319969.0
90107,375259.0,"Try refreshing the page. Also, you can use Chrome. I have found it to be more stable.",318499.0
90107,375275.0,your data connection is very slow try with high speed data,318017.0
90107,375288.0,"why invalid syntax shows. i wrote print reduce(reduce_func,1st) now showing invalid syntax . pl help",319969.0
90661,378080.0,"codeacademy, tutorialspoint, hackerrank, geeksforgeeks are some of the good ones that I know and have used. Check out a similar question:https://learn.upgrad.com/v/course/208/question/90665",301652.0
90661,378105.0,"The practice and graded questions provided to you should be enough for now. Apart from that, we also have an optional module on Data Structures and Algorithms in Python the link to which is given below: https://learn.upgrad.com/v/course/208/module/6685 This module contains many programming questions on common algorithms and data structures that you can go through. If you require more practice question, a simple Google search can always help. :)",306040.0
88916,368530.0,Can somebody tell me in how many ways a set operation like union can be written.,319869.0
88916,368785.0,I think we should not discuss graded questions in Discussion Forum as well.,304813.0
88916,368550.0,"U are absolutely correct, this happened with me as I was solving Python problem. Though it took me some time but referring the notes and lectures again really helped to get a solution.",318358.0
88936,368612.0,it means some of the sample test cases have passed some others have failed. you may want to check your code for different inputs,319898.0
88936,368619.0,"When you submit your answer, your solution is tested against some sample test cases(same as the ones when you verify) and some non sample test cases. So partially correct means the sample test cases have passed. But the non sample(hidden) test cases have failed. Your solution should be generic enough to be true for all test cases. Hope this helps.",310511.0
88936,368621.0,"I believe, after code submission, it could be: Accepted : Means all the hidden test cases also passed. Partially Correct : Means some test cases have passed. (Generally meaning you missed some special case etc.) Rejected : Hidden test cases not passed.",317987.0
88936,368666.0,"One of my submission is showing as ""Submitted"" and not showing ""Accepted"", and there is only one possible test case to which it is passed in ""Verify"" button. What should I do? Shall I proceed with ""Report a mistake"" ?",306248.0
88936,368705.0,"Hi Arpit, I think you can try ""Report a mistake"" or else you can click on the help button on top right corner of the Upgrad learning platform. I believe Upgrad has very good support system who can respond within quick time.",310481.0
88936,368774.0,"Partially correct mean, some of the hidden test cases are failed. please recheck the solution.",304813.0
88936,369131.0,"Pratially correct mean, some of the test cases haven't resulted in expected output. Though it doesn't show the values of the failed test cases. Its better to execute the code first in Jupyter notebooks and check for multiple values.",300685.0
88936,370336.0,"The coding questions on the platform have two kinds of test-cases, i.e. sample and non-sample test-cases. The sample test-cases are the ones which are evaluated when you click on 'Verify' and the non-sample test-cases are the ones which are hidden and are evaluated when you click the 'Submit' button. Now, for some of the questions, there are no non-sample test cases. For such questions, if your answer has been verified correctly by clicking the 'Verify' button, your code is correct. And when you click on submit, it will just show the code in the 'Submitted' state, so don't worry. But be sure to always click on 'Submit' for all the questions irrespective to the test-cases, otherwise, your progress would be stuck.",319721.0
88936,370376.0,"Its not working for all the inputs , working for some.",306009.0
88936,370510.0,May be for some inputs your code is not giving the correct result.. Quick test; check if your code is handling the 0! as well!!,316349.0
88940,368638.0,You can comment out several lines once using Ctrl + / command,318328.0
88940,368639.0,Select multiple line and use ctrl + / to comment multiple lines in python - jupiter .,317991.0
88940,368637.0,"Hi, You have to select the lines(subsequent) you want to comment and then press Ctrl + / in Windows Cmd + / on Mac Hope this helps.",310511.0
88940,368651.0,in addition to the above mentioned answers for block commenting in jupyter u can try the following press down Alt key and use the mouse pad to select the section u want to comment out. the cursor become elongated to include all the lines you have selected. now press #. attaching a screenshot,319898.0
88940,369277.0,"As the most of the answers are w.r.t. jupyter, you can try using Ctrl + 1 if you are using Spyder.",318780.0
88940,380669.0,Ctrl + / in jupyter.,317149.0
88950,368681.0,"Check if you can see a Tick Mark against each of the sections. If not, then it means that you have missed completing some or the other part for that particular section. Please note that you need to attempt each of the mcqs, the coding questions and also watch the videos on the coding question solutions.",313826.0
88950,368770.0,If you have completed it then it will surely refersh to as done!,304813.0
88950,370338.0,"The coding questions on the platform have two kinds of test-cases, i.e. sample and non-sample test-cases. The sample test-cases are the ones which are evaluated when you click on 'Verify' and the non-sample test-cases are the ones which are hidden and are evaluated when you click the 'Submit' button. Now, for some of the questions, there are no non-sample test cases. For such questions, if your answer has been verified correctly by clicking the 'Verify' button, your code is correct. And when you click on submit, it will just show the code in the 'Submitted' state, so don't worry. But be sure to always click on 'Submit' for all the questions irrespective to the test-cases, otherwise, your progress would be stuck.",319721.0
88979,368878.0,"I tried the same code snippet in my Jupyter Notebook, when pressed Shift+Enter, prompt to enter input values is shown. Please restart the kernel if problem persists.",314730.0
88979,368956.0,You can do 3 things: 1) Kernel --&gt; Interrupt 2) Kernel --&gt; Restart kernel 3) Restart notebook! (might lose some data but is easier!) Hope this helps :),318495.0
88979,368851.0,"Click on Kernel -&gt; Restart and Run All Probably, you might have pressed shift+enter or ctrl+enter twice and kernel keeps waiting until first commands gets completed.",318329.0
88979,368861.0,"From the menu select Kernel ---then select interrupt which stops the process then you start running the code ....hope it will work ..,....,...",307843.0
88979,368858.0,Try to shutdwon and restart the jupyter notebook. The code is running perfectly fine in my jupyter notebook. Do let me know if problem persist.,317991.0
88979,368883.0,Restart kernal or Ipython notebook...whichever is easier for you.,306248.0
90842,378639.0,"You need to use ipl17.set_index('Team', inplace=true) which means it modify the DataFrame in place",318804.0
90842,378718.0,"DF variable is not updated when you perform action on DF. It will only update if you do inplace or assign it back to original dataframe. Either do ipl17.set_index('Team', inplace=true) OR ipl17 = ipl17.set_index('Team')",318368.0
90842,380052.0,InPlace parameter is to be set to True,311803.0
88748,367638.0,"you are getting a none value at the end probably because you have not provided any return statement in the rec(n) function. There is a bunch of print statement inside the rec() which executes and prints the numbers when the function is called from inside of the print statement (print(rec(3))) but when print(rec(3)) executes, it will print a none value because your rec() does not return anything. Hope this helps.",318374.0
88748,367657.0,"If you want to print the value of a function, you need to mention the return statement with in the function. return command exits from the function and returns back the value to its caller.",318804.0
88748,367671.0,"To stop "" none "" from printing, add "" return(' ') "" to the end of the function(within the function definition) before the last print statement. Hope this helps.",313691.0
88748,367846.0,"Deval using print statements in the function will print but using function in print will print the value returned by the function. In your case your function is return nothing as you haven't used return statement in your code, which is the reason you will be getting None after all the print statements in the end.! Try this simple code. You'll understand! :): def n(): print('yes') print(n()) Hope this helps :)",318495.0
88748,367999.0,"tl;dr: You're nesting print() statements. Just call rec(3) instead of executing print(rec(3)), and you'll be fine. And, always exit a function using the return clause. You can either use a return clause. Or you can just chuck one of the print statements. The thing is, whenever you run code such as this: print(print(4)) The output will always be: 4 None. When two or more print statements are nested, then there will be a corresponding number of ""None"" printed to the standard output (your screen). So, if you nest 3 none statements (such as this): print(print(print(4))), the output will be: 4 None None. Also, as the others have already pointed out, always end a function with a return clause. Using the return clause ensures that you've come out of the executable environment of the function. This means that any code you write after the return won't be executed , wich is what you want. If you use print() to get the output of a function, then there's a chance that the code below the print() statement executes (I haven't come across such a scenario yet.)",306733.0
88748,367653.0,"Hi Deval, You are getting None because the function rec(n) is not returning anything. You can try : # Read the input n = int(input()) k=n+1 def rec(n): #print (k) if n &gt; 0: print(-n) if n &gt; 0: return rec(n-1) elif n==0: print(n) return rec(n-1) elif n &lt; 0: if n!=-k: (print(-n)) return rec(n-1) print(rec(3))",317460.0
88986,368890.0,It could be because you are trying to use the list as a key. or. may be you are using trying set([list1]). would be easier if you could share the code.,317998.0
88986,368896.0,"The main input list is a nested list from which individual lists are derived. If you try to convert a nested list to a set, then you would get the ""Unhashable List"" error. Modify your logic such that you canwork with the individual lists instead of the nested-list.Won't be able to divulge more information. Hope this helps.",313826.0
89029,369091.0,"Hey Rohith, First thing I want to tell you every code has test case which is given internally after submitting. When you verify and run your code then u will be passed but after submission there is a reason behind when u will be rejected there is hidden test cases you have to pass all or else your code is not right better you jus clarify it self. I jus felt yesterday for the same in python graded question I got mistake my code .so better check your code carefully then submit and try in firstly jupiter workbench. I hope you understand..!!",305847.0
89029,369202.0,"In short, no marks if the answer is rejected.",310974.0
89029,369836.0,"It is boolean ,either right or wrong and if you are right then you get full marks else you will get no marks.",320685.0
88893,368444.0,"Hi Aman, that is a very good question and intrigued me as well. So, after going through some blogs on it, i realised that it is more efficient to convert the set to list and then sort it. Although, there would be no difference in the output (as you noticed), there is huge difference in the time it takes to sort in both the cases. So, in cases of huge datasets, such minor thing could make a huge difference in time required. which is very advantageous. You can go through the following link where they have tested the computation time as well. You will find it interesting. https://stackoverflow.com/questions/27612352/huge-difference-in-timing-between-sorting-a-set-vs-sorting-a-list-in-python Hope it helps. :)",317998.0
88893,368440.0,I have also only used the sorted() function and have not done converion to a list using list() constructor. The Python 3 Documentation clearly mentions that sorted() function returns a new sorted list . Link to related Python 3 Documentation: https://docs.python.org/3/howto/sorting.html#sorting-basics My understanding is that the problem statement had asked for rerturning a sorted list and that can be achieved by usage of just the sorted() function and hence no significance of converting it to a list() before applying sorted().,313826.0
88893,368533.0,"I did some more reading on this and turns out that the sorted() function internally first converts the input to a list and then proceeds to do the sorting. The source code for sorted() function's C-Python implementation can be found here: https://github.com/python/cpython/blob/master/Python/bltinmodule.c Here, if you search for sorted, you will notice that as part of its implementation it first creates a newlist and calls the PySequence_List which is equivalent of executing list() constructor. https://docs.python.org/3/c-api/sequence.html#c.PySequence_List So, I think there is no difference between sorted(x) and sorted(list(x)) where x is an iterable.",313826.0
88765,367726.0,"Yes, you can start and resume after sometime. Not required to finish it completely. The MCQ's and coding questions have two attempts each.",313691.0
88765,367758.0,"Yes, you can resume your MCQ's and coding questions any time you want. For coding question you can atttempt as many time as you want but can submit only 2 times. After two submission no more submission will be allowed.",318368.0
88765,367785.0,"You have two attempts for them.So as long as you have final attempt left , you can take your time.",319912.0
88765,367841.0,Ofcourse its possibly buddy! You will be seeing the no. of submits available for every question. So to answer your query: 1. It is not mandatory to complete all graded questions in one go. It is possible to exit from it and resume after some time. 2. You can submit the code/answer as per the no. of submits made available by the instructor for that particular question. 3. Make sure that you complete the submissions before deadline as not adhering to that might lead to loss of grades! (Don't wan't to scare..just want to guide! xD ),318495.0
88765,367842.0,"Answer to your questions: Is it possible to exit from it and resume after some time? Or once we start the test, we must finish it completely? Yes its perfectly fine to exit from it and resume later. It is not compulsory to complete whole test in one go though in my opinion Will there be more than one attempt allowed for that graded test? For MCQ's the attempt are fixed and you can't attempt question if max. limit reached. For Coding you can run your code as many time as possible. And you can verify as many time as possible. But submission has limit. And you can't submit beyond the maximum limit of submission.",317991.0
88765,368963.0,"Yes you can exit and resume later as there is no time limit on any graded question . Yes there are many questions with two attempts you can see on right corner, near submit button there is mentioned the maimum no of attempts and no of attempts you have taken. Hope this helps.",320685.0
89087,369309.0,"Try running the code for various input values, especially the edge case of zero which should return a value of 1 as factorial of zero is 1.",313826.0
89087,369357.0,I think if you share input and output value for which you are getting error maybe it will help in clarifying and I think sharing input output value should not be issue only think pls do not share the code,300687.0
89087,370506.0,"Hey, just a quick test since you're claiming that it works fine in Jupyter notebook; check if your code is handling the 0! as well!!",316349.0
89087,369371.0,"Make sure you have used reduce() to solve the problem, as that is one criterion mentioned. Also share your code for a better insight.",309451.0
89087,371064.0,Thanks guys. I got it.,300719.0
89144,369629.0,,301117.0
89144,369619.0,"Hi Namrata, Can you tell me what error you are getting? It should work both, maybe all test cases might not be passing in code. Please check with different edge cases in jupyter.",315028.0
89144,369623.0,,301117.0
89144,369632.0,"Hi , I think you are getting some other output that's why the test case is failing. As I can see the 3 factorial should be 6. Your output is not reflecting, check if you are printing the value.",315028.0
89144,369685.0,please send screenshot of ur code,318358.0
89144,369718.0,"Hi Namrata, Please send ur code so that I can see the mistake but first I would suggest to try referring to the below structure /steps of understanding and implementing the code: &gt; We know what reduce does - it applies a function which you want to implement over a range of value. &gt; The problem statement talks about handling 0! = 1, I would suggest to use if - else loop to handle this, for example if input_value == o then handle it else: u can define the reduce function. &gt; Now function which can handle (reduce(lambda a,b: function , &lt;range of values to which it should be applied&gt;) Now in the above - Function is the condition u want to imlement over range of values. if u r still not able to get it, pls share the code so that I can make out whre you are going wrong....",305656.0
89144,370375.0,Hope you have handled both 0! and 1!,306009.0
89144,370478.0,"Hi Namrata, Even I got the same error. After handling 0 factorial it worked fine",312093.0
89144,370503.0,"Hey Namrata, just a quick test since you're claiming that it works fine in Jupyter notebook; check if your code is handling the 0! as well!!",316349.0
89144,370689.0,Do not try to fit the 0! logic into the reduce function. Handle it with 'if' block explicitly.,318007.0
89144,374566.0,I too had the same issue. I did couple of things before I got it right and one of this fixed - don't know which one. Try these and see if it helps. 1. I had the code copy pasted from Jupyter and it had some extra lines which I thought may have hidden characters that the coding console may not recognize. I removed these empty lines from the code. 2. I didn't click 'Run Code' before 'Verify' first time when it failed. So I clicked run code - which was successful and then did the 'verify' and then it worked.,301654.0
89147,369875.0,I think you are truly enjoying python programming which is a great start :) Good luck.,310974.0
89147,369953.0,"Thats true, there are functions and logics very simple to perform a CODING though the same approach require complex logic in other PROGRAMMING LANGUAGES.",318454.0
89152,369687.0,What error r u getting ?,318358.0
89152,370731.0,"Hi Chaitanya, You will get indentation error if you have used somewhere tab or space incorrectly, pls check ur code once again and pls remember in python we have to write code in a very structured way. See if u have missed any tabs or spaces which shd hv been used...check are u using tab while printing output.",305656.0
89152,369691.0,Can you share screenshot of error you are getting ?,317991.0
89152,369708.0,"Hello Chaitanya, I think this is a graded python questions. Do not paste your code here as per the policy. Hint : You can use if else condition here. If n is zero, simply print 1 as this case can't be handles by reduce() In all other cases, use reduce() between the range 1 and (n+1). factorial is x*y",320195.0
89152,369727.0,"Hi, To answer your query I would suggest to first understand what reduce exactly does. &gt; Reduce is used to implement a function over a range of values hence u will have to define a fxn first and then provide the range: Syntax: reduce(lambda a,b: &lt;define function&gt;, &lt;define range&gt; Now as suggested in the previous answer, use if-else to handle the 0! part. please try, it should work, refer previous answer and apply in the structure suggested by me",305656.0
89152,370330.0,hello guys...thank you so much for replying....and sorry for the late reply,320606.0
89152,370524.0,"guys, i am getting an indentation error while inserting value 0 saying name 'reduce' is not defined, eventhough I kept "" from functools import reduce"" after else, rest of the values i am getting the correct factorial...help me",320606.0
89152,375647.0,"For me while solving this question, I am getting ""No sample testcases passed. (0/2)"" while clicking on Verify but when I click on ""Run code"" it says ""Code execution successful!"".. what to do?",310508.0
90817,378508.0,"Please take a closer look. The third list in the ""Expected Output"" is [12,15,16] whereas your ""Solution Output"" is [12,15]. There is something missing in the logic of your code because of which you are getting incorrect output. Please relook at the code that you have written. Also, do not hard-code any inputs in th code. Hope this helps.",313826.0
90817,378494.0,"Hi Giri, By executing, do you mean Run code? Execution of your code may be fine but the sample test cases will have to pass for Verify to pass. Do check the input and output and re check your code.",318329.0
90817,380695.0,The reason why the test cases were failing is I was using if else conditions and maths operations but in the problem they have asked us to use set operations. after using the set operations the solution was accepted.,307005.0
90832,378576.0,you need to consider that in Python index is starting from 0. I hope if you consider this you will get the correct answer.,301648.0
90832,378577.0,Create an array in jupyter notebook with the same shape and having elements in a sequence starting from 1. Then do a manual indexing to get the value 100.,313826.0
90832,379703.0,I faced similar issue but created an array in python and run the code to validate the answer. It did help. Please take note of python indexing.,319302.0
90832,378597.0,"Array position stars form [0][0] Your index (8,4) means 9th row 5th column i.e in 8 rows total element is 96 elements captures and writeen as i.e array[7][11] now after 4 elements you will get the 100 element. as array[7][11] -- 96th element and this last element of 8th row as per shape column is 12 array[8][0] -- 97th element ... array[8][1] -- 98th element .... similary you can find the 100th element",307843.0
90958,379388.0,Yes 11.59 PM IST,307843.0
90958,379417.0,Try to submit it aleast before 1 hours or so. Because you never know about internet and there can be some problem on Upgrad website itself.,318368.0
90958,379599.0,"Yes, Please upload it an hour before to avoid any last minute internet issues. After you upload, please download the same and check if all is well with the file. Good Luck.",316202.0
90958,380051.0,"Yes, you can submit anytime. The system will be open. doing it after deadline will penalize 30% marks. Only few hours left! Good luck",311803.0
90959,379386.0,"Here D IS DISCTIONARY In Python Dictionary is read as (key,value) so the key here are 1,2,3,4 and the value here are (Raj, 22) and so on hope this helps",307843.0
90959,379416.0,"Here keys are 1,2,3,4 so print function will print out 1 2 3",318368.0
90959,379456.0,"hi Naren, somewhere i read this i don't remember now but your dictionary output for 'For' loop will result into key values only since For is iterable function which pulls out the keys from the dictionary..",316349.0
90959,379482.0,"If you want to print values from the Dictionary D = {1:['Raj', 22], 2:['Simran', 21], 3:['Rahul', 40]} for val in D.values(): print(val) This will print: ['Raj', 22] ['Simran', 21] ['Rahul', 40]",317811.0
89219,369925.0,You can use XOR bitwise operator. 1 ^ 1 i.e 1 XOR 1 = 0 Use this to find the element which is not repeating.,317689.0
89219,371021.0,"Hello Aditya, But still my test case is getting failed. Code is running successfully.",320195.0
89219,371001.0,"Hi Nishan, If I try running the code you provided earlier, I see error as in the below image: As seen above, its an indentation error. Please try to make your code like the below image and run. This should work.",318355.0
89219,370877.0,"Hello Aditya, I am getting below error.",320195.0
89219,371100.0,"Hello Sushmiha, FYI",320195.0
89219,369940.0,"Hi Nishan, The objective here is to find the unique numbers in the given array. In the hint they're mentioning use bitwise operator i.e. | (OR) , &amp; (AND) , ^ (XOR). So, the next question comes here is which bitwise operator would help me in finding unique numbers or help me in eliminating duplicates. If we take XOR(^) of two numbers i.e. 1 ^ 1 0 i.e. it gives me 0 if the two numbers are same. 1 ^ 1 ^2 0 ^ 2 2 In this 1 xor 1 gives me 0 and any number N xor with 0 gives me N itself which is the unique number. So, in order to solve this question using bitwise XOR, there are some constraints: Duplicates numbers must be consecutive There should be only one unique number in the given array I'll recommend you going on to http://xor.pw/# to play with some numbers, you'll get a better understanding. print(""XOR unleashed!"") Happy Coding!",318355.0
89228,369968.0,"Reduce function itself iterated through the input privided. Here input is range. So no need of recursive call. just multiply two number of the input like:- reduce(lambda x,y: x*y, range(1,a+1))) and deal case of zero using if block",312746.0
89228,369939.0,or is it can we can use range and factorial together as both take positional argument ?,302735.0
89228,369974.0,"Suel - you are using reduce function. Reduce function takes 2 arguments from the list and return 1 value. eg. you need to find what 4! ie. 1*2*3*4 is then form a list [1,2,3,4 ] Reduce will takes first 2 values (x,y), pass it through lambda function &amp; return single value to replace. ################ from functools import reduce n=4 print(reduce(lambda x,y: x*y, range(1, n+1))) ########### in this case range(1, n+1) will create list as [1,2,3,4] reduce function will take first 2 values as x,y . So x =1 y =2 lambda will return 1*2 = 2, which will replace (1,2) . so now you have [2, 3,4] reduce function will take first 2 values as x,y . So x =2 y =3 lambda will return 2*3 = 6, which will replace (2,3) . so now you have [6,4] reduce function will take first 2 values as x,y . So x =6 y =4 lambda will return 6*4 = 24, which will replace (6, 4) . so now you have [24] Which will be your final answer.",318458.0
89228,369951.0,"Reduce function will iterate over all elements of the list which is passed. In your case: range(1,a+1). So, you don't need to specify factorial(x-1). No need for recursive call.",317689.0
89230,370108.0,"Means you are code is effective, thats why few test cases failed, please relook on your code and then try again.",318319.0
89230,369963.0,"No, I doubt. The fact that one test case is failing means program is not accurate. I haven't read anywhere that score is based on logical approach. It may not be physically possible to evaluate each person's code/logic and score it proportionately. Hence they have standard and non-standard test cases to evaluate code.",318458.0
89230,370500.0,Probably you didnt take care of the last restriction that is why it failed as I can see lat list is empty,317979.0
89230,372654.0,Thanks guys for the clarifications...,318454.0
89230,373890.0,https://learn.upgrad.com/v/course/208/session/19874/segment/101115 Even I faced the same issue for Set Operation. I attempt is failed for 1 of test case. Is it possible to validate or display the failed test case details to debug?,306731.0
89230,374431.0,"No, I don't think so that you can debug on here, but you can use jupyter notebook to check your error. TIP: They have given the example, so what ever is the output shown in the example will be matched at the time of submit.So if you getting the same output as the example ,then your solution will be probably correct.",310529.0
89184,369786.0,"The code might be producing wrong output for hidden test cases, so that is why it is getting rejected. Please verify the code so that it produces correct output for all inputs, not just one input.",313691.0
89184,369802.0,"Mr. Vijjini, Your code can be simplified further a lot. I would suggest you to read below link on sets operation and redo the solution https://docs.python.org/3/tutorial/datastructures.html#sets eg, Students who play both cricket and football but don’t play hockey &gt;&gt; this can be written as - SC=set(C) print(sorted(list(SC&amp;SF-SH)))",318458.0
89184,369790.0,The code is tested with sample code which has the output for the second answer as ['16' ] But when your code is submitted it is tested with multiple test cases which might not have output for second question as single item. So if it has multiple outputs it will not be in sorted format Apply sorting to the second print statement and try it in jupyter notebook first as it is graded question print(sorted(list(C_Set.intersection(F_Set).difference(H_Set)))) Hope this helps,308635.0
89184,369854.0,"1. Sorting is missing for the 2nd condition. 2. In this input, H has all the values in common with C &amp; F, Due to which you couldn't make much difference If you check the 3rd condition i.e., print(sorted(list(C_Set.intersection(H_Set).difference(C_Set.intersection(F_Set)).union(C_Set.intersection(F_Set).difference(C_Set.intersection(H_Set)))))) if you represent this condition in a pictorial way, it looks some thing as below. So, the output is giving only the C &amp; F related values. That means, F.intersection(H) condition is missing. Hope this might resolve your problem.",318804.0
89330,370402.0,"Yes Varun I did so. Then it worked with Verify in upgrade console. Now during submit its failing , that too says Test case #2. I am not able to understand what input they are testing with.",306009.0
89330,370399.0,"I dont see what input was provided for Test Case #2. You can see Test case #1 passed. Unable to understand what failed , what was input for Testcase #2.",306009.0
89330,370449.0,"Run you code against various test cases(especially edge cases) in jupyter notebook to see if the output is coming as per the expectation from the problem statement. When running the code in the coding console, make sure that you are not hardcoding the input values. The code stub provided in the console has been written to accept the inputs for the various test cases. Instead of hardcoding the input, make use of the variables in which the input values are getting stored.",313826.0
89330,370396.0,"Try to run your code on Jupyter Notebook first. If that works, then try it on UpGrad console. The test cases are hidden.",314547.0
90787,378363.0,Yes as long as it works :),310974.0
89340,,nan,
89346,371083.0,"Please find below 4 different ways to utilize the range() method in reverse order. range(9, -1, -1) range(10)[::-1] list(reversed(range(10))) [9-i for i in range(10)] All the above statements will print the below output. [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] Hope this helps!!",314730.0
89346,370638.0,A good suggession. I will use this going forward where I require to perform decrements. Thank you very much for your suggession.,311729.0
89346,370953.0,list(reversed(range(10))),320633.0
89846,373527.0,Factorial of 0 is always 1 and it can be done using simple if condition.,318368.0
89846,373535.0,It is not necessary to find it using loops. You can use if condition for factorial of 0 and rest of the factorials can be calculated in the else condition,318358.0
89846,373672.0,"hi Bhushan, In reduce operation since you're multiplying the two variables x,y and that continues for the entire range.. and if it starts with 0 within Reduce; it'll be super ambiguous (not sure even if doable).. and hence we separate out the 0 case using 'if' condition and then your reduce operation in 'else'.. hope this helps ☺️",316349.0
90356,376681.0,"Hi, Please share question and your input snap where you are not getting to run?",305847.0
90356,376738.0,"I think this question is not graded question. Try below code: list = [6,7,8,9,2,3,4,5] series_1 = pd.Series(list) series_2 = series_1.apply(lambda x:x**2) Hope this will help.",317991.0
89114,369472.0,A screenshot of the code would help,306733.0
89114,369581.0,"Since this is a graded question, I think the weightage is associated to each passed test case. I'm guessing :)",310974.0
89114,371077.0,We have the same weightage for all the test cases. It's not that one test case has more weightage than the other.,319721.0
89114,371080.0,"Please understand that this is on a the website platform over a coding console. So, it takes more memory than the usual python code.",319721.0
90092,375141.0,Try launching the course in incognito mode or clear all cookies and cache data and relogin.,317689.0
90092,375147.0,"Have you already tried the Upgrad help on this issue. Please find below the link for the same: https://help.upgrad.com/coding-console/the-coding-question-does-not-load After trring these steps, if you still encounter the same issue then I would suggest you to reach out to your Student Mentor.",313826.0
90092,375381.0,I had the same issue but tried with Firefox and now it’s solved. I saved lot of time now without a need to reload. Please give a try with Firefox if you haven’t tried yet.,318328.0
89576,371834.0,"You should do integer division. n = n//10 else n is a float and it is coming close to 0. And down the lane somewhere rev is becoming inf Basically, n is not becomh 0 and coming to very small no, a is always becoming a very small integer, And in this case a each time rev is multiplied by 10, once it is reaching very gigh value and becoming positive infinite.",318554.0
89576,371826.0,try n=n // 10 instead of n/10 cuz latter wont give integer value. check nd see if that solves the problem,317998.0
89576,371837.0,Little improvisation is needed in your code. Just convert n = n/10 into integer. Your code will work fine. Hope this helps.,317991.0
89576,371890.0,Statment below return float value not integer. n = n / 10 To get the integer value please use n = n//10;,318368.0
90806,378434.0,after checkpoint 4 I am able to confirm its movies dataframe,317577.0
90806,378497.0,Cleaning data is to make sure that you have got the concept right.,318329.0
90806,378516.0,The three major tasks in the assignment are : Getting the data and inspection Cleaning Data Analysis and Insights. End of Task 2 means that the input data has been cleaned and is ready for analysis. Hope this gives a hint on what datasets to be used in Task 3.,313826.0
89959,374552.0,"Hi Om, Firstly, you have not included the question for which you are seeking advise. Secondly, since this is a graded question, the maximum that can be done is to guide you in coming up with a solution by yoursel and nobody would be in a position to provide you the complete code. I would suggest you to explain what is it that you have already tried and where are you getting struck.",313826.0
89959,374664.0,"Below is the clear instruction from one of the TA regarding ""do not ask or post direct solutions for the graded questions"". Please have a look. https://learn.upgrad.com/v/course/208/question/89445 Now, comes your problem 'how to write code for this question as i am from testing background', It does not matter whether you are from development background or testing. What matters is effort. Try to write code as per the instruction given in question and if you get any error you can ask for clarification and explanation. I am sure every one will be more than happy to assist you in that case.",317991.0
90098,375213.0,removing the last closing bracket-still error,300690.0
90098,375185.0,Hard to identify with all those () :) What is the error that you're seeing?,310974.0
90098,375202.0,Following error-,300690.0
90098,375208.0,6 opening parenthesis and 7 closing parenthesis.,310974.0
90098,375222.0,"Below code is working fine for me, Please check. input_list = [[1,2],[3,1,2,7],[12,24,5,6,1,3]] C = input_list[0] F = input_list[1] H = input_list[2] C=set(input_list[0]) F=set(input_list[1]) H=set(input_list[2]) print(sorted(list(C&amp;F&amp;H))) print(sorted(list((C&amp;F)-(C&amp;F&amp;H)))) print(sorted(list(((C&amp;F)-(C&amp;F&amp;H))|((C&amp;H)-(C&amp;F&amp;H))|((F&amp;H)-(C&amp;F&amp;H))))) print(sorted(list(set(range(1,21))-(C|F|H))))",318368.0
90098,375209.0,You have an extra closing bracket at the end. Just remove the closing bracket and it should work. Number of opening brackets should be same as closed .,318368.0
89400,370835.0,The link is really helpful to know and make ourselves aware about different concepts and properties of Python.,305334.0
90099,375219.0,Yes. If the question ask you to follow specific function/class/technic then you need to use that only to solve the problem. If you have submissing remaining go ahead and resubmit it again.,318368.0
90099,380199.0,No where it is mentioned to solve this question using Set only use of sorted() function is mentioned so we can go through list comprehensions too.,318011.0
89590,376486.0,"Hi Akshay, It might be failing for non sample test cases. Non sample test cases are verified once the code is submitted. Please check if you are hard coding something in the code or check the details of the rejected code by clicking on the pen style icon mark. Hope it helps, Thanks!",320603.0
90493,377476.0,There are 2 types of test cases 1st is sample test case nd 2nd is no-sample test case. If input is given and output is showing it means your sample test are given. Non sample test cases are hidden inside the upgrade coding console.for your convenience u can check your test case with different input for your evaluation. I hope you understand..!! Happy learning..!!,305847.0
90493,377484.0,You can see the sample test case only which is already shown i.e. input and expected output. Sample test case is run while verifying. Other test cases are executed when we submit and these test cases are not shown.,317689.0
90493,377495.0,"Types of test cases 1. Sample Test Case -- Executed at the time of “Verify” and “Submit.” -- We can see the Input and Expected output at all times -- Mainly used to check how code is performing against some test cases so that we can make changes and then submit the code. 2. Non-Sample Test Case -- Executed only at the time of “Submit"" and that to behind the scene. We can not see these test cases. -- We can see the Input and Expected output only once all the attempt for submission get exhausted or have got the question completely correct. -- Mainly used to check whether solution is generic enough to pass every edge case or not.",317991.0
90493,377810.0,It is not possible to know all the Testcases before submitting the answer. But it is possible to test with different test cases by giving your own custom input in the 'Input' tab,318334.0
90493,378527.0,For the Final submission we can not know test case. Please check for hard coding etc in code if its failing in final submission,306009.0
89350,370648.0,"As much as I remember matamatics, I think factorial function is used with whole numbers only. I don't see any application or mathamatical series where factorial of any negarive number is calulated. Even if you go by recursive logic of factorial, it halts at 0 with value of 0! = 1 and doesn't go beyond that. If we proceed with negative mumbers, it will not end up as on subtaction of every nuber you'll get another negative number and 0 will not come where you can halt the function.",311729.0
89350,370783.0,the factorial function n! is only defined for non-negative values of n.,306248.0
89350,370539.0,"For this coding question point of view, you should handle the case when the -ve number is given like the way you are asked to handle 0. Hint You can use the default value argument in reduce function to handle the exceptions.",310974.0
89350,370532.0,"Factorial of Negative number is not defined. Hence I believe we should raise exception in this case, and inform the calling function to change the value to positive.",318554.0
88917,368546.0,"For union between sets: set1.union(set2) or you can use or '|' operation as ""set1 | set2""",318358.0
88917,368564.0,thanks Subarna,319869.0
88917,368558.0,"I came across two different ways. 1. To use the union() function eg: A = {1, 2, 3, 4, 5} B = {4, 5, 6, 7, 8} print(A.union(B)) {1, 2, 3, 4, 5, 6, 7, 8} 2. To use the '|' operator eg: A = {1, 2, 3, 4, 5} B = {4, 5, 6, 7, 8} print(A|B) {1, 2, 3, 4, 5, 6, 7, 8} The difference between the two forms is that union() you can give any number of sets as parameters eg: set1.union(set2, set3, set4….) A = {1, 2, 3, 4, 5} B = {4, 5, 6, 7, 8} C = {2,5,7,9} print(A.union(B,C)) {1, 2, 3, 4, 5, 6, 7, 8, 9} Similarly for other set operations you can use both the operator or function | for union. &amp; for intersection. – for difference ^ for symmetric difference",309451.0
88917,368573.0,Go through the below link which explains all the possible set operations in python https://snakify.org/en/lessons/sets/,318804.0
88917,368575.0,"Hello Ankur, You can try below example as well in Jupyter. Some more examples: x1.union(x2) and x1 | x2 both return the set of all elements in either x1 or x2 : x1 = {'foo', 'bar', 'baz'} x2 = {'baz', 'qux', 'quux'} x1.union(x2) {'foo', 'qux', 'quux', 'baz', 'bar'} x1 | x2 {'foo', 'qux', 'quux', 'baz', 'bar'} Output : {'bar', 'baz', 'foo', 'quux', 'qux'} More than two sets may be specified with either the operator or the method: a = {1, 2, 3, 4} b = {2, 3, 4, 5} c = {3, 4, 5, 6} d = {4, 5, 6, 7} a.union(b, c, d) {1, 2, 3, 4, 5, 6, 7} a | b | c | d {1, 2, 3, 4, 5, 6, 7} Output: {1, 2, 3, 4, 5, 6, 7}",320195.0
88917,368630.0,There are 2 ways of writing set operations. Plese refer below for the different operations: a-b a.difference(b) ------------------ a|b a.union(b) ------------------ a&amp;b a.intersection(b) ------------------ a&lt;=b a.issubset(b) ------------------ a^b a.symmetric_difference(b),310511.0
88917,369522.0,"Hi, set1 = { 2 , 4 , 5 , 6 } set2 = { 4 , 6 , 7 , 8 } set3 = { 7 , 8 , 9 , 10 } union of two sets print ( ""set1 U set2 : "" , set1.union(set2)) union of three sets print ( ""set1 U set2 U set3 :"" , set1.union(set2, set3))",314183.0
90589,377887.0,"still sometime it has little confusion, can special we can't mention in question itself",318319.0
90589,377882.0,"Thank you for the suggestion Rohit. A small detail to find out if it's a single or multiple choice question is to observe the shape of the option identifiers. If it has a circular pointer, that means, it's a single choice question. If it has square-shaped pointers, then that's a multiple choice question.",319721.0
90736,378149.0,"That is not mandatory. If you post your code and be specific in your question, we can provide our suggestions.",318084.0
90736,378178.0,"I faced issues when ever I tried using only if condition in filter and map. I don't think it will work. But, I can help you with another way to filter. Please try this: dataframe_name[dataframe_name.column_name &gt; condition] in place of &gt; you can use others. *Note: ""="" isn't working here also you can try dataframe_name[dataframe_name.column_name.any_inbuilt_function] I believe it should help many of our friends",310585.0
90736,378343.0,no it's not mandatory to use else with if statement. syntax: lambda x: x if (condition) it's equivalent to: if (condition) : x Hope it helps :),318495.0
91069,380274.0,"AFAIK, once the assignment is submitted it is final and no scope for re-submission.",313826.0
91069,380756.0,"Since you stated that ' I have wrongly submitted my assignment', it show that you already have submitted the assignment. Now you can not submit it again. The steps to submit assignment was already mentioned clearly in below link. https://learn.upgrad.com/v/course/208/session/19876/segment/101224",317991.0
91069,381569.0,"You can contact your mentor, they can suggest you something.",319721.0
91038,380022.0,you might have missed few code submissions.. cross check thw complete module once..,316349.0
91038,380050.0,Check the coding questions. Some of them do not load correctly. You will have to refresh each page and check.,311803.0
91038,380047.0,"Go through the module again. You might have missed completing any video or you have not attempted any question, quizz etc.",317991.0
91038,380147.0,May be few videos you left ..check again,319869.0
90015,374770.0,Please provide the value for input_list...That might resolve ur error,318358.0
90015,374778.0,"Hi Abhilasha, please use this code in your jupiter notebook to readvariables import ast,sys C = set([2, 5, 9, 12, 13, 15, 16, 17, 18, 19]) F = set([2, 4, 5, 6, 7, 9, 13, 16]) H = set([1, 2, 5, 9, 10, 11, 12, 13, 15] ) then you go with your print statement",306242.0
90015,375178.0,The error is because The Coding Console is using Python 2 and Jupiter notebook uses Python 3. You need to import libraries specific to Python 3 to clear those errors.,311868.0
90015,374798.0,"input_list = ast.literal_eval(input_str) The above line of code is specifically required for the coding console only, as in the coding console the input is received from the Input Tab or the sample/non-sample test cases. A quick and dirty way to test the same in any other iDE like Jupyter Notebook would be to create the input variable and directly assign the expected input value. For ex., if the expected input is a list containing three elements, you could do input_list = [1,2,3] Hope this answers your query.",313826.0
95730,405404.0,you can 'edit' the bin size from the Runs dimension newly created when you plot the Histogram.. check the dimensions section.. and right click on Runs there.. Edit it for resizing bins..,316349.0
95730,405418.0,when you load the data to tableau you must have split the data fir runs at the same place when you go for runs column you will see that there is a option create bin where you can create bin with 10 difference,318017.0
95730,405634.0,"Hi, Right click on the bin created by you and then you can see the size of bin where you can edit the size as per your requirement.",300687.0
95730,405726.0,"Hi Prakhar, Editing the bin is optional here. This can be solved without editing the size of the bin as well. SR from CSV can be used as an additional measure &amp; information can be visualised better with colour. Hope this helps you..",310508.0
96310,410357.0,df['strike_rate']=df[expression] This will create a new column in your dataframe,307176.0
96310,410727.0,"Hi Khushbhu, You might want to create a column named df['Strike rate'] and it would be the runs scored per balls faced multiplied by 100. Then you can filter the rows where a batsman has scored 100 or above and sort the values by Strike rate in descending order",311160.0
96310,413036.0,"Filter the rows where batsman has scored a century. Now, create a new column SR = runs/balls *100. Sort by SR desc in df and you will get the answer",317689.0
96550,411430.0,"Question itself is very clear that ""(including the students who submitted after the second deadline)"". So you can guess what is required.",317991.0
96550,411449.0,check all those students who have submitted after 3rd jan 2017 11.59.59 thats it,318017.0
96550,411523.0,"Firstly, convert the 'submit_time' column to datetime datatype. Secondly, perform your analysis to count the number of students who submitted after 23:59:59 of 3rd jan 2017",311160.0
96550,413125.0,Yes. We need to take all students who submitted after first deadline.,317689.0
96550,414284.0,There are some data with 2016 year. That should be excluded since its past year information.,301114.0
96647,411702.0,"I see that you have put some kind of a filter on the submit_time column. Remove the filter and take a closer look at the way the date has been captured in this column. Hint : There are different ways in which a date can be captured like ""dd-mm-yyyy hh:mm:ss"" , ""mm-dd-yyyy hh:mm:ss"", ""dd-mon-yyyy hh:mm:ss"", ""mon/dd/yy hh:mm:ss"" etc.",313826.0
96647,413127.0,There are a total of 198 records having different dates as submission_time. Please remove filter and convert the string date to datetime format in python.,317689.0
96647,414879.0,"I think you have converted the submit_time to datetime format using the format parameter for only time. So it is taking the default date for all records. Don't specify anything in the format parameter. Instead use infer_datetime_format=True in to_datetime(). It will automatically detect the format. Or if you want to use the format parameter then use format=""%m/%d/%y-%H:%M:%S""",304319.0
97077,415039.0,You can use column.dt.day to extract the day without applying lambda,318329.0
97077,415352.0,we can extract the column .dt.day to extract the day,306996.0
97077,414507.0,you can do both the things even column.day will give you the day and lambda function will give you the day. benifit of using lambda function can be the format which we can provide appropriately.,318017.0
97077,416094.0,"You can solve this using many approaches. Lambda function is one of them. If you like exploring it, I'd say give it a try. If you don't like it, use other alternative approaches",309211.0
96201,409731.0,We need to consider the average of all the 3 ratings and round them to one decimal (which is the default value of round function). Taking average on a single variable makes little sense as it would invariably return the same value.,311160.0
96201,412355.0,"I think as mean means the average so you an take the mean and round it accordingly. cust_rating['avg_rating']=round(cust_rating[[""rating"", ""food_rating"",""service_rating""]].mean(axis=1),0) This method worked for me not sure its the right approach or not",300687.0
96201,413039.0,"It would be rounded average of food_rating, service_rating and rating.",317689.0
96201,413395.0,"avg_rating = A cumulative AVG of scores of 'rating', ' food_rating', 'service_rating'",310508.0
96201,413884.0,"When the Question have asked to find out the Average Rating, then by default it becomes clear that there are three different Rating given, Average Rating can only be average of the three, therefore you need to add each rating column wise and then find the average by dividing the Sum(addition ) by 3. The resulting Average rating column then should give you the Round Rating (i.e. if Rating is 1.85 it should be shown as 2). Hope this clears your doubt.",315560.0
96226,409726.0,We should be considering the entire 'submission' column for the calculation,311160.0
96226,410735.0,"Take a look at the ""submission"" column. try to extract the '.zip' text from the submission column and you have to consider all the submit_time.",306242.0
96226,410739.0,Thanks all.,320689.0
96226,413101.0,Extract format from the submission column and based on that derive the percentage.,317689.0
97195,,nan,
97157,415453.0,Can you please confirm if the submit_time column is converted to Datetime datatype?,311160.0
97157,415450.0,Got the issue...pls ignore,303228.0
98603,419883.0,This is nice questions!! thanks for asking!! Atleast for Coding questions we've the 'Show Solution' to see the ideal way of getting the answer.. But what about the objective questions which are solved through Python?? :),316349.0
98603,419712.0,what were the difficulty cant share directly but can help with watever problem you are facing you can tell the error or whatever you are not able to understand,318017.0
98603,419926.0,"I would suggest you to try it once in python and wherever you are stuck, you can pose the problems faced by you in the discussion forum. We can see if this can be addressed by fellow students, if not TA's can pitch in and resolve the open questions",318084.0
98748,420687.0,Why would you want to plot date and time? Is it some parameter against Datetime??,316349.0
98748,420691.0,The below link will help: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot_date.html https://matplotlib.org/gallery/ticks_and_spines/date_demo_rrule.html#sphx-glr-gallery-ticks-and-spines-date-demo-rrule-py,318476.0
98748,420725.0,you can use countplot with y as count and x as time,318017.0
96411,410662.0,in the data cleaning section,318017.0
96411,410669.0,"There is 'Click here' link the sentence.. anyways, for your quick reference sending the link below.. http://www.censusindia.gov.in/2011census/C-series/C08.html download the dataset 'C-08' for 'India'.. :)",316349.0
96411,410903.0,Click the button corresponding to C-08 and India and dataset gets downloaded.,301890.0
96411,411058.0,Click on th download button Down Arrow corresponding to Indai C-08 in the below URL http://www.censusindia.gov.in/2011census/C-series/C08.html,304693.0
96411,411632.0,click on column C-08 in the row corresponding to India to download the data set,311860.0
95616,404768.0,kaggle and github has cool datasets,318017.0
95616,404774.0,The MIT License (MIT),318329.0
95616,405537.0,Can we extract the datas from Kaggle and Github for practice?,311117.0
95616,406551.0,You could use web scraping technology to get real-time data from websites.,318732.0
95616,407355.0,"Bro try google data sets,,data.gov best resources ibhope these help u.",304692.0
95616,412279.0,MIT License (MIT) Open the license file available,313228.0
95616,410002.0,You can download large amount of data set from https://data.gov.in/,301113.0
95616,411062.0,Github and Google Data has enough data where you can try!,304693.0
95616,415071.0,kaggle is also good for dataset,301646.0
96512,411957.0,"Students marks in reading subject depends on mother's education. Higher mother education, chances of getting good marks in reading subject is high. This is one of my observation.",307495.0
96512,413065.0,Higher mother education is very good chance of good marks by child in reading.,310952.0
96462,410964.0,"Different parts of this plot can be made in Excel and Tableau. Like the main page anaysis is done through excel and other parts in which graphs/plots are there, we can done those things in Tableau and then combine as one dashboard.",318429.0
96462,411746.0,"You can check dash by plotly , a python framework.",304398.0
95781,405721.0,"No, for the complete session on Univariate we learned about the plot of single numerical parameter (either ShippingCost, Sales or Profit). cannot remember of having considered Gender anywhere!!",316349.0
95781,407000.0,"The reason this is univariate analysis or rather segmented univariate analysis only is that you are comparing the average marks, which is one variable. The gender is the basis of segmentation in this case.",313517.0
95781,411259.0,This session is a segmented Univariant analysis. here we take only one variant (measures) and segment them based on various categories (Dimensions) and anlyze them. In Bivariant you will have 2 variants (measures),301113.0
95781,412016.0,You are analysing only one variable i.e. marks scored. We are not comparing it against any other variable for example sales vs profit. We are just segmenting marks based on gender to see if there is any pattern.,317689.0
97134,415108.0,"Try increasing the figure size as shown below: plt.figure(figsize=(15,10))",310974.0
97134,415102.0,Please refer the below link regarding rotating the x-axis labels. https://stackoverflow.com/questions/31859285/rotate-tick-labels-for-seaborn-barplot,313826.0
97018,414118.0,Number of literate persons / Total number of persons if it is total literacy rate Number of male literates / Total number of males if it's make literacy and so on,318329.0
97018,414574.0,You can use excel to filters the records and calculate the percentage for each state. Now you can easily get the highest and lowest state.,317845.0
97018,416060.0,In python you can calculate literacy percenatge for each state for year 2011 and compare ( total literates/ total population ) * 100,317996.0
96694,412047.0,"there are many interactive tools for reports like tableau , ibm cognos , power bi which show very beautiful outcomes",318017.0
96694,412164.0,It looks like Tableau as it have those powerful features to visualize data,304693.0
96694,413151.0,The tool is their own website and I am not sure which tech stack they are using for interactivity. https://gramener.com/nas/,317689.0
96694,412996.0,It's Tableau as it have those powerful features to visualize data and also it is in program.,310952.0
96711,412092.0,The question is basically to know the ratio of the number of illiterate females under age group 20 - 24 among the total females.in that age group.,316202.0
96711,412137.0,You're supposed to find the percentage of illiterate females out of total females in age group 20 to 24.. this is expected to be done in Python only.. hint try to group your data on age group anf then aggregate the data for total females and illiterate females.. then you simply see the data points against the 20 to 24 age group.. now just divide both the values to get the required answer.. hope this helps..,316349.0
96711,412811.0,"The question is asking for a division if you read it carefully. You can solve it via Excel also but Python would be a good choice as the next questions will get easily solved via Python, so my advice is that you divide the two columns and find percentage and check for the asked Age group rather than group by and other methods. A similar division method was demonstrated in Pnadas graded Questions (Upgrad solution) , check for that. P.S. : Adjust the excel file accordingly as adviced on the platform before importing in Python.",315560.0
96711,412838.0,"The question is asking for a division if you read it carefully. You can solve it via Excel also but Python would be a good choice as the next questions will get easily solved via Python, so my advice is that you divide the two columns and find percentage and check for the asked Age group but perform Group By on Age Group and use the group by for Sum and Division. A similar division method was demonstrated in Pnadas graded Questions (Upgrad solution) , check for that. P.S. : Adjust the excel file accordingly as adviced on the platform before importing in Python.",315560.0
96716,412146.0,,316349.0
96840,413980.0,,320251.0
96840,413254.0,"I have created a pivot table and see that the average is ~36% across all labels. So, the answer provided is correct. Please check that you are using the correct data file and that you are choosing the entire data in the calculation.",313826.0
96849,413178.0,Got it. We need not group the ages. Visually we can see whether the age group 55+ is more or less literate etc.,304319.0
96476,411136.0,It is all about how much your data is deviating from the average value.. Example: 1. How much you sales data is deviating across a year.. 2. How much scores of students are deviating from the average marks of the class..,316349.0
96476,411142.0,standard deviations can be used where the data has no outliers,318017.0
96476,411747.0,Hi Nitesh Some nice examples are given on wiki page too https://en.wikipedia.org/wiki/Standard_deviation,334535.0
96476,411756.0,"1_If your data set having outliners or sudden variation than expected data for particular column , we wil use standard deviation to take decision to include it or not . It will give more approximate impact of that value with average . 2_Data sets like temperatre data set of various places and banking data sets, particulary in healthcare dignosis data sets etc.,",318732.0
96640,411691.0,"Taking the example in the video- products of a resturant and their correlation Starter has a high positive correlation with bread (87) which means, high sale of starters results in high sale of breads and vice versa. However, starter has a negative correlation with tiffin(-30)which means, high sale of starters results in low sale of tiffin and vice versa. Similarly. starter has negative correlation with beverage(-30),maincourse(-36),packing(-66) etc So if you go column by column , you will understand the relationship of the products based on the value given whether they are positively or negatively correlated .",311254.0
96640,412251.0,Coorelation matrix means how a variable moves with movement in other. It ranges from -1 to 1. -1 means when var1 increases var2 decreases and vice versa 1 means when var1 increases var2 also increases. 0 means no relation between var1 and var2 values. But in ideal world we rarely see exact of these values. As shown in examples we can have good correlation between different currencies.,317689.0
96513,411333.0,"Examples of continuous variable which can be considered as categorical variable? The length / height of an object. Mass &amp; Weight of an object. Temperature pressure density volume distance So even if their values are like 11.10987,12.11989,13.98765,14.7654 You can put the values in bins like 11-15,15-20 and that will serve as categorical values.",311254.0
96513,411368.0,"Some other examples could be Age, Weight, Salary details of a person. These can be created as a Range and can be used as Categorical variables.",316202.0
96513,412118.0,"In general, variables (and data) either represent measurements on some continuous scale, or they represent information about some categorical or discrete characteristics. For example, the weight, height, and age of respondents in a survey would represent continuous variables; in industrial or medical applications, survival/failure times are also continuous variables. However, a person's gender, occupation, or marital status are categorical or discrete variables: either a person is male or female, never married, married, or divorced, etc. Some variables could be considered in either way. For example, a person's rating of someone else's attractiveness on a 4 point scale may be considered a continuous variable, or we may consider it a discrete variable with 4 categories. Time series data are usually collected for continuous variables, over time. For example, stock quotes for a particular stock over successive trading days represent a time series of data.",318732.0
96513,412185.0,"Runs can be converted in 50s and 100s. Age can be used to derive senior citizen, adult etc.",317689.0
96377,410494.0,Please check the below link: https://stackoverflow.com/questions/43021762/matplotlib-how-to-change-figsize-for-matshow Need to additionally pass fignum=1 parameter .,313826.0
96377,411101.0,"Please use sns.heatmap() for generating better plots. You need to choose only those columns for which you wish to generate the correlations instead of selecting all the columns. Also, use the plt.figure(figsize(x,x)). This way you would be able to generate bigger plots. Try out sample code below: plt.figure(figsize=(15,7)) sns.heatmap(curr_jul_16[['Indian_Rupee','Australian_Dollar','U.S._Dollar','Japanese_Yen','Euro']].corr(),annot=True,cmap='Blues') where curr_jul_16 is the dataframe which contains the data present in the currencies.csv file.",313826.0
96377,412242.0,"import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np df = pd.read_csv('currencies.csv') corr = df[['Euro', 'Japanese Yen', 'U.K. Pound Sterling', 'U.S. Dollar', 'Australian Dollar', 'Indian Rupee']].corr() # following is the code to plot the heatmap f, ax = plt.subplots(figsize=(10, 8)) sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax)",317689.0
96377,414307.0,"I used the following and was able to view for all corr = currencies.corr() plt.figure(figsize = (24,18)) sns.heatmap(corr, xticklabels = corr.columns.values, yticklabels = corr.columns.values) corr.style.background_gradient().set_precision(3)",310509.0
96970,413769.0,"What is possibly happening here is that your query is returning a row with less than 29 items. And then in your code you are trying to insert that into a row of dataframe which has 41 columns (and so needs 41 items, which are not there). Hence the error: ""ValueError: cannot copy sequence with size 41 to array axis with dimension 29.. Check the number of columns in your main dataframe and the one which you're updating.. the columns count is not matching..",316349.0
96806,412779.0,You have not provided the full path of the currencies.csv file. Check the location of the file and put that path in the command. For example:currencies.csv is in d drive in a folder named learning df=pd.read_csv(r 'D:\Learning\currencies.csv'),311254.0
96806,412797.0,"Add full path in read_csv() where your 'currencies.csv' file is kept. Ex: currn = pd.read_csv(""C:/PGDDS/bivariate_analysis/practice/currencies.csv"", encoding=""utf-8"") Hope this will help.",317991.0
96806,413047.0,Please specify full path,311004.0
96806,413708.0,"If Jupyter notebook and file are in same path, please use the following - df = pd.read_csv(""./Bivariate_Analysis/nas.csv"") MyDir | -------- Bivariate_Analysis\ |-------------------------nas.csv | -------- notebook.ipynb",312479.0
96806,413582.0,"Add full path in read_csv() where your 'currencies.csv' file is kept. Ex: currn = pd.read_csv(""C:/PGDDS/bivariate_analysis/practice/currencies.csv"") or try this Add full path in read_csv() where your 'currencies.csv' file is kept. currn = pd.read_csv(r""C:/PGDDS/bivariate_analysis/practice/currencies.csv"") Hope this helps.",315560.0
95878,407090.0,The dataset provided may show up an error in Excel due to incorrect settings. Please either open the dataset in Python or change the column settings to date/time after converting the file from csv to Excel format.,313517.0
95878,412248.0,"You need to split the month column into year and month and take 2008 data from there. For 2001 and beyond it is showing as 1-Jan, 8-Jan. Here 8-Jan is january 2008. This needs to be handled in the code.",317689.0
95878,411581.0,"yes i am also facing same issue, cant see any data for 2008",300733.0
95878,411582.0,,318429.0
97005,413953.0,Please do not give the right anwer here. My perspective is only to understand ambiguity in answer list :),313228.0
97005,414869.0,I have personally not felt ambiguity. The wording of the options were descriptive and we need to carefully read through each one of them.,311160.0
96783,412619.0,Group by is not needed. In excel use correl function In python use .corr function,311254.0
96783,412725.0,"there is other way round to do this if you're facing problem in converting the Month to datetime datatype.. you can split the Month column on (-) which will give two separate columns one containing 'Mar' and other containing '08' And then usual way, filter on 08 and find the Pearson correlation using .corr()..",316349.0
96783,412640.0,"You need to first filter the data only for year 2008 and then find the correlation. For this, you should first convert the column 'Month' to a datetime datatype. Please refer the link for doing this conversion : https://learn.upgrad.com/v/course/208/question/96437/answer/410771 Once you have the column in datetime format, you can filter and find correlation as follows: df[df.Month.dt.year == 2008].corr()",313826.0
96783,412748.0,filter the data for the year 2008 and use .corr function in python. If you are comfortable with excel. you can do the same by using correl function for values belonging to year '2008'.,311860.0
96783,413383.0,"Hi, to find the correlation using python, you have to first convert the date from type Object to DateTime, in order to do so you can use apply() function. After converting the date column filter out the data for year 2008 and apply .corr function.",305129.0
96813,412805.0,It seems there is some error in the way you are using read_csv(). Currences.csv file is not there in your D:/learning/ folder. Have you placed your currencies.csv file inside learning folder in D drive ?,317991.0
96813,413317.0,"Can you try the path as r""D:\\learning\\currencies.csv"" (Or) ""D:/learning/currencies.csv""",311502.0
96813,413392.0,use csv file but the file download is xls. so change it to csv.,310952.0
97090,414958.0,"The value is present. Although several have had the same issue. If you want to check outside Python, apply a filter on the csv on the date column and check for the value. You'll see it there.",316416.0
97090,414566.0,verify the csvfile . I can see in the downloaded csvfile .,317845.0
97090,414570.0,"Data for the year 2008 is present. Please note that you need to first convert the month column to datetime format using the correct format identifires. You can do something like this: df['col1'] = pd.to_datetime(df.col1,format=""%b-%y"") Please note that the date ""format=""%b-%y"" means the date has been mentioned in ""Mon-YY"" format, meaning capitalised first three characters of the Month followed by the year without the century. Hope this helps.",313826.0
97090,415059.0,ones which en with -08 are the datas for 2008,314629.0
97090,415190.0,have changed data type to datetime in pandas and even checked in csv file yet i am not able to see any 2008 dates,320690.0
97090,415475.0,I'm facing the same issue. No data present between 2000 and 2018. The excel has only 242 rows.,301654.0
97090,415310.0,"I would suggest to download the csv file again and then try the following commands: df = pd.read_csv('EDA_Gold_Silver_prices.csv') df['Month'] = pd.to_datetime(df.Month,format=""%b-%y"") And then check for records for the year 2008.",313826.0
96437,410751.0,as.date () str_to_date(),318017.0
96437,410771.0,"You can do something like this: df['col1'] = pd.to_datetime(df.col1,format=""%b-%y"") Please note that the date format expected is ""Mon-YY"" meaning capitalised first three characters of the Month followed by the year without the century.",313826.0
96437,410966.0,"you can use datetime package. For e.g. import datetime object to date conversion -&gt;datetime.strptime(""date in string type"", format) date to string conversion -&gt;datetime.strftime(""date in date type"", format) For details please refer the beow link. https://docs.python.org/3/library/datetime.html",310467.0
96437,412188.0,Refer to following link for more info. https://stackoverflow.com/questions/50437679/set-the-format-for-pd-to-datetime,317689.0
96138,408739.0,"Hey Bhaswati, Close the browser, clear cache &amp; cookies, check your internet connection and try to run the video again. If you're still facing the issue, report it using the ""report a mistake"" tab at the end of the page. Hope this will help.",302742.0
99043,421723.0,"Hi Puja, As these are graded, we would not be able to provide you with any solution and we also recommend not to share or discuss any graded questions on the discussion forum. If you have any doubts regarding the questions you are free to ask your mentor to connect with the module owner to help you. For MCQs, the answer can be figured out and the explanation is provided below every right or wrong answer as a feedback. Hope this helps. Regards, Sumit Shukla",301912.0
96847,413520.0,In python use df.corr() follow this link: https://stackoverflow.com/questions/29432629/correlation-matrix-using-pandas,312490.0
96847,413220.0,"In Excel , you can use correl() function and in Python use .Corr() function",311254.0
96847,413374.0,Till now it wasn't covered we're just learning how to read it and basic Heat map in Plot session but I'm expecting tht to be taught in details in Machine Learning session as mentioned.. anyways for the graded question it is easy plot where you can take help of google.. this can be done in matplotlib ir seaborn or any other library.. try searching on google for basic heat map..,316349.0
96847,413389.0,can also see link below https://www.youtube.com/watch?v=b0Bd0b6Slzs https://stackoverflow.com/questions/3949226/calculating-pearson-correlation-and-significance-in-python,310952.0
96847,415055.0,down vote accepted You can use pyplot.matshow() from matplotlib : import matplotlib.pyplot as plt plt.matshow(dataframe.corr()),314629.0
96847,415299.0,In excel you can use - Correl() function In python you can use - df.corr() you can plot the same in python using pyplot.matshow() from matplotlib,318021.0
96847,415057.0,You can use pyplot.matshow() from matplotlib : import matplotlib.pyplot as plt plt.matshow(dataframe.corr()),314629.0
96847,413155.0,df.corr() is the command,318017.0
96535,411400.0,Can you specify which tool are you using?,315277.0
96535,411409.0,df.isnull(axis=0),318017.0
96535,411412.0,df.isnull().sum(axis=1) will give the count of missing values in your rows.. this can be set with greater or less or equal or not equal condition to update your final dataframe to show only the filtered result..,316349.0
96535,411550.0,same as columns.Just specify 'axis=1' explicitly in the function,305655.0
96535,411667.0,"df.isnull().sum(axis=1) As you want to calculate the row in which columns are missing, Now you need to set it as greater than, smaller than, equal to or not equal to any value.",317689.0
96535,415461.0,"Shruti, df.isnull().sum() This function will give you the number of missing values in each columns. By default, the system takes columns for calculation of missing values. When you need to calculate missing values for a row, you need to define axis in sum(). like : df.isnull().sum(axis = 1). This will give you number of missing values in a row. Remember the movies dataset assignment, there too we removed rows with missing values graeter than 5. Please refer to following link for more details: https://pandas.pydata.org/pandas-docs/stable/missing_data.html",302735.0
96559,411490.0,you can use text to column function in excel and use delimiter also,318017.0
96559,411554.0,I got an example from one of my friends and am posting it here. Number plates have information on state and area in prefix of a number. KA 01 AB 1234 My friend was asked to seperate the items in the Number plates. He could not use slicing as number plates from certain regions did not follow this format and had to write a function and filter the details seperately.,308637.0
96559,412642.0,if it's number plates then most likely the character will be delimited by space or -. y not use split functionality..?? column.apply(lambda x:x.str.split(-)). this outputs a list which can be stored in another column..,319898.0
96559,413318.0,df['column'].str.split('char') will return you df with values after spliting you can required row using df['column'].str.split('Char').str[index] based on your requirement.,301118.0
96639,411656.0,"Convert to string and Use replace(''x,''y)",313526.0
96639,411633.0,Try to use split function and split the values on '_' (underscore) and use 'apply' to accommodate a function to fetch the 2nd value of each list as you are interested in the values which are right to the underscore.,311160.0
96639,411785.0,Simple tweak is to use series.str[index:],318007.0
97642,417760.0,"If you compare values literally, then mean would always be greater than variance I think",318329.0
97642,417769.0,Mean you would see since both variance and standard deviation are sum in roots so the increase is less as compared to Mean,307843.0
97642,417852.0,standard deviations has the greatest variation,318017.0
96257,409714.0,The question is correct.There are missing values in the dataset. Please recheck again. I got 5 missing values and were able to delete it from the dataframe.,311254.0
96257,409724.0,hi sir apply the below query: marks = marks[marks.isnull().sum(axis=1)&lt;5],318017.0
96257,409736.0,"There are 2 rows which have exactly 5 missing values in them. When you export to csv , put filters on all the columns and filter only ""Blanks"" and you will find 2 such rows. To remove such rows from the dataframe, you need to select all the other rows with missing values !=5 and reassign it to the original 'marks' dataframe. You can refer to the code belwo for achieving the same marks = marks[marks.isnull().sum(axis=1) !=5]",313826.0
96233,409780.0,Do R also has this option of cleaning the data ?,318814.0
96233,409564.0,you should do the cleaning in python only as it can tell you the percentage of data after cleaning each time but such a thing is hard to understand in the excel cleaning,318017.0
96233,409575.0,"All the activities mentioned can be done in both : using Python and/or using excel. However, for ease of working and maintaining a steady flow of the processes, one of the tools should be used for the entire of the activity. I personally feel that the activites should eb done in python, as we will be able to use the same code and replicate the activities for any future datasets of similar nature , whereas doing in excel would mean to do the enitre process manually all over again. Further, the quiz questions asked further in this course are based on python and hence it is better that we stick to python as a tool for all these activities. Hope this answers your query.",313826.0
96233,410349.0,"Well, all sorts of cleaning can be done by Python and I'd suggest to use python for cleaning data and doing analysis since we're trying to use python for any kind of cleaning/analysis. This would greately enhance our understanding of Python and alo make us fluent in the language. Basics of python and its application has already been taught by our teachers. Still, there maybe some areas which we need to find a solution online by googling. Then we'd know how do we handle similar scenarios in future.",307176.0
96414,410660.0,you have to replace cust_id with blank from the cust_id numbers then only digits will be left,318017.0
96414,410680.0,"Hi Lalitha, I think you are talking about following Q Removing extra characters from a column Description The given data frame 'customer' has a column 'Cust_id' which has values Cust_1, Cust_2 and so on. Remove the repeated 'Cust_' from the column Cust_id so that the output column Cust_id have just numbers like 1, 2, 3 and so on. Print the first 10 rows of the dataset ’customer’ after processing. HERE, you are expected to ""strip"" what is mentioned before the number/digit. What you have mentioned is about dropping duplicates which is not required as per the expected output. Please read about ""strip"" or ""lstrip""fuction online. Please try following link this will definitely help you. Link below : https://stackoverflow.com/questions/38277928/remove-special-characters-in-pandas-dataframe/38277932 https://stackoverflow.com/questions/33257344/how-to-remove-special-characers-from-a-column-of-dataframe-using-module-re. Hope this helps you out.. Thank you.",310508.0
96414,410798.0,"Hi, you are going in completely different direction . Its not the duplicate function you need to use. Its string split method you need to use. Watch this video for more clearance. https://www.youtube.com/watch?v=pq5LFwR4Emg",315560.0
96414,410971.0,"You can make use of replace function. replace('Cust_','')",317996.0
96414,411027.0,Hi Use a map function to remove the first 5 characters in the column for eg : map(lambda x : str(x)[5:]),306729.0
96414,411305.0,"You can use str.split on the column to get the required output. str.split('separator').str[number required from split 0,1,2,etc.] e.g. str.split('|').str[2]",319302.0
96414,411748.0,"Hi Sreeram, You can make use of df['y']=df['y'].map(lambda x:x.lstrip('m')) , where y=name of the column m=pattern to be used or the pattern which you want to strip. Example xyz_tom,xyz_cat,xyz_ctr and i want tom,cat,ctr then pattern will be ""xyz_"" as this is common across all values. Thanks and Regards Somnath",314617.0
96427,410684.0,"Hi Lalitha, You have to try to round off reaction column in the dataframe called sleepstudy. Here I see that you have not specified which column you want to round off. Also (20) is not required in the code. Instead of that mention column header which is to be rounded off. Try following an example Lets say I want to round off AVG column in dataframe called cricket then I will try following. cricket['AVG'] = cricket.round({'AVG': 1}) print(cricket.head(10)) Hope this helps you out.. Thank you.",310508.0
96427,411043.0,"Hi Lalitha, You can use the pandas round function to round the decimal value as specified. For eg df['Columnname'] = df.round({'columnName' : roundvalue})",306729.0
96427,411067.0,"you need to round it OFF to 1 decimal place and you havent mentioned it there. One simple way to deal with these questions is to Download the First Assignement (Movies) solution, it is available in that solution video on how to round off the values. Its esy to get , just watch the Solution uupgrad video. Hope it helps.",315560.0
96427,411696.0,Use round function. df[column].round(1),317689.0
96456,410968.0,One way I could recollect is to manually scan through the distinct values in your dataset.. now this you could do by sorting your data in an order or any other logic as per the understanding of data..,316349.0
96456,411158.0,"There is no one sure shot way of finding the same. We need to take samples of data and observe or if you have some previous patterns data, you could apply that.",318329.0
96456,411292.0,"The approaches would vary depending on the needs. For example, to tackle the data quality issue in the investment case study (i.e., ""Analytics"" spelled as ""A0lytics"" and so on) we could try following the below method: Create a unique set of the various categories from the category_list column of the mapping column dataframe say set1 Create unique set of the various categories from the category_list column of the master_dataframe say set2 Set difference set1 and set2. This will give you a list of categories present in the mapping daatframe but not in master_frame (which is very wierd as any category mentioned int he master_frame.category_list should be coming from the mapping.category_list). So, this leads us to the data quality issue and we notice the pattern of ""na"" being replaced by ""0"". One more technique would be to see if we can group by the values in the columns we are interested. This way we can get a summary of the various categories present in the column and hence may lead to identifying any data quality issues.",313826.0
96492,411270.0,"Choose the cells and click on ""Conditional Formatting--&gt;Color Scales--&gt;1st Option"". PS : The color coding is of not much significance here and can be ignored. PPS: Please tag the question to correct topic. This topic should have been tagged to the ""Univariate Analysis"" Module.",313826.0
96492,411388.0,"Hi Naga, Also research on ""excel conditional formatting"". The below is will take you to videos related to this. https://www.google.co.in/search?q=excel+conditional+formatting&amp;rlz=1C1AOHY_enIN708IN708&amp;source=lnms&amp;tbm=vid&amp;sa=X&amp;ved=0ahUKEwjVotXDmufeAhXIvI8KHSOwAQgQ_AUIDigB&amp;biw=1309&amp;bih=735",334535.0
95689,405130.0,I don't see any problem. It's been asked to remove all rows with missing values equal to 5.,318329.0
95689,405137.0,"You want your dataframe to be showing all the values with &lt;5 or &gt;5 null values in rows and hence !=5 (not equal to 5) which ultimately removes exaclty 5 number of missing values.. so, this is correct.. Please let me know if this still leaves you in question..",316349.0
95689,410944.0,The rows having 5 missing values should not be part of the dataframe. Hence != is the correct way,317996.0
95689,410484.0,"You need to think it like that: if you will use this : marks =marks[marks.isnull().sum(axis=1) == 5] marks Dataframe will save values of rows which have 5 values missing rather than removing those, thats why you need to use not equal to.",315560.0
95689,410448.0,"You can use the thresh parameter to solve the question, I used: marks.dropna(axis = 0, thresh = 3) to solve this question and then used the isnull().sum() to get the answer. Hope it helps. for referring to the syntax and usage of thresh above, please refer the below link: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html",305656.0
95689,405139.0,"From the above description given by you, if we use below code then marks =marks[marks.isnull().sum(axis=1) == 5] this will not remove rows having 5 missing values in marks dataframe infact it will fetch those rows having 5 missing values and store it back to marks dataframe. So, you have to do reverse of it which is given by you in above description marks =marks[marks.isnull().sum(axis=1) != 5] this will get those rows which doesn't have 5 missing values and store it back to marks dataframe. I would suggest to go through third video under Treating Missing Values in below link for more clarification. https://learn.upgrad.com/v/course/208/session/19864/segment/101071 Hope this clarifies your doubt.",317991.0
95689,411671.0,"You want to remove all rows having 5 columns missing not include them. So, your solution will keep only the values which have 5 missing values not remove them.",317689.0
96698,412400.0,"Are you talking about the python questions ? Use the URL specified, open in another window, you can download as csv",313228.0
96698,412044.0,its nsa data having students and there family and marks info with other info that is used in lecture as well .,318017.0
96698,412064.0,Kindly use the data set present in the below link: https://learn.upgrad.com/v/course/208/session/18011/segment/91636,311160.0
95637,405497.0,Good Job Shadab!!!,319696.0
96519,411342.0,you've to update your dataframe 'marks' in such a way that it doesn't show rows having exactly 5 null values. in mathematical terms it shows &lt; (less than) or &gt; (greater than) 5 null values but not exactly 5 null..,316349.0
96519,411356.0,"Look at the first assignment 'Movies', you will find all the answers of upcoming questions from that assignment.",315560.0
96519,411435.0,I would suggest to go through third video under Treating Missing Values in below link for more clarification. https://learn.upgrad.com/v/course/208/session/19864/segment/101071 Hope this will help.,317991.0
96519,411670.0,"It means to remove all rows which have exactly 5 columns as null. For marks dataset:if any row has any 5 column out of following 6 columns as nulls or missing than remove them Prefix, Assignment, Tutorial, Midterm, TakeHome, Final",317689.0
97284,416511.0,"Generally, up to 30 % missing data is acceptable. But as you get closer to 50%, you are losing valuable information. You should look at the data points and see if you can impute any part of it to get a more complete set. Talk to your customer.",308637.0
97284,416348.0,"You need to first understand that the reason behind missing data. For example if you consider the uber case study, the drop time is missing for No cars available and that is valid and need to handle it accordingly. So whichever column you need to analyse, make sure you understand the data in that column and what drives the data available for it.",317460.0
97284,416344.0,"In such cases, you need to check with Data Engineers and Domain Experts on why such huge percentage of data is missing for columns which are relevant to the analysis at hand, so that you can look at alternatives on imputing that data from a different source or some other calculation etc. It would also help in making changes in the process of data collection so that relevant data is captured.",313826.0
97284,416340.0,then the anaysis is wrong the date should be returned upto 77 percent after cleaning is done,318017.0
97284,416967.0,"Missing 40% of data in analysis is worrisome! We should go back to the design board to check on the way data was collected or the way sample Qs were asked. If we are still convinced about the data (i. e. 40% missing data is fine) then we should look at deriving more variables from domain experience before starting analysis. Hope, this helps!",318827.0
96846,413164.0,"Hey Neha, Yes you're using the correct function!! Try dp.drop_duplicates(subset = None, keep = 'first', inplace = False)",302742.0
96846,413378.0,"Also Neha, (since i faced the same prpblem at first) check whether you're dropping duplicates on complete column or that particular column of interest.. if you're dropping from complete column that will cause the length to be 0 thereby deleting all the rows.. try to drop duplicated only from Scores and see if that helps..",316349.0
96846,414175.0,Thank you Rashmi and Hemant,318077.0
96878,413396.0,you can also use :Customer.cust_id.str.extract('(\d+)') this command will extract the digits only,318017.0
96878,413367.0,"You're splitting your column without setting the expand argument and extracting the first value with index 0 and then updating your main dataframe with the first value in the same line.. you may want to treat it in two lines, one for splitting and other for extracting anf updating.. try Something like; A = customer['cust_id'].str.split('_', expand=True) customer['cust_IDs] = A[0]",316349.0
96878,413371.0,"You need to perform the split operation for the entire column i.e., apply the split function for every value in the column. For this you need to us ethe apply() function. Further, since we need to keep only the numeric part appearing after the underscore, we need to select the second element after split which is [1] in this case. Sample code to split a value by underscore and choose the second element : df['col'].apply(lambda x : str(x).split('_')[1])",313826.0
96878,413519.0,or you can use: df.colname.str.split('_').str.get(0),312490.0
96878,413736.0,"You are missing .str customer['Cust_id']=customer['Cust_id'].str.split('_',1),str[0] However, the numeric part we want would be in str[-1] So correct code would be customer['Cust_id']=customer['Cust_id'].str.split('_',1),str[-1]",301654.0
96957,413640.0,You already will have it loaded into sleepstudy variable. ALl you will have to do is use it and solve the problem. pydataset is a library that contains a lot of datasets to play around. This library is enabled on the server that evaluates your code written on upgrad. data('sleepstudy') helps load the dataframe. Refer to this link to know more https://pypi.org/project/pydataset/,318007.0
99126,422243.0,you have to ignore missing values coz it will impact the mean value and that would not be correct but keep in mind that you should not remove those rows which will pact your analysis like if you have missing value in x column amd you remove the complete row from data frame but that x column is not used in your analysis then you can keep that missing value one thing to note here is that when you do the data cleaning action you should recover 77 percent of the data if your cleaning remove more then 23 percent of rows than its not correct . hope this helps,318017.0
99126,422253.0,"Yes, that is logical and it also highly depends on the Business understanding if NULL's can be ignored or can be imputed!! but, it would always be good to avoid it if cannot be imputed.. since imputing it to 0/1 is exaggerating your data while computing mean/standard deviation.. For example; LastName, Date etc.. missing values cannot be imputed but rather should be ignored..",316349.0
97333,416564.0,Check if you're specifying the number of bins correctly.. that plays an important role here.,316349.0
97333,416613.0,"Try using below code while reading the CSV EDA_census = pd.read_csv('EDA_census_partial.csv' , sep='\s*,\s*', header=0, encoding='ascii', engine='python') I hope it works",317845.0
97333,417546.0,"Try using below code while reading the CSV EDA_census = pd.read_csv('EDA_census_partial.csv' , sep='\s*,\s*', header=0, encoding='ascii', engine='python') On using this solution as suggested above, it worked but the column names gets disappeared. When i use EDA_census.info() or EDA_census.head() , it wont show the column headers. Is there any solution to this.",310179.0
96317,410073.0,"you can 'edit' the bin size to show data in the format 0-10, 10-20 etc.. from the Runs dimension newly created when you plot the Histogram.. check the dimensions section.. and right click on Runs there.. Edit it for resizing bins..",316349.0
96317,410048.0,"After cleaning the data, I plotted the graphs in Pyhton itself in two ways: 1. Created a distplot using the cleaned ""Runs"" column. Have set hist=True and bins=250 to get a cleare plot. 2. Created a new dataframe which has 2 columns : Runs_Scored, Count. Using this dataframe, plotted a barplot with x=Runs_Scored and y=Count. Either of the plots is sufficient to answer the question. Hope this helps.",313826.0
96317,410227.0,Right click on data --- Create ---- bin--- change accordingly,303082.0
96406,410634.0,"If you are using log on both the axis, the analysis remains the same as you are using the common parameter to plot, for example, if the output is a straight line, they are directly dependentent and otherwise. For more information you can refer : https://en.wikipedia.org/wiki/Log%E2%80%93log_plot",315277.0
96412,410661.0,runs are actually in strings first you need to split and remove * then change the type after this runs will be a measure but since its a string it is taken as dimension,318017.0
96412,410667.0,"Yes, you're correct it should be in 'Numeric' measure category.. But since it has got ' * ' (asterisk, telling the player was not out in the match), the data type is understood as 'string' by Tableau.. But, you're supposed to first split the column into two.. and treat 'RUNS' as measure with only numeric attributes and other column as original with ' * ' in it.. I hope this helps.. :)",316349.0
96412,411452.0,"You need to first check the Datatyoe of runs or any column which you are going to use to plot a histogram. In this case, according to question you need RUNS , i.e you should convert it in Float but before that clean the Data (RUNS) and remove * and any other value , otherwise Datatype conversion will throw an error. After, coversion you can plot a histogram either in Python or export the Dataset in excel and plot it in Tablue.",315560.0
96412,412101.0,"Just to understand some more, once the ' * ' is removed and the data is cleaned for the ""RUNS"" column is the data a ""ordered categorical variable"" or a "" numeric variable""",317575.0
96860,413504.0,"For us to Bin a dataset, based on any particular column, that column needs to have an order. e.g consider Age. Below 25 is young 25 - 50 is mid age above 50 is old Whereas random custom id's do not have any such order assosciated with them.",317996.0
96860,413614.0,"First thing is, they're randomly generated. So, it's pretty useless. Any patterns that might be seen there might can only be due to chance. Second, it's a categorical variable. Which means that it's already ""binned"" (in a certain sense). So, why'd you want to bin it?",306733.0
96860,413710.0,Binning is to be done where you can group the data for large observations. But Unique customer id is unique for each customer so one customer id belongs to only one customer and is useless for other customer so there is no use of grouping thoose ids.,305651.0
96860,413314.0,"Data binning (also called Discrete binning or bucketing) is a data pre-processing technique used to reduce the effects of minor observation errors. The original data values which fall in a given small interval, a bin , are replaced by a value representative of that interval, often the central value. customer id is not a good choice for binning you should bin on some value which is a dimension and have repeated values in it like age , age will have repeated value and you can bin it even if they are random",318017.0
97361,416823.0,If Your python file is in the same local drive.. then direct pd.read_csv will work.. Or else you will have to give the file location..,305129.0
96558,411487.0,"i guess the important point in energy example was related to the spikes on every round figured values 10, 20, 30, 40, 50 etc.. and the reason as Sir explained was mostnof the operators are not actually visiting the Customers site to note their meter reading but rather manipulate the reading.. anf mostly human tendency on manipulation is to make reading close to the rounded off values.. and hence we see a high spikes on every rounded off values.. whereas other wise you'll see a very nice increasing or decreasing trend.. this was the interesting fact that Sir shared in the example.. i cannot remember of increase in 10 to 50 but it could be because of less units generating less bill! not sure.. but the interesting insight that i captured from that example is mentioned above..",316349.0
96558,411485.0,"the peaks at units where the energy slab (based on usage) changes is mostly due to rigging of energy meters to avoid the significant increase in cost per unit electricity for each successive slab. But there are some peaks even at units where energy slab does not change. this is due to random values of units assigned to some customers by people supposed to take readings of these meters. this maybe due to any reason like avoid travelling, laziness, physical difficulty in reaching the place etc. in such cases when assigning random values it is most likely to be a rounded off figure like 10-20-30 etc.. and hence we can find peaks at these points.",319898.0
96558,411790.0,"The spikes at 10,20,30,40 are cooked numbers without actually taking the reading due to various reasons. As, prof. explained in the video that there is a general tendency to cook rounded off numbers, hence the spikes.",317689.0
96965,413838.0,"As a workaround, use column_name.astype(int). Can you please share the df.info() output that you are seeing?",311160.0
96965,414294.0,"Apart from blank/null values, take care of 'DNB', 'TDNB' and values ending with '*' such as 200* in the Runs column. This should be followed by changing the data type of the column.",306726.0
96395,410580.0,you will have to plot a histogram with runs as the bins to get the most runs scored segment .,318017.0
96395,411152.0,I was able to find the answer in excel using the Frequency function,317996.0
96395,411241.0,convert runs to float and do the same plot,318756.0
96395,412489.0,Convert the Runs field to numeric and then plot a histogram with bin size as 10 and also there is no need to use the log scale.,304319.0
96968,413831.0,it showing only 76.8percentile only there is no 78 percentile?,310624.0
96968,413761.0,"Hey Rasheed, Your answer will be numeric. You have to mention the count of shares that falls at the 78th percentile!",302742.0
96968,414122.0,"Hi, The question basically wants you to find 78th percentile of data i.e. ' shares' column. Answer is numeric. Hope this helps :)",300730.0
96968,414108.0,Yes add the numeric value as an answer,301641.0
96805,412774.0,"use something like this : df.columns = pd.Index(map(lambda x : str(x)[:0], df.columns))",318017.0
96805,413076.0,"Hi Chetan, the issue is the strip command. strip basically removes the leading and trailing spaces. If you want to remove all spaces, you need to use df[:1,].str.replace("" "","""") Also, you can refer to the below link for using different types of strip and in which conditions you should choose which one. http://www.datasciencemadesimple.com/remove-spaces-in-python/",318397.0
97023,414142.0,You can just remove the rows which are above 95 percentile. Then calculate the mean on the new data frame.,301648.0
97023,414117.0,Even I've faced the same problem by following the method in the link provided. Remove the outliers above 95 percentile and you will find the answer in the options.,318329.0
97023,414234.0,You have to remove the outliner above 95% percentile and calculate the mean. you can use df['column name'].quantile(percentile) to calculate the percentile You will have to remove rows above 95 %. Once you have final data frame you can calculate the mean,317845.0
97023,414353.0,"First, you have to remove outliers above 95% and create a new dataframe. After that, you can take mean for the new data frame.",318532.0
96153,409125.0,Check this out https://en.wikipedia.org/wiki/Rank-size_distribution,310974.0
96188,409251.0,"Hi, You need to use python to solve the problem. You can use tableau for data visualisation part.",319696.0
96188,410455.0,Python to be used for problem solving &amp; Tableau for data visualisation. In the exam more of Python would be asked more so better to practice it the correct way by solving in Python. Good luck..,310508.0
97089,415037.0,I am not sure of the video but yes y = cx^b in linear equals to y = bx + c in log scale,318329.0
97581,417583.0,Univariate analysis is primarily to check the distribution of one variable. Based on The graph that is plotted you can check whether the distribution is normal or skewed. That will just give you a visual intuition. For the actual values you can do a describe.,318084.0
96090,408504.0,you can get any percentile using the method quartile if that is what you are looking for.,318329.0
96090,408954.0,"Check below - sns.boxplot(y=[0.5,0.10,0.25,0.50,0.75,0.95]) plt.show() s=pd.Series([0.5,0.10,0.25,0.50,0.75,0.95]) s.quantile([0.25,0.5,0.75])",318458.0
96090,411697.0,Yes we can get any percentile with quartile function in Python,304693.0
96189,409282.0,DNB is do not bat and 0 is that he scored 0 it will definitely impact your analysis as things like average will be different in both of the cases . hope this helps 😁,318017.0
96189,409331.0,"In my opinion it depends on for how many no. of innings the batsman did not bat. For example:- If a batsman has played in 100 matches and if it DNB for 5 matches then it would not affect the batting average much. And putting 0 in place of DNB might be a fair option. But if a batsman DNB for say 30 matches then it will definately affect the batting average and putting 0 in place of DNB would not be a good practice. And for 4s column it is fair that you put 0 if a batsman DNB, because if he did not bat then its clear that he could not hit 4s' and putting 0 is good practice. Hope this help.",317991.0
96189,409806.0,"how did you do the replacing of DNB and 4s ""-"" to 0",312199.0
96189,410923.0,Replace DNB &amp; TDNB with 0 values,314183.0
96189,411728.0,"There is no DNB in 4s column only , "" - "" is there, you can replace it by string to float replace method and replace it by "" 0 "" ., and then plot a bar chart or distplot or histogram to find the answer.",315560.0
97681,417972.0,Are you using pd.read_csv?,318329.0
97681,418029.0,Attach a screenshot if the code that is throwing this error.,313826.0
97681,418208.0,"Hi Atulyan, Check out the recommendation towards the end in below link https://github.com/pandas-dev/pandas/issues/11493",334535.0
95598,405350.0,Try Including the 95percentile value while removing ouliers. ie retain data where shares &lt;= value for 95 percentile.,310511.0
95598,404565.0,Mean value is average and not necessary to be present in the list. Perhaps you are looking for median value.,306248.0
95598,404793.0,"Hi Nagaraju, As you have mentioned that question is specifically asking for the mean. You have removed outliers because it would have serious impact on the mean value. Regarding your query that mean value is not in the list of options. I don't find any option of mean values in the above link. But, assuming list you are referreing to, as the array of numbers whose mean value is calculated, it is not necessary that mean value will be one of the numbers in the given array.",319696.0
95598,405592.0,"Some additional links have been provided for removing outliers. In the original link, the outliers are removed using the concept of standard deviation which uses mean as a central measure whereas in this case, you need to use the concept of percentiles to remove them(this uses the median as a central measure). Therefore, both methods will yield different mean values.",313517.0
95598,411700.0,Mean is an average value of shares before and after removing outliers so it wont be there,304693.0
98351,418473.0,"Yes, that's because the lambda method you used returns only true or false. You could use something like this popularity_df[popularity_df.shares &lt; popularity_df.shares.quantile(0.95)]",318329.0
98351,418497.0,You can use the below code: popularity_df[popularity_df['shares'].apply(lambda x: x &lt; popularity_df.shares.quantile(0.95))],311160.0
96202,409328.0,TAs are recommending to use python to solve the problem &amp; tableau for data visualisation part. Please check - https://learn.upgrad.com/v/course/208/question/96188,318458.0
96202,411695.0,Analysing should be done in Python then we can use Tableau for visualization. Doing this in Python will help you for case studies going forward,304693.0
96202,412053.0,"You need to do that question in Python , as it won't be possible to do it in Tablue. Mode means the most common occuring value in a Column of a Data Frame. So, you need to focus on a particular Column of the Dataframe and count the number of most(highest occuring) number value in the column which is in Question.Perform some Data Cleaning on that column also. For more info look into Assignment solution provided by Upgrad on Data Cleaning , the answer is more or less explained there. Hope this helps.",315560.0
96443,410826.0,"A look at the "" 3. Shares vs Channel type "" plot indicates that the "" Social Media "" Articles ( labelled "" data_channel_is_socmed "") is clearly the most shared article type. Further, "" Social Media "" Articles, the 75th percentile is way above the 2500 mark and the median is approximately near the 2000 mark. Approximately 60% of the social media articles have been shared 2500 times. Hope this clarifies.",313826.0
96443,411721.0,Ass the box plot indicates in social medi 60 % of the socail medai articles have been shared 2500 times make it highly likely.,318476.0
96443,411997.0,"Box plot for social media is having 75th percentile higher than 2500 and its mean is closest to 2500 than any other articles. So, it seems like approx 60% of social media articles have 2500 shares which is pretty good chance.",317689.0
96674,411847.0,For the graded questions analysis.. we have to use python and hence cleaning can be done there.. instructions are given so.. And the questions before the graded questions.. Tableau can give better visibility..,305129.0
96674,412249.0,"Python... Remember that we are trying to solve big data problems. (not so big right now, but...) Also, for some questions, we need more control.",308637.0
96674,412347.0,"These questions are for our own practice. Whichever tool you feel you need more practice, choose that one. And if time allows, do it in all.",304319.0
96674,411892.0,you can use either of the two using Excel also you can solve a problem for univariate analysis,318017.0
96674,412689.0,"Do not stick to a tool. These questions here seem to be for a quick analysis and one should be able to use any tool of their comfort to tell the answer quickly. If you can tell it using Tableau or Python or Excel, it is up to your comfort.",318007.0
96674,411865.0,We can do plotting in python as explained in the visualisation course.,317689.0
96674,411914.0,"I think for the non-graded questions, excel is fine as Anand sir has also done the univariate analysis in excel.",317460.0
96674,412056.0,"Go for Python only for both Data Cleaning and Data Visualisation. Its more precise and easy and you also wont get confused in importing Data in tablue, PS : Data Cleaning is only possible in Pythin. However, for visualisation you can opt for Tablue. Enough answers are given on this platform on how to visualise in Python , look for those. Those are really helpful and easy to get.",315560.0
96674,412161.0,Its adviced to do this in Python for future Assignments or case studies,304693.0
96674,412187.0,Python is best option for same https://learn.upgrad.com/v/course/208/question/96188,310952.0
96674,413700.0,Use Python ..,300735.0
96674,413713.0,You should try to use Python. Excel can be used for answering questions but if you use python you get more familiar with the syntax and the more you use the more hands on you get.,317460.0
96674,415621.0,"tool doesnt matter, what matters is your analysis, do it in python if you need to practice. otherwise excel or tablue also will be fine",302735.0
95728,405349.0,Yes. And then calculate mean.,310511.0
95728,405430.0,It's been asked to remove the outliers using the method followed in the mentioned link and it says retain values between mean - (2*std) and mean + (2*std),318329.0
95728,405597.0,"Some additional links have been provided for removing outliers. In the original link, the outliers are removed using the concept of standard deviation which uses mean as a central measure whereas in this case, you need to use the concept of percentiles to remove them(this uses the median as a central measure). Therefore, both methods will yield different mean values.",313517.0
95728,412244.0,Yes. Please use correct logical operator for filtering and understand the links provided. Actually the solution is a lot more easier than what the links describe. You can use a specific function for faster processing as described in stack overflow link provided.,308637.0
95728,413022.0,"once you remove the outliers, you will get a new data, please do the mean of the new dateat to get the new mean of the number of shares after removing the outlier.",320606.0
96732,415579.0,"Hi Chetan, Are you referring to below link https://learn.upgrad.com/v/course/208/session/18014/segment/91664",334535.0
96732,415580.0,"Hi Chetan, Please also review additional material on link https://learn.upgrad.com/v/course/208/module/7359",334535.0
96732,412322.0,"It just gives an Idea. For better understanding first solve it in excel. Calulate Lower Percentile Outliers and High Percentile Outliers , remove these values from main column where you are calculating Ouliers and then calculate the mean.",315560.0
96740,412309.0,"In this case, variable X refers to the numbers of 4's hit by Tendulkar. You can find it by identifying the frequency of 4's by using value_counts().",311160.0
96740,412501.0,Most common value depend on the type of variable you are analysing. For a Univariant varibale is can refers to the frequecy dstribution i.e count of the values appers like hitting 1 4s five times in five innings show frequecy of 5.,318476.0
96920,413502.0,data.columns will get you all the columns name you have with the dataframe.Please look if the shares is present in the columns is present or not,318476.0
96920,413483.0,do a strip action on the column,318017.0
96920,413610.0,"The question description gives a clue in the Note. The df[‘column_name’].mean() function is used to obtain mean of a particular column.( Note : The ' shares' column has space before the first s ). You can verify this by using dataframe.columns So, use a space while mentioning the column name in the square braces. eg: df['column name']",318007.0
96920,413525.0,data = pd.read_csv( r 'C:/Users/shruthi.hr/Downloads/popularity.csv') this might help if you are not able to load the file from the path. /U in file path may cause this error. r prefix forces it as raw string.,312490.0
96920,413630.0,You need to put space ' shares'.,315679.0
96920,413936.0,In Details is says a Note... Note : The ' shares' column has space before the first s). https://learn.upgrad.com/v/course/208/session/18014/segment/91663 First remove that sapace from csv file then use it.,310952.0
96920,413465.0,"The column name has a space in the beginning. So, try using ' shares' instead and it should work.",313826.0
101332,436670.0,"Hi Aman, These are just optional practice questions so that anyone who's interested in practising R can attempt these. You can skip these if you feel like. As for python is considered, the complete program and assessments are in python and will help you hone your programming skills. Additional practice content for python has been shared already for python here - https://learn.upgrad.com/v/course/208/module/6685 Request you to go through it thoroughly. Happy learning :)",301618.0
126768,558276.0,"Assume a Linear Regression equation - y = a + bx1 + cx2 + dx3 Linear in Linear Regression means that the coefficients of features (b,c,d) are linearly related to the target variable(y). In case of linear regression the only two operations that can be applied between the coefficients are multiplying them by constant or adding them. But we cannot raise them to power or multiply them with each other in which case it won't be linear anymore.",312063.0
126768,552845.0,"Here comes the explanation nicely given by Professor; It is not necessary for the raw attributes to be linear to target variables but their co-efficients. This chapter deals exactly with what you had asked; I suggest you go throgh this. https://learn.upgrad.com/v/course/208/session/31155/segment/164578 Summarising the important takeaways from the lecture: In generalised regression models, the basic algorithm remains the same as linear regression- we compute the values of coefficients which result in the least possible error (best fit). The only difference is that we now use the features ϕ1(x),ϕ2(x),ϕ3(x)....ϕk(x) instead of the raw attributes. The term 'linear' in linear regression refers to the linearity in the coefficients, i.e. the target variable y is linearly related to the model coefficients. It does not require that y should be linearly related to the raw attributes or features - feature functions could be non-linear as well . 'Linear' in Linear Regression The second point mentioned above is so important (and so often confused) that it is worth elaborating - the model is called 'linear' because the target y is linearly related to the coefficients . To fully understand this, it is crucial to note that in regression, the coefficients a0,a1,a2,...,ak are your variables , i.e. you are trying to find the optimal coefficients which minimise some loss function. On the other hand, the features ϕ1(x),ϕ2(x),ϕ3(x)....ϕk(x) are actually constants since you are already given the dataset (i.e. the values of x, and hence ϕ(x), are fixed - what you are trying to tune are the coefficients).",301121.0
126768,552388.0,"""Can we create a linear regression model for the same?"" Yes, we certainly can. Quoting from a Quora answer (Link given below. Do go through it, it explains the 2nd part of your question also.) ""In the simplest regression, univariate linear regression, if the independent variable is uncorrelated with the dependent variable you will get a Beta of zero. A Beta of zero is as meaningful as any other Beta. People sometimes confuse knowing something is zero with not knowing anything, but that is a fallacy."" Source: https://www.quora.com/Does-the-predictor-variable-have-to-be-correlated-with-the-dependent-variable-in-a-linear-or-any-type-of-regression-Is-it-a-necessary-condition ""What exactly does linear in Linear Regression mean"" I remember this explicity being explained by the proffesor in our course. (do go through that part again for revision) From what I remember, Linear in Linear regression implies that the predictor/feature variables are Linearly Related. For Example, if F1,F2,F3, are your feature (predictor) variables and c1,c2 and c3 are co-efficients then they can be related as: c1*F1 + c2*F2 + c3*F3 OR, c1*F1 + c2*(F2 +F3) BUT NOT like following, c1*F1 + c2*(F2 * F3) since that would NOT be linear (note how F2*F3 would make it a quadratic equation). So, that is what is meant by Linear in Linear Regression, that the feature variables are linearly related. Hope that helps.",317998.0
135743,,nan,
104511,,nan,
120924,524954.0,"Hi Raveena, This will depend on many things. For example on how many parameters you selected for the grid search. On your system configuration like RAM and processor etc. So tips to reduce the running time is try to use all the cores of your CPU and do parallel processing. Try reducing the number of hyperparameters considered for the grid search. Try reducing the train dataset size as well. Alternatively, try using cloud infrastructure if local resources are taking a lot of time. Try using google colab or other cloud services.",344353.0
120924,524957.0,"Use ""n_jobs=-1 "" as one of the parameters in GridSearchCV functions. It means that the computation will be dispatched on all the CPUs of the computer.",302740.0
120924,525604.0,"After using ""n_jobs = -1"", GrisSearch returned the output in about 25 minutes for me. Macbook with 4GB RAM.",319302.0
138146,596225.0,"Yes Arpit, PLease follow the below link for more details https://stats.stackexchange.com/questions/55072/svm-confidence-according-to-distance-from-hyperline",344894.0
138147,596247.0,"Hi, SVM works on binary classification concept. When we want to solve the multiclass problem, then SVM will take one class as a primary class and all other class will assume as one secondary class and will create hyperplan between them. Thanks",344894.0
134325,584414.0,RFE can be considered as a wrapper method which is build on top of other model based selection algorithms such as SVM or regression. So it is based on the idea to repeatedly constructing a model (for example an SVM or a regression model whichever is chose for model building) and repeat the process on different subset of features to drill down to best performing subset of features.,316147.0
134325,583841.0,RFE is feature selection method. So it is not mandatory that the algorithm is Linear Regression only. It can be used with other machine learning algorithm also.,317991.0
134325,583872.0,"Hello Rahul, Rather than feature selection, RFE, is used, in fact, to eliminate the features based on their least contribution to predictors. It need not be linear but RFE will not work without feeding the coefficient value Higher the absolute value of coefficient then feature is considered to be important to be selected by the model. Please note that scaling is of extreme importance for RFE.",301121.0
116639,504513.0,"Lambda is the balancing factor. If we are choosing a high value for lambda, it implies we are giving more importance on making the model simple (one of the way is to make model coefficients simple). Simlarly, if we are choosing a low value for lambda, it implies we are giving more importance to bring down the error(RSS). In this case we are not giving any emphasis on making the model simple. Please refer the TA verified answer in below link to get more details https://learn.upgrad.com/v/course/208/question/116442/answer/503382",310467.0
116639,505339.0,https://www.coursera.org/lecture/ml-regression/how-ridge-regression-balances-bias-and-variance-697eG,311404.0
118097,509008.0,it is a package that you can import from sklearn library,311686.0
117874,508371.0,you can refer the following link for TA varified answer for similar question https://learn.upgrad.com/v/course/208/question/117882,302738.0
98345,418417.0,Do not use lambda as it would be applied on all rows. Either apply lambda only on rows which are non null (Filter) or use column.dt.date and so on,318329.0
98345,418476.0,Thanks...it worked...,318479.0
97335,,nan,
99579,426676.0,Its available here. https://learn.upgrad.com/v/course/208/session/19535/segment/99359,318368.0
99579,426020.0,Its not yet released as of now. Whenever the solution will be released you will see session 2 for Uber assignment solution in below link: https://learn.upgrad.com/v/course/208/module/6563,317991.0
97327,416541.0,Launch anaconda navigator and go to Environments tab and click on Updatable on right hand side and you can update seaborn to 0.9,318429.0
97327,416608.0,You can try below steps: 1. Open anaconda prompt 2. execute this command : python -mpip install seaborn==0.9.0 You can also follow below link https://stackoverflow.com/questions/51422146/install-the-latest-version-of-seaborn-0-9-0-through-pip Do let me know if this works or not ?,317991.0
97329,416536.0,"Hi, Please refer the below link.Hint is format is not mandatory .try without the format. https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime raw_data['Mycol'] = pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')",300687.0
97329,416563.0,you can simply convert the datatype to datetime for your complete column and it will be applied across the Series.. you don't need to specify the format,316349.0
97329,416812.0,import datetime convert pd.to_datetime() and select the write code.,308639.0
97331,417061.0,"Yes, Demand is the total number of requests (Trips Completed + Cancelled + No Cars Available) and Supply is the total number of Trips Completed. Gap is simply the difference between Demand and Supply",316202.0
97331,416554.0,gap=totaldemand-total supply. You 've to calculate total trip time &amp; total trip completion time. Then gap= total trip time-total trip completion time. Hope it will hep you.,320689.0
97331,416705.0,"Demand is the number of requests of types of status. Supply is what Uber could serve its customers i.e., the completed trips",318329.0
97331,416742.0,Demand = number of cab requests done. i.e. requests done while trip completed + trip cancelled + cabs unavailable Supply = Number of cab availability. Only trip completed is considered as supply.,306245.0
97331,417077.0,Demand is the Total numder of request = Trip completed + no car available. Supply is the total trip completed. Gap is the difference between Demand and supply.,314183.0
97331,417119.0,Demand = number of cab requests done. i.e. requests done while Trip completed + Trip cancelled + Cabs unavailable Supply = Number of cab availability. Only trip completed is considered as supply.,315560.0
99386,424452.0,Check this existing Q&amp;A https://learn.upgrad.com/v/course/208/question/98398,318762.0
96306,410789.0,You need to do two things here: find the time slots where the most demand is observed and what type of demand is observed( cancellation or no cars available). Then compare it with the number of trips that are getting successful to get an idea about the supply-demand problem.,313517.0
97211,416034.0,The data is only for few days. So it is correct. You can also verify by checking the date range.,311857.0
97211,416035.0,This dataset is for 5 days only. I think this is mentioned somewhere in the video.,317689.0
97211,416040.0,the dataset is only for 5 days for a single city,318017.0
97211,416041.0,Thank you all.,300687.0
97211,416114.0,it's as expected,318732.0
97211,416805.0,You have to work on only five working days Monday-Friday.,308639.0
97352,416651.0,You have to clean the data. You have to format the date in one format.,318429.0
97352,416670.0,"In a problem statement hints are given as follows: Data Cleaning and Preparation - Hints Identify the data quality issues and clean the data so that you can use it for analysis. Ensure that the dates and time are in the proper format. Derive new variables which will be useful for analysis. You have to apply all the learning which you have gain during Data Cleaning and Preparation of EDA(if applicable). For ex:- it could be missing values, wrong values, inconsistent format of date etc. You need to take care of all this whichever is applicable. Hope this will help.",317991.0
97350,417453.0,"The simplest way to think about it is: what is the logical gap? The gap is the unserviced demand, which in this case is No cars Available + Cancelled",319357.0
97350,417245.0,Demand is the total requests which will include Trips Completed + Cancelled + No Cars Available. Supply is the total trips Completed.,301643.0
97350,417037.0,"i think Supply = Number of cab availability. (i.e. Trips completed + Cancelled) as even for cancelled cases the Cab was available, but it was the user or driver who cancelled it. so based on my understanding only no Cabs available should be condidered as supply gap",316036.0
97350,417073.0,Demand is total requests placed supply is the trips completed only,304693.0
97350,417060.0,"Yes, Demand is the total number of requests (Trips Completed + Cancelled + No Cars Available) and Supply is the total number of Trips Completed.",316202.0
97350,416661.0,"Hey Khusbu, Refer this - https://learn.upgrad.com/v/course/208/question/96306 Hope this discussion will help.",302742.0
97350,416744.0,Demand = number of cab requests done. i.e. requests done while Trip completed + Trip cancelled + Cabs unavailable Supply = Number of cab availability. Only trip completed is considered as supply.,306245.0
97350,417157.0,Demand is Total request and supply is trip completed.,314183.0
98303,418221.0,Plots with Analysis results will be good enough.,303673.0
98303,418283.0,You need to make plots and accordingly your calculations should done for finding out the supply and demand gapping ! So that you can conculde &amp; highlight your answers for problem 1 &amp; problem 2 more emphasised,311466.0
98303,418237.0,Evaluation Rubric talks about 'Numbers being correct' and also about mentioning the logics used.. So it wil be better to have calculations are also explained in the Python file as comments. In PDF only plots will suffice I think.,311686.0
97682,417971.0,"Check your version of seaborn, if it is below 0.9 then you need to upgrad to 0.9 version of seaborn. Please go through this link. This might be of help. https://stackoverflow.com/questions/51402579/module-seaborn-has-no-attribute-any-graph https://stackoverflow.com/questions/51846948/seaborn-lineplot-module-object-has-no-attribute-lineplot Hope this will help.",317991.0
97682,418008.0,"Hi Shashwat, Vipul is absolutely correct. This can be confirmed in Seaborn release notes. You can get release notes here, https://seaborn.pydata.org/whatsnew.html (Check for API changes in version 0.9.0 (July 2018))",307495.0
97682,418313.0,To update seaborn: 1)open anaconda navigator 2)go to environment tab 3)click the playsymbol on base(root) and choose open terminal 4)run this command : conda remove seaborn 5)After it is successfully uninstalled i.e. you can check by searching the package seaborn in the search option then install the new package 0.9.0 using the command conda install seaborn=0.9.0,301114.0
97367,416896.0,"Hi , You can use temp variable to save the results as dataframe. You should take care of indexes while saving as dataframe , below link should help. https://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-object-to-dataframe",305652.0
97367,416822.0,Hi.. converting the groupby to a dataframe will work..,305129.0
97367,416932.0,"Approach mentioned by Abhishek is correct, however we need reset the index. In this case, the old index is added as a column, and a new sequential index is used. After this, you can use newly created dataframe for plotting. 2nd approach can be DataFrameGroupBy.plot . You will find more deails on documentation. In this approach you dont need to create new dataframe.",307495.0
97369,416780.0,if you think it is unwanted.. you can delete it.. telling the exact answer to your question wouldn't be fair since it is part of Assignment.. anyways we'll be playing completely with hour data only.. so think wisely and take a call on your own.. :),316349.0
97369,416782.0,Since this is an assignment can't answer directly but can give you approach. Problem hints states as follows: Ensure that the dates and time are in the proper format. Derive new variables which will be useful for analysis. So you have to decide whether second portion is useful for analysis or not. Also you can go through below link for TA's response and check whether second is required or not. https://learn.upgrad.com/v/course/208/question/96081 Hope this will help,317991.0
97375,416814.0,You can also visit below link for TA comments on similir kind of question. https://learn.upgrad.com/v/course/208/question/96081 Hope this will help.,317991.0
97375,416793.0,"Do we need to calculate the time slots as . Morning/Noon/Evening/Night for Plotting - Yes you can, to make your analysis specific and simpler How to do that - You can extract the hours from the Request Time column and can bin them to the number of ranges which you think is ideal and accordingly, you can create a new column and assign bins to be Morning/Noon/Evening/Night etc",311160.0
97376,416820.0,Yes both columns have different format.. which you will have to define while converting into datetime format.. once you convert the columns to datetime format.. both will be in same order..,305129.0
97376,416817.0,Problem hints are as follows: Data Cleaning and Preparation - Hints 1. Identify the data quality issues and clean the data so that you can use it for analysis. 2. Ensure that the dates and time are in the proper format. Derive new variables which will be useful for analysis. Its already mentioned that dates and time should be in proper format. Its upto you till what level you want to perform data cleaning. Hope this will help.,317991.0
97376,416795.0,I personally have not felt any further need of cleaning the data,311160.0
97376,416971.0,Good start! Look out for getting hours from date timestamp fields.,318827.0
97376,416993.0,"Convert the datatype of the column and you'll get the uniform date time. Try picking up Hours, Weekdays, etc for analyzing the data on these basis.",312953.0
97379,416819.0,Yes.. both has to be datetime format for Analysis..,305129.0
97379,416818.0,"Yes, it should be fine as long as both the column has consistent date format.",317991.0
97379,417058.0,"Yes, it is fine as long it is consistent. You can simply use datetime function to convert these and it works well for the analysis",316202.0
97111,414856.0,"Looking at the Problem and the questions we need to answer, it is safe to consider the time till minutes precision.",311160.0
97111,415978.0,"Hi Srivasthav, 1_ It depends on your respective analysis of case study but if you take seconds into consideration then it wil be bit complicated to deal. So it not required to deal with seconds",318732.0
97111,414874.0,you can seperate them using split function in python or using column to text in excel and give - as delimiter. for day month and year we have function in both excel and python,318017.0
97111,415448.0,"Hi, Please refer the below link.Hint is format is not mandatory .try without the format. https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime raw_data['Mycol'] = pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')",300687.0
97111,414910.0,"On conversion of these columns to datetime format, the data where the ""seconds"" precidion is not available would be assigned a default value of zero seconds, which I believe is a very godd approximation. Moreover, the analysis does not require a drill-dow to the ""seconds"" level, and hence can be handled in a convinient way with proper justifications.",313826.0
97381,416816.0,Yes if the pickup point is City means the trip is City to Airport and if pickup point is Airport then the trip is Airport to City..,305129.0
97381,416813.0,"Dataset is only for the trip either from city to airport and airport to city. So yes if pickup point is ""city"" it means the cab request is for ""city to airport"". And if pickup point is ""airport"" it means the cab request is for ""airport to city"".",317991.0
97381,417120.0,"It means that the Request is made by a commuter in the City, that he wants to go to the Airtport, Thats why his pickup poin tis City.",315560.0
97118,415829.0,Don't open source file with Excel as It formats the dates. Read the file in python directly to have original format,304693.0
97118,414954.0,explore Request Timstamp column....call that particular column.,318741.0
97118,415007.0,check all the values of request timestamp. There are some /'s and some -'s,318329.0
97118,415030.0,See the row 5 and row 6 you shpuld see the difference in the formats,300727.0
97118,415052.0,"If you open the files directly in excel, then you will not see the different date formats as excel will automatically format them to a standard format. Check by either import the csv in Python dataframe or by opening it directly in Notepad++ or Notepad.",313826.0
97118,415398.0,Thanks Vinay.. In that case should I go ahead as there s no need to change the date format? May be a student mentor can answer this. I downloaded the Uber dta on my PC and from there uploaded for analysis.I did not see any error after convering Request_timestamp column to datetime type,300698.0
97118,415552.0,"Yes, there are differences. But no issues while converting from object type to datetime.",312479.0
97118,415977.0,Its advisable to do in Python only by using timestamp as it is most important type of Data Cleaning operation on date time which will be common in most of the case study and analysis.,318732.0
97118,416059.0,"Actually, there are more issues with the time format. As suggested, checking data in a text editor or in pandas data frame is a good idea for understanding the problems. Also, we are advised to have the full solution in a jupyter notebook. So, we need to handle in python only...",308637.0
97118,416360.0,"This issue may be caused due to modification of data. When the file is read in Python, it is observed that the date has two different formats.",318788.0
97392,416862.0,you can put the grouped df into a variable and use it in any dimensional value,318017.0
97392,416928.0,"Approach mentioned by Deval is correct, however we need reset the index. In this case, the old index is added as a column, and a new sequential index is used. After this, you can use newly created dataframe for plotting. 2nd approach can be DataFrameGroupBy.plot . You will find more deails on documentation.",307495.0
97392,417051.0,You can also refer to the below link for more help on plotting a groupby dataframe: https://stackoverflow.com/questions/23415500/pandas-plotting-a-stacked-bar-chart,316202.0
97395,416861.0,you can give time using lambda function to any of the field which has time value,318017.0
97395,416865.0,Use pd.cut to derive time slots.,318429.0
97395,417045.0,You can also use a simplpe .loc function to create a time slots. You can assume a time range for which you can use the .loc function and assign a name to the time slots accordingly.,316202.0
97395,417321.0,use pd.cut,314678.0
97395,417099.0,You can follow the discussion https://learn.upgrad.com/v/course/208/question/97375 https://learn.upgrad.com/v/course/208/question/96081,332326.0
98308,418252.0,"Hey Arjun, TA has confirmed to include that in the PDF format of PPT Refer this discussion - https://learn.upgrad.com/v/course/208/question/97695 Hope this helps!",302742.0
98308,418285.0,you have to mention it in the ppt,318017.0
98308,418353.0,"Yes, We have to write 100 words about gap between and supply in PPT and then convert PPT to PDF",318461.0
97131,415827.0,python only,304693.0
97131,415649.0,We are supposed to EDA as much as possible in Python. Excel can be used for double cheking the results if you want to be double sure.!,310508.0
97131,415631.0,Only in python,318328.0
97131,415110.0,It has to be done in python only. I would recommend not to even open the csv file directly using excel as it messed up my formats after loading into data frame.,310974.0
97131,415065.0,I think the conversion has to be done in Python only.,313826.0
97131,415157.0,As per my understanding We must do the conversion in Python only.,300687.0
97131,415546.0,Python only for sure. They say that the code we write has to be applicable to their raw data so the csv should be completely untouched.,316416.0
97131,415652.0,"clearly in python because 1) it will help you learn python 2) python is far more powerful tool than excel, and in real world scenarios more than one data cleansing operation needs to be done, and doing them using one bteer platform is advisable 3) Doing in python would help you achieve automation. If you receive multiple csv from multiple sources, then I hope you would not prefer doing manual operations in multiple files",304814.0
97131,416054.0,"The entire solution must be in the jupyter notebook. So, Python only.",308637.0
97131,416080.0,"Here is what the Submission page says: Make sure you have not made any changes to the original dataset provided to you . Your Python code should work on the dataset given to you as part of the problem statement. You are not allowed to make modifications in the dataset using excel and then use it in your Python code . Entire data processing must be done in Python only . During grading we will be running your code on the dataset provided by us, in case your code gives errors with that, then marks will be deducted accordingly",309211.0
97131,416127.0,They have mentioned in question that we have to it in Python and submit the .ipynb file,306011.0
97131,416404.0,Python can be helpful in accumulating data from various sources simultaneously and merging them to form a single dataset so it is better to use Python for Data Analysis.,300706.0
97131,416535.0,"Let me end the confusion here, during the conversion we need to focus on the date in such a way that Dates format match each other , eg .YYYY-MM-DD , i.e. every date entry must follow this pattern, after importing and fixing Dates.There are some methods explained for this on the link provided by upgrad. Also , you can check below this youtube link for the same : https://www.youtube.com/watch?v=igWjq3jtLYI&amp;t=69s Hope this helps",315560.0
97131,416550.0,Hi You 've to handle it through python because assignment is submitted through python. During evalution they will check original excel. Hope it will help you.,320689.0
97406,417076.0,"You need to consiber both, only then you will be able to infer where Uber is making a loss and where it needs to improve its cab availability . However, if you watch the upgrad Video with more attention you can easily guess that a comparison is needed between the two, only then you can deduce where improvisation is needed.",315560.0
97406,416902.0,You have to consider to and from— city to airport and airport to city..,305129.0
97406,416894.0,"It's written to and from. So you need to consider all the requests since this dataset has only two pickup points, City and Airport",318329.0
97406,416960.0,"We need to consider to &amp; fro from airport to city as pick up points. While plotting graphs too, we need to utilize both these parameters. Hope, this info helps!",318827.0
97406,416944.0,without considering both to and from we cannot analyse gap between to and from supply. so we need to consider both.,304693.0
97406,416941.0,"As it is mentioned 'To and From' , It means when pick up point is Airport to City or City to Airport. So all the data in the dataset should be considered for analysis.",317460.0
97406,416992.0,You need to consider both City as well as Airport pickup points to extract meaningful analysis at what time and place were the drivers cancelling the cab requests.,312953.0
97406,416996.0,we need to consider both,320606.0
97406,417035.0,"The problem statement should have been clearer to state that the Uber data file provided (the CSV file) contains trips to and from the airport only. Without that it is not clear that if pickup point is 'City' then we can assume that the drop-off point is 'Airport' And this statement does not help clarify things either because it does not directly reference the CSV file - so it is open to interpretation what is the target of this statement: ""Note: For this assignment, only the trips to and from the airport are being considered.""",300694.0
97406,417154.0,"We have to consider airpot to city and city to airport. This statement is given: ""Note: For this assignment, only the trips to and from the airport are being considered.""",314183.0
97406,417371.0,"You need to consider both, in short consier the whole dataframe as it only contains pickup point as either city or airport.",318448.0
97406,418005.0,We need to consider both City &amp; Airport as pick-up points. Further data analysis in Python should consider both these pick-up points. Plots should be drawn with both these points.,318827.0
97415,416970.0,You can use Bar plot with value count of each status.. I don't see a huge difference between plotting it on Histogram or Bar as far as the difference in the frequencies for each status is appearing clearly..,316349.0
97415,417005.0,you can plot a histogram also if you bring the values of the column also,318017.0
97416,416968.0,"I beleive it is difficult to compare three different graps, rather we can plot three lines on a single graph, it will make more sense while comparing and analysing",315277.0
97416,416969.0,"I guess Frequency of requests cannot be clubbed into the other two points.. it has to be plotted individuallly to quickly check which is most frequent problem.. The other two can be or should be clubbed to see which type of request in which time slot is occuring the most.. whereas without even clubbing also it can be seen clearly.. but it depends on your presentation, how you want to present it to your client.. Hope this helps..:)",316349.0
97416,417529.0,"My 2 cents are, let x axis be the time range, and y axix be the number of request, and then plot two lines one with the total frequency and other with cancelled/no available cars. This all should be in one graph",304814.0
97199,416058.0,You may choose either and see what makes more sense for analysis....but try to give justification/reasoning used.,311857.0
97199,417062.0,"Yes, you need to identify the Problematic Requests (Whether it is from City to Airport or From Airport to City or both ) and can plot using the Frequency (Count of Requests)",316202.0
97199,416078.0,I agree with Pradnya. each one of us will have a different approach. In my opinion providing the closer analysis that matches problem statement is what's desired as per rubrics,309211.0
97130,415074.0,"I believe the data provided is for dates 11-July-2016 to 15-July-2016. So, you will have to find a way to convert the two date formats to the right datetime values. I have used the to_datetime() function alongwith some of its parameters to achieve this. Please take a look at the documentation for more details on how you can try achieve this.",313826.0
97130,415295.0,"Hi, you can use to_datetime function and Please Note: the format argument isn't required.(optional) to_datetime . try it without trying to match your format. Pls refer the below link https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime",300687.0
97130,415566.0,"When you import the dataset using Python API, datetime column will be imported as an Object Type (string). You need to apply pd.to_datetime() function for the same column to convert it from object to datetime. Later, you can apply datetime functions to extract the required information from the column for your data analysis. The pd.to_datetime() will do most of the work for you. It is not requried to use format string.",312479.0
97130,415698.0,Look for the arguments to pass in pd.to_datetime function..,318741.0
97130,415979.0,please go through this https://stackabuse.com/converting-strings-to-datetime-in-python/ it will hepl you all the datetime conversion,318732.0
97130,415982.0,Got it guys. Thank you..,300727.0
97438,417004.0,i did not used custom bin you can set the plot for hours for three diff status,318017.0
97438,417229.0,I assume you want to get derived metrics using custom bin. You can Pandas cut method to achive this. First you need to create the bins and use it in pd.cut(). For details see the Pandas documentation - https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.cut.html,307495.0
97438,418707.0,You can do using pd.cut(). Refer to the link below: https://stackoverflow.com/questions/45751390/pandas-how-to-use-pd-cut,304319.0
97438,417994.0,"Hi, this is a part of the assignment. you can do it in any way you think is appropriate. one of a good way to plot is to make new columns for the time of day for the trip in the data set as you studied it in EDA. Now you can use this column to plot the bar chart (which is the same as a histogram). You can do it any way you think is appropriate. Marks would be given on the analysis and the understanding.",332326.0
97445,417041.0,"Yes, you need to create few derived metrics for this. You will have to convert the Date Time from object to datetime format, and then use this to derive the metrics. With this, you can find the Supply &amp; Demand gap for the Problematic time slots.",316202.0
97445,417020.0,you can create derived matrices to do the analysis that can be date month year time etc .,318017.0
97448,417100.0,"By default, when you restart the notebook none of the cells are executed. The output that is visible after restarting is the residual output from previous run. You need to execute the cells one by one again to get the output.",313826.0
97448,417109.0,"then how do i submit this notebook, if the evaluator will open he wont find any plot",308495.0
97448,417112.0,I believe the evaluator will in any case run all the cells and generate the output.,313826.0
97448,417115.0,"In pivot, when we run the cells, the pivot table is drawn but one has to drag the headings to make the chart. evaluator wont know which headings I had drawn in pivot charts",308495.0
97448,417999.0,"Hi Rashmi, Yes, evaluators do run your code after submission. But still, if there is any extra work to do like dragging the heading, you may think of applying some other method here which you think is appropriate. Again if you could not think of something in this type of case, it is recommended that you should properly comment on the process that needs to be done. But, the later can lead you to partial marks in some section.",332326.0
97457,417079.0,i did with pd.cut finally but just wondering why for with ifelse not working.. can you also please try and see if you're able to crack the code?.,316349.0
97457,417071.0,May be i am diverting from original question and trying to suggest different solution. I assume you are trying to create new column with different hours of day such as morning 8AM-11AM. I suggest you to check the Pandas cut method. pd.cut(). Please go through the documentation for detail.,307495.0
97457,417087.0,"I used the if elif approch and it worked for me, but i did not used in loop i created the function which is talking a argument and on that i used if elif conditions.",320073.0
97457,417111.0,"Create a function having the if-elif-else conditions. Then using the apply() method on a column which has the hour of booking, you can call this function to derive the time slots and assign it to a new column in your main dataframe.",313826.0
97457,417326.0,"thanks guys, this was a good practice i did it in multiple ways with function, if else and also with np.where method too.. :)",316349.0
97457,417584.0,Using apply you can write a lambda function as well..,318084.0
97457,417628.0,I have used if else condition it is working fine for me. Or you can use lambda function.,314183.0
97459,417095.0,Demand = 'Trip Completed' + 'Cancelled' + 'No Cars Available' Supply = 'Trip Completed',313826.0
97459,417069.0,"Answer from TA quote and unquote, You need to do two things here: find the time slots where the most demand is observed and what type of demand is observed( cancellation or no cars available). Then compare it with the number of trips that are getting successful to get an idea about the supply-demand problem . Please go through this discussion. https://learn.upgrad.com/v/course/208/question/97350 https://learn.upgrad.com/v/course/208/question/96306",307495.0
97471,417106.0,"The columns containing the dates need to be converted to 'datetime' datatype by using the to_datetime() function. Explore various parameters that can be passed to this function, like the yearfirst, dayfirst etc. The function is robust enough to handle the various issues related to the date fromats present in these columns.",313826.0
97471,417891.0,"Hi Chetan, You have mentioned that you are using lstrip to remove 0 It is recomended not to remove anything from the data . Please check other queries on Discussion Forum about this. This has been raised multiple times. Hope this helps you.",310508.0
97493,417179.0,how are you survive count of status? trying using countplot which takes only X values and simple.,318329.0
97493,417205.0,"Pull ""Pickup Time"" to Columns Pull ""Status"" to Rows. Right-Click on the pill and select ""Measure--&gt;Count"" Pull ""Status"" on the ""Color"" marks card Again pull ""Status"" on the ""Label"" marks card , right-click on the pill and select ""Measure--&gt;Count""",313826.0
97502,417243.0,"The columns 'request timestamp' and 'drop timestamp' can be directly converted to 'datetime' datatype using to_datetime() function. Afte the conversion,details like date,time,month,hour,year etc can be extracted from the converted field using .dt accessor objects.",313826.0
97502,418105.0,"Use pd.to_datetime(). Look at the Video link provided by Upgrad, on how to convert the Date.",315560.0
97515,417315.0,use datetime function firstly to convert string to datetime use the keyword bydate with it,318017.0
97515,417316.0,"Hi, How did you extract the time from request timestamp? You will have to convert the column to datetime type using pd.to_datetime method and then you can use df.column.dt.hour to extract the hour",318329.0
97515,417311.0,Hint:- Replace all '/' to '-' and then convert to datetime format using function pd.to_datetime(),303673.0
97528,417487.0,"In evaluation rubrics, it's stated that The demand and supply are defined properly and the numbers are correct. So, I am assuming we may have to combine Cancelled and No Cars Available and compare it against Completed and print the numbers. Waiting for TA to confirm.",318329.0
97528,417348.0,Problem hints states that: Find out the gap between supply and demand and show the same using plots So I think we have to calculate first and then plot it. Rest TA's can give more insight. Hope this will help.,317991.0
97528,418010.0,"Hi, As a lot of learners are facing the same issue. This is a part of the analysis and should be done your way what you think is appropriate. Just to describe the flow- 1. calculate the demand in 24 hrs for each day. 2. Base on the demand, divide the time of day into slots. 3. now you have demand for the time of day. 4. Now check for the all 3 status in each slot. (you can use charts and graphs) and find the problematic slot. 5. Do the further analysis according to the pickup point in the problematic slot. The above given is just a flow and guidance as a lot of learners are facing the same issue. marks would be allotted even if you do it in some other way and the analysis is correct. Bar charts, histograms and pie charts can be used whenever you think is required.",332326.0
97530,417356.0,Filter the dataframe only for value = 'x' and then plot the graph.,313826.0
97530,417890.0,"You can try: New=df.groupby(['X','Y','Z']).count() Use New on X vs Time. Some useful links below:https://stackoverflow.com/questions/48238305/bar-plot-with-groupby",310508.0
97354,416675.0,For stacked bar chart refer: https://matplotlib.org/gallery/lines_bars_and_markers/bar_stacked.html FOr Grouped Bar chart refer: https://python-graph-gallery.com/11-grouped-barplot/ python-graph-gallery.com is a really helpful source for all possible charts. You can find almost any chart type there.,312490.0
97354,416938.0,You can also use Seaborn FacetGrid. You need to go throught the documentation for more details. Look for palette and hue_order from documentation examples.,307495.0
97354,417257.0,"@Harshal R. Zele, Thanks for the reply. Here's the issue. The legend does not appear whatsoever. I tried a workaround by specifying a colour dictionary to the palette. ------------------------------------------------------------------------------------------------------------------- palette ={'category1':'colour1', 'category2':'colour2', 'category3':'colour3'} g = sns.FacetGrid(df, hue=""categorical_variable"", size=5, palette=palette) g = g.map(sns.distplot, ""req_hour"") plt.show() ------------------------------------------------------------------------------------------------------------------- So I know what colours correspond to the categories, but the legend still won't pop up. I also tried palette='Set1', didn't help. Upon doing some surfing on the web, I found out in certain cases plots will not appear as we thought it would in seaborn, and people generally recommend some other plots for those cases. Edit:https://github.com/mwaskom/seaborn/issues/1166 -&gt; Here, I tried the method by @jakarakas, didn't work.",301652.0
97354,417330.0,"This is how i am trying and i am able to see the legends and hue with respect to category. g = sns.FacetGrid(df, col=""var1"",hue= 'var2', height=5, aspect=1,palette=""Set1"",hue_order=[""sub_cat1"", ""sub_cat2""]) (g.map(plt.hist, ""var3"").set_axis_labels('X-axis','TY-axis')).add_legend()",307495.0
97541,417393.0,"The columns 'request timestamp' and 'drop timestamp' can be directly converted to 'datetime' datatype using to_datetime() function. Please explore some of the parameters of the to_datetime() function like yearfirst,dayfirst,format etc. and see which can be used in this scenarios.",313826.0
97541,417458.0,"You will have to look at the csv, figure out what different formats the timestamps are in. After this there are two ways you can proceed : 1. Convert the timestamps by replacing elements in the string to make it uniform. Convert to datetime. 2. Convert to Datetime explicitly passing the format Hope this helps!",319357.0
97541,417499.0,"When converting to datetime funtions, it has a parameter of dayfirst. Kindly explore that. Syntax : pd.datetime(df['col_name'],dayfirst=True)",317460.0
97541,417637.0,"With pd.to_datetime, you have to use some parameters",314183.0
97541,417949.0,"You can also use (parse_dates=[date field], dayfirst=True) while importing csv file.",310467.0
97546,417448.0,you can do it using if else statement in both python and excel and then do analysis,318017.0
97546,417416.0,You can use if else in excel to create time slot. But why do you want to create time slot in excel ?,317991.0
97546,417459.0,"hey Darshana, i really spent long time on doing this.. and finally landed up by cracking the code with three logics.. see which one is useful for you to Bucket your column data; 1. Easiest and straightforward single line code using pd.cut method 2. You can use np.where method to categorize your data.. this works quite similar to your nested loops in Excel 3. You can define a function using if-elif-else conditions for each bucket.. and then call it to add a new column in your dataframe.. hope this helps.. :)",316349.0
97546,417750.0,"pd.cut code is giving me a blank column, I am not sure whats wrong in that. Could you please help?",308967.0
97390,416974.0,I'm assuming all the numbers (values) with which plots are created.. :) we can wait for TA to answer this if I'm not correct though!!,316349.0
97390,417053.0,"We need to identify the Problematic time slots and understand the Demand and Supply Gap in thos time slots. Basically Demand is the total number of requests during the time slots and Supply is the number of ""Completed Trips"" in those time slots. This is the number they are referring to in the evaluation rubrics.",316202.0
97390,417673.0,"After you have identified the problematic time-slots (both the time-slot and the associated problem should be identified correctly with proper reasoning), you need to determine the demand and supply numbers for those time-slots.",313517.0
97552,417440.0,As you have some null / NAT value in Drop time . You can do type cast after that to make them synch .,311861.0
97552,417442.0,You can also try this for request time df['Hour'] = df['R_timestamp'].apply(lambda time: time.hour),311861.0
97552,417455.0,"yes exactly, if you scroll down or filter your dataframe to show only the 'cancelled' &amp; 'no cars available' status, there are NaN's for Drop time whcih is converted to NaT with .dt.hour method and resulting in float datatype for your coMplete column.. but i don't think that harms any of your calculation.. because the decimal part is 0 anyways for all the values.. hope this answers your query.. :)",316349.0
97552,417646.0,we can directly make column with time from timestamp df['time'] = df['timestamp'].dt.time,312019.0
97552,417952.0,"You can cast the float to int later. hour=[Date field].dt.hour -&gt; gives float value. If you have null values, impute it and then convert it to int as follows [hour].astype('int')",310467.0
99401,424498.0,"All customer requests received is a demand. So all status of trips will lead to the demand. The gap is the requests which are not fulfilled, such as, the cars unavailable and cancelled ones. Supply is calculated from the requests for trips completed.",301644.0
99401,424480.0,Demand = 'Trips Completed' + 'Cancelled' + 'No Cars Available' Supply = 'Trips Completed',313826.0
97569,417534.0,"Try this pd.datetime(df['col_name'], dayfirst = True)",310419.0
97569,417538.0,"Thanks Venkatesh and Darshana, it worked. Can either of you also confirm if NaT for Drop timestamp after conversion NaN values is as per expected output.",319302.0
97569,417530.0,"Try without giving format, only pass statement pd.to_datetime(df[]). It should work",303673.0
97591,417651.0,When Na values occurs in any dataset we follow one of two approach 1. Either impute it with mean / median or any other value or 2. We delete rows having Na values. But if the column containg Na values is important for analysis and above two method may distract analysis it is better to leave Na values as it is. Hope this will help.,317991.0
97591,417591.0,"Hey Maya, ""NaT can be handled, they are just like null/nan. isnull and isnan works well with NaT too."" Refer this discussion for more clarity - https://learn.upgrad.com/v/course/208/question/96748 Hope this will help!",302742.0
97588,417610.0,In video presenter described about the approach Uber followed to solve the problem. While in results expected there is no mention of idle time or cab wating time. And to calculate idle time or cab waiting time I think the data has enough information and you would require few derived matrics to acheive this. Hope this clarifies your doubt.,317991.0
97588,418386.0,"df.groupby(['driver id', 'request date'])['request timestamp'].shift(-1) ,, use this command , and find the difference between new column created via this command and Request timestamp.",315560.0
97594,417594.0,"Hey Sanchit, Refer this dicussion - https://learn.upgrad.com/v/course/208/question/97341/answer/417177 Hope this will help!",302742.0
97594,417600.0,"I am not getting any o/p and has been waiting for more than 15 mins , although i feel the dataframe size can be a contributing factor.",315464.0
97594,417882.0,"Hi Sanchit, I had posted the same query a few days ago. For me as well it typically occurred while using Hist. The kernel was dying out. I switched to Seaborn 0.9 &amp; the issue got resolved. I know how it must be testing your patience..!! https://learn.upgrad.com/v/course/208/question/97341/answer/417177 For Seaborn latest version follow this link. https://stackoverflow.com/questions/51422146/install-the-latest-version-of-seaborn-0-9-0-through-pip Hope this helps you..",310508.0
98324,418347.0,you can plot a pie chart to know the distribution of a dimension,318017.0
98324,418352.0,Yes you can use it but histogram give better to showcase the frequcny distribution.,318461.0
98324,418703.0,Pie chart is generally used to show part of a Whole. I would use bar chart to plot frequency distribution of a categorical variable. To plot distribution of quantitative variable I would use a histogram,317996.0
98324,418320.0,"Certainly, pie chart is very useful to visualize discrete/unordered univariate analysis.",310974.0
97592,417635.0,You should choose appropriate plot to show. You can plot between a category and its frquency i.e. univarience plots. Check univariance syntax for the same.,318429.0
97592,417650.0,count of cancellation is a quantitative value so its better to use bar plots,318017.0
97592,417761.0,Refer to the below link for displaying values in the plot https://stackoverflow.com/questions/43214978/seaborn-barplot-displaying-values,318329.0
96081,408506.0,We may have to perform EDA and see the relation between timings and problems. Then decide on the slots.,318329.0
96081,409215.0,"Hi Chandan, You can make segmens of different hours of the day into 4-5 slots. You can take any time and name them as you wish. You can segment logically like 8 to 11 am as morning rush hours because most of the people go to office and many people have flight in those hours. You can also perform EDA to identify which time slots are rush hours and which are mild ones.",319696.0
97601,417643.0,use lower(fieldname),318017.0
97601,417655.0,"If you will see it is giving the destination address, Lambda function calculates not when it is assigned, but when it is calculated. Try printing the ""Quarter_islower"" column",315277.0
97601,417649.0,you are assigning the is lower value to a new df thats why its giving you the object dont assign it and check . for better results rather the checking change the type to lower(),318017.0
97601,417881.0,"Map function would always return the Map Objects. You can convert them to list by passing map to the list constructor. You can use the below code: df['Quarters_islower'] = list(map(lambda x : x.islower(), df['Quarters']))",311160.0
97601,418367.0,df['Quarters_islower'] = df['Quarters_islower'].apply(lambda x: str(x).islower()) Because x need to be converted into string then only we can apply string function,318461.0
97632,417724.0,"Please check below link ofr a similar query: https://learn.upgrad.com/v/course/208/question/97599 Basically, you need to use the to_datetime() function and explore the various parameters that the function supports like yearfirst,dayfirst etc. and use a parameter which can handle the different date formats in the data.",313826.0
97632,417725.0,You can follow approach metioned in the below link https://learn.upgrad.com/v/course/208/question/97544,317991.0
97637,417776.0,Python charts is must and tableau chart is optional.,307843.0
97637,417735.0,"As mentioned in the "" Problem Statement "", we need to do the entire assignment in Python including the plots, however may recreate plots in Tableau for presentation purpose. So, plots are required in the python notebook aswell.",313826.0
97637,417885.0,Yes Tableau would always add weightage. Though its not mandatory but you can use it.,310508.0
97637,418260.0,"Using Tableau doesn't hurt. You can import the Python curated data, run through the analysis in Tableau and compare if your plots obtained in Python are similar. Technically, they have to be a spot-on match since it is the same data you are running for a particular kind of report (bar / histogram etc)",309211.0
97637,418705.0,"Sanchit, it is your choice whether you want to make 5 different charts or club the visualisations in jupyter. In either cases, they should be informative and very clear.",304319.0
97638,417733.0,You can go through below links to proceed further. https://learn.upgrad.com/v/course/208/question/97544 https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html Hope this will help.,317991.0
97638,418234.0,"As it is mentioned in the assignment that the first and foremost step should be the data cleaning and formatting. Thus, you should follow various techniques to achieve the same.",318772.0
97638,418710.0,use to_datetime() function to convert them into DateTime format.,304319.0
97644,417752.0,"This time there is no powerpoint presentation template provided, you have to create on your own for Uber Case Study.",317991.0
97644,417888.0,"You can use old case study empty template or creat a new one.Plots can be created in Tableau as well, it would always add weightage. Though its not mandatory but you can use it.It is mentioned in the problem page as well.. https://learn.upgrad.com/v/course/208/question/97637",310508.0
97651,417789.0,"This should help Df[""Day""] = DF[""timestamp""].dt.day as long as it has non blank value .",311861.0
97651,417823.0,"is it ""09 days HH:MM:SS"" ?? you can split on space!",316349.0
97651,418019.0,you can follow the discussion forum for this. I am sure you will find an appropriate solution to deal with the date format issue.,332326.0
97656,418253.0,"you cancreate buckets of time, 4 hours each , that wud help you in analysis further. to create buckets you can, define bins and labels, thenuse pd.cut()",308495.0
97656,417815.0,"You can come up with the timeslots based on the hour's, create a custom function to return a timeslot given the hour and use this function to create a new column using apply method on the hour column.",318329.0
97656,417821.0,i really spent long time on doing this.. and finally landed up by cracking the code with three logics.. see which one is useful for you to Bucket your column data; 1. Easiest and straightforward single line code using pd.cut method 2. You can use np.where method to categorize your data.. this works quite similar to your nested loops in Excel 3. You can define a function using if-elif-else conditions for each bucket.. and then call it to add a new column in your dataframe.. hope this helps.. :),316349.0
97656,417886.0,You can try pd.cut. but in my experience creating a Lambda function was easier to handle the 0 hrs in the excel sheet. Lambda was flexible for defining the timslots ranges.,310508.0
97656,417989.0,"Use a user defined function, thats easy with map function.",315560.0
97656,418205.0,try pd.cut,312357.0
97656,418266.0,"There are many approaches to solving a problem. Once you convert the dates to Datetime format,you can use Pandas.series.dt.hour and then assign it to a new column in the data frame. Based on the extracted hours, you can create bins. Since there are many suggested approaches, choose the one that is more concise, faster execution time (which does not matter very much here since dataset is very small ; however per Rubrics, they do consider how long or short your coding is; longer code is hard to read , understand and interpret)",309211.0
97658,417833.0,"Yes , you can .",311861.0
97658,417838.0,You can put any plot and it need not be the ones you plotted in python.,318329.0
97658,417887.0,Yes Tableau would always add weightage. Though its not mandatory but you can use it. It is mentioned in the problem page as well.. https://learn.upgrad.com/v/course/208/question/97637,310508.0
97658,418022.0,Just make sure the required graphs are in python too. you can plot extra graphs for analysis in the presentation but at least the mandatory graphs are to be present in the python notebook,332326.0
97658,418268.0,"Per problem statement, you can have any number of relevant python plots that strengthen your analysis. So Yes, absolutely.",309211.0
97657,417850.0,you have to check the time which a driver has to wait at airport until the next ride is available,318017.0
97657,417872.0,I felt that the data set provided doesn't give much of an insight on the Idle Time. Did anyone find out an interesting pattern out of it?,311160.0
97657,417853.0,"You can use df.groupby(['driver id', 'request date'])['request timestamp'].shift(-1) to create another column and take the difference between request timestamp and the new column to find the idle time",318329.0
97657,418027.0,you will get very intresting insight when you corelate with why drivers not accept trip from City to Airport,315455.0
97657,418350.0,Find the difference between timestamp of driver arrivied at airport and driver took the next ride from aiport,318461.0
97657,418398.0,"However, all of you are saying the correct thing , more or less and approach is also right but the results coming out are much different than expected. For eg. Its easy to see Idle time as 4-5 Hrs at many place, but sometimes Idle time is coming more than 11-12 Hrs which doesn't make any sense. Also, time like this -1 days +07:39:42 , is coming, and I can't figure out what it means. Can anyone help.",315560.0
97657,418994.0,"- Ideal time does not make much sense here, ideal time results doesn't seem real",300735.0
97661,417848.0,you have to check at what times there is a greater demand then the cabs available and trips completed with the places where there is cancelled and no cabs available. after finding the times at which the demand is greater as compared to the supply you have to suggest ways to improve it,318017.0
97661,418028.0,"Cause &amp; Remedies.. Here we have first highlighted the major issues.. Now we should assume that our recommendations are like a ""strategic planning for coming quarters"" to the leadership team at Uber for dealing with the issues we have highlighted in this study better.",310508.0
97661,418023.0,This is very subjective based on your understanding. The ideas may vary but a basic logic should relate your idea to the analysis you have done.,332326.0
97667,417876.0,close the jupyter and restart sometime due to cache it gets slow,318017.0
97667,417961.0,"Hi, you can also check the datatype of the columns you are plotting. It happens with me also some time back creating the histogram and when i checked the datatype was object. when i converted to float, graph comes within seconds.",320073.0
97667,417922.0,"Please check your commands which you are running , if its normal command like info , describer then restart the Jupyter . If you are plotting graph , please plot on smaller scale for testing purpose . You can use Google Colab if you have RAM issue .",311861.0
97667,418850.0,open task manager and check if there are multiple python kernels running. this could also lead to slow performance. Kill all python process and start notebook fresh.,312199.0
97667,418391.0,"matplotlib instance s are object, once you create the object it stays in the memory untill the code is running thus you need to clear or destroy each object explicitly so as to free the memory and fast computing of the program.",318772.0
96094,409228.0,You have to convert the 2 different date time format to any one single date time format which is recognized by python. It is necessary because it will help in your analysis later.,319696.0
96094,415179.0,"You can use pd.to_datetime() function, all the dates will be taken care off",317984.0
96094,415363.0,"Hi pls try the to_datetime function.Please not format is option.so try without the format all format will be taken care. raw_data['Mycol'] = pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f') https://stackoverflow.com/questions/26763344/convert-pandas-column-to-datetime",300687.0
96094,416091.0,"in my opinion, this challenge was introduced to guage skills in data cleaning , part of EDA. to_datetime is one way of approach. always refer to Python documentation instead of googling out.; This is what our industry expert Bikash Debnath told in EDA which has been helpful to me. here is the link for to_datetime : https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html",309211.0
98337,418392.0,"You can show the frequency of a categorical variable with either Bar, Histogram or Pie. Aim is to identify and read the plot to generate the outcomes out of it. Try another possibilities if Pie doesn't help you..",316349.0
98337,418403.0,you can plot any graph which can clear mention the analysis outout it can be pie or bar or histogram,318017.0
98337,419081.0,You can follow below link for youe query https://stackoverflow.com/questions/21572870/matplotlib-percent-label-position-in-pie-chart https://pythonspot.com/matplotlib-pie-chart/ Hope this will help.,311466.0
98337,418712.0,A Bar is better suited for this condition. But still there is no Thumb rule.,318780.0
98340,418401.0,please restart the kernal and try again if you still face issue please share the error message,318017.0
98340,418412.0,Use the following to import.... import matplotlib.pyplot as plt %matplotlib inline,301644.0
97695,418021.0,"From what I understood, We need to put analysis and recommendations in PDF. But, It's better TA confirm this.",318329.0
97695,418025.0,You should include it in the PDF you will submit.,332326.0
97695,418356.0,We have to submit in PDF format,318461.0
98346,418423.0,You can achieve this in matplotlib . Please go through with the below link which has provided the documentation : https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pie.html,301648.0
98346,418979.0,You can follow below link for youe query https://stackoverflow.com/questions/21572870/matplotlib-percent-label-position-in-pie-chart https://pythonspot.com/matplotlib-pie-chart/ Hope this will help.,317991.0
98368,418484.0,catplot has been introduced in newer versions of seaborn and might not be present in default conda installation. Follow the below link to upgrade seaborn to 0.9.0 https://stackoverflow.com/questions/51422146/install-the-latest-version-of-seaborn-0-9-0-through-pip,318329.0
98368,418486.0,"Someone guide me plz,Is it because of lower version of Anaconda?",300721.0
98368,418559.0,"Check your version of seaborn, if it is below 0.9 then you need to upgrad to 0.9 version of seaborn. Please go through this link. This might be of help. https://stackoverflow.com/questions/51402579/module-seaborn-has-no-attribute-any-graph https://stackoverflow.com/questions/51846948/seaborn-lineplot-module-object-has-no-attribute-lineplot This can be confirmed in Seaborn release notes. You can get release notes here, https://seaborn.pydata.org/whatsnew.html (Check for API changes in version 0.9.0 (July 2018)) To update seaborn: 1) Open anaconda navigator 2) Go to environment tab 3) Click the playsymbol on base(root) and choose open terminal 4) Run this command : conda remove seaborn 5) After it is successfully uninstalled i.e. you can check by searching the package seaborn in the search option then install the new package 0.9.0 using the command conda install seaborn=0.9.0 Hope this will help.",307495.0
98368,418974.0,try pip install seaborn --upgrade on anacondan navigator.It should help,300684.0
98368,419229.0,Use below command from you conda prompt conda install -c conda-forge seaborn https://stackoverflow.com/questions/51422146/install-the-latest-version-of-seaborn-0-9-0-through-pip,318438.0
98361,418568.0,"I faced same issue. Please double check the 1st step of data cleaning. I was doing wrong step for converting Request timestamp and Drop timestamp in same format(YYYY/MM/DD HH:MM:SS). After correcting date, i was able to see the correct graphs. Hope this helps.",307495.0
98361,418621.0,There is hint given in problem statement as follows: Data Cleaning and Preparation - Hints Identify the data quality issues and clean the data so that you can use it for analysis. Ensure that the dates and time are in the proper format. Derive new variables which will be useful for analysis. You were supposed to do these steps. From your problem description I can infer that you have not done above second step properly. Recheck it and your problem will be solved. Hope this will help.,317991.0
98361,419352.0,"Hi Avinash, We need to clean the Request (and Drop) Datetime data in Python as the file contains data in mixed fromats. Using the same file in Tableau directly will result in the error you are facing. Instead, create new CSV files from Python dataframes once you clean the data in your code and complete the required analysis. You can then use these CSV files in Tableau. Hope this helps.",311686.0
98366,418713.0,"Divide the time slots into divisible of 24, ie. 4hrs,6hrs or 8hrs",318780.0
98366,418518.0,"You can define 5-6 day slots, dividing the 24 hours into those many slots. Just make sure to mention what you have considered as a part of your comment.",315028.0
98366,418466.0,"A couple of time slots has already been mentioned in the problem like early morning and late evening, Considering that as a benchmark you can use time slots such as early morning,morning, afternoon, early evening,late evening,early night,late night and so on. I do not think there is a hard and fast rule about the time range, We can divide that according to our own convinience as long as we mentions the range in the assignment.",318374.0
98366,418668.0,"You can consider time slots when typically people travel, and devide them into some logical slots. hope this help.",306011.0
98366,419097.0,"You can divide the time slots into early morning , morning, afternoon, evening ,late night ! accordingly to time slot you can think you can assign",311466.0
98366,419241.0,"You can make a bucket of 4 hours each, 12 to 4 am late night, and so on",308495.0
98366,419748.0,No rule we should divide based on what slots we think appropriate. Ideally in 24 hours 4-6 should be good enough.,306009.0
98366,418696.0,"Draw some plots across all hours of the day. IF you see any patterns in grouping of some hours, make that a category",317996.0
98344,418419.0,"There might be a problem in calculating the idle time if the results you are getting has more than 1 day. There are outliers in the results but you should be considering the trips which are complete for shifting the column and calculating the idle time. Also, Calculate idle time at airport and idle time at city seperately in two columns for easier illustrations.",318329.0
98344,418421.0,"Guys, How do you even calculate idle time? At what level are you guys calculating it? With the given date set is it even possible to calculate it?",318084.0
98344,418724.0,You can take median of the value within the time bucket and it will get rid of outliers,317514.0
98344,418874.0,"In my opinion, there isn't not enough data for us to calculate idle time. For starters, we dont know what hours the driver stops working for the day. The 11-12 hour idle time that you are seeing can easily (actually, likely) be inactive hours and not idle time. Moreover, there are several instances where and airport drop is not followed by a city drop, but by another airport drop. These are possibly the cases where the driver did not get a return fare- he might have waited (idle time) for an unknown period of time or not waited at all. How do you treat these data points? Idle time would also not be very useful for this analysis. By cancelling trips, drivers are artificially creating lower idle times at the airport. You would probably see idle time in the same ball park for the entire 24 hour period. Caveat- I haven't actually done this analysis, but basing this on supply-demand dynamics Just to reiterate, this is just my an opinion. Take it for what it is worth",305653.0
98372,418538.0,"If you do a countplot against all requests, you get the demand plot If you do a countplot against the requests with status'Trip Completed', you get the supply plot",318329.0
98372,419742.0,First we need to find out the Problematic request type (Airport to City for Unavailability of Cabs and From City to Airport for Cancellation). You would have also identified the timeslots where this problem occurs. Now simply plot a stacked bar or a grouped bar chart which shows the count of requests for each request type. This will show the gap between the demand (Which is the sum of all three request type) and Supply (Count of Trips Completed). Use the groupby and count in your plot query. Refer to the below link for more guidance: https://stackoverflow.com/questions/23415500/pandas-plotting-a-stacked-bar-chart,316202.0
98379,418663.0,Check on what data/column you are trying to run. I've faced such problem when there was an error in my calculation and the plot going to infinite kind of loop.,318329.0
98379,418695.0,First run the plot on limited rows like for a particular Day or for First 100 records if we are not sure how our plot will behave . For example if we are plotting bargraph on all Request ID ...,311861.0
98385,418662.0,Follow the below link https://stackoverflow.com/questions/39444665/add-data-labels-to-seaborn-factor-plot,318329.0
98385,418976.0,Apart from the link provided above you can also visit below link https://stackoverflow.com/questions/28931224/adding-value-labels-on-a-matplotlib-bar-chart,317991.0
98407,418742.0,Tou can have a slide in your power point for analysis then convert that powerpoint into a PDF.,314197.0
98407,418753.0,We have to include it in PDF. You can see TA comment in the link below with similar question https://learn.upgrad.com/v/course/208/question/97695/answer/418025 Hope this will help.,317991.0
98407,419245.0,We can use PPT presentation and convert it into pdf and then submit.,314183.0
98407,419737.0,"You will have to create a presentation and provide your insights there. Please note to convert it to PDF before uploading. You an also write your insights in the ipython file. You can use the ""Markdown cells"" to write your comments",316202.0
98406,418723.0,whatever you do it will give you the same result . if you divide it by days and add it up it will give you the full weeks data .,318017.0
98406,418781.0,"usually the size of data will be huge and it wouldn't be possible to analyze it on every days data.. better approach would be to aggregate it over timeslots with complete period of time.. and yes it depends on what observations to be made.. if you're expecting the results on a particular day, then go ahead with aggregating it over a day.. but yes the trend would be same, here for assignment you can see a trend with aggregating the complete data on timeslots..",316349.0
98398,418684.0,You mean numbers? I don't think they will be same.,318329.0
98398,418690.0,Identifying most problematic time slots - These will be the time slots fow which staus is cancelled or no drivers avalable Slots with highest demand supply gap - demand (all request) - supply(trips completed) = requests withs status cancelled of no drive available Thus same Please advise if I'v misunderstood the questions,315464.0
98398,418692.0,"I think both are different .. For first question :- You can simply plot Countplot or Box Plot on the given fields . We can easily quantify how many request for either Airport or City is being done vs Day , month , hourly . This will tell us if number of complete request , or incomplete request is at which time and what for . For Second Question - We need to create new field as Demand-Supply . Then see how Demand-Supply is varying across various paramter .",311861.0
98398,418783.0,- Check for the hours where demand and supply gap is huge for both where pick-up point City or Airport - Most problematic is where this gap is higher for City or Airport,300735.0
98398,418967.0,Both are varying and different across the 24 hours.,300684.0
98398,418981.0,"Deal this data in 3 parts- first when trips are completed, second-cabs not available, third trips cancelled, you will able to see the gaps",300735.0
98414,419317.0,I also saw same error but another tab was opened where I got the option to download the iPython notebook.,300698.0
98414,418778.0,Jupyter notebook Autosaves with checkpoint creations.. so you shouldn't miss the codes.. But I'm confused when you say 'downloading notebook'.. from where are you trying to download it??,316349.0
98414,418823.0,I am downloading the UBER assignment (file in jupyter notebook containing codes and graph) from jupyter notebook. it throws error. I try opening the downloaded file it doesn't open and throws error. how do I upload my notebook for submission ?,308495.0
98414,418860.0,"not sure if i followed you correctly.. may be the file is corrupted.. if you've the original file in your machine which you might have uploaded previously, you can try zipping it and uploading it back again..",316349.0
98414,418938.0,Request TA to help me with downloading the kernel from jupyter notebook. It throws error. 500: : Internal Server Error AM not sure how am going to upload the kernel for uber assignment submission,308495.0
98414,418987.0,You need to turn off the pop up blocker .Try downloadng after that.it should help,300684.0
98414,419014.0,"am able to download it, but the file doesnt open and it throws error 500 : Internal Server Error",308495.0
98414,419238.0,"Thanks Mahima, i could locate the python notebook in the folder. But wasnot able to open it. On google, came across https://stackoverflow.com/questions/47267932/associate-ipynb-files-with-jupyter-notebook-in-windows-10-anaconda used below commands, pip install nbopen python -m nbopen.install_win and was able to open the file",308495.0
98414,419222.0,"I am guessing you are using the ""Download as.."" option in the File Tab. If that is indeed the case, then you can directly go to the workspace folder containing all your python notebooks and then choose the file that contains the solution(the original file). Jupyter automatically saves all the files in .ipynb format and it is not necessary that you need to download it to access it. Please check if this method works.",313517.0
98421,418840.0,Yes data set given contains only July data. And it depends on you whether you feel creating month and day is useful or not. I would suggest you to create these two derived metrics and see yourself whether it gives any additional / hidden pattern of dataset.,317991.0
98421,419233.0,"Yes, data set has given for 11 july to 15 july. It is upto you, how are you analysing the data set. We have to use effective way to show the data.",314183.0
98421,418768.0,"i guess yes, but again it depends completely on how you look at your data to explore it to the extent possible and extract more and more out of it.. like for calculating the idle time the driver spents at airport or city, needs your dataset containing date column.. for hourly calculations needs you to extract time and hour.. so, play around with it and see what you need for your analysis and plots you're creating.. it's completely a part of presenting your data in a way effective to your client (could be on day basis, hour basis, minutes basis).. :)",316349.0
98418,418752.0,Below link has the Results Expected for this assignment. https://learn.upgrad.com/v/course/208/session/19534/segment/99356 In that there is no mention of idle time or waiting time. So I think we don't need to calculate idle time. Rest TA can confirm.,317991.0
98418,418774.0,"True, idle time is not expected as a part of Assignment.. But it is completely your choice to deep dive and explore more and more data to generate insights in your dataset.. and since this is learning experience for all of us, i guess it would be a good practice to play around with this data.. Idle time is extractable and there are few posts as well on this.. atleast for me idle time calculations was really a nice study to learn DateTime functions.. if you've time, i would recommend spending time on it.. otherwise i guess it is not a part of the questions in assignment.. :)",316349.0
98418,419237.0,"In the problem statement, It is not mention to analyse the idle time.",314183.0
97544,418011.0,"Will be a good practice to check for duplicate rows &amp; null values before doing the datetime conversion. Also, verify your info after datetime conversion.",318827.0
97544,417413.0,"use dayfirst= True Example: pd.datetime(df['col_name'], dayfirst=True)",310419.0
97544,418014.0,you can explore the parameters that can be passed to this function,332326.0
98440,419348.0,"Hi, As the question says 'Find out the time slots when highest gap exists ?' we need to find the period with highest gap. Only one time slot will suffice I believe for further analysis. However you can mention top 2-3 such slots in comment based on the magnitude of the Gap.",311686.0
98440,418843.0,"It solely depends on you that how many time slots you want to create. You can create 4,5,6 etc. number of time slots depending on your understanding of problem.",317991.0
98440,418891.0,you can create time slots acc to your choice for example you can create morning noon evening and night with the req hour of your choice,318017.0
98471,419049.0,"If you check .info() onyour dataframe, you'll observe the blanks only in 'Drop Date' where the trips were cancelled or no cars available.. and this being blank is valid.. we can play around with this data without filling nulls in Drop date column..",316349.0
98471,418965.0,There is no need to fill a value in place if Na. You can visit below link for more info. https://learn.upgrad.com/v/course/208/question/97591 Hope this will help.,317991.0
98471,419809.0,Thanks,318240.0
98442,418892.0,you can give x and y labels using xlabel() and ylabel() commands,318017.0
98442,418945.0,you can iterate over the object returned by plot and make use of text method to display values on each bar Refer : https://stackoverflow.com/questions/39444665/add-data-labels-to-seaborn-factor-plot,318329.0
98442,419240.0,"You can xlabel , ylabel and title to stack your bar plots.",314183.0
98447,,nan,
98452,418893.0,"yoi can use Df.groupby(['driver id','request date'])['request timestamp'].shift(-1) and put it in a new column and after that you can subtract req timestamp with new column",318017.0
98452,418935.0,You don't need to find wait time for this analysis. The data is inadequate to do so.,310974.0
98452,418999.0,Below link has the Results Expected for this assignment. https://learn.upgrad.com/v/course/208/session/19534/segment/99356 In that there is no mention of idle time or waiting time. So I think we don't need to calculate wait time. Rest TA can confirm.,317991.0
98456,418934.0,Refer this https://stackoverflow.com/questions/12126318/subtracting-dates-with-python,310974.0
98456,419051.0,"Hey Snehal, it appears that you're extracting the 'Time' and subtracting those values.. You don't need to do that.. rather directly subtract the 'datetime' fields.. Simply, convert your 'Request datetime' and 'Drop datetime' to datetime and then subtract it.. it should work.. Hope this helps you..",316349.0
98456,418992.0,For subtracting date in pandas dataframe you can go through below link also: https://stackoverflow.com/questions/44600752/datetime-in-pandas-dataframe-will-not-subtract-from-each-other Hope this will help.,317991.0
97599,417619.0,"There are other parameters as part of pandas.to_datetime too. I mean, look for parameter which will try to understand the raw date and time format then convert it approprite. Please read the documentation and search for correct parameter. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html",307495.0
97599,417622.0,It will come in this format only. dayfirst = true means that all the day will come first for all the dates even if so e date day is in between year and month.,320073.0
97599,417644.0,In to_datetime if dayfirst = True then date is parsed in this form only i.e 2012-11-10 form. You can cross verify with official documentation link below: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html Hope this will help.,317991.0
97599,418177.0,dayfirst=True. This expression is used to read the input &amp; not for formatting the output or result,312093.0
97599,418273.0,"Ok, if I understand your question correctly, Even after using dayfirst = True , you are still seeing the YYYY-MM-DD when printed out. Python is storing the data as DD-MM-YYYY ; it is just that when you print it out using head() , the display is always oriented as YYYY-MM-DD for Datetime data type. I faced same issue; if for visualization/display purposes, you might want to show it as DD-MM-YYYY, use strftime syntax of which is provided in https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior with lambda/ apply function and convert the datetime back to object just to show DD-MM-YYYY. When you then print out (after conversion of datetime to Object using strftime), you can see the dates visually output as DD-MM-YYYY",309211.0
97599,418683.0,"dayfirst=True means while converting date in string format to date format, the first value is taken as day instead of month. it doesn't alter the format of the display",304319.0
97599,420590.0,Thankyou for the response everyone ! Understood the difference.,308673.0
98460,,nan,
98462,418933.0,"df_time is a group by object, you can't access it directly like that. Refer the previus class notes on group by and you will get it.",310974.0
98472,418964.0,"Hey Gagan, Demand = Trip completed + Cancelled + No cars available Supply = Trip Completed only So ya Gap basically is only cancelled and no cars available. You need to find out the gap and plot it. Whichever way you want. Hope this helps!",302742.0
98472,419342.0,"As the assignment rubric says 'the number s should be correct' I think it will be better to find out demand, supply and Gap for time-slots/request types and then plot the Gap as mentioned in the question. The formulae for the same already given by Rashmi in another answer to your question.",311686.0
98477,418997.0,You can go through below links to get solution for similar error https://stackoverflow.com/questions/39937140/seaborn-boxplot-typeerror-unsupported-operand-types-for-str-and-int https://stackoverflow.com/questions/49843309/jupyter-notebook-typeerror-unsupported-operand-types-for-str-and-int?noredirect=1&amp;lq=1 Hope this will help.,317991.0
98483,,nan,
98465,418942.0,use &amp; instead of and,318329.0
98465,419031.0,try using pandas.cut() that would be easier.,302738.0
98484,419338.0,Only zip file to be submitted as per the guidelines. You facing any issue in creating zip file instead of rar?,311686.0
98484,419043.0,"No, only zip file is needed to upload",318429.0
98484,419230.0,use only zip folder with pdf and python file,318017.0
98484,419569.0,Use zip file to upload.,314183.0
98497,419677.0,"Hi Amarjit, You can try: New=df.groupby(['X','Y','Z']).count() Use New on X vs Time. Some useful links below:https://stackoverflow.com/questions/48238305/bar-plot-with-groupby https://learn.upgrad.com/v/course/208/question/97530/answer/417890/comment/103063 Hope this helps you..",310508.0
98497,419350.0,"Hi Amarjit, This discussion may help. https://learn.upgrad.com/v/course/208/question/97367 1st approach - Converting the groupby to a dataframe will work..However we need reset the index. In this case, the old index is added as a column, and a new sequential index is used. After this, you can use newly created dataframe for plotting. 2nd approach can be DataFrameGroupBy.plot. You will find more deails on documentation. In this approach you dont need to create new dataframe.",307495.0
98518,419287.0,Similar kind of question has been answered in below link. Please go through it. https://learn.upgrad.com/v/course/208/question/96094 Hope this will help,317991.0
98518,419334.0,"While converting the respective columns in Datetime type, pls explore daysfirst option of the function used.",311686.0
98520,419407.0,Are you trying to use the column as date before converting? can you post the line you are using,318329.0
98520,419267.0,"Shruthi, you will need to convert the data in the column to a string before you can use split function. Try converting the column's data to a string type using astype",316253.0
98520,419517.0,Try using the pandas inbuilt function to_datetime(),318780.0
98535,419416.0,please check your internet connection if it still doesn't work report it on the same page lower right corner,318017.0
98535,419565.0,are other videos working fine? if yes then you can report the assignment specific issue. if no then there might be some bandwidth issue or some settings issue(blocker etc.),311686.0
99425,424909.0,"hi Srinithi, try replacing / with - before conversion to datetime.",311686.0
99425,424998.0,"Hi Srinithi, This has already been discussed on this forum. Kindly try using dayfirst=True",311160.0
99425,425269.0,You can use dayfirst=True in pandas.to_datetime() method to resolve this problem.,317991.0
98572,419734.0,First we need to find out the Problematic request type (Airport to City for Unavailability of Cabs and From City to Airport for Cancellation). You would have also identified the timeslots where this problem occurs. Now simply plot a stacked bar or a grouped bar chart which shows the count of requests for each request type. This will show the gap between the demand (Which is the sum of all three request type) and Supply (Count of Trips Completed).,316202.0
98572,419733.0,"bar plot shows this nicely.. you'll have three bars one for demand, 2nd for supply and third being optional gap between them.. as Chandan said, demand will be Count of all the values with all the three statuses.. and supply will ne count of values with only Trip completed status.. hope this helps.. :)",316349.0
98572,419564.0,you need to find demand and supply for the identified slots and request types. demand - supply will give the gap which needs to be then plotted. demand is total number of the requests made and supply is the number of requests completed. hope this helps.,311686.0
98576,419562.0,"hi, you can clean these columns before converting to datetime and bring all values in same format. try having the same delimiter i.e. / or - in all values before conversion.",311686.0
98576,419603.0,"Hi Bhagyasree, For a more general approach you can also link into STRPTIME and STRFTIME functions. They are general date reformatting functions and very helpful. Regards, Soumik",305839.0
98576,419645.0,"Hey, explore dayFirst option as an argument to fix your issue.",318329.0
98576,419938.0,"Hi, you can follow the various different discussion on the discussion forum for this. I am sure you will find the answer.",332326.0
98576,419717.0,use the keyword dayfirst in while converting it into datetime,318017.0
98576,419948.0,You can apply a lambda to your dates and call datetime.strftime: check the link below https://stackoverflow.com/questions/32253798/how-to-swap-months-and-days-in-a-datetime-object,305838.0
98576,420229.0,pt one argument in the function... dayfirst = True,318344.0
97287,416347.0,"'lineplot' is introduced in July 2018. Check your version of seaborn, if it is below 0.9 then you need to upgrad to 0.9 version of seaborn. Please go through this link. This might be of help. https://stackoverflow.com/questions/51402579/module-seaborn-has-no-attribute-any-graph Hope this will help",317991.0
97287,416542.0,Launch anaconda navigator and go to Environments tab and click on Updatable on right hand side and you can update seaborn to 0.9,318429.0
97287,417082.0,"Hi Deval, You could follow the discussion below on StackOverflow for this purpose. https://stackoverflow.com/questions/51846948/seaborn-lineplot-module-object-has-no-attribute-lineplot If the problem still exists, Please give proper error information so we could help you more.",332326.0
97287,418327.0,To update seaborn in Anaconda: 1)open anaconda navigator 2)go to environment tab 3)click the playsymbol on base(root) and choose open terminal 4)run this command : conda remove seaborn 5)After it is successfully uninstalled i.e. you can check by searching the package seaborn in the search option then install the new package 0.9.0 using the command conda install seaborn=0.9.0,301114.0
97218,416721.0,NA values in drop column makes sense for cancelled trips. As trip was cancelled there is no corresponding drop time.,317996.0
97218,416231.0,"Yes the values can be left as NA for Cancellations and No cars avaiable because I think if you try to drop them , it would mislead the data. If the Uber driver did not take the trip , then the drop time will be NA only.",317460.0
97218,416086.0,NA values in drop timestamp mean that values are not available. They have to be handled before moving ahead. This is part of Data Cleaning,309211.0
97218,416088.0,We can leave them I believe since they are not part of the analysis.,310974.0
97218,416102.0,Thats upto you whether you want to remove or leave as it is. But make sure the analysis doesn't get distracted.,317991.0
97218,416176.0,"I believe the drop timestamps are going to be empty for requests which are cancellations or no cars available. If that's the case, We should not be removing them",318329.0
97218,416236.0,Don't remove those. they are the major part of the sample size. and the presence of NA is itself an information for this assignment.,312490.0
97218,416470.0,"Request time(pick up time) needs to be worked on as compared to ""Drop request"" column.Drop has ""3914""NA &amp; they can be left as it is. Thank you",310508.0
97218,416539.0,NA for drop time are for cancellation and no cars available so dropping them will affect the analysis since we are going to analyse on those features..So i would not drop them....,300687.0
97218,416559.0,"I have filled NA values as 0 instead of removing them as it will be considered as part of our analysis, is it fine?",307494.0
97218,416887.0,Yes the NA value in drop time reflects the true practical scenario. We have to leave NA as it is,318814.0
97218,416963.0,"NA values are needed for correct data sets on cancelled &amp; NA Uber cars. Concept of retaining NA values will become clearer once you start plotting Bar, Line, Pie charts or during invoking of Heat maps",318827.0
97218,417088.0,"Seems we should keep droptimestamp NA, wheat about drive id NA, do we need to keep them . This also in big percentage.",312019.0
97218,417156.0,NAN values are more then 50%. We don't need to remove NA values.These are not null values.,314183.0
97221,416110.0,"Here frequency of requests will be taken on the basis of request id column. By the way there another same post on this, you can check it out -&gt; https://learn.upgrad.com/v/course/208/question/97199",318429.0
97221,416117.0,While you go with graph you will get count on bar graph. refer univeriate analysis in EDA you will get better clarity .,318732.0
97221,416178.0,you can use countplot of seaborn which takes single column and displays the frequency of the values,318329.0
97221,416219.0,"To visualize the frequency of the cars availability or cancellation , histograms plots are very useful. Some use ful link for visualization : https://pandas.pydata.org/pandas-docs/stable/visualization.html",317460.0
97221,416226.0,"It means you have to create a frequency distribution(unlike histograms it can be a bar chart). Univariate bar charts of dimensions are usually based on their count(aka frequency). so, use the count to understand how many of total requests are completed, cancelled or otherwise.",312490.0
97221,416553.0,Yes it is requests. timings. Since it is frequency it will be a range. You can use Hystogram by using seaborn library.,320689.0
98669,420118.0,yes.. it is absolutely the right approach!! ☺️,316349.0
98669,420120.0,When Na values occurs in any dataset we follow one of two approach 1. Either impute it with mean / median or any other value or 2. We delete rows having Na values. But if the column containg Na values is important for analysis and above two method may distract analysis it is better to leave Na values as it is. Hope this will help.,317991.0
98669,420686.0,"Yes, please don't delete the NAs as you'll end up with no demand supply gap!",316416.0
96748,412878.0,"For datetime64[ns] types, NaT represents missing values. Pandas objects provide intercompatibility between NaT and NaN . So no need of conveting NaT to NaN",310467.0
96748,415568.0,"In addition, the df.isnull().sum(axis = 0) does not give differences in BEFORE and AFTER conversion.",312479.0
96748,415535.0,It is just to show that there are missing values of datetime datatype. These are similar to NaN which of object datatype.,317689.0
96748,412461.0,"Those can be handled, they are just like null/nan. isnull and isnan works well with NaT too.",312490.0
96748,415952.0,They are same as nan.no implications.,318426.0
96748,416084.0,isnull function works fine for identifying NaT. https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.isnull.html,309211.0
96748,417632.0,These values are not affecting our analysis.,314183.0
98170,418037.0,Demand = number of cab requests done. i.e. requests done while Trip completed + Trip cancelled + Cabs unavailable Supply = Number of cab availability. Only trip completed is considered as supply.,332326.0
97258,416243.0,'Cancelled' and 'No Cars Available' are the requests from which we need infer while you could use 'Completed' requests for comparision purposes.,318329.0
97258,416520.0,"In order to analyse the supply and demand we need to take into account Cancelled or No Cars available as well as posted in hint section,",317689.0
97258,416246.0,"One of the main aspects of the analysis is to find the supply-demand gap and recommendations for resolving this gap. For such an analysis, you will have to consider all the statues.",313826.0
97258,416264.0,In Results Expected of problem statement it is clearly mentioned that Visually identify the most pressing problems for Uber. Hint: Create plots to visualise the frequency of requests that get cancelled or show 'no cars available' ; So it means you have to consider all the status equally important.,317991.0
97258,416357.0,The business problem states - ‘The aim of analysis is to identify the root cause of the problem (i.e. cancellation and non-availability of cars) and recommend ways to improve the situation.’ So we should consider all the types of trips as our goal is to find out the reason for uber problem of cancellation and non-availability of cars.,317460.0
97258,416473.0,"Along with 'Trip Completed' , Cancelled or show 'no cars available' are equally important.",310508.0
97258,416912.0,"we have to consider all the three types of statuses for analysis so that we can compare the percentage of completed, cancelled and no cars.",304693.0
97250,416247.0,You can follow this link to understand more https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib.pyplot.subplots **fig_kw : All additional keyword arguments are passed to the pyplot.figure call.,318329.0
99446,424994.0,"Please contact your Student Mentor for the same. I believe, they could help you with it.",311160.0
99446,425113.0,I believe you can submit the PPT .. not sure if there will be deduction of marks. It will be best that you contact your mentor.,314048.0
98203,418118.0,no such number is specified. Just make sure you support your analysis through the graphs and charts wherever possible,332326.0
98203,418359.0,there is not specific number for that. It is upto you to find which graphs are best way to communicate the answers.,318461.0
98212,418121.0,I don't think they would be specific about it as long as the graphs represent the best of what we intend to showcase,311160.0
98212,418357.0,It is upto you. you can only use matplotlib library to showcase your outcome,318461.0
98213,418130.0,"Hi, this is a part of understanding the assignment. We cannot provide you with the answers. You can follow the discussion forum questions regarding this and I am sure you will find something substantial.",332326.0
98213,418133.0,"Why should you fill anything there. Cancelled Trip means , the ride never happened.Think carefully why the Trips are being cancelled.And then you will find the answer to your question. Or , you should again watch the Upgrad video for a better understanding of the problem.",315560.0
98213,418355.0,Not need to fill any thing NA's with any thing. It indicates that the trip is cancelled by driver.,318461.0
97174,415778.0,Replace 'Nan' with 'NaN' and you won't get this error.,314197.0
97174,415813.0,datetime columns have NaT instead of NaN values so you can use isnat() in place if you are dealing with date time columns,318741.0
97174,415856.0,"Please go through below links, this might be of help for you. https://stackoverflow.com/questions/30485986/type-error-in-visualising-pandas-dataframe-as-heatmap https://stackoverflow.com/questions/36000993/numpy-isnan-fails-on-an-array-of-floats-from-pandas-dataframe-apply/41215209",317991.0
99572,426091.0,"Did you create different time slots? If so, could you get the actual numbers for each of them in each of the category (Trips Completed, Cancelled and No Cars available), before adding them to form 'Demand' and 'Supply' for total traffic as well as for taxis starting from Airport and City separately. These number are required before plotting Demand - Supply graph for both overall as well as individual graphs for taxis starting from airport and starting from City",301121.0
99572,426182.0,"I too received the same feedback. Can I ask, how many marks were deducted?",301652.0
99572,426226.0,Data Analysis 44/70,318329.0
99572,426335.0,"From the feedback, it appears that you have not put the value labels on the plots used for the presentation. Although you may have provided the numbers as part of a write-up / summary, I believe providing the numbers on the plots would help in grasping the story behind the visualization more quickly.",313826.0
99572,426978.0,"Hi Nagaraju, Do not discuss assignment feedback on DF as there might be others who are yet to attempt/submit and might get cues out of it. If you have any specific issues, the right channel is to get in touch with your student mentor and after discussing the case, you can raise a reval request if its genuine. I hope it helps.",301618.0
99572,426681.0,"While presenting graph should have label as number which are representing. Raw number, Percetage number etc.",318368.0
100480,431926.0,go through the below link : https://stackoverflow.com/questions/23415500/pandas-plotting-a-stacked-bar-chart,318017.0
100480,432065.0,Thanks this helps!,305842.0
99672,,nan,
88479,366539.0,You can try Kaggle. https://www.kaggle.com For learning more you can explore this site : https://elitedatascience.com/start-here Hope this helps! ☺️,318495.0
88479,368443.0,https://d29xdxvhssor07.cloudfront.net/schools/1399/courses/25012/paid/143226/online/attachment_Python_for_Data_Analysis.pdf?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9kMjl4ZHh2aHNzb3IwNy5jbG91ZGZyb250Lm5ldC9zY2hvb2xzLzEzOTkvY291cnNlcy8yNTAxMi9wYWlkLyoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE1Mzg0NzQ1OTZ9LCJEYXRlR3JlYXRlclRoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTUzODQ1NjU5Nn0sIklwQWRkcmVzcyI6eyJBV1M6U291cmNlSXAiOiIwLjAuMC4wLzAifX19XX0_&amp;Signature=Et4IHI0zzHccdRW6jvqXq2j8BXHoE~DAnGLwFaiXH5KWiBkbDkde6RskForOQyKUvmMoFXhSrGuIkJ0OQq3FjElHt8SwWKJmp8Z3p7fmJ0mcHRjR~fEEs-DXjrliOX~MQWhNhOVAeznug4Ira7hqto3mx7iT4WqeZKtQ0WPUK4sbulDRNymJ77Zf7vH5X~oxJsXuT6vnwJ7fUMLw2-yJKR4ZcAIjdyc0DgvbYdrlhMZNBV1ZKGuwqwILgS-ea5qLffrqYlrnuMtiCSRzHzMt6xBsteoqJB57ZU8Gs~8Gp6szPHSLS5DV-sCAph4MytVKkmw-~JoNJcG4A7KcznYUvw__&amp;Key-Pair-Id=APKAIRUT3HEDYSOHI3UA Python book for data analysis,317982.0
88479,367365.0,Please go through this article from Analytics vidhya on 18 must read books for a Data Scientist. https://www.analyticsvidhya.com/blog/2016/10/18-new-must-read-books-for-data-scientists-on-r-and-python/ Here are the links for two PDFs: Hands on Programming with R - Garrett Grolemund R Cookbook - Paul Teetor,319721.0
88550,381815.0,https://learn.upgrad.com/v/course/113/question/51710 please refer to the answers to these questions,319721.0
88550,366884.0,"I did this long back, But what I remember is ,you need to check mark MYSQL Visual Studio behind this pop up and then hit next button. If you still face issues please refer below Youtube video : https://www.youtube.com/watch?v=UgHRay7gN1g",308966.0
88933,368627.0,"Apart from concepts, for practical hands-on with the Python language, one can refer https://www.hackerrank.com/domains/python They have topic-wise questions (easy, average, difficult) that might help in getting confident with Python.",317987.0
89013,368991.0,"We can ask but I think UpGrad and IIITB are doing their best in providing insights into what happened in the previous batches and the contacts when necessary. So, let's focus on the content provided for now for learning in the first couple of months.",310974.0
89013,369007.0,"I spoke to two of my friends who did the course last year. In their batch, the course was in R which was easy compared to Python. Not sure if interaction with the previous cohort students will help us or not.",314547.0
89013,370708.0,"Hey Abhishek, if you can let us at UpGrad know why do you want to interact with students in the previous cohort we can help you. You can raise any questions you have or any problems you are facing or if you want to know anything we're here. Please reach out with specific problems and we'll be glad to help you with them.",319721.0
89650,372223.0,I found 3 ways: 1. You can use the Cell menu on top -&gt;Select Run All 2. You can use Kernel menu -&gt;Restart and Run All 3. Open the commands palatte (P)-&gt;type run all cells-&gt;enter,309451.0
90013,376218.0,"list1 = [] list2 = [] m = len(list1) n = len(list2) #initially we check if there are any empty lists as our inputs #if one of the lists is empty, then we directly print out the other list as our final list #if both are empty then our final list too will be an empty list #here we're adding the list with the highest last element because we need not change it or sort it later if list1 == []: list3 = list2 elif list2 == []: list3 = list1 elif list1 == list2 == []: list3 = [] elif list1[m-1] &gt; list2[n-1]: list3 = list2 + list1 else: list3 = list1 + list2 i = 0 j = 0 k = 0 #once we know that our both the given lists have elements in them, we try and sort them using two-pointer method #we assignm, i and j as our pointers to first and second lists respectively #we compare elements of in list1 and list2. #then, we add the smaller element to our final list while (i&lt;m)and(j&lt;n): if list1[i] == list2[j]: list3[k] = list3[k+1] = list1[i] i += 1 j += 1 k += 2 elif list1[i] &lt; list2[j]: list3[k] = list1[i] i += 1 k += 1 elif list2[j] &lt; list1[i] : list3[k] = list2[j] j += 1 k += 1 print(list3)",319721.0
90013,376283.0,"list1 = [] list2 = [] m = len(list1) n = len(list2) #initially we check if there are any empty lists as our inputs #if one of the lists is empty, then we directly print out the other list as our final list #if both are empty then our final list too will be an empty list #here we're adding the list with the highest last element because we need not change it or sort it later if list1 == []: list3 = list2 elif list2 == []: list3 = list1 elif list1 == list2 == []: list3 = [] elif list1[m-1] &gt; list2[n-1]: list3 = list2 + list1 else: list3 = list1 + list2 i = 0 j = 0 k = 0 #once we know that our both the given lists have elements in them, we try and sort them using two-pointer method #we assignm, i and j as our pointers to first and second lists respectively #we compare elements of in list1 and list2. #then, we add the smaller element to our final list while (i&lt;m)and(j&lt;n): if list1[i] == list2[j]: list3[k] = list3[k+1] = list1[i] i += 1 j += 1 k += 2 elif list1[i] &lt; list2[j]: list3[k] = list1[i] i += 1 k += 1 elif list2[j] &lt; list1[i] : list3[k] = list2[j] j += 1 k += 1 print(list3)",319721.0
90013,376286.0,"Hey, Snehal, please find the correct code above. Ther are a lot of test cases in the mentioned problem, so we need to be careful with those. It'd be grerat if you share the code you have tried and we can see if there are any improvements which can be done with them. The code I wrote is one of the possible answers, so if you share your approach we can discuss that too.",319721.0
89307,371101.0,That's great!,319721.0
89307,371152.0,started reading the book Python for Data analysis(Orelly). Can be used as reference material as well. can be downloaded by just typing the name,317982.0
89307,372148.0,"Hi, you may refer below;",301110.0
88821,369834.0,That's great!,319721.0
88821,404081.0,Tableau Lecture Notes - https://cdn.upgrad.com/UpGrad/temp/7e969b8b-2869-4a85-afb3-c58bd3b2a6dc/LN%20-%20Tableau.pdf,318458.0
88821,404046.0,Box Plots (Tableau videos) https://www.tableau.com/learn/tutorials/on-demand/reference-lines https://www.tableau.com/learn/tutorials/on-demand/box-plots https://www.tableau.com/learn/tutorials/on-demand/building-dashboard https://www.tableau.com/learn/tutorials/on-demand/dashboard-interactivity-using-actions https://www.tableau.com/learn/tutorials/on-demand/story-points https://www.tableau.com/learn/tutorials/on-demand/dashboard-layouts-and-formatting,318458.0
92693,390521.0,"ALTER is used to alter the table structure, table name etc. Using ALTER We can perform Table structure changes' examply to columns, ALTER + ADD -- to add new column and ALTER + MODIFY to modify existing COLUMN's datatype/size and ALTER + CHANGE to rename existing COLUMN /and DATATYPE But UPDATE Is to modify record values. It wont change Datatype/Name of COLUMNS You can refer https://www.tutorialspoint.com/mysql/mysql-alter-command.htm https://www.tutorialspoint.com/mysql/mysql-update-query.htm",318454.0
92693,390489.0,"ALTER is a DDL (Data Definition Language) statement. Whereas UPDATE is a DML (Data Manipulation Language) statement. ALTER is used to update the structure of the table (add/remove field/index etc). Whereas UPDATE is used to update data. in other words The ALTER changes the table in the database, you can add or remove columns, etc. But it does not change data (except in the dropped or added columns of course). While the UPDATE changes the rows in the table, and leaves the table unchanged.",317689.0
124944,545183.0,You can refer to this book - Hands-On Machine Learning with Scikit - Learn and TensorFlow By O'Reilly Media,420665.0
82268,337181.0,Welcome Happy Learning and socializing,312357.0
82268,336828.0,Happy Learning :),305845.0
82268,336884.0,Welcome.. We all are excited to learn new world of data science.. I joined 2days back and finished R an Python.. Contents are lucid and to the point.. Hope you also enjoy.. Happy learning..,312746.0
82268,339795.0,Welcome to upgrad and All the best for the course!!!,306038.0
85179,,nan,
77209,308578.0,"Thanks Sreenath. I did complete R, atleast to the intermittant level. But when I got onto the learning paltform, I would simply click on continue learning and that path would always force me to go for Python next. But then I realized that if I chose the Program tab, then I can skip Python and move on to the other modules.",300717.0
77209,308483.0,"Hi Veena, irrespective of your language choice, you have to complete the below modules in the preparatory course Intro to SQL Tableau SQL In addition, you will be assigned either ""Intro to Python"" or ""Intro to R"" based on your choice of language.",301276.0
77209,324005.0,"There is no Preparatory Cource for Tableau then why its mentioned I can see R, Python, SQL prep. but no Tableau. Why ?",301110.0
82287,337584.0,Non-sampled test cases are visible once you have exshausted all the attemps.(either right answer or false answer),304813.0
82287,337678.0,Please avoid hardcoding and ensure it works for generic test cases.,301109.0
82287,337050.0,"I think, Test cases can not be viewed. Run your code by providing custom test cases. If you can share your code, we would give better ans.",312746.0
82287,337071.0,Please share your code and let us know in which question you have got this.,301648.0
82287,337149.0,"just verified, You can not view the non sample test cases untill you are able to pass that challenge.",312746.0
82287,337287.0,"Do not do any hard coding. Keep it simple and do according to the objective asked in the question. Then only your code will pass the test cases. For e.g hard coding means you would have defined the input the values maybe for list or array. Hence if any test cases provides some other inputs, the test cases are bound to fail.",310482.0
74870,297992.0,"If you are more likely at programming side ,let's take an example you are passionate to write the code good at loops ,case conditional statements then Python would be apt for you...",300693.0
74870,297874.0,"I have chosen Python, as it has wider appeal. Also, with whatever links Upgrad provided, Python is gaining more prominence nowadays. R is still the goto language for data analysis as of today. But if you want to learn a language with wider appeal apart from DS, Python might be the one.",300685.0
74870,298385.0,its depends...but for data scientist or analyst R,301131.0
74870,298461.0,"I chose R as it is harder to learn and it will be better to have support via Upgrad and IIIT-B Python is definitely gaining popularity and, based on all new blog posts, the leading language for DS. It is an easy language to pickup so I will learn that myself outside of Upgrad in parally with learning R. I see myself using Python much more but a good DS needs to know both R and Python, IMHO",300694.0
74870,298476.0,"Hey Prakash, Definately Ptyhon is getting more preference over R now days. Howeverpick any one language for now and have more focus to learn the Techniques &amp; Presentation of findings-storytelling . Language will come and go away over the period but business problem and techique will existis for ever. I found R more easy over python :)",301110.0
74870,298747.0,"No choice is wrong. Python has an advantage in ML (machine learning), deep learning (Tensorflow etc) and building tools. It is a general purpose open source language that can be used for data analysis, creation of web pages and API's etc etc. On the other hand R was built by statisticians for statisticians purely for data analysis, and has an advantage in statistics and visualization. R is also an open source language. Both languages have huge community support.",300748.0
74870,299324.0,"I belive Python is a better choice for someone new for Data Analytcs like me. Its comparitively easy to learn and I believe can be used to solve all the problems that R can.Also, Python is beter suited for ML.",300698.0
74870,302547.0,"R has computational limitation in long run. But thats when you are dealing with large data set. Also just learn the concept using any language. You can also switch to another one later on with experience. As I am already working in Python, I chose python.",303082.0
74870,319078.0,R. Given its stability and more usage.,307016.0
74870,326516.0,R is a specific for data Science. Python is used in various other places along with data science. choose wisely. good luck!,308643.0
84779,347442.0,"Hi Rohit, Follow Randy on Linkedin Platform: https://www.linkedin.com/in/randylaosat/ He seems to be helping the aspiring Data Scientists through his frequent posts and comments.",311160.0
84779,347548.0,The content provided here is quite good and sufficient for a beginner. Recommend you to go through this first and look for sources outside if it doesn't quench your thirst for more. Happy learning !!!,310974.0
84779,347802.0,"Data Science is vast and expanding. Understanding and learning everything in one day is not possible. I will suggest, prepare what is being taught in extensive way. This is good enough for you to get a job or crack interview. Thereafter, you can try extensive handson and learning more algorithm.",301555.0
84779,372842.0,"Hi Rohit, Here is a list of some of the most useful blogs and websites for everyone—from data science experts to complete newbies. In addition to offering advice and learning resources, this list of data science blogs is also full of websites to keep informed about job openings,current news, trends, and opinions from professionals. 1. Data Science Central Run By: Vincent Granville Website link: DataScienceCentral.com 2. SmartData Collective Run By: Social Media Today Website link: SmartDataCollective.com 3. What's The Big Data? Run By: Gil Press Website link: WhatsTheBigData.com 4. No Free Hunch Run By: Kaggle Website link: Blog.Kaggle.com 5. insideBIGDATA Run By: Rich Brueckner Website link: InsideBIGDATA.com 6. Simply Statistics Run By: Jeff Leek, Roger Peng, and Rafa Irizarry Website link: SimplyStatistics.org 7. Datafloq Run By: Mark Van Rijmenam Website link: Datafloq.com 8. Data Science 101 Run By: Ryan Swanstrom Website link: 101.DataScience.Community 9. Dataconomy Run By: Dataconomy Media Website link: Dataconomy.com 10. Data Science Report Run By: Starbridge Partners Website link: StarbridgePartners.com/Data-Science-Report",318822.0
81384,331674.0,"It is advisable to focus on the machine learning concepts instead of the software, be it R or Python. They are just a means to implement your theoretical knowldege. If you know one of them, you can pick up the rest in a month's time.",301276.0
81384,331929.0,You can go throgh below link for your answer which reduce your doubts. https://www.datacamp.com/community/tutorials/r-or-python-for-data-analysis?utm_source=adwords_ppc&amp;utm_campaignid=1455363063&amp;utm_adgroupid=65083631748&amp;utm_device=c&amp;utm_keyword=&amp;utm_matchtype=b&amp;utm_network=g&amp;utm_adpostion=1t1&amp;utm_creative=278443377092&amp;utm_targetid=aud-299261629574:dsa-473406585795&amp;utm_loc_interest_ms=&amp;utm_loc_physical_ms=1007828&amp;gclid=EAIaIQobChMIjJrskbnr3AIVVxePCh2PqgbtEAAYASAAEgIcevD_BwE,307843.0
81393,331561.0,"To the 1st question This is where python is different. You don't particularly need any programming background to pick up in python since it is very easy (english like) to interpret various keywords and what they can do. However, it helps if you are from computer science background since you can relate to the programming constructs if you have taken the course in your graduation.",310974.0
81393,334920.0,"In the Python track you will find details on Python while in R you will find the same on R. In the introductory tutorials of Python you will learn the basics of this language. I would rather suggest to learn both R and Python. For Python, you can supplement your learning with a fantastic Youtube Playlist by Derek Banas : https://www.youtube.com/playlist?list=PLGLfVvz_LVvTn3cK5e6LjhgGiSeVlIRwt Hope this helps. Happy learning!",310217.0
81393,332013.0,"It is not necessary to have much of a similar programming background to learn Python and/or R. Both are vary easy and you can learn the basics of it from the prep course offered. If you still have doubts, you can refer to some youtube channels or other online learning platforms. The basics concepts like, lists , functions, arguments, variables and many such is taught in the python section.",304389.0
84820,347538.0,"Map, Reduce &amp; Filter are not straight forward to understand but if you apply them once, you won't forget. See if the below ones help you: http://book.pythontips.com/en/latest/map_filter.html https://www.python-course.eu/lambda.php",310974.0
84820,348227.0,"Yeah, even i noticed the same problem, i had to look upon other tutorials on youtube to properly understand it.",316368.0
84820,370243.0,"i agree...the tutorials were not clear enough on the concepts, while the problems given were of a higher and difficult level ( esp. for people with non programmming background)",310509.0
81850,333714.0,I am still figuring out and on research mode between the two languages. Before hitting one button out of two,312357.0
81850,333707.0,"Since you belong to the Market Research what I found both R and Python are similar what they are doing. However if you have maths background then Python is bit easy than R and you will found pyton will more intresting with other visualization. Now it is youc choice and see what is your strong area and which comes under what plate,ultimately you have to decide. Better we all have to decide early :)",307843.0
81850,334021.0,"Once you go through introductory module, go through in detailed by chlicking all the links. Spend some time, you will get to know according your strong area. Python is usually used nowadays almost everywhere, so once you start working on it, it will be easy for you.",311117.0
81850,333680.0,"The article in Introductory module-&gt;R vs Python , will give you the message that sometimes R is suited for a particular task and sometimes Python. This article specifically -&gt; (https://qz.com/1063071/the-great-r-versus-python-for-data-science-debate/) Take a look at the same dilemma discussed previously; might help you decide:- 1. https://learn.upgrad.com/v/course/208/question/81700 2. https://learn.upgrad.com/v/course/208/question/77126 3. https://learn.upgrad.com/v/course/208/question/77130 4. https://learn.upgrad.com/v/course/208/question/75903",301652.0
81850,338298.0,"My take on to your question - R has full package of built-in statistical libraries. R is widely used in Research and statistics and rigorous analysis. When it comes to Python, as it is general purpose capable of handling all sort of statistical libraries, you can do everything you do in R, the main advantage you get is you can expose your model as service or consume one. R is considered a better toolkit for visualization like ggplot2 while Python also has visualization lib like matplotlib, bokeh etc but in terms of better thing, R is a winner. Third point, Both are equally capable from data handling point of view. R has a steep learning curve than Python. So you can decide if your inclination more on research and all math stuff then go for R or if you want all blend of benefits then Python is the one you would want. I hope it clears your dilemna.",304813.0
81859,333703.0,"I found upgrad is best and first platform for me which uplift the passion currently and smooth learning. Below are the list i found intresting as well sharing 2 from the list, pls chk if it helps. https://www.datasciencecentral.com/ https://www.feedspot.com/infiniterss.php?q=site:http%3A%2F%2Fwww.kdnuggets.com%2Ffeed",307843.0
81859,335494.0,Check out these websites https://www.analyticsvidhya.com/ https://www.kdnuggets.com/ https://www.datasciencecentral.com/,301276.0
77024,307060.0,"Hi Shankar, R or Python is just a medium to implement your machine learning skills. So the focus must be on understanding the fundamentals of ML. If you learn either of them (R or Python) you can grasp the other in a month's time.",301276.0
77024,307343.0,Thanks Sreenath,303227.0
77024,307520.0,I would like to prefer Python. Any Comments?,304396.0
77024,307810.0,Python is the only thing I would prefer.,304692.0
77024,317002.0,"Well, Python works better in production. But, R is way better to wrangle data as of now. The choice is really yours. I started with R. But, I learned Python somewhere along the way. My suggestion: Learn both. Data science becomes way easier if you do.",306733.0
77024,339053.0,"As per my reasearch, following are my insights 1) Dataanalysis point of view R and Python both will do well and pretty much similar. 2) R has 12000 packages available in CRAN (open-source repository). The upside is it is possible to find a library for whatever the analysis you want to perform. Downside is that we may lost in finding the right package from this huge repository. 3) However with Python most of the job can be done with five libraries Numpy, Pandas, Sccipy, Scikit-learn and seaborn. 4) Python also makes the replicability and accessibility easier than R. Moreover, if we want to use the results of your analysis in an application or website, Python is the best choice. I feel the above point 4 is very important for my need.",314244.0
75903,302239.0,"A data scientist needs to be a jack of many trades. As a successful data analyst/scientist, you will eventually end up knowing both R and Python down the line. Who knows, you may end up using SAS for your organisation, it still holds the highest job market share. We all need to be flexible enough to deal with whatever is thrown at us. For the course, go for something that will help you understand the basics. Other than that everything else is usually syntax and libraries. I'd suggest python. Python is fast gaining ground because of its ML and deep learning capabilities. And it is equally good as R in data analysis. If you want to eventually venture into AI look no further. The trend is fast moving towards Python, but then you can never place bets on a language in the open source world. Also, a language being easy is not a bad thing. Python is heavily used in Google, Facebook, Instagram and in many big companies because it is a powerful language and used not just for analytics.",300748.0
75903,302358.0,Please refer Introduction to Data Management Module 1 Session 1,301131.0
75903,304474.0,"If there is no immediate need for learning and using either at your current job/role, which seems to be your case, choice can be python. Python is far simpler and easy to pick up. In some cases like mine, there is a need to learn and use R at work, so I picked up R. Again there is Open R, Microsoft R of 2 kinds MRO, MRS and what not !",300717.0
75903,306582.0,"R basically eases up your job when you make a presentation and graphs. But when it comes to computation, Python is fast with large data sets. You can choose either of it for start",303082.0
75903,326515.0,"Hi Antony, Easier things can be learnt by everyone. The difficult ones which might be useful in long run are chosen by few due to the level of difficulty. My personal suggestion is to work with heart and soul with the one the helps you in the long run. Rough times are not permanent but your learnings are. Initially it might be difficult but as time passes it turns to a child's play. Wish you all the best!",308643.0
77130,307567.0,"From a learning point of view, my personal recommendation is Python as it's learning curve is less steeper.",301276.0
77130,307769.0,Python is the best way for beginners,304692.0
77130,307838.0,R will be easier to learn.,304816.0
77130,308659.0,Choose any one langauge and master it either R or Python and try to have some intermediate level of understanding of other. Anyway these languages are just data funnel but data scientist main role is to provide useful insights from data,300735.0
77130,308576.0,Definitely go for Python !,300717.0
77130,309069.0,Go for Python .,301648.0
77130,311795.0,i am also new to programming but i have little bit knowledge about Python.very simple syntax easy to understand..i will tell you obiously Python.,305847.0
77130,312076.0,"I'll say Python.Comparitively easy to learn and as per recent surveys is a preferred language for DS,ML etc.",300698.0
77130,313038.0,Go for python because the syntax is easy to understand and python will be the language of data science followed by R,305838.0
77130,331228.0,Python is better compared to R as per current industry requirements.,310585.0
77130,319237.0,It's a subjective question. It all depends on how you want to use this skill going forward. R and Python both are open source programming languages but Python has more community support. It is also helpful if you are planning to grow more into deep learning and AI. I would suggest Python if you are completely new to both languages.,307486.0
84932,349010.0,welcome in Upgrad..!!,305847.0
84932,348060.0,Welcome !!!,317980.0
84932,347949.0,"Welcome Puneet, Wish you Great Learning !!!",310974.0
84932,347962.0,Welcome!!,318446.0
84932,347996.0,Welcome Puneet to This forum. Happy Learning..,312746.0
84963,348111.0,"Welcome, happy learning !!!",310974.0
82502,338230.0,Welcome.. We all are excited to learn new world of data science.. I joined 5days back and finished R an Python.. Contents are lucid and to the point.. Hope you also enjoy.. Happy learning..,312746.0
83412,343129.0,"yes both you can read in preparotory session but after starting the course what majorirty has choosen, we have to follow that either Python or R.suspected Python will be start as i can see in majorithy vote of python(80%).",305847.0
83412,343108.0,But even if chose a certain prepretory module the course commencemnet will be either Python or R right ?,315464.0
83412,342590.0,Welcome to the Data Science family. For Safer side :- you can choose R which has both modules. What i can predict after seeing the voting percentage that course will start in Python.,312746.0
83412,342629.0,"Yes, you can study R if you have chosen python or vice-versa. It is up to you.",301648.0
83412,343012.0,"yes , you will be able to access R sessions also even if you choose Python",315765.0
83412,343081.0,"Yes, absolutely you can access both the preparatory sessions.",304692.0
83412,343337.0,"The course will be in Python, so i will suggest try to code mostly in python. R Session is for your skill up. Normlly, companies wants you to have good skill in R/Python and some decent knowledge on the other tool too and hence both the tool is provided for you.",301555.0
83412,344227.0,Yes u can access R content even after prep course,308437.0
85283,349814.0,"Very Difficult question.. 1- Even i am from java background, I prefer R since My company is already using it. 2- Some apis are not available in python, In that case we have to use R. But on other side of the coin :- 1- we should know python because it has richer api for deep learning.",312746.0
85283,351744.0,"Hello All, Today i have started Data Science Journey:-) Good Whishes to All:-)",320008.0
85283,350420.0,"Python is good for all Data Science componets except Timeseries, where we have better inbuilt packages in R. It is companieschoice, you can do same thing in both. What you want is a choice you have. However, companies needs someone having good knowledge in one and little info on the other.",301555.0
85317,349989.0,There are various books but I am following below two books:- 1- Learnign Python ( it gives you deep insight from very basic to advanced level)- it has around 1600 pages 2- The quick Python book - For quick study ( around 365 pages),312746.0
85317,350423.0,"There are many books in python available, depends on what you are tryig to learn you can choose. Basic Pytho, Advanced or python for data science, graphs with python are the categories based on which you shoild select books. https://medium.mybridge.co/19-free-ebooks-to-learn-programming-with-python-8f6f0ad4a7f8",301555.0
85317,351592.0,For entry level: Think in Python Dive in Python Head first Python For intermediate to expert level: Python cookbook Python in a nutshell Fluent Python Effective Python,317845.0
85317,375065.0,"I started out with a ""byte of python"", its extremely simple and the language is not at all dense. And the best of all its free.",318019.0
82639,338839.0,"Your code is perfectly fine. Just run your code by clicking Source button in R-Studio. Dont run using ""Run"" button.",312746.0
82639,338922.0,"Readline when used as declaritive mode with multiple lines of coding and executed it retuns NAN as the compiler doesnot understand what to read having multiple lines. Therfore fore when readline is used in such way you first run only the readline line and provide the input then run the next line of commands. i.e number&lt;-as.integer(readline(prompt=""Enter number"")) # first run this and provide input then run thebelow command it will work for(i in 1:number) { print(paste(number,""x"",i,""="",number*i)) } # Note there are also alternative way in which you can run the all lines of code at once in R -studio just you need to speifically tell the complier in the beginnning you can google it more .",307843.0
83479,342933.0,Thanks... i didn't see that.,315765.0
83479,342988.0,Curriculum is divided into following manner on upgrad:- Program -&gt; Courses -&gt; Modules-&gt;Sessions-&gt;Chapters. Tablue Module is available in Course 2.,312746.0
83479,342932.0,Did you try this?,310974.0
87005,358793.0,https://chat.whatsapp.com/A3RKBo4bRjnK035iXnOgpZ,314730.0
85750,353366.0,"Hi, Can you let us know which module are you referring to ?",301618.0
85750,353374.0,Coding and Solution Module,320008.0
88711,367413.0,"Use excel functions for this 1. In another column use functions =IF(ISNUMBER(A2), A2, 10) //** this is for second row ie A2 **/ 2. Select and drag the cell till the bottom, you can use shortcut also for copying forumla 3. Then use average on the computed values. The trick of using another column was shown in one of the videos by Prof. S. Anand in the prep content.",306248.0
88711,367395.0,"You need to change all ""10+"" mentioned in the columns to ""10"" you can use find and replace to do this. after doing this calculate the average, you will get the correct answer. because at the moment, 10+ is not being counted in your average (ie excel just ignores that row, since it is not a number), and you need to count it as 10 to get the real average. (as mentioned in the question). Please upvote, if it helps.",317998.0
88711,367398.0,"you need to replace all in Experience column, 10+ to 10 , I was also confused in it.",318319.0
88711,367400.0,"The column "" YearsOfExp "" has some cells with value as ""10+"" which is a value of string type . All the rest of the values are of numeric type. When the average function works on all the values in this column, it will consider only the numeric values thus leaving out the cells with value as ""10+"". So we need to come up with a data prepration step to consider this situation. I would suggest to create another column say ""Exp"" which gets populated based on the value in the ""YearsOfExp"" column. If the value is ""10+"" set the value as 10 else keep the value as is. Later you can use the ""Exp"" column for answering this question.",313826.0
88711,367553.0,"Select Years of experience column, click ctrl plus H Replace 10+ with 10 and then click on Replace all. Let the column years of experience be selected and Now see the bottom right bar where average is displayed as 4.83... close to 5",317811.0
88711,367636.0,Replace 10+ with 10. it is being treated as string and hence not used in calculating average.,317689.0
88711,367668.0,Just replace 10+ with 10 to get the correct average.,301890.0
88711,367887.0,if you are using R to solve the problem load csv file into Rstudio Replace 10+ to 10 change the column to as.numeric get the mean on column,300708.0
88711,368105.0,you have to replace 10+ with 10 for this question. seems like 10+ won't be considered as numeric.,300685.0
88711,368667.0,"'10+' is a string and 10 is a number. If we tried to find the average on the column values, it will ignore the 'non-numeric values', in our case, it ignores '10+' There are some 190+ values of '10+' all of which are getting ignored leading to incorrect result.",317987.0
87446,361336.0,You can install the Jupyter in two ways: 1. using Anaconda- Install Anaconda its distribution of python. once you install Anaconda all the python related tools will be installed. you can easily install the additional tools/package 2. Using python pip- if you are familiar with Python then you can use python package manager PIP. Below is the links. http://jupyter.org/install,317845.0
87446,361286.0,Use below link to download Anaconda. it comes with Jupyter notebook. https://www.anaconda.com/download/#windows,320073.0
87761,364330.0,"Please do make sure whether your system is 32 bit or 64 bit because tableau has different versions for both. Once you finished with Installation, it will pop the product activation window (you need to enter product key) follows by Email id form. You can use the product key mentioned in the first segment of the module. Activate with your product key: TCHR-A128-D3A0-AD9C-CB67 Or Tableau offers 1-year free Subscription for College Students. So as you have taken admission into IIIT-B, you can use your IIIT email id and register for the student subscription on tableau website. They will send you the Subscription key through email within a day.",319721.0
90066,375013.0,Please let me know in the comments. We are supposed to create group of 4 members.,317990.0
90066,375265.0,Hello Sandhya.. I would love to be a part of the group. You can ping me on WhatsApp - 9300508989 And then we can form a group for the case study.,318499.0
90074,375263.0,Hi Varun! It's good to see you as a part of this course. Let's have a lot of fun while learning!,318499.0
90074,375092.0,"Welcome, Varun. Have a gr8 learning experience.",310974.0
89850,373852.0,I think there is no such list that is shared with us and some people has share google sheets to update their details location wise.,318329.0
88284,365380.0,"if you have done all the session (except graded session which will unlock on 30th september ) of sql and R, Your progress should be 50% in each. Once you complete the graded session, you will get 'Completed' status.",312746.0
88284,365621.0,Wait for the practice questions to be launched. You are good to go for now.,319721.0
92611,390219.0,Yes .. It's running. whats the issue at your end?,319006.0
123899,546070.0,"You can refer to this link:- http://www.sthda.com/english/wiki/writing-data-from-r-to-excel-files-xls-xlsx You can also refer to the following link for more details: https://help.upgrad.com/ If the problem persists, do let us know.",420665.0
125095,545538.0,"there are multiple option available in market check out for mongo db, Cassandra or Hbase.",318476.0
85083,348604.0,"Maths module is equally important but if you are fairly new to programming side, first concentrate on that as you can always go back to Maths module since it'll be more of a revision.",310974.0
85083,348673.0,"But do finish all the modules, even it is optional.",301555.0
78757,318061.0,"SOH stands for 'Sine equals Opposite over Hypotenuse', which is how it is mathematically calculated. The below links will be useful http://www.mathwords.com/s/sohcahtoa.htm https://www.mathsisfun.com/algebra/sohcahtoa.html",301276.0
82418,338019.0,"The issue is with the square brackets in tc expression . You cannot add an integer and a list with a + operator. ( [ .75*(n-1) ] is treated as a list and 3 is of type int, these cannot be added using + operator) (Note-&gt; TypeError: unsupported operand type(s) for +: 'int' and 'list' )",301652.0
82418,338171.0,"Its working now, thanks",302734.0
77935,,nan,
82430,337916.0,"The way to find an answer is this what i have done, is there any better way to do it?",302734.0
77964,312621.0,"Yes, it's optional, however, It is always better to clear your concepts with the topics covered under that section which will help you in near future when data science and machine learning comes into picture. If those concepts are clear it will be easier for you to relate them in real time examples.",300688.0
77964,313031.0,If your concept is cleared for the mentioned topics then you can proceed with the further sessions,305838.0
77964,317342.0,"Bro its better to complete Linear Algebra, Statistics and probability, and calculus. I have covered these topics.",304692.0
77964,338435.0,"It is optional. However, a brush up of the concept will be good.",314048.0
77964,339181.0,"Sorry, I just joined Upgrad 2-3 days ago and did not see there were multiple answers posted.",314048.0
83548,343357.0,then hope this helps,310585.0
83548,343322.0,click on prelaunch program and you can find the pre learning mathematics.hope it helps.,300687.0
83548,343688.0,"Use below path, Program --&gt; course1(Introduction to data management) --&gt; Module1 (Welcome to Pre Launch Program) --&gt; Session2 (Pre read topics in Mathematics )",312746.0
83889,345520.0,Got answer...else statement should start immediatly after closing if statement {} should not be in next line,315455.0
89376,370669.0,It means if u are using an IF statement in python you need to use indentation of 4 white spaces or tab so that the next statement after the IF statement is executed as part of IF statement only...If no indentation is given the folloelwing statement is not part of IF,318358.0
89376,370675.0,"In programming languages when we write if-else statement we enclosed the next statement of if and else into curly braces ' { }' to tell compiler that statment inside curly braces belongs to particular if or else. For ex: if(a&gt;5) { a=a+1; print(a); } else { print(b) } Similarly if we want to apply same thing in Python, we don't have luxury of curly braces { } to write same code in python. So to write same code in python we use indentation i.e space. For ex: if(a&gt;5): a=a+1 print(a) else : print(b) So here statement a=a+1 and print(a) are indented and belongs to if statement. A good practice is to use 4 spaces as indentation which is also a size of tab (when we press tab it moves to four space forward). Hope it helps to clarify your doubt.",317991.0
89376,370845.0,use the flower brackets not a problem with them because they are not readable by the python. For your convenience but try to get on with the indentation bro,304692.0
88876,368417.0,"You can try and do it in the following way: def rec(x): if(x==-1*n) print(x) else: rec(x-1) print(x) rec(n) Here n is the given input, and we perform a recursion call on numbers 1 less than n till it becomes equal to -n. when it is -n we just print it, and in other cases we print it after the recursion call is over. for eg, if n was 1. then we would call the function recursively till x becomes -1. at this point we print -1. after that the remaining part ( print in else part) of the fuinction is carried out, which will print 0 and 1 in sequence after that. hope this helps.",317998.0
88876,368427.0,My take at this : n = int(input()) def ret_num(n1): if abs(n1) &lt;= n : print(n1*-1) n1 -= 1 return ret_num(n1) ret_num(n),313826.0
111724,481356.0,Writing Custom Algorithms is beyond the scope of this Course and I believe would need deep understanding of at least code of the existing algorithms.,311160.0
88691,367289.0,I would recommend to stick to what UpGrad is providing (additional resources given as well) in this learning platform as they are focused on concepts that are needed for analytics. This should be more than enough.,310974.0
88691,367255.0,"Hi Gowrisankar, I found this link to be very helpful in learning Python. https://www.w3schools.com/python/ It explains each feauture/functionality with examples. Hope it helps you and others as well.",317998.0
88691,367332.0,"Hi Gowrisankar, For Basics: https://www.w3schools.com/python/ Basics and a bit of DS libraries: https://www.learnpython.org/ https://docs.scipy.org/doc/scipy/reference/tutorial/ http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/ For realtime scenarios and practice visit https://www.kaggle.com/",320195.0
88691,367437.0,"Following this method will allow you to learn programming principles using the Python language rather than just learn Python as a programming language. l earn Python the Hard Way has been the go to free resource to learn Python along with general good programming principles. This resource may still require you to develop in Python 2.x which is good because there is still a butt load of 2.x code out there. Finish this course and make sure you understand everything. However, Python moved forward and so should you. https://inventwithpython.com/ is a brilliant resource for learning Python 3 (again from scratch). There are five tutorials/courses to do. Finish all of them. Now that you have a great basis, register on CheckiO and solve as many problems as you can. Here you are learning to code by playing a game. Solve as many challenges as you can, then try and solve each challenge with as little code as possible. Also register at https://www.hackerrank.com and start solving problems. The better you do, the better your chances of getting hired. These guys are hooked up to some serious employers, and your rank counts. Remember to register on Stack Overflow where you will find willing and helpful programmers that can answer any well formulated questions you may have. By now you will have enough knowledge to tackle Test-Driven Development with Python - this knowledge is critical as a skill in the workplace. And there you have it - a full curriculum that you can do within three to six months (depending on your aptitude and the amount of time you have available). please upvote if this helps. :)",305847.0
90433,377136.0,There are many logic you can choose a sub-array 0. Below is the link which helps to find out the sum for the specific hashin logic whicn is O(n) i.e it takes min time to provide the list of all sub-arary which adds to 0. https://stackoverflow.com/questions/5534063/zero-sum-subarray,307843.0
92637,390300.0,Only way is practice :),310974.0
92637,390312.0,"Creating List [ ] [1,2,3] Creating tuple () (1,2) When you are traying to extract any value from list/tuple.dataframe/series etc that time you should use [ ] Getting 2nd value from list lst = [1,2,3] lst[1] Getting 3rd value from tuple (1,2,3)[2] Get value is [ ] Calling functionis always ( ) Now one good example, creating array in numpy np.array([1,2,3]) np.array is a function. Hence ( ) It takes list as argument, hence [1, 2, 3] Now to get 1st value from array we should use square bracket passing the index np.array([2,3])[0] Now len() is a function. When you want to get length of a tuple you may want to pass tuple to the len function. len((1,2,3))",318554.0
92637,390458.0,"this is just a rough understanding which is helping me out till now; when you're extracting any column or row data.. or you're referring to any column or row data.. or any calculations involve rows or columns content passing; we use [ ] whereas for all other normal calculations, function, any other operations involve using round brackets; ( ) very lenient but have been helping me till now..",316349.0
92637,390866.0,"Practice and remember. Else install JetBrains PyCharm Community Edition. This software helps you write your code and when you enter ""."" or [] it will give you possible values that can be written. Something like helper for script. Scala user uses intelliJ from JetBrains. If you know IntelliJ you will realize it is similar to that. I do not use this, but can recommend. JetBrains are easy to use.",301555.0
92637,390484.0,"Functions have (). Whereas if we want to access elemets of list, tuples, arrays, dataframes we use [],",317689.0
92637,391106.0,More practice more perfection.,314183.0
91426,389927.0,"In prepatory session, it is on first page .you can download from there.",314183.0
91426,382275.0,have you completed sql in prepatory seasion ? company_db is there on the first page of that session go get it.,318017.0
91426,382281.0,You can get the company DB Schema from here - https://learn.upgrad.com/v/course/208/session/15786/segment/79808. Thanks.,311502.0
91426,382342.0,The company DB is also available as an inbuilt schema in MYsql workbench. So you can find it on the left navigation panel in workbench.,310511.0
91426,387357.0,Thank you this helped me. I am able to create Companydb schema and created all the table using DDL scripts provided by RC video. in the pre-launchin session: Basic SQL Data Retrieval with SQL session. Create companydb schema in MySQL then execute DDL commands provided in the session. Pre-Launch Preparatory Content keyboard_arrow_right Module 3 keyboard_arrow_right Session 1 Basics of SQL,316215.0
88678,367160.0,"Something wrong with the code. I belive it has something to do with vowels or something, right? please share the question or your code snippet to debug the error",317998.0
88678,367162.0,"import ast,sys input_str = sys.stdin.read() input_str = 'analytics' if input_str[0] in ['a','e','i','o','u']: print (""YES"") else: print (""NO"")",304338.0
88678,367165.0,"why have you written input_str = 'analytics' this part? after writing this, you have basically changed the input to 'analytics' and thus output will always be YES. try again after removing that line.",317998.0
88678,367166.0,"just remove this line ""input_str = 'analytics'"" from your code. ie. the 3rd line. remove it entirely and try again. What that line is doing is hard coding your input to be ""analytics"". so, whatever input you give, your input will be set to 'analytics'. Also, you should use the lower() function to make the code better. since, some inputs might contain Capital vowels, and those will give an output NO according to your code. so, your code should be: import ast,sys input_str = sys.stdin.read() if input_str[0].lower() in ['a','e','i','o','u']: print (""YES"") else: print (""NO"") Hope that helps. Please upvote my answer, if it helps. thanks.",317998.0
88678,367176.0,"Some guidelines on writing code in the coding console: 1. The console is already having a code stub to accept the input required for the problem statement. So, there is no need to write any code for getting the input required for the problem statement. 2. For the reason mentioned above, the input should NOT be hardcoded. 3. There would be a section in the console marked as ""# Write your code here"" or ""#Type your answer here"". The logic pertaining to the problem statement needs to be put here. In the code snippet provided, the variable input_str has been hardcoded to ' analytics '. As a ersult, any other input provided through other test cases would get over-written to 'analytics' and end up printing ""YES"" all the time. Remove the hardcoding and it should execute successfully.",313826.0
88678,367219.0,"no need to give any hard coded input , Question is already taking from user or test case will provide the input. below code will work: import ast,sys input_str = sys.stdin.read() if input_str[0] in ['a','e','i','o','u']: print (""YES"") else: print (""NO"")",318319.0
88678,367263.0,Since you already getting system input why introduced again. Might be you have tried coding elsewhere and paste it here. Using lower function is a good practice whenever input judged.,319869.0
88678,367652.0,"Don't hardcode any input. In the code stubs while running it automatically takes in input. In the following code the input_str has been commented out as if you set it to 'analytics' than it will always show output as true. import ast,sys input_str = sys.stdin.read() # input_str = 'analytics' if input_str[0] in ['a','e','i','o','u']: print (""YES"") else: print (""NO"")",317689.0
88678,367873.0,"Basically, Run code checks your code syntactically, not functionally. Verify and Submit checks your code syntactically based on input test cases. Now, the coding questions on the platform have two kinds of test-cases, i.e. sample and non-sample test-cases. The sample test-cases are the ones which are evaluated when you click on 'Verify' and the non-sample test-cases are the ones which are hidden and are evaluated when you click the 'Submit' button. So its possible that your code is syntactically correct and passes the sample test cases, but is failing the non-sample test cases. Hope it is clear now.",310511.0
88678,367886.0,code failing on clicking submit button could be because of code unable to handle some test cases. go through the code and make sure code is accepting input value and not hardcoded. also check if there is any memory overflow etc,300708.0
89232,370033.0,Take a look at following documentation to create multiple indexing. https://pandas.pydata.org/pandas-docs/stable/advanced.html,317689.0
89232,370228.0,"You can use below command as well to generate the index while creating the dataframe df = pd.read_csv( 'myfile .csv' , index_col = [list of columns])",320073.0
112491,485393.0,try this: https://stackoverflow.com/questions/40305122/log-file-to-pandas-dataframe,318017.0
112491,485411.0,What I understand from you is that you would like to add text to a file for the purpose of recording and analysing at various stages of the codes. Here is the link for the same. Using this you will be able to write to a text file and close after doing that. https://stackoverflow.com/questions/31499257/how-to-write-a-data-to-a-text-file-in-python I hope this helps.,301121.0
112491,485564.0,Logging in python is very efficient and easy to use. You just have to define a python module for logging using python internal logging module. You can define as many logger as you want. You can also configure it to print the output to a console as well write to a file. This can be refer from here: https://stackoverflow.com/questions/49580313/create-a-log-file https://gis.stackexchange.com/questions/246809/python-script-to-generate-log-files-with-time-and-date-and-output-events-and-err,311117.0
112491,485536.0,"You can refer to the ""Logging"" library which is bascially used for tracking the flow of the program like debuging, information, warning messages and error messages https://realpython.com/python-logging/",312259.0
89267,370144.0,"Sorting the data frame on Text sorts by text only since April starts with with A so its sots before March since March staats with M and it appears after a,b,c.....m. Hope it helps",307843.0
89267,370195.0,"April is the first month when you order in an ascending order, alphabetically.",310511.0
116442,503369.0,Hope this helps!,313200.0
116442,503405.0,"When lambda = 0,there is high chance of overfitting. This is because when lambda is zero it is unregularized regression which means we are concerned about minimizing the error terms only, without considering complexity. So when we try to minimize the error terms it ends up in over fitting since the model tends to learn everything (hidden patterns,noise and inconsistencies) about the dataset in the process. Lower values of alpha results in underfitting It means that the model is simple and not equipped to deal with hidden patterns,noise and inconsistencies on the test dataset which results in underfitting.",311254.0
116442,503382.0,"In regularized regression, the objective function has 2 parts, Error term and Regularization term. Regularization term also has 2 parts 1. hyperparameter (regularization coefficient, lambda) 2. Sum of square of model coefficients -Ridge Sum of absolute values of model coefficients -Lasso Lambda is the balancing factor. If we are choosing a high value for lambda, it implies we are giving more importance on making the model simple (one of the way is to make model coefficients simple). Simlarly, if we are choosing a low value for lambda, it implies we are giving more importance to bring down the error(RSS). In this case we are not giving any emphasis on making the model simple. When lambda=0, the importance is on making the Error term (RSS) as low as possible. That will result in a complex model. And it will work well with train dataset. However it results in vey low test accuracy when tested on unseen test data. This will lead to Overfitting . When lambda is very high , the importance is in making the model as simple as possible, resulting in a very simple model compromising on accuracy of the model. This leads to Underfitting .",310467.0
118062,509051.0,You can know order of magnitute of value of alpha by below piece of code: model_cv.best_params_,318429.0
118422,513344.0,"Hi Chetan Answering your questions one at a time. 1) Why is scaling done before the train/test data split? this impacts the test accuracy -- This seems to be an error as it is not advised to share any details about the training data to the learning algorithm and this will defeat the purpose. Appropriate changes will be made in the content to reflect the same. 2) what kind of scaler is ""scale""? -- Can you please elaborate which python function you are referring to here? 3) Since scaling is done on the entire X data, even dummy variables seem to be scaled...why is this? -- Scaling will not have any affect onn the dummy variables.",304281.0
118422,513490.0,"2) i am referring to the above code here 3) when i did the assignment using the above process, i saw that even the dummy variables were scaled.",310509.0
116596,504082.0,Refer this:,310974.0
116596,504134.0,"The Explained Sum of Squares (ESS) is the difference between mean of the output and predicted values . The Residual Sum of Squares (RSS) is the difference between the actual and the predicted values: If the value of ESS is close to TSS, then the model is able to explain the total variance in y to a large extent. On the other hand, if ESS is much smaller than TSS (and thus RSS is large), then the model is not able to explain much of the variance in y.",310467.0
116596,504138.0,"TSS, RSS and ESS (Total Sum of Squares, Residual Sum of Squares and Explained Sum of Squares) Consider the diagram below. Yi is the actual observed value of the dependent variable, y-hat is the value of the dependent variable according to the regression line, as predicted by our regression model. What we want to get is a feel for is the variability of actual y around the regression line, i.e., the volatility of ϵ. This is given by the distance yi minus y-hat. Represented in the figure below as RSS. The figure below also shows TSS and ESS – spend a few minutes looking at what TSS, RSS and ESS represent. Now ϵ = observed – expected value of y Thus, ϵ = y i – y^ . The sum of ϵ is expected to be zero. So, we look at the sum of squares: The value of interest to us is = Σ (y i – y^ ) 2 . Since this value will change as the number of observations change, we divide by ‘ n ’ to get a ‘per observation’ number. (Since this is a square, we take the root to get a more intuitive number, ie the RMS error explained a little while earlier. Effectively, RMS gives us the standard deviation of the variation of the actual values of y when compared to the observed values.) If s is the standard error of the regression , then s = sqrt(RSS/(n – 2)) (where n is the number of observations, and we subtract 2 from this to take away 2 degrees of freedom*.) Now <v:shape id=""Picture_x0020_7"" o:spid=""_x0000_i1027"" type=""#_x0000_t75"" alt=""regressionTSS"" style='width:87.6pt;height:20.4pt;visibility:visible;mso-wrap-style:square'> <v:imagedata src=""file:///C:/Users/Vipul/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png"" o:title=""regressionTSS""> Source - https://www.riskprep.com/component/exam/?view=exam&amp;layout=detail&amp;id=131&amp;Itemid=57 Hope this will help.",317991.0
116596,505017.0,"TSS , RSS and ESS (Total Sum of Squares, Residual Sum of Squares and Explained Sum of Squares) The residual sum of squares (RSS) , the sum of squared residuals (SSR) and the sum of squared errors of prediction (SSE) are the same. So the RSS = SSR = SSE The explained sum of squares (ESS) , is the same as the model sum of squares (SSR) but the residual sum of squares (RSS) is not the same as the SSR. So here the ESS = SSR ≠ RSS The expression is known as the total sum of squares (TSS). This sum can be divided into the following two categories: Explained sum of squares (ESS): Also known as the explained variation , the ESS is the portion of total variation that measures how well the regression equation explains the relationship between X and Y . You compute the ESS with the formula Residual sum of squares (RSS): This expression is also known as unexplained variation and is the portion of total variation that measures discrepancies (errors) between the actual values of Y and those estimated by the regression equation. You compute the RSS with the formula The smaller the value of RSS relative to ESS, the better the regression line fits or explains the relationship between the dependent and independent variable. Total sum of squares (TSS): The sum of RSS and ESS equals TSS RSS + ESS = TSS R 2 is the ratio of explained sum of squares (ESS) to total sum of squares (TSS): R^2 = ESS/TSS You can also use this formula: R^2 = 1 - RSS/TSS B ased on the definition of R 2 , its value can never be negative. Also, R 2 can’t be greater than 1, so 0 ≤ R 2 ≤ 1 please refer the below link: https://www.dummies.com/education/math/business-statistics/test-the-estimated-regression-equation-using-the-coefficient-of-determination-r2/",314183.0
116597,504081.0,"Yes we still need to do feature engineering (extraction/transformation), RFE before Ridge/Lasso.",310974.0
116597,504147.0,"Please visit below link, similar kind of question is answered with explanation. Although it does not talk about RFE specifically but the explanation is given for feature selection methods. https://stats.stackexchange.com/questions/149446/do-we-still-need-to-do-feature-selection-while-using-regularization-algorithms Note: RFE also comes under feature selection method. https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html Hope this will help.",317991.0
116597,504661.0,Thank you all. I am facing another issue while RFE selection. I am trying to select 25 column out of around 250 column and it's running for longer time and system hungs up. Can you please suggest how it can be resolved.,308965.0
116597,504860.0,"It is not very clear from the link provided above if overfitting is not the reason for choosing the feature selection (RFE) and what is the difference in feature selection made using Lasso regularization and RFE ?. As from the lecture Lasso reduces the insignificant or redundant features ( assuming them as highly correlated features) coefficients to zero - do we still need to do a feature elimination before Lasso ?. Also, please help to understand example scenario where both (RFE &amp; Lasso reg ) are required to arrive best possible feature set . TA please help.",311115.0
116597,504776.0,"What is the outcome of this? I read the article shared by Vipul above but it doesnt solve the issue. Should we try two models, one with RFE and other without RFE for Lasso and Ridge? How does it make sense to perfrom RFE for Lasso? TA please respond,",304814.0
117995,508724.0,"We need to undertake following total count of model building: 1 + ( p ( p+1 ) / 2 ) for example if u have 3 predictors.. in 1st round u will not drop any predictor &amp; make model with all predictors.. in round 2.. you will drop each of the 3 predictors once and make model with other 2 remaining predictors.. note the model KPIs.. then in round 3.. you will drop 2 predictors and make model wilth 1 remaining predictor, note the model KPIs... in total you will make 7 models with 3 predictors..",318791.0
118442,510548.0,"Lambda is something which will not be optimized by the model but needs to be tuned by the user. Hence, it is a hyperparameter.",318329.0
118442,510995.0,It is a hyper parameter because Lambda is regularization factor of the model and it is a parameter set before learning begins. It is not a learning parameter.,301121.0
117642,507644.0,"AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form [-2log L + kp ], where L is the likelihood function, p is the number of parameters in the model, and k is 2 for AIC and log( n ) for BIC. AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model, so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. Each, despite its heuristic usefulness, has therefore been criticized as having questionable validity for real world data. But despite various subtle theoretical differences, their only difference in practice is the size of the penalty; BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC. AIC and BIC are both approximately correct according to a different goal and a different set of asymptotic assumptions. Both sets of assumptions have been criticized as unrealistic. Understanding the difference in their practical behavior is easiest if we consider the simple case of comparing two nested models. In such a case, several authors have pointed out that IC’s become equivalent to likelihood ratio tests with different alpha levels. Checking a chi-squared table, we see that AIC becomes like a significance test at alpha=.16, and BIC becomes like a significance test with alpha depending on sample size, e.g., .13 for n = 10, .032 for n = 100, .0086 for n = 1000, .0024 for n = 10000. Remember that power for any given alpha is increasing in n. Thus, AIC always has a chance of choosing too big a model, regardless of n . BIC has very little chance of choosing too big a model if n is sufficient, but it has a larger chance than AIC, for any given n , of choosing too small a model.",314183.0
117642,507507.0,Please refer to the TA approved link given below. It is not absolute of them but values to compare two different models https://learn.upgrad.com/v/course/208/question/106829,301121.0
117642,507484.0,"AIC and BIC are both penalized-likelihood criteria. They are sometimes used for choosing the best predictor subsets in regression and often used for comparing nonnested models, which ordinary statistical tests cannot do. The AIC or BIC for a model is usually written in the form [-2log L + kp ], where L is the likelihood function, p is the number of parameters in the model, and k is 2 for AIC and log( n ) for BIC. AIC is an estimate of a constant plus the relative distance between the unknown true likelihood function of the data and the fitted likelihood function of the model so that a lower AIC means a model is considered to be closer to the truth. BIC is an estimate of a function of the posterior probability of a model being true, under a certain Bayesian setup, so that a lower BIC means that a model is considered to be more likely to be the true model. Both criteria are based on various assumptions and asymptotic approximations. BIC penalizes model complexity more heavily. The only way they should disagree is when AIC chooses a larger model than BIC. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative. BIC works well for datasets having more records.",301648.0
118058,510320.0,You can refer to the additional modules for the same.,319302.0
118058,509980.0,Upgrad has provided additional modules to understand the images. The content on that model is easily understandable and you'll be able to understand the contours and intersection shown in diagrams. please refer to: https://learn.upgrad.com/v/course/208/module/10777 Hope this helps.,302735.0
118064,508909.0,"This code is added to plot the graph the neg mean absolute error/r2 score performance for test and train data . To make the grpah more legible, it has been restricted to plot only for alpha till 200.",316211.0
118505,511039.0,Evaluate in general on any linear Regression instead of just using statsmodels.api for modeling. Is there any preBuilt function or do we need to write our own Function in python.,317984.0
118505,511106.0,"This is how I did : from RegscorePy import * print(""AIC "" + str(aic.aic(y_train, y_train_pred, len(X_train.columns)))) print(""AIC "" + str(aic.aic(y_test, y_test_pred, len(y_train.columns))))",310974.0
118505,511823.0,"https://learn.upgrad.com/v/course/208/question/118333/answer/510214 Unless it's for your own academic purpose, you can use GridSearchCV and/or cross_val_score to calculate the socres.",318438.0
118505,516793.0,"Use the below calculations for AIC and BIC. It is taken from the source code of regscorepy library. resid = np.subtract(y_test_pred, y_test) rss = np.sum(np.power(resid, 2)) aic_score = n*np.log(rss/n) + 2*k BIC = n*np.log(rss/n) + k*np.log(n) k = len(X_train.columns) n(sample size) = len(X_train)",318756.0
117308,506220.0,"I also faced the same confusion initially, but while going through Comprehension- Features Subset Selection, https://learn.upgrad.com/v/course/208/session/31156/segment/164590 , Continue this iteration by increasing the value of d by one till you reach d=p (if d&lt; p+1) then this loop will continue. If d&gt;p then it will stop where, d is a counter, and p is the total number of predictors. TA may verify the correctness of same.",311117.0
115987,501590.0,6 to the power uses less bits of the system as compared to other equation so the system will take much time in calculating 5.825 to the power 2. complex are equation which has multiple features involved and the coefficient of the features is a float.,318017.0
115987,502198.0,"Hi Nihal, Please follow the attached video for your question https://www.youtube.com/watch?v=Q81RR3yKn30 Please comment here if you have still doubt on this.",344894.0
115989,501666.0,lambda is the ridge regularisation factor which is added so that the data can perform better when tested on the test data set,318017.0
115989,502211.0,"Hi Nihal, X*X^T create a square matrix always create Square matrix but it might be singular (Determination of matrix = 0), then that case if we add lambda*I, which will make non-singular square matrix. Which will always be inevitable",344894.0
115989,503868.0,"Hi Nihal, X*X^T would create a square matrix, but all square matrix are not invertible. it is possible to have a noninvertible square matrix. One way to check whether a matrix is invertible or not is to check for its determinant. If the determinant of the matrix is zero then the matrix is noninvertible. Adding a lambda*I matrix to X*X^T increases the possibility to have an invertible matrix for (X*X^T +lambda*I). To get more insight on invertible matrix you can refer to below video: https://www.khanacademy.org/math/precalculus/precalc-matrices/intro-to-matrix-inverses/v/singular-matrices?modal=1 Hope this helps",306725.0
138769,599020.0,"In my opinon, the performance of a model can be improved by the following ways broadly: 1. More Data and better and refined (cleaned and less data quality issues) Data. Since, the better the quantity and quality of data, the better the performance of the model would be. 2. Optimum algorithm and fine tuning of it by tuning Hyperparameters. Since different algorithms would be better for different use cases, trying different models and using the most optimum one along with fine tuning by the hyperparameters would help get us better model performance. 3. Creating of Ensembles. As we were taught in the decision tree modules/course, an ensemble can be created to get a better overall model performance than an individual model. Hope that helps.",317998.0
117324,506094.0,"there is no rule that we need to do it before or after. t testing data points represent real-world data. Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance). Therefore, you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points.",317982.0
117324,506104.0,"Honestly there's no rule in feature scaling to be done before/after test-train split. But, its better to scale variables after test-train split because scaling before splitting makes the values of test data to be somewhat related to train data. That's in a way we can call cheating. To have a good model without any bias, I'd suggest to do scaling after test train split in that way, the test data will be scaled as per the data available in test only. This will give you a more rhobust model testing.",307176.0
117324,506403.0,"When you scale a dataset you actually take into account ALL the rows of data - so you have two options: 1) scale BEFORE split - this is not a good idea IMHO because you are implicitly tying up the test and train dataset patterns to each other and therefore not ensuring a good test if your model is generic enough or not 2) scale AFTER the split - this is better because the test and train datasets are not influencing each other (if you look at the ways we normalise or standardise you will see that the resulting output are affected by the twos present in the dataset, ofcourse)",300694.0
117051,504662.0,yes. outlier treatment is a part of data preparation and should be done before these models also.,311686.0
117075,504895.0,"a. Simple explanation is that the metrics will be undefined when n&lt;p. b. In order to be able to perform backward selection, we need to be in a situation where we have more observations than variables because we can do least squares regression when n is greater than p. If p is greater than n, we cannot fit a least squares model. It's not even defined. For more details check out the following link: https://discuss.analyticsvidhya.com/t/why-backward-selection-can-not-be-used-when-n-p/8161/5",311857.0
117077,504796.0,Yes it is lambda that was used by the professor in the videos. Alpha is the hyper parameter in jupyter notebook.,310974.0
117077,505001.0,I had raised this question before. Please find TA verified answer for this in the below link. https://learn.upgrad.com/v/course/208/question/116233,310467.0
117077,506986.0,alpha is the Name of Parameter that was termed by the professor as Lambda. So both alpha or lambda is nothing but regularization parameter So we are just creating a list of all possible values of Alpha/lambda as Param and passing it to GridSearchCV to find the optimal value of alpha,317984.0
116126,502256.0,complex nature is not only about the bits complexity has many features if one of your coefficient is 50 and other is 2 then the complexity of the two variables will make the equation complex as one variable change the equation much more.,318017.0
116126,502474.0,"Hi Chandan, Did you get a chance to go through the additional content on Ridge and Lasso regression? https://learn.upgrad.com/v/course/208/session/31696/segment/167497 If not, I suggest you take a look. It might answer your question. For a ridge regression, the regularization contour is circular with origin at 0. Now the general equation of a circle with centre (0, 0) and radius ‘r’ is given by f(x,y)=x2 + y2 = r2. In our context, if you think x and y are coefficients(assuming only 2 coefficients for the model), then x2 + y2 are sum of squares of coefficients. Thus the regularization term is given as sum of squares of coefficients.",310511.0
117875,508227.0,"You need to see what is the significance of an empty value. Say for a flat, the number of swimming pools might be kept as empty implying that there are no swimming pools in that flat. You need to have a look at the data dictionary and see where the null represents something and where it is the case if genuinely missing data points. Then you need to deal with it accordingly.",304281.0
118209,509678.0,This should help you https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/,310974.0
137984,596866.0,It could help you eliminate features when the data points are lesser than that of the features,311160.0
137984,599390.0,"It is more beneficial in terms of computation resources needed. In the case of Best feature selection, there will be computation for all possible combination. However, with the forward stepwise or backward stepwise feature selection, the number of models to evaluate comes down drastically.",317987.0
117135,505010.0,"i get that, i meant asking mathematicall the relation between them; lambda increases, RSS decreases since we're emphasizing on regularization term lambda decreases RSS increases since we're emphasizing on error term correct??",316349.0
117135,505009.0,"Lambda is the balancing factor. If we are choosing a high value for lambda, it implies we are giving more importance on making the model simple (one of the way is to make model coefficients simple). Simlarly, if we are choosing a low value for lambda, it implies we are giving more importance to bring down the error(RSS). In this case we are not giving any emphasis on making the model simple. When lambda=0, the importance is on making the Error term (RSS) as low as possible. That will result in a complex model. And it will work well with train dataset. However it results in vey low test accuracy when tested on unseen test data. This will lead to Overfitting . When lambda is very high , the importance is in making the model as simple as possible, resulting in a very simple model compromising on accuracy of the model. This leads to Underfitting .",310467.0
117135,505067.0,Please refer the below link: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/,314183.0
117135,505338.0,https://www.coursera.org/lecture/ml-regression/how-ridge-regression-balances-bias-and-variance-697eG this link explain better. our professor didnt tell this. but question is coming from this content,311404.0
117135,505025.0,"Hemant, its opposite. Let me put in this way. As lambda increase , emphasis is on regularization term, which is to make model simpler choosing to compromise on model accuracy, which increase RSS . Similary, when lambda decrease , emphasis is on decreasing the error term(RSS) which makes the model complex. Hope, this help!",310467.0
117135,508296.0,"Refer to link below clearly explains effect of changing value of lambda on RSS, variance, bias http://wavedatalab.github.io/machinelearningwithr/post4.html",301118.0
117135,508788.0,"I have seen all the answers but not satisfied with any. Here is the perfect answer : First understand RSS : it is Residual sum of square which means (Actual value- Predicted value)^2 , i.e. if RSS is large (Actual value- Predicted value) is large that means model has not guessed the data points correctly and giving a large difference between (Actual value- Predicted value) and incorrectly guessed most points and linear regression line is a not a good fit but if RSS is small (Actual value- Predicted value) is small this means model is not making many errors and covering maximum data points and model is good fit with good regression line covering maximum data points but here is the twist as the model fit is good and RSS is small that means variance is high also and model complexity is also high , which in turn means model won't work good on Test Data , so in ridge regression when ""alpha"" or ""lamda"" increases it decreases model complexity and this increases RSS that means increases in Alpha is making training data less accurate and decreasing the model complexity but also giving good results in Test Data. So, we need to find a good value of Alpha which won't distort the perfect fit line too much i.e. won't increse RSS too much but also reduce model complexity. In last, increasing ""Alpha""(Lambda) will decrease model complexity by increasing RSS(errors in perfect fit Linear regression Line) and decresing Alpha will decrease RSS and increase model complexity. Threfore, with the help of CVV we need to find the right value of Alpha(Lambda) whihch will strike the right balance between variance/bias/RSS/Lambda",315560.0
116211,502631.0,Please refer to the link given below https://learn.upgrad.com/v/course/208/question/107163 AIC and BIC can be used for comparison purpose of two models of same nature.(not absolute values themselves) Both these penalize discourage using more features instead of fewer.,301121.0
117459,506889.0,when the model has 0 parameter that means the slope of the model will be 0 and it will only have a intercept for it. in simple words there cant be a model with 0 parameter in forward you need a single parameter to start with and that should be the one which is highly correlated with the target variable and for backward you can have all parameters and can remove the ones which are not imp one by one,318017.0
116233,508410.0,Yes Scikitlearn uses alpha for lambda,305650.0
116233,502672.0,lambda,318360.0
116233,502673.0,Alpha are the co-efficients that model is trying to determine. Where as Lambda is value we supply to control behaviour of the model. Hence that is the hyperparameter,317996.0
116233,502685.0,"hi Keerthi, I also got confused here. but subsequent videos cleared the doubt. actually while explaining the concept professor is using the term 'lambda' for the hyperparameter. but in python coding, this value is to be passed as 'alpha' for the model. the value passed as 'alpha=0.1' will be treated as hyperparameter in the code for the same purpose.",311686.0
116233,502707.0,"Lambda value explained in theorotical explanation is used as alpha by scikit learn models. So to answer your questions wrt the video sessions, Lambda and Alpha are same.",307495.0
116233,503873.0,"Theoritical purpose it is denoted by Lamda, but in python library it is used as Alpha",311004.0
116233,505645.0,Alpha is not the hyperparameter. Scikitlearn uses alpha for lambda,301643.0
116233,508821.0,Scikitlearn uses alpha for lamba !,311466.0
122163,531582.0,scalling is only necessary when the data is highly distributed and also l2 regularisation handle the outliers so scalling is a option but not a necessity all the time.,318017.0
116241,502663.0,"If the absolute value is not taken (the signs of the errors are not removed), the average error becomes the Mean Bias Error (MBE) and is usually intended to measure average model bias. Reference https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d",310974.0
116241,502818.0,"For the Mean Square Error, sklearn.cross_validation.cross_val_score flips the sign and always return negative value. In short Mean Square Error is positive only sign is flipped For more details please follow the below link https://github.com/scikit-learn/scikit-learn/issues/2439",317514.0
117157,505349.0,refer to the bias-variance trade-off plot.. thqt captures this very well.. consider this based on thr complexity &amp; simplicity of the models with the lambda value.. and then see the trend of variance/bias if decreasing/increasing with effect.. looking at this plot does help a lot.. i could solve all correctly looking through this plot..,316349.0
117157,505404.0,"RSS is a measure of bias. As lambda increases, bias increases. Bias increases, variance decreases (bias-variance trade-off)",305653.0
116270,503098.0,It is the same as bias (RSS) and variance (lambda).,310974.0
116270,502794.0,"RSS is nothing but error term. lambda is the regularization coefficient(hyperparameter) of the regularization term. lambda is the balancing factor. It is the hyperparameter that we give as an input to learning algorithm(ridge regression). It plays an important role in the type of model that is produced by ridge regression. If we are choosing a high value for lambda, it implies we are giving more importance on making the model simple. Simlarly, if we are choosing a low value for lambda, it implies we are giving more importance to bring down the error(RSS).",310467.0
116270,503418.0,"Lambda is multiplied to the predicted values and than added to the error term in the ridge regression equation. So, if the lambda is way too high than your error terms become insignificant and vice versa.",317689.0
117867,509882.0,"Hi, After calculating Lasso &amp; Ridge coefficient, train your model with that coefficient and evaluate your results.",344894.0
116362,503398.0,"Mallo'w Cp is one among the many metrics used to account both fit and simplicity of a model. Formula is as given below. Mallow's Cp = (RSS+ 2 dσ2)/n I assume, the second question is based on one of the questions (non graded ones) in the module, where it compares ​Cp​ and ​BIC. BIC = (RSS+ ln(n) dσ2)/n The only difference between the above two formulas are highlighted in bold. when n&gt;7 for e.g. n=8, ln(8) =2.08 which is greater than 2. So BIC value &gt;Cp value when n&lt;=7, ln(n) &lt;2, so Cp value &gt; BIC value Hope, this helps!",310467.0
117510,507096.0,"You can zip the columns of X train with the model coefficients. Model coefficients will be in the same order as the order of the columns of X train that lasso/ridge used. Below code can be used. cols = X_train.columns cols = cols.insert(0, ""constant"") list(zip(cols, lm.coef_))",310467.0
117510,507907.0,"You are correct. They are coefficients of different features in an array. In the Lasso, you will find many zeros in that array, which means those features are getting eliminated.",301121.0
117910,508582.0,This should be pretty obvious if you've gone through the course content. You use cross validation.,310974.0
117910,508825.0,"@Sahil, The hyper parameter controls the trade off between how accurate the model is in the training data and how generalised the data to be. The model is dependent on the value of hyper parameter. We do a grid search over a range of values of hyper parameter. We do not know what its value will be. We will try a range of hyper parameters and through cross validation and find out the average accuracy / error incurred by a model produced by the value of hyper parameter. We do this over multiple values of hyper parameters and see which one works for us",314048.0
117916,508488.0,Through plot we can see where it deviates more. Nearby we can do some increment values of alpha in loop we can build the different models and see the accurary. This shows best alpha value with train and test accuray.,312019.0
117916,508580.0,"Select a value where the metric value of train and test are close. In doing so, make sure you are not selecting a value that results in overly complex/simple model.",310974.0
117916,508757.0,"use ""model_cv.best_params_ "" to find the value. This will give the actual best value of alpha rather then just seeing at graph.",313767.0
117879,508250.0,"Basically, we use Lasso to select features and then you can apply a ridge on the selected features to get the relevant model.",304281.0
117879,508638.0,"Hi Vikas, does this mean that we need to first do Lasso and then on the Non Zero coefficient variables we need to do Ridge?",304814.0
118034,509026.0,"Hi, Please go through the below link https://discuss.analyticsvidhya.com/t/why-backward-selection-can-not-be-used-when-n-p/8161/5 https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/model_selection.pdf",344894.0
118033,508796.0,"Hi Rishi, I suggest you access the link given below for the discussion on this topic. https://discuss.analyticsvidhya.com/t/why-backward-selection-can-not-be-used-when-n-p/8161",301121.0
120861,524887.0,All the steps will remain the same. The cost function which will be used by the algorithm will consist of the regularization parameter. Rest all the steps will remain unchanged.,344353.0
126340,550933.0,"Hi Chetan, To answer your question let's take the cost function, as discussed for regularization Cost value = Error value + regularization value. Error value &amp; Regularization values are functions of coefficients. Regularization value, ONLY, is also directly proportional to λ. Error value is inversely proportinal to coefficient (as higher no of coefficient means lower error) Regularization term is directly proportinal to coefficient Now, for a particular value of α, the regularization term will increase if λ, thus increasing the overall COST. Similarly, with decreasing λ, the overall COST will also come down. So, to correctly interpret your question, you must understand, λ DOESN'T influence Error value. It influences the Regularization term and overall COST value. So, as we increase λ, the regularization ensures that the model doesn't select too many coefficients and overfits in the process",305839.0
116490,503698.0,"hi, 'y' as function of the attributes means y can be determined using some kinds of operations on the attributes. y= a+bX1+cX2 y=sin(X) y=X1^2+X2^3 all these are 'y' shown as functions of attributes only. however the question is more interested in finding the features that can be derived from raw attributes which then can be used to determine 'y' as a linear equation model over those features.",311686.0
116490,504302.0,Can the TA please give a clarification on the choices given as answer on this question? I understand that this is a graded question and the explanation cannot be given until submissions are complete for this section. However I would appreciate an explanation after that time period of submissions are over.,317149.0
116489,503692.0,"I am not sure if I understood your question correctly. but let me attempt. idea is to get a model which is not complex. complexity can arise from multiple factors including values of coefficients and the format of the raw attributes that come in final equation. for example if attributes are X1 and X2 then a model like y = 2.5+5X1^3+7ln(X2) will be a complex model. so, instead we should create features in the data set itself for X1^3 as X3 and for ln(X2) as X4 and then use these features in our model making the equation as y=2.5+5X3+7X4. this model is simpler. hope this helps.",311686.0
116518,503800.0,Same questions has been answered by different colleagues. For more clarification you can visit below links with answers verified by TA's. https://learn.upgrad.com/v/course/208/question/116290 https://learn.upgrad.com/v/course/208/question/116238,317991.0
116518,503797.0,"Hi Praneeth, In Generalized Linear regression we are checking if the coefficients ae linear instead of x1, x2 as x1, x2 data are with available with us in dataset. These coefficients are variables till we finalize the model. Coefficients a,b,c are linear but coefficient d is inside the sin function. Sin(dx4) can be expressed as multi-degree polynomial using Taylor's series, which means you will get various degrees of the coefficient d. Hence it is non-linear. I hope this clarifies.",301121.0
116518,504568.0,"If you closely observe, the coefficient d in the equation is not linearly related to y. By simple rule, a linear relationship should make an incremental change in the output variable and vice-versa, whereas cosine function is not linear. Try looking at the relationship between y and d only, the presence of cosine function makes it non-linear.",306726.0
118107,509049.0,Try implementing solution in below link: https://stackoverflow.com/questions/53723928/attributeerror-series-object-has-no-attribute-reshape,318429.0
118107,509191.0,thank you,305804.0
118047,510863.0,"Hi, Errors need to be independent but NOT normally distributed https://newonlinecourses.science.psu.edu/stat504/node/216/",344894.0
117279,505776.0,Y should be linear with the coefficients. csin(dx4) - has two coefficients c and d. y is not linearly dependent on these 2 coefiicients.,310467.0
117279,505833.0,The coefficient of feature variable x4 is not linearly dependent on Y. Y is dependent on SIN(d). Hence it is not linear,317996.0
117279,506101.0,Remember the concept of Linear Regression. Its not the linear relation between the variables(features) but actually the linearity in the co-efficients of variables. 'd' here is a co-efficient and its contained in the sin() function sin(dx4) which defies the linear relation between co-efficients rule to determine the equation as Linear.,307176.0
117279,506103.0,Hi Malika Please refer to the TA approved link given below. https://learn.upgrad.com/v/course/208/question/116518,301121.0
117279,506235.0,Same questions has been answered by different colleagues. For more clarification you can visit below links with answers verified by TA's. https://learn.upgrad.com/v/course/208/question/116290 https://learn.upgrad.com/v/course/208/question/116238 https://learn.upgrad.com/v/course/208/question/116518,317991.0
117279,508438.0,"you are multiplying coefficients c and d in the last term, hence non-linear",320690.0
117279,507965.0,"Hi The function is non linear because of terms ex2,csin(dx4). Exponential and trigonometric functions are non linear in nature. If function consists of x^2,x^3...then also it is non linear. Using Taylors series and Maclaurin s series trigonometric functions,exponential functions can be expressed as infinite series Example: <o:OLEObject Type=""Embed"" ProgID=""Equation.3"" ShapeID=""_x0000_i1025"" DrawAspect=""Content"" ObjectID=""_1615407087""> <o:OLEObject Type=""Embed"" ProgID=""Equation.3"" ShapeID=""_x0000_i1025"" DrawAspect=""Content"" ObjectID=""_1615407108""> <o:OLEObject Type=""Embed"" ProgID=""Equation.3"" ShapeID=""_x0000_i1025"" DrawAspect=""Content"" ObjectID=""_1615407108""> <o:OLEObject Type=""Embed"" ProgID=""Equation.3"" ShapeID=""_x0000_i1025"" DrawAspect=""Content"" ObjectID=""_1615407045""> sinx= 1-(x^3)/6 + (x^5)/120+...... https://www.math24.net/taylor-maclaurin-series/",308638.0
116021,501762.0,Below link might help:- https://www.quora.com/What-is-the-difference-between-linear-and-non-linear-features,320103.0
116021,502044.0,A linear function by definition is a function in the first degree. It has no powers or exponents or log. You can have a linear function in two or more variables. total degree of any term in the function should not be more than one. ex: x1 +x2 is linear function in two variables. x1 * x2 is not a linear function as the degree is 1+1=2 Hope this helps.,311857.0
116021,501996.0,Try thinking in this way. If you increase/decrease both x1 and x2 and see how new feature increases/ decreases. If this seems to follow a linear trend in case of x1 + x2 than its linear. But in case of x1*x2 or x1/x2 it will not increase/decrease linearly to the orignal attributes.,317689.0
116042,501851.0,"Hi Darshna, X1, X2 and X3 are the raw attributes. and features are cos(x1), e^-x2 and e^x3",344894.0
116042,501997.0,"Raw attributes is x1, x2 and x3. Similar to linear regression operation of kind: y = ax1 + bx2 + cx3 +d. Here raw attributes are x1,x2,x3. If you apply some feature engineering i.e. creation of new feature of kind x1/x2 than raw features would be x1 and x2. But in this given example you are just applying a function to the raw attributes to get the regression curve. As given in the description in the video page: The derived features could be combinations of two or more attributes and/or transformations of individual attributes .",317689.0
116054,,nan,
116156,502364.0,"On additional step, to verify the performance on the test data to check if the model is not overfitting",317514.0
116156,502325.0,"""in order to best fit a non-linear curve to the data, we figure out how close is the set of predicted values to the set of given values for the response variable just like in simple linear regression. "" In this case the response variable is number of customers. Plot the original data and plot the predicted values based on Polynomial and curve_fit functions. Whichever is closest to the original data points is the best fit between the two. But even before getting to that, looking at the original data plot you might get an idea/hint about what kind of function would give a best fit. As mentioned in 'Generalized Regression Framework - 1' ""the generalised regression process consists of the following two steps: Conduct exploratory data analysis by examining scatter plots of explanatory and dependent variables. Choose an appropriate set of functions which seem to fit the plot well and build models using them""",311857.0
116156,509085.0,"What exactly is the use case of Polynomial and Curve_fit? A) Curve_fit will use non-linear least squares to fit a function to data. In this case Michaelis-Menten function. In fact, you can define you own function and use curve_fit to fit the model to data or optmize the coeff. of the model. the syntax is: scipy.optimize. curve_fit (f, xdata, ydata) If my understaing is correnct, polynomial feature will create a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. Here too you can fit and transform the data. You can try one thing: 1) use polynomial feature to get a function that defines your data, then run a curve_fit to fit and optimize the model. 2) run curve_fit with Michaelis-Menten function and compare which model is suitable for your data. Hope this helps!",302735.0
125585,547569.0,you should check the best variables using rfe and then filter the required variables .,318017.0
116080,502010.0,"hi Darshna, As the subsequent videos explain, once we start using the term 'features', it includes all the functions that can be applied on any raw attribute or combination of those. So, even the raw attributes as they are or multiplication of two or more raw attributes will also be considered as 'features'. And not all such possible features are used in modeling. As mentioned in one of the videos , we need to decide some of the possible features from business logic, graphs (like scatter plot) , intution, some statistical hit-&amp;-trial etc. And then move ahead using those features.",311686.0
117832,508096.0,This should be obvious by now but you can refer this https://blog.algorithmia.com/introduction-to-loss-functions/,310974.0
116238,502668.0,By looking at the common functions? https://www.mathsisfun.com/sets/functions-common.html,310974.0
116238,502652.0,"A pure sine or cos curve can actually be expressed as a polynomial function using Taylor's power series which is fairly complex but degree of that polynomial may have to be high to come very close to sin or cos curve. Having said that, best curve fit need not pass through all the points original curve but approximation shoul be fine.",301121.0
116238,502732.0,1. First simple way is sin(0) is 0 while cos(0) is 1 2. Second sin(90) is 1 while cos(90) is 0,317514.0
116238,502935.0,Sine curve pass through origin. Cosine doesnt .,316211.0
116238,505094.0,Sine and cosine function graphs have different behaviour having range from -1 to 1 . So these behaviour of graphs helps us to identify the function,318427.0
116244,502671.0,Hi Karthik Please refer to TA answered link with examples on raw attributes and features and this will clear your doubts. https://learn.upgrad.com/v/course/208/question/116042,301121.0
116244,502797.0,"Hello bro, Raw attributes are generally like Eg: Date of the construction, Price, square feet etc. Feature in Date is make a seperateColumn age of the House: [current date - Date of the construction] this is the feature using the raw variable according to our purpose.",304692.0
116244,502813.0,Features are functions derived from the raw attribute.Features can be infinite based on the available raw attributes . As features can be created by using multiple functions on the raw data we can have infinite features.,310629.0
116244,502662.0,"Raw attributes are the variables/attributes/columns that come with your data as is with no transformations. Features are what you derive from the raw attributes and are those which are gonig to be inputted to the model. More often than not, raw attributes can't be inputted to model as is as they may not explain the variance better.",310974.0
116244,505098.0,Raw attributes are basically the data provided to us and we can derive various functions from this raw data.,318427.0
117158,505194.0,"hi, if we create features from target variable then it means that we already know the value of target variable. this further means that we will use this 'value' (through the features created) in our model to predict that very same 'value'. the purpose of regression model is thus getting defeated.",311686.0
117158,505340.0,"Target variable is the one which your model is trying to predict. The training data will be having this variable using which your learning algorithm learns and eventually we come up with a model. The real test data will not be having the 'target' variable at all. And the model is expected to predict this. For ex, suppose you are developing a model to predict BP of a person based on age,sex,height and weight. You get a dataset containing all these attributes plus the target variable bp , using which you build a model. Now, you canas a new person to enter the details to your model i.e., age,sex,height and weight and yoir model will predict the BP for those values. As you can see, in this case, the BP was not knwon to the model beforehand. Hope this answers your query.",313826.0
117852,,nan,
116288,505092.0,"Linear attributes follow a linear relation , mostly following a straight line graph where as non - linear attributes does not follow straight line and builds a non - linear model.",318427.0
116288,502891.0,As mentioned in the session Linear combination of attributes is ax1+bx2 Example perimeter=2(length+breadth) Non linear combination of attributes x1*x2 Example area= length*breadth,311254.0
116288,502934.0,"Linear : Dependent variable varies linearly with respect to independent variable. y= mx + C is a standard linear equation where the degree is 1. (Previously it was taught to us as y = B0 + B1x during our linear regression course. Non Linear: Dependent variable varies non-linearly with respect to independent variables. y = ax^1 + bx^2 + ......zx^n etc where the degree of this equation is n. Also sin x, cos x, e^x can be expressed as multi dgree polynomial equations and hence they are also non-linear. I hope this clarifies.",301121.0
116288,503148.0,"Linear attribute/variable means 2 variables have linear relationship, that is if v1 increases by x%, v2 will increase/decrease by y%. And this relationship will maintained throughout for every % increase/decrease. v1 &amp; v2 can relation suche that when one increases, the other can decrease or vice versa. Non-Linear variables : v1 &amp; v2 have a relationship but the % increase/decrease doesnt remain constant like above. Example Quantitiy sold Vs Price curve below : it is known that when products are sold in bulk, manufacturing price is reduced. Below curve represents the same. However, reduction is price tries to stabilize beyond Quantity T'. So there is no linear relationship between Quantity and Price but there is a non-linear relation.",316211.0
116290,503235.0,"Since the attributes c and d are multiplied with each other, the equation becomes non-linear. So for an equation to be linear the raw attributes shouldn't be multiplied with or raised to each other such as a0.a1, a3log(a5), aa32.",310505.0
116290,502881.0,If you see exponential (e^x2) graph it is a linear curve trending upwards whereas in case of sin(dx4) it is in the wave form shape which makes this equation non linear. https://www.mathsisfun.com/sets/functions-common.html,311254.0
116290,502890.0,"Hi, In Linear Regression the term linear is understood in 2 ways - 1. Linearity in variables 2. Linearity in parameters (coefficients) Linear regression however always means linearity in parameters (coefficients), irrespective of linearity in explanatory variables. You can refer the link below for more detailed explanation https://www.datasciencecentral.com/profiles/blogs/learn-the-concept-of-linearity-in-regression-models",318021.0
116290,502942.0,"HI Ashish Please refer to the TA approved link below. https://learn.upgrad.com/v/course/208/question/116238 Please note that all the trigonometic functions (sin x, cos x) are equivalent to multi-degree polynomial Hence they are non-linear.",301121.0
117207,505401.0,I think subtraction is also linear. Following link may help you. https://newonlinecourses.science.psu.edu/stat504/node/216/,310522.0
117207,505453.0,Subsraction is also provides linerirty.,313526.0
117207,506102.0,"Yes, subtracting, adding are both linear.",307176.0
117207,507974.0,Sum and differences of any number of linear terms is again a linear.,308638.0
117225,505667.0,"Hi Subhransu, If 1 is not present, the dot product between the vectors will be incorrect as constant value will not be considered",319876.0
117225,505457.0,1 is for the constant. Here a0 is the constant. 1 gets multiplied with a0.,310467.0
117225,505690.0,"Another way is to look at the matrices dimensions and you will realize that without the 1 for constant X is 5x3, a is 4x1 and y is 5x1 and hence X*a will not be possible. Once you add/include 1 for the constant a0 the matrix multiplication is possible.",311857.0
117225,506099.0,"Because of the presence of constant in the Linear equation. If you recall, y=mx+c . C is some constant. We're addressing this in the feature vectors by putting 1(unit) as constant.",307176.0
108359,468214.0,Read all about it here https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f,310974.0
108359,468233.0,Can go through the link for detailed explanation: https://www.researchgate.net/post/Why_is_the_likelihood_function_used_on_maximum_likelihood_estimation_multiplicative,311117.0
108359,468226.0,"Hi Vidhya, Likelihood function gives an idea to estimate the parameters of a probability distribution function. It gives us an idea of how well the data summarizes these parameters. Multiplication is chosen simply because: For any two events A and B in a sample space S we say that A and B are independent iff P(A and B) = P(A∩B) = P(A∩B) = P(A)P(B) So, we consider events to be independent hence we need to multiply the probabilities Some useful links: https://stats.stackexchange.com/questions/211848/likelihood-why-multiply",318344.0
108359,468650.0,"If you remember from the statistics module, Multiplication Rules for Probability : P(X በ Y) = P(X) * P(Y) - if X and Y are independent events The probability of n independent events occuring simultaneously is given by the product of their individual probabilities. Thus here we consider the product.",310511.0
108404,468647.0,"If you remember from the statistics module, Multiplication Rules for Probability : P(X በ Y) = P(X) * P(Y) - if X and Y are independent events The probability of n independent events occuring simultaneously is given by the product of their individual probabilities. Thus here we consider the product and not the sum.",310511.0
108404,468736.0,You can see the difference via graphical presentation in this link: https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/ It maybe helpful.,311117.0
107271,464132.0,beta 1 is similar to what we have used in linear regression so it should be kept minimum and beta is a constant value which must be kept in integer but you can try more value of b1 and b based on the type of data you have.,318017.0
107271,464422.0,"As mentioned in the video lecture , we can use any values of slope, i.e. b1, and constant, i.e. b0 whatever the case required for making sigmoid curve. This app is basically made for showing how to make curve by changing the values, for practice purpose, as same type of app's used in earlier sessions also. These values are preset for this example only , we can change as per the requirement.",311117.0
107493,465094.0,Looking forward to see you there! ***We are going to conduct this live session on the Zoom platform. Link for download is given in email.,311117.0
107493,546027.0,"Hi Pratik, we are in same group for case study... my contact number is 8368463915. Please connect with me.",317990.0
107493,465105.0,These app's are made by upgrad members as shown in earlier video's: https://learn.upgrad.com/v/course/208/session/18002/segment/91561,311117.0
109076,470887.0,Optimization is a big part of machine learning. Almost every machine learning algorithm has an optimization algorithm at it’s core. Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost). Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm,318476.0
109076,470942.0,"Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. Please go through the link for more understanding: https://machinelearningmastery.com/gradient-descent-for-machine-learning/",311117.0
107123,464254.0,https://stackoverflow.com/questions/30499018/difference-between-linear-and-logistic-regression-why-is-logistic-regression-ca Mathematical explaination of your question,317982.0
107123,463027.0,We haven't yet learn classification. We have only learnt regression which is on how to predict continuos values. and Classification is totally different topic on how to predict discrete (binary?) values. And clustering is how to group data without knowing before hand what the groups are - unsupervised learning. https://machinelearningmastery.com/classification-versus-regression-in-machine-learning/,300694.0
107123,463590.0,"Linear regression predicts the outcome which can have continuous values, whereas logistic regression predicts outcome which is discrete. Logistic regression falls under the category of supervised learning. It measures the relationship between categorical dependent variable and one or more independent variables by estimating probabilities using logistic/sigmoid function. Logistic regression is a bit similar to linear regression or we can see it as a generalized linear model. This link may help you in detailed knowledge: https://stackoverflow.com/questions/30499018/difference-between-linear-and-logistic-regression-why-is-logistic-regression-ca",311117.0
107123,463985.0,Logistic regression is a bit similar to linear regression or we can see it as a generalized linear model. Logistic regression also does the same thing but with one addition. It pass the result through a special function called logistic /sigmoid function to produce the output y. Please refer the link: https://machinelearningmastery.com/logistic-regression-for-machine-learning/,314183.0
107123,464224.0,clustering means grouping the data.it form like unsupervised group.Logistic regression bit of similar to linear regression.in linear regression we can learn about categorical variable and dependent variable and also one or more non-numeric variable. in logistic regression we can learn sigmoid function and in the classification we can learn about predict discrete variables,306996.0
107123,464472.0,https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4,308639.0
107533,465265.0,likelihood function gives the max value of probability of a logistic distribution which means that the graph of sigmoid function should be such that the points are away so that max probability can be attained,318017.0
107533,465347.0,Try this https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f,310974.0
107533,465374.0,"Likelihood Function has been explained here, pls refer teh links: http://www.medicine.mcgill.ca/epidemiology/hanley/bios601/Likelihood/Likelihood.pdf https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1",311117.0
107560,465393.0,"Thanks guys! It helps and very much appreciated. I overlooked the fact, the maximizing is calculated on the basis of the data set value whether the probability is 'yes ' or 'No.' and the product is calculated accordingly.",311115.0
107560,465372.0,"Hi , Please check the below thread """"https://learn.upgrad.com/v/course/208/question/107385/answer/464680"" To put in simple terms , we are trying to find the maximum probability 1) P5 -- it is the probability of diabetes 'yes' 2) 1-P6 -- it is the probability of diabetes 'no' P5 should be maximized and P6 should be minimized. Extedning further , you can mention P5 and (1-P6) need to be maximized for uniform representation.",305652.0
107560,465384.0,"Hi Srinivasan, First of all, Pi stands for the probability of person i being diabetic. From the given plot i.e. the actual dataset: Person 5 is diabetic Person 6 is non-diabetic In simple terms the reason we're maximising P5 is that person 5 is a daibetic person. Therefore, if we want such a model that tells that P5 is diabetic, we have to maximise it. In the opposite case, if we maximise 1-P5, then this would go away from the real dataset i.e. our model would tell that P5 is non-diabetic which is not correct actually. For P6, we're maximising 1-P6 because Person 6 is a non-diabetic according to the dataset that we have. Thus, we want to build such a model that tells that Person 6 is non-diabetic. Therefore, we need to maximise 1-P6 so that P6 remains minimised and the model which we're building gives us correct predictions. You must refer to the link shared by @Prathap. Hope this helps! Thank you!",318355.0
108231,467626.0,Maximum Likelihood algorithm justifies the choice of a parameter in a model by maximizing the probability of getting a particular set of observations from the model. It makes use of the likelihood or log-likehood functions https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1 https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/,310952.0
108231,467796.0,"Try the app given in "" Finding the Best Fit Sigmoid Curve - II "" in https://learn.upgrad.com/v/course/208/session/25992/segment/133963. and look at the change in curve shape for finding the best model, and try to analyse in each variation of B0 and B1 for maximization. It will help to understand.",311117.0
108249,467689.0,Is this depending on their likelihood of being diabetic or non diabetic..,319869.0
108249,467763.0,"If you look at the graph, P6 value is 0.00 which means non diabetic. P5 the value is 1.00 which means diabetic Hence, P6 to be minimized but for P5 should be maximized",311254.0
108271,467822.0,"For derivation of Sigmoid curve equation , please refer the link: https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e The explanation for, why is the exponential used, please go through the below link : https://math.stackexchange.com/questions/2289661/why-does-the-sigmoid-function-use-e. Maybe helpful in clarification.",311117.0
107936,466279.0,"Simple way to look at it is, Logistic Regression is a classification model and helps you make predictions in cases where output is Categorical value where as Linear Regression outputs the numberical Value",311160.0
107936,466320.0,Linear regression is generally used when we are predicting continous variable. Whereas Logistic regression is predicting categories which aren't continous but can be binary. Refer to following link for better understanding. https://datascience.stackexchange.com/questions/9362/when-to-use-linear-or-logistic-regression,317689.0
107936,466851.0,"The two main important differences between logistic and linear regression are: 1. Dependent/response variable in linear regression is continuous whereas, in logistic regression, it is the discrete type. 2. Cost function in linear regression minimise the error term Sum(Actual(Y)-Predicted(Y))^2 but logistic regression uses maximum likelihood method for maximising probabilities.",314621.0
108166,,nan,
107937,466272.0,"It is same as X + Y = 1 and you know X = 4Y. Therefore, Y = 0.2 and X = 0.8. Replace X as the probability of person suffering from Diabetes and Y, prob of no diabetes",311160.0
107937,466319.0,P(Diabetes) = x P(No Diabetes) = 1 - x P(Diabetes) = 4 * P(No Diabetes) x = 4(1-x) 5x = 4 x = 0.8,317689.0
106449,461805.0,"Hi, Yes, it is likelihood probability of A variable being 'Yes' or 1 Please for more details follow the attached link https://towardsdatascience.com/a-gentle-introduction-to-maximum-likelihood-estimation-9fbff27ea12f",344894.0
106449,463998.0,"Hi, Please refer the below link: http://users.stat.ufl.edu/~winner/qmb3250/notespart2.pdf",314183.0
106449,464912.0,"As mentioned in the video, maximising is only a convention. When you look at the values of the sugar levels, you need to find the appropriate probabilities to ensure getting the correct result of diabetic or non-diabetic. For doing this you can either minimise or maximise the cost function. If you want to take the convention of non-diabetic i.e., minimisation, you will take P1, P2, P3, P4, and P6 as they are (since lower blood sugar will indicate non-diabetic) and minimise the rest i.e., P5, P7, P8, P9, and P10 (so probability of non-diabetic is low for high blood sugar). If you want to take the convention as diabetic i.e., maximisation you will take P5, P7, P8, P9, and P10 as they are (since higher blood sugar will indicate diabetic) and maximise the rest i.e., P1, P2, P3, P4, and P6 (so probability of diabetic is low for low blood sugar). Hope this clears your doubt.",310505.0
107926,466195.0,"Kindly refer to the below link: https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220 Also, this topic is covered in the Additional Resources",311160.0
107926,466194.0,"This is basically a Machine Learning parameter. In ML, cost functions are used to estimate how badly models are performing. In simple terms, a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference or distance between the predicted value and the actual value. For more details, please refer this link: https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220",311117.0
107930,466206.0,"Hey Kaushiki, I think, they have completely assumed both of these values for calculation purposes. https://learn.upgrad.com/v/course/208/session/25992/segment/133961 As you can see the same in the non-graded questions mentioned in the link above, wherein they have considered β0 = -15 and β1 = 0.065 as these values. Hope this helps!",301655.0
107930,466335.0,"Hi, The coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using maximum-likelihood estimation. Maximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data). The best coefficients would result in a model that would predict a value very close to 1 (e.g. male) for the default class and a value very close to 0 (e.g. female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that minimize the error in the probabilities predicted by the model to those in the data (e.g. probability of 1 if the data is the primary class). Please go through the link for more details https://machinelearningmastery.com/logistic-regression-for-machine-learning/ https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html",344894.0
107933,466290.0,"Sigmoid curve helps us by providing probability of the classification. For example, the probability of person classified as Diabetic. We need to choose the right B0 and B1 in order to get maximum Likelihood to be able to make the curve fetch us better results. Maximum Likelihood is achieved by maintaining maximum P and (1-P) values for 1 and 0 respectively.",311160.0
115651,499720.0,You can get a binomial output.,301648.0
115651,499364.0,"Hi Ramkmal Please refer to the TA approved link given below where similar point was discussed, even though it was somehat reverse of what you are asking for. (In Logistics regression you will get binomial output) https://learn.upgrad.com/v/course/208/question/115437.",301121.0
115651,500062.0,you will always get a numerical output which is the probability of outcome result. Now it's upto requirement or train data that after which probability you have to consider value as 1 and rest 0.,305651.0
115651,500836.0,"Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). The dependent variable is binary or dichotomous, i.e. it only contains data coded as 1 (TRUE, success, smoker, etc.) or 0 (FALSE, failure, non-smoker, etc.).",314048.0
107385,464696.0,"Hi Shayari, First of all the P here is the probability of someone being diabetic. Maximising P(i) means to increase p to such an extent that it(probability) is high. Thus, the person must be diabetic. As seen in the screenshot above, P5 is the probability of the person who is diabetic and P6 is the probability of the person not being diabetic. Alternatively, P5 is the probability of the point at the upper end (i.e. which has high probability) of the graph and P6 is the probability of the point at the extreme bottom (i.e. with low probability) of the graph. Now, in order to find the best line which will predict these points to occur at these respective positions, we say that we need to maximise P5 so that our model predicts that P5 is high and thus diabetic whereas P6 is to be minimised i.e. the probability should be as low as possible for P6 so that our model predicts it as non-diabetic. Thus, doing this would give us the best model and thus have higher chances of predicting the correct values. Hope this helps! Thank you!",318355.0
107385,464680.0,"Hi, If you observe the diabetes dataset , P5 has diabetes (yes) and P6 has no diabetes(no). Same has been represented in graph, here low probability (minimize) is ""not having diabetes"" and high probability (maximize) is ""having diabetes"". So to represent these patient details accurately , P5 should be maximized and P6 should be minimized. Extedning further , you can mention P5 and (1-P6) need to be maximized for uniform representation.",305652.0
110087,,nan,
107378,464641.0,"Hey, Yeah , it might be a better fit.As mentioned, this should be confirmed by maximizing the proababilities product with cost function(MLE).",305652.0
107378,464642.0,"Yes, thanks Anuj.. It got clear after watching the later videos.",312479.0
107378,464638.0,"Hey anuj, 1. By just looking at the curve, you can just guess it, by observing the number of yellow bars and saying that your likelihood value will be maximum along this curve 2. But, infact you can never judge a sigmoid curve by just seeing it and saying that this is the best curve, you'll have to calculate the value of likelihood and see whether it is best or not Hope this helps!",301655.0
114607,494747.0,"Another good read on how the name ""Regression"" got struck http://blog.minitab.com/blog/statistics-and-quality-data-analysis/so-why-is-it-called-regression-anyway",313826.0
114607,494662.0,"Good question Subhin. So, my understanding (after doing some reading) is that the term ""regression"" has been stuck to because of its historical usage. A quote from an answer on Quora (attaching the link below) ""In 1984 a book called “Classification and Regression Trees” was published by Leo Breiman, Jerome Friedman, Charles Stone, and Richard Olshen. Their use of the term “Regression” for continuous variables has stuck within the machine learning fraternity; it should have been “Estimation”—or anything else other than regression."" You can refer to this link for more detailed explanation, it is an interesting read. https://www.quora.com/Why-is-logistic-regression-called-regression-if-it-doesnt-model-continuous-outcomes Hope that helps.",317998.0
114607,494851.0,"I don't think ""continuous outcome"" is a requirement for a model to be called a regression. Nonetheless, the outcome of logistics regression (log odds) is actually continuous. It is when we apply a threshold probability to it that it turns discrete",305653.0
108316,,nan,
107423,464911.0,"First, you have to encode Categorical Features next, divide the data set into train and test for model",317845.0
107423,464864.0,Variable encoding should be done prior to splitting data into training and test data.,313826.0
107423,464901.0,"Variable encoding or dummy variable creation is done prior to test train split. So, your dataset will comprise of all categories before you split it and start training.",317689.0
107423,465031.0,"One line answer Regression guarantees interpolation of data and not extrapolation. Long Answer: Interpolation basically means using the model to predict the value of a dependent variable on independent values that lie within the range of data you already have. Extrapolation, on the other hand, means predicting the dependent variable on the independent values that lie outside the range of the data the model was built on. To understand this better, look at the diagram below. The model is built on values of x between a and b. When you wish to predict the Y for X1, which lies between a and b, it is called interpolation. On the other hand, extrapolation would be extending the line to predict Y for X2 which lies outside the range on which the linear model was trained.",310974.0
112438,485145.0,intially we can go for logistic regression approach . we can also do with PCA to compare the best model and it's accuracy.,318732.0
112438,485276.0,"Hi Parna, Please go through the RFE for feature extraction.",344894.0
112438,486067.0,"Hi Parna, Although it is possible to use PCA here if we convert the values from categorical variables to numerical variables, PCA is most effective when capturing most variance and since we will be assigning values, PCA will not do a very good job of deciding which variables capture maximum variance. It would therefore be advisable to first delete columns with large null values, then manually drop some coumns based on your judgement of whether they are relevant or not, then create dummy variables and perform RFE. Hope this helps",316416.0
108409,468622.0,"Hey Arjun, I am also feeling trivial about your question, but I am sharing a link with you, which I found on internet https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html Hope this helps!",301655.0
108409,468667.0,"Adding to Rajarshi's , In the telecom case study example, group by on the variable and get the count of ""No"" values in churn variable. That becomes ""No of good"". Get the count of ""Yes"" values in churn variable. That becomes ""No of bad"".",310467.0
108409,468638.0,"You can get the information from the dataset. For eg, in the telecom case study, for a particular categorical feature variable, you can do a group by on the variable and find count of churns and non-churn cases for different values of the variable. Hope this helps.",310511.0
108409,468702.0,"Steps of Calculating WOE: For a continuous variable, split data into 10 parts (or lesser depending on the distribution). Calculate the number of events and non-events in each group (bin) Calculate the % of events and % of non-events in each group. Calculate WOE by taking natural log of division of % of non-events and % of events. If you go along with formulas, pleas refer this; WOE= ln(DistributionofGoods/ Distribution of Bads) Distribution of Goods - % of Good Customers in a particular group Distribution of Bads - % of Bad Customers in a particular group The concept of WOE in terms of events and non-events is just like Good and Bad Csomers . It is calculated by taking the natural logarithm (log to base e) of division of % of non-events and % of events. WOE= ln(% of Non-Events/ % of Events) Further, you can refer the case study in link: http://ucanalytics.com/blogs/information-value-and-weight-of-evidencebanking-case/",311117.0
108409,468935.0,"Thanks guys, all your suggestions helped in understanding the concept. However I think if we had gotten an exercise based on WOV like we got for dummy variables, I would be more confident in applying this approach, since it seems pretty important.",318335.0
108321,468038.0,"In the fine classing, the range is quite narrow and if the model is found unstable it is desirable to go coarse classing. For example 2 bins of fine classing are clubbed to form one bin in coarse classing table to increase the stability of the model. In the 'tenure' example 0-1 and 2-5 bins (244, 383) are made as 0-5 (244+383 =627) in the No.of Goods column I hope this clarifies.",301121.0
108321,468034.0,"WOE pattern of the groups post binning - coarse classing : Dataset is bucketing for showing the representation that Coarse binning is showing the monotonic trend in fine binning, while plotting the graph.",311117.0
108321,468424.0,"Generally, 10 or 20 bins are taken. Ideally, each bin should contain at least 5% cases. The number of bins determines the amount of smoothing - the fewer bins, the more smoothing. If someone asks you ' ""why not to form 1000 bins?"" The answer is the fewer bins capture important patterns in the data, while leaving out noise. Bins with less than 5% cases might not be a true picture of the data distribution and might lead to model instability.",314183.0
108672,469781.0,"Variable distribution stability- This means that the distribution of a variable in the train and the test dataset should similar. For example if you take income, in the test dataset the distribution of income is as follows 10% - above 50 Lakhs 20% above 30 lakhs so on and so forth The test dataset should have similar distribution. The extent of variability is calcuted by Population Stability Index(PSI) where if it is &lt;.1, variable distribution is stable &gt;.25, variable distribution is not stable",311254.0
108672,469800.0,"Population Stability Index (PSI) compares the distribution of a scoring variable (predicted probability) in scoring data set to a training data set that was used to develop the model. The idea is to check ""How the current scoring is compared to the predicted probability from training data set"". PSI = (% of records based on scoring variable in Scoring Sample (A) - % of records based on scoring variable in Training Sample (B)) * In(A/ B) For more, please refer: https://www.listendata.com/2015/05/population-stability-index.html Variable Distribution : The probability distribution of a discrete random variable is a list of probabilities associated with each of its possible values. It is also sometimes called the probability function or the probability mass function. For more,can refer: http://www.stat.yale.edu/Courses/1997-98/101/ranvar.htm Maybe it can give some sight.",311117.0
109672,472961.0,""" Accuracy "" is one of the most intuitive performance measure and it is simply "" a ratio of correctly predicted observation to the total observations "". Higher accuracy means model is preforming better. Accuracy = (TP+TN)/(TP+FP+FN+TN), i.e. (currectly predicted levels/ Total Number of levels) For more, please refer this link: https://stackoverflow.com/questions/47437893/how-to-calculate-logistic-regression-accuracy",311117.0
115787,501050.0,"Hi, Please find the attached links https://www.opendatagroup.com/blog/how-to-monitor-your-machine-learning-models https://www.parallelm.com/model-governance-needed-production-ml/ https://www.quora.com/How-do-you-manage-machine-learning-models-How-do-you-keep-track-of-machine-learning-models-as-you-iterate-over-them-How-do-you-compare-the-performance-of-different-models-data-sets-What-are-some-pain-points-in-your-workflow",344894.0
108461,469007.0,"Please follow the link, you will get clear understanding: https://stepupanalytics.com/information-value-iv-and-weight-of-evidence-woe/",311117.0
108461,468947.0,You can go through below link for detailed explanation on WOE and IV https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html Hope this helps.,317991.0
108323,468029.0,Thanks Brijesh for quick response,307710.0
108323,468028.0,You can refer the link attached for getting the difference. It is explained with graphical presentation: https://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression You can refer this too: https://stats.stackexchange.com/questions/29325/what-is-the-difference-between-linear-regression-and-logistic-regression,311117.0
108323,468035.0,The main difference between linear regression and logistic regression is that the linear regression is used to predict a continuous value while the logistic regression is used to predict a discrete value. You can read more here http://pediaa.com/difference-between-linear-regression-and-logistic-regression/,317845.0
108323,468049.0,"The two main important differences between logistic and linear regression are: 1. Dependent/response variable in linear regression is continuous whereas, in logistic regression, it is the discrete type. 2. Cost function in linear regression minimise the error term Sum(Actual(Y)-Predicted(Y))^2 but logistic regression uses maximum likelihood method for maximising probabilities.",307710.0
108323,468046.0,"Difference Between Linear and Logistic Regression.The essential difference betweenthese two is that Logistic regression is used when the dependent variable is binary in nature. In contrast, Linear regression is used when the dependent variable is continuous and nature of theregression line is linear. please refer the below link: https://techdifferences.com/difference-between-linear-and-logistic-regression.html",314183.0
108296,,nan,
108451,468999.0,"Target value of Linear Regression is Numerical value, When predicted value is close to actual value error is considered minium which is desired. Target value of Logistics value is Categorical (Yes or No). In this case our prediction should be equal to actual value which means we are maximising the probablility of getting that desired categorical value (for example Yes) I hope this clarifies.",301121.0
108451,469022.0,"Linear Regression is used to establish a relationship between Dependent and Independent variables, which is useful in estimating the resultant dependent variable in case, independent variable change. Logistic Regression, on the other hand, is used to ascertain the probability of an event. And this event is captured in binary format, i.e. 0 or 1. Further, you can follow the link: https://stats.stackexchange.com/questions/29325/what-is-the-difference-between-linear-regression-and-logistic-regression Will be helpful for you.",311117.0
108457,469045.0,Data inconsistency can be in various forms. The ones that Hindol explained are very specific. Please also go through the attached link https://dzone.com/articles/customer-churn-analysis-using-logistic-regression,310974.0
108382,468380.0,Information Value Variable Predictiveness Less than 0.02 Not useful for prediction 0.02 to 0.1 Weak predictive Power 0.1 to 0.3 Medium predictive Power 0.3 to 0.5 Strong predictive Power &gt;0.5 Suspicious Predictive Power Important Points Information value increases as bins / groups increases for an independent variable. Be careful when there are more than 20 bins as some bins may have a very few number of events and non-events. Information value should not be used as a feature selection method when you are building a classification model other than binary logistic regression (for eg. random forest or SVM) as it's designed for binary logistic regression model only. Useful link on WOE and IV https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html,311254.0
107975,466443.0,"Yes. The lecture notes are present on the ""Summary"" page of the last Module ""Logistic Regression: Industry Applications - Part II"".",313826.0
107975,466408.0,"They are usually at the end of the module, so am sure they will at the end of this module",304022.0
108200,467518.0,"Replied on the same topic here : https://learn.upgrad.com/v/course/208/question/108142. Please go through, may be helpful.",311117.0
108200,468032.0,"After we finalized our model, binning using WOE makes the model stable. Moreover WOE and in turn, IV (Information Value) gives a predictive power of the model.",301121.0
108200,468051.0,Please refer the below link: https://stats.stackexchange.com/questions/237578/should-all-parameter-coeffiecients-be-negative-for-woe-in-logistic,314183.0
113485,489515.0,"When you have built the model, and would have checked the VIF while doing RFE, you would have got some variables in decreasing order of VIF. From there you can find the top 3 variables.",311117.0
113485,489574.0,The summary statistics of the model would provide the name of the variables used in the model alongwith various parameters associated with them. Try to identify the parameter which would signify the magnitude by which the variable has an impact on the outcome and chosse the top 3 variables based on this parameter.,313826.0
113485,489632.0,Please refer to the TA answered link below https://learn.upgrad.com/v/course/208/question/112958,301121.0
113485,489982.0,Top 3 being asked from the list of variables. You need get the best 3 out of them,306009.0
114771,495382.0,"Sensitivity is an absolute quantity, the smallest absolute amount of change that can be detected by a measurement. Accuracy can be defined as the amount of uncertainty in a measurement with respect to an absolute standard. Sensitivity is the better metric as the number of items correctly identified as positive out of total true positives",312953.0
114771,495445.0,"Sensitivity is TP/(TP+FN) which deals with all the actual Positives. (This is also called Recall) Recall: Probability that an actual 'Yes' case is predicted correctly. Precision is TP / (TP+FP) - Probability that a predicted 'Yes' is actually a 'Yes'. Too much of sensitivity will affect precsion and vice versa and there should be a trade off. Please note that Accuracy is different as it attends to all the data in confusion matrix and brings out accuracy of all correct predictions(TP positve and TN negative) divided by all samples put together. Best matric is depend on your business case, what do you want to correct predict.",301121.0
108437,468741.0,"A dummy variable (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc. if multicollinearity is high we drop Insignificant prediction Variables",317845.0
108437,468821.0,"The process is explained in the session, but why it is being done is described here. Please go throgh it: https://towardsdatascience.com/super-simple-machine-learning-by-me-multiple-linear-regression-part-1-447800e8b624",311117.0
108437,468708.0,"Dummy variables (also known as binary, indicator, dichotomous, discrete, or categorical variables) are a way of incorporating qualitative information into regression analysis. Qualitative data, unlike continuous data, tell us simply whether the individual observation belongs to a particular category. Suppose we have columns like Male = Yes and Female=No . Representing Yes and No as 0 and 1 Male=1 means the customer is male However, Female=0 also means that the customer is male Suppose we drop Male_dummy column, the Female_dummy column is enough to provide us the necessary information. If 0 then Male if 1 then Female That's why dummy variables are created for yes and no columns and one column is dropped to prevent multicollinearity.",311254.0
108437,469695.0,Dummy variables are created in case we have multiple levels in categorical data. We represent binary categorical data using 1's and 0's. We create dummy variables to check the multicolinarity and that is why the dropping takes place after checking the correlation only.,318789.0
108257,467779.0,You can apply scaling to the entire dataset only after Standardizing/normalising. The stepwise process for standardizing on the entire dataset is mentioned in the link: https://stats.stackexchange.com/questions/201961/do-i-apply-normalization-per-entire-dataset-per-input-vector-or-per-feature,311117.0
108257,467770.0,"It is extremely important to rescale the variables (few columns may have small integer values) so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the classificaion model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use scaling on numerical variables so that the units of the coefficients obtained are all on the same scale.",318822.0
108257,467754.0,"Scaling will be applied only to the numerical variables. This is primarily done so that the challenges arising from different units/scales will be avoided. Hence after converting all the categorical variables in to binary, we apply scaling only to the numerical variables.",316202.0
108257,467832.0,The main reason to use scaling on only the training set is to avoid leaking information from the test set. Reasoning: A model shall be applied on unseen data which is in general not available at the time the model is built. The validation process (including data splitting) simulates this. So in order to get a good estimate of the model quality (and generalization power) one needs to restrict the calculation of the normalization parameters (mean and variance) to the training set. https://stats.stackexchange.com/questions/77350/perform-feature-normalization-before-or-within-model-validation,317845.0
108257,492814.0,"When we do scaling on train data, we learn the parameters of scaling on the train data and in the same time we scale the train data. While, on the test data we use the scaling paramaters learned on the train data to scale the test data. If we scale and train on the entire data frame then the model is not predicting anything. Model has not learned anything. It has seen all the data",301643.0
109432,472012.0,"The main difference is in the kind of your dependent variable. The "" binary logistic regression "" is used for predicting the outcome of a categorical dependent variable (i.e., mortality of a disease, 'yes - no question') based on one or more predictor variables (features). However, "" Generalised linear regression model "" is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables denoted 'x'; x1, x2, x3, . . .etc. You can refer the below link for more understanding from discussions: https://www.researchgate.net/post/what_is_the_difference_between_running_a_binary_logistic_regression_and_generalised_linear_model",311117.0
109432,472300.0,"The generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. Generalized linear models were formulated for various other statistical models, including linear regression, logistic regression and Poisson regression. Maximum-likelihood estimation remains popular and is the default method on many statistical computing packages. The following are common used GLM model, simle linesr reg is a special case, so id Logistic, Using Logit Log p/(1-P), transfor p from (0, 1) to (-infinite, positive infite), which can used the linear regression idea. Ref : https://www.researchgate.net/post/what_is_the_difference_between_running_a_binary_logistic_regression_and_generalised_linear_model",318368.0
107437,465029.0,"You are right, the interpretation is not easy for the analyst. This is called Reference Encoding:",310974.0
107437,465091.0,Strategies for choosing the Reference Category in Dummy Variables : How to choose the Level for dropping from dummy variables is strategically explained in the link: https://www.theanalysisfactor.com/strategies-dummy-coding/,311117.0
107826,465839.0,"K THANKS, sry my bad just in a hurry posted the question.",304692.0
107826,465836.0,"Hi Jetendra, It is clearly mentioned in the explanation after the video why they have dropped specific columns instead of the first. Below is the copy paste of the explanation provided: So the process of dummy variable creation was quite familiar, except this time, you manually dropped one of the columns for many dummy variables. For example, for the column ‘MultipleLines’, you dropped the level ‘MultipleLines_No phone service’ manually instead of simply using ‘drop_first = True’ which would’ve dropped the first level present in the ‘MultipleLines’ column. The reason we did this is that if you check the variables ‘MultipleLines’ using the following command, you can see that it has the following three levels: Dummy Variable Levels Now, out of these levels, it is best that you drop ‘No phone service’ since it isn’t of any use because it is anyway being indicated by the variable ‘PhoneService’ already present in the dataframe. To simply put it, the variable ‘PhoneService’ already tells you whether the phone services are availed or not by a particular customer. In fact, if you check the value counts of the variable 'PhoneService', following is the output that you get: Levels of the Variable 'PhoneService' You can see that the level 'No' appears 682 times which is exactly equal to the count of the level 'No phone service' in 'MultipleLines'. You can see that the dummy variable for this level, i.e. 'MultipleLines_No phone service' is clearly redundant since it doesn't contain any extra information and hence, to drop it is the best option at this point. You can verify it similarly for all the other categorical variables for which one of the levels was manually dropped.",311160.0
105925,457189.0,the variable which have hugh value must be scaled to something which can be easily interpreted for exmaple monthly charge can be in thousand and other variable can be in 0 and 1 so its hard to interpret these values in a graph,318017.0
105925,457292.0,"If you can maintain the list of columns without dummies, then you can apply any scaling method. But, if you do not want to maintain such list, you could apply MinMaxScaler on all columns as it does not affect the dummies.",318329.0
105925,464859.0,"Rest all variables were categorical and hence coverted to dummy variables having 0 or 1 value only. So, they don't need to be scaled. These 3 variables were not on similar scale, so he used standard scaler to bring every variable to same scale.",317689.0
105925,465088.0,"Hi Utkarsh, First of all, we do scaling to achieve the following: We can compare the features among one another Gradient Descent (in case of linear regression) happens at much faster rate Some of the key points when scaling variables are as below: The values of all the columns must lie in the same range We do it only for continous variables and not for categorical variables If the scale is from 0 to 1 which is by default we need not scale the dummy variables which also have the values as 0 or 1 To answer, why we did for only 3 variables can be understood from the above points. To add on, please refer the below points: We did for only 3 variables highlighted above and not for others because All the other variables are either 0/1 after creating the dummy variables and the other variables like PhoneService already have values as 0/1 Hope this helps! Thank you!",318355.0
107081,462692.0,Through plotting Box plots,318827.0
107081,462706.0,"Hi Neerajakshulu, May be this link can help you out https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba",301655.0
107081,463050.0,This is a good question - I am also stuck figuring this out. There are approx 15 numerical features - if we drop outliers in each of them (with say using box plots) - we might not have data left and how will dropping outliers affect out final model -- hmm,300694.0
107520,465229.0,"The variables which have been dropped have very high correlation with their complimentary variables meaning 'MultipleLines_Yes' is highly correlated with 'MultipleLines_No' ( -0.82) , 'OnlineSecurity_Yes' is highly correlated with 'OnlineSecurity_No' (-0.63) and so on. Please find attached a screenshot of correlations between these variables. Hope this clarifies your query.",313826.0
107520,465607.0,"MultipleLines_Yes is highly correlated with MultipleLines_No ( -0.82) , OnlineSecurity_Yes is highly correlated with OnlineSecurity_No (-0.63) and others similarly. Any high positive value or any low negative value means a high correlation.",317811.0
107545,465314.0,"Hi Jayashree, You can drop highly correlated values, as the values which they might have can be strongly similar to any other rows to which they are correlated. So, dropping them and then going ahead with model building is also the step that Mirza followed in his lectures . Hope this helps!",301655.0
107545,465373.0,The same ahs been replied. Please refer the discussion: https://learn.upgrad.com/v/course/208/question/107520,311117.0
109399,471959.0,"The value of ""alpha"", i.e. level of significance is set by the researcher before examining the data. For setting off these points, it is found that the local maxima in the pdf’s second derivative, these are the points where the change in slope of the density curve is greatest. So, the setting of ""alpha "" is arbitrary. ""alpha"" is commonly set to 0.05, 0.01, 0.005, or 0.001. For more details, you can go through these links: https://en.wikipedia.org/wiki/P-value https://www.quora.com/Why-is-the-standard-p-value-0-05",311117.0
107549,465346.0,"In the EDA session, it was mentioned that we can remove very high/low values which disproportionately affects the results. However, if there are too many outliers,you can replace them with median value rather than removing them.",311254.0
107549,465313.0,"Hi Rajesh, I hope that to treat outliers, if any are detected, you can go ahead with the standardisation process using minmax scaler or any other one. Please go through this link: https://www.kdnuggets.com/2018/08/make-machine-learning-models-robust-outliers.html Hope this helps!",301655.0
107549,465383.0,"Though it has been explained for treating outliers in earlier modules. But still have confusion then, you can refer this link : https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba",311117.0
107549,465377.0,"some common techniques that are used to handle outliers: Deleting observations: We delete outliers values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers. Transforming and binning values : Transforming variables can also eliminate outliers. Natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Imputing: Like imputation of missing values, we can also impute outliers. We can use mean, median, mode imputation methods Treat separately: If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and then combine the output Please refer the below link for more details. https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/",310467.0
108272,467823.0,"StandardScaler: Especially when dealing with variance (PCA, clustering, logistic regression, SVMs , perceptrons, neural networks) in fact Standard Scaler would be very important. it can be used for algorithms that assumes zero centric data Min-Max Scaling produces values of range [0,1]. When dealing with features with hard boundaries this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.",317845.0
108272,467817.0,"The same has been discussed in: https://learn.upgrad.com/v/course/208/question/108011 Please go through, may be helpful.",311117.0
108129,467088.0,"The variable 'Churn' is not dropped from the 'telecom' dataset anytime. Even when the command X = telecom.drop(['Churn','customerID'], axis=1) is run, the 'inplace' parameters is not passed to the drop function and hence the value of inplace is by default 'False' . Hence, only a view of the 'telecom' dataframe is generated without the columns 'Churn' and 'customerID' which gets assigned to the dataframe 'X'. It could have also been done using 'y' series using churn = sum(y)/len(y)*100 The reason for finding the 'churn rate' is to see if the dataset has an high imbalance(i.e., check if the number of records with 'Churn'=1 is very less compared to records with 'Churn'=0, however in this dataset that is not an issue).",313826.0
108363,468260.0,"A detailed discussion you can get from this link: https://dius.com.au/2017/08/03/using-statsmodels-glms-to-model-beverage-consumption/ where different ""Model Families"", (i.e. Negative Binomial &amp; Binomial) has been taken into considerations for model building. Maybe you can get some clarification.",311117.0
108142,467329.0,Please go through the link below. https://www.quora.com/Why-do-people-use-weight-of-evidence-WOE It gives an insight as to why log is used to calculate WOE,311254.0
108142,467198.0,"WoE displays a linear relationship with the natural logarithm of the odds ratio which is the dependent variable in logistic regression. Therefore, the question of model misspecification does not arise in logistic regression when we use WoE instead of the actual values of the variable. For further details, please refer the below link: https://stats.stackexchange.com/questions/189568/replacing-variables-by-woe-weight-of-evidence-in-logistic-regression",311117.0
108411,468619.0,"Hey Ujala, I think you're right , because I can say that The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (&lt; 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable. In order to dig more deeper, you can go ahead and look into this link: http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients I hope this helps you out in a better way!",301655.0
108411,468633.0,"Null Hypothesis (H0) :β1=0 Alternate Hypothesis (HA) :β1≠0 If you fail to reject the null hypothesis that would mean that β1 is zero which would simply mean that β1 is insignificant and of no use in the model. β1 and thus, the independent variable it is associated with is insignificant in the prediction of the dependent variable. Similarly, if you reject the null hypothesis, it would mean that β1 is not zero and the line fitted is a significant one. Here β1=0 means the slope is 0. Meaning the fitted line is horizontal which signififes that the feature variable doesnt have a linear relationship with the dependent variable.",310511.0
108411,468715.0,"""Correlation different from zero"" is useless as a hypothesis: it does not permit one to obtain any observable information about correlation that wouldn't already be known in the absence of that assumption. Please see stats.stackexchange.com/questions/31 Further, Learn more about theoretical Null Hypothesis: https://www.sciencedirect.com/topics/earth-and-planetary-sciences/null-hypothesis",311117.0
107611,465544.0,"Yes, it happens. Feature selection is really important factor. This can be seen in the link below: https://machinelearningmastery.com/feature-selection-machine-learning-python/",311117.0
108339,468229.0,Check this :,310974.0
108011,466589.0,"There dosen't seem to be any rationale behind using the standardization methods that have been applied in either the linear regression or logistic regression demonstrations. However, following are a couple of articles that explain on how to choose the method: https://jovianlin.io/feature-scaling/ https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e",313826.0
108011,466579.0,"It's a good observation. I have searched and gone through many discussions had on researchgate.net on attached link: https://www.researchgate.net/post/What_are_the_best_normalization_methods_Z-Score_Min-Max_etc_How_would_you_choose_a_data_normalization_method It's combined output results, but mostly in the favour of standard scalar considering different examples.",311117.0
108011,466769.0,"In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms eg Logistic regression.The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation. You can refer below link: https://en.wikipedia.org/wiki/Feature_scaling or https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html",314183.0
107897,466088.0,"RFE is used when you have too many features and it is not feasible to check each individually. So RFE helps in narrowing down that to 10,15 or 20 based on your need and you can work on them and check whether they are significant or not and multicolinearity exists or not. Stats model is useful for few features where it is feasible to check rach of them individually and not time consuming.",311254.0
107897,466165.0,"Hi Jay, You can find some tit bits about RFE and some other packages in the link below: https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2 Hope this helps to clear your doubt regarding which to select and when!",301655.0
107897,466130.0,RFE removes least significant features over iterations. So basically it first removes a few features which are not important and then fits and removes again and fits. It repeats this iteration until it reaches a suitable number of features. StatesModel is a little less robust as it just removes less important features based on a threshold given as a parameter. You can refer the link also: https://datascience.stackexchange.com/questions/23539/difference-between-rfe-and-selectfrommodel-in-scikit-learn,311117.0
108066,466845.0,transform will just do the transform but fit transform will do the transform and will fit the transformation to all the variable generally fit transform is done for train data set and transform to the test data set.,318017.0
108066,466886.0,"Hi Krishanam We use fit_transform function to learn from the train-data. Once we build the model, we use the learning form the train data and apply that to the test data (for model evaluation) For applying it we use the transform functiom. in short : fit_transfrom to learn transfrom : to apply the learning Regards Darshan.",311032.0
108066,466842.0,"fit() : used for generating learning model parameters from training data transform() : parameters generated from fit() method,applied upon model to generate transformed data set. fit_transform() : combination of fit() and transform() api on same data set So we apply fit_transform() on train dataset and only transform() on test data set.",314621.0
108066,466848.0,"When you call StandardScaler.fit(X_train) , what it does is calculate the mean and variance from the values in X_train . Then calling .transform() will transform all of the features by subtracting the mean and dividing by the variance. For convenience, these two function calls can be done in one step using fit_transform() . The reason you want to fit the scaler using only the training data is because you don't want to bias your model with information from the test data. If you fit() to your test data, you'd compute a new mean and variance for each feature. In theory these values may be very similar if your test and train sets have the same distribution, but in practice this is typically not the case. Instead, you want to only transform the test data by using the parameters computed on the training data. Refer below Link: https://stackoverflow.com/questions/48692500/fit-transform-on-training-data-and-transform-on-test-data",320195.0
108201,467513.0,"Logistic regression is more sensitive to the mismatch between training distribution and test distribution. Hence, class balance is required. More discussions are mentioned here in the attached link, by taking balance and imbalance data's. https://datascience.stackexchange.com/questions/17910/bad-classification-performance-of-logistic-regression-on-imbalanced-data-in-test",311117.0
108201,467522.0,"Hi Chetan, Class imbalance occurs in Rare Incidence Population data. Suppose you are applying a model on data of credit card frauds, frauds are generally 0.1% of the total data, so in a whole year you can find very less number of cases of frauds, and model will not be able to learn from such data. To tackle this problem we pick the data from stratified sampling so that we can enrich the data with fraud cases. See, you are not handpicking the fraud data points into your data, but you are smartly stratifying the data so that you have enough cases of frauds for the model to learn.",318344.0
108201,467546.0,Class imbalance occurs if the total number of a one class is far less or exceeds than the total number of another class( or difference in between positive and negative data). More can get from below link http://www.chioka.in/class-imbalance-problem/ https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/,310952.0
107902,466123.0,"The variable TotalCharges was imported dtype=object i.e., it was imported as string . There are 11 rows in the dataset which have a space for the TotalCharges column. You can check this by either filtering in the churn_data csv file. Alternatively, you can check this by running the below command immediately after creating the merged telecom dataset telecom.TotalCharges[telecom.TotalCharges == ' '].count() Since TotalCharges should contain numeric values, we convert the values to float type during which the spaces get converted to null.",313826.0
109019,470765.0,"No. it's absolutely fine. If you have noticed in telecom.info() ; the values are showing as; TotalCharges 7032 non-null float64 , instead of 7043 . which verified that you are going correctly.",311117.0
109019,470750.0,"If you look at the churn_data excel sheet for the Charges column, you would find that there are 11 empty rows with a space which is a valid value of object data type. They have been exposed once you converted the column to numeric data type. So, you aren't missing out on anything :)",311160.0
109019,470788.0,Please refer below link on similar query https://learn.upgrad.com/v/course/208/question/108129,313826.0
107004,464276.0,"Hi, Please follow the attached links https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40 https://github.com/pandas-dev/pandas/issues/12042 https://datascience.stackexchange.com/questions/27957/why-do-we-need-to-discard-one-dummy-variable",344894.0
107004,465460.0,‘MultipleLines_No phone service’ was dropped not because of the value count being low. It was dropped as it was redundant. This variable was being indicated by the variable ‘PhoneService’ already present in the dataframe. The valuecount was mentioned to show that the numbers were same in both Phone Service and ‘MultipleLines_No phone service',317514.0
107905,466164.0,"Hey Nihal, I think thta it must not be there in the dataset for some specific reason because the data which we have in our hand is itself sufficient to predict whether or not a customer will churn in the future. I think, IIIT-Bangalore and upGrad guys must have thought through for this thing. Hope this helps!",301655.0
107575,465464.0,"Thanks Brijesh. Got the below points from the link you shared. If you don't mention the random_state in the code, then whenever you execute your code a new random value is generated and the train and test datasets would have different values each time. However, if you use a particular value for random_state(random_state = 1 or any other value) everytime the result will be same,i.e, same values in train and test datasets.",310467.0
107575,465443.0,"It doesn't matter if the random_state is 0 or 1 or any other integer. What matters is that it should be set the same value, if you want to validate your processing over multiple runs of the code. For more detailes, please go through the link: https://stackoverflow.com/questions/42191717/python-random-state-in-splitting-dataset/42197534",311117.0
107575,465475.0,"train_test_split() splits the data randomly into training set and test set using a random_state value. If the random_state value is not provided, then it internally generates a random_state and uses that to split the data. As a result of this, the random_state is not in our control and everytime a new random_state value may get generated and result in different train and test data set. This is not a desirable situation, since our model is based on the train dataset and having a different train dataset everytime would produce different results everytime. Hence, we explicitly pass the random_state value, so that the data is still split randomly with a specific initial random_state value. As a result of passing a fixed random_state value, the data that is split into train data and test data are always the same and hence the data used for model building is consistent.",313826.0
107575,466085.0,"To put it simply, it is too split the data-sets with the exact same rows in it",304022.0
115634,499123.0,"Hi Praneeth Please refer to the TA approved link in discussion forum below, which will clear your doubt. https://learn.upgrad.com/v/course/208/question/106429",301121.0
107956,466396.0,"When you perform get_dummies on a single variable it will return you the columns with just the values. For example, we have Column by name XYZ and it contains 123, 456, 789 as the row values through out the column. When you perform get_dummies on XYZ the output would have 123, 456, 789 as the column names. If you add prefix = 'abc' the output columns would be abc_123, abc_456, abc_789",311160.0
108342,468235.0,"You can get good sigh form this link: https://www.analyticbridge.datasciencecentral.com/forum/topics/handling-imbalanced-data-when-building-regression-models, and https://towardsdatascience.com/dealing-with-imbalanced-classes-in-machine-learning-d43d6fa19d2.",311117.0
108342,468232.0,"Hey Avinash, Your question is also pretty much new for me. So, I did some searches on Google and I found this: https://elitedatascience.com/imbalanced-classes Hope this helps to understand the concept in a better way!",301655.0
108342,468197.0,"Hi Avinash, Class imbalance occurs in Rare Incidence Population data. Suppose you are applying a model on data of credit card frauds, frauds are generally 0.1% of the total data, so in a whole year you can find very less number of cases of frauds, and model will not be able to learn from such data. To tackle this problem we pick the data from stratified sampling so that we can enrich the data with fraud cases. See, you are not handpicking the fraud data points into your data, but you are smartly stratifying the data so that you have enough cases of frauds for the model to learn. Some important links: https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/ https://datascience.stackexchange.com/questions/17910/bad-classification-performance-of-logistic-regression-on-imbalanced-data-in-test http://www.chioka.in/class-imbalance-problem/",318344.0
113263,488487.0,Check this out https://stackoverflow.com/questions/34668868/unable-to-run-logistic-regression-due-to-perfect-separation-error,310974.0
113263,488491.0,It might be because any of the fields that are using have only one single value. Please refer below link. https://stackoverflow.com/questions/34668868/unable-to-run-logistic-regression-due-to-perfect-separation-error Such fields are not required. You can drop them since it does not capture any varience in the data.,310467.0
113263,488654.0,"This happens when all of the values in one of the predictor categories (or a combination of predictors) are associated with only one of the binary outcome values.i.e. either 0 or 1. When this happens a solution cannot be found for the predictor coefficient. You can drop such columns, as they are insignificant. You can refer, https://stackoverflow.com/questions/53041669/error-perfectseparationerror-perfect-separation-detected-results-not-availab",311117.0
113263,488788.0,The issue was I was passing the dependent variable 'Converted' to X while y was set to 'Converted' variable. I removed the dependent variable from X and the model ran successfully. Thank you!,310522.0
107517,465244.0,Variable Selection for Elimination: Different models and procedures have been explained for variable selection for elimination in the pdf of the attached link: It may be helpful. http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf,311117.0
107517,465348.0,"Madhu, whether you check the VIFs first or the p values first and eliminate them, the order doesn't matter. The end goal is to get rid of multicollinearity and insignificant variables. It's just fine whichever method you follow. The results might vary a bit but for the most part what i've seen is they won't. Try building a model by checking VIFs first and eliminating based on them and the p values first and eliminating based on them. You will see that these two overlap and the end result wouldn't be too different. I generally like to follow this:",310974.0
107517,465274.0,"Variable elimination: I agree with you that variable elimination is hard to get, but with little effort we should be able to get. Now, first step is to understand why do we even bother to do the featur elimination. Answer (as per my understanding) is there are certain features which are highly correlated with each other i.e. if the model has to predict their effect on target variable, only one would be sufficient. You would have seen in first heatmap, there were good amount of area with either dark red or lighter version, which indicated some kind of relationship among variables. Hence we eliminated these variables. Next, when we build a model, we get the p-values for the variables. We know if the p-values is less (Typically less than 0.05) then that feature is significant. anything greater than that, the variable is not. You can refer the t-statistic and z-statistic to understand this part better. For now just have above thing in mind. Next is, when we have lower p-values and model is also good, we look at the VIFs, which indicates how all the variables are correlated with each other. HeatMap in first step has given a relatively good idea to see the relationship, but VIF gives the exact numeric value. If the value of VIF is greater than 5, you can eliminate that eliminate. One thing keep in mind, eliminate one variable at a time and again check the values. Rebuild the model again, and you should be okay to go ahead with the latest model.",301649.0
107517,465949.0,"In the manual feature elimination while building the last model ,the variable with MultipleLines_Yes having p value = 0.019 was not removed.The p value is still higher than 0.05. Why wasnt it was dropped?",301114.0
108027,466693.0,"Hey Parul, Yes, you're right. Refer the link below https://towardsdatascience.com/demystifying-confusion-matrix-confusion-9e82201592fd Hope this helps you out in a better way!",301655.0
108027,466658.0,Confusion Matrix is the same as Hypothesis testing - Error Matrix . Type 1 Error = False Positives (FP) Type 2 Error = False Negatives (FN) Hypothesis Testing Error Matrix: Confusion Matrix:,313826.0
108027,466849.0,Thanks everyone.,305845.0
108027,466766.0,"Hi Paru, Its both the same thing. One might observe a differnce in confusion matrix which is due to the differnce in placement of yes/true and no/false in confusion matrix. Please see the image below. Also note : FP or Flase Positive is the type 1 Error FN or False Negative is known as the type 2 Error Regards Darshan",311032.0
107445,464991.0,"Based on the sources in internet this is my understanding/observation Dummy Encoding should be used for ordinal categorical values - eg Low,Medium,High. One hot encoding is for categorical values which has no order- eg category of flowers, cars etc",311254.0
107445,465026.0,Both are used for unordered categorical variables but what you chose for which algorithm matters:,310974.0
107445,465250.0,"Hi, One-Hot Encoding and Dummy-Encoding both are the same. Dummy-Encoding is a function of pandas, internally work as OHE only. Please go through the attached link for more information https://stackoverflow.com/questions/41136853/whats-the-difference-between-dummy-variable-and-one-hot-encoding https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn",344894.0
107445,585217.0,"Hello Vinay, Can you pls contact me on 9620888890? Need some clarification on a topic. Thanks.",308437.0
107445,465934.0,"Hi Vinay, I was also had similar confusion over the question you have posted. Sharing a useful link for the same: https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding Thank you",310508.0
114922,496878.0,"Hi, Please follow the attached link http://www.stat.unipg.it/bacci/slides/ASMOD_talk2.pdf",344894.0
113181,488076.0,"yes, columns with p value greater than 0.05 should be dropped (since they would be having high correlation with another column and thus not useful for our model) and columns with VIF value more than 5 should also be dropped. do note, this has to be done one column at a time until you arrive at a model where all VIFs are less than 5 and p values are less than 0.05. You can also refer to this article for more insight as to why columns with high p values should be dropped: https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf hope that helps.",317998.0
113181,488296.0,"yes - we have already done this for linear regression assignment drop all high p-value one at a time first when none left then drop high VIFs one at a time - after dropping one column, you need to generate p-values again and drop all high p-values again There have been many posts about thin in the forum already",300694.0
114923,495938.0,"Likelihood functions play a key role in frequentist inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, ""likelihood"" is often used as a synonym for ""probability"".",313200.0
114923,495940.0,"Probability refers to the occurrence of future events, while a likelihood refers to past events with known outcomes. The likelihood function is always defined as a function of the parameter theta equal to (or sometimes proportional to) the density of the observed data with respect to a common or reference measure, for both discrete and continuous probability distributions.",319006.0
114923,496085.0,"Likelihood function describes the likeliness (probability)of a particular distribution(Continuous, Discrete or Bernoulli)outcome from a series of conducted random experiment in terms of a mathematical function. We check for the maximum likelihood estimation using Gradient descent or Closed form. The aim is to find the value of parameters at which the likelihood of a particular distribution is Maximised. I’d suggest to go through this link : https://stats.stackexchange.com/questions/112451/maximum-likelihood-estimation-mle-in-layman-terms",305655.0
114923,496089.0,"Likelihood function describes the likeliness (probability)of a particular distribution(Continuous, Discrete or Bernoulli)outcome from a series of conducted random experiments in terms of a mathematical function. We check for the maximum likelihood estimation using Gradient descent or Closed form. The aim is to find the value of parameters at which the likelihood of a particular distribution is Maximised. I’d suggest you to go through this link : https://stats.stackexchange.com/questions/112451/maximum-likelihood-estimation-mle-in-layman-terms",305655.0
115437,497867.0,you dont get probability or say yes no kind of results using logistics regression and you get price type of output using linear regression you cant predict if a person will churn or not using linear regression,318017.0
115437,497961.0,"HI Soumik I can understand the difficulty you have to go through in making linear regresion to do the job which logistics regression does naturally. We have to remember 2 points given below by the different nature of modus operandi of both techniques. Linear regression uses the technique of minimizing the variance so that model is reliable to predict, Logistics regression maximizes the probability of binomial output to check. In the method you have mentioned, it does not end there after getting the score between 0-99 for the purpose of probability but problem of getting into many more assumptions will creep in for interpretation of these scores. But i must tell you, you are trying to do something which is thought provoking.",301121.0
108467,469048.0,"In the context of Matrices (2x2) contain 2 rows and 2 columns and each element in the Matrix is represented by their [RowNumber, ColumnNumber]. In the Example: a is [0,0] b is [0,1] c is [1,0] d is [1,1] From the above explanation and the below screenshots it should be clear how TN is calculated: Actual/Predicted Not Churn Churn Not Churn True Negatives False Positives Churn False Negatives True Positives",311160.0
108441,468799.0,"Precision attempts to answer the following question: What proportion of positive identifications was actually correct? =True Positive/(True Positive+False Positive) Recall attempts to answer the following question:What proportion of actual positives was identified correctly? =True Positive/(True Positive+False Negative) We can take an example of diagnosis of a diabetic for 200 people Actual/Predicted Non Diabetic Diabetic Non Diabetic 75 (TrueNegative) 25(False Positive) Diabetic 30 (False Negative) 70 (True Positive) Precision in this case would be - Out of 95 (70+25) people positively identified , only 75 positive identifications is actually correct. Precision= True Positive/(True Positive + False Positive)= 70/(70+25) Recall in this case would be - Out of 100 (30+70) , only 70 actual positives is identified correctly Recall= True Positive(True Positive+False Negative)=70/(70+30) if the diabetic threshold increased, False Positive will decrease and False Negative would increase. This means people who has diabetes will now be predicted as non diabeteic thus increasing the False Negative value and decreasing the False positive value In such a case, Precision would increase but Recall would decrease. Similarly, if the diabetic threshold is decreased, people who are non diabetic will be predicted as diabeteic thus increasing the False positive and decreasing the False negative value. In such a case, Precision would decrease but Recall would increase. https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall",311254.0
108242,467775.0,"A model with high TPR and low FPR is definitely better. As mentioned in the session, you can draw a ROC curve with the TPR and FPR values. If the area under the curve is huge value i.e high tpr and low fpr, it implies a good model .",311254.0
108242,467810.0,"The good model is the one in which TPR is high and FPR is low. The area under the curve tells you how good a model is. If the curve is more towards the top-left corner, area is more, and hence, the model is better. As you can see, of the three curves, curve 'C' is most towards the top-left corner and thus, has the highest area resulting in it being the best model. After putting the values in python, you may find the result.",311117.0
108242,468398.0,"A good model is one that optimizies the TPR and FPR where TPR is high and FPR is low. As the video indicated, this is a trade of and the area under the curve is used for this optimization process. Thus an ROC curuve with higher area is a better model than one with lower area. Whenver area under the curve comes into the picture for optimization, we use calculus for optimization and when we use the double differential. we can maximize and minimize at a particular point in order to evaluate which is the best fit. So calculus is the underlying math used here.",316416.0
109632,472846.0,"hi Raveena, in the context of given question,cutoff means we will declare any value above that as 1 and value below that as 0. seeing the predicted probability and label assigned(1/0) we can find out what all can be the probable cutoffs.",311686.0
109632,473071.0,The idea is to the find out the range of cutoff and then you would be able to find out which are all the cutoffs valid out of the 4 choices given. To find range of cut off: You have to see Heart Disease and Predicted Label and judge where Predicted label is same as Heart Disease and where it is not; This will give you a clue on range of possible cutoff. 2nd method which is laborous: Try each of the 4 choices given on all the lines where you would be able to find which are valid and which are not valid. Please note this has more than one correct answer.,301121.0
107484,468020.0,"Total number of samples has to remain the same. So when TP will increase, FN will decrease.",310505.0
107484,465186.0,"The thing to note in this is that while TP increases from 1050 to 1190, FN will decrease from 350. This is because, total no of churn actual cases in the sample is held constant. Hope this helps.",310511.0
109090,470918.0,Below is a good link: https://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/ Hope it helps.,318476.0
109090,470926.0,"Hey Vinodhini, I think the link below will help you to understand things better: https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/ Hope this helps!",301655.0
109090,470940.0,"ANOVA and regression are not only related, but they’re the same thing. “ANOVA is just a special case of Regression”. For analysis portion, you can refer this: https://stats.stackexchange.com/questions/59879/logistic-regression-anova-chi-square-test-vs-significance-of-coefficients-ano",311117.0
107865,466117.0,"You are writing a function for roc curve which needs fpr on the x-axis and tpr on y-axis. As mentioned in the session we're giving the actual Churn values and the respective Churn Probabilities to the curve. actual is y_train_pred_final.Churn which are actual Churn values and assigned to variable fpr probs is y_train_pred_final.Churn_Prob which are the the respective Churn Probabilities and assigned to variable tpr drop_intermediate this is an optional parameter and means whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves in the above code it is False, which means threshold will be there on the plotted ROC curve. # Calculating area under the curve by passing the auc_score = metrics.roc_auc_score( actual, probs )",311254.0
107865,466099.0,"Hi Neha The Code has a custom function which does the following 1)Calculate the ROC Curve Values 2)ROC AUC Score 3)Plot the ROC Curve. These above mention operation requires two common values as input. ie, the actual churn vaules and its probability, which is passed as the input to the function Inside the custom function, it calls two predefined function to calculate ROC Curve and AUC Score, i.e.https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html metrics.roc_curve metrics.roc_auc_score ROC Curve function creates FPR,TPR and Thresholds for the plotting of ROC Curve plt.plot([0, 1], [0, 1], 'k--') --&gt; it plots the diagonal line for refference. Links for syntax and further refference AUC Score : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html ROC Curve : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html",311032.0
108099,467062.0,"The same has been discussed here : https://learn.upgrad.com/v/course/208/question/107865 Apart from this, it is explained step by step under the head "" The ROC Curve "" on Kaggle.com. Maybe helpful for you. Link is attached for your reference: https://www.kaggle.com/captcalculator/logistic-regression-and-roc-curve-primer",311117.0
108252,468023.0,"This is simple like TRUE &amp; TRUE = TRUE, FALSE &amp; FALSE = TRUE other combinations are FALSE",306735.0
108252,467706.0,"Hey Pralay, First one says that, NOT CHURN cases are correctly predicted as negatives, that's why they have been given the name as True Negatives. While the second case, CHURN ones, they have been correctly predicted as True Positives. Here, in the matrix Negative is only a word given to NOT CHURN cases and Positive for CHURN cases. Don't confuse it with the True and False values. I hope this helps you to understand it in a better way!",301655.0
108252,467789.0,"From the session-3(Metrics beyond accuracy) : The different elements in this matrix is as follows. Actual/Predicted Not Churn Churn Not Churn True Negatives False Positives Churn False Negatives True Positives The first cell contains the actual 'Not Churns' being predicted as 'Not-Churn' and hence, is labelled 'True Negatives' (Negative implying that the class is '0', here, Not-Churn.). The second cell contains the actual 'Not Churns' being predicted as 'Churn' and hence, is labelled 'False Positive' (because it is predicted as 'Churn' (Positive) but in actuality, it's not a Churn). Similarly, the third cell contains the actual 'Churns' being predicted as 'Not Churn ' which is why we call it 'False Negative' . The fourth cell contains the actual 'Churns ' being predicted as 'Churn' and so, it's labelled as 'True Positives' . Please analyse the "" Actual/Predicted "" head and ""b old marked "" in above 4 situations carefully, hope you will understand.",311117.0
108160,467312.0,"Hey Keerthi, Yes, you're right they have same formulas, but they actually come from different domains. Have a look at this link: https://stats.stackexchange.com/questions/362332/is-there-any-difference-between-sensitivity-and-recall Hope this helps you out!",301655.0
108160,467325.0,"Yes, both have the same formula and both are same also. The difference for why using this is explained in that session itself. You can once again go through : https://learn.upgrad.com/v/course/208/session/25994/segment/133982",311117.0
108160,468010.0,"Yes, both have the same formula. please refer the below link: https://classeval.wordpress.com/introduction/basic-evaluation-measures/",314183.0
108080,466984.0,To get an intuition of the data at hand I guess since they are not really used in evaluating the model.,310974.0
108080,467054.0,"Please go through the heading "" Different target conditions "" in the link : https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values and "" Core Knowledge "" in the link: https://www.med.uottawa.ca/sim/data/Sensitivity_and_Prevalence_e.htm you will probably get the sight and difference on this.",311117.0
108080,468013.0,"The positive and negative predictive values ( PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnotstics test that are TP and TN results, respectively.The PPV and NPV describe the performance of a diagnostic test or other statistical measure. A high result can be interpreted as indicating the accuracy of such a statistic. The PPV and NPV are not intrinsic to the test; they depend also on the prevelence. Although sometimes used synonymously, a positive predictive value generally refers to what is established by control groups, while a post test probability refers to a probability for an individual. Still, if the individual's pre test probability of the target condition is the same as the prevalence in the control group used to establish the positive predictive value, the two are numerically equal. In information retrieval , the PPV statistic is often called the precision .",314183.0
108041,466732.0,?precision_score --&gt; This will display the help documentation fro the ?precision_score function.,313826.0
108041,466753.0,? - Gets the description of a function. In this case It computes the precision. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.,310467.0
107945,466333.0,yes its very much possible when you have senstivity and accuracy intersect and specificity forms a different path,318017.0
136385,589868.0,Logistic Regression With logistic regression we are interested to model some probability p as a function of an explanatory variable x. The naive approach to set p = β0 + β1x + e doesn’t work because given normally distributed residuals e it is inconsistent with the fact that 0 ≤ p ≤ 1. Instead one divides the probability p by (1−p) in order to obtain the so called odds ODDS = p/ (1 − p) and postulates that the log odds or logit z = log(ODDS) = log (p/(1 − p )) is a linear function of x: z = log (p /(1 − p )) = β0 + β1x. This is the (binary) logistic regression model. This is the transformaiton,301121.0
137292,592940.0,"Intersting question. The denominator is to blame for sure for the jumpy curve but I think it is better explained if you consider that for most of the other metrics the denominator is either total negetives or total positives. These are constants for any dataset that you will consider. TP + FN for instance it the total number of positives in the dataset. For precision the sum of TP and FP or the number of positives predicted, depends solely on the algorithm and thus this can change wildly with changes in the algorithm or the hyperparameters.",319357.0
78415,316874.0,"Hi prashant, I just wanted to mention that you donot need to type: input_str =' This is my first code' , in your solution. This is hard coding. If you do this your solution becomes specific for this particular input. This solution might fail for other sample inputs. Your solution will work fine even if you omit the above mentioned line.",304815.0
78415,316742.0,"Hi, This is failing because of the ' ' parameter i.e. space you have provided in the parameter for lstrip. It will pass if the string had leading spaces but it won't if the string had tabbed before the text. And the 2nd test case has tabbed spaces before the text input_str.lstrip() will pass both test cases, you might be knowing this solution.",304694.0
87100,358658.0,"list.append(x) Appends an element to the end of an existing list The return type of the method is 'NoneType' , meaning it returns nothing The above statements also implicitly mean that the append() method does not return the whole of the appended list back. Hence, when we do b = a.append('Python') 'Python' gets appended to the list a if it already exists and returns a value 'NoneType' which gets assigned to the variable b . Instead, the code has to be written as a.append('Python') b = a",313826.0
90170,375786.0,"Yes, But I think any language would have some kind of timer library to store the current time which we can make use of to calculate the time taken by a function or a peice of code.",318329.0
90170,375796.0,"Yes, It really huge time saving specially when using numpy python library in comparison to standard list when working with billions of records in a dataset.",317811.0
81314,330858.0,"Hi Anil, Please follow below you tube videos, hopefully you have Windows OS you will be able to install easily. https://www.google.com/search?q=How+to+install+python+and+jupyter+in+Windows&amp;ie=utf-8&amp;oe=utf-8#kpvalbx=1",307843.0
81314,331628.0,"Documentation is quite clear, I followed it step by step and I could able to install it properly. I suggest to give a another try.",306011.0
81314,331907.0,Goto control panel and uninstall the anaconda and try to reinstall and as Prashant said document is quite explanatory . all the best,300687.0
87280,359664.0,"The below worked for me without any issue: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) answer = input_list[2][0] print(answer)",310974.0
87280,359669.0,"hey thanks for the info, i have been decalring the input_list , guess thats why it wasn't accepting",314313.0
87280,359851.0,Yes. The inputs are dynamic and we should not be defining them with static values in any of these questions,305845.0
81869,343034.0,"facing same problem..In the assignment they r teaching lstrip,rstrip..I used as per that.when code is running successfully then why submission is being rejected..And..""This to my first code"" was not mentioned anywhere.How can I understand what went wrong with my code?",308964.0
81869,334030.0,syntax is len('Python') please remove the quote before len and after the closed brackets and also remove the ? mark hope it works,300687.0
81869,333762.0,"Antony - Check your code your solution output are printing twice ""This is my first code"" But in Expected output it should print only once ""This is my first code"". This is why your submit gets rejected, check your you may use below kind of code. input_str = ""This is my first python program"" print(input_str)",307843.0
81869,333775.0,Not only Solution output and expected output must be match but also you should use the query that has been taught in the course for the assignment you are working. If you use code other method which is not part of current assigmnet your result may rejected. This is what my thinking.,307843.0
81869,333766.0,"I understand i could have written twice, but there is no way i can reset it so i cant have a look at it what went wrong. But in the second problem the run code says ok but says rejected on submission.",302734.0
81869,333830.0,"the next exercise, i tried to run this to find the answer and it says syntax error.",302734.0
81869,335859.0,use only below commnad - len('Python') Remember do not use extra single quotes and question mark (given in the question ) while running above commnad. Above command when you run in notebook gives the length of python that is 6.,312746.0
81869,338948.0,"I also faced similar issue for 1st Python problem. My output got printed only once. verfification was successful also. But though got rejected because they need output ""This to my first code"" but output should be ""This is my first code"" where input was "" This is my first code"". ""This to my first code"" was not mentioned anywhere.",311404.0
78218,314424.0,"It means that you have to start from the last element and pick elements at difference of 2.In this case a[0] = 2,a[2]=6,a[4]=4,a[6]=2. :: -- traverse the entire string - -- start from end 2 --- Difference of 2",300698.0
78218,314403.0,"hi Shravan, The first two columns signifies that you have to execute command for whole variable. - refers that you have to start in reverse direction. 2 refers to the jump i.e. alternative value needs to be printed.",301559.0
78218,314470.0,thnx got it,300693.0
78218,314478.0,then hw o/p of this a[3:0:-2] is 42,300693.0
78218,328499.0,+ means left to right - means right to left or reverse and integar refers to the value's postiton interval so in this case its 1 2 3 4 5 6 7 8 in reverse order 8642,303082.0
81336,331237.0,"Hi, It seems you are dealing with something related to lstrip function. You should not get this if you run any code. In case you are using lstrip function in the code, &lt;variable_name&gt;.lstrip() should work properly. Or would help if you could show the code you are using.",304694.0
81336,331341.0,You've forgotten the brackets at the end of your. This could be one reason. Make sure you've got your parenthesis after calling the lstrip() method. It should work.,306733.0
81336,331432.0,Screenshot of the error message will help,308437.0
81336,331283.0,"Seems You forgot to call .strip() and stored the method instead: Below can be illustrate The lstrip() returns a copy of the string with leading characters stripped. All combinations of characters in the chars argument are removed from the left of the string until first mismatch. Example: Working of lstrip() random_string = ' this is good ' # Leading whitepsace are removed print(random_string.lstrip()) # Argument doesn't contain space # No characters are removed. print(random_string.lstrip('sti')) print(random_string.lstrip('s ti')) website = 'https://www.programiz.com/' print(website.lstrip('htps:/.')) When you run the program, the output will be: this is good this is good his is good www.programiz.com/",307843.0
85021,349410.0,thanks for sharing,310217.0
84680,346886.0,"The last line of the question states ""Print both the answers as sorted lists, i.e. convert the final sets to lists, sort it and then return it."". So you need to apply sorted() and list() on the difference and symmetric_difference sets before assigning the value to answer_1 and anser_2 variables. Hope this helps.",313826.0
84680,346918.0,"I think this time they changed the question. When i completed the course, they did not ask for sorting. you can find the sample solution below :-",312746.0
84680,352242.0,Convert the sets into lists again.,318370.0
84680,358140.0,"Hi All, I am getting below output when running below code. Can anybody explan why it is coming in curly brackets. olution output {1} {1, 7, 8, 9} Code: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1) #Type your answer here set_2 = set(list_2)#Type your answer here answer_1 = set_1.difference(set_2)#Type your answer here answer_2 = set_1.symmetric_difference(set_2)#Type your answer here sorted(list(answer_1)) sorted(list(answer_2)) print(answer_1) print(answer_2)",320195.0
81342,331289.0,"Yes, it always opens in broswer. You are all set if you can create notebooks and run commands. You're not missing anything. Now, ""Go Tiger"" :)",310974.0
81342,331330.0,My understanding is Jupyter notebook comes with enterprise edidion which company gets it when they purchase for any cloud application in that clould application can provide some profiles and entriprse edition GUI of its own GUI. And one which we are using is open sourse which is standard endition comes with version example is 3.6 and it only opens with browser only having properties. Hope it helps,307843.0
81342,331619.0,By default it opens in browser however I read in documentation that it has a commond interface as well.,306011.0
81342,331338.0,"Jupyter always opens in the browser. One reason is to make deployment and sharing easier. If you want to use a shell, try the IPython version. But, Jupyter is better when you're just starting out though.",306733.0
84174,346287.0,.format(c) will replace {0} with the value of c. Kindly go through this link to know better https://www.geeksforgeeks.org/python-format-function/,310585.0
84174,346340.0,"print(""c is: {0}"".format(c)) Above line is formatting the 0th element of format() function. In this case it is c which is a list. I have attached an example to format 1st element of format function:-",312746.0
84181,346397.0,"var = ""Pythons syntax is easy to learn, Pythons syntax is very clear"" print (var.replace("","", "" &amp;""))",311855.0
84181,346347.0,"Alternatively, You can also use list subscritpion. e.g.",312746.0
84181,346300.0,try this.,310585.0
84181,349138.0,"hey manmeet trry var= ""Pythons syntax is wasy to learn, Pythons syntax is very clear"" answer = "" &amp; "".join(var) print(var) it will work",317062.0
84717,347092.0,"Found the Solution , Guys. Let it be. After running this code, we have x = 12345 , y = 54321 and z = 'hello' . The value of the tuple t is unpacked into the 3 variables x , y and z . Note that the number of variables has to exactly match the number of elements in the tuple, or there will be an error.",317070.0
81892,333847.0,"No, it wasn't some luck or brute force, your logic is correct. The question wanted you to use indexing properties on Strings, and you did exactly that.",301652.0
81892,334029.0,thanks amit and Ashish,300687.0
81892,334071.0,Your logic is correct. Moreover you can experiment different type of indexing . Below is another code snippet hope it will give a variation . first_name = (input_str[6:10]) second_name = (input_str[:5]) customer_code = (input_str[11:]),301648.0
81892,333880.0,"Your logic is correct and also your understanding is correct otherwise three output can't be correct by luck :) However below is the explanation. let say the string position as below Kumar_Ravi_003 k(0),u(1),m(2),a(3),r(4) Kumar position string _(5),R(6),a(7),v(8),i(9) _Ravi position sting _(10),0(11),0(12),3(13) _003 Position String import ast,sys #Here input_str is String and it is reading the console input input_str = sys.stdin.read() #first_name is Ravi and above shown Ravi position starts from R i.e 6 and ends at 9, so select [6:9) first_name =input_str[6:10] #write your answer here #Second name kumar starts with k from string position 0 ands ends at 4 so select [0:5] second_name =input_str[0:5] #write your answer here #Custome code 003 starts with letter 0 i.e from 3rd string position ands ends at 14 so select [11:15] customer_code =input_str[11:15] #write your answer here print(first_name) print(second_name) print(customer_code)",307843.0
81892,335661.0,"It would be better, if we split the string by '_'. If we just go by the positon then if the length of Firstname, Lastname or unique customer code changes then output can be unexpected. If we split the sting by =_' then we get a list whose first element would be lastname Kumar and so on. Thanks!",304813.0
81892,336630.0,"The solution that you have written is the static one and will work only for this example because the length of the name, surname might not be the same always. As explained by Md Liyakat, use the String split property that returns the list in order [lastname,firstname,code] and it will hold true for every use case. you can then just extract from list using the appropriate index.",305845.0
81892,338968.0,"As sugested by Md Liyaka and Parul, this is static logic and we should split by _. It will be applicable for any input. Length can't be used as paramaeter if it need to be applicable for all test cases.",311404.0
81892,338973.0,"Below code I used and it accepted. second_n,first_n,customer_c=input_str.split(""_"") first_name = first_n second_name = second_n customer_code = customer_c print(first_name) print(second_name) print(customer_code)",311404.0
81892,355155.0,"use split over '_' and convert string to list and print first,second,third of list for a more generic soln.",318005.0
77636,310315.0,Both a and b are allocated memory. a contains the address of 32. b also contains the address of 32. Both a and b thus are the references to the memory location containing 32. This is unlike C or C++ where a and b would contain the value 32,304319.0
77636,310686.0,"Hi, Unlike General Data Structure, Python stores data in its specific data structure - List, Tuple, Matrices. So, when you say a= 32 , it is considered as a = [32], thus, value for a[0] = 32. Same way , b=[32], hence b[0] = 32. It is similar to array with the exception that the data elements can be of different data types. We can have both numeric and string data in a python list.",300691.0
77636,313341.0,"I dont think they are pointing to same memory location, it is just that the value stored in both of these veriables is same.",306011.0
77636,314098.0,"Hi, a=32, this creates an integer object 32 in memory and its reference is assigned to variable a now, b=32 this creates another reference variable 'b' which refers to the same integer object in memory we can verfy this by using id() function: if(id(a) == id(b)): print(""a and b have same id"") output: a and b have same id",304815.0
77636,317344.0,"If we are assigning the same numbers to a and b then their memory location is the same. In the above example, both a &amp; b are sharing the same id.",304692.0
78790,318230.0,"You are right buddie.. it returns ""None"" when used inside print statement. Below is the explanation with example - b = [1,2,3] print(b) #returns [1,2,3] b.append(4) #appends 4 to the list but return nothing as we don't have print statement print(b) # prints [1,2,3,4,5] print(b.append(5)) # returns none as we have called print on append and as append returns nothing then it prints ""None""",304694.0
78790,318757.0,,304692.0
78790,321074.0,"yes, when append is in print statement then output is none. eg; x=[a,b,c,d] print(x.append(e)) output is; None.",305847.0
87151,358947.0,List comprenhension can be used to replace all occurences of 'R' with 'SAAS' if that is the goal. List comprenhensions are covered as part of the Python prep course. DA_languages = ['SAAS' if x == 'R' else x for x in DA_languages] This would check each element of the list DA_languages. If the value of the element is 'R' then it is rplaced by 'SAAS' else it is returned as it is.,313826.0
87151,359124.0,"If you only want to remove duplicates from a list, Then fastest way is: first convert list into set then convert that set back to list. Now your list will have only unique values.",320073.0
87151,359145.0,"Append() -- is used to add a new object to the list. Remove() -- is used to remove the element from the list. this is the reason why you see 'R' removed every time you run the code. See the changes; In In[58] 'R' appeared 4 times and then 3 times and then 2 times in your snapshots.. Now, if your intention is to replace the 'R' with 'SAAS' then your code is not doing that. You may want to use the code which is correctly suggested by Vinay above.. DA_languages = ['SAAS' if x == 'R' else x for x in DA_languages]",316349.0
87151,359740.0,"You can try the above methods mentioned. But if you know what are the number of values you need to remove, use a loop. And do it as many times.",319721.0
80298,325407.0,"I checked how I did it and the only difference is I explicitely converted the input_dict.values() to list. Accepted mine. input_dict = {'Jack Dorsey' : 'Twitter' , 'Tim Cook' : 'Apple','Jeff Bezos' : 'Amazon' ,'Mukesh Ambani' : 'RJIO'} sorted_x = sorted(list(input_dict.values())) print(sorted_x)",300748.0
80298,344038.0,This one line code also works fine which internaly converts to a sorted list. value_list = sorted(list(input_dict.values())),304696.0
80298,352224.0,By defination dictionaries can not be sorted. However I've tried your code in Jupyter and got the expected output. Source : http://thomas-cokelaer.info/blog/2017/12/how-to-sort-a-dictionary-by-values-in-python/,318370.0
81449,331766.0,I've done reverse and even I have the same question.,310974.0
81449,331814.0,It will be beneficial for you if you could complete Python as well with R.,301648.0
81449,332062.0,All the modules given in Learning tab must need to completed which as due date of 29-Sep-2018. The couse is designed in such a way that before moving to the specualization we all have basic and prerequisite and later on our mentor when discuss the difference between R and Python for that case to make us undersstand we need to know basic of R and Pyton. that is why the advance topic is locked and it will be covred by mentors once we complete the prerequisite.,307843.0
81449,331906.0,Even I have opted Python but commpeted R..,300687.0
81449,332305.0,"Hey everyone, In my humble opinion, this is the course preparatory duration and therefore intros to both R &amp; Python are available. The choice needs to be made once the course begins from the 30th of Sept. As per the curriculum the Python segment is mentioned as an optional in the tools and languages section. I am personally fully interested to take up both, but as a choice is mandatory, I want to go with Python as it has a broader range of applicability in software programming compared to R. Would love to hear your takes on the same. Cheers, Rubik",311315.0
81449,332009.0,"You can choose either of the two, but at the beginning you have to learn the basics of both and continuewith the one you have chosen",304389.0
81449,332148.0,You should be doing Python as well because the percentage of people opting for R is less than 25% so most probably the course will be conducted in Python and not in R.,305655.0
81449,332198.0,"The learning track of both the languages differ. you can notice that 1. if you opt for python then the courses you should be covering in pre-prep is : Python, SQL, Excel, Tableau(Common) 2. if you opt for R then you should go through Common + Python +R",305845.0
81449,332329.0,"Hello All, I am interested in learning both R and Python. I have opted for python as it is turning out to be the language for Data science with the collection of libraries in python has for various ML/AI/Deep Learning problems we will encounter once we are in this field. However, I have completed the course on R (Advanced part is locked) and Python R is a good start if you are coming from a non programing back ground. Syntax is very simple and it gives you a feel for the subject. More than the programming language, I will focus on the methods applied to solve a certain problem we might encounter. Once you understand the problem, and approach you take to solve it, python or R is just a mechanism to implement the Solution. There are advantages and limitations in using both the languages which we will learn once we start using the programs. Warm Regards, Rajesh R",300708.0
81449,332643.0,Anamika Firstly python will be our chosen language for data science based on voting results So learn python and it’s must R is neither mandatory nor optional In the sense if u learn R it will benefit u tat u will learn additional language. Since the forum is offering free content on R I suggest learn that...,308437.0
84810,347430.0,please try this,310585.0
84810,347433.0,it is happening because you are printing the whole list. Please find logic to print only the get the requirement.,310585.0
84810,347940.0,print(input_list['Tim Cook']),300690.0
84810,347558.0,"import ast,sys input_str = sys.stdin.read() input_dict = ast.literal_eval(input_str) print(input_dict['Tim Cook'])",311041.0
84810,347469.0,"Successful code execution only means that the syntax of the code is correct,but does not verify the logic of the code. As mentioned in the above comments, the logic of the code needs to be changed as per the problem description. Another solution would be to use the dictionary get() method as shown below. It is a good practise to use the get() method since it does not throw an error if the key is not found in the dictionary, where as the normal convention of dict['key'] results in an error and could pose a problem if the exception is not handled properly. Also, get() has an option to provide a default return value in case the key does not exist.",313826.0
84280,346659.0,"In Simple words Kernal is Similar to our human heart, which does the hard work like pumping the blood.It is just for understanding. Kernel will take care of the hard work like talking to hardware, managing memory and many more things Kernel is the only thing which is in between your OS (Operating System)and underlying hardware, it's translates OS commands and provides necessary resources for OS. OS is a bundle if applications but real driver is Kernel. Just think you move mouse it is kernal which takes the hardware input and translate the command as per Operating unit and in system then you can seen movement of mouse. Hope it helps Kernel = Heart !",307843.0
84280,346627.0,A ' kernel ' is a program that runs and introspects the user's code contained in a notebook document. go through these for better understanding: 1. https://ipython.org/ipython-doc/3/development/kernels.html 2. https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html Hope it helps,310585.0
84280,346933.0,"A notebook kernel is a “computational engine” that executes the code contained in a Notebook document . When you open a Notebook document , the associated kernel is automatically launched. When the notebook is executed (either cell-by-cell or with menu Cell -&gt; Run All ), the kernel performs the computation and produces the results. Depending on the type of computations, the kernel may consume significant CPU and RAM. Note that the RAM is not released until the kernel is shut-down.",312746.0
90124,375349.0,"Hello Anushree, It is working fine for me.",320195.0
90124,375355.0,What is the error message you are getting?,318804.0
90124,375361.0,"Hi Anushree, I remember doing this and as far I remember you might want to try spliting it with &amp; surronded by spaces. Below is the statement for the same. input_str.split(' &amp; ');",318368.0
90124,375379.0,We need to use below to split input_str.split('&amp;'),318358.0
90124,375372.0,"When I ran the above code I observe below behaviour of code: When you don't assign the ""str.split('&amp;')"" to any other or same variable split won't work as follows But when we assign ""str.split('&amp;') to any variable or str it works. Hope you now know where problem is coming in your code. Let me know if it works or not .",317991.0
90124,375741.0,"Hi Anushree, If we use split at '&amp;' as shown in the below image, its working fine. If its not a graded question, kindly show your code, the issue may be that you're not assigning the string after splitting as told by Vipul in his second example or something else.",318355.0
90124,376393.0,your code is correct.. but the error is because of unnecessary indent at the start of 'import' statement .. allign your complete code to left..,316349.0
90124,376375.0,"hi all, thanks for the answers. below screen shots, not working for me",321850.0
78896,318756.0,Here the above statement converts the above-given input to your console converting into the input_str as the input is not mentioned in your console.,304692.0
78896,329763.0,"ast.literal_eval raises an exception if the input isn't a valid Python datatype, so the code won't be executed if it's not. Use ast.literal_eval whenever you need eval . You shouldn't usually evaluate literal Python statements.",303082.0
78895,318951.0,will print the value from starting till end [:],305838.0
78895,318755.0,"Hello, The output of your below statement is given below, Here the DA_languages{:} functions as collecting all the values in the list from First to last.",304692.0
78895,319361.0,It will print value from starting to the end.,305847.0
78895,329765.0,[:] means end to end or all value of the list,303082.0
88492,366789.0,"Yes, interesting implementation of these functions as they belong to different classes maybe &lt;string&gt;.split (""separator"") ""separator"".join(list)",300717.0
88492,366178.0,My thoughts on this are both join and split are functions (or methods) of string type. list class do not have this function. when you are using split() you are basically splitting a string whereas when you use join() you are joining elements of a list. this is why the syntax of join has to have a string (like '-') in the beginning and pass the list as an argument. I liked this question. I didn't give a thought to this while I was going through Python module.,319898.0
88492,366838.0,"Split function can be used to break the line into tokens and Join can be used to join multiple tokens into single line Both function does the different operation that is the reason for different syntax for instance- shayam&amp;98980945324(name&amp;mobile) you can break the above string to name and mobile var=shayam&amp;98980945324 list=var.split('&amp;') Join can be used to join multiple string into one single string list1=['python','R'] str='&amp;'.join(list1)",317845.0
88492,366935.0,"Writing a method for one type is easier than implementing it for several types. Split is invoked on a string &lt;string&gt;.split(comparator) Join if invoked on a iterable as you suggested must be implemented in all iterables. It's cleaner and easier to implement it on String. &lt;string&gt;. join(iterable) Unclean way is to implement join method in all iterables like list, tuples, etc",320074.0
88492,367098.0,"Join: This is used to concat the values from list/tuple with a separator to form a string. Example: dataList = ['This', 'is', 'test'] finalVal = "" "".join(dataList) print(finalVal) Returns: This is test Split: This is used to explode the string by using a separator and form a List. Example: strVal = ""This is test"" strList = strVal.split() print(strList) returns: ['This', 'is', 'test']",312953.0
81536,332395.0,"You can use the below code . import ast,sys input_str = sys.stdin.read() input_tuple = ast.literal_eval(input_str) a=list(input_tuple) a.append('Python') tuple_2=tuple(a) print(tuple_2)",301648.0
81536,332386.0,"Check the below code it seems by mistakenly you used insert function as assighn ment with list input_tuple=('Monty Python', 'British', 1969)# this is correct code input_tuple having item list ""Monty....."" tuple_1=list(input_tuple)# this is correct codetuple_1 is converted and assigned similar to list of input_tuple tuple_1.append('python') #this is correct code appending python in tuple_1 list it appends in the last tuple_2=tuple(tuple_1) #this is correct tuple_2 is same as tuple_1 print(tuple_2) #this is correct code tuple_2=list(input_tuple) #This is correct code #tuple_2=insert(3,'python') # This is wrong code insert is function which insert the items in the list isted use below tuple_2.insert(3,'pythons1') # Use this code for the insert and it insert in tuple_2 at third position tuple_2=tuple(tuple_2) # all codes below is correct, just I used puthon1 , python2 to more understang print(tuple_2) tuple_2=list(input_tuple) tuple_2 += ['pythons2'] tuple_2=tuple(tuple_2) print(tuple_2)",307843.0
81536,332165.0,"Since 'python' and 'Python' are two different Strings, you are not able to get the correct output. Capitalize the ' p ' in 'python', (please see again what element the question asks you to add.) Also, your 2nd approach will show an error, you need to write tuple_2 . insert(3,'python') instead of tuple_2 = insert(3,'python') since insert() method does not return any value, but it inserts the given element at the given index",301652.0
75981,314617.0,single / represent normal division in python where as double// exclude the float value that's why value varies this is built in python . For more clarity go through the link https://www.tutorialspoint.com/python/python_basic_operators.htm,305838.0
75981,302931.0,"For (b//a) if we give a=777 and b=7.77 b//a=100.0,not 99 and b/a is also 100.0.Again if a=777 and b=0.777 the value changes to 999.Why is the value changing because while there is shift in decimal point?Similar case for negative operands (b//a) for -777//77.7 is -10 but -777//7.77 is -101.0. -11/3 can be expressed as (-3+2/3) or (-4+1/3).Omitting the fraction,since -4 is further away from 0 hence -4 is the output is what I understood. But b//a of 777/77.7 should be 10 since it completely divisible.I am sorry,I am still confused.",301651.0
75981,302514.0,"(b//a) ---&gt;&gt; Floor Division - The division of operands where the result is the quotient in which the digits after the decimal point are removed . But if one of the operands is negative, the result is floored, i.e., rounded away from zero (towards negative infinity) 9//2 = 4 and 9.0//2.0 = 4.0, -11//3 = -4, -11.0//3 = -4.0 print(a / b) ---&gt;&gt; this is normal divison",303082.0
75981,302822.0,It is also called as an integer division. So 77.7 Will be considered as 78 as a divisor. and when 777 is divided by 78 then the closest floor integer will be 9.,300500.0
75981,302846.0,"Python 3 does implicit floating point division for both integers and float data types when / is used. a/b is normal division. So here 777/77.7 = 10.0 (implicit conversion to float). // is floor division. Which means the fraction is omited, and the floored value is returned. For positive numbers floored value is the number closer to 0. For negative numbers floored value is the number away from 0. eg 777//77.7 = 9.0 eg 22/22.0 = 1.0 22//22.0 = 1.0 22//22 = 1 22/2.2 = 10.0 22//2.2 = 9.0",300748.0
84869,347657.0,"The list append() method does not return anything or inother words it returns a 'None' value when an append is done on a valid list. Hence, the variable out_type is assigned a value of &lt;class 'Nonetype'&gt; when this line is executed out_list=in_list.append('Python') Further, trying to apply the tuple() constructor on out_list results in the error that you have got. Try the below code: input_list = list(input_tuple) input_list.append('Python') tuple_2 = tuple(input_list) # Make sure to name the final tuple 'tuple_2' print(tuple_2)",313826.0
84869,348545.0,you can direcly code as: tuple_2 = tuple(input_list.append('Python')) print(tuple_2),304813.0
84869,348807.0,"@Md Liyakat, You would still get the error TypeError: 'NoneType' object is not iterable",313826.0
81555,332249.0,You should index on the list not on the string object in this case as the data is the list object. Try below: answer =input_list([2][0]),310974.0
81555,332300.0,"Here is your code which you have written,it seems by mistake you are getting error. however below is explanation where you get error. import ast,sys#line 1 this is correct input_str = sys.stdin.read() #line 2 this is correct this is library function use for read input_list = ast.literal_eval(input_str) #line 3 this is correct used to store the read value in above steps answer =input_str([2][0])##line 4 here is Mistake as you assign input_str instead of input_str #You can use below code #answer =input_list([2][0]) # this is correct as input_list has values of list [SAS................................) print(answer)#line 5",307843.0
81555,332697.0,"Thanks guys but still facing the issue.Please advise [['SAS','R'],['Tableau','SQL'],['Python','Java']] Solution output Traceback (most recent call last): File ""/code/source.py3"", line 5, in &lt;module&gt; answer =input_list([2][0])#Type your answer here TypeError: 'list' object is not callable Expected output Python ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) answer =input_list([2][0])#Type your answer here print(answer)",300687.0
81555,332703.0,"Hi Maya, There are 2 issues in your folloeing code expressions: answer =input_ str( [2][0] ) #Type your answer here 1. First issue is the object name, you are trying to access input_ str where you should be refering input_ list 2. Second issue is with the index notation. While refering the indexes for alist you dont use round brackets. So instead of ( [2][0] ) you need to use [2][0] So correct expression would be answer =input_ list[2][0] #Type your answer here Hope this helps.",306725.0
81555,332869.0,Thanks Pulkit now its working.Maybe the third mistake was the extra brackets .Does the extra brackets can be an issue?,300687.0
81555,339338.0,Yes extra brackets will never work as it will not identify which element to extract. Without extra brackets it will work only.,311404.0
80168,324667.0,"Hi Madhu, As long as strip has a character/characters that matches the leading and trailing characters of a string, strip() should work. Check for any spaces/characters that you have not accounted for. eg.: str = 'MY OH MY' print(str.strip('MY')) Will print 'OH' str = ' MY OH MY' #notice the leading space print(str.strip('MY')) Will print ' MY OH'",300748.0
80168,324642.0,"Sir, its working sir",304692.0
80169,324649.0,"Sir, I hope Strip function is only used to remove the characters at the both ends only. As you can see in the above example in the first one we can remove ""%"" this as we didn't mentioned any space, but in the second one we mentioned space so it could not remove ""%"" this symbol.",304692.0
80169,324653.0,"Hi Madhu, Strip() will only strip the character from the start and the end of a string. If you need to remove all occurrences of a character from a string there are several ways to do it, the easiest being replacing the character that you want to remove, with blank (empty quotes) : print ('Hello % there % how % are % you %'.replace('%', '')) (You can also use the filter function, which will be covered later in the course). ..And from what I have read about the size of strings for a 64 bit python installation, one python string can use up to more than 100 GB of RAM.",300748.0
80169,327093.0,To remove % from the whole string. You can use split() to convert into a list without % and then use join() with a blank space,304319.0
83649,343901.0,"1- Why do you want so ?.. what is the use case ? 2- It gives us a hint that jst seeing the output , we can tell it is list or dictionary. It comes quite handy in lots of situation. 3- Even if you want , jst print the dictionary using for loop. e.g.",312746.0
88758,367686.0,"try searching in the programs. ie, where the start menu is over there you can type and search for any program. search for anaconda there. or you can also install again..and while installing choose to create shortcuts on desktop and task menu also.",317998.0
88758,367695.0,"Go to Start Menu or click on windows icon at bottom left of screen of your device, There you will be able to find anaconda click on..itw ill expand and show you anaconda navigator..",317811.0
88758,367831.0,This is where you can find Anaconda navigator: C:\Users\(Your PC User Name)\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Anaconda3 (64-bit) Or Go to control panel --&gt; Right click on the Control panel grid header --&gt; Select More --&gt; and check the location option. Now the program location shows up in the control panel.,318495.0
88758,368670.0,Have you got the resolution?,308966.0
88758,367931.0,"if still you are unable to locate, you can access Jupyter notebook using command line , just run from cmd: jupyter notebook it will open it on browser. I am mostly using it like that only.",318319.0
81671,332766.0,"DA_languages = ['R','Python', 'SAS', 'Scala', 42] If you use below then it not print none rather it will print all the list Print( DA_languages ) However if you use print(DA_languages.append('R')) Then it is print None, because DA_language.append(""R"") calling a funtion appends whic appends items in the last of list thats it and does not retrun anything. And whenever print stament takes any argumentt as funtion which does not return anything in that case compiler iterprets print to give outputput as None. Same you can check below example for more illustration. Whenever you don't explicitly return anything, you implicitly return None . You wrote the function to print one of the two values x or y , but not to return anything. So None was returned. Then you asked Python to print the result of calling the function. def jiskya ( x , y ): if x &gt; y : print y else : print x print ( jiskya ( 2 , 3 )) gave the ouput: &gt;&gt;&gt; 2 None",307843.0
81671,337541.0,Because the function is not returning any value the print function will recieve a null vaue which in python is None. Hence a none is printed on the console.,306729.0
81671,332806.0,"1. Check out the documentation of append () function. It doesn't return any value. 2. If you don't specify a return statement in a function, and still ask for it's return value, it will display None . #The output of the following program is None: a = 5 b = 6 def sum(x,y): z = x+y print (sum(a,b)) #The output of the following program is 11: a = 5 b = 6 def sum(x,y): z = x+y return(z) print (sum(a,b)) 3. None is Python's equivalent of null .",301652.0
88685,367282.0,"To add to the above, also check if your System Variable/path has been correctly set or not.",317998.0
88685,367206.0,"Install the latest version of Anaconda once installation completed update it using Anaconda command prompt conda update --all if issue still persist try deleting the file Please delete this file C:\Users\Admin1\.condarc and try again (this file might be hidden, please enable show hidden file)",317845.0
88685,367963.0,"Try installing it again.. Happened with me too,along with other issues.. I uninstalled ir and tried again. now its working fine..",314221.0
81677,332832.0,"1. Here is a very well written article on memory allocation in Python. ( The tag analogy is quite interesting. ) Link: http://foobarnbaz.com/2012/07/08/understanding-python-variables/ 2. Also, read the following sections of the article: a) Mutable vs. Immutable Objects b) Making References to Values Link: https://medium.com/@tyastropheus/tricky-python-i-memory-management-for-mutable-immutable-objects-21507d1e5b95 I am sure, these articles will clear your doubts.",301652.0
80668,327801.0,"Every tutorial will be almost same you need to practice alot to get a hold over it . For more clarity and how it basically works you can refer to below link, as it explain the term step by step. https://www.pythonforbeginners.com/lists/list-comprehensions-in-python/",305838.0
80668,328082.0,"Examples in the provided link for list comprehensions are more like a functions in mathematics. if you are pretty aware of the functions, that might help you to get started with it. if not, you can have an insight of it as ""Functions"" topic will be under Mathematics section. Hope this will help you!",300686.0
80668,328486.0,Thanks All for your responses,301109.0
80668,328387.0,List is very important topic in Data Structure python. its very easy to understand interms of practising.Please find below link which will give detailed explanation about Lists https://www.youtube.com/watch?v=tw7ror9x32s.,304396.0
80523,326911.0,"Difference between 2 sets - Denoted by A-B. Contains all elements of A that is exclusive to A only and do not belong to B Symmetric difference - Denoted by A △ B. Contains all elements of A and B that are exclusive to each set. If A and B are overlapping,then the symmetric difference will exclude all elements that belongs to A ∩ B. Hope this helps",305839.0
80523,327026.0,The symmetric difference of two sets Aand B is the set of elements which are in either of the sets A or B but not in both. whereas in difference if A and B are two sets. The set difference of A and B is a set of elements that exists only in set A but not in B for more clarity go through the link below https://stackoverflow.com/questions/50871370/python-sets-difference-vs-symmetric-difference,305838.0
80523,327246.0,Think of symmetric difference as : (A-B) U (B-A) OR (A U B) - (A ∩ B),301652.0
80523,328070.0,"Lets A and B are the two sets: A = {1,2,3} B = {2,3,4,5} .difference()- A.difference(B) denotes ""(A-B)"" - means ""elements of set A which are not be in set B"". So in the above example the difference is {1} since only 1 is not in B. B.difference(A) denotes ""(B-A)"" - now the output is {4,5} since 2, 3 are alredy in set B. In ""difference"" method : output of A-B is different from (B-A) .symmetric_difference() - means the elements which are not common in both set A and B. output will be - {1,4,5} since 2,3 are common in both the sets. In symmetric difference - &gt; the output of (A-B) will be same as (B-A) or you can simply say (A-B) = (B-A)",300686.0
80525,326902.0,"Get the values using the dictionary's ""values()"" method and then sort the output using the sorted function.",306733.0
80525,326906.0,value_list = list(input_dict.values()) print(sorted(value_list)) This will suffice. Please use a Python compiler or Online Python compiler and run your codes. A specific compiler will help you by pin pointing the code piece where the issue had happened. https://www.tutorialspoint.com/execute_python_online.php - This is a basic online compiler,305839.0
80525,360999.0,l1=sorted(list(input_dict.values())) name = l1[1],313770.0
80525,328225.0,New_list=list(input_dict.values()) Sorted_list = sorted(New_list),308437.0
87210,359243.0,"Hi Sreekar, AFAIK, there is no standard built-in function by the name ""nest()"". We just need to pass the correct indexes to extract a particular element from the nested list(or any list in general). answer = input_list[2][0] print(answer)",313826.0
87210,359246.0,"nest is not a function, looks like your a bit confused on how the nested list wrks. You should be able to get the desired result by the just using print(input_list[2][0]) The above will give you Python as the output. The first index [2] refers to the list ['Python', 'Java'] within the main list and second index [0] extracts Python from the aforementioned list.",312259.0
87210,359408.0,"nested list contain the list as the element inside. for instance list=['SAS','R'] , list1=['Tableau','SQL]', list2=['Python','Java'] if we want to store the above list in one single list, we can use nested list. input_list = [['SAS','R'],['Tableau','SQL'],['Python','Java']] this internally store as the matrix Column 0 1 Row 0 [0][0], [0][1] ['SAS','R'] 1 [1][0], [1][1] ['Tableau','SQL] 2 [2][0], [2][1] ['Python','Java'] Java -&gt; print(input_list[2][1]) Python -&gt; print(input_list[2][0]) SQL -&gt; print (input_list[1][1]) You can print any element like above.",317845.0
84783,347372.0,"The same code is working fine when I tried. However, you could get such an error if you have created a variable by the name ""list"" by mistake. In that case, python would try to use the variable ""list"" instead of the in-built constructor ""list()"" and hence result in an error. This could be a reason for you are getting the error. Attached is a sample execution of this kind of a scenario.",313826.0
81702,332990.0,"The first line of your program has an error. pimport ast,sys Just remove the ' p ' from import. In order to use stdin and literal_eval , you need to first import the modules (in this case, ast and sys ) they are a part of. import keyword helps us do that.",301652.0
81702,332989.0,"Is your code not working ? I have implemented similar type of logic and it works. import ast,sys input_str = sys.stdin.read() input_tuple = ast.literal_eval(input_str) tuple_to_list=list(input_tuple) tuple_to_list.append('Python') modified_tuple=tuple(tuple_to_list) print(modified_tuple)",301648.0
81702,339479.0,"The first line of your code should be import ast, sys instead of pimport ast, sys",306729.0
81702,347370.0,"import ast,sys input_str = sys.stdin.read() input_tuple = ast.literal_eval(input_str) a=list(input_tuple) b=a.append('Python') tuple_2=tuple(b) print(tuple_2) I am not getting why the cited above code is showing error while executing.",300690.0
81702,356430.0,"Hello Subhashis, Could you please try with below code. It will work. a=list(input_tuple) a.append('python') tuple_2=tuple (a) print(tuple_2) You are inserting b in tuple syntax so result will come as none and your code is falling because you need to add 'python' in your actual input_tuple so it should be a. I hope that you are clear now.",320195.0
81705,332942.0,"input_dict = {'Jack Dorsey' : 'Twitter' , 'Tim Cook' : 'Apple','Jeff Bezos' : 'Amazon' ,'Mukesh Ambani' : 'RJIO'} #it seems need sort on the Values not on Key chk if this works value_list =list( input_dict.values() ) #Type you answer here print(sorted(value_list))",307843.0
81705,332983.0,"In the above question we were asked to sort the list based on Values of a Dictionary. Here in this dictionary values are like 'Twitter' 'Apple' etc. Below logic will work. import ast,sys input_str = sys.stdin.read() input_dict = ast.literal_eval(input_str) value_list = list(input_dict.values()) print(sorted(value_list))",301648.0
81705,349130.0,The answer should be value_list = input_dict.values() print(sorted(value_list)),317062.0
88770,367787.0,Excellent answer Indranil. I was reading about this indexing only and getting confused. but you have articulated it very well. thanks.,317998.0
88770,367794.0,"Two-dimensional slicing you are most likely to use in machine learning. It is common to split your loaded data into input variables (X) and the output variable (y) with use of colon ("":""), it will gives from and up to element after slicing. For the input features, we can select all rows and all columns except the last one by specifying ‘:’ for in the rows index, and :-1 in the columns index. X = [:, :-1] For the output column, we can select all rows again using ‘:’ and index just the last column by specifying the -1 index. Y = [:, -1] data = array([[11, 22, 33],[44, 55, 66], [77, 88, 99]]) # separate data X, Y = data[:, :-1], data[:, -1] print(X) print(Y) for detail information check the link https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/",317845.0
88770,367765.0,""":"" is optional in case you want to get the full array. But in case you want to get a slice of it, then you have to use it. Say I have an 2d array: x = array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) to get only 6,7,8 I have to use x[1,1:4] I am yet to go through the course, but believe this is waht you are looking for. In normal python array also, you can see &gt;&gt;&gt; x=[1,2,3] &gt;&gt;&gt; x [1, 2, 3] &gt;&gt;&gt; x[:] [1, 2, 3] &gt;&gt;&gt; From numpy documentation: https://docs.scipy.org/doc/numpy/user/quickstart.html array([[ 0, 1, 2, 3], [10, 11, 12, 13], [20, 21, 22, 23], [30, 31, 32, 33], [40, 41, 42, 43]]) &gt;&gt;&gt; b[2,3] 23 &gt;&gt;&gt; b[0:5, 1] # each row in the second column of b array([ 1, 11, 21, 31, 41]) &gt;&gt;&gt; b[ : ,1] # equivalent to the previous example array([ 1, 11, 21, 31, 41]) &gt;&gt;&gt; b[1:3, : ] # each column in the second and third row of b array([[10, 11, 12, 13], [20, 21, 22, 23]]) ------------------------------------------ One thing to note, this is for column selection not for row. ie, b[1,] will work but b[,1] wont. For that you have to use b[:,1] So, my guess it is better to go with "":""",318554.0
87233,359397.0,make use of patterns in your input. In this case your objective can be achieved by using the split() functionality of the string. Try it.,319898.0
87233,359406.0,"You can use split() function with any length of data. it really matters the token that you are using to split the function. for instance input_str='dolphBlaineCharlesDavid_EarlFrederickGeraldHubert_001' Still, you can break this using the split function. input_str='dolphBlaineCharlesDavid_EarlFrederickGeraldHubert_001' listname=input_str.split('_') print(listname[1]) print(listname[0]) print(listname[2])",317845.0
87233,359407.0,"Yeah that works, thanks. For anyone looking for the script : input_str = input(""Enter details in the format Lastname_firstname_number :"" ) input_list = input_str.split(""_"") print(input_list[1]) print(input_list[0]) print(input_list[2])",316132.0
87233,359483.0,"The split function is independent of length of string. It just sees the delimiter and breaks the string on behalf of that and puts them in list. For eg. Kumar_Ravi_003 if you use input_str.split('_') You'll get ['Kumar','Ravi','003'] Similarly, for Jha_Gopal_01 you'll get ['Jha','Gopal','01'] And now all you have to do is print the elements of the list accordingly. So use split() function to make it generalised. Hope this helps :)",318495.0
79282,320378.0,answer = input_list[2][0] print(answer),304692.0
79282,320262.0,It's giving an error because you are printing the input list itself and not extracting 'Python' You need assign input_list[2][0] to answer and print it -,304694.0
79282,322005.0,"HI Prashant, Please read the code snipped again, you have missed mentioning the indices while assigning input_list to answer. Below is the snapshot for the same. You only printed input_list[2][0], but assigned whole input_list to answer variable.",301559.0
79282,322897.0,"How to sort eletement in calculated SET ? Example: list_1 = [4,5,6,7,8] list_2 = [5,6,9] set_1 = set(list_1) set_2 = set(list_2) r = (set_1.difference(set_2)).union (set_2.difference(set_1)) print (sorted(list(r))) print (set(sorted(list(r)))) o/p : [4, 7, 8, 9] {8, 9, 4, 7} if using ""sorted"" option by converting into list the output is getting in list format i.e. in square bracket [ ] if coverting agin into SET then sorting is not working. Required output : { 4,7,8,9 }",304812.0
83528,343209.0,1. Please do not hard code anything . 2. you can refer below logic : output_list = input_str.split('&amp;') print(output_list),301648.0
83528,343199.0,1- Error - In your output_list python starts with p (small p). It should be in caps 2- Please do not hard code anything in the code. input list should be split by '&amp;' in the code. please find screen shot of solution below for your ref -,312746.0
83528,351974.0,In addition to everyone's input the main 2 reasons it didn't verified the code. 1. Printed output is case sensitive 2. There are white spaces in the expected output while your list doesnt.,318370.0
85175,349411.0,Learn to Program: https://www.youtube.com/playlist?list=PLGLfVvz_LVvTn3cK5e6LjhgGiSeVlIRwt refer to the video on lists in the above playlist,310217.0
85175,349519.0,"In Python, unlike other programming languages, the range [0:2] means all the values starting with index 0 and excluding the value at the index 2. That is why you got the first two values in your list which are at index 0 and index 1. If you want all the 3 values print(nest[0:3]) or print(nest[:3]).",314730.0
85175,349552.0,"Hi Mahesh, This concept is known as splicing. 4 points to note in splicing: 1) [:] - Refers to all the elements of list 2) [a:] - Refers to all elements starting from position/index 'a' till end of the list 3) [:a] - Refers to all elements till position/index ' a-1' (important) 4) [a:b] - Refers to all elements starting from position/index 'a' till position/index ' b-1' Example: list=[1,2,3,4] 1) [:] will give [1,2,3,4] (all elements) 2) list[1:] will give [2,3,4] (from index 1 till end) 3) list[:3] will give [1,2,3] (from 0th index/position till (3-1) i.e 2nd index) 4) list[1:3] will give [2,3] (from 1st index till (3-1) i.e 2nd index) Hope it helps :)",318495.0
85175,349544.0,"Hi Mahesh, If we have str = ""Hi, welcome to CS50!"" and we perform the operation: print(str[0:7]) -&gt; it gives -&gt; the characters starting index 0 to (7-1) i.e. 6 . Thus, we get ""Hi, we"" as the output. Morevoer, if you're familiar with Java's substring() function, this str[0:7] is similar to that. It will always give you the output which consists of one character less than mentioned in the ending index. In your case we'll get the output as [ [1,2,3,4], [5,6,7], [8,9,10] ] when we use print(nest[0:3]) or print(nest[0:]).",318355.0
85175,349283.0,"Hi Mahesh, the statement : nest=[[1,2,3,4],[5.6.7],[8,9,10]] as this is a nested list u need to consider [1,2,3,4] as item 1 ,[5,6,7] as item 2 and so on the indexing [0:2] means - to start at first element in the list (note in python that the slicing index starts at 0 ). The 2 means to end at the secound element in the list, but not include it ( that is 2-1 index). The colon in the middle is how Python's lists recognize that we want to use slicing to get objects in the list. u can find more here: https://www.pythoncentral.io/how-to-slice-listsarrays-and-tuples-in-python/ hope this helps.",317822.0
85175,349693.0,"Hi Mahesh, In python we do no include the last index. nest=[[1,2,3,4],[5.6.7],[8,9,10]] in ur nestes list [0:2] means 0 - lastindex-1 so it will be 0,1 elements only. This is for all cases ,your last index will not be considered and the slicing will be done till end index -1",317982.0
85175,349919.0,"Mahesh, [0:2] means 0,1 *excluding 2 guys, please give to the point answer in a minimum possible text",318344.0
85175,349947.0,"Mahesh, In python, when you sepcify a range the max value of range is excluded. So, in print(nest[0:2]) 2 is excluded. Hence, it prints only 0th and 1st position value. To get your expected answer you need to type print(nest[0:3])",311855.0
85175,350079.0,If a list x has n no of elements the index of first element is 0 and that of last i.e nth element is (n-1).,300690.0
85175,350211.0,"in python slicing works as [0:L-1], so if you have [0:2] . it will give you 0th and 1st value . [-1] -- will give you the last element [:] will give you all",303082.0
85175,350239.0,"Nested List is a list that contains list as its elements. To access the elements of a list we usually perform splicing which in return provides us a list of desired number of elements. Such desired number of elements can be obtained in several ways. Also, In list, indexing starts from 0. Suppose list1 is a list of elements [1,2,3,4,5,6,7]: 1. [ : ] = [1,2,3,4,5,6,7] i.e all the elements of the parent list 2. [2 : ] = [3,4,5] i.e. all elements of parent list with starting from ndex 2. 3. [ : 4] = [1,2,3,4] i.e. all elements of parent list upto index (4-1). 4. [1 : 5] = [2,3,4,5] i.e. elements of parent list with index from 1 to 4. In nested list, accessing of the elements of sub list (element of parent list) can be done in following way: Suppose list2 = [ [1,4,2,3] ,[5,2,8], [5,6,8,3]] then list2[2][2] = 8",318772.0
80545,327081.0,"Hi, If you are using Jupyter Notebook, make sure that you execute the previous commands where the string is getting created. As it worked fine for me.",304319.0
80545,327177.0,"Madhusudhan it will not run if you directly go and run a particular line , if you want to run go to cell menu and click on run all cell then it will definitely run ( cell menu is located at the top) . Hope this will clear.",305838.0
87666,364025.0,Open jupyter notebook and there on top right you could see upload option,318732.0
87666,363920.0,you can open the *.ipynb using jupyter note book. https://www.youtube.com/watch?v=F5NsvOUDXi8,317845.0
87666,364323.0,"Hi Naveen, You can install Jupyter(http://jupyter.org/) or you try to open the .ipynb file using jupyter.org on temporary basis. If you installed Jupyter, just run jupyter by clicking on the icon, once you click, the jupyter will open and after a while you can access the folder structure from the default browser itself and can navigate to the desired *.ipynb file you wish to open. Example: The folder structure AS-IS opens in default internet browser.",317418.0
83672,344015.0,Test case pass means the your output matches the expectected output but the method or way you use to get the ouput is not correct console you can see the first video i.e understanding console points. there it is specifued in details.,307843.0
83672,344019.0,Check this out!!,310519.0
83672,344040.0,Please do not hard code input_str in your code. In verify steps it checks only given input and expect output. But when you submit it check other inputs as well. Please check out the sample solution below:-,312746.0
83672,345326.0,"I too got the same error at the beginning. The reason for this error is, you are giving the ""input_str"" again in your code. The first two lines of code is acutally fetching the ""input_str"" already: ``` import ast, sys input_str = sys.stdin.read() ``` so you just need to pass "" input_str = sys.stdin.read()"" to get the appropriate answer.",307486.0
83672,358547.0,"Just pass Input_str in every question, no need to hard code it.",317418.0
83717,,nan,
83803,352233.0,URL : https://www.oreilly.com/learning/how-do-i-use-the-slice-notation-in-python This article was veryhelpful in understanding both.,318370.0
83803,345038.0,"Double Colon comes when we do slicing and do not provide some argument. single colon is separator for various arugments in slicing. From the docs of pyhton : a[start:stop:step] The start and step arguments default to None. Slice objects have read-only data attributes start, stop and step which merely return the argument values (or their default). e.g if you are include an empty stop and step parameter, you will get double colon. &gt;&gt;&gt; a = [1,2,3,4] &gt;&gt;&gt; a[2:] [3,4] &gt;&gt;&gt; a[2::] [3,4] And to understand what the step parameter actually does: &gt;&gt;&gt; b = [1,2,3,4,5,6,7,8,9,10] &gt;&gt;&gt; b[0::5] [1, 6] &gt;&gt;&gt; b[1::5] [2, 7]",312746.0
85191,,nan,
83804,345060.0,"Sorry, I did not understand your question initially but after seeing Naga's answer i got that you are asking about single quotes rather than single colon. I agree its good idea to follow string declaration with single quotes. But there might be some situation where you have to use double quotes (if you are not using escape sequence) or vice-versa. e.g if string contains sinngle quotes with in itself.",312746.0
83804,344990.0,"In the offical tutorial document of python, single quotes are used everywhere. So, I feel it a good habit to follow the offical conv entions.",310585.0
80746,328165.0,Regular expression is one way to do it. Another way is to simply use replace. #very simple example using replace. The string has 2 tabs and 2 spaces both leading and trailing the text.,300748.0
83184,341905.0,You can get help from the section below : https://learn.upgrad.com/v/course/208/session/15860/segment/80228 Windows 10 https://www.youtube.com/watch?v=Q0jGAZAdZqM Other version: https://www.youtube.com/watch?v=dgjEUcccRwM,301648.0
83184,351352.0,Alok and Aniruddha gave a good source for installation process. In addtion to that I would like to add another video to install the same on Mac OS. URL : https://www.youtube.com/watch?v=YJC6ldI3hWk,318370.0
83184,341933.0,I am assuming that you are using windows OS. you can follow the attahced pdf in the course itself. OR follow below you tube video:- https://www.youtube.com/watch?v=SjKtDEEv0_E,312746.0
83431,342677.0,There are two things which are very important while doing coding exercises:- 1- Never do hard coding in the code. 2- Always make your code generic e.g. plz dont use your own list of input.,312746.0
83431,343093.0,"You have changed the code variables in the question, The above code is correct But in the assignment environment, you should not change the variables because they are taken from other places.",304692.0
83431,342632.0,"You need to use input from input_list which is provided by upgrad coding console. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) answer = (input_list[2][0]) print(answer) Modify your code as below : print(input_list[2][0])",301648.0
83431,343446.0,"To answer your question please look 2 points to understand the concept and hope it will clear your doubts. 1. Your code is correct when you run independent in the python notebook and it will give out put as below because here you are using the assigned list having name lst and it contians 3 list element 2. But when you run the same code in Assignment it will not work because each answer of upgrad expects to get the input of list using the read function only if you see carefully the console in upgrade use import which is use to import and then use read function so whatever upgrade console has input string we can read and work therefore when you work in console always use below format and don't erase this format else console will not accept your answer. mport ast,sys input_str = sys.stdin.read()",307843.0
83820,344959.0,You can try below.,307843.0
83820,345682.0,"I would like to give a theoritical eplanation for Amit's answer, Since it is a nested list the heirarchy for extraction follows: outer list element &gt; inner list1 element &gt; inner listn element hence [2][0]",305845.0
83820,345760.0,Can you try this,310443.0
83820,345830.0,"Amit, its a nested list. Access elements like this ListName[ index of outer element box ][ Position of the element in the innermost list]. input_list[ 2 ][ 0 ] will give you python.",315028.0
83820,348536.0,input_list[2][0] givees Python.,304813.0
85781,352348.0,"There is no difference between single and double quotes. Both are internally stored as string objects. Strings need to be defined in matching single quotes or double quotes. However if the string itself has single-quote(s) then it has to be enclosed in double-quotes and vice-versa. Python tries to keep the string representation as simple and unambigious as possible. In your examples, even though the string was defined in double-quotes, the interpreter keeps it simple and shows it in single-quotes. However, if the string itself has a single quote in it, then it has to be defined using either the double-quotes or the escape sequence. To keep the representation unambigious, the representation is done within double-quotes so that the single-quote in the string is handled properly. &gt;&gt;&gt; 'What\'s happening?' ""What's happening?"" &gt;&gt;&gt; ""whats's happening now?"" ""whats's happening now?"" Check out the below link which is a goos read on this topic. https://docs.python.org/3/tutorial/introduction.html#strings",313826.0
85781,352610.0,Single n double quotes can be used for strings in python,308437.0
85781,352643.0,"Short answer: almost no difference except stylistically. Short blurb: If you don't want to escape the quote characters inside your string, use the other type. eg: string1 = ""He turned to me and said, \""Hello there\"""" would be slightly more unsightly than saying string2 = 'He turned to me and said, ""Hello there""' The same applies to single quotes/apostrophes.",319721.0
85781,352726.0,"No real difference between the two.It can be chosen purpously like: For example a text Hello “Vinod” is to be stored as a string, then one should use single quotes as: Str1= ' Hello ""Vinod"" ' Where as if string should resemble Hello ‘Vinod' then it should be declared as follows: Str2= "" Hello 'Vinod' """,300684.0
85781,352803.0,Absolutely no difference in Python and R as well!,301890.0
85781,353406.0,"String literals can be enclosed in matching single quotes (') or double quotes ("").",318802.0
80530,326951.0,"The issue is like Pranesh said, brackets after set_2: answer_1 = set_1.difference(set_2 () ) You can avoid this mistake in the future by remembring that () is used for functions, whether it be inbuilt or custom. Set_2 is not a function: Correct answer: answer_1 = set_1.difference (set_2)",300748.0
80530,326895.0,"You've added a pair of parenthesis next to set_2 while computing answer_1. That's what's causing the error. What the error means is that you're trying to call an object, just like you would call a function. You're getting that error because you can't teachnically call objects. Rewrite the code to compute answer_1 like this: answer_1 = set_1.difference(set_2) It should work fine.",306733.0
80530,328077.0,"answer_2 = set_1(setdiff(set_2)) I have seen that you have called "".setdiff"" to evaluate symmetric difference which is not a method. set_1.symmetric_difference(set_2) is the correct way to calculate symmetric difference. Also, Paranthesis will also not be included. Hope this will help you.",300686.0
80530,329213.0,For answer1 use difference function For answer2 use symmetric_difference function,308437.0
80530,339507.0,The best way to do as others have mentioned : set_1 = set(list_1) set_2 = set(list_2)#Type your answer here answer_1 = set_1.difference(set_2)#Type your answer here answer_2 = set_1.symmetric_difference(set_2)#Type your answer here print(answer_1) print(answer_2),306729.0
80530,341178.0,"I have used the following code to calculate the same. answer_1 = set_1.difference(set_2) #Type your answer here answer_2 = (set_1.difference(set_2)).union(set_2.difference(set_1))) #Type your answer here - In the symmetric difference, I am getting the right output but not in the order of the answer it is expecting. - As per the defination of the set the order does not matter. - My question is how to resolve this kind of issue.",301124.0
80530,341373.0,To find symmetric difference there is an inbuilt function check out Name is symmetric difference I guess not sure,308437.0
80530,341374.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1) set_2 = set(list_2)#Type your answer here answer_1 = set_1.difference(set_2)#Type your answer here answer_2 = set_1.symmetric_difference(set_2)#Type your answer here print(answer_1) print(answer_2)",306729.0
80530,342317.0,"Correct code for this: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1) set_2 = set(list_2) answer_1 = set_1.difference(set_2) answer_2 =set_1.symmetric_difference(set_2) print(answer_1) print(answer_2)",311952.0
89468,371219.0,"If nest is array or list, below answer will explain you the details. nest[index] -&gt; nest is an array or list and index is the index value you are extracting from the array or list. Note : Index starts from 0 in list and array. nest[1] will print second element from the list or array. If nest is not array or list, can you please provide more context on the problem?",318368.0
89468,371789.0,"Hi, The command helps to extract the 1 index value from a nested array(a list or a tuple). nested array consists of lists and tuples as the elements. so you will get the second list or tuple from the nested array as the output since indexing starts from 0.",310504.0
89468,371252.0,"Looks like you are trying to print the second element of nest because indexing starts from 0. Now nest can be any of the data type collection eg list,array,tuple etc. Please provide the code if you need further explanation. Happy to help!!",309451.0
85249,349481.0,You would need Jupyter Notebook installed to work with these files. I believe the instructions are provided to install Anaconda which also includes Jupyter Notebook. Please follow those instructions and try to open through Jupyter Notebook web platform,313826.0
85249,349483.0,"1st Method : There is directory when you open python notebook, open the directory till you reach the .ipynb file and then open. 2nd Method : There is upload button you can upload you file when you open th python noteebok and it automatically opens your desired notebook.",307843.0
85249,351919.0,you can not open these files like any other file we used to open on desktop so open it from inside the jupyter note book Make a folder for all your sample exercises python note book and save all these files you are downloading from upgrad portal in same folder ..it will be easier to locate your python files,319869.0
83361,343085.0,"For the basic level it is enough but in course period of time you can be able to use Pandas[for data driving purpose], numpy[for numbers], scikitlearn[for ML], matplotlib[for visualization] packages in the main course.",304692.0
83361,342390.0,"I was told by Aiswarya Ramachandran, an alumini of our couse that the prep content for python is only in-depth",310585.0
83361,342359.0,"As part of this prep course, study the material what is provided. This should cover the basics. Then for Data Science, Numpys, Pandas, Scikitlearn , matplotlib python packages/libraries can be learnt. From there you can move or advance yourself to become a good Python Developer for DS",310482.0
83361,342421.0,"1- The python course will provide you the intermediat level knowledge if you are following all the lectures and refernce docs. Try to solve each problem yourself. 2- If anyone wants to become good data scientist in future, He/She should have advance level knowelge of core python and some of python libraries. some important libraries are - 1- NumPy 2- SciPy 3- Pandas 4- SciKit-Learn 5- TensorFlow etc. But i would suggest to focus on the course itself as of now. It is sufficient. We would get advanced level knowledge during the course itself.",312746.0
89940,374443.0,Refer: http://www.datasciencemadesimple.com/get-list-column-headers-column-name-python-pandas/ Also watch the loc and iloc video..2nd session I guess.. a = dataframe_name['column_name'],314547.0
89940,374463.0,You can use loc and iloc functions to do this or if you want a particular set of columns use df.column_name or df[column_name],318358.0
89940,374467.0,If you want to check which are present in a data frame you can use dataframe_name.columns If you want the values in a particular column in a dataframe dataframe_name.column_name or dataframe_name['column_name'],308635.0
89940,374492.0,"In order to print the column of a data frame check the below example Assigning an index column to pandas dataframe df2 = df1.set_index(""State"", drop = False) The df2 dataframe would look like this now: To extracts the values for the rows from Alaska through Arkansas for years 2005 to 2007: df2.loc[""Alaska"":""Arkansas"",""2005"":""2007""] Below output is generated Extracting a column of a pandas dataframe df2.loc[: , ""2005""] Below output is seen I hope this clears your doubt",300688.0
89940,374623.0,Simply put column with square parantesis for df. df['column_name'],318368.0
90635,378040.0,"Can you share the steps you tried? The procedure you followed? Please understand that the discussion should enhance your knowledge. So, if you're asking for solutions without trying, that wouldn't be encouraged.",319721.0
79572,321935.0,Got you.Thankyou Kiran (y),300684.0
79572,321904.0,"I am not sure why input_list is treating as a string in your case. Basicially if you have edited the code at the right place it should treat as a list. May be if you provide the entire code it would be helpful to debug. Meanwhile, using below code, i could execute it w/o any error- import ast,sys input_list = (sys.stdin.read()).split(',') input_list.pop() input_list.append('SPARK') print(input_list)",304694.0
79572,321909.0,"Entire code is: import ast,sys input_list = (sys.stdin.read()).split(',') input_list = input_list.pop(3) input_list.append('SPARK') print(input_list)",300684.0
79572,321947.0,"The reason you are getting string is due to the split operator, because split convert list to string .",305838.0
79572,321938.0,"try below code will work input_list = ['C','R','Python','Spark'] input_list.pop(3) input_list.append('Spark') print(input_list)",305838.0
79572,321914.0,"if you assign input_list.pop(3) to input_list, it will assign 4th word. So, if you see type of input_list then it would be string and no more a list. Try below example, you will understand input_list = ['C','R','Python','Spark'] input_list = input_list.pop(3) print(input_list) type(input_list)",304694.0
83568,343364.0,Check if you are connected to the internet and also check your internet speed through an online speed test. Ensure that your internet bandwidth has a minimum 1Mbps connection. Reload the page after clearing your browser cache. Try opening the page in incognito mode or in a different browser. Check your firewall settings to make sure brightcove is not blocked from your network and get the below domains whitelisted on your VPN and local firewall software for watching videos and seamless experience over learning platform,310585.0
83568,343356.0,try logging out then close browser and reopen.,310585.0
83568,343639.0,"Refresh the page and quickly scroll down to see the question, thats what I did and it worked each time!",304813.0
83569,343363.0,As per my understanding to add any item to the list we use append wherein to remove any item from the list we use the pop function.,300687.0
83569,343570.0,"append is used to add any item in the list using item itself where pop is used to retrieve any item from a list using index no. say x=['A','B','C','D'] y=x.append('E') print(y) would give y=['A','B','C','D','E'] on the other hand z=x.pop('1') print(z) would result into z=['A','C','D'] x.pop('1') would return 'B'.",300690.0
83569,343579.0,"Ignoring previous one see this one append is used to add any item in the list using item itself where pop is used to retrieve any item from a list using index no. say x=['A','B','C','D'] y=x.append('E') print(x) would give x=['A','B','C','D','E'] on the other hand z=x.pop('1') print(z) would return z='B' print(x) would give x=['A','C','D'].",300690.0
83569,343694.0,you can use th help function as described below :-,312746.0
83569,344222.0,Append add an item to list at the last position Pop removes an item based on position specified from the list,308437.0
83569,343437.0,"lets see below is my list myList = [""The"", ""earth"", ""revolves"", ""around"", ""sun""] &gt;&gt;&gt; myList ['The', 'earth', 'revolves', 'around', 'sun'] Now we will see Appends myList.append([""a"", ""true""]) &gt; myList.append([""a"", ""true""]) &gt; myList # Here append function add the request at last in the string ['Yes', 'The', 'earth', 'revolves', 'around', 'sun', ['a', 'true']] pop removes the item at a specific index and returns it. &gt; a = [4, 3, 5] &gt; a.pop(1) # Here pop(1) removes the second position element i.e because sting position starts from 0 ... 3 &gt; print(a) # once you print a now it will print only [4,5] [4, 5]",307843.0
85353,350286.0,Python should show 100% since there are no graded assignments. Check again if you have missed completing any.,310974.0
85353,350715.0,Thank You I shall check again,308431.0
85353,350930.0,There might be some sub-section that you have missed. I skipped the secyion on installing Jupyter notebook and the status was bot showing as 100% complete,300698.0
85353,351196.0,"This might happen if the video is not watched till the end. Also make sure that all in-video questions are answered. Could you please close the window and retry logging in and run the video? Also, ensure that you have good Internet connection. You can also refer to the following link for more details: http://help.upgrad.com/learning-platform/video-playback-issues",319721.0
83419,342579.0,"when i check test case 2: python gets changed to penthon, apple gets changed to applo. Pls help",302735.0
83419,342582.0,pls refer to screenshot of test case two,302735.0
83419,342607.0,"when you verify your code it only checks your code with given input. But when you submit the code it also checks with different or random inputs. Try to share your code, if you face any issue furthur.",312746.0
86470,356244.0,"Hi, id function returns the identity of an object. This is an integer which is unique and constant for the lifetime of the object. This is the address of the object in memory. For example: id(1) is 1667984448",314730.0
86470,356239.0,id () is an inbuilt function in Python . Syntax: id (object) As we can see the function accepts a single parameter and is used to return the identity of an object. This identity has to be unique and constant for this object during the lifetime. Two objects with non-overlapping lifetimes may have the same id () value.,310974.0
86470,356406.0,"In python, ""everything is an object"" . Each object is identified by a ""constant-unique-random"" integer which is known as the ""identity"" of the object. The built-in function id(object) returns the ""identity"" of the object. https://docs.python.org/3/library/functions.html#id Please note that only in C implementation of Python(also known as CPython), the ""identity"" of the object is the memory address of the object, but there is no such guarentee in other implementations of Python ( like Jython, PyPy etc).",313826.0
86470,358180.0,There are 10 students in a class. Each student reside at their home. id() of each student will give you their residence address. Same in the case of python,318780.0
83252,341898.0,Can you please paste your code here and the problem .May be list are not in correct order.,301648.0
83252,341917.0,"1- List maintains insertion ordere in python. so even if the elements are same in the list but there order is different list will not be treated equal. so please check the input list in your case. 2- I am posting a sample solution which you can refer:- if you paste your code, it will be easier for us to suggest more concrete answers.",312746.0
83252,342873.0,Thanks for the response! Figured out my issue. I was using the set function for both answers and that was resorting the sequence.,312490.0
83252,345973.0,"set is an unordered collection of non-duplicate elements. So, when you convert a list of elements to a set, only the unique elements from the list will be present in the set, but the order may not be maintained.However, if necessary, the set can be converted to a sorted list by applyinfg the ""list()"" constructor and the ""sorted()"" function on the set as shown.",313826.0
83393,342462.0,This is the concept of deep and shallow copy. Check the link below for a better understanding. https://www.programiz.com/python-programming/shallow-deep-copy,309451.0
83393,343087.0,Concept of Deep and Shallow Copy you can get clarified here https://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/,304692.0
83393,342583.0,"when we use = operatore it uses shallow copy i.e. it does not make a new list. it makes a new reference variable which points to the old list. if you make any changes using the new list varibale you are indirectly making the changes in old list. that is why you are able to see changes in both. In Contrast, deep copy makes a new list. so if you dont want to change your old list, use deep copy option.",312746.0
88814,367986.0,Could you share your complete code?,318084.0
88814,367989.0,"You probably used an in-place modification method such as remove() or append() while creating the list ""a"". Maybe you did something like this: a = otherList.append(some_element) The error message means that ""a"" essentially has the value None. So, it's an object of NoneType. Check your code for that, fix it and it'll be fine.",306733.0
88814,367991.0,"it might be because of syntax error if you can copy/paste the sample code here, we can identify the exact issue Note: please don't copy/paste the code if it's graded question here is sample code to convert the list to the tuple list=['test1','test2','test3'] t=tuple(list) print(t) O/P ('test1', 'test2', 'test3') Hope that helps",317845.0
88814,367992.0,"I think your object a is None. Try this a=[1,2,3] tuple_2=tuple(a) print(a) Output is &gt;&gt; (1, 2, 3) However if a is None then you will get TypeError.",318458.0
88814,368021.0,"Please check the error. The error clearly says 'NoneType object is not iterable. Which means in this case your list objet in this case ""a"" is not list but None. Can you try to do, print(a == None) Believe that will give True. In case you are returning the list from any method check if you are getting the value or None.",318554.0
88814,368680.0,"you can try the following code, since, as per error, the list seems to be blank in some case: if len(list_variable): tuple2 = tuple(list_variable) else: tuple2 = ()",317987.0
88814,368028.0,"The error is mentioning that the object "" a "" is an object of type "" NoneType "". I reckon you are trying to do something like: a = a.append(""new_element"") or a= list1.append(""new_element"") Please note that the method append() just appends an element to an existing list and does not return the appended list back. In fact, the return type of the append() function is 'NoneType', meaning it returns nothing. In the above case, 'NoneType' gets assigned to the list2. The correct way to append an element to a list and then assign it to a new object is list1.append(x) a= list1 Related question: https://learn.upgrad.com/v/course/208/question/87100 This error could also occur if you assign the print() function to an object. Ex.: a = print(list1) or something similar. print() function also returns a ' NoneType ' object. Hope this helps. PS: Please include code snippet to the question for more clarity.",313826.0
86985,358185.0,"Hi, This is a nested list.. with index starting from 0 so in this case it is 0,1,2 So suppose if you want to take out python out of it.. So code will be Print(input_list[2][0]) So it refers to 2nd list of the nested list and in that 2nd list it refers to the first index value which is Python..",305129.0
86985,358237.0,"nested list contain the list as the element inside. for instance list=['SAS','R'] , list1=['Tableau','SQL]', list2=['Python','Java'] if we want to store the above list in one single list, we can use nested list. input_list = [['SAS','R'],['Tableau','SQL'],['Python','Java']] this internally store as the matrix Column 0 1 Row 0 [0][0], [0][1] ['SAS','R'] 1 [1][0], [1][1] ['Tableau','SQL] 2 [2][0], [2][1] ['Python','Java'] Java -&gt; print(input_list[2][1]) Python -&gt; print(input_list[2][0]) SQL -&gt; print (input_list[1][1]) You can print any element like above.",317845.0
86985,358271.0,"A nested list is a list that appears as an element in another list. To extract an element from the nested list, we can proceed in two steps. First, extract the nested list, then extract the item of interest. It is also possible to combine those steps using bracket operators that evaluate from left to right. Eg. nested = [""hello"", 2.0, 5, [10, 20]] innerlist = nested[3] print(innerlist) item = innerlist[1] print(item) or print(nested[3][1]) Hope this helps you understand the concept of nested lists.. And for your query.. answer will be: print(input_list[2][0])",318495.0
86985,359226.0,"You can think this is a kind of 2D array. for the code input_list = [['SAS','R'],['Tableau','SQL'],['Python','Java']] every single list can be considered as a row of the array.You have to simply write the index of the element in a sequence you are seeing. First step is the index of list, then the index of the element you want to extract i.e. for ""Tableau"" it is 1,1 you simply write input_list[1][1].",318851.0
86985,359383.0,Thanks for the info,303227.0
86985,358171.0,"The code for your answer is below. input_list = [['SAS','R'],['Tableau','SQL'],['Python','Java']] print(input_list[2][0]) Explanation is given below. input_list is here 2 dimensional array and understand the element is stored as below. ------------------col0------col1--------------- input_list = [ SAS R -------------Row0-------------------- Tableau SQL -------------Row1-------------------- Python Java -------------Row2--------------------- Now you want to retrieve the Python see above diagram it in lie in row2,col0, therefore the codefor your answer is print(input_list[2][0])",307843.0
86985,359720.0,"Nested list can be accessed just as an 2D array , if you have done programming . the first index r of input_list[r][c] represents the (r +1)th row and the second index c represents (c+1)th column of the list so just imagine the data of list is stored in the form of table and you need row number and column number to access the individual element of the list . like input_list = [ [ 'SAS' , 'R' ] , [ 'Tableau' , 'SQL' ] , [ 'Python' , 'Java' ]] can be thought as r /c 0 1 0 SAS R 1 Tableau SQL 2 Python Java and can be accessed by the row and column numbers .",320685.0
83130,341297.0,you can see below screen-shot for your reference-,312746.0
83130,341404.0,To assign a variable to another varible first declare the varible and then it can assigned to another variable. Alok example is good one. a = 5 b = 6 sum = a+c # Here first variable a and c being declared and then it is being assigned to anaother variable. Note : Just to make sure when assigning Data type should be matched left and right.,307843.0
85446,,nan,
85455,350769.0,"When you click on verify , all the test cases are automatically run by the compiler",318427.0
85455,351835.0,"When you click on verify, only the sample test cases are run. All the non-sample test cases are run when you submit. Here Test case 2 is non-sample test case which cannot be known beforehand. All you can do is try running your code for different inputs and check the output",318576.0
85449,350731.0,"It seems, you will not get the solution code (Or) Answer. It only validates your code with different set test data/Test Cases and all test data should evaluate as Correct.",311502.0
85449,351148.0,"For the best solution you can try various ways in which a program can be solved and observe the memory utilised in the output screen when your code gets passed. The lesser the memory, the better the code. This is what i did while coding.",305845.0
85449,351192.0,Thanks Siva &amp; Parul.There is also a sample solution available after submission accepeted or number of attempts over in the details popup window. But what I think is for some particular problems ony not for all.,317980.0
85469,350784.0,Try it! print(input_str.split('&amp;')),304813.0
85469,350805.0,"split is a function/method in python to convert string to a list. if a string is to broken then simply use input_str.split(' ') with a space between the quotes. Incase the split is to be performed when a special character is encountered like in this case ""&amp;"" then use input_str.split('&amp;') to obtain the desired result.",312259.0
85469,351582.0,input_str=“I love Data Science & Python” array=input_str.split(“&”) print(array) You can use Split() to break the string in parts by providing character or substring of string.,317845.0
87253,359541.0,"user_name = [""user1"", ""user2"", ""user3""] email_id = ""uSEr1@host.com"" // Take the email_id, split it by '@', this will return an list of values, in this case ['uSEr1', 'host.com'] //Then take the first element from this list and then check if it is present in user_name list. // I am considering all elements in user_name list is in lower case hence after extracting username //from email_id making it in lower case before comparison print(email_id.split(""@"")[0].lower() in user_name)",318554.0
85489,350917.0,"If your are using your current session from very long time, Jst restart the notebook server, and reopen the notebook. It worked for me. Mac sometime limits the background ideal threads. Just try it out.",312746.0
85489,351382.0,"Command worked fine for me without restarting the server. However, the command only worked when there is an object in the cell snippet. Hope this helps.",318370.0
85462,350768.0,As the ending node is -3 so according to the rule while printing element in a list it prints to one node less then sepcified so it prints till -4 over here,318427.0
85462,350781.0,Thanks got it.,318077.0
85462,351264.0,,300690.0
85462,350822.0,"In general, [a:b] will give you elements starting from position 'a' till position ' b-1'. So in this case: DA_languages = ['R','Python', 'SAS', 'Scala', 42] print(DA_languages[ : -3]) You'll get elements starting from position 0 till position '-3-1' i.e -4(or 1st index here). So you'll get : ['R', 'Python'] Hope this helps :)",318495.0
85491,350907.0,"Do not hard code input string. Since you are hardcoding the input, Testcase 2 is failing. A sample solution is :- # Reading the input as a string; ignore the following two lines import ast, sys input_str = sys.stdin.read() # Write your code here final_str = input_str.lstrip() #Type your answer here print(final_str)",312746.0
85491,350933.0,It can't fail.. Since i already tried it. plz look below screen shot -,312746.0
85491,351373.0,As mentioned by alok you are hardcoding the input. The different testcases will have different input strings(that is what makes them different!) So instead of hardcoding..fetch the input using input() function. Eg. input_str=input() After that apply the split function and voila you'll get your answer! Hope that helps :),318495.0
85491,351445.0,In the code section the value of inp_str is already being input.So do not hard code. Just add the code to trim the string in the section mentioned and all the test case would pass.,317460.0
85491,351907.0,it will return value as None .so run input_str.lstrip() above then print input_str ...it will work,319869.0
93767,395185.0,"it's the difference between two columns but not the dataframes themselves. Since, column is a series your object and can be converted to set easily, that's the best option I would say.",318329.0
93767,396188.0,"Yes, you can using is in",317811.0
84127,346073.0,"You should remove leading space only, do not remove trailing space. use lstrip() function instead of strip() function. e.g input_str="" This is my first code"" input_str.lstrip() this will work. Please note - when you use verify, it only checks for given input But when you submit it also checks some random inputs. Random input can contain trailing space. e.g. input_str="" This is my first code "" and your code is removing trailing spaces as well that is why it is failing.",312746.0
84127,346151.0,Please do not hardcode your input. You might be hardcoding your input string. Please find the sample solution below:-,312746.0
84127,349409.0,strip() also works. I think you are missing to share additional information on what exactly you did. Can you share more information on what is sample test #2. Where did you get that from?,318084.0
84127,349721.0,"Hi Premnath, Sample Test #2 can be seen only after submitting. While verifying or running the code, we get to see only the Sample #1. It was my mistake of hardcosing the output in the ""Print"" statement. It is all fine now. Thanks for checking.",316202.0
85520,351078.0,"I also faced same problem with some questions, But after refreshing the page i was able to RUN , Verify and Submit the code without any further issue.",320073.0
85520,351113.0,"I have faced the same issue, I used to shuffle between the Tabs - from Code to Input and back to Code page. Later, used to validate the code by clicking on Verify (Or) Run Code buttons.",311502.0
85520,351813.0,may be you are not clicking on click here button on Input tab to enter Input value to your code,319869.0
85520,351879.0,I also face this issue many times but refreshing the tab always works for me....just check is it working for you,317980.0
85520,368381.0,The answers suggested above are not working for me.,312747.0
80210,324913.0,"Can you share the screenshot of the code, I think the problem is with code you are using .",305838.0
80210,325166.0,"input_dict = {'Jack Dorsey': 'Twitter', 'Tim Cook': 'Apple','Jeff Bezos': 'Amazon','Mukesh Ambani': 'RJIO'} T = input_dict['Tim Cook'] print(T)",305790.0
80210,325229.0,"Vipan, The question you have asked and the solution code you have mentioned doesn't matches. 1. The solution code you have mentioned is perfectly fine and it is related to another question i.e. to extract the name of the company headed by Tim Cook 2. The question or the issue which you are encountering is related to appending 'Python' word at the end of the tuple. It would help if you can cross check your solution for this question - list1 = list(input_tuple) list1.append('Python') tuple_2 = tuple(list1) print(tuple_2) As tuples are immutable, tuple needs to converted to list first, then append 'Python' and then convert them back to tuple to print it.",304694.0
80210,325448.0,vipan as you mention you are new to coding for more clarity on tuple to list conversion you can go through the link below .Hope it will help https://stackoverflow.com/questions/16296643/convert-tuple-to-list-and-back,305838.0
80210,325963.0,"Vipan, Whenever our submittion doen't match with the test case expected result, we get status as failed. You may need to work on sulution that will match with the result test case.",306011.0
85685,351940.0,"Hello Nishan, The code you have mentioned is executing and It is working. But in the Question they want to check it with Indexing only. So It is showing Error",306008.0
85685,351724.0,"You have to first split the input_str using split() function. arr=input_str.split(“ “) This will give you array looks like [kumar,ravi,003] Just read little bit about array and accessing data from array using index. first_name=arr[1] second_name=arr[0] customer_code=arr[2] After that use print() to print all values Please let me know if you want me to paste here full code. Happy learning.",317845.0
85685,351821.0,"A few guideline on writing code 1. Do not hardcode the input in your code. The logic to get the input is already coded in the stub and no need to change this. In this case, the below lines get the input string import ast,sys input_str = sys.stdin.read() 2. Write code which will cater to all types of use cases. Here the problem statement can be generalised to split the input string based on underscore ""_"" character. Hence, using the method split() on ""_"" character will work. For refer below for more information on split() method https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split",313826.0
85685,351880.0,"For most of our preparatory tutorials it is always recommended to use input from stdin using input_str, hardcoding the input will not work for code submission.",317980.0
85685,351999.0,"The question here assumes your code to be generalized for any such input: LASTNAME_FIRSTNAME_CODE Now when we actually understood the question, lets solve this by splitting the string (using '_' in this case): parts = input_str.split('_') parts variable now becomes a list. The structure of the list will be: ['LASTNAME', 'FIRSTNAME', 'CODE'] Now, we can access the elements of this list. Thus, you can use this to get FIRSTNAME: first_name = parts[1] # 1 being index for FIRSTNAME in list and similarly for LASTNAME and CODE.",317987.0
85685,352016.0,Bro you have hardcoded the solution. You are expected to use split function. 2nd testcase is using middle name as Revi(PFB) But your code will always print Ravi as first name which will be different from expected output and hence your code will work for only the input ('Kumar_Ravi_003') and fail for other test cases. You can simply use the split fn along with input_str. Split('_') to prevent hardcoding! Hope that helps.,318495.0
85685,351681.0,"Hi, looks you are hard coding the values of first_name ,second_name and customer_code . This is not correct approch. you have to use split() method to split the input_str and then assign the indivisual variable using [] . Please apply this approch. I don't want to give you all the code right now. Please work on this and if you want i can give you the solution code.",320073.0
85685,352039.0,Thank to all for the valuable response.,320195.0
85685,352567.0,"Hey Nishan! You need to use the given input directly and not give the input yourself by hardcoding. So far you have learnt the slicing method. Try implementing that. Hint: If I want to print py from 'python' I would use, in my code [0:2].",319721.0
86505,356518.0,"Hello Shrikant, Try the below link: https://stackoverflow.com/questions/41034866/running-jupyter-via-command-line-on-windows",312259.0
86505,357814.0,just run from cmd: jupyter notebook It will open jupyter notebook on browser.,318319.0
86505,364835.0,Hi...Got to know.It should be opened with jupyter,308638.0
86505,357868.0,Just clear the cookies from browser and try it again with same browser and different browser.It should open.,320195.0
86505,364834.0,I am not able download using Jupyter notebook provided in module 2. How to go about it,308638.0
86505,358383.0,"You can start jupyter from anaconda navigator, if you have anaconda already installed on your machine...worked for me",318332.0
85586,351123.0,symmetric_difference,313826.0
85586,351130.0,you can use below method: set_A.symmetric_difference(set_B),320073.0
85586,351374.0,The symmetric difference of two sets A and B is the set of elements which are in either of the sets A or B but not in both. The symmetric_difference() returns a new set which is the symmetric difference of sets Aand B. Syntax: A.symmetric_difference(B) The symmetric_difference takes a single argument (set) Hope it helps :),318495.0
85586,351774.0,"Symmetric Difference: Considering the two sets 'A' and 'B'. If we need to find the elements that are present either in 'A' or in 'B' but not in both 'A' &amp; 'B'. Symmetric Difference = (A ∪ B) - (A ∩ B) Python Syntax for same equation: a = {1,2,3} b = {3,4,5} result = a. symmetric_difference (b) R Implementation (Someone please confirm if I am correct): symdiff &lt;- function( x, y) { setdiff( union(x, y), intersect(x, y))} symdiff(1:3, 3:5)",317987.0
85613,351367.0,"ast.literal_eval() is used to check whether the datastructure passed to it belongs to one of the python datastructues like strings,numbers, lists, dictionary, set etc. For more details: https://docs.python.org/3/library/ast.html?highlight=literal_eval#ast.literal_eval In the coding console problems, the initial coding stub has been written in such a way as to receive the input using the sys.stdin.read() method, then followed by the ast.literal_eval() method to validate that the input provide in the stdin through ""Input"" tab belongs to one of the python datasturctures. I have played around with the coding console to pass a non-python datastructure. Attached images below.",313826.0
87365,360322.0,"In one of the videos, the professor has discussed about the lstrip() , rstrip() and strip() functions that can be used on string elements to remove spaces from specific locations of the string(i.e., leading, trailing or both). Would suggest you to analyse which one of these functions is suitable in the given problem statement. PS : In the coding console, you need not write any code to accept the input string. This has already been taken care of. We just need to write in the logic pertaining to the problem statement. This is true for all the coding console questions related to python.",313826.0
87365,360369.0,"Please watch the second video in page ""The Basics"" in Python module from 6':15"" to end to answer your query.",319898.0
87365,361582.0,input_str = ' This is my first code' print(input_str) input_str=input_str.lstrip() print(input_str),318332.0
87365,363999.0,"import ast, sys input_str = sys.stdin.read() final_str = input_str.lstrip() print(final_str)",311004.0
90443,377189.0,"Here are the steps to open the iPython (i.e. ipynb) notebook file: Launch Jupyter Notebook application Once started, your browser would be redirected to ""http://localhost:8888~"" Browse towards the location where you have saved this file Select the file and your notebook would be opened in a new broswe tab Hope that answers your question.",318085.0
90443,377191.0,Thank you.,311046.0
87357,360788.0,"Hi, I've used the same way as Vinay has used but a small correction finally before converting set to a list. Instead of using answer_1 = sorted(list(set_1.difference(set_2))) answer_2 = sorted(list(set_1.symmetric_difference(set_2))) I've used below. I haven't used the key word list before conversion, Sorted() function itself has converted set to list. I think using less functions is always optimal. But i want to listen from others about it. answer_1 = sorted(set_1.difference(set_2)) answer_2 = sorted(set_1.symmetric_difference(set_2)) print(type(answer_1)) print(answer_1) print(answer_2) Result is {1, 2, 3, 4, 5, 6} {2, 3, 4, 5, 6, 7, 8, 9} &lt;class 'list'&gt; [1] [1, 7, 8, 9]",318328.0
87357,360318.0,"The problem statement asks us to convert the lists into sets, find the difference and symmetric difference of the sets and print the sorted list of the answer. Below is my take on the solution. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1)#Type your answer here set_2 = set(list_2)#Type your answer here answer_1 = sorted(list(set_1.difference(set_2)))#Type your answer here answer_2 = sorted(list(set_1.symmetric_difference(set_2)))#Type your answer here print(answer_1) print(answer_2) Please note that ""set()"" is an unordered collection with unique elements. Hence, when a list is converted to a set using the ""set()"" constructor, the result is a collection of unique elements but the order may vary as compared to the order of the elements in list and hence the need to convert the answer to a sorted list.",313826.0
87376,360816.0,"Hi, Whats the error message?",318328.0
87376,361461.0,Can you post the error message? Are you using Anaconda distribution? More information about Jupyter installation. you might find it helpful. You can install the Jupyter in two ways: 1. using Anaconda- Install Anaconda its distribution of python. once you install Anaconda all the python related tools will be installed. you can easily install the additional tools/package 2. Using python pip- if you are familiar with Python then you can use python package manager PIP. Below is the links. http://jupyter.org/install,317845.0
85634,351391.0,"Hi Abhinay, sequence number shown is the number of times your code is executed. Hope this helps.",303227.0
85634,352002.0,"The number signifies: The total number of times this code block have executed, in that Jupyter Notebook.",317987.0
85634,352019.0,"It's awesome that you noticed this sequence number which many of us ignored just like that. On doing research for this I found out that this is this sequence number is not any random number it signifies the total number of times this code block have been executed. Further to add, If what you want to remove the numbers themselves, so that each cell shows In [ ] (instead of something like In [247] which is leftover from some previous incarnation of the kernel), use ""Cell"" &gt; ""All Output"" &gt; ""Clear"" (in Jupyter Notebook 5.4.0) or ""Edit"" &gt; ""Clear All Outputs"" (In Jupyter Lab 0.32.1). This will remove all the numbers, even if you're in the middle of running a notebook. It will not reset the numbering back to 1; e.g. if the last cell you executed was 18, the next will be 19. If you're using this because you want clarity about which cells you've executed during this run of the kernel and which cells you haven't executed yet, use ""Cell"" &gt; ""All Output"" &gt; ""Clear"" (or ""Edit"" &gt; ""Clear All Outputs"") immediately after you start (or restart) the kernel. This can be useful when restarting a kernel, or when opening a saved or duplicated notebook. This will also remove all outputs from the notebook. Hope this helps :)",318495.0
88840,370491.0,"You have mentioned spaces inside .split("" &amp; ""). Correct Solution is: input_str = 'I love Data Science &amp; Python' output_list = input_str.split( ""&amp;"" ) print(output_list)",317811.0
88840,368119.0,"split() function: The split() function breaks the string at the separator and returns a list of strings. The split() method takes maximum of 2 parameters: Separator - The is a delimiter. The string splits at the specified separator. If the separator is not specified, any white-space (space, newline ) string is a separator. Max split - The max split defines the maximum number of splits. The default value of max split is -1, meaning, no limit on the number of splits. More detail visit following link- https://www.tutorialspoint.com/python3/string_split.htm https://www.programiz.com/python-programming/methods/string/split",317845.0
88840,368117.0,"Please remove the leading and trailing spaces of the parameter within the split function. i.e. output_list = input_str.split( ""&amp;"" )",312259.0
88840,368115.0,"Little modification required in your code, Problem lies in below code output_list = input_str.split( "" &amp; "" ) You have added whitespaces in split() function before and after &amp;, just remove those spaces you will get desired output. Do let me know if it works or not.",317991.0
88840,368128.0,"Hello, Please remove the space in the argument to the split function. It should not be input_str.split( "" &amp; "" ) but it should be input_str.split( ""&amp;"" ) . There shouldn't be space before and after the &amp; . If you make this change you will get the expected output. Please upvote if this answer helps.",313691.0
88840,368134.0,can you remove the space provided to the &amp; in split function. that should fix it.,300685.0
88840,368172.0,"Remove the spaces in split("" &amp; "") and make it split(""&amp;"")",318495.0
85747,353045.0,Thanks Vinay for details:-),320008.0
85747,352383.0,"The number of submissions for any coding console problem varies and is mentioned at the bottom of the coding console. The ""Show Solution"" option might not be there to all the questions. If it does, it comes up after either successful submission or exhaustion of the number of submissions. Also, the non-sample test-case will be visible only after successful submission or exhaustion of the number of submissions.",313826.0
85747,370245.0,i cannot see the solution even after submitting it twice and exhausting the number of submission options. this is not helpful as i am not able to view the correct solution to the problem,310509.0
86064,352653.0,"Dictionary another type of data structure. It had two elements, keys and values, which is clearly explained in the lecture. Linked List is also a data structure but it very much different than a Dictionary. In a linked list, every element is linked to the next element with the help of address pointers. Refer to this for further clarification. https://www.geeksforgeeks.org/linked-list-set-1-introduction/",319721.0
86064,352938.0,"No. Python dictionaries are more similar to hash tables. In fact, Python dicts are implemented as hash tables. This answer delves into the nooks and crannies of the implementation. https://stackoverflow.com/questions/327311/how-are-pythons-built-in-dictionaries-implemented And to answer your question, a Python dict differs from a linked list in (pretty much) the same way a hash table differs from a linked list.",306733.0
93773,395253.0,"you can use isinstance function. isinstance(objectName, pd.core.frame.Series) &gt;&gt; returns True if it is series isinstance(objectName, pd.core.frame.DataFrame) &gt;&gt; returns True if it is DataFrame. Alternatively - type(objectName) returns the type. eg . &gt;&gt; type(""test"") str",318458.0
93773,395255.0,Type( ),312758.0
93773,395258.0,you can use type() and pass the argument on which you want to test the results.. it will return if dataframe or series..,316349.0
93773,395283.0,type(object_name). This will return the type of the object.,317689.0
93773,395277.0,"You can use isinstance () isinstance (var, pd.DataFrame) should return true if var is data frame you can read more here https://codeyarns.com/2010/01/28/python-checking-type-of-variable/",317845.0
88903,369663.0,Still my code is getting rejected even it is showing pass by verify and runcode.,320195.0
88903,368528.0,"You might not have used the ""print"" statement..Please check if this helps",318358.0
88903,368494.0,"Hi Anamika, If your test case is getting failed then your final submission will get reject.Do not hardcoded the solution. It should pass both Test case as well as code which you are running then only your final submission will pass. Expected output is not coming when input user gives any input while running code. Due to that only your test case is getting failed.",320195.0
88903,368513.0,,310611.0
88903,368548.0,Is this one of the questions from the additional optional python module. Please put the problem statement to understand what is expected.,313826.0
88903,369651.0,"Hello Vinay, My logic is also not working here.",320195.0
88903,369667.0,"There are a few things that need to be changed in the above code. Variables a and b are accepting the input. Need to pass these to the range function I see multiple if conditions which need to be changed to a casacading if-elif-else statement In the first if statement, need to use the logical operator ""and"" instead of the bit-wise operator ""&amp;"". Try this out: a = int(input()) b = int(input()) for n in range(a,b+1): if n % 3 == 0 and n % 5 == 0: print(""FizzBuzz"") elif n % 3 == 0: print(""Fizz"") elif n % 5 == 0: print(""Buzz"") else: print(n)",313826.0
86063,352639.0,"Got the Answer: Var=""I love Python programming"" print(Var[-11:])",310617.0
86063,352863.0,You can alse get the same output with the below ranges. print(Var[14:25]) or print(Var[14:]),314730.0
86063,354075.0,"In general, [a:b] will give you elements starting from position 'a' till position 'b-1'. For var[-11:-1] you'll get elements from position -11 until element '-1-1' i.e -2 ( or n) So this it the reason you'll get "" Programmin "" as output instead of "" Programming "" To get the expected output you can use any no. which is greater than the length of total string(24 here) i.e var [-11: 25] or var[-11:26] or so on..also you can leave it 2nd value as empty i.e var[-11:] For more clarification refer my answer to this question: https://learn.upgrad.com/v/course/208/question/85462 Hope this helps :)",318495.0
90370,376801.0,"When you click on launch button, python opens on a host server via internet explorer.Wait for sometime you will see it.",300688.0
90370,377582.0,Same Here i am also havig the same issue ..,312822.0
90370,388628.0,Hey hello My group mate,312357.0
90370,377847.0,"https://stackoverflow.com/questions/41520282/cant-open-jupyter-notebook-with-anaconda can you please try the steps mentioned in the link above. There, you are advised to upgrade or uninstall and reinstall anaconda.",319721.0
89760,,nan,
86159,353508.0,"In the previous versions of python, there was no guarentee on the order of the ""key:value"" pair elements being stored in the dictionary and hence in the video we see that the insertion order is not maintained. However, I think starting verion 3.6, the insertion order is maintained.",313826.0
86159,354081.0,"From the Python Docs (They will be your friend while using Python) It is best to think of a dictionary as an unordered set of key: value pairs, with the requirement that the keys are unique (within one dictionary). which means that they are stored in no fixed order and hence printed out in no fixed order! Since, the dictionaries are likely implemented using hash tables, there is no 'order' that they follow. If you want an ordered dictionary, collections.OrderedDict() might be useful.",318495.0
84087,345975.0,"It's not mandatory...however recommended. Most of the hands-on is taught using Jupyter Notebook which comes along with anaconda. Anaconda also comes with the necessary packages like numpy, scipy etc. which will be useful later.",313826.0
84087,346071.0,I think Anaconda will be used throughout the course duration so better to install so that you can practice in parallel. But It's not mandatory PIP will work.,315028.0
84087,346011.0,"It's not mandatory but anacode provide more utilities than pip. Quoting from the Conda blog : Having been involved in the python world for so long, we are all aware of pip, easy_install, and virtualenv, but these tools did not meet all of our specific requirements. The main problem is that they are focused around Python, neglecting non-Python library dependencies, such as HDF5, MKL, LLVM, etc., which do not have a setup.py in their source code and also do not install files into Python’s site-packages directory. So Conda is a packaging tool and installer that aims to do more than what pip does; handle library dependencies outside of the Python packages as well as the Python packages themselves. Conda also creates a virtual environment, like virtualenv does.",312746.0
86155,353407.0,Explanation Line 2 &amp; 3 : System Provide(upgrade console) input and readed and stored in input_list as list. Explanation Line 4 &amp; 5 : We stored the first and 2nd element of list from the input_list independently. Explanation Line 7 &amp; 8 : converting the list[0] in set and stored in set_1 as set same for list[1] and set_2 Explanation 9 : Now we use the functionality of fifference of set 1 1nd set 2 Explanation 10 :Now we use the functionality of symetric difference of set 1 1nd set 2 Explanation 11 and Explanation 12 : Print your Answer. Hope this helps,307843.0
86155,353479.0,"Please note that ""set()"" is an unordered collection with unique elements. Hence, when a list is converted to a set using the ""set()"" constructor, the result is a collection of unique elements but the order may vary as compared to the order of the elements in list. https://docs.python.org/3/tutorial/datastructures.html#sets Here, the problem statement explicitly asks for converting the results to list , then sort and then print. Please refer below code: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1)#Type your answer here set_2 = set(list_2)#Type your answer here answer_1 = sorted(list(set_1.difference(set_2)))#Type your answer here answer_2 = sorted(list(set_1.symmetric_difference(set_2)))#Type your answer here print(answer_1) print(answer_2)",313826.0
86155,355598.0,"It is taking time to execute code (Hang on! This seems to be taking time...) and then I am getting error ""Sorry!Please try again"" while runing above code.",320103.0
88951,368741.0,"Followed the following answer on stackoverflow : Now the notebooks are not getting autosaved. https://stackoverflow.com/a/45980165 Navigated to C:\Users\&lt;user-name&gt;\.jupyter\ . Replace &lt;user-name&gt; with your user name. Also, the path may be a little different depending on the way you have installed it. Created a new folder custom Created a new file custom.js and saved the following line inside the file Jupyter.notebook.set_autosave_interval(0); // disable autosave After this I restarted the jupyter notebooks and opened a new notebook. Now the notebooks are not getting autosaved.",313826.0
88951,368804.0,For people running macOs here are the steps: 1. Check the path where jupyter is installed # jupyter --path config: /Users/subarna/.jupyter 2. Create a folder 'custom' and a file custom.js inside that /Users/subarna/.jupyter/custom/custom.js 3. Edit the file custom.js and add the line Jupyter.notebook.set_autosave_interval(0); // disable autosave 4.Save the file 5.Restart jupyter.,309451.0
86168,353764.0,I'm using it regularly. Don't find any issue of that sort. Check if your laptop is slow.,310974.0
86168,354184.0,Try mini-conda. It's a lightweight implementation of the Anaconda stack. You get all the modules. Install that and link it to your Jupyter notebook installation. Check the Anaconda installation guide for the complete procedure.,306733.0
86168,354614.0,You can also use google colabs as an alternative to jupyter notebook and you can easily sink it with your google drive account and can also commit your work in github.,320685.0
86168,354112.0,"As far as i've used Jupyter it sometime lags but mostly its fine. But if you really want to try something else then, Try this online IDE(Jdoodle): https://www.jdoodle.com/python3-programming-online Or you can use Eclipse software as well. Hope this helps :)",318495.0
86168,358184.0,you can use Spyder and run your code from it's terminal. you have to use F9 to run a snippet of code.,318780.0
86168,368892.0,"If you are comfortable, go ahead and install python directly, use it with eclipse. You will need to install pydev plugin from eclipse marketplace. You can also use IDLE instead.",313228.0
88957,368789.0,"Your internet connection is very slow or else it is not connected as per the error. Please follow the below step. Clear the catche from your browser first. Try to reload your router and laptop. Check other sites if it is woking fine then it should open. If it doesn’t help then you can download video in your mobile and laptop. Install internet download manager on your computer. Start your course, from there you can download individual videos by using IDM and watch later. Hope this helps.",320195.0
88957,368832.0,Net Speed is 30 Mbps... Also other sites are working properly . I have tested these things... Cleared cache.. Changed browser as well . No Luck !!,308966.0
88957,368958.0,"There are some steps that are mentioned on the Help section. Checkout the below link: http://help.upgrad.com/learning-platform/video-playback-issues As the help guide says, if you are not able to resolve the issue even after trying all the steps then you may report the issue through ""Report a mistake"" link.",313826.0
86223,356124.0,Thank you for sharing this with everyone.,319721.0
85373,350336.0,Please do not hard code. Use below code for your reference -,312746.0
85373,350545.0,"There are many ways to resolve this alok has provied one method alredy. However below is the explanation. let say the string position as below Kumar_Ravi_003 k(0),u(1),m(2),a(3),r(4) Kumar position string _(5),R(6),a(7),v(8),i(9) _Ravi position sting _(10),0(11),0(12),3(13) _003 Position String import ast,sys #Here input_str is String and it is reading the console input input_str = sys.stdin.read() # input provided as 'Kumar_Ravi_003' don't store it separetely in any varible. #first_name is Ravi and above shown Ravi position starts from R i.e 6 and ends at 9, so select [6:9) first_name =input_str[6:10] #write your answer here #Second name kumar starts with k from string position 0 ands ends at 4 so select [0:5] second_name =input_str[0:5] #write your answer here #Custome code 003 starts with letter 0 i.e from 3rd string position ands ends at 14 so select [11:15] customer_code =input_str[11:15] #write your answer here print(first_name) print(second_name) print(customer_code)",307843.0
85373,352028.0,Dude you have hardcoded the solution. You are expected to use split function. 2nd testcase is using middle name as Revi(PFB) But your code will always print Ravi as first name which will be different from expected output and hence your code will work for only the input ('Kumar_Ravi_003') and fail for other test cases. You can simply use the split fn along with input_str. Split('_') to prevent hardcoding! Hope that helps.,318495.0
85373,351147.0,"I just have to comment on both the answers: 1. For answer that Amit Singh provided, the code will fail the testcases for which the first_name does not start with index 6 and for that matter exceeds the length of 4 which is applicable for second_name and customer_code. 2. For answer provided by Alok Singh, the method is correct however I feel that instead of writing input_str.split('_') thrice just save it in another list like split_string = input_str.split('_'). and retrieve the values using index on this variable.",305845.0
88961,368739.0,"what exactly do you mean by printing a return statement? if you could elaborate. if you mean to say, that is it possible to print something after a return statement in a function, then no i dont think it is possible.",317998.0
88961,368740.0,"Hello Siddhant, Well return and print are entirely two different processes. Whereas print will display information to the user or through the console. The print() function in Python is a function that outputs to your console window whatever you want to print out. Return is used for collecting data from a method that fulfills a certain purpose (to use later on throughout your program).The return statement terminates the execution of a function and returns control to the calling function.",320195.0
88961,368757.0,"At the end of the function you can have a return statement, which will return a value. That value can be printed. For example, in the below code, the return value of the function def is printed in the last line def sqr(n): a = n*n return a print(sqr(3)) Hope this clarifies your question.",318084.0
88961,368901.0,"What I am asuming is you are trying to return and print any variable ? YES!! It is possible.. Well you can definitely return values from function and print alsoyou can work on those values further. Let me explain this with an example: a= 4 b= 5 c=0 def sum(a,b,c): c = a+b return c print (sum(a,b,c)) Output : 9 In above piece of code i have created a sum function and passed 3 arguments. I have given a &amp; b values and kept c null . what actually happened is when i called Sum() function , values of a &amp; b is being carried to function and function got proccessed and added values of a &amp; b .then it returned C in which sum is stored. I have used printed statement out side the function. So basiacally value of C i.e 9 has been returned out of function and then being printed.",308966.0
88961,368761.0,"Before using the return statement, you can print whatever variable or string that you want to return as shown below: def funct(a): b = a+1 print(b) return b",318358.0
83864,345392.0,"Remove name and profession keys and just give 'Label' key and 'NA'. Hope it helps,",310585.0
83864,345456.0,"HI, As Naga correctly pointed out that get function can take 2 variables as input: First is the key for which value needs to be fetched and the second one is the default value which gets printed when key is not found in the dictionary. You can refer to below link: https://www.geeksforgeeks.org/get-method-dictionaries-python/",301559.0
83864,346411.0,"In this question I didnt put the comma(,) only thats why I was getting the error. When should we use comma(,) and semicolon(:).",308639.0
86229,355795.0,"Yes, I faced this issue. I believe it could be an issue with connecting to the server.",314730.0
86229,356029.0,I faced this issue when i was submitting the code in my office network. Did not face the problem while on home's network.,305845.0
86229,356397.0,I am still facing this issue.Safari version 11.1,314197.0
86235,354374.0,"Ypur testcase is failing because you have hard-coded the input_str i.e you have given a permanent value to input_str, whereas the test case provides value to input_str dynamically. So you have to remove the hardcoding for input_str. This can be done using this: input_str=input() which was already done in the given problem stub. Types of test cases Let us get a quick understanding of the two types of test cases : Sample Test Case Executed at the time of “Verify” and “Submit.” You will be able to see the Input and Expected output at all times The primary purpose is to check how your code is performing against some test cases so that you make corrective changes and then submit the code. Non-Sample Test Case Executed only at the time of “Submit.” You will be able to see the Input and Expected output only once you have exhausted all the Submissions or have got the question completely correct. The primary purpose is to check the correctness of the solution code. PFB the details tab and non-hardcoded code! For more details on this coding platform : Upgrad Platform Hope this helps.",318495.0
84113,346042.0,"As of now- Multiline comments are not supported in R using /**/. But there are some short cuts in IDEs which can make your life easy- R Studio (and Eclipse + StatET): Highlight the text and use CTRL + SHIFT + C to comment multiple lines in Windows. Or, command + SHIFT + C in OS-X.",312746.0
86352,355614.0,"In General, It's not necessary to install Microsoft Visual Studio because Anaconde comes with Spyder IDE. Latest versions of Visual Studio IDE is enabled with Python. If you are well versed with Visual Studio earlier, then you can install it as well.",311502.0
86352,355629.0,Anaconda comes with an option to install visual studio code. It’s like notepad++ that supports multiple languages. You can ignore the visual studio code installation and use spyder IDE.,317845.0
90244,375966.0,"You can use input_list[2][0] to extract it. [2] - tells to go to the 3rd element of list ie['python','java'] [0]- tells to select the first element of the inner list.",309451.0
90244,375958.0,you can use; input_list[2][0] the first square bracket signifies 3rd row and second square bracket as 1st column.. the indexes for the same are 2 and 0.. indexing starts from 0.. and now print this input _list.. hope it helps..,316349.0
90244,376020.0,s = string[2][0] print(s) Hope this helps,317149.0
90244,375962.0,"Consider this list as follows: 0th 1st column input_list = [ ['SAS' , 'R' ], 0th row ['Tableau' , 'SQL' ], 1st row ['Python' , 'Java' ] ] 2nd row Imagine it as [2X2] matrix To extract 'Python' input_list = [['SAS','R'],['Tableau','SQL'],['Python','Java']] print(input_list[2][0]) Hope this will help.",317991.0
86342,355370.0,Try to open it using command prompt type - jupyter notebook and it would open up automatically without prompting for any authentication. Incase if it still exists as a workaround you can copy paste the token that appears in the cmd prompt n that should solve the issue.,312259.0
86342,357023.0,"down vote You can remove the password completely with: jupyter notebook --ip='*' --NotebookApp.token='' --NotebookApp.password='' Without --NotebookApp.password='' , when connecting from a remote computer to the local sever with: jupyter notebook --ip='*' it still asks for a password, and that is a good security default which can be irritating at times. Tested on Jupyter 4.4.x. Also, check, http://jupyter-notebook.readthedocs.io/en/latest/security.html#server-security for more info, including disabling the feature.",319721.0
86342,359416.0,Thank you SB and Denney Joy for your suggestions. It worked with copy pasting the token into the password field which was generated through command prompt.,312623.0
86279,354609.0,The split() function should work without any additional imports. Would be helpful if you could post the error you are getting.,313826.0
86279,355268.0,It should work without any issue. split() function doesn't require any import,317845.0
86355,355635.0,Please replace the below lines - answer_1 = sorted(set_1.difference(set_2)) answer_2 = sorted(set_1.symmetric_difference(set_2)) with answer_1 = set_1.difference(set_2) answer_2 = set_1.symmetric_difference(set_2) and check again.,311502.0
86355,355928.0,I tried again now and its working fine. Thanks all of you for your help.,320103.0
86355,355643.0,"The code seems to be fine. Maybe there is some issue with the coding console. Even I am not able to execute it on the coding console, but it executes successfully on jupyter notebook. Maybe give it a try sometime later.",313826.0
86355,358388.0,"Its not a code issue, its a network issue. Console is unable to connect to the server because of the network connectivity. I also faced this issue..I changed the network and its working fine now",318332.0
89976,374661.0,You Can use PyCharm As well!,301648.0
89976,374844.0,normally PyCharm IDE is used in IT industry where we have to work on multiple files.,318429.0
89976,375050.0,You can use any IDE which you are comfortable with.,317689.0
89969,374652.0,np.isnan works on float or int. Your age column must be a string object hence it is throwing this error. Try converting it to float or int or use pd.isnull instead.,317689.0
89969,374663.0,np.isnan can be applied to NumPy arrays of native dtype (such as np.float64). So here you need to typecast if your column datatype is not in NumPy native type or you can use pd .isnull before np.isnan.,301648.0
89969,374698.0,Currently age column is String. You can convet age value to int or float using below statement and then use isnan on the new column. df['age'] = df['age'].astype(float); Above statement will put NAN when value is not convertible to number(Float). df[np.isnan(df['age'])],318368.0
87194,359211.0,"Hi Ayush, First need to download 'punkt' package with nltk library. Then use tokenize.sent_tokenize(sentence) from nltk import tokenize p = 'Hello Mr. Scott, today I am not well. So kindly inform Mr. Peter that I wont be able to come today.' tokenize.sent_tokenize(p) ['Hello Mr. Scott, today I am not well.', 'So kindly inform Mr. Peter that I wont be able to come today.'] It is running with ""wont"" but not with won't. Don't know how to include this. Hope I can learn regex down the course.",317418.0
85153,349008.0,firstly you have to use Split function. dont print like that as i can see your code.,305847.0
85153,349175.0,You can simply use the split fn along with input_str. Split('_').. Go through the snap. When you will go ahead in this module then u can solve same problem with loop and lambda fn to avoid shared code.,305847.0
85153,349183.0,it is since you've hard coded due to which hidden test cases are getting failed.,310585.0
85153,349394.0,"Your code should be able to process any string passed to it as the input in the prescribed format. Hence, hard-coding the values will result in an error.",313826.0
85153,349546.0,"Hi Hemant, What you have done is absolutely correct according to the sample test case that is shown to us while attempting the question. However, the most impotant point in any question we're solving is that it will be checked against some other inputs also that are called hidden test cases which would be shown to you only after you submit the solution successfully or you run out of attempts. So, the takeaway here is that you need to write the logic which would give you firstName, secondName and customerCode keeping in mind that all these three will be provided to you in a single string separated by '_'. All the best, Happy coding!",318355.0
85153,349553.0,Dude you have hardcoded the solution. You are expected to use split function. 2nd testcase is using middle name as Revi(PFB) But your code will always print Ravi as first name which will be different from expected output and hence your code will work for only the input ('Kumar_Ravi_003') and fail for other test cases. You can simply use the split fn along with input_str. Split('_') to prevent hardcoding! Hope that helps.,318495.0
85153,350117.0,output_str=input_str.split('_') print(output_str[0]) would give 'kumar' print(output_str[1])would give 'ravi' print(output_str[2])would give '003',300690.0
87425,360997.0,"input_list[2:] would give you all the elements of the nested list with index 2 i.e., ['Python','Java']. To get just the first element, you have to provide the index number pertaining to this element i.e., index number 0 (zero) since indexing starts from zero in python. input_lis[2][0] would provide you 'Python'.",313826.0
87425,361003.0,thanks vinay,302743.0
87425,361579.0,"input_list = [['SAS','R'],['Tableau','SQL'],['Python','Java']] print(input_list[2][0])",318332.0
87429,361455.0,"we don't have the option to download the video and course in learn.upgrad.com However, you can use the mobile app to download the course content. Install UpGrad mobile app from AppStore Once you open the module and session you get download option on the top right corner of the screen. now you are good to go without an active internet connection. You cannot run/submit the code using the mobile app",317845.0
87429,361560.0,"Install internet download manager on your computer. Start your course, from there you can download individual videos by using IDM and watch later. Hope this helps.",318332.0
87429,364828.0,I have managed to download the videos from the IOS app. But there is no way to view them incase of low speed internet or watch it on Desktop for convenience. The videos are absolutely not understandable on mobile. Thats very difficult situation for me.,302734.0
87429,364820.0,There's no option to download.,304816.0
87428,361128.0,"Reduce function : Suppose you have data as list_num =[ 22,45,32,20,87,94,30] and you have function x ifx &gt;y else y Step 1 - Reduce function reads as val1 = fun(a1,a2) i.e for your case its 22&gt;45 (returns 45 i.y) Step 2 Value of Val2 = fun(val1,a3) i.e for your case its 45 &gt; 32 i.e for your case its 45 &gt; 32 (return 45 here x) Similiary .....it is process step3 till it ends. Hope it clear.",307843.0
87428,361203.0,"Syntax: reduce( function , iterable ) Here, function should always take only 2 arguments and return a single value. iterable is any sequence type object like list, tuple etc. The way reduce() works is, that it takes the first 2 elements of the sequences and passes it to the function. The result obtained from the function is then made the first argument , the next element in the sequence is made the second argument and the function is called again. This goes on till we reach end of the sequence. Please read the python documentation for more details(link provided below) https://docs.python.org/3/library/functools.html?highlight=reduce#functools.reduce If either the function or the iterable does not conform to the conditions laid out by the reduce() function , then we get an error. Hope this answers your query.",313826.0
87428,363916.0,"Here's what I understand from your question: 1. You want to know how reduce() works . @VinayDharwadkar's answer tells you this. 2. You want to know what happens when there are multiple lists and you want to apply the same reduction to all the lists. To use the same lambda for multiple lists, create a list of lists and use a for loop to iterate over them. Then pass each list to the reduce() as the second argument. If this is what you are looking for, then here's the logic. I'm going to leave the implementation for you to figure out.",306733.0
89103,371067.0,That's great!,319721.0
89107,369411.0,"Hey vinod, Firstly I want to tell you in python intersection denotes as an operator ""&amp;"" and union denotes ""|"".so don't confuse between them they are same. I hope you understand..!!",305847.0
89107,369412.0,No. They are just 2 ways of representing set operations. Listed below are the different representations of some set operations: a-b a.difference(b) ------------------ a|b a.union(b) ------------------ a&amp;b a.intersection(b) ------------------ a&lt;=b a.issubset(b) ------------------ a^b a.symmetric_difference(b) Hope this helps.,310511.0
89107,369421.0,"They are same only 2 differents ways of representation. please refer the below may be it would help This is how you perform the well-known operations on sets in Python: A | B A.union(B) Returns a set which is the union of sets A and B . A |= B A.update(B) Adds all elements of array B to the set A . A &amp; B A.intersection(B) Returns a set which is the intersection of sets A and B . A &amp;= B A.intersection_update(B) Leaves in the set A only items that belong to the set B . A - B A.difference(B) Returns the set difference of A and B (the elements included in A , but not included in B ). A -= B A.difference_update(B) Removes all elements of B from the set A . A ^ B A.symmetric_difference(B) Returns the symmetric difference of sets A and B (the elements belonging to either A or B , but not to both sets simultaneously). A ^= B A.symmetric_difference_update(B) Writes in A the symmetric difference of sets A and B . A &lt;= B A.issubset(B) Returns true if A is a subset of B . A &gt;= B A.issuperset(B) Returns true if B is a subset of A . A &lt; B Equivalent to A &lt;= B and A != B A &gt; B Equivalent to A &gt;= B and A != B",300687.0
89107,369433.0,"There are two different ways: 1. To use the union() function eg: A = {1, 2, 3, 4, 5} B = {4, 5, 6, 7, 8} print(A.union(B)) {1, 2, 3, 4, 5, 6, 7, 8} 2. To use the '|' operator eg: A = {1, 2, 3, 4, 5} B = {4, 5, 6, 7, 8} print(A|B) {1, 2, 3, 4, 5, 6, 7, 8} The difference between the two forms is that union() you can give any number of sets as parameters eg: set1.union(set2, set3, set4….) A = {1, 2, 3, 4, 5} B = {4, 5, 6, 7, 8} C = {2,5,7,9} print(A.union(B,C)) {1, 2, 3, 4, 5, 6, 7, 8, 9} Similarly for other set operations you can use both the operator or function | for union. &amp; for intersection. – for difference ^ for symmetric difference",309451.0
87281,359787.0,input_str.split('&amp;'),318495.0
87281,359836.0,"I guess @Koustav wants to replace ',' with '&amp;' If that is the case; use the replace operator as; split_string_1 = string_1.replace(',' , '&amp;') print(split_string_1) please let us know if this is not what you expected. I'm guessing it based on the expected output from your snapshot in the question.",316349.0
87281,359849.0,"Hey Koustav, You need to first split the string on ',' and then join on '&amp;' using join() function input_str = (sys.stdin.read()).split(',') string_1 = ' &amp; '.join(input_str) #Type your answer here print(string_1)",305845.0
87281,359863.0,"In this scenario actually, you have two task 1. break the input_str into the list 2. join the list into a string with '&amp;' let's break it using split() str_list=input_str.split(',') if you print(str_list) -&gt; you will get list Now let's join the list to a single string out_str=""&amp;"".join(str_list) if you print(out_str) -&gt; you will get a single string completed code: str_list=input_str.split(',') print(str_list) out_str=""&amp;"".join(str_list) print(out_str) I hope this helps.",317845.0
87281,359753.0,"It is very simply use the which word you want to split as per below , here it is '&amp;' so use below . Hope it helps",307843.0
87281,363919.0,"Or, you can use the replace() string method. It's a shortcut. Read the docs before you use it.",306733.0
87418,360957.0,"When you create a sequence type object in which the individual elements of the sequence are seperated by commas and often enclosed using the round brackets ( ) , you create an object of type tuple . https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences Tuples are immutable in nature, meaning the these objects cannot be altered . https://docs.python.org/3/glossary.html#term-immutable Since tuples are immutable, we cannot do operation list append or delete elements from it. Hence, you see the error. In case you wanted to create a list type of object (which are mutable in nature), then you should enclose the indiviual elements of the sequence in square brackets [ ] and then you would be able to do operations which modify the list elements like append, pop, etc. Please see below examples. Hope this helps.",313826.0
87418,360946.0,List should always be declared with square brackets []. when rounded brackets () are encountered Python understands it as a tuple. A tuple cannot be changed. It does not have an append (). declare listo as a list. Your problem would be solved,319898.0
87418,361557.0,http://net-informations.com/python/iq/tup.htm,318332.0
87418,361775.0,"In addition to comments by R Muraleedharan, you can do. one more thing to solve your problem. since tuples are immutable so first convert your tuple to the list. And try to append any strings, you should be able to append.",312623.0
87418,362752.0,List should be created in square brackets,311119.0
87493,361848.0,"In python, e ach object is identified by a ""constant-unique-random"" integer which is known as the ""identity"" of the object. The built-in function id(object) returns the ""identity"" of the object. https://docs.python.org/3/library/functions.html#id Please note that only in C implementation of Python(also known as CPython), the ""identity"" of the object is the memory address of the object, but there is no such guarentee in other implementations of Python ( like Jython, PyPy etc).",313826.0
87579,362908.0,Just think as below and use choice as per below. Stack : Assume it plate system in party where the plate kept over and over to another plate.so when you are hungry whenever you pull the plate it will be from the top. Stack is something which kept first but out in the last.(i.e First in Last Out) Queue : Assue you are standing in ticket counter of cimema hall where whoever enter first out first so Queue is something which in first and out first (i.e First in First Out) So now when you take element from list according to sequence is Queue and when you pick the elect from last to first is Stack.,307843.0
87579,363912.0,I've written a simple implementation of the queue and stack using python lists. You can find the code here: https://hastebin.com/bowizisere.rb Hope this is what you were looking for!,306733.0
87579,362997.0,"Stack: The stack is the list with LIFO(Last in first out). let's assume our list stacklist=[1,2,3,4,5] print(stacklist) #adding elemeent to stack like Push we have to add the element to last position stacklist.append(20) print(stacklist) # remove the element pop, the element should be removed that is in the last position so we will use -1 index item=stacklist[-1] print('pop item: '+str(item)) del stacklist[-1] #new stack List print(stacklist) Queue: It's the list with FIFO(first in first out) let's assume our list queqelist=[1,2,3,4,5,6] print(queqelist) # add element to quque, we have to add the element in the last postion using append additem=35 queqelist .append( additem ) print( queqelist ) # remove the item from the queue, here we have to remove the element from the first position so we will use 0 index del queqelist[0] print(queqelist) Complete code that you can run in Spyder: stacklist=[1,2,3,4,5] print(stacklist) #adding elemeent to stack like Push stacklist.append(20) print(stacklist) # remove the element pop item=stacklist[-1] print('pop item: '+str(item)) del stacklist[-1] #new stack List print(stacklist) queqelist=[1,2,3,4,5,6] print(queqelist) # add element to quque additem=35 queqelist.append(additem) print(queqelist) # remove item from queqe del queqelist[0] print(queqelist)",317845.0
87579,362974.0,"Stack : Works on the principle of LIFO(Last In First Out) . Meaning, whenever an element is pushed to the stack, it should go the the last position and whenever an element is pulled from the stack it should be fetched from the end of the stack. Python Implementation using append() and pop() methods: Queue : Works on the principle of FIFO(First In First Out). Meaning, whenever an element is pushed to the queue, it should go the the first position and whenever an element is pulled from the queue it should be fetched from the end of the queue. Python Implementation using insert() and pop() methods: Maybe these are not the most ideal ways to implement stack / queue and there may be some specalised packages for better implementation.",313826.0
89788,373084.0,"Use list comprehension as shown below list1 = [6,7,8,9,2,3,4,5] print([x**2 for x in l1])",313826.0
89788,373944.0,"Below mentioined code Using list comprehension is much easier and readable: list_1= [6,7,8,9,2,3,4,5] series= [x**2 for x in list_1] print (series)",307176.0
89788,373068.0,"Hi Gopichand, U can do it in many different ways:- l1= [6,7,8,9,2,3,4,5] print(list(map(lambda a:a*2,l1)))",317822.0
89788,373088.0,"Using lambda function and list comprehesion, this can be achived. Please find the sample below. list = [1,2,3,4] print(list(map(lambda a:a*2,list))) print([x**2 for x in list])",318368.0
89788,373178.0,"Hello Gopi, one more solution. series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x: x**2) print(series_1) print(series_2)",320195.0
89788,374860.0,"Hi, series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x: x**2) print(series_1) print(series_2)",308639.0
89788,375773.0,"series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x: x**2) print(series_1) print(series_2)",317811.0
89789,373085.0,"You need to concate the data frames and group by Country column. Once you groupby country column, you can use sum function on Medal column. df = pd.concat([gold, silver, bronze]) df[['Medals']] = df[['Medals']].astype(float) print(pd.DataFrame(df.groupby('Country').Medals.sum()).sort_values('Medals', ascending = False))",318368.0
89789,373083.0,"Hello Gopi, Set the index to 'Country' for all the 3 data frames using set_index method for data frames. Ex. gold.set_index('Country', inplace = True) Add the three dataframes. Ensure to use the fill_values argument to avoid the NaN values. Ex. total = gold.add(silver, fill_value = 0) Then sort the output data frame using sort_values method for data frames. Hope this helps",320195.0
87648,363800.0,"Hi Naveen, would be helpful if you could attach the code snippet that you are executing.",313826.0
87648,363866.0,Yes please.. code snippet,305845.0
87648,363898.0,"Your code probably has nested print statements inside a loop or something. When you nest print functions, ""None"" is returned as the output after printing the required data onto the stdout (which is usually the screen.) Check your code for that.",306733.0
87648,363909.0,"Thanks, Please find the code below used in excecution. import ast,sys input_str = kumar_ravi_003 first_name = print(input_str[6:10]) #write your answer here second_name = print(input_str[0:5]) #write your answer here customer_code = print(input_str[11:]) #write your answer here print(first_name) print(second_name) print(customer_code)",318347.0
87648,364023.0,"Answer : first_name = input_str[6:10]#write your answer here second_name = input_str[0:5]#write our answer here customer_code = input_str[11:] #write your answer here print(first_name) print(second_name) print(customer_code) Note : You have used print while assigning to variable which is not required , remove and try with above",318732.0
87648,364058.0,"The trouble is with the print when you extract the part of string and assign it to the variables. first_name = print(input_str[6:10]) -&gt; this will print Ravi second_name = print(input_str[0:5]) -&gt; this will print Kumar customer_code = print(input_str[11:]) -&gt; this will print 003 first_name,second_name and customer_code will be none after executing the above statements since print only prints the value and doesn't return anything. So when printing the values in these variables, it prints None. So the first 3 line with correct values you are seeing in the output is from: first_name = print(input_str[6:10]) second_name = print(input_str[0:5]) customer_code = print(input_str[11:]) The 3 None printed in the output are from : print(first_name) print(second_name) print(customer_code) Removing print when assigning value to first_name,second_name and customer_code should solve the issue.",301654.0
87648,364379.0,"Hi Naveen, Please try the following code.Do not write print while assigning to variable. import ast,sys input_str = sys.stdin.read() first_name =input_str[6:10] #write your answer here second_name =input_str[0:5] #write your answer here customer_code =input_str[11:15] #write your answer here print(first_name) print(second_name) print(customer_code)",320195.0
87648,364340.0,"Hi Naveen, Please try the following code snippet. No need to write print with the assignment. first_name = input_str[6:10] second_name = input_str[0:5] customer_code = input_str[11:] print(first_name) print(second_name) print(customer_code)",317418.0
87676,363959.0,"ast.literal_eval() is used to check whether the datastructure passed to it belongs to one of the python datastructues like strings,numbers, lists, dictionary, set etc. For more details: https://docs.python.org/3/library/ast.html?highlight=literal_eval#ast.literal_eval Check out the below link for a similar question: https://learn.upgrad.com/v/course/208/question/85613",313826.0
87676,364293.0,Here the above statement converts the above-given input to your console converting into the input_str as the input is not mentioned in your console.,319721.0
89816,373246.0,There are quite a few differences. Please check out the official link below: https://docs.python.org/3.0/whatsnew/3.0.html,313826.0
89816,373270.0,Hello Umesh Below link could explain difference better. https://www.geeksforgeeks.org/important-differences-between-python-2-x-and-python-3-x-with-examples/ Happy Learning !,308966.0
89816,373298.0,Nice article here. https://www.geeksforgeeks.org/important-differences-between-python-2-x-and-python-3-x-with-examples/,318368.0
89816,373309.0,"There are many differences but to summarize and explain we can commonly talk about below five most important: 1&gt; Python 3 has improved integer division. In Python 2, if you write a number without any digits after the decimal point, it rounds your calculation down to the nearest whole number. That is 5/2=2 due to rounding. To get the correct result you have to do 5.0/2.0=2.5. However, in Python 3, the expression 5 / 2 will return the expected result of 2.5 without having to worry about adding those extra zeroes. 2&gt; Both have different Print Syntaxes. In Python 2 it is print ""Hello"" . In Python 3 , it is print () function , print(""Hello"") 3&gt; Better Unicode support in Python 3 In Python 2, strings are stored as ASCII by default–you have to add a “u” if you want to store strings as Unicode in Python 2.x. Unicode is more versatile than ASCII. Unicode strings can store foreign language letters, Roman letters and numerals, symbols, emojis, etc., offering you more choices. 4&gt; Python 2 and Python 3 have sometimes incompatible libraries. Many of today's developers are creating libraries strictly for use with Python 3.Similarly, many older libraries built for Python 2 are not forwards-compatible. 5&gt; Python 2 is a Legacy but Python 3 is future Since Python 2 has been the most popular version for over a decade and a half, it is still used in the software at certain companies. However, since more companies are moving from Python 2 to 3 and it is estimated that python 3 will completely take over 2 by 2020. Instagram is already using Python3 and Facebook has started its switch. :)",311869.0
87743,364275.0,"id() basically returns the object's (a list in this case) identity/ address . it is a unique number, an interger, which remains so during the lifetime of the object. u can type help(id) in the prompt for getting basic info regarding id in Python",319898.0
87747,364280.0,"Hey, you will recieve all the solutions on the platform. We will publish the solutions once all the students submit it.",319721.0
89268,371095.0,That's great!,319721.0
89268,383795.0,This is very useful reference.,304026.0
86101,352949.0,"These are the ones that you get usually, are you seeing something different?",310974.0
86101,352959.0,,319898.0
86101,352984.0,"All the modern IDE supports AutoComplete, it totally depends on your IDE and supported language by your IDE. You might be seeing more suggestion because of more library added to your IDE. Try using Spyder and check if you are getting more suggestion like you have mentioned here.",317845.0
86101,354063.0,"As correctly mentioned by Amani, all the modern IDE supports AutoComplete. The words which you are seeing are the commonly used keywords stored in cache of the IDE. Now coming to the keyword and local, these are nothing but the scope of the word. Keyword : Any special/reserved word in a particular language. Identifier : Identifiers are the user defined names of variable, function and labels. Local variable : This is the variable which can be used only inside the current function which it is in.Or in simple words only the function where this variable is defined can only use this variable. Global variable: This variable can be used throughout the program by any function. So when you use any word randomly,it voilates the syntax of the language and hence you are getting error! Hope this clarifies what are these random words! :)",318495.0
86101,354220.0,"I have Question Here. How do we know the hidden test cases ...Here I tried simple code to lstrip the spaces. Testcase#1 is pased. Showing the Testcase#2 not passed. How to know,hidden Testcases. Please advice. input_str = ' This is my first code' print(input_str.lstrip())",318846.0
86101,354583.0,""" This to my first code"" is the Testcase_2. I have to add I tried submitting with your code ""print(input_str.lstrip())"" and had a succesfull result.",319898.0
86101,354617.0,"@Rajani for your second query regarding lstri(): Your testcase is failing because you have hard-coded the input_str i.e you have given a permanent value to input_str, whereas the test case provides value to input_str dynamically. So you have to remove the hardcoding for input_str. This can be done using this: input_str=input() which was already done in the given problem stub. Types of test cases Let us get a quick understanding of the two types of test cases : Sample Test Case Executed at the time of “Verify” and “Submit.” You will be able to see the Input and Expected output at all times The primary purpose is to check how your code is performing against some test cases so that you make corrective changes and then submit the code. Non-Sample Test Case Executed only at the time of “Submit.” You will be able to see the Input and Expected output only once you have exhausted all the Submissions or have got the question completely correct. The primary purpose is to check the correctness of the solution code. PFB the details tab and non-hardcoded code! For more details on this coding platform : Upgrad Platform Hope this helps.",318495.0
88236,364963.0,Make sure the tuple/list is not empty. Print it before running your code.,310974.0
88236,365089.0,"The error is mentioning that the object "" list2 "" is an object of type "" NoneType "". I reckon you are trying to do something like: list2 = list2.append(""new_element"") or list2 = list1.append(""new_element"") Please note that the method append() just appends an element to an existing list and does not return the appended list back. In fact, the return type of the append() function is 'NoneType', meaning it returns nothing. In the above case, 'NoneType' gets assigned to the list2. The correct way to append an element to a list and then assign it to a new object is list1.append(x) list2 = list1 Related question: https://learn.upgrad.com/v/course/208/question/87100 This error could also occur if you assign the print() function to an object. Ex.: list2 = print(list1) or something similar. print() function also returns a ' NoneType ' object. Hope this helps. PS: Please include code snippet to the question for more clarity.",313826.0
88234,364946.0,I faced the same issue in Safari. It's working fine with Chrome. Cheers.,318004.0
88234,365522.0,I have the same issue on Chrome on a Windows laptop. Below is all I see:,315022.0
88234,365577.0,Not sure but could be something to do with the network restrictions. I faced the same issue when connected to my office network - using chrome windows laptop but works fine on my personal laptop - using chrome on mac.,319302.0
90799,378499.0,You could also do it in the following way tempRow = rowA.copy rowA = rowB rowB = tempRow,318329.0
90799,378409.0,"you need to specify the row numbers in 2darray and that should equals the order in which swap in rows expected; &lt;2darray&gt;[[x,y]] = &lt;2darray&gt;[[y,x]] hope it helps..",316349.0
89370,370698.0,"word[-12:-18:-1] this means you are telling Python to extract the letters in a reverse step of 1. Take this example: word=""ABCDEFGHIJKL"" word[-2:-10:-3] 'KHE' -2= K -10=C So start extracting the letters from K till C at a step/interval of 3 in a negative direction i.e from right to left.",309451.0
89370,370684.0,"Lets take an example: a = ""Python"" below is the indexing position for each of the letters [0] = P = [-6] [1] = y = [-5] [2] = t = [-4] [3] = h = [-3] [4] = o = [-2] [5] = n = [-1] Any position in an index can be referred to either by positive index number or a negative index number. when we slice a string using index number, we provide, starting index position and ending index position. starting index position should be smaller than the ending position. Going by that logic, [-18:-12] is the correct representation and [-12:-18] violates that rule. Hope this explanation helps.",318084.0
89370,370676.0,"Because when you are trying to word[-12:-18] basically you are trying to read the array/string in reverse. Which is not the dafualt case. Now when you are mentioning [-12:18:-1] then it is working cause you are instructing to read the string in reverse. Now the last index is -1. I am not sure the string on which you are doing this. But for example: &gt;&gt;&gt; ""Python""[-4:-6:-1] P y t h o n -6 -5 -4 -3 -2 -1 so it is -4 to -5 exclude -6. hence output will be ty",318554.0
89381,370746.0,What code you are using for this? for sample you can extract values from dictionary by dict.values() then capsulate this output in list and then sort the list.,320073.0
89381,370741.0,"non-zero exit code most probable cause - syntax error - runtime error (lesser probablity, since inputs are controlled by DoSelect console)",306248.0
89381,370745.0,"input_dict = {'Jack Dorsey' : 'Twitter' , 'Tim Cook' : 'Apple','Jeff Bezos' : 'Amazon' ,'Mukesh Ambani' : 'RJIO'} To get sorted list of keys: sorted(input_dict.keys()) or sorted(input_dict) To get sorted list of values: sorted(input_dict.values())",318554.0
89381,370752.0,"It is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionary is unordered and its elements are accesed via keys. Now if you wanna sort a dictionary you can still do that. Sort by keys: sorted(input_dict) Sort by value: its a bit tricky sorted(input_dict, key=input_dict.get) -&gt;it sorts the data structure on the key returned by input_dict.get. and it serves as the element of sort comparision. Hope that helps!!",309451.0
89381,370742.0,"Hi Vishal, The given question is to be done like this: value_list = list(input_dict.values()) print(sorted(value_list)) You can compare this with your code and see where the problem arises, or please share your code so that it can be debugged. Hope that helps.",317998.0
89397,370865.0,"On the Python console when you type a name/variable which exists, python use str() representation of the name. In that case it wont take care of you escape characters. If you are not using print it won't work as expected. But if you are using print and still getting unexpected results. Please provide the exact code you are trying.",318368.0
89397,370827.0,"Actually in both of the mentioned scenarios, \n is working. Can you provide the screen shot of the code what you are trying for.",318804.0
87404,360759.0,Thank Amani! I corrected myself and submitted the code. It worked this time!,300699.0
87404,360718.0,"it's because you have hardcoded the string to the variable. you should not remove the first two lines: import ast,sys input_str = sys.stdin.read() it's reading the data from the input and verifying with sample test case. when you run it will work fine but when it's running for other hidden testcase it's failing while submit . import ast,sys input_str = sys.stdin.read() #write your code array=input_str.split(""_""); first_name = array[1]; second_name =array[0]; customer_code =array[2]; print(first_name) print(second_name) print(customer_code)",317845.0
87404,360716.0,"Some guideline on writing code in the coding console: 1. The console is already having a code stub to accept the input required for the problem statement. So, there is no need to write any code for getting the input required for the problem statement. 2. For the reason mentioned- above, the input should NOT be hardcoded. 3. There would be a section in the console marked as ""# Write your code here"" or ""#Type your answer here"". The logic pertaining to the problem statement needs to be put here. For String Split related question: In the video prior to this question, we have been introduced to the method of string slicing method. You need to use this method to split the data as per the problem statement. Please check the below link which discusses about the same question: https://learn.upgrad.com/v/course/208/question/85685",313826.0
88288,365376.0,"In python, e ach object is identified by a ""constant-unique-random"" integer which is known as the ""identity"" of the object. The built-in function id(object) returns the ""identity"" of the object. https://docs.python.org/3/library/functions.html#id Only in C implementation of Python(also known as CPython), the ""identity"" of the object is the memory address of the object, but there is no such guarentee in other implementations of Python ( like Jython, PyPy etc). Please read further only if you are looking for more detailed explanation about id() function. Please note that, the id() function returns the ""Identity"" of the Object and not of the variable name. In python, variable names are just like references or labels that point to a particular object. Check out the below link which tries to explain the concept. https://realpython.com/python-variables/ The id() function itself is not very widely used in real-world programming. The is operator is more widely used to check whether two variables point to the same object. Id() function cannot be used to check if two variables have the same value or not. Consider the example below for equality and identity checks, which uses three lists: list1, list2 and list3 Hope this helps.",313826.0
88288,365361.0,id is function which returns the address or location of the object which is stored in memory. the id of two same value stored by two different objects points to same location and so id is same for this two objects. Also note the adress return is interger. Lets explain this in simple word id is something we can say is address or location of your house so it must be unique.Also note your location and and your family location is same. therfore id can be same for different objects also. Example. Lets a =2 lets id(a) say 1245678 return by python and it is memory location where 2 is stored now ( i.e 1245678 is adress where 2 is stored in memory of python) lets c = 5 then id(c) can be any integer meory location where value of c i.e 5 can be stored but not same as id(a) because value of a and c is different. therfore your python code id(a) ==ic(c) will return false. Now lets store again value 2 in different object say b= 2 then the id of (a) must be equl to id(b) because 2 is stored alredy in a location therefore b will not duplicate the value 2 in memory rather it will point the same memory location where object a is pointing. so in this case id(a) == id(b) returns true. Therefore this function we used to identify if any two or more objects storing the same value and evaluate accordingly.,307843.0
90036,374907.0,This might help: https://stackoverflow.com/questions/252703/difference-between-append-vs-extend-list-methods-in-python,311686.0
90036,374946.0,The extend() method takes a single argument (a list) and adds it to the end I found below link useful which clearly explain exten() with examples. https://www.programiz.com/python-programming/methods/list/extend Also have a look into below link for extend() vs append() https://stackabuse.com/append-vs-extend-in-python-lists/ I hope it will help you.,317991.0
90036,374915.0,Below link explain the use of list.extend() method. Generally when we want to add a sequence inthe end of the list then we use this method. https://www.tutorialspoint.com/python3/list_extend.htm,320073.0
90036,388630.0,Hello my Group mate,312357.0
90036,376293.0,"http://thomas-cokelaer.info/blog/2011/03/post-2/ please go through this also, it gives differences between extend() and append().",319721.0
87311,360075.0,To print without the newline character pass '' (nothing between single quotes) to the end argument.,313826.0
87311,360486.0,"To print an output that is spanning multiple lines like a paragraph, mention the output in three single or double quotes in the print statement as below.",314730.0
87311,359899.0,"Hi Vishnu, In python print() function has following structure: print(value1, ..., sep=' ', end='\n', file=sys.stdout, flush=False) here end='\n' by default means every print statement will end with new line . you can change it like in below code if you want to print in different manner",320073.0
87311,364818.0,\n inserts a new line while printing any text.,304816.0
89715,372638.0,I'd say just put all your questions and doubts in this fourm however silly or simple they might seem. I see the fourm is quite helpful and everyone will help you to clarify it. And the reply is as fast as you go and browse yourself but some people and explain you in layman terms. Try to post some of your course related doubts and see if that can help you to understand more.,318328.0
89715,372721.0,"Hello Josyula, I am also not from IT background. Even i am also facing difficulty to apply the logic and solve the problem. I would like you to follow the below step for better understanding. 1) Keep on practicing on jupyter notebook. Take example from internet for each topics. 2) Before playing any session or module video, Try to understand logic from yourself. You will get the same files in the starting of the every video which professor is using throughout the session. Then go for the video so you will get more confident . if you have any doubts after playing video then you can posted over discussion forum. 3) Try to resolve all solution by yourself only. At the last you can take help from google and discussion forum. Happy Learning :) A dream doesn't become reality through magic; it takes sweat, determination and hard work. :)",320195.0
89425,371027.0,Main diff is that if u r using sorted function u need to print that and the list doesnot change internally but list.sort() sorts the list internally and printing the list will return sorted list,318358.0
89425,370967.0,sort() doesn't return any value while the sorted() returns an iterable list.,314547.0
89425,370961.0,"And, that's the difference :",310974.0
89425,371055.0,"Sorted is a function. So, you call a function and pass some variable to it. list2 = sorted(list1) So, here list2 is a sorted version of list1. But the original list1 remains intact. Both these list1 and list2 are different objects. Whereas when you call list1.sort() which is a method it sorts list1. So, even if you assign list2 = list1.sort() Both your list1 and list2 will reference the same object now which is sorted version of original list1.",317689.0
89425,371069.0,"sorted ( iterable , * , key=None , reverse=False ) - Is a function which sorts the elements of an iterable (like list, tuple etc.) and returns a new sorted list. list.sort() - is a method belonging to the class list and hence can be applied only to list objects. The method does an in-place sorting of the elements of the list and returns 'None'. For more details check out the link below: https://docs.python.org/3/howto/sorting.html#sorting-how-to",313826.0
89425,372554.0,"list1.sort() will change the order of originial list. But, sorted(list1) will store the result in a copy of list1.",301643.0
91622,383627.0,"Hello Naren, Could you please share the snapshot of this error. Plese restart your browser and try it again.",320195.0
91622,385224.0,"Hi, It is working fine now.",311046.0
90420,377045.0,"AS you know the indexing bydeafult in python starts as from zero but the above output should be a natural number and zero is not a natural number. So you need to use the Range function to assign the index to start as 1 x= pd.Series(np.array(range(1,n+1))**2, index = range(1,n+1))",300687.0
90420,377039.0,"I can imagine from you solution output that you may have write code as follows: series = pd.Series(np.array(range(1,n+1))**2) You just need to add index part into above code as follows: series = pd.Series(np.array(range(1,n+1))**2, index = range(1,n+1)) Hope this will help.",317991.0
90420,377070.0,"While creating set, you can pass the index for the Series series = pd.Series(np.array(range(1,4+1)),index=range(1,4+1)).apply(lambda x : x**2) series",318368.0
94815,400128.0,I don't see any issue,310974.0
94815,400145.0,"Please use val = {'Name' : 'pallu', 'Age' : [25,27] } instead of val=dict(Name='pallu', Age=[25, 27]). I am using Python 3.6.5 installed with Anaconda. The below also works for me - &gt;&gt;&gt; val=dict(Name='pallu', Age=[25, 27]) &gt;&gt;&gt; val {'Name': 'pallu', 'Age': [25, 27]} &gt;&gt;&gt; OR -- - &gt;&gt;&gt; val= { 'Name' : 'pallu', 'Age' : [25, 27] } &gt;&gt;&gt; val {'Name': 'pallu', 'Age': [25, 27]}",312479.0
94815,400155.0,"working for me val=dict(Name='pallu', Age=[25, 27]) print(val) type(val) {'Name': 'pallu', 'Age': [25, 27]} dict",315679.0
94815,400521.0,"val=dict(Name='pallu', Age=[25, 27]) : This code should work as it is. But if it not working - Most likely you have assigned some other definition to word 'dict'. eg. if I do - dict = {'a' : 1, 'b' : 2 } # here i am creating a variable called dict of type dictionary. After this, you will get TypeError: 'dict' object is not callable, when you use dict(Name='pallu', Age=[25, 27]) (Reason: you have overriden defination of dict function.) solution is - Change the variable name to some other name (eg. dict1), then restart the program/shell.",318458.0
90459,377293.0,"Set the index of the dataframes to 'Country' so that you can get the countrywise medal count Add the three dataframes and set the fill_value argument to zero to avoid getting NaN value Sort it in descending order gold.set_index('Country', inplace = True) silver.set_index('Country', inplace = True) bronze.set_index('Country', inplace = True) total = gold.add(silver, fill_value = 0).add(bronze, fill_value = 0) total = total.sort_values(by = 'Medals', ascending = False) print(total)",300687.0
90459,377306.0,"What output or error you are getting, can you please show so that exact solution can be given.?",317991.0
90459,377324.0,"You have to add all three data frame to a single data frame. while adding the data frames you have to ignore the NaN values because all data frames are having few different columns. once you get the final data sort that and print. Maya Kavuri has posted complete code, you can refer that code",317845.0
91743,384453.0,"“Swift+tab” can help you, for further more information use “swift+tab+tab”.",318328.0
91743,384059.0,I think a simple tab would give you the expected result.,310974.0
90540,377656.0,Please let us know the error you are getting.,313826.0
90540,377655.0,what exactly is the question here Shristi? I think you posted one of the questions present in the learning module.,304814.0
90540,379116.0,I know where you are stuck. One can apply grouped function on multiple columns at a time. Provide an array as input instead of single column.,318007.0
90540,377662.0,Are you getting any error or you want solution for this. Please share error details if you are facing error.,317991.0
90590,377895.0,"You can go through below link to get more insight of ""expand=True"" https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html Hope it helps.",317991.0
90590,377896.0,"https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.str.split.html Please go through the link, they have clearly explained what the expand=true does for the split method. If you still need help, let me know.",319721.0
90590,377909.0,yeah got it. expand=False (default) return Series/Index expand=False return DataFrame/MultiIndex,318319.0
88923,368586.0,"max() functions returns only one maximum value from the list. So in order to get all the values which have the mximum value, max value can be used along with the list comprehensions or any Loop. For example: names = ['Earth','Air','Fire','Water'] 1. find out the maximum length in an array ln = max(len(x) for x in names) 2. Now filter the list based on the length lt = [x for x in names if len(x) == ln] print(lt)",318804.0
88923,368631.0,"I've used ""water"" before ""Earth"" in the list. Here is the result:",318328.0
88923,368589.0,"very nice query!! I found this in the python official docs If multiple items are maximal, the function returns the first one encountered. This is consistent with other sort-stability preserving tools such as sorted(iterable, key=keyfunc,reverse=True)[0] and heapq.nlargest(1, iterable, key=keyfunc) . https://docs.python.org/3/library/functions.html#max",309451.0
88923,368782.0,short answer first encountered is returned.,304813.0
88923,368582.0,"Reproducing from the python documentation ""If multiple items are maximal, the function returns the first one encountered."" Link for python documentation: https://docs.python.org/3/library/functions.html#max In this case, since ""Earth"" was encountered first and ""Water"" lster, the function returned ""Earth"".",313826.0
93918,395940.0,"Use the below code whcih is very efficient and easy way out; ls = ['a','b','c','a','d','t','s','g','s','y','q','l','n','d'] {v: ls.count(v) for v in ls} You can do it with your code as well but that will be very complex and difficult... you may have to use the multiple if conditions.. Rather use the above code whcih is very efficient and short.. :)",316349.0
93918,395952.0,"Hi Abhishek, Agreed with Hemant , you can not implement (efficiently ) the same code in dictionary comprehension . If you want the same code to be implemented in dictionary comprehension then you need to use multiple dictionaries. Source: Stack overflow https://stackoverflow.com/questions/26731675/python-count-occurrences-in-a-list-using-dict-comprehension-generator Please use the above link for the further analyis and why it is not memory efficient to implement the same code in dictionary comprehension. For the same result , you can use the count() function as below: ls = ['a','b','c','a','d','t','s','g','s','y','q','l','n','d'] d = {(v,ls.count(v)) for v ni ls} Thanks.",305652.0
95116,401934.0,"There's not really any ""raw string ""; there are raw string literals , which are exactly the string literals marked by an 'r' before the opening quote. A ""raw string literal"" is a slightly different syntax for a string literal, in which a backslash, \, is taken as meaning ""just a backslash"" (except when it comes right before a quote that would otherwise terminate the literal) -- no ""escape sequences"" to represent newlines, tabs, backspaces, form-feeds, and so on. In normal string literals, each backslash must be doubled up to avoid being taken as the start of an escape sequence. This syntax variant exists mostly because the syntax of regular expression patterns is heavy with backslashes (but never at the end, so the ""except"" clause above doesn't matter) and it looks a bit better when you avoid doubling up each of them -- that's all. It also gained some popularity to express native Windows file paths (with backslashes instead of regular slashes like on other platforms), but that's very rarely needed (since normal slashes mostly work fine on Windows too) and imperfect (due to the ""except"" clause above). r'...' is a byte string (in Python 2.*), ur'...' is a Unicode string (again, in Python 2.*), and any of the other three kinds of quoting also produces exactly the same types of strings (so for example r'...', r'''...''', r""..."", r""""""..."""""" are all byte strings, and so on). Not sure what you mean by ""going back "" - there is no intrinsically back and forward directions, because there's no raw string type , it's just an alternative syntax to express perfectly normal string objects, byte or unicode as they may be. And yes, in Python 2.*, u'...' is of course always distinct from just '...' -- the former is a unicode string, the latter is a byte string. What encoding the literal might be expressed in is a completely orthogonal issue.",318017.0
95116,402006.0,Python raw string treats backslash (\) as a literal character. This is useful when we want to have a string that contains backslash and don’t want it to be treated as an escape character. Raw strings just tells the interpreter that whatever is in the quotes is raw and does not need any formatting and does nothing for any special characters in between strings. You can refer following link https://www.journaldev.com/23598/python-raw-string for better understanding of raw string.,320685.0
95116,402733.0,"There are two types of string in python: the traditional str type and the newer unicode type. If you type a string literal without the u in front you get the old str type which stores 8-bit characters, and with the u in front you get the newer unicode type that can store any Unicode character. The r doesn't change the type at all, it just changes how the string literal is interpreted. Without the r, backslashes are treated as escape characters. With the r, backslashes are treated as literal. Either way, the type is the same. In the image above the command can be given even without r at the begining..",310508.0
95116,402055.0,Vinod - \n has a meaning of new line (Enter button) in most of the languages inlcuding Python. ie. print('Vinod \nMudapaka') will be printed as - Vinod Mudapaka But what if you literally want to print \n ? Hence raw string form is important. It can also be achieved by using \\ also.,318458.0
95116,402051.0,"Consider X = 'Hi\nHello', Here backslash(\n) is considered as a special character by default, which means new line. So the output will be Hi Hello But when you done want it to be considered it as a special character, r should be used which tells interpreter dont do any formatting, consider the value as it is. X = r'Hi\nHello' Output: Hi\nHello",318804.0
102729,439318.0,in the first one you are saying that print all the things before -3 i.e. output 42 is -1 scala is -2 and likewise in the second one you are asking to print in a reverse order which is represented by - sign and since you have passed 3 it will print every third entry in the reverse order. -1 is 42 then skip 2 then -4 is python hope this helps,318017.0
102729,439899.0,"Thank you for the answers. It helped me clear my doubts. DA_Lang=['R','Python','SAS','Scala',42] 1) print(DA_Lang[1::3]) o/p: ['Python',42] Starts with index 1 and prints Python at index 1 and then skips 3 elements i.e. index1,index2,index3 elements and then prints 42 2) print(DA_Lang[0::3]) o/p: ['R', 'Scala'] Almost same as above but starts with index 0. 3) print(DA_Lang[::3]) ['R', 'Scala'] If start index is not specified it is understood as 0 in case of positive skipping 4) print(DA_Lang[::-3]) [42, 'Python'] Here index starts with -1 so it prints 42, then it skips 3 elements from the reverse order and prints Python at index -4",301114.0
102729,439662.0,1) print(Dataframe[start_index_value : end_index_value]) 2) print(Dataframe[start_index_value : : number_of_value_you_want_to_skip]) '- ' sign indicates a reverse order -3 signifies that you have to skip 2 values and consider the next value OR you can also interpret it as considering the start value you have to skip total of 3 values in reverse order and consider the next,318451.0
102729,440491.0,"Also, an additional observation, its one of the coolest ways to reverse the string. DA_Lang='Python' print(DA_Lang[::-1]) o/p --&gt; nohtyP",314084.0
102729,445198.0,"Looking at the syntax, The first one displays all elements before the position -3 which in your case is ""SAS"" The second has two ':' which syntactically means it will take three arguments, i.e., start, end and step size. Since only step size is given that too negative, it starts from the last position which is from ""42"" in your case. So, 3 step size from the first element at the right end which turns out to be ""Python"".",306726.0
142184,614361.0,"Vinay is right and very succinctly put I might add. This is also a good marker for the need of using inplace = True. If your function returns an output, the variable has not been modified.",319357.0
142184,614124.0,"Usually the functions / methods that mutate (change the contents) of an object do not return anything (i.e., return 'None'). List append() also belongs to this category of functions. It appends the object to the end of the list but returns 'None' and hence the behavior.",313826.0
81422,331649.0,"Hi, It seems you are running print(DA_languages[-1]) directly without defining DA_langugages. As an illustration, I have directly runned 2nd code in below image before 1st, and i get the same error. You need to define the variable first before using it. When I run 1st code and then run 2nd code, it will not give error.",304694.0
81422,331801.0,"Thanks Kiran, this solves the problem.",306011.0
78093,313263.0,Every module in this preparatory course has an introductory part.All these modules can be easily understood even for someone who is from non coding backgroud. The idea is you should the logic of the program if that concept is clear you can code in any language just by knowing few syntax. Hence dont worry whether it includes an introductory module or not. Just watch the videos I am sure you can understand the concepts.,300688.0
78093,323176.0,Always better to get a revision of the conceptbefore starting to solve problem,303082.0
78093,332865.0,"the basics of python are all available on Youtube. Please check for Playlist by Derek Banas called Learn To Program. This is a great resource to learn Python if we have experience with any other proramming language. If not, you should start with a DataCamp course on Python or an intro course from Data Quest. Even better, Coursera has great Introductory python couses. Hope this helps.",310217.0
78093,365832.0,I too felt the same .should have started from basics of python instead of consoling codes in the beginning class,320251.0
80628,327690.0,"No you can switch to python if you find it convenient for your future projects , it is not necessary to complete one session and only then you can proceed further infact whenever you feel you can jump to session of your choice itis flexible",305838.0
80628,327694.0,Thanks 👍,302734.0
80628,327812.0,"No two are different track, you can take the path pf Python without paying attention to R. I would recommend you to learn both and choose your own track in which you are comfortable .",301648.0
80628,328099.0,"Hi Anthony, It is not necessary to start with the given order of modules. You can start with any of the course according to your choice. These courses are completely independent of each other. You can start with Python in between, but it will be helpful if you complete R as well before the start of the main course.",300686.0
79885,323470.0,"They will explain each and every syntax required, once you proceed further.",305838.0
79885,326132.0,There is last chapter in which this is cleary defined for each languge I have just completed this for one of language cheers!!,307843.0
80591,328201.0,"Hi Neena, i have also started first time Python language and after completion of upgrad content right now i am comfirtable with Python. now i want to explore more about this so jus reading dataquest its really good...!!",305847.0
80591,327287.0,just went through dataquest! it really good! Thankyou!,307709.0
80591,327289.0,"Neena I think U first go through Upgrad content...it is explained in simple way...if ur not new to coding, then this content will be easy for u to understand... If u have less time I recommend U to follow C S dojo on YouTube for python videos... Hope dis helps",308437.0
80591,327370.0,"Neena Upgrad contents are really good to start with, even Iv'e gone trough DataCamp's Intro to Python course, it is good for beginers.",306011.0
80591,327675.0,Khan Academy is one of the good places to start with the basics of Python https:// www.khanacademy.org,307486.0
80591,327281.0,"Go through the content given here. If you feel the need to understand the language better, read Head First Python, 2nd ed. Once you have the basics down read this book ""Python Data Science Handbook"". It gives you a complete overview of the Python data science environment. As for websites, I recommened dataquest. They've got amazing tutorials and an equally amazing blog to go along with that. Then there's DataCamp's Intro to Python course. There are some tutorials on Kaggle too, but dataquest has all the others beat in terms of content (my opinion).",306733.0
80591,328152.0,"I believe the contents given in Upgrad should be enough to learn R &amp; Python both. If any one still wants to explore more and deep dive please refer some youtube or create an account in udemy.com and practice that using the sessions they provide, these are again online sessions and they are paid.",310430.0
80591,334925.0,"There are various sites available to learn Python. Datacamp and Datquest are 2 very good (free) options. IN addition to this, you can look for Courses from udacity and Coursera. If you are a complete beginner, the introduction to programming in Udacity or Programming for everybody using Python in Coursera is the best choice. In Youtube, in case you already have some experience with any other programming language, you can follow the Learn to program playlist (available for Free) by Derek Banas. https://www.youtube.com/playlist?list=PLGLfVvz_LVvTn3cK5e6LjhgGiSeVlIRwt Hope this helps.",310217.0
80591,336258.0,"Hi Neena, you can try Udemy.com for more python lectures, its a paid version, good thing is that you will get life time access to the selected python lecture. hope this help !",306012.0
83573,343382.0,"As per my understanding In pop it can return the value and also the advantage is it ask for item number or cand say index value whereas in remove function we need to provide the item name and its doesnt return the value when you ask to print p where we use pop function it wouls return the item name or value which is deleted or can say removed wherein when you say print d where we applied the remove functionn it returns as none DA_languages = ['R','Python', 'SAS', 'Scala', 42] p=DA_languages.pop(1) print(p) d=DA_languages.remove('R') print(DA_languages) print(d)",300687.0
83573,343428.0,"remove removes the first matching value, not a specific index: Suppose you have list &gt;&gt;&gt; a = [0, 2, 3, 2] &gt;&gt; a.remove(2) # Here a.remove remove the first matching 2 i.e from the second place and when u print a it prints below. &gt;&gt;&gt; a =[0, 3, 2] pop removes the item at a specific index and returns it. &gt;&gt;&gt; a = [4, 3, 5] &gt;&gt;&gt; a.pop(1) # Here pop(1) removes the second position element i.e because sting position starts from 0 ... 3 &gt;&gt;&gt; print(a) # once you print a now it will print only [4,5] [4, 5]",307843.0
83573,343700.0,You can use help function as described below :-,312746.0
83573,343577.0,"pop would retrieve any item from a list using index no.It would return that item.On the contrary remove would not return any thing.It is used to eliminate an item from a list using item itself. x=['A','B','C','D'] y=x.pop(1) print(x) would give x=['A','C','D'] print(y) would give y='B' on the other hand z=x.remove('B') print(z) would give nothing where print(x) would yield x=['A','C','D']",300690.0
83573,345705.0,pop :- It will return the current element and then delete it. remove:- It will not return anything and will just delete the current /selected element.,306038.0
86433,358057.0,"Whoah, Needa. That's a really cool visualisation. I don't think there are any tools which would deliver such sophisticated graphics. Probably, they have coded and used support from different libraries/tools. Check out these popular tools if you're curious. https://www.creativebloq.com/design-tools/data-visualization-712402",319721.0
86433,358519.0,This is awesome!! Just wondering where does the PowerBI sits.. which is again a powerfull data visualization tool..,316349.0
83572,343360.0,"not described in videos ,, dirext coding",315560.0
83572,343367.0,"If it's for sciPy and numPy, u can check this link https://www.scipy.org/scipylib/download.html",310585.0
83572,343368.0,you cn find the file which has step by step instruction,300687.0
83572,343459.0,All Python packages can be downloaded here. https://pypi.org/,304692.0
83572,343697.0,Use package managers to install library. ( I dont recommend to install any package and its releated dependeny one by one.) Two good package managers are - 1- pip 2- anaconda how to install package using pip - pip install &lt;your_favorite_library&gt; e.g pip install pandas how to install package using anaconda - conda install &lt;your_favorite_library&gt; e.g. conda install pandas,312746.0
83572,343701.0,"If you have installed Anaconda and using python, then go to Anaconda.org site and search for the package which you need. There are clear cut instructions on how to install it via the conda. if conda is not in your path, then go to the scripts in Anaconda install folder and invoke the conda and follow the instructions to install the same. For e,g conda install &lt;packages&gt;",310482.0
83572,344223.0,Details are given clearly in the module video n content itself. Any doubt ping me on 9620888890,308437.0
87255,,nan,
129603,,nan,
79317,320406.0,"filter(), as its name suggests, filters the original iterable and retents the items that returns True for the function provided to filter(). map() on the other hand, apply the supplied function to each element of the iterable and return a list of results for each element. For more clarity go through the link https://www.python-course.eu/lambda.php",305838.0
79317,320384.0,"Let's see why we need map function in first place - Objective 1: To find the squares of each element in a list Solution1: Can we do like this? first_list = [2, 4, 5] print(first_list*2) # It prints 2,4,5,2,4,5, so basically it just repeats the elements 2 times and doesn't squares it Solution 2: Using for comprehension or map function first_list = [2, 4, 5] print([x**2 for x in first_list]) # It gives the desired result #---------------Let's look using map function--------------- print(list(map(lambda x:x**2,first_list))) # It also gives the desired result, then why we need map function. Lets look one more example - Objective 2: To print sum of each element from 2 list by their position wise and output as a list second_list = [1,1,1] #-----------Using for loop------ for i,x in enumerate(first_list): for j,y in enumerate(second_list): if i==j: print(x+y) #It gives the desired result but not in list. Note: to convert it into list, we might have to do more operations which i am not doing right now #-------------Let's use map function------------ print(list(map(lambda x,y:x+y,first_list,second_list))) #-- It gives the desired result and in list and also the code is just 1 line and is more elegant and efficient So this clears why we need map function at first place and now it's easy to say why we need filter function- As the name suggests, filter creates a list of elements for which a function returns true. The filter resembles a for loop but it is a builtin function and faster. See below link for more examples - http://book.pythontips.com/en/latest/map_filter.html",304694.0
79317,320370.0,A map is a function which is used in the loops so that it implements functions at the same time. A filter is a function that can be used as a boolean operation it gives True OR False so that it can be later combined with the desired list.,304692.0
79317,322604.0,"The filter() method creates an array filled with all array elements that pass a test implemented by the provided function. The filter method is well suited for particular instances where the user must identify certain items in an array that share a common characteristic. For example, consider the following array: var playersArr = [ {name: 'Jason', footedness: 'left', position: 'forward'}, {name: 'Blake', footedness: 'right', position: 'defense'}, {name: 'Philip', footedness: 'right', position: 'goalie'}, {name: 'Logan', footedness: 'left', position: 'defense'}, {name: 'Will', footedness: 'right', position: 'forward'} ]; Let’s find all of the players in the array that are left-footed using the filter() method! var leftFootArr = playersArr.filter(function(player){ return player.footedness === 'left'; }); console.log(leftFootArr); /*This will log: [{name: 'Jason', footedness: 'left', position: 'forward'}, {name: 'Logan', footedness: 'left', position: 'defense'}];*/ The map() method creates a new array with the results of calling a function for every array element. The map method allows items in an array to be manipulated to the user’s preference, returning the conclusion of the chosen manipulation in an entirely new array. For example, consider the following array: var agesArr = [25, 36, 49, 64, 81]; Let’s take the ages inside of the array and find their respective square roots. function root() { var roots = agesArr.map(Math.sqrt); return roots; }; root(); //This will return: [ 5, 6, 7, 8, 9 ];",307843.0
87117,358741.0,"Please use the 'in' operator instead '==' ( equal operator ). Like below - if input_str[0] in ['a' or 'e' or 'i' or 'o' or 'u' or 'A' or 'E' or 'I' or 'U']: print (""YES"") else: print(""NO"") (Or) if input_str[0] in 'aeiouAEIOU': print (""YES"") else: print(""NO"") The == operator compares the values of both the operands and checks for value equality. Equal operator evaluates with one value whereas in operator evaluates over a range of values.",311502.0
87117,358750.0,"The code looks almost correct, except that you have missed checking for 'O'. Given the test cases that are executed for this problem, the code should have executed successfully. Please check if you have entered the code correctly and that the initial code stub is not tampered. However, there are other pythonic ways of writing the same code in a more concise manner. Ex. vowel = 'AaEeIiOoUu' print(""YES"" if input_str[0] in vowel else ""NO"")",313826.0
87117,358914.0,"you can do it like this a = ['a' , 'e' , 'i' , 'o' , 'u' , 'A' , 'E' , 'I' , 'O' , 'U'] for x in a: if x == input_str[0]: print (""yes"") else: print(""no"")",320685.0
87117,358925.0,"Logical Operators like OR,AND, NOT require expressions that evaluates to (returns) either TRUE or FALSE (Boolean Value) on either side. In your case input_str[0]=='a' returns a Boolean value depending on the input but 'e' or any subsequent characters donot return values unless they are explicitly assigned a Boolean value. ( eg u have assigned 'e' = TRUE). using a list is preferable as mentioned by others here and I would add that instead of evaluating separately for upper and lower cases u can convert your input to any one of the cases and halve the length of your list (or expression). eg. input_str[0].upper() in [A,E,I,O,U]",319898.0
87117,358730.0,"Hi Arjun, your if statement logic syntax is wrong. it should be input_str[0] == 'a' or input_str[0] == 'e' and so on.. or better you can create a list of vowels against which you want to check the first letter. And then checkif the first letter is present in that list or not.",320073.0
87117,358965.0,"Here there is syntax error.. only the condition str[0] == 'a' is being evaluated and no other comparisons will happen or to say will return a Boolean value. You can correct this in 2 ways: 1) compare with each vowel separately i.e str[0] == 'a' or str[0] == 'e' and so on.. 2) compare with the elements of the list containing vowels. vowel_list=[a,e,i,o,u] if str[0].lower() in vowel_list: print('YES') else: print('NO') Hope this helps :)",318495.0
87117,359592.0,"If input_str[0] in range('a','e'','i','o','u') : print(""YES"") else: print(""No"")",319759.0
87117,359749.0,"Hi, Can use this. if input_str[0] in ['A','E','I','O','U','a','e','i','o','u']: print(""YES"") else: print (""NO"")",305129.0
87117,361567.0,"if input_str[0].upper() in 'AEIOU': print (""YES"") else: print(""NO"")",318332.0
90406,376993.0,"Some of the things that I noticed in the code: "" &amp; "" operator is a bit-wise and operation. You need to use "" and "" operator for logical conditions. The condition following &amp; i.e., x([len(x)-1] in check) has not been constructed properly. Please re-look at the order of parenthesis and square brackets. "" in "" is an membership operator meaning it checks whether x[0] is either 's' or 'p'. This is incorrect for this problem , as words starting with 'p' and ending with 's' will also be filtered. Try by including 'peas' as one of the words in the list. Hint for finding the last element of a string: make use of negative index. Hope this helps you resolve the errors.",313826.0
90406,377015.0,"Looks like few syntax error 1. the filter function syntax is wrong, list is outside the lambda function. the correct one : filter(lambda x: x[0].lower()=='s' and x[-1].lower()=='p',input_list) 2. you are using '&amp;' operator that is not supported on string type its should be 'and' 3. instead if using len() function you can use negative index to get the last character. I will give an example where get the words start with 's' and end with 'p' from list input_list = ['hdjk', 'salsap', 'sherpa'] sp=list(filter(lambda x: x[0].lower()=='s' and x[-1].lower()=='p',input_list)) print(sp)",317845.0
90406,377081.0,You are missing a closing bracket in the statement.,318368.0
81930,337079.0,"1- for loop runs here 3 time since d has 3 elements. 2- Most of the function returns some valure in python. In this case, add function returns ""none"" which is caught by print function. and furthur it got printed.",312746.0
81930,334059.0,"The output is None None None because of the below reason. # Here d is set having elements 0,1,2 d = {0, 1, 2} # below d in bold is varible in first line and it reads element one by one so it first loop read 1, then 2 and 3 # i.e three times this loop will be there as there is 3 elements in the set. for x in d: #here the code d.add(x) invoking the function add on set d and if the element from the loop will be new #then it will add in the set element list and return nothing means none and hence it prints 3 times none. print(d.add(x)) Execute below code it will help you to understand morehere same add function adding element to the set d whenenever add function will have new element in the list but it is similar and print None when we try to print calling the add function , add function in list just add the element in set but return none. d={0,8,7} e=(0,1,2,3) #f=(""012"") for x in e: print(d.add(x)) # print(e[x]) #print(f[x]) #print(d.add(3)) print(d)",307843.0
81329,331208.0,"count = len(list(filter(lambda x: x[0]=='S',input_list)))",304694.0
81329,331220.0,There is already a thread regarding this discussion. You will find many different solution over here - https://learn.upgrad.com/v/course/208/question/79999,304694.0
81329,334522.0,"count = sum(list(map(lambda x: x[0].upper() == 'S' ,[word for name in input_list for word in name.split()]))) as it says we need to count the word i have also spit the names in individual words, then creating a list of True and False for each word based on the the condition and then sum of the list to get the count.",306725.0
81329,341994.0,if input_str.startswith('a' or 'e' or 'i' or 'o' or 'u') : print('YES') else: print('NO'),310634.0
83913,345615.0,You can use below code :-,312746.0
83913,345755.0,input_str='analytics' vowel='aeiou' x=input_str[0] if x in vowel: print('YES') else: print('NO'),300690.0
83913,346394.0,"one more solution: var = ""analytics"" list_vowel = ['a','e','i','o','u'] if var[0].lower() &lt; "" "".join(list_vowel): print (""YES"") else: print (""NO"")",311855.0
83913,346353.0,"import ast,sys input_str = sys.stdin.read() vowels={'a','e','i','o','u'} if input_str[0] in vowels: print('YES') else: print('NO')",313228.0
83913,345981.0,"##Another Solution input_str = 'analytics' print(""YES"" if input_str[0] in ""AEIOUaeiou"" else ""NO"")",313826.0
83913,346404.0,"We can try this way also - input_str = 'analytics' print(input_str[0]) if input_str[0] in 'aeiou': print(""YES"") else : print(""NO"") ** if input_str[0] in 'aeiouAEIOU': - Try with this condition as well.",311502.0
83913,349069.0,"Hi...I wrote the below code but getting syntax error. What is the mistake ? #Write your code here #Use capital YES or NO vov = ""aAeEiIoOuU"" if input_str[0] in vov print(""Y"") else print(""N"") Getting below error Solution output File ""/code/source.py3"", line 7 if input_str[0] in vov ^ SyntaxError: invalid syntax Expected output YES",318479.0
83913,353686.0,"import ast,sys input_str = sys.stdin.read() if input_str[0] in ['A','E','I','O','U','a','e','i','o','u']: print(""YES"") else: print(""NO"")",306009.0
83913,354304.0,"import ast,sys input_str = sys.stdin.read() list1=['a','e','i','o','u'] list2=['A','E','I','O','U'] if input_str[0] in list1: print (""YES"") elif input_str[0] in list2: print (""YES"") else: print (""NO"")",320197.0
83913,356337.0,if input_str[0].lower() in 'aeiou': print('YES') else: print('NO'),318454.0
83913,359368.0,"import ast,sys input_str = sys.stdin.read() if input_str[0] in ('a','e','i','o','u'): print (""YES"") else: print (""NO"")",303082.0
84705,347228.0,"Check this: vowels = ['a','e','i','o','u'] list_vowel =[word for word in input_list if word[0].lower() in vowels]",310974.0
84705,349810.0,"vowels = ['a','e','i','o','u'] list_vowels =[word for sentence in input_list for word in sentence.split() if word[0].lower() in vowels] print(list_vowels) check this out.",317062.0
84705,351639.0,"Hi subham your code works but I didnt understand this. Which have made bold kindly help me. vowels = ['a','e','i','o','u'] list_vowels =[word for sentence in input_list for word in sentence.split() if word[0].lower() in vowels] print(list_vowels)",308639.0
84175,346341.0,"x=[] for i in (1,n+1): y=x.append(i**2) print(y)",300690.0
84175,346344.0,Please do not hard code range values. remember range gives values 1 to n-1. e.g. you can use below code :-,312746.0
84175,346303.0,"change range(1,5) to range(1,n+1 )",310585.0
84175,346591.0,Thank you Guys !,300727.0
84175,346621.0,n = int(raw_input(...)) a=1 while a &lt; n: print a*a a += 1,315455.0
84175,346864.0,"import ast, sys n = int(input()) count_list=[] count_list=[x**2 for x in range(1,n+1)]; print(count_list)",314617.0
84175,348524.0,"Use advanced way thought :) !!! map----- list(map(lambda x:x**2, range(1,5)))",304813.0
84264,346622.0,For more shortcuts please go through below link http://maxmelnick.com/2016/04/19/python-beginner-tips-and-tricks.html Hope it helps!,310585.0
84264,346928.0,There might be two reason for this issue:- 1- Your queris might be not returning anything on stdout. 2- Your kernal might not getting recommended heap size on ram. Check if your windows console or mac terminal is working in background or not.,312746.0
84797,347395.0,"The reduce(apply_function,iterable) function reduces the items in an iterable to a single value using the apply_function that is passed as the first argument. The type of the return value is same as returned by the apply_function. In the above case, the apply_function is ""lambda x,y:x if x&gt;y else y"" which returns a value of int type ( value 89 to be precise). And applying the list() constructor on a single integer value is not allowed as list() expects an iterable and hence the error . Try below code(in which the outer list() has been removed) and it should work: answer =reduce(lambda x,y:x if x&gt;y else y,input_list)",313826.0
84797,347789.0,"the issue is we are asked to print greatest value, but in your code you have made a lsit. Just drop off the list keyword everything will run fine",310585.0
81458,331920.0,"Depends on the problem at hand. Map works on iterable data structures and the most common one is list. So only we are seeing many examples of map with list. Thnx for asking this. Made me try other options ☺️ Check this out. Used set, tupe, list &amp; dict:",310974.0
81477,332316.0,"result=lambda x,y:x**y print(result(6,7))",300690.0
81477,331891.0,"Try this! cube = (lambda x, y: x**y) print(cube(6,7))",304813.0
81477,331935.0,"Here You go ,Please execute the below logic. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) x = int(input_list[0]) y = int(input_list[1]) def squared(x,y): out= x**y return(out) print(squared(x,y))",301648.0
81477,332068.0,"See below code #Step 1 . First Define the function # Here cube is function name and def is keyword to denote funtion which actullay tells we are defining #function to the compiler and return is also a keyword return the cube of any num which is parameter def cube(num): out = num**3 return(out) #Step 2. Call the function, here I have passed 2 as parameter to get the cube of 2, you can pass any integer cube_ 2= cube(2) print(cube_ 2) You will get the result once you run the first step and then second step which call the function cube",307843.0
81477,337076.0,"use below code :- def squared(x,y): return(x**y)",312746.0
81477,351482.0,"def squared(x,y): out= x**y return(out) print(squared(x,y))",318353.0
81477,353689.0,"Reducing the number of variables is a good practice during programming. we can directly return the value from the function: def squared(x,y): return(x**y)",315383.0
84713,347327.0,"use below one, import ast,sys input_str = sys.stdin.read() vowels={'a','e','i','o','u'} if input_str[0] in vowels: print('YES') else: print('NO')",313228.0
84713,347104.0,Tried this aswell .I feel there might be some issue @ final.append] line,310585.0
84713,347106.0,"u can copy and paste code from here import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) vowel = ['a','e','i','o','u','A','E','I','O','U'] final = [] leng = len(input_list) for x in range(0,leng): word = input_list[x] alphabet = word[0] for vowel in alphabet: final.append(input_list[x]) print(final)",310585.0
84713,347174.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) vowel = ['a','e','i','o','u','A','E','I','O','U'] final = [] leng = len(input_list) for x in range(0,leng): word = input_list[x] alphabet = word[0] if alphabet in vowel: final.append(input_list[x]) print(final)",311041.0
84713,347111.0,"This can be achieved in two simple lines as it says we need to use list comprehension: vowels = ['a','e','i','o','u'] list_vowel =[word for word in input_list if word[0].lower() in vowels]",305845.0
84713,347107.0,"some breath taken finally import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) vowel = ('a','e','i','o','u','A','E','I','O','U') final = [] leng = len(input_list) for x in range(0,leng): word = input_list[x] alphabet = word[0] if alphabet in vowel: final.append(input_list[x]) else: continue print(final)",310585.0
84713,347942.0,"input_list =['something', 'something-else'] vowels = ('a','A','e','E','i','I','o','O','u','U') [print(word) for word in input_list if word[0] in vowels]",317987.0
84713,347364.0,,300690.0
84713,348054.0,print([word for word in input_list if word[0].lower() in 'aeiouAEIOU']),313826.0
84713,348959.0,"list_vowel = [item for item in input_list if item[0].lower() in ('a','e','i','o','u') ] print(list_vowel)",301650.0
82111,337495.0,Its indentation. It plays an important role in python,311855.0
82111,335347.0,"if a==b --- It is first evaluated since it is true then only it enter inside otherwise it goes to final else if c==d -- This is only evaluated if a==b return true and prints wow if c==d returns true else return try again print(""wow"") else: print(""Try Again"") else:---Final else ..... ......",307843.0
82111,335403.0,"As per your example code compiler will first evaluate if a==b condition and if that returns true it will go for next block. In this way, if c==d return true it will print the relevant output else it will print Try again ( whatever you will give in the next subsequent else block ). F Finally, if a==b return false it will evaluate the corresponding else block.",301648.0
82111,335424.0,"I think its indentation, is it?",300721.0
82111,336050.0,At first it will check a==b.if true it will check c==d.if true it will print 'wow' else pritnt 'Try Again'. if a==b false it will print ........ ........,300690.0
82111,336627.0,"Unlike other programming languages where {} defines the scope of block, here in python indentation plays the majot role. No brackets are used. If code is not indentated properly then you might get the undesired output or even an error.",305845.0
82111,336633.0,We can have multiple IF condition in any language and always the first IF takes the first precedence in most of the language even in python. In python the intendention for two condition is separated by line and then a space and your code is properperly intended so the behaviour of the code is similar what your thought.,307843.0
82111,337628.0,"open and closed beacket is thats why necessary , else you will get error. if (a==b) { if (c==d) { print(""wow"") } else { print(""try again"") } } else ......",311404.0
82111,356347.0,"because of indent even we can code multiple lines between 'if' , 'elif', 'else' conditional blocks but indention is must if not applied 'indent' properly , the compiler will show 'error' for eg- if a==b if c==d print(""wow"") print('Hi') print('Test') else: print(""Try Again"") else:",318454.0
88506,366204.0,It seems Pass is not any instuction in Python which python can understand instead you can use None. Please try that option with None and confirm. That will be good learning for all of us if it works.,307843.0
88506,366207.0,"lambda will do this operation on every item in your input_list. If u break it down for a particular item in your input_list it would look like this- check[0] = input_list[0] if (input_list[0][0] == 's' and input_list[0][-1] == 'p') else pass let me simplify further: check[0] = input_list[0] if condition else pass The problem is not lambda but any type of above assignment with condition. In such cases else must have a valid value to assign to variable, check[0] in this case. I don't know what the official terms are for the above kind of statements but i have learned this practice. Basically if your condition is false you can't go ahead without assigning anything. From the look of your code I suspect you don't need map but filter. But I don't know the full context.",313515.0
88506,366212.0,"Thank for the response guys I used the below code instead and it worked check = lambda x : x if (x[0] == 's' and x[-1] == 'p') else False sp = list(filter(check,input_list)) print(sp)",312160.0
88506,366217.0,"As you all said, 'else' slways needs a value, I tried but didnt work. Hence used False along with 'filter' since filter checks true/false, this worked or me. Good learning",312160.0
88506,367001.0,"Hi Pradyumna. just to add to what you said above. What i think is that, Lambda returns an object type. So, it needs a value. It is not that ""else"" always needs a value, instead ""lambda"" always needs a value, since it has to return an object. so, pass would work in ""else"" normally, but wont work in lamda. please correct me if i am wrong. cuz even i am new to python.",317998.0
88506,367149.0,"Hi Amani. I think you are using 'pass' as a string in your above code. and that is why it is working with lambda in your code. Otherwise pass is a keyword , and since lambda returns an object it needs a value and cannot simply pass an iteration . please correct me, if i am wrong.",317998.0
88506,367131.0,"You can not use pass as it's not a reserved keyword instead of that use 'pass' input_list=['sap','san','smp','text'] check = list(map(lambda x : x if (x[0] == 's' and x[-1] == 'p') else 'pass',input_list)) print(check) ['sap', 'pass', 'smp','pass'] once you get the list to remove all elements that are equal to 'pass', so you will get the list of elements that satisfy your condition. You can achieve the same using filter. The filter function creates a list of elements for which a function returns true. Here is a short and concise example: input_list=['sap','san','smp'] check = list(filter(lambda x : x[0] == 's' and x[-1] == 'p',input_list)) print(check) ['sap', 'smp']",317845.0
88506,366206.0,"lambdas can only contain expressions - basically , something that can appear on the right-hand side of an assignment statement. pass is not an expression - it doesn't evaluate to a value, and a = pass is never legal. lambdas implicitly return the result of their body, so you need to use None instead of Pass. list(map(lambda x : x if (x[0] == 's' and x[-1] == 'p') else None,input_list))",318319.0
84893,347804.0,"Hi Aditya, You can make use of the filter function to do that in one line. We use filter when we have to filter out the elements based on condition. Code: count = len ( list ( filter ( lambda word : word[0].upper() == 'S',input_list ) ) ) print(count) Based on the below condition all the words of the list will be iterated. And the words which has first letter 'S' will be added to the list. Finally the length of the list will give me the no of words. lambda word: word[0].upper() == 'S' //Condition to check for the words in the list. Note:Filter offers an elegant way to filter out all the elements of a sequence ""sequence"", for which the function function returns True.",315028.0
84893,347807.0,"Or to use map you can use the below logic while iterating over all the words of the list . If the word starts with 'S' it will return 1 else will return 0. Finally sum function will return the length of the no of items mapped. count = sum (map (lambda word : 1 if( word[0].upper() == 'S') else 0, input_list)) Hope it helps! Rahul",315028.0
84893,347814.0,"The problem statement says that we need the count of words satrting with 'S"". so we could do it by the below piece of code count = sum(map(lambda x : x[0] == ""S"",input_list))",313826.0
84893,347865.0,short and solid. Thanks Vinay!,310585.0
84893,347945.0,"count=print(len(list(map(lambda x:x[0].upper=='S',input_list))))",300690.0
84893,355326.0,,300690.0
81652,332791.0,The same question has been answered previously. Please visit the following link:- https://learn.upgrad.com/v/course/208/question/79999,301652.0
81652,332700.0,"Hi, you can use this : import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) count= list(map(lambda x : 'S' in x[0], input_list)) print(count)",301648.0
81688,332839.0,"Yes, this can be achieved using join as well. However, the question to achieve it using reduce is just to test our undestanding of reduce. Using join and reduce: input_list = ['I','Love','Python'] seperator = ' ' print(seperator.join(input_list)) #gives I Love Python print(reduce(lambda x,y:x + "" ""+y , input_list)) #gives I Love Python",300748.0
81688,332842.0,thanks a lot Shalini :),308962.0
81688,333850.0,applying reduce function is giving following error.,300690.0
82450,338040.0,"A simple solution which you can consider - sp =list(filter(lambda x:x.startswith('s')&amp;x.endswith('p'),input_list) )",312746.0
82450,338063.0,"output_list=list(filter(lambda x:x[0]=='s' and x[-1]=='p',input_list)) print(output_list)",300690.0
82450,338022.0,"Use below , the cocept used it checking the first charated as x[0and then last charter as first calucate the length and then use the subscript on last character, hope you got it input_list = ['hdjk', 'salsap', 'sherpa'] sp = list(filter(lambda x: x[0]=='s' and x[len(x)-1]=='p',input_list)) print(sp)",307843.0
82450,338277.0,"You can use the below logic as well. output = filter(lambda x : 's' in x[0] and 'p' in x[-1] ,input_list) print(list(output))",301648.0
82450,338029.0,"#Try this sp = list(filter(lambda x:x if (x[0] == 's' and x[-1] == 'p') else None, input_list)) # Or this, if you don't want to use if else sp = list(filter(lambda x:x[0] == 's' and x[-1] == 'p', input_list)) #Errors in your code: 1. Enclose s and p in single or double quotes. Otherwise, Python will think these are variables named s and p . 2. You don't need to use lower() on 's' and 'p' since they are already in the lower case. 3. In lambda expression you don't need to write input_list[0] == 's' . Instead , you should use the argument of the lambda expression (in my code it's x ). This is because lambda expression will automatically be applied to the individual elements of the iterable (i.e., input_list ). In your code, it will check whether 'hdjk' == 's' and whether 'sherpa' == 'p' . (Check this:- print(input_list[0]) and print(input_list[-1]) ) 4. You will also have to write the else block in the lambda expression. See how to write if else statement in one line in Python. (https://stackoverflow.com/questions/11529273/how-to-condense-if-else-into-one-line-in-python) 5. Instead of &amp; , use and operator. (Both of them serve two very different purposes in Python)",301652.0
82450,338493.0,"As Ashish said whenever you compare any character it should be in quotes not only in python but most of the languages even R ,SQL,C so on as per my knowledge and the lower function is not required since you are directly comparing with the lower case letter's','p'. below is the code sp = list(filter(lambda x: x[0]=='s'and x[-1]=='p',input_list) print(sp)",300687.0
82450,338837.0,Guess u hv to mention else condition as well ... check once,308437.0
82450,342077.0,"sp = list(filter(lambda x: x[0].lower()=='s' and x[-1].lower()=='p',input_list)) This command will work for all test cases as they asked explictly s and p will be in small case.lower",311404.0
82450,349365.0,"Hi Abhishek, check this out:- import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) sp = list(filter(lambda x:x[0].lower().startswith('s') and x[-1].lower()=='p',input_list)) print(sp)",314617.0
82450,351619.0,"Use the below script, it is working: sp = list(filter(lambda x: x[0]=='s' and x[-1]=='p',input_list))",318353.0
82450,354420.0,"Try using below code sp = list(filter(lambda x: x[0] == 's' and x[-1] == 'p',input_list))",303673.0
82450,359875.0,"This also works s_p = lambda word : word[0]=='s' and word[-1]=='p' sp=filter(s_p,input_list) #Write your code here print(list(sp))",318335.0
82450,365453.0,"sp = list(filter(lambda x: x[0]=='s' and x[-1]=='p',input_list)) print(sp)",310529.0
79614,322599.0,It means n to the power 2 i.e i.e n= 1 then 1 power 2 = 1 2 power 2 = 4 3 power 3 = 9 4 power 2 = 16 and so on,307843.0
79614,322500.0,n^2 means n square,305838.0
79614,322455.0,"Hi Prashant, you have to use n inside the range function. Dont set values directly in range, it can be that you want n^2 (square) till 10 or 15, it will depend upon your input. So, use n inside range.",301559.0
79614,322673.0,"As it is mentioned to include n square also in range (1,n), that's why use for loop till n+1 I hope pic below will clear the doubt.",305838.0
79614,323197.0,N to the power of 2... N square,308437.0
79614,323625.0,"If you are working in R, if you want to assign square of an object, then we can use the caret symbol (^) #example, i want to calculate square of 10 ten_square &lt;- 10^2 where as in python, the caret function has a different meaning. To square an object, we use two asterisk ten_square = 10**2 Essentially, in python, the ^ symbol calculates the bitwise calculation of XOR function . here is more info: https://realpython.com/python-operators-expressions/ I trust this helps",301561.0
81961,337096.0,"You can subscript only String, list, tuple and dict. If you try to subscript any other type, you will get this error. In your case, you are subscripting int type, so you got this error.",312746.0
81961,334449.0,"""for x in num_list"" iterates through each element in the list. So, essentially, x is representing each integer in the list num_list. x[] is not valid, hence the error.",310974.0
81961,334452.0,"""for x in num_list"" iterates through each element in the list. So, essentially, x is representing each integer in the list num_list. x[] is not valid if x is an integer, hence the error.",310974.0
81961,334517.0,"Check below what is in your num_list string since I assueme as list of string 1,2,3,4 it should works same you can try it will loop 4 times and add the elelment to new list as here it appends the 2,4 in the new list.The error you may got because you have declared some intergegers in the list as you have not shown the full code and it is my guess only.",307843.0
81961,334747.0,"In your loop, x is an int and not a list. Hence you cannot subscript it. You can subscript num_list but not x.",304319.0
88541,366397.0,"ESC takes the jupyter to a diff mode where you can add or remove cells above and below the current cell. To execute the current cell, select it again and run ctrl+enter.",310974.0
87250,359616.0,"You need not specify the type of a variable beforehand in Python.That means you dont have to specify that the variable is an integer,string, double etc. because Python understands this from the first assignement. When you write the line of code 'for x in range(1,n+1)' it is interpreted as ""x=1, check whether x &lt;n+1, do the following statement (as long as its properly indented), then x=2, check whether x &lt;n+1...... "" so on and so forth until x= n+1. At this point the condition becomes false and the ""loop"" is complete. So you can see you are effectively assigning n+1 values for x; it is not done automatically.",319898.0
87250,361585.0,"Python is automatically defining X variable as it has encountered in for loop statement and values will be assigned from range function 1 by 1. X will be accessible in your next line of codes also. Try this: list_squares = [] n=3 for x in range(1,n+1): list_squares.append(x**2) print(list_squares) print(x)",318332.0
79999,337157.0,"you can use below code using map only- l=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] count=sum(list(map(lambda w,c=0:c+1 if w.startswith('S') else c , l))) print(count)",312746.0
79999,324051.0,"Since it was mentioned to use 'Map', I did it this way: (Although I do think Filter is the way to go ahead with this one) # input_list = ['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] count = map(lambda x:x if(x[0] == 'S') else None , input_list) # leave words starting with S but replaces other words to none. # count is now ['San Jose', 'San Francisco', 'Santa Fe', None] count = list(count) for i in range(count.count(None)): count.remove(None) # remove all 'none' and the list now contains only the words that start with S # count = ['San Jose', 'San Francisco', 'Santa Fe'] print(len(count))",300748.0
79999,325473.0,"Hi , here is something intresting i found ,when the condition is true on index by default python takes the 1 as count (TRUE=1) when it is false count taken as 0(FALSE=0) Below are the observations: input_list=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] z=((list(map(lambda x:x[0].startswith('S'),input_list)))) print(z) o/p [True, True, True, False] **if i tried to add '0' to the expression** input_list=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] z=((list(map(lambda x:x[0].startswith('S')+0,input_list)))) print(z) o/p [1, 1, 1, 0] **In above expresion i have added zero, if i would have added 1 then the o/p is [2,2,2,1],these clears on the bases of true condition indexes return 1 and in false 0** Note:-If we do any numeric operation on the above expression without adding zero by default the number functions works on indexes and that what hapend to me input_list=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] z=sum((list(map(lambda x:x[0].startswith('S'),input_list)))) print(z) o/p 3",300693.0
79999,331251.0,"My solution using list comprehension, lambda, map",310974.0
79999,323725.0,"This is how you can count the number of words that start with ‘S’ in input_list. &gt;&gt; count = sum (map (lambda w : 1 if( w[0] == 'S') else 0, input_list)) # The lambda expression returns 1 if the first letter of it's argument is 'S' else it returns 0. # The map() function will return a map with 1's and 0's. 1's will be present as many times as the number of words in input_list that start with 'S'. If you apply sum() function on map() , you will get the required value i.e. count .",301652.0
79999,323723.0,"I did not use MAP function here, i found it easy to use using FILTER function - count = len(list(filter(lambda x: x[0]=='S',input_list))) print(count) Would like to know if anyone finds using MAP function.",304694.0
88576,366639.0,"str() is function/method in python used to convert a value to a string . +str(sqr_4) , where + is used to concatenate with the statement in the print function and the str(sqr_4) converts the int data type to string that would allow you to print. Please note your function in the code does not return a value and due to which your output would be as below sqr_4 isNone To avoid this use "" return out "" in place of print(out), take a look at the screenshot below which would give you more insight.",312259.0
88576,366730.0,"Lets break this +str(sqr_4 + --&gt; Used for concatenation (in case of strings) / addition (in case of numbers) str() --&gt; Used to convert a data type to string data type eg. str(1) --&gt; '1' sqr_4 --&gt; The value you got from the function(Here it will be None as your function is not returning anything). So basically +str(sqr_4) is converting sqr_4 to string at first and then concatenating it with ""sqr_4 is"" Hope this helps :)",318495.0
83314,343099.0,In python indentation is not for just readability but it has syntactical significance. If you indent your else correctly it should work finr as shown by Vishal Thanks,306729.0
83314,342138.0,"Same logic , try this . first = input_str[0] vowel= ""aeiou"" if first in vowel: print('YES') else: print('NO')",301648.0
83314,342149.0,I submiited your code it worked fine. I think you might have submitted old code. Please try to solve the problem form scrtach again. You wont get any issue. Even i have tested some scenraio in your code which gives me expected result. for your ref -,312746.0
83314,342456.0,I am getting following error-,300690.0
83314,342464.0,Don't indent 'else'. Try this: a = 'virat' vowel = 'aeiou' if a[0] in vowel: print('yes') else: print('no') Though this will not work if your string is in capitals,312507.0
83314,342472.0,,312507.0
80436,326259.0,"I won't give you the answer. However, I'll give the logic, so that you can write the code. Every string is similar to a list. So, it's just the same as extracting the first element of a list and passing it through an expression.",306733.0
80436,326406.0,"&gt;Take the vowels in a list and run a for loop on this list. &gt; Check if the first letter of input list is same as the vowel, if yes set a flag and break out. &gt; If no then continue the process and check with the next vowel. &gt; Check the flag and print yes/no.",300748.0
80436,326724.0,String indexing is the concept to be applied... Check if str[0] is a vowel (I guess u have to check for both small and capital letters based on question) U have to check for 10 letters totally... I suppose this helps...,308437.0
80436,327121.0,you can also use the .startswith() function.. however that will take multiple check conditions.,305845.0
80436,327226.0,"You can use the startswith() function , to get the output much faster",301655.0
80436,329759.0,"Please find below sample code which explains how to compare using if - else and construct check to first letter from alpha is vowel or not. Step 1 . First store the first string input here as in question full string is stored in list input_str Step 2 : Get the first element of string by using input_str[0] Now it is easy just compare input_str[0] with vowel letters as given below , it will give you excat result ""YES"" The first if statement checks the first litter of input string {alpha} which is {a} and hence first if conditions returns the ""YES"" #Write your code here if('a' in input_str[0]): print(""YES"") elif('e' in input_str[0]): print(""YES"") elif('i' in input_str[0]): print(""YES"") elif('o' in input_str[0]): print(""YES"") elif('u' in input_str[0]): print(""YES"") else: print(""NO"") #Use capital YES or NO #Us",307843.0
80436,341995.0,if input_str.startswith('a' or 'e' or'i'or'o'or'u') : print('YES') else: print('NO'),310634.0
87640,363760.0,"the code snippet you shared has two errors, 1. The TypeError here gives the information you require. lambda function required two inputs ie the logic and the input list but u gave just 1. in this case you have given your logic! which is what you need to do on an input) but you have not passed the input itself (inp) 2. you don't have two inputs: each time you run your loop (lambda expression) only one value gets passed from input list inp. which means in the 1st iteration x takes the value of the first element from input list and subsequently 2nd iteration passes second element; so on and so forth. so you don't require a second variable y. try to modify your code accordingly.",319898.0
87640,363791.0,"you have defined the filter function without input filter(lambda x: x % 2, input) as you have just one input, you dont have to use y it should be x sp=filter(lambda x,y: x[0]=='s' and y[-1]=='p') the correct code: input_list = ['hdjk', 'salsap', 'sherpa'] sp=filter(lambda x: x[0]=='s' and x[-1]=='p',input_list) print(list(sp))",317845.0
87640,363899.0,"Your code is missing the iterable. The elements of the iterable are passed as input to the function, which is the first argument to the filter function. Plug in the iterable, and change your lambda to what @AmaniPrasad writes. You'll get the answer.",306733.0
87640,369199.0,"1. provide 2nd argument which is an input list to the filter function. 2. Convert the output to a list Here is the correct Code: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) sp= list (filter(lambda x,y: x[0]=='s' and y[-1]=='p' ,input_list )) #print(sp,input_list)-- You dont neet this now. Instead print(sp)",301124.0
87595,363145.0,"The code stub provided for this question uses ""input_str"" variable to accept the input string. In case you have not modified the original code stub, then you need to use ""input_str"" variable in you if condition as: if input_str [0].lower() in the_vowel: print('YES') else: print('NO') Hope this helps.",313826.0
87595,363150.0,"what is the error you are getting? I tried your piece of logic. It runs successfully. What I can think of is you have declared the input string as ""input_st"", but in the if-else block you are using ""myword'. Here is the code I wrote based on your query: the_vowel=""aeiou"" myword = 'analytics' if myword[0].lower() in the_vowel: print('YES') else: print('NO')",309451.0
80920,,nan,
89449,371151.0,You are missing the space next to * (In str1 += '* '). That is also a character. Which is why -2.,300748.0
89449,371145.0,"That is because of the following reason: inside the if part every time when you are appending to the str1 string you are appending TWO characters , one is the * symbol and the other is blank space. So, from the iteration when i becomes 5, when the control comes to the else part, the characters of your string (str1) will be: * space * space * spcae * space * space Now, you can evidently see that, you need to remove the last 2 characters to get the number of stars to be 1 less than the current number of stars. that's why -2 has been used instead of -1. if you use -1, it would just remove the space in the i=5 iteration, then remove the * in the i=6 iteration then again remove space in the i=7 iteration and so on. Hope that clarifies.",317998.0
80922,328892.0,One way is to use filter instead of map. You can also use the remove () function of List.,301652.0
80922,329129.0,"Try using the lambda function as it is memory efficient. list(filter(lambda x : x[0]=='S' , i/p_list))",305845.0
80922,329133.0,"You cant avoid the None if you use map(funct,list_city) and that is because map works on EACH element of the list and returns the result of each element when they are passed through the function. For this you can use filter. Filter will filter through the input list and will leave only those elements that fulfill a certain criteria.",300748.0
80922,331291.0,Try this:,310974.0
80891,328729.0,Here is one way to do it:,300748.0
80891,329131.0,Here is as simple as it can get:,305845.0
80891,331894.0,we can use list comprehension here! just one lne of code...:) result = [alpha1+alpha2 for alpha1 in x for alpha2 in y],304813.0
80876,328639.0,"When you apply items() function on a dictionary, it will return a list of (key-value) tuple pairs. To iterate over those pairs, the syntax is: for key, value in my_dict.items(): So in your case variable roll_num will be assigned the keys of the dictionary( students_data ). And the keys of students_data happens to be the roll numbers of students(1, 2, 3, 4, 5) in your case. So, Python just followed the instructions you gave and assigned the keys from dictionary(which were roll numbers of students_data ) to the variable roll_num .",301652.0
82367,337755.0,Code i have used,305129.0
82367,337754.0,,305129.0
82367,337746.0,"its not working showing error- they have also given example- For e.g. if the input list is: [ ['Ankur', 'Avik', 'Kiran', 'Nitin'], ['Narang', 'Sarkar', 'R', 'Sareen']] the output list should be the list: ['Ankur Narang', 'Avik Sarkar', 'Kiran R', 'Nitin Sareen']",305129.0
82367,337745.0,Check this,310974.0
82367,337762.0,"use below reference it works dynaminc you can write lamda or loops and work for dynamic also. colors = [['Ankur', 'Avik', 'Kiran', 'Nitin'], ['Narang', 'Sarkar', 'R', 'Sareen']] print(colors[0][0]+' '+colors[1][0])",307843.0
82367,337763.0,i reset the solution it worked..,305129.0
82367,337758.0,"if you have two input list - then ans should be - But you have one input list which has lists of list like this: [ ['Ankur', 'Avik', 'Kiran', 'Nitin'], ['Narang', 'Sarkar', 'R', 'Sareen']] then ans should be :-",312746.0
82367,338041.0,"Check- a=['MS','Virat','Suresh'] b=['Dhoni','Kohli','Rayna'] c=list(map(lambda x,y:x+' '+y)) print(c)",300690.0
82367,338042.0,"Sorry,check this one. a=['MS','Virat','Suresh'] b=['Dhoni','Kohli','Rayna'] c=list(map(lambda x,y:x+' '+y,a,b)) print(c)",300690.0
82367,348619.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) s = input_list[0] m = input_list[1] name =list(map(lambda x, y: x +' '+y,s,m)) print(name)",314617.0
82367,338044.0,,300690.0
82367,348643.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) first_name= input_list[0] last_name = input_list[1] name =list(map(lambda x,y: x +' '+y,first_name,last_name)) print(name)",314617.0
82367,350414.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) first_name = input_list[0] last_name = input_list[1] name = list(map(lambda x, y: x[0].upper()+x[1:] +' '+ y[0].upper()+y[1:], first_name,last_name)) print(name) try this out it will work",317062.0
82367,351240.0,"Check this out : concat_string = lambda x : x[0] + "" "" + x[1] name = list(map(concat_string,list(zip(first_name,last_name))))",318756.0
82367,351597.0,"Use the below code: use space between two + symbols name = list(map(lambda x, y : x +' ' + y,first_name, last_name)) print(name)",318353.0
80894,328725.0,"Loop on the list and remove the items that do not start with R eg: x = ['Rohit','Bhubaneswar','Ramesh','Virat'] x = list(x) for item in x: if (item[0] != 'R'): x.remove(item) print(x) You will be left with ['Rohit','Ramesh']",300748.0
80894,329121.0,"You can also use String startswith() functionality in python. It returns a boolean value and code will be much shorter inside the loop. Also, you can skip creating another list as the question only asks to ""extract"" the words.",305845.0
80894,328730.0,"x=['Rohit','Bhubaneswar','Ramesh','Virat'] #Create an empty list y to store words which start with R y = [] # Use for loop to iterate over every element of list x and check the condition. len(x) will return the no of elements # in x i.e. 4, but indexes in Python start from 0. Therefore we subtract 1 from len (x) so that for loop iterates 4 # times only. &lt;i will have values 0, 1, 2, 3&gt;. for i in range(0,len(x)-1): # Check condition using if statement if(x[i][0] == 'R'): # Use + operator for list concatenation y = y + [x[i]] print(y) #Filter will be a much better implementation in this case. x=['Rohit','Bhubaneswar','Ramesh','Virat'] y=list(filter(lambda w:w[0] == 'R', x)) print(y)",301652.0
80894,330439.0,"Python is a very simple language you generally don't need to write much to do things this a one liner for your problem Considering , x=['Rohit','Bhubaneswar','Ramesh','Virat'] y = [final for final in x if x[0].upper() == 'R'] #that's it Output: [""Rohit"",""Ramesh""]",304391.0
80894,331892.0,Bettter to use list comprehension as shown in the videos. Utkarsh has just done that!!,304813.0
80914,328900.0,"1. Write a function that accepts a single argument. This function's purpose will be to check the condition in question and return the argument if the condition is true. 2. In the function's body, check using if conditional statement whether the argument starts with an 's' and ends with a 'p'. You can use startswith () and endswith () function of String for this purpose. Return the argument if the condition in the if statement stands true. Write the return statement inside the body of if statement. 3. Use the function just written (that checks your provided condition) and the iterable ( input_list in our case) in the filter () function. Convert the filter into a list. #The program will look similar to what's given below, replcae those angular brackets &lt;&gt; with your own code: input_list = [' hdjk ', ' salsap ', 'sherpa'] def &lt;function_name&gt;(&lt;argument&gt;): if(&lt;condition&gt;): return(&lt;argument&gt;) my_list = list(filter(&lt;function_name&gt;, &lt;iterable&gt;)) print(my_list)",301652.0
80914,329128.0,"It can be achieved in a single line using the lambda function inside the filter function. list(filter(lambda x : x[0]=='s' and x[-1]=='p', input_list))",305845.0
80914,329857.0,"You can use lambda Function and Filter to extract the required list of names from the input_list. input_list = ast.literal_eval(input_str) sp = filter(lambda x : 's' in x[0] and 'p' in x[-1] ,input_list) print(list(sp))",301648.0
85246,349550.0,"Hi Surya, If we have a list x that contains strings as ""San Jose"" , ""San Francisco"", ""Georgia"", so on.. Now, using x[0] would give us ""San Jose"" using x[2] would give us ""Georgia"" But when we use map function on List just as you have used, it would do as follows: list(map(labmda x:x[0]==='S')) -&gt; Map function will operate on each of three strings that we too above in x. Thus, x[0] would mean 0th character of String ""San Jose"", then map would operate on second element in the list and again x[0] would be 'S' from ""San Francisco"". Happy coding!",318355.0
85246,349614.0,"String is a sequence based data type. It means you can subscript string variables as you subscript list. In Pyhton, If you give index which is out of range of that varibale it does not throw an error unlike other languages. input_list1=['San Jose'] print(input_list1[5]) ---&gt; No error So in your case - 1- lambda x : x[0] == 'S', input_list . --&gt; checks first letter of each element of input list. 2- lambda x : x[0][0] == 'S', input_list --&gt; Still checks first letter of each element of input list. since x[0][0]--&gt;x[0] is first letter of each input, x[0][0] does not return anything so it uses x[0] only.",312746.0
85246,350315.0,"@Vinay, please see below. input_list=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] print(input_list[0]) print(input_list[0][0]) print(input_list[1]) print(input_list[1][4]) San Jose S San Francisco F Output of input_list[0] is ""San Jose"", while output of input_list[0][0] is ""S"". How do you explain this w.r.t. my original question. As suggested by @Aditya, may be it is related with the Map function.",301650.0
85246,349867.0,"Lets say, X=""Something"". X[0][0] is interpreted as follows: First, only the bold part of the commaned in found out X[0] [0] --&gt; This gives a string of length 1. In this example, it is a string ""S"" . So, essentially X[0] is now ""S"" . Then the next index is applied. ""S"" [0] --&gt; Since ""S"" is a string with only one character, the zeroth index exists and hence returns the value at the zeroth index i.e., ""S"" . Infact , X[0][0][0][0] would also return the value ""S"", dur to the same reason as mentioned above. Please see the example attached and try it out. @Alok, If we pass an out of range index, we DO get an error. Please see the example attached.",313826.0
85246,350386.0,"Surya, Output of input_list[0] is ""San Jose"" because ""San Jose"" is the first element of input_list. Output of input_list[0][0] is ""S"" because ""S"" is the first element(char) of the string ""San Jose"" which in turn is the first element of input_list. Play around wuth the code below to getting a feelingon how indexing works with lists in python. input_list=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] for idx1,word in enumerate(input_list): print(""Element {} of input_list identified by index {}"".format(idx1+1,idx1)) print(""input_list[{}] : {}"".format(idx1,word)) print(""=""*50) for idx2,char in enumerate(word): print(""input_list[{}][{}] : {}"".format(idx1,idx2,char)) else: print(""=""*50)",313826.0
85246,351581.0,"Use the following code. It will work. S = list(map(lambda x:x[0]=='S',input_list)) count=sum(S) print(count)",318353.0
85246,352539.0,"This should work count = sum(list(map(lambda x:x[0] == 'S', input_list))) print(count)",303673.0
85246,362988.0,if doing by creating a separate function and then applying map what would be the code?,312050.0
85246,365425.0,"worked for me: count = sum(list( map(lambda x : x[0] == 'S',input_list ))) print(count)",310529.0
87602,363450.0,"Range Function range(start, stop, step) start : Starting number of the sequence. stop : Generate numbers up to, but not including this number. step : Difference between each number in the sequence. Your code will run for 1 to 10 with difference of 2 . 1,3,5,7,9",317845.0
87602,365229.0,"range() function can also take a third parameter. This is for the interval. range(start,stop,interval) for i in range(1,10,2) it travese trough 1to 10 and gives result with the interval of 2 another example: list(range(7,1,-2)) it will give result with -1 interval output:[7, 5, 3]",319444.0
87602,402003.0,Its printing values starting from 1 to 10 and printing alternative numbers from 1 so output will be 1 3 5 7 9,314629.0
80780,328233.0,Your lambda syntax is incorrect. What are you trying to do with range?,300748.0
80780,328399.0,"Yep. I don't know why you've added range(0, 4). But, that's what's causing the error. Also, why are you trying x[1][0]? When you map through a list like this: map (func, list) each element of the list is sent to func separately. So, I don't think you need to x[1][0]. x[0] will suffice.",306733.0
80699,328019.0,"You have an unclosed quotation after the : on line 1. Rewrite it as my_word = input(""Enter your word"" :"") It'll work",306733.0
80699,328276.0,"As Ashish and Pranesh suggested, please try: input(""Enter your word : '')",301559.0
86374,355931.0,"You could also try the below code: sum(map(lambda x : x[0] == ""S"",input_list)) Some more similar questions on the discussion forum https://learn.upgrad.com/v/course/208/question/85246 https://learn.upgrad.com/v/course/208/question/84893",313826.0
86374,356025.0,"This is how I have done it: 1. will work without using list() 2. used .upper() function as a string starting with 's' will fail the test case. count=sum(map(lambda x : 1 if x.upper().startswith('S') else 0,input_list)).",305845.0
86374,356074.0,"1st method: def first_s(word): if word[0]=='S' or word[0]=='s': return 1 else: return 0 count= sum(list(map(first_s,input_list))) print(count) 2nd method: count= sum(list(map(lambda x: 1 if x[0] =='S' else 0 ,input_list)))",318585.0
86374,356093.0,"Since this is boolean results into True; i counted the True results; count = list(map(lambda word : word[0] == 'S',input_list)).count(True)",316349.0
85308,349917.0,"str1 += '* ' is operation just like b+=1 in java or any other language. It adds(in case of int) or appends(in case of string). You can think this as str1 =str1+'*"". + here acts as a concatenation operator. As Alok explained, str1 = str1[:-2] is slicing the string here. let suppose u have, str1= * * * * *, str1[:-2] will give you * * * *. It will leave the last element of the string in every iteration.",305845.0
85308,349910.0,"Inside for loop two things are happening- 1- in if block - String concatenation is happening with '*"" until five stars. 2- in else block - String slicing is happening. i.e. str1=str1[:-2] --&gt; means slice the current str1 from first element to second last element. thats why 1 star deducted in every iteration.",312746.0
85308,350829.0,"This code is doing 2 things: 1. Concatenation 2. Splicing(Cutting out a part from the list/string) Here IF block is adding new character to the original string. Eg. If str1='' then after 1st iteration --&gt; str1='* ',2nd iteration str1='* * ' and so on till 5th iteration. In 6th iteration code goes to else block where splicing is happening. So after 5th iteration str1='* * * * * ' Now str1[:-2] will give you string starting from 0th position till '-2-1' i.e -3 position(i.e cutting out last 2 characters) So after 6th iteration str1='* * * * ',7th iteration str1='* * * ',8th iteration str1='* * ',9th(final) iteration : '* ' Hope this helps :)",318495.0
85308,359079.0,"Hi There Actual expectation in this program outcome is as below * ** *** **** ***** **** *** ** * ( above output when you use str1 = str1[:-1] ) in your program "" str1 = str1[:-2] "" will delete two '*' instead of one '*' in every iteration once &gt;=5 satisfies.....because of the -2 position cutting the last two characters in output ..so the output is like below. Reason: like in range , in array function the second positional will not be considered . So str1[:-2] skips last two character and copies characters from 0 to -3 position into str1. And its causing below output. if you use str[:-1] , you can get corrected output. you can test it in jupyter or python command promts run results. output when you use str1 = str1[:-2] is below * ** *** **** ***** *** *",318454.0
88786,367806.0,Can anyone tell me why is thhis happening?,315560.0
88786,367811.0,You have hardcoded the input. The test case will provide you a custom/different inputs for which your code will fail as it will always give same output for any input provided by systems. You can instead give input as: input_list=list(input()) or it will already be there in the problem stub! Hope this helps :),318495.0
88786,367812.0,"It is not working because one of the inputs given might be A pple ie, the first letter is capital. so, your code will not detect it as a vowel since you are not converting the first letter to lower case. you could use: if word[0].lower() in number_vowel: it should work then. Furthermore, as the question states, you should be using list comprehensions here You can do it as follows: vowels=['a','e','i','o','u'] list_vowel = [word for i in input_list if i[0].lower() in vowels word=i]# [Type your answer here] print(list_vowel) Also, avoid hardcoding inputs as that also causes problems, since everytime your program would be checking against the same input only. Hope that helps.",317998.0
88786,367838.0,"1. First error can be seen is you hardcoded input_list, but while submitting the input will be different so this might be the one reason of error. 2. Secondly you have not taken care of lower case. If submission has any word with first character as capital For ex 'Apple' then you need to convert it to lower case before going further. 3. Third is question required to be solved using list comprehension so there might be possiblity of giving error although if code is executing fine it should accept it.",317991.0
88786,367847.0,"thanks , will try this",315560.0
88786,367969.0,"input_list=['wood', 'old', 'apple', 'big', 'item', 'euphoria'] Solution:- print([item for item in input_list if item[0].lower() in 'aeiou']) This is using list comprehension.",317811.0
88786,368055.0,"Please do not hardcode the input ( in this case input_list ) as the code stub provided in the coding console already has provision to accept input. Further, the problem statement has asked to make use of the list comprehensions to derive at the solution. You could try the below code: print([word for word in input_list if word[0] in 'aeiouAEIOU'])",313826.0
88786,367976.0,"if you are using Upgrad online IDE you cannot hardcode the input values because IDE read the input values from the input tab. while doing the comparison try to convert the extracted part of the word to lower case and compare with the input string. You don't have to use number_vowel = ['a','e','i','o','u'] you can use simple vowel_str='aeiou' becase IN opeartor check for matching char sequence . list_vowel =[] vowel_str = 'aeiou' input_list=list(sys.stdin.read()) for word in input_list: if word[0]..lower() in vowel_str : list_vowel.append(word) print(list_vowel) You can use the filter function to do the same. for detailed information check the link below https://www.python-course.eu/lambda.php",317845.0
83423,342580.0,"In general, any lasting effect that occurs in a function, not through its return value, is called a side effect. There are three ways to have side effects: Printing out a value. This doesn’t change any objects or variable bindings, but it does have a potential lasting effect outside the function execution, because a person might see the output and be influenced by it. Changing the value of a mutable object. Changing the binding of a global variable So avoid above three things to not have side effect issue. You can follow the below ref for more detail - http://interactivepython.org/runestone/static/pip2/Functions/SideEffects.html",312746.0
83686,344138.0,"In fisrt case you are returing a value but in second case you are just printing the value inside the funtion. In python void function returns ""None"" as a functional programming standard.",312746.0
83686,344176.0,"Your code is much like 1st Case : greater = 7 # here 7 is assigned to greater and hence print funtion return 7 print(greater) 2nd case greater = print(7) # Here print func return none because defn of print is such that it retrn none after evaluate If you want to specifically assign value to varible you can use your custom function. this crearly suggest print function returns none. See below is an example of custom function here s is being return. ========================================================== def printer(s): print(s) return s greater = lambda x,y : printer(x) if (x&gt;y)==True else printer(y) print(greater(4,7))",307843.0
86439,356047.0,"Please read the explanation of the add() method as below: The set add () method adds a given element to a set if the element is not present in the set. Syntax: set. add (elem) The add () method doesn't add an element to the set if it's already present in it otherwise it will get added to the set. Reson for returning None: Python adds an implicit "" return None "" statement to the end of any function. Therefore, if a function doesn't specify a return value it returns None by default. print(d) - gives you the same set {0,1,2} print(d.add(4)) - also prints None however 4 gets appended to the list.",305845.0
83289,342108.0,The below code doesn't fail for me:,310974.0
83289,342168.0,"1- What is ""lrslt"" in your code ""print(len(lrslt))""; if you have not defined 'lrslt"" in your code and applying len function on it , you will get the error. you might have done some typo there. 2- you can refer below screen shot-",312746.0
83289,342355.0,you have converted the rslt into list and printed it but not saved the list. So if you want the length of the list u need to again convert into list as len(list(rslt)) or l=list(rslt) and then len(l),304319.0
83289,342453.0,Error is not encountering for the following code-,300690.0
83289,346339.0,"Question explicitly tells to use map. Used lambda, filter and map all in one statement. Try this once. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) print(len(list(filter(None,list(map(lambda x: x if x[0].upper() == 'S' else None,input_list))))))",313228.0
83289,347198.0,"Hi, is there anyway to write this using just map fucntion?",312376.0
83289,347326.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) filtered_list= list(map(lambda x: x if x[0].upper() == 'S' else None,input_list)) remove_list= ['None'] final_list = [x for i,x in enumerate(filtered_list) if i not in remove_list] print(final_list)",313228.0
83750,344602.0,"you should know the operator precedence and associativity to understand this. In your case :- please note ""() ""has highest priority. ""and"" operator has greater priority then ""or"" operator. Expression in the if statement is evaluated left to right. using this rule 1- if False and False or True and True: - equivalent to -&gt; if (False and False) or (True and True): 2- False and [(False or True) and True]} - since brekets have the highest priority, those expression in the brackets evaluted first. I am attching one example for your ref:- One Excerpt from python docs is below:- 6.16. Operator precedence The following table summarizes the operator precedence in Python, from lowest precedence (least binding) to highest precedence (most binding). Operators in the same box have the same precedence. Unless the syntax is explicitly given, operators are binary. Operators in the same box group left to right (except for exponentiation, which groups from right to left). Note that comparisons, membership tests, and identity tests, all have the same precedence and have a left-to-right chaining feature as described in the Comparisons section. Operator Description lambda Lambda expression if – else Conditional expression or Boolean OR and Boolean AND not x Boolean NOT in , not in , is , is not , &lt; , &lt;= , &gt; , &gt;= , != , == Comparisons, including membership tests and identity tests | Bitwise OR ^ Bitwise XOR &amp; Bitwise AND &lt;&lt; , &gt;&gt; Shifts + , - Addition and subtraction * , @ , / , // , % Multiplication, matrix multiplication, division, floor division, remainder [5] +x , -x , ~x Positive, negative, bitwise NOT ** Exponentiation [6] await x Await expression x[index] , x[index:index] , x(arguments...) , x.attribute Subscription, slicing, call, attribute reference (expressions...) , [expressions...] , {key:value...} , {expressions...} Binding or tuple display, list display, dictionary display, set display",312746.0
85364,350334.0,"Sometimes it happens:- Please follow the below steps: 1- check your internet speed. 2- Refresh the page one or two times. 3- If issue still persist, Close the browser and reopen the session",312746.0
85364,353791.0,"Try firefox, this problem does not appear there.",301650.0
85364,351202.0,"Could you please close the window and retry logging in and run the video? Also, ensure that you have good Internet connection. You can also refer to the following link for more details: http://help.upgrad.com/learning-platform/video-playback-issues",319721.0
85737,355192.0,"Your code looks good, might be some issue with the browser or because in the assignment we were asked to use x &amp; y and you used a &amp; b. I did it in a bit different way which you can try as well. Here is my code: """""" a = int(input_list[0]) b = int(input_list[1]) #Write your code here greater = lambda x, y: max(x,y) print(greater(a,b)) """""" Hope it helps.",307486.0
85737,352103.0,"Your code looks great, I can run the same code and it worked for me. Might be an issue with the web browser. try clearing the browser cache and check if that helps https://kb.iu.edu/d/ahic .",317845.0
88830,368063.0,"Try this out: greater = lambda x,y : x if x&gt;y else y print(greater(a,b))",313826.0
88830,368057.0,"here is the lamda function greater = lambda a, b : a if a&gt;b else b print( greater (9, 6)) Hope that helps :)",317845.0
88830,368070.0,Can you please share the screenshot of error you are getting ? Posting a solution will solve your problem but understanding problem and the error will give you much more than solution.,317991.0
88830,368084.0,"Here is the lambda function : greater=lambda a,b:a if a&gt;b else b print(greater(a,b)) Try the above..if still fails check that you have not altered any of the input statements given in the console as they are required for proper execution of the code in the console. Try to reset everything and start afresh.",309451.0
88830,368192.0,"Since you have not provided your solution..I'm assuming that you might have hardcoded(given permanent values to input) the solution. So in order to remove hardcode reset the problem stub.. And for solution part: print(lambda x,y: x if x&gt;y else y) General Suggestion : Don't seek for answers seek for logic behind them.. It will be helpful in future! ☺️ Hope this helps ☺️",318495.0
88830,368269.0,"greater = lambda x,y : x if x&gt;y else y print(greater(x,y))",317811.0
88830,368821.0,Can we use Ternary operator ( ?: ) with lembda expression ?,320687.0
88830,368997.0,"higher = lambda x,y: x if x&gt;y else y higher(2,3)",304692.0
88830,369299.0,I agree with vipul sharing the code will solve the problem but would not help is understanding the issue Pls share the screen shot of code as this is non graded should not be an issue to share the screen shot,300687.0
80804,328457.0,you can use lambda function for this as that would be more easy and efficient way of doing this type of problems,305838.0
80804,328392.0,"Try this: import ast,sys input_str = sys.stdin.read() if input_str[0] in ('a','e','i','o','u','A','E','I','O','U'): print(""YES"") else: print(""NO"")",301652.0
80804,328393.0,Your code will work in jupyter for the input provided by the user but not in the UPGRAD coding console. The input is already being passed using input_str = sys.stdin.read() statement in the console. You dont need an input from the user. So instead of the word variable just use input_str and remember this for any other questions you attempt on the console.,300748.0
80804,329853.0,"Your code Will not work here in UPGRAD coding console. Moreover you can apply List Comprehensions to make it easier and faster. Below is one of the core logic : vowels = ['a','e','i','o','u'] list_vowel =[word for sentence in input_list for word in sentence.split()if word[0] in vowels] print(list_vowel)",301648.0
80804,331434.0,Two times input command used Maybe that’s causing the problem,308437.0
80804,333676.0,"your code is perfect (because you have written the complete program)but in upgrad console we need to refer the variable which is already defined that is input_str elsse it throws error if we use anyother variable name .hope it helps import ast,sys input_str = sys.stdin.read() if input_str[0] in ('a','e','i','o','u','A','E','I','O','U'): print('YES')#Write your code here else: print('NO')#Use capital YES or NO",300687.0
87530,362218.0,"Map, Filter and Reduce These are three functions which facilitate a functional approach to programming. We will discuss them one by one and understand their use cases. Map Map applies a function to all the items in an input_list. Here is the blueprint: Blueprint map(function_to_apply, list_of_inputs) Most of the times we want to pass all the list elements to a function one-by-one and then collect the output. For instance: items = [1, 2, 3, 4, 5] squared = [] for i in items: squared.append(i**2) Map allows us to implement this in a much simpler and nicer way. Here you go: items = [1, 2, 3, 4, 5] squared = list(map(lambda x: x**2, items)) Most of the times we use lambdas with map so I did the same. Instead of a list of inputs we can even have a list of functions! def multiply(x): return (x*x) def add(x): return (x+x) funcs = [multiply, add] for i in range(5): value = list(map(lambda x: x(i), funcs)) print(value) # Output: # [0, 0] # [1, 2] # [4, 4] # [9, 6] # [16, 8] Filter As the name suggests, filter creates a list of elements for which a function returns true. Here is a short and concise example: number_list = range(-5, 5) less_than_zero = list(filter(lambda x: x &lt; 0, number_list)) print(less_than_zero) # Output: [-5, -4, -3, -2, -1] The filter resembles a for loop but it is a builtin function and faster. Note: If map &amp; filter do not appear beautiful to you then you can read about list/dict/tuple comprehensions. Reduce Reduce is a really useful function for performing some computation on a list and returning the result. It applies a rolling computation to sequential pairs of values in a list. For example, if you wanted to compute the product of a list of integers. So the normal way you might go about doing this task in python is using a basic for loop: product = 1 list = [1, 2, 3, 4] for num in list: product = product * num # product = 24 Now let’s try it with reduce: from functools import reduce product = reduce((lambda x, y: x * y), [1, 2, 3, 4]) # Output: 24 Source: http://book.pythontips.com/en/latest/map_filter.html Hope this helps.",318495.0
87530,362278.0,"Filter- You can apply filter function on a list and it will return the subset of a list that satisfies the condition. for instance: get the list of numbers greater than 5 from a list. my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9] output_list = filter(lambda x: x &gt; 5, my_list) print(output_list ) your output will be [6, 7, 8, 9] Reduce- You can use the reduce function to transform a given list into the single value. It basically keeps operating on pairs of elements until there are no more elements left for instance: get the multiplication of all the elements of the list. from functools import reduce my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9] multiplication=reduce(lambda a, b: a * b, my_list) print(multiplication) Output 362880 get the sum of all the elements of the list. from functools import reduce my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9] sum=reduce(lambda a, b: a + b, my_list) print(sum) Output 45",317845.0
87530,362237.0,"Map, Filter and Reduce These are three functions which facilitate a functional approach to programming. We will discuss them one by one and understand their use cases. Map Map applies a function to all the items in an input_list. Here is the blueprint: Blueprint map(function_to_apply, list_of_inputs) Most of the times we want to pass all the list elements to a function one-by-one and then collect the output. For instance: items = [1, 2, 3, 4, 5] squared = [] for i in items: squared.append(i**2) Map allows us to implement this in a much simpler and nicer way. Here you go: items = [1, 2, 3, 4, 5] squared = list(map(lambda x: x**2, items)) Most of the times we use lambdas with map so I did the same. Instead of a list of inputs we can even have a list of functions! def multiply(x): return (x*x) def add(x): return (x+x) funcs = [multiply, add] for i in range(5): value = list(map(lambda x: x(i), funcs)) print(value) # Output: # [0, 0] # [1, 2] # [4, 4] # [9, 6] # [16, 8] Filter As the name suggests, filter creates a list of elements for which a function returns true. Here is a short and concise example: number_list = range(-5, 5) less_than_zero = list(filter(lambda x: x &lt; 0, number_list)) print(less_than_zero) # Output: [-5, -4, -3, -2, -1] The filter resembles a for loop but it is a builtin function and faster. Note: If map &amp; filter do not appear beautiful to you then you can read about list/dict/tuple comprehensions. Reduce Reduce is a really useful function for performing some computation on a list and returning the result. It applies a rolling computation to sequential pairs of values in a list. For example, if you wanted to compute the product of a list of integers. So the normal way you might go about doing this task in python is using a basic for loop: product = 1 list = [1, 2, 3, 4] for num in list: product = product * num # product = 24 Now let’s try it with reduce: from functools import reduce product = reduce((lambda x, y: x * y), [1, 2, 3, 4]) # Output: 24 Source: Maps,Reduce,Filters Hope this helps :)",318495.0
85471,350799.0,"Hello Nakul, The error appears when the variable is not initialised/assigined any value. Also i see an inconsistency in the Error message(i.e. score name is not defined) as the code in the question looks to be incomplete, you will have to replace the input values to score/n. A sample for the same written below: score = int(input(""Enter score:- "")) if score==100: print(""Perfect"") elif 90&lt;=score&lt;100: print(""Distinction"") elif 65&lt;=score&lt;90: print(""First Class"") elif 40&lt;=score&lt;65: print(""Second Class"") else: print(""Failed"")",312259.0
85471,352006.0,"Then, it could be due to the IDE that we are using. I believe you are using Jupyter Notebook, in the first cell execute by pressing ENTER+SHIFT to execute and free text would appear just below that cell ; Enter score. Then proceed to the consecutive cells and press ENTER+SHIFT KEY, you should get your result If your still facing issues, please let me know which IDE your are using",312259.0
85471,352027.0,"Your code which you have provided is incomplete.So you might not have assigned a variable. I'm assuming you are ussing 2 variables : n and Score_name So here goes the code: n=int(input('Enter the Score: ')) if n == 100: score_name=""Perfect"" elif 90 &lt;= n &lt; 100: score_name=""Distinction"" elif 65 &lt;= n &lt; 90: score_name=""First Class"" elif 40 &lt;= n &lt; 65: score_name=""Second Class"" else: score_name=""Failed"" print(score_name) Note: n should be int (when you input something it's data type is string so i've used int(input()) to convert it to int) Hope this helps :)",318495.0
81266,330525.0,It seems your default input function taking argument as string and not conveted the string to in or float which is prerequisite for the multiplication. Please check below. Therefore you are getting error in python as multiplication is applican on int or Float. Since you have not provided the code but you can check below it will work in your system. And you can modified as per your wish weight = input('Enter Weight:') height = input('Enter height:') bmi= int(weight)/(int(height)*int(height)) float(bmi) print (bmi),307843.0
81266,330764.0,"age = input(""How old are you?: "") print (age) print (type(age)) Results------ How old are you?: 35 35 class 'str' &lt;&lt;--- This is problem! To answer your question, since Python 3.x doesn't evaluate and convert the data type, you have to explicitly convert to ints with int() like this - weight = int(input('Enter Weight:')) height = int(input('Enter height:'))",304694.0
81266,330765.0,"Amit's logic is spot on. And, you can convert the input to a particular data type right when you're getting them from the user. So, this would also work and be much cleaner: weight = float(input(""Enter Weight: "")) This way you can avoid multiple calls to the same function within a single program. If you know that the user is going to input an integer, you can set the type to int. But, it's safer to use a float and later convert to int if the data after the decimal isn't important.",306733.0
81266,330767.0,"age = input(""How old are you?:"") print(age) print(type(age)) Results------ How old are your? :35 35 class 'str' &lt;&lt;--- This is problem! To answer your question, since Python 3.x doesn't evaluate and convert the data type, you have to explicitly convert to ints with int() like this - weight = int(input('Enter Weight:')) height = int(input('Enter height:'))",304694.0
81266,330768.0,"age = input(""How old are you?: "") print (age) print (type(age)) Results------ How old are you? 35 35 class 'str' &lt;&lt;--- This is problem! To answer your question, since Python 3.x doesn't evaluate and convert the data type, you have to explicitly convert to ints with int() like this - weight = int(input('Enter Weight:')) height = int(input('Enter height:'))",304694.0
81266,331032.0,There is no particular function to take only int/float input? Bcoz every time we need to use int() or float () for conversion...it’s cumbersome!!,308437.0
81212,330285.0,"Largest = lambda x,y: x if x&gt;y else y print(Largest(3,4))",301648.0
81212,330032.0,"By Using the below lamda function is returning the max element. greater = lambda a, b : max(a,b)",307843.0
81212,330103.0,"greater = lambda x,y: x if x&gt;y else y print(greater(3,4))",304694.0
81212,331431.0,"def lambda (x,y): If x>y : return x Else: Return y greater = lambda (x,y) print (greater)",308437.0
81212,331884.0,"min = lambda a,b : a if a &gt; b else b min(2,3) print(min(a,b))",303968.0
85677,351650.0,"Hi you can refer to below code- list_answer = list(filter(lambda x : x % 5 == 0, input_list))",320073.0
85677,351655.0,"Hey Prabhakar, The below should give you an extract of a list with multiples of 5 print(list(filter(lambda x: x%5==0,input_list)))",312259.0
85677,352479.0,"Please elaborate on your question and tell us what you tried to answer your question. If there is an error when you tried to solve it, we can look into what that error is and learn about the cause for the error too. Though the above comments answer the question you asked, this might not help you learn the concept well. So, try not repeating this again.",319721.0
85677,354410.0,"Try using below code list_answer = list(filter(lambda x: x%5 == 0, input_list))",303673.0
85677,376909.0,you can also do it via list comperhension as : [ x for x in input_list if x%5==0],317577.0
85648,351673.0,"I think it is due to the way the coding platform is designed. The code written is probably executed as a script with the text entered in the ""Input"" tab passed to the script as input. Inside the script, either input() or sys.stdin.read() could be used to read the input passed.",313826.0
85648,352257.0,"The inputs are given from the input tab of the coding console. Refer to the image below,",319721.0
82349,337511.0,"Can you try below. input_list = ['All','you','have','to','fear','is','fear','itself'] #p=list((lambda s: "" "".join(input_list.split()))) from functools import reduce p=reduce(lambda x,y:' '.join(input_list),input_list) print(p)",307843.0
82349,337600.0,"you can use the below logic/ code. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) from functools import reduce print(reduce(lambda x,y : x+ ' ' +y, input_list))",301648.0
82349,337622.0,"you can use concatenation:simply print(reduce(lambda s1,s2 : s1+ ' ' +s2, input_list)",304813.0
82349,337768.0,"#Consider this:- my_string = 'Hello-World' my_list = list(my_string) print(my_list) Output:- ['H', 'e', 'l', 'l', 'o', '-', 'W', 'o', 'r', 'l', 'd'] Reason:- The list() function will convert a string into a list of single characters . (https://stackoverflow.com/questions/7522533/how-can-i-turn-a-string-into-a-list-in-python) Yo must be aware that reduce() will return only a single value . And in your case, you assign the returned value to p (the returned value is a string. Try this:- print(type(p)) ) . But, when you convert p into a list using the list() function, it disintegrates the string to character values, exactly in the way mentioned above. So, like everyone else has mentioned above, don't use list(reduce(&lt; code &gt;)), a simple reduce(&lt;code&gt;) will do the job.",301652.0
82349,337804.0,"Regarding Error in your code - Unlike filter() and map(), reduce() function does not return reduce object, it return the value which you are accmulationg in the lambda function. In your case, you are already getting string (p) from reduce function. So dont convert p to a list in your print statment. Final code will look like this :- 2- Better and simple version of your code :-",312746.0
82349,338054.0,"It can be done as follows input_list=['I','Love','Python'] output_list=' '.join(input_list) print(output_list)",300690.0
82349,349369.0,"Hi Maya, Check this solution:- import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) from functools import reduce print(reduce(lambda x,y: x+' '+y,input_list)) Thanks and Regards Somnath",314617.0
82349,350120.0,output=( ).join(input),300690.0
82349,370986.0,"this is perfect : print(reduce(lambda x,y : x[0]+ x[1:]+ "" "" + y[0] + y[1:] ,input_list))",315560.0
86169,,nan,
86185,353717.0,Please check the link below on a similar query https://learn.upgrad.com/v/course/208/question/84797,313826.0
86185,353794.0,"You are using list keyword which expects an iterable value as input. However, your function returns only one integer value. And, coverting that value into a list is giving an error. Remove the list keyword and check again. It will work fine.",311855.0
86185,354014.0,"I am not sure if you have imported reduce from functools. Here is the code to print the largest value in the list using reduce. from functools import reduce lis2 = [11,22,33,44,55] funct = lambda x,y: x if x &gt; y else y print(reduce(funct,lis2))",314730.0
86185,354065.0,"Your code should include, from functools import reduce Once you include this try running it. Reduce function here returns a single value which is of int type. You'll get below error: TypeError: 'int' object is not iterable This is so as python doesn't allow conversion from int to list. Try removing list before the reduce function, you'll get the output. Hope it helps :)",318495.0
86185,358524.0,Reduce results into a single value based on the defined function. And so we don't actually need to use 'list' in the Print statement. Try to remove 'list' and see if that works for you or not.,316349.0
86185,359892.0,"When printing, dont use the list as prefix to reduce, since reduce function returns only one value. Else you can try this: answer = reduce(lambda x,y : x if x&gt;y else y, input_list) print(answer)",318335.0
96664,411778.0,"By using the set's you are altering the order. May be it is expected to print in the same order. Try filtering using the conditions in a single statement Something like this print(list(filter(lambda x:x[0].lower() == 's' and x[-1].lower() == 'p',input_list)))",318329.0
96664,411864.0,Sets would generate random order.,311160.0
86225,354119.0,"The lambda keyword in Python provides a shortcut for declaring small anonymous functions. Lambda functions behave just like regular functions declared with the def keyword. They can be used whenever function objects are required. For example, this is how you’d define a simple lambda function carrying out an addition: &gt;&gt;&gt; add = lambda x, y: x + y &gt;&gt;&gt; add(5, 3) 8 You could declare the same add function with the def keyword: &gt;&gt;&gt; def add(x, y): ... return x + y &gt;&gt;&gt; add(5, 3) 8 Now you might be wondering: Why the big fuss about lambdas? If they’re just a slightly more terse version of declaring functions with def , what’s the big deal? Take a look at the following example and keep the words function expression in your head while you do that: &gt;&gt;&gt; (lambda x, y: x + y)(5, 3) 8 Okay, what happened here? I just used lambda to define an “add” function inline and then immediately called it with the arguments 5 and 3 . Conceptually the lambda expression lambda x, y: x + y is the same as declaring a function with def , just written inline. The difference is I didn’t bind it to a name like add before I used it. I simply stated the expression I wanted to compute and then immediately evaluated it by calling it like a regular function. Before you move on, you might want to play with the previous code example a little to really let the meaning of it sink in. I still remember this took me a while to wrap my head around. So don’t worry about spending a few minutes in an interpreter session. There’s another syntactic difference between lambdas and regular function definitions: Lambda functions are restricted to a single expression. This means a lambda function can’t use statements or annotations—not even a return statement. How do you return values from lambdas then? Executing a lambda function evaluates its expression and then automatically returns its result. So there’s always an implicit return statement. That’s why some people refer to lambdas as single expression functions . Hope this clarifies the practical usage of Lamba! :) For more : Lambdas",318495.0
86225,356143.0,"@Vinod, of course, we cannot use lambda for complex functions. It needs to be noted that lambda is an expression whereas def is a statement. Since lambda is an expression, it can only contain other expressions (no statements are allowed). Also, user-defined functions follow a lengthier protocol to be called. We can use these with map, reduce and filter but we need to define them before using them directly in these functions. For example, def squareit(n): return n**2 print(list(map(squareit, first_list)))",319721.0
86226,354115.0,"First your syntax: reduce ( Func , Iterable ) In this case, Func must be a two-parameter function that generates a single result. The function reduce()will take the first two elements of iterable, e1and e2, and pass them to the function Func , Func(e1, e2). The result is passed to the function Func again, along with the next element of the iterable, e3, Func(Func(e1, e2), e3). The process repeats itself to the next element of the Iterable Func(Func(Func(e1, e2), e3), e4), and so on until the last element is consumed. See the following example: &gt;&gt;&gt; def product (x, y): ... return x * y ... &gt;&gt;&gt; reduce (product, [1, 2, 3, 4, 5]) 120 The following diagram illustrates the evolution of the function reduce()on the elements of the last list. In short, the function reduce()applied to the function produto()and to the list of integers from 1 to 5 reproduced the factorial of 5. The summation could also be implemented by the function reduce()with the following lines: &gt;&gt;&gt; def sum (x, y): ... return x + y ... &gt;&gt;&gt; reduce (sum, range (6)) 15 The result is the sum of the integers 0 + 1 + 2 + 3 + 4 + 5 = 15. The code is only illustrative, there is a function sum()for this. Note : If Reduce function see's more than 2 argument or if the function used returns more than 1 value then you'll get TypeError for attribute mismatch. Hope this helps. :)",318495.0
86226,354106.0,"The way reduce() works is, that it takes the first 2 elements of the sequences and pass it to the function. The result obtained from the function is then made the first argument , the next element in the sequence is made the second argument and the function is called again. This goes on till we reach end of the sequence. Please read the python documentation for more details(link provided below) https://docs.python.org/3/library/functools.html?highlight=reduce#functools.reduce If however, the function used in reduce() mandatorily accepts more than 2 arguments, then we encounter a Typeerror mentionng that only 2 arguments were passed where as more argumets were expected. Please see below example for more clarity. &gt;&gt;&gt; l [1, 2, 3, 4] &gt;&gt;&gt; def sum_3(a,b,c): ... return a+b+c ... &gt;&gt;&gt; reduce(sum_3,l) Traceback (most recent call last): File ""&lt;stdin&gt;"", line 1, in &lt;module&gt; TypeError: sum_3() takes exactly 3 arguments (2 given)",313826.0
86192,353922.0,My point of view is if - else is conditional statement where as for loop is iteration. i.e if-else execute statements based on the conditions evaluated true. and for loop continues evaluting the same statment as iteration (or number of times) till the loop construct does get satisfied. Therefore if-else and for loop is entirely different for our programing purpose. But if you want to construct if-else behaviour to be same then you can do it like using lamda function inside the if or else condition but again it is choice.,307843.0
86192,354076.0,"The if else condition would come before the forst for loop. Let me try to explain with an example. Suppose you have a nested for loop as below: &gt;&gt;&gt; for i in range(1,5) : ... for j in range(5,10): ... if i%2: ... l.append(i*j) ... else: ... l.append(i+j) ... &gt;&gt;&gt; print(l) [5, 6, 7, 8, 9, 7, 8, 9, 10, 11, 15, 18, 21, 24, 27, 9, 10, 11, 12, 13] The same logic can be applied using list comprehension as follows: &gt;&gt;&gt; [i*j if i%2 else i+j for i in range(1,5) for j in range(5,10)] [5, 6, 7, 8, 9, 7, 8, 9, 10, 11, 15, 18, 21, 24, 27, 9, 10, 11, 12, 13] Please do note that there may be some other ways to achieve the same.",313826.0
86192,354107.0,"[i*j if i%2 else i+j for i in range(0,5) for j in range(5,10)] is equivalent to: for i in range(0,5) : for j in range(5,10): if i%2: l.append(i*j) else: l.append(i+j) print(l) So here, for every element in range 0 to 5(i.e 5 times upto 4) if else will be executed 5 times(because of 2nd loop). Steps of execution: 1) Enters 1st loop. 2) Enters 2nd loop. 3) Executes If-else block 5 times (as range of 2nd loop is from 5 to 10 i.e 5 times) 4) Repeats 4 more time from step 1(as range of 1st loop is 0 to 5 i.e 5 times and 1st time it already executed above). Hope this clarifies your doubt. :)",318495.0
86192,356104.0,https://www.tutorialspoint.com/python/python_for_loop.htm Refer to the examples mentioned here. You'll know where to use if else with for loops and nested for loops.,319721.0
86728,357729.0,For Basics: https://www.w3schools.com/python/ Basics and a bit of DS libraries: https://www.learnpython.org/ https://docs.scipy.org/doc/scipy/reference/tutorial/ http://www.gregreda.com/2013/10/26/intro-to-pandas-data-structures/ For realtime scenarios and practice visit https://www.kaggle.com/,312259.0
86728,358522.0,# download 'Python Data Science Handbook' # follow C S dojo on youtube,316349.0
86728,358743.0,you can use below platform also to learn Python for data science : https://www.datacamp.com/courses/intro-to-python-for-data-science,320073.0
89936,374416.0,strip() is used to remove any leading/trailing spaces from the sentence. here you're getting same result even after omiting it since your input sentence itself doesn't have any leading/trailing spaces in it.. ☺️ try giving some space at the start/end of the sentence and see the difference.. hope this helps..,316349.0
89936,374624.0,Strip is used to remove blank spaces for starting and ending of the string.,318368.0
89936,374419.0,"strip() is an inbuilt function in Python programming language that returns a copy of the string with both leading and trailing characters removed (based on the string argument passed). For example: # strip() method string = "" mahendra singh dhoni "" # prints the string without stripping print (string) # prints the string by removing leading and trailing whitespaces print (string.strip()) o/p will be mahendra singh dhoni mahendra singh dhoni",317991.0
89936,375776.0,Strip here is used to remove leading and trailing whitespace characters,317811.0
89766,372898.0,Please refer - https://stackoverflow.com/questions/30421373/what-does-in-in-ipython-notebook-mean-and-how-to-turn-it-off,311502.0
89766,372993.0,"When a cell is displayed as ""busy"" with an asterisk i.e In[*], it can mean one of several things: The cell is currently executing. An ""execute"" command was sent to the cell, but another cell is currently executing. The cell will execute when its turn comes. The kernel was interrupted / restarted while the cell was executing, it is currently not executing. If you want to interrupt/stop the execution, go to the menu Kernel and click Interrupt.If it doesn't work, click Restart. You need to go in a new cell and press Shift + Enter to see if it worked. Hopw it will help. Source:- stackoverflow",317991.0
86360,355679.0,"This is nice question. So let me correct the code at first..You used A instead of 'A'..which will give A not defined error. So the code will be: d={'A':x*3 for x in 'abc' } print(d) Output will be: {'A':'ccc'} Reason: x*3 for x in 'abc' will itereate 3 times over a,b,c giving values as aaa,bbbb,ccc respectively after ever iteration. So after 1st iteration d={'A' : 'aaa'} After 2nd iteration : d={'A': 'bbb'} (Value got updated in the dictionary for key 'A') After 3rd(last) iteration: d={'A':'ccc} So this is how you get final output!",318495.0
87277,359667.0,"You are not executing the frist cell clearly, if you had, a number shows up between [] for that cell:",310974.0
87436,361190.0,"For dictionary comprehension , the code for generation of the key:value pair should be enclosed in curly brackets { } . n = 5 dictionary = { key:key**2 for key in range(1,n+1) } print(dictionary) PS: Do not hard-code the value of n in the coding console as the code for getting the input and assigning to the variable n has already been included in the code stub.",313826.0
87436,361446.0,"First two lines of code read the input data in online IDE import ast,sys input_str = sys.stdin.read() Don not hardcode the value line n=5 it should take from sys.stdin.read() n=sys.stdin.read() so your code should look like import ast,sys n=sys.stdin.read() dictionary = {key:key**2 for key in range(1,n+1)} print(dictionary)",317845.0
89064,369265.0,"Yes, you can do it in this way: a=2 b=5 greater = lambda x,y: x if x&gt;y else y print(greater(a,b)) Here the lambda function takes x,y as two inputs. it then checks the condition x&gt;y. if it is true it returns x otherwise it returns y. Hope it helps.",317998.0
89064,371065.0,"You can do this, x,y = 2,3 max = x if x&gt;y else y print ( max )",319721.0
87385,360504.0,"You are using n**2 that is printing square of 4 the code should be list=[i**2 for i in range(1,n+1)] print(list) [1, 4, 9, 16]",317845.0
87385,361577.0,"import ast, sys n=int(input()) list=[i**2 for i in range(1,n+1)] print(list)",318332.0
89101,369402.0,Use time library for checking which option is superior in terms of performance. import time use t1 = time.time() before the execution of the code (for ex: loop statement) t2 = time.time() after the end of the execution of the code t2-t1 will give you the time taken for execution.,318084.0
89101,369385.0,"If you consider mylist=[1,2,3,4] print( [i for i in mylist]), is identical to print(mylist) , which prints the string representation of mylist, which means square brackets enclosing a comma-delimited listing of each element's string reproduction. If you loop over mylist and print each item separately with the print statement, it will go over the list and print the string representation of each of those items. for i in my list: print (i) If you want the output as a list, then List Comprehension can be used",318804.0
89101,369382.0,"You can use either of them in any given scenaria. However, List comprehensions have some advantages over loops and hence the professor also advised on using them instead of for loops. The advantages are: 1. It is faster since python allocates list memory first , before adding list elements. And this improves the efficiency of the code considerably when working on large data sets. 2. It reduces the number of lines of code required and makes it more elegant and readable. Hope that helps.",317998.0
89101,369370.0,Well it depends on your choice. But List Comprehension are fatser than normal for loop contruct. So its always better to use that coding which gives better performance in terms of execution time. I came across a useful blog on list comprehension. Hope that will clarify your concern. Below is the link: https://www.analyticsvidhya.com/blog/2016/01/python-tutorial-list-comprehension-examples/,317991.0
89101,369694.0,"List comprehensions are ideal when you have one condition. The single line of list comprehension is crisp and increases the readabality. But, if the logic requires more than one condition, the list comprehensions may become longer and confusing. Check this link for interesting description about when to use and when not to use List comprehensions. https://www.makeuseof.com/tag/python-list-comprehension-guide/",314730.0
89101,369489.0,"tl;dr: Use a list comprehension as long as it does not hinder the interpretability of the code. As people have already pointed out before, list comprehensions are way faster than for-loops. They're more elegant too. But the caveat is this, list comprehensions cannot be used for complex operations. Imagine 4 nested if-elif-else suites. There is no way you can fit such a big chunk of code within the recommended 80 character line (check PEP8 and PEP20). This means that you have a piece of code that's trying really hard to be elegant but ends up looking ugly. In such a scenario, you have only two choices: 1. Use better logic which simplifies the operation or 2. Write better code. And, as a rule of thumb remember that clean code is inherently better than fast code. It's better to write code that works, before optimizing it. So, while list comprehensions maybe faster than for-loops, they're best when the operation you're carrying out is simple. (Zen #3) Long story short, use list comprehensions as long as the operation is simple. Else, use a for-loop and save both future you and your collaborator a lot of hurt. As a measure, you can check to see if your list comprehension is exceeding 80 (to be on the safer side, 60) characters. If it yes, rethink your approach, or use that for-loop Hope this helps!",306733.0
87678,364247.0,"Hi Shrikant, The question is not wrong. They are just making us comfortable to use the map function. This is how I did it in a single line: count=sum(map(lambda x : 1 if x.upper().startswith('S') else 0,input_list)) print(count) PS: It can be achieved with filter aswell.",305845.0
87678,363975.0,The question is very much correct. The problem can be solved using map() . Hint: A lambda function could be used to map the elements that start with 'S' and the result of the map function can be passed to another function which can find the sum of the elements mapped.,313826.0
87678,363988.0,"Yes the problem can be solved using Map you can use map with lamdafunction to check if words start with 'S' Example of the Code: m=map(lambda x: x[0]=='s', input_list) mList=list(m) print(mList)# here your list will be [False, True, True] just count the True in the list and you are good to go count=mList.count(True) print(count)",317845.0
87678,366329.0,"Hi, it can be done by map fun = lambda x:1 if x[0]==""S"" else 0 count = sum(map(fun,input_list))",320685.0
87678,367146.0,"Hi, Plase check this code. count = (sum(map(lambda x: x[0].upper().count('S'), input_list)))",322683.0
87678,370091.0,"These are the four possible solutions which can be tried: 1&gt; count = list(map(lambda x: x[0]=='S', input_list)).count(True) 2&gt; count = sum(map(lambda x: 1 if x[0]=='S' else 0 , input_list)) 3&gt; count = sum(map(lambda x: True if x[0]=='S' else False , input_list)) 4&gt; count = sum(map(lambda x: x[0].upper().count('S'), input_list))",311869.0
87730,364182.0,"d = {0, 1, 2} - This creates a set with 0,1,2 as elements (not a list which is more widely used) d.add(0) would try to add 0 to the list but as 0 is already present and sets dont take duplicate values nothing will happen. However if d.add(3) was written then d would now be equal to {0,1,2,3}. So during the first iteration of the for loop:- print(d.add(x)) would simply try to add 0 to the set d. However as d.add(x) statement does not produce any string as output 'None' would be printed as a result (it's a default). Same during the next two iterations when x is 1 and 2.",319721.0
87730,364252.0,Please find my answer to the same question in other discusion: https://learn.upgrad.com/v/course/208/question/86439,305845.0
89666,372254.0,"Please try the below code: nums = len(list(map(lambda x:x.split()[0].lower().startswith('s'), input_list))) print(nums)",318084.0
89666,372256.0,"Assuming you want to count the number of words starting with 's'. Firstly you are taking input as input_list(which I assume is a list)-&gt;so you dont need to split a list, the usage of split() is wrong. split() is used to convert a string to list. you can simply do something like: lambda x:1 if x[0].lower()=='s' else 0 Now you put that in a map() with the input list and do a sum() on that. It will count the 1's and give you the total number of words starting with 's'",309451.0
89666,372263.0,"As per your solution you are trying to count the number of words which starts with ""s"". In this case map will not work and map always should return something. You will need to write else statement if you are using map. For you solution, you can use filter, which filters data based on the condition. input_list = [""asdf"",""asdfasd"",""sdfasdf""] nums = len(list(filter(lambda x: x[0].lower().startswith('s'), input_list))) print(nums)",318368.0
89666,372270.0,"The question asks us to count the number of words that start with 'S' in the given input_lis using map() function. Since it has been explicitly mentioned to check for 'S' (capital s) there is no need to apply the .lower() method to convert ot to 's' (small s) and then compare with 's'. Instead we can directly compare with 'S'. Als, there is no need to split each word, as we are just interested in the first character of the word. Hence, no need for applying .split() method. We can directly check using x[0]. Finally we can write a lambda function for implementing the above logic, wrap it in map() so that it gets applied for each of the words in the input list and take a sum() on top of it to get the count. The entire thing can be written as: print(sum(map(lambda x : x[0] == ""S"",input_list))) Hope this helps.",313826.0
89311,370292.0,"There are a lot of approaches to solve the same program. In this question, using join will work great but our instructors also want us to solve it by using reduce function just to force us to think. If you are having problem remember this, reduce will return one thing at the end. Your iterable is the input_list and you have to append it's elements to one another. Write a lambda function that accepts two parameters and join them. Program will look something like this: &gt;&gt;&gt;input_list = ['I','Love','Python'] &gt;&gt;&gt;from functools import reduce &gt;&gt;&gt;my_string = reduce(lambda x,y: &lt;expression&gt;, input_list) &gt;&gt;&gt;print(my_string) I have not written the expression in the lambda function. Give the question a try. If you still need help, i'll tell you the answer.",301652.0
89311,370303.0,"You can obviously use join(), but the quiz specifically expects you to answer it using reduce(). Its for the practice and understanding. In programming you would come across scenarios where you can solve a problem using n number of ways. But you have to chose the most optimal way keeping in mind the space-time complexity. This wont matter in small code snippets but would make a really huge impact on performance in real time scenarios. Happy Coding!!!",309451.0
89311,370473.0,"from functools import reduce input_list = ['I','Love','Python'] print(reduce(lambda x,y : x + "" "" + y, input_list))",317811.0
89311,370288.0,"it can be done by numerous ways, but they have asked to use the reduce function because they are trying to teach you about reduce function and how it is used in problems. so, you can do it like this: answer=reduce(lambda x,y: x+"" ""+y,input_list) print(answer) here, the lambda function will first take the input x,y as the 1st and 2nd element of the list . then return the concatenation of those. this returned value (concatenation of x,y) becomes the value of x for the next iteration and the value of y comes from the next element in the input list . again the concatenation is repeated and result returned which is again taken by x. this goes on, till no element is left in the inpu list, and you are then left with the concatenation of the entire list. hope that helps.",317998.0
89311,392443.0,"There are a lot of methodologies here, but we have specifically instructed to use reduce, hence following the same This is the piece of code i have used reduce(lambda x,y: x+' '+y,input_list[])",319732.0
90781,378320.0,"The filter() function returns an object of 'filter' type which is an iterable. We need to convert it into a list using the list() constructor. Please make the below change and the code should run successfully: list_answer = l ist( filter(mul5, input_list) ) Hope this helps.",313826.0
90781,378338.0,filter() function gives an object whereas the output is expected to be a list. Use list() to convert the object to list as below: print(list(list_answer)) --&gt; This will print list :),318495.0
89812,373220.0,"Hello Payel, It is working fine.",320195.0
89812,373221.0,"I checked your code in jupyter and it is working perfectly fine. Are you running your code in jupyter ? . If yes, you need to give the input_ist in question as input.",318358.0
89812,374287.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_vowel = [item for item in input_list if item[0] in 'aeiouAEIOU'] In your code, it may take more time as it needs to search in many alphabets 'aeiouAEIOU' If you can modify it to save time:- list_vowel = [item for item in input_list if item[0].upper() in 'AEIOU']",317811.0
88679,367256.0,Keep sharing such things ..really interesting to know,319869.0
88679,367231.0,Ohh! that is interesting. thank you for that wonderful insights @Vinay @Amani,317998.0
88679,367234.0,"I would like to add to the above answers. If you still want to use range() function in Python 2.x, write this at the top of the program:- &gt;&gt;&gt; from builtins import range Another trick is to use &gt;&gt;&gt; from builtins import * to use all the functions that might not be compatible in older Python.",301652.0
88679,367178.0,"The xrange() function is available only in Python 2.x and is no longer available in Python 3.x. Infact, the Python 2.x's xrange() function itself has been implemented as Python 3.x's range() function. Check out the what's new section on 3.0 and range() has a special mention https://docs.python.org/3.0/whatsnew/3.0.html#views-and-iterators-instead-of-lists There are a lot of heated debates on whether Python 2's xrange() is indeed any faster than Python 3's range(). Turns out there isn't any difference/significant difference. Checkout some interesting discussions https://stackoverflow.com/questions/15014310/why-is-there-no-xrange-function-in-python3 https://stackoverflow.com/questions/135041/should-you-always-favor-xrange-over-range On running the code found here , Python 3's range() is faster than Python 2's xrange() on my machine.",313826.0
88679,367210.0,"In Python, Both range() and xrange() functions produce the integer numbers between a given start value and stop value. xrange() has been deprecated i n Python3 range() creates a list i.e. range returns a Python list object, for example, range (1,500,1) will create a python list of 499 integers in memory. remember, range() generates all numbers at once. $ python -m timeit 'for num in range(1000000):' ' pass' when we execute above loop 5 times and found that average time per loop with range() is 92.8 millisecond. xrange() functions returns an xrange object that evaluates lazily. that means xrange only stores the range arguments and generates the numbers on demand. it doesn’t generate all numbers at once like range(). and this object only supports indexing, iteration, and the len() function. python -m timeit 'for i in xrange(1000000):' ' pass' when we execute above loop 5 times and found average time per loop with xrange() is 52.3 millisecond. you can find more information on following link: https://pynative.com/python-xrange-vs-range/",317845.0
88679,367399.0,"Found this on Stack overflow: range() vs xrange() in python : range() and xrange() are two functions that could be used to iterate a certain number of times in for loops in Python. In Python 3, there is no xrange , but the range function behaves like xrange in Python 2.If you want to write code that will run on both Python 2 and Python 3, you should use range(). range() – This returns a list of numbers created using range() function. xrange() – This function returns the generator object that can be used to display numbers only by looping. Only particular range is displayed on demand and hence called “lazy evaluation“. Both are implemented in different ways and have different characteristics associated with them. The points of comparisons are: Return Type Memory Operation Usage Speed Memory Operation Usage Speed 1. Return Type : range() returns – the list as return type. xrange() returns – xrange() object. 2. Memory : The variable storing the range created by range() takes more memory as compared to variable storing the range using xrange(). The basic reason for this is the return type of range() is list and xrange() is xrange() object. 3. Operations usage : As range() returns the list, all the operations that can be applied on the list can be used on it. On the other hand, as xrange() returns the xrange object, operations associated to list cannot be applied on them, hence a disadvantage. 4. Speed : Because of the fact that xrange() evaluates only the generator object containing only the values that are required by lazy evaluation, therefore is faster in implementation than range()",310511.0
88679,368209.0,"Simply to put, the difference is 'xrange' returns a generator (xrange object) object that can be used to display numbers only by looping. ' range() ' will create a list of values from start to end. This will become an expensive operation and in terms of memory on very large ranges. ' xrange() ', on the other hand, is much more optimized. it will only compute the next value when needed (via an ' xra nge' sequence object) and does not create a list of all values like range() does. However, an important thing to note that, in Python 3.x, the function ' xrange ' does not exist anymore (Deprecated). The ' range()' now does what ' xrange() ' in Python 2.x, so to keep your code portable, you might want to stick to using range() instead.",311115.0
88679,368256.0,previously we are using xrange() in python 2.7 but know it is changed to the range(),304692.0
88679,368267.0,"Thank you @Srinivasan @Jetendra and everyone. I have read about it, posted it here to share with everyone.",317998.0
88679,368351.0,"Read!!! For the most part, xrange and range are the exact same in terms of functionality. They both provide a way to generate a list of integers for you to use, however you please. The only difference is that range returns a Python list object and xrange returns an xrange object. What does that mean? Good question! It means that xrange doesn't actually generate a static list at run-time like range does. It creates the values as you need them with a special technique called yielding . This technique is used with a type of object known as generators . In this course,you might have seen in the upper right corner of the code stub Python 3. xranage() is not present in Python 3.x. So Python 3.x's range function is xrange from Python 2.x. The range function now does what xrange does in Python 2.x, so to keep your code portable, you might want to stick to using range instead.",304813.0
89342,370636.0,"The issue you are facing is because you must have changed the code stub already given. Hard coding the value will obviously fail in Upgrad console. Make sure you have the initial stub, then write your piece of logic import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) first_name = input_list[0] last_name = input_list[1] Try reloading the language stub and start afresh.",309451.0
89342,370548.0,"Instead of taking list1 = ['Ankur', 'Avik', 'Kiran', 'Nitin'] list2 = ['Narang', 'Sarkar', 'R', 'Sareen'] use the code which is given in the beginning of the file to read a string and convert it into multiple lists. This would enable your code to take input in runtime. Use following to take input during runtime into list_1 and list_2 import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1]",317689.0
89342,370470.0,"name = list(map(lambda x,y : x + "" "" + y , list1,list2)) print(name) This is the correct solution. Try refreshing the page and then submit in console",317811.0
89342,370450.0,"i have tried thesse two solutions : 1. Hard Coded -- Failed in Upgrad Console list1 = ['Ankur', 'Avik', 'Kiran', 'Nitin'] list2 = ['Narang', 'Sarkar', 'R', 'Sareen'] name = list(map(lambda x,y : x + "" "" + y , list1,list2)) print(name) *in Jupyter Console it is Pass and giving the right result. 2. Generic Solution -- Failed in Upgrad Console. name = list(map(lambda x,y : x + "" "" + y , list1,list2)) print(name)",315560.0
89342,370454.0,"* But during ""Run Code "" and Verify both solutions are Passed.",315560.0
89342,370467.0,The generic solution seems fine to me and should have run successfully. Please attach a screenshot of you coding console to check further.,313826.0
89342,370793.0,"Try this: generic: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) first_name = input_list[0] last_name = input_list[1] proper = lambda x,y : x[0].upper() + x[1:] +' ' + y[0].upper() + y[1:]#Write your code here print(list(map(proper, first_name,last_name)))",305335.0
89342,371079.0,you need to remove the unnecessary segment if there is any.i faced similar issue earlier which was resolved on cutting the code short:elegance as the proffessor repeats.if you could paste a screenshot,300684.0
89342,379264.0,"try this first_name = input_list[0] last_name = input_list[1] name = list(map(lambda x,y : x + "" "" + y , first_name,last_name)) print(name)",321850.0
88238,365116.0,"The lambda function is looking for the minimum of the 2 values. The condition is : (x if x &lt; y else y) x will be the output if it is lesser than y else it will return y, which suggests y is lesser than x.",319302.0
88238,364962.0,"Evaluate this min(9999, 9996) lambda(9999, 9996: 9999 if 9999&lt;9996 else 9996) The above is equal to : if(9999&lt;9996): return 9999 else: return 9996",310974.0
88238,365004.0,"since 9999 is not lesser than 9996 ,hrnce it returns 9996..",300693.0
88238,365188.0,"Lambda function which takes arguments,here x and y are the two arguments and its comparing two arguments lambda x,y : x if x&gt;y otherwise y If x = 101*99 and y= 102*98, then the output should be which is if y becauce (102*98).=9996 and x(101*99)=9999 if(9999 &lt; 9996): return 9999 else: return 9996",319444.0
86217,354078.0,"Hi, in the statement return(x*y), you gave only one asterisk, update the statement as return(x**y), you will get the expected output.",314730.0
86217,354083.0,"The problem statement has asked to return x**y (x raised to y) where as you are returning x*y (x multiplied by y). Below code change would fix the issue def squared(x,y): return(x**y)",313826.0
86217,355806.0,"Hi , As pointed out , please use double asterisk (**) to get exponentiation operator. Python operators explained in detail here https://www.w3schools.com/python/python_operators.asp .",305652.0
86217,354116.0,Use ** instead of * * is used for multiplication ** is used for power Hope this helps :),318495.0
89398,370830.0,"You can combine both the statements and print the final result, which is print(sum(list(map( lambda x: x[0]== ""S"",input_list))))",318804.0
89398,370841.0,"Instead use the sum function to the whole function print(sum(list(map( lambda x: x[0]== ""S"",input_list))))",304692.0
89398,370859.0,"There are many ways of accomplishing the same. What you have done is exactly right. Below are few other approches using list comprehension. word_list = [""adfas"", ""dfsdf"" , ""asdfasdf,"" ,""dfgdfg"", ""asdf asdf""] OPTION : 1 (Using startswith function along with list comprehension and then sum of all 1's) print(sum(w.startswith('a') for w in word_list)) OPTION : 2 (Using startswith function along with list comprehension and then lenth of words) len([word for word in word_list if word.startswith('a')])",318368.0
89398,370869.0,"Siv - Filter is right function here, as you would filter all words starting with 'S' and use len() to get count. len(filter(lambda x: x[0]=='S', input_list))",318458.0
89398,370873.0,"map is more meaningful if you want to run some operation on each of the element in list. eg. square of Number, Making first letter of the each word capital.",318458.0
89398,371022.0,"You could further reduce the code as belwo: print(sum(map(lambda x : x[0] == ""S"",input_list)))",313826.0
96566,411545.0,"On the coding console, click on the 3-dots next to ""Python-3"". Then click on ""Load Language Stub"" A window pops up asking for confirmation. Click on ""I'm Sure"". Note:This will remove any code written, so it is advised to copy the code and save it somewhere like on a notepad beofre resetting the code.",313826.0
88320,365478.0,"The filter() function returns an object of type filter . Hence, we need to use the list() constructor to convert the returned filter type object to a list type object. Enclose the filter function in list() as shown below and it should work list_answer = l ist( filter(divid5, input_list) )",313826.0
88320,365477.0,U must convert it to list before printing. print(list(list_answer)) should do it.,315471.0
88320,369197.0,Convert the final output to a list using list() function.,301124.0
89433,371046.0,When I tried your code mentioned above on my jupyter notebook it is working fine. Attached is the screenshot of my notebook. Please share the screenshot of code which you wrote in you jupyter notebook.,317991.0
89433,371047.0,I am able to execute the same code on my Jupyter Notebook. Can you share the error message screenshot from Jupyter Notebook. May be this link will help you - https://stackoverflow.com/questions/31087111/typeerror-list-object-is-not-callable-in-python/31087151. Thanks,311502.0
89433,371048.0,"Seems like u r assigning value to a variable ""list"". Please donot assign the value to list varaiable as it is a function",318358.0
89433,371368.0,"Again when I tried your code mentioned above on my jupyter notebook it is working fine. Then I searched over stackoverflow and found a useful link to understand error "" 'list' object is not callable"". Have a look into it and try to make some changes in your code as per the explanation. https://stackoverflow.com/questions/31087111/typeerror-list-object-is-not-callable-in-python/31087151",317991.0
89433,371362.0,Tried for other code also same error is coming,307496.0
89304,370282.0,"I see that you are hardcoding the input_list. This won't work in upgrad coding console, although its a correct code. You must adhere to the below FAQ: The following code: import ast,sys input_str = sys.stdin.read() is used to read the input from the standard input/output stream. It will be present in almost all the coding questions that are present because when you're writing code in any console, you need to read the input from and write the output to the standard I/O stream. Also, note that the variable name in the second line is 'input_str'. This is because all the inputs provided to the standard I/O stream are read as a string and you need to convert them as required. This will also be done for all the codes. For example, the code in this page requires you to read a list. Hence, the following code is used to convert the read input from string to list: input_list = ast.literal_eval(input_str)",309451.0
89304,370279.0,"dont hardcode your input list. simply read it from the input stdin. the code for reading that would have already been there by default. the code is like this, and you should write your code below that. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) #your code here",317998.0
89295,370248.0,"Could you please share the error. try using below - print(list(map(lambda x : x**3,input_list)))",300691.0
89295,370249.0,"input_list= [5,6,4,8,9] could be the reason. You cannot hardcode the values in DoSelect IDE, it takes the input from the test case. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) //Your code here print(cube)",317845.0
89295,370290.0,"Don't put any input list as input_list= [5,6,4,8,9] in the code as it would be added into the run time using the first few lines pre written defining the input_list to be fetched run time. Also you can write your list cube as following: cube = list(map(lambda x: x**3, linput_list)) print(cube)",317689.0
89295,370257.0,"Although your solution is correct, question states that ""create a list 'cube' which consists of the cube of numbers in input_list"". So you were asked to create a separate list named 'cube' which should contain cube of numbers. But in your solution you are not creating any separate list and just printing the cube of numbers. Thats why DoDelect console is giving error. Make a list 'cube' and then print(cube). Hope this will solve your problem.",317991.0
89295,370442.0,"print(list(map(lambda a : a**3,input_list)))",304692.0
89295,370477.0,"print(list(map(lambda x :x**3, input_list)))",317811.0
91229,381236.0,the last condition should be else and not Else as Python is case-sensitive Ref: All the keyword in Python would be highlighted in green,312259.0
91229,381244.0,'P' in Print should be in small case 'p'.,311502.0
91229,381234.0,python is case sentive try else: rather than Else:,306735.0
91229,381579.0,"You can find usage of print and if-else statement as follows: &gt;&gt;&gt; x = int(input(""Please enter an integer: "")) Please enter an integer: 42 &gt;&gt;&gt; if x &lt; 0: ... x = 0 ... print('Negative changed to zero') ... elif x == 0: ... print('Zero') ... elif x == 1: ... print('Single') ... else: ... print('More') You can see how print is used and how if-else is used. All are in lower caps. Modify your code as per the given syntax and your code file run fine. Hope this will help.",317991.0
91239,381277.0,"In Python, conditional statements like If- Else or For loops need to follow indentation syntax. As you are using two if statements use correct indentation to align the conditions. Refer to Pre-Launch prepatory program : Control Structures : If - Else - Elif",317460.0
91239,381256.0,"In python, the blocks of code belonging to a if-else condition, for-loop, function etc. are decided by the indententaion level. Please check the below link for a clearer understanding of the importance of indentation in python code. https://www.programiz.com/python-programming/statement-indentation-comments In the above code, you are getting the indentation error because the python interpreter is expecting some statement(s) that should be executed after the for loop, but note that the statement(s) should be indented (atleast by one space) to be considered as a part of the for loop. Hope this helps.",313826.0
91239,381439.0,"as the error suggests, al the statements that belong UNDER the for statement/loop need to be indented slightly (in other programming languages we might use brackets and braces instead, but in Python we use indentation to decide what code is executed under which control and flow structure)",300694.0
91239,382013.0,"in Jupyter notebook, indentation doesn't happen automatically unlike other such as pycharm etc. do I have to align everytime I am writing code in it??",317558.0
91239,382343.0,The first if not placed inside for. there itself it is goind wrong.,317811.0
91239,382318.0,The error is due to missing indentation for the code block within the for loop.,319302.0
90738,378162.0,The extra None should not appear. You havent shared your code for me to comment on what is wrong.,318084.0
90738,378166.0,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) x = int(input_list[0]) y = int(input_list[1]) def squared(a,b): print(a**b) print(squared(x,y))",303115.0
90738,378173.0,"Please drop ""print"" in ""print(squared(x,y))"" and try, it works. Check the screenshot",310585.0
90738,378342.0,"You're getting none because your function is not returning anything thing i.e return statement is missing in your function. To resolve either add the return statement in your function i.e return x*y or just use function call instead of print statement in the end. i.e squared (a,b)",318495.0
90738,378313.0,"In python, if a function does not have an explicit return statement, then it retunrs a 'None' type object by default. print(squared(6,7)) -- Makes a call to the squared function passing 6 and 7 as the arguments. squared() function prints the value 6**7 and returna a 'None' type object back to the line print(squared(6,7)) and hence 'None' gets printed again. You can resolve this in two ways: def squared(a,b): return a**b print(squared(x,y)) def squared(a,b): print(a**b) squared(x,y) Hope this clarifies.",313826.0
95134,402014.0,I found the issue issue was if aplhabet in 'aeiou': instead of alphabet I typed aplhabet in the line if aplhabet in 'aeiou':,314629.0
95135,402041.0,"Here, you would have got an error as KeyError : 1 . Which says value doesn't exist. if val[1]&lt;25 means you are trying to pick 2nd value from each key value pair. But we you consider the Data 2:{'divya':20}, it is declared as a dictionary where the 2nd value is not present. So instead of declaring the values as dictionary, use the list data={1:['shruthi',27], 2:['divya':20]} Hope this helps",318804.0
95135,402011.0,"The mistake is in data variable as the second value is not array and it is dictionary and cannot be accessed by val[1] so change your second entry to list. The correct code would be :- count=0 data={1:['shruthi',27], 2:['divya':20]} for key, val in data.items(): if val[1]&lt;25: count+=1 print(count) The difference is from data={1:['shruthi',27], 2:{'divya':20}} to data={1:['shruthi',27], 2:['divya':20]}.",320685.0
95197,402317.0,small correction - words=[word for sentence in paragraph for word in sentence.split() if word[0][0].lower() in vowels],311502.0
95197,402386.0,use .lower(),312490.0
95197,402539.0,"in place of vowels=['a','e','i','o','u'] you can use vowels='aeiou' and word[0].lower() the complete code: paragraph=[""this a beautiful day"", ""Thank god everyday for the things that you have"", ""Indigo am good girls"", 'bangalore weather is awesome'] vowels='aeiou' words=[word for sentence in paragraph for word in sentence.split() if word[0].lower() in vowels] print(words)",317845.0
95197,402280.0,"Try this code - paragraph=[""this a beautiful day"", ""Thank god everyday for the things that you have"", ""Indigo am good girls"", 'bangalore weather is awesome'] vowels=['a','e','i','o','u'] words=[word for sentence in paragraph for word in sentence.split() if word[0][0] in vowels] print(words) Thanks.",311502.0
95197,402843.0,You need to use word[0].lower(). As word[0].lower would not return any output in this case.,317689.0
95197,403182.0,"Your concept is correct ! just use below with lower() paragraph=[""this a beautiful day"", ""Thank god everyday for the things that you have"", ""Indigo am good girls"", 'bangalore weather is awesome'] vowels=['a','e','i','o','u'] words=[word for sentence in paragraph for word in sentence.split() if word[0].lower() in vowels ] print(words) Gives this: just try to read diff between lower and lower() ['a', 'everyday', 'Indigo', 'am', 'is', 'awesome']",304813.0
95197,404621.0,"You need to add lower() i.e. brackets after lower or upper function , only then this will work , you are missing that in your code.",315560.0
112616,,nan,
112413,,nan,
112334,,nan,
111211,,nan,
111402,485327.0,first collect appropriate data from the resources given..each q requires different set of data,317982.0
111402,485356.0,Do we have to apply any Machine Learning Concept such as logistic Or We can predict the team based on EDA analysis. ? Which approach to follow and idea,306243.0
111402,479818.0,"Hi, There is an entire course dedicated to the challenge. It has all the details. Please have a look at course 6.",302738.0
111402,479855.0,"The problem statement has Result Expected very much straight forward about what needs to be done. 1. Basically, you need to perform PCA on the dataset in order to obtain the Principal Components, 2. Perform Clustering on the countries using a suitable number of components and 3. Show the clusters on those components as well as a few selected variables. All these has been covered in the module, also you can go through the discussion forum question regarding this assignment, it will also help you to move forward. Hope this helps.",317991.0
97057,414357.0,"#Demo of using strip method str_a = "" Python is Outstanding "" str_b = str_a.strip() print(""The Original String:"", str_a) print(""With strip method:"", str_b)",318476.0
97057,414362.0,"Hi, can use- df.columns = pd.Index(map(lambda x : str(x)[:0], df.columns)) or df[:1,].str.replace("" "","""")",305129.0
95506,403966.0,"I believe, this occurs only in the latest version of Ipython jupyter notebook.",311160.0
95506,404007.0,"If you use %matlplotlib inline then plt.plot() should populate the plot for you inside the notebook. Otherwise, you'll have to explicitly call the plotted figure by using plt.show(). Try it out and let me know if that helped.",308962.0
95506,404270.0,This is same behaviour in jupyter notebook as anyother variable be it numpy array or any integer. In notebook if you create a pandas object and call it with name and than do a df.head(),317689.0
95506,405658.0,"I am also observing the same, in my note book, don't have to use plt.show(). Also plt.show() don't show anything.",301113.0
95506,405763.0,"In the latest version of Jupyter and Python , show command is not needed.",315560.0
95522,404021.0,"Sorry, got the answer in the second example of the subplot. It was mentioned in the comments # Optional command, since matplotlib creates a figure by default anyway",301114.0
95522,404052.0,"If believe, if you plan to create only one figure you can skip the figure command. in case you plan to create multiple combination of charts, it would be good to use figure command and specify which one you are working on right now. Also figure command as its own arguments which can be useful if you plan to design your chart area: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html",306725.0
95522,405650.0,"A figure means the whole picture in which all the plots are shown. if you want only one plot you can leave it, but it is highly used in subplots to show that all the subplots are going to be shown in only one picture or more than one.",315560.0
96142,408882.0,Uddhav - Box plot is right plot in this case. Did you attend today's live Expert session? If not use below link - https://www.youtube.com/watch?v=7TmoGOv1TB4 (fast forward to 24:25 min where he explains box plot) The expert explains: Data points lies outside (Q3 + 1.5 * IQR ) and (Q1 - 1.5 * IQR ) are treated as outliers. Hope this helps.,318458.0
96142,408827.0,"Ideally to find the outliers and know the spread of data, we use Box plots.. whereas Histogram is to analyze the frequency of the variable.. the count of appearance of data within a band.. it won't tell you the exceptional data points (outliers) since it will be a single bar colored completely.. Scatter plot would also be of help here.. but again to know the statiscal data median and percentile where your data lies better plot would be box plot.. hope this helps.. :)",316349.0
96142,408839.0,"Specifically for identifying outliers , scatter plot s the most preferred one.",318340.0
96142,408843.0,Please note the key question here - how to create a scatter plot with one variable?,318762.0
96142,409016.0,Uddhav - We were right. Correct answer is Box plot. I reported it as error &amp; it is not corrected. Please check below.,318458.0
95548,404238.0,Below link might be of your use to solve your problem. Please have a look into it. https://stackoverflow.com/questions/24120023/strange-error-with-matplotlib-axes-labels https://stackoverflow.com/questions/19442060/matplotlib-pyplot-titlestring-returns-error/29297334 Hope this will help.,317991.0
95929,406679.0,"You have a valid point. However, I went with the only option which would execute successfully and also with the assumption that ""blue line with circles"" means an imaginary line of blue color with circle as marker for the points.",313826.0
95929,406992.0,"In 'bo', b stands for blue and o for circles. '-' is not required.",304319.0
95929,407320.0,"Even i also did the wrong answer in 1st attempt, as it is mentioned in the question as ""blue color with circle"", i thought ""circle"" should be written as staement. And ""o"" &amp; ""O"" was not not clarified to me.",311117.0
95929,407449.0,"there was a link in the question to read how to use colour,marker before attempting the question",318005.0
95569,404405.0,"plt.subplot(3,3,7) A plot of array of size 3x3. Now, 7, you need to count from top right until you the come to 7 which is 3rd row 1st column.",310974.0
95569,404479.0,"(3,3,7 ) means plotting an array of 3,3 [1,2,3 4,5,6 7,8,9] 7 is the 1st sub-plot from the left in the last row.",317689.0
95569,404939.0,Hey guys! thank you for your answers. I got the clarification.,310179.0
95591,404567.0,It is used to get the current x and y limits and also to set the minimum and maximum points of x and y axis,318329.0
95591,404672.0,"plt.axis([20,80,200,800])",313826.0
95593,404563.0,"Box plot is used to know the percentiles, median and the outliers while scatter plot is used to know the distribution across from which you can find concentrations.",318329.0
95593,404572.0,Box plot is helpful in understanding distribution(range). Scatter plot is generally used for 2 variables and helps to understand the correlation between the two.,306248.0
95593,404577.0,You can go through below link to get the difference between box plot and scatter plot. They gave very good and detailed explanation. https://www.slideshare.net/JuranGlobal/box-plots-vs-data-plots-63349549 Hope this will help.,317991.0
95593,404889.0,"Box-plot: A box plot is also called a box and whisker plot and it's used to picture the distribution of values. When you use a box and whisker plot you divide the data values into four parts called quartiles. You start by finding the median or middle value. The median splits the data values into halves. Finding the median of each half splits the data values into four parts, the quartiles. Each box on the plot shows the range of values from the median of the lower half of the values at the bottom of the box to the median of the upper half of the values at the top of the box. A line in the middle of the box occurs at the median of all the data values. The whiskers then point to the largest and smallest values in the data. scatter plot: A scatter plot is an excellent tool for comparing pairs of values to see if they are related. Scatter plots are frequently used by researchers. Let's pretend that you are researching shopping trends; you want to know when during the day people shop in your store. You station someone at each door to count how many people enter the store at 15 minute intervals. So you count how many people come in between 9:00 and 9:15, 9:15 to 9:30 and so forth. After the survey you plot the data for a 3-hour period which gives you 12 points on the scatter plot based in 15 minute time slots. Let's say the scatter plot looks like this for data gathered in the afternoon. There doesn't seem to be any relationship between the time period and the number of shoppers.",317845.0
95593,404688.0,"You question says you have wrong understanding. It is not the scatterplot to know the distribution of single variable. It is histogram that is used to know the distribution of data in a single variable. A bar graph displaying how many values fall into a set of intervals. 10 values in the range of 10-20, 8 values in the range of 21-25, 4 values in the range of 26-40. The show the spread of data directly but not in percentiles (that help showcase the dominance and umimportance). Box plot is helpful when one would like to get the distibution or spread of values in a quantitative measure. In other words, percentiles, quantiles, outliers etc. 'Odd man out', 'the most common', dominance and umimportance, kind of things are easy to find using box plots. Scatter plot is a nicer way to visually help how the values are scattered. In this case, numbers (like in earlier cases) are diffiicult to project visually but multiple dimensions can be displayed using shapes and colors.",318007.0
95599,404556.0,where is y initialization?,318329.0
95599,404562.0,Your x variable list has 30 items and your y variables list has 20 items. You need to have the same count of items for x &amp; y.,306248.0
95610,404633.0,its just a identifier used for differentiation and 2 2 1 gives that in a 2*2 matrix the plot will be present at 1st position.,318017.0
95610,404641.0,"Hi Vinay, In the notebook provided, the mentioned subplot 3 is actually subplot 1 of figure 2 and subplot 4 is subplot 2 of figure 2. Please dont get confused by the comments provided in the notebook. Refer the bold parts below and update the same in your notebook locally so that you do not get confused locally. Do get in touch if any other clarity is required. plt.figure(1) # figure 1 subplot 1 plt.subplot(2, 2, 1) plt.title(""Linear"") plt.plot(x, x) # figure 1 subplot 2 plt.subplot(2, 2, 2) plt.title(""Cubic"") plt.plot(x, x**3) # figure 2 subplot 1 plt.figure(2) plt.subplot(2, 2, 1) plt.title(""Log"") plt.plot(x, np.log(x)) # figure 2 subplot 2 plt.subplot(2, 2, 2) plt.title(""Exponential"") plt.plot(x, x**2) plt.show()",318479.0
95610,404744.0,"You are right, it doesn't make sense. It should be plt.subplot(2, 2, 3) &amp; plt.subplot(2, 2, 4) for sub plots 3 and 4.",310974.0
95610,404662.0,"Answering your 1st question. No it is not incorrect. 1) Please dont get confused by the comment #subplot 3 and #subplot 4. They are actually subplots 1 and subplots 2 for figure 2 2) Writing the command as plt.subplot(2,2,3) will result in a 3rd subplot for the same figure. For example, for plt.figure(1), by using plt.subplot(2,2,1) and plt.subplot(2,2,2) plot can be created in the 2x2 matrix but only col 1 and col 2 of row 1 is utilised for displaying the subplots. Similar case for figure 2 subplots as well. This can be because if in future additional subplots, if required, can be added by utilising the row 2. You can refer the example given below where additional subplots are added for plt.figure(1) and plt.figure(2). I hope I have clarified your 1st question. Answering your 2nd question. 1) Let me give you the bigger picture of figures. You can think of it like a container for, you can say, similar kind of subplots or subplots under a same head. 2) By defining subplots under a figure you can later come back and do any modification required for that subplot under that figure. In the example below you can see that I again modified the subplot 2 inside figure 2 to also include the exponential plot in red on the -ve x axis. The original plot is modified to include the change as well. I hope I have clarified your second question as well. Ex. plt.figure(1) # figure 1 subplot 1 plt.subplot(2, 2, 1) plt.title(""Linear"") plt.plot(x, x) # figure 1 subplot 2 plt.subplot(2, 2, 2) plt.title(""Cubic"") plt.plot(x, x**3) # figure 1 subplot 3 plt.subplot(2, 2, 3) plt.title(""Square"") plt.plot(x, x**2) # figure 1 subplot 4 plt.subplot(2, 2, 4) plt.title(""Square Root"") plt.plot(x, x**0.5) plt.suptitle(""Figure 1"") # figure 2 subplot 1 plt.figure(2) plt.subplot(2, 2, 1) plt.title(""Log"") plt.plot(x, np.log(x)) # figure 2 subplot 2 plt.subplot(2, 2, 2) plt.title(""Exponential"") plt.plot(x, np.exp(x)) # figure 2 subplot 3 plt.subplot(2, 2, 3) plt.title(""Cube root"") plt.plot(x, x**(1/3)) plt.suptitle(""Figure 2"") # switch back to figure 2 subplot 2 and make some changes plt.figure(2) plt.subplot(2, 2, 2) plt.title(""Exponential"") plt.plot(-x,np.exp(x),'r') #plot on -ve x axis in red colour plt.show() #closes all the open figures which are not to be used later. plt.close('all') #Specofic figures can be closed as plt.close(1) Figure 2 subplot 2 after adding changes",318479.0
95639,404893.0,Use the plt.tight_layout() function just before the call to plt.show().,313826.0
95639,404899.0,"You can use plt.tight_layout(). tight_layout automatically adjusts subplot params so that the subplot(s) fits in to the figure area. Here is a cluttered plot: Now add plt.tight_layout() to the code : Also you can check matplotlib.pyplot.subplots_adjust -might help as well, https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots_adjust.html https://matplotlib.org/users/tight_layout_guide.html",309451.0
95993,407328.0,"Yes you can say that , the basic Matplotlib syntax and MATLAB sytax is very similar.",317991.0
95993,407431.0,That's a good question. Many people have that question. Thanks for bringing it up. I request you to read this article on this - https://www.reddit.com/r/Python/comments/5n0etx/matplotlib_vs_matlab_plotting_tools_discussion/,318458.0
95661,404948.0,suppose you have figure of 3X3. so in that fig there would be 9 subplots. e.g there position can be understand in the below diagram of metric 1 2 3 4 5 6 7 8 9 so question is asking which subplot can not be be placed at 2nd or 5th or 8th index.,312746.0
95661,408389.0,"Let me try to explain this with another example. When you have 3X4 Matrix with a representation of Alphabets in sequence. Then the below data will be available in the Matrix. A B C D E F G H I J K L Then it means, that you have 2nd column in the matrix as 'B', 'F', 'I' &amp; their positions are 2, 6, 10. For the module question, try to get the position of the subplot for each option provided &amp; check whether it fits into 2nd column of the subplots. Hope it is clear !",310518.0
96014,407502.0,it might be as the pyplot is in interactive mode or some show command has already been used befor this code was executed in some previous code,318017.0
96014,407916.0,"I think, the plot is still saved in the memory. I also faced the same problem. try plt.close(), plt.clf() for closing the plot and figure and see if that helps.",318084.0
95692,405346.0,"plt.plot(x,y,'bo') gives you a plot object which will have a x array and a y array the marking will be a blue circle . plt.show() will give you the plot of the above command",318017.0
95692,405145.0,"These two are different commands and you have to use them in separate lines as follows: plt.plot(x,y,'bo') plt.show() And if you want to keep both command in same line you can use semicolon "";"" as follows: plt.plot(x,y,'bo'); plt.show() Hope this will help",317991.0
95692,405272.0,"the line plt.plot(x,y,'bo')plt.show() contains two commands. defining the plot and showing the plot Normally in any programming language, you will have to use a separator or new line for every instruction. python supports "";"" as a separator. so you can use plt.plot(x,y,'bo');plt.show() or plt.plot(x,y,'bo') plt.show()",317845.0
95692,405594.0,"plt.plot(x,y,'bo') gives you a plot object plt.show() will gives plot a command",306996.0
95353,403369.0,Thanks.. Got it resolved.. Matplotlib inline had to be used. Is there any other way like Jupyter notebook settings?,300698.0
95353,404006.0,%matplotlb inline will make your plots visible inside the notebook,308962.0
96287,409841.0,"In short, the values that you are going to use are not the original ones but the logarithmic value of it like log(x). This is to ensure that you don't lose the data if it's scale is very less compared to higher values in the same data set. More reading here https://en.wikipedia.org/wiki/Logarithmic_scale",310974.0
96287,409848.0,"You might have noticed the range of data points when plotted for Raw data, it was completely scattered and couldn't be analyzed properly. To bring such data in an organised manner we plotted it on lograthmic scale, simply arranging your Y scale in such a manner that your data points are showcased properly on the scale of 10 to the power.. You'll see the Y scale as 10, 10 to the power 1, 10 to the power 2 and so on.. This is just for the sake of similicity in reading you data on the plot.. Hope this helps.. :)",316349.0
96287,409949.0,Umesh - Please check link below. It is nicely explained there how log scale is very useful when data is skewed with help of scatter plot. http://onlinestatbook.com/2/transformations/log.html,318458.0
95469,403742.0,figure is not mandatory for creating plots and a default size is chosen if you don't specify it,318329.0
95469,403847.0,It is a good practice to create the figure first and then create the sub-plots within that figure.,306011.0
95545,404199.0,"Imagine subplots fitting in a plot as matrix (4,4,1) now if u are defining 1st sub plot then it will be(1,1,1) or (111) here we enter next row after filling all columns in a row,but while defining subplots u can define in any order,hope this helps.",301115.0
95545,404256.0,"Hi Karthik, First let me correct one mistake. subplot(121) means a figure with one row and two columns, and it is the first subplot. subplot(122) means the second subplot of the same figure. Let me give you more clarity on this. 1 ) A figure can contain multiple subplots. 2 ) Number of sub plots is defined as size of a matrix. eg, (1,4) means 4 subplots from left to right, (2,2) means 4 subplots two above two below like cells of a matrix. 3) There can be multiple figures. A figure can be said as a container of subplots. Refer below example (pls dont mind the cluttered plots) Reference: https://matplotlib.org/gallery/subplots_axes_and_figures/figure_title.html",318479.0
95545,405762.0,"Subplot(1,2,1) means 1 row 2 columns and subplot 1 of the figure. Likewise if you have another subplot you can write (1,2,2) , that means 1 row, 2 columns and this subplot belongs to second column, therfore last 2 defines second subplot of the same figure.",315560.0
95940,406748.0,restart your machine and check again if it repeats check the background of your machine.,318017.0
95940,406758.0,"After restarting my system and reopening Jupyter , i need not to do anything. It just imported matplotlib",313767.0
95940,406902.0,always better to use pip to install dependencies in python try:: pip install matplotlib seaborn do remeber to open the CMD windows in administrator mode,312731.0
95940,407301.0,"Hi Neerajakshulu, I also faced same issue while importing matplotlib and seaborn . If the zip file from upgrad page is downloaded and unpacked/extracted again and re-run it should work fine. If you are still facing issue then it could just be the Jupyter notebooks known issue where you have to cloase &amp; re-run all the commands of .ipynb file all the way from top. Hope this helps you.",310508.0
95940,407419.0,upgraded anacoda to latest version and launched jupyter notebook https://jupyter-notebook.readthedocs.io/en/latest/security.html linked helped me to resolve issue. Now I am able to run.,315455.0
95940,407513.0,Try installing using pip or sudo pip (if you dont have admin permissions?),300708.0
95499,403936.0,"Subplot is used to plot two or more plots in one figure. Subplot(m,n,p) divides the figure into mxn matrix of sub-windows and selects the p-th sub-window for drawing the current plot. The number of a sub-window into the matrices is counted row by row ie the sub-window corresponding to element (i,j) of the matrix has number (i-1)*n + j. Example subplot(2,2,3) means 4 plots - 2 across rows 2 across columns and current plot will be in second row first column subwindow.",311254.0
95518,404106.0,Its an SVG embedded inside a template. useful links: https://css-tricks.com/lodge/svg/ https://blog.theodo.fr/2017/01/integrating-and-interacting-with-svg-image-files-using-twig/,310508.0
95518,404012.0,My guess is that they have been done in D3.js,313826.0
95518,404050.0,"There is some usage in SVG , CSS in D3.js maybe.",301648.0
96991,413880.0,tableau is a reporting tool and python can be used for both analysis and graphs so its good to use python but yes you can do things in tableau and excel as well,318017.0
96991,413882.0,"All the analysis that we do using Pandas is complex enough to do the same with Tableau. Data cleansing, EDA and visualization. All works best with Python",318329.0
96991,414286.0,"I agree with your point that Tableau is much more robust and has lot of variance. And its upto you what to use for Python or Tableau. But What I think is the purpose of learning Python visualization is different. Suppose you are doing data analysis on a dataset in python. You performed data cleaning and then you want to get the spread of data, what would you do - 1. Plot boxplot in Python then and there itself or 2. Export dataframe and import it into Tableau and then visualize. Which one is time saving.? Now suppose you need to perform analysis for 10 dataframe in Python. Would you still export dataframe and visualize in Tableau or would you use Python Visualization.? So I think purpose of learning Python visualization is to get acquainted with it and use it when required. Hope this will help.",317991.0
96991,414377.0,"Evereyone has its own advantages. As Tableau was developed for the data visualization only and it performs its function very well. It is a tool which is used for data visualizations only. Whereas the Python has no limitations, we can use it for data clening, data mining, data anallysis, data wrangling etc almost for everything related to data. Although I agree with the point that tableau is more robust but when we are using python, everything is at one place. We don't have to install multiple tools for each operation.",318532.0
96991,414633.0,"I had the exactly opposite question: Why to learn Tableau when I can do all sorts of visualizations in Python? Python and Tableau have their own pros and cons. Pyhton can be used for data-cleaning, EDA,ML Algos, Visualizations, etc. All the activities required during the lifecycle of a data science project can be achieved in Python. One more advantage is that all these libraries are open-source, meaning you need not pay anything for using these. However, generation of visualizations requires programming knowledge and the quality of visualizations is not on par with Tableau. Tableau produces visualizations with easy drag and drop feature, selection of various types of plots etc. This makes it very easy to use even for people with less/no technical background. The quality of visualizations is also very good and features like dashboards, stories help in generation of presentations which can be direclty delivered to an audience comprising of Business oriented people like senior management, business analysts,domain experts etc. whitout going into the nitty-gritties of complex code. However, this comes at a price since you need to have a Tableau License. So, the choice of the tool to be used for visualizations would depend on various factors like the need of analysis, the audience to whom the prsentation would be done, availability of Tableau License, etc. But since these may vary from project to project (or org t oorg) the expectation is that we should be in a decent position to generate visualizations(atleast the basic ones) using either Python or Tableau.",313826.0
95511,403976.0,pre attentive are the attributes such as length size color which we use for example in tree map and attentive attributes are numericals units like how many times kohli scored 30 runs its easy to represent pre attentive attributes rather then attentive attributes,318017.0
95511,404180.0,"The whole point of doing visualizations is to enable the reader to see what we want them to see before they even know. So if you want to draw the eyes of your audience, you can change the characteristic of the element you want people to see. You can change the size, the color, orientation, shape, line, length or width, you can put in an enclosure, intensity, or spatial position. All of those are preattentive attributes.",317689.0
95294,403082.0,it was just for example purpose I think.,318329.0
95294,403137.0,https://cdn.upgrad.com/UpGrad/temp/85846295-9b6c-49d3-a325-d25346ee94ae/Capture.JPG,318329.0
95294,403540.0,This is just for illustration purpose only and just an image.,317689.0
97256,416424.0,Thanks for sharing Sweta.,334535.0
96963,413698.0,"use below code : fig = plt.figure() fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold') ax = fig.add_subplot(111) ax.boxplot(data) ax.set_title('axes title') ax.set_xlabel('xlabel') ax.set_ylabel('ylabel')",300735.0
95776,405728.0,"Hi Chetan, The code that are running is correct. You may want to re-run the ""Jupyter Notebook"" from the begining. If you were getting correct plots for previous commands such as example below. (this is just before what you were trying to run) Then it could just be the Jupyter notebooks known issue with re-running all the way from top. Hope this helps you.",310508.0
95776,405660.0,Chetan - please check your main dataframe..I guess your 'Sales' values seem to have affected.. the actual plot is like this which correlated correctly with the given answer 30000 to 40000;,316349.0
95778,405699.0,Figsize basically changes the size of tiles. its Just the short form of figure size.. Try changing the size you will see the size increase or decrease as per the dimensions you choose.,305129.0
95778,405684.0,it makes the display page of that much size as given in (),318017.0
95778,405688.0,"It makes the plot of size 10,8. You can change the size to (12,10), etc to know the difference.",303229.0
95778,407319.0,"Hi Chetan, Figsize resizes the entire width &amp; height as per the instruction given figsize =( , ). Based on this when you make the heatmap, it will stretch &amp; re-organize the tiles to fill in the available space given in the command. Ref images below: Hope this helps you..",310508.0
95778,407315.0,"Change of size gives clarity on data, as it also shows the very small details. Go thgrough the videos lecture, understand this.",311117.0
95787,405725.0,Use distplot function and in argument mention bins=50,318429.0
95787,405766.0,You can always use Shift+Tab keys to know the arguements the function would support by pointing the cursor inside the function call. It gives you the brief explanation of what each arguement does.,311160.0
95791,406085.0,"Hi Rajani, Please go through all 3 modules https://learn.upgrad.com/v/course/208/session/19859/segment/113407 https://learn.upgrad.com/v/course/208/session/19856/segment/101022 https://learn.upgrad.com/v/course/208/session/19857/segment/101028",334535.0
95796,406004.0,You can supress the warning by using the below at the top of your page: import warnings warnings.filterwarnings('ignore') Hope this helps.,319302.0
95796,406006.0,"I also got the same msg. Above code supressed the warning, but what is the meaning of that warning... can any one answer for it .",301113.0
95796,406018.0,This is what i found on stackoverflow and seems it is related to some future version changes: https://stackoverflow.com/questions/52110869/futurewarning-using-a-non-tuple-sequence-for-multidimensional-indexing-is-depre Will dig some more to see if can find more details.,319302.0
95823,,nan,
95827,406100.0,hi aditya i felt the same please go through the below link it might help you https://youtu.be/6gdNUDs6QPc,318017.0
95827,406906.0,"Think viscualisations are mostly few lines (2-3 atmost) of code, and we need to know the different types and the relevant syntax in Python. Just like Excel, we need to know the types of plots avaiable for us and when to use them. Think the course is pointed in this direction.",312731.0
95822,406093.0,Sales is on the x axis in your plot. you probably should be using xscale('log') instead of yscale. alternatively you can orient your box plot in the vertical direction in seaborn.,319898.0
95822,406108.0,Try the following code which plots the sales values using logarithmic scale on the vertical ( y ) axis sns.boxplot(y=df['Sales']) plt.yscale('log') plt.show(),313826.0
95822,406125.0,"Hi, Yes, it depends on how you want to see the figure. Like in the above fig.sales is on the x axis. That is why the box plot is shown horizontally instead of vertically. So in this case if you want to see the figure more clearly, set the x axis as log. Or else set the y axis as sales like sns.boxplot(y=df[‘sales’]) and then try to change the yaxis scale to log as you have written above..",305129.0
95860,406240.0,"I'm guessing you are talking about the plot. Well, there is no pre-set image. The image is being built accordng to the data that is passed.",310974.0
95860,406252.0,image can be built using the matplotlib using the proper dataset and commands in python,318017.0
95860,406289.0,Based on provided data Matplotlib generates the plots at run time. You can read more here https://matplotlib.org/users/pyplot_tutorial.html,317845.0
95860,406291.0,The image being read is missing in the folder directory of the notebook. Hence it is giving an error.,317689.0
95889,406452.0,This is happening because the Label of the subplots is overwriting the x-axis of other subplots in the same column. You can use sns.tight_layout() function just before the plt.show() function to generate cleaner plots.,313826.0
95889,406470.0,,314313.0
95889,410402.0,You need to use plt.tight_layout() as its a function of matplotlib,306725.0
97103,415031.0,Looks like there are some decoding issues. Try viewing the dataset independently and try using the codec that can import the data properly,318329.0
97103,415043.0,"There are a few changes that you need to do : Pass the path as raw string by suffixing r before the quotes in pd.read_csv() call. Also, you need to assign it to a variable. Ex. df = pd.read_csv(r""C:\Users\...\global_sales_data\market_fact.csv"")",313826.0
95626,404798.0,"are you referring to this - Anaconda3\lib\site-packages\matplotlib\axes\_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg. warnings.warn(""The 'normed' kwarg is deprecated, and has been "" It seems to be a seaborn warning.... Seaborn causes the warning. Nothing to do here, seaborn patch should be released soon.",311857.0
95626,406084.0,"asking to use distplot , you can also use sns with distplot i.e seaborn with distplot to make subplots(rugplots).",315560.0
95715,405323.0,"A very good question Arpit. Though I was also not aware about usage of this two. So I searched for this and found few links useful. Here is the brief discription from the link. Pearson's correlation coefficient r with P-value. The correlation coefficient is a number between -1 and 1. In general, the correlation expresses the degree that, on an average, two variables change correspondingly. If one variable increases when the second one increases, then there is a positive correlation. In this case the correlation coefficient will be closer to 1. For instance the height and age of children are positively correlated. If one variable decreases when the other variable increases, then there is a negative correlation and the correlation coefficient will be closer to -1. The P-value is the probability that you would have found the current result if the correlation coefficient were in fact zero (null hypothesis). If this probability is lower than the conventional 5% (P&lt;0.05) the correlation coefficient is called statistically significant. Below are the links to get more insight. https://www.medcalc.org/manual/correlation.php http://www.eecs.qmul.ac.uk/~norman/blog_articles/p_values.pdf Hope this will help you.",317991.0
96833,413004.0,"I got my answer. It is in https://seaborn.pydata.org/introduction.html? set a xlim as follows: sns.distplot(df['column'][:200], rug=True).set(xlim=(Lower bound, Upper bound)) eg: sns.distplot(df['column'][:200], rug=True).set(xlim=(0,100))",309211.0
95785,405715.0,You can ignore this warning. The existing one is going to depricated in seaborn. This warning will not effect your results.,318846.0
95785,406009.0,Can anybody explain how to disable this warning in the jupyter notebook? I see there are no such warnings in professor's notebooks.,311115.0
95785,406143.0,Thanks Anand.,311115.0
102897,440126.0,"it shows the frequency of occurrence of each value.. if you've many distinct values, it automatically tries to club them in a cluster and plota it accordingly.. i hope it clarifies your doubt..",316349.0
102897,440114.0,Distplot is a histogram where the y-axis represents the frequency of each value. The total value of all the frequencies will be equal to 1.,318804.0
95492,404071.0,"I assume you are aksing from the perspective that the Home Segment in Technology category is shown only as a ""Line"" in the ppt. If you run the code in python you can see that it is a profitable category under Home Segment. Hope this helps...",318605.0
95492,404829.0,"If you refer to the corresponding jupyter file you will find the following statement - ""On an average, only Technology products in Small Business and Corporate (customer) categories are profitable. "" So in home segment the median profit is zero.",311857.0
95949,406868.0,"Thanks. Used lineplot() for the same. Infact lineplot() is much easier to use than tsplot(), as we need not create the group by dataframe.",304319.0
95949,406861.0,"These are just warning that says use ""lineplot"" instead of ""tsplot"" as it has been deprecated. In July 2018 , lineplot has been introduced and it has all the capabilities that tsplot has. So if you use tsplot in production code then it will create problem. So they are giving warning to not use tsplot any more. You can get more insight from below link: https://seaborn.pydata.org/whatsnew.html So for the assignment purpose you can still use tsplot, but for production and all other codes skip using tsplot. Hope this will help.",317991.0
95800,405856.0,"plt.figure(figsize=(10,8)) plt.subplot(121) sns.barplot(x='Product_Category',y='Sales',data=df) plt.title(""Avg Sales"") plt.subplot(122) sns.boxplot(x='Product_Category',y='Sales',data=df,estimator=np.median) plt.title(""Avg Profit"") This is the code",318846.0
95800,405914.0,"Hi I the second code you are using below code: sns. boxplot (x='Product_Category',y='Sales',data=df,estimator=np.median) you have to use barplot in place of boxplot . Do the above changes then your code will work.",320073.0
95800,405912.0,"sns. boxplot () function does not take any argument by the name ""estimator"". I think you intended to use the sns.barplot() function which has the named argument estimator. Please refer to the official documentation here: Barplot Boxplot",313826.0
95937,406750.0,time series will have date or month or anything which is realted to time on x axis please see that if you are passing one of the mentioned thing for x -axis,318017.0
95937,406836.0,"Seaborn does not support labelling the x-axis to a more readable DateTime value as mentioned by the creator of Seaborn in this link: https://github.com/mwaskom/seaborn/issues/242 On googling, you wiil find some workarounds mentioned but I couldn't understand much out of those links and didn't give them a try.",313826.0
95937,406974.0,"Anyways, the tsplot() will no longer be supported in the future versions. So instead use lineplot() or boxplot(). The labelling coes automatically and even the view is much better.",304319.0
95944,406862.0,"a) Use log function when the range of data points is too large to fit on the plot. For example, data points a=1000,b=10000,c=100000 Log function helps to compress these kind of data points and makes the plot readeable. Example if x=10, log(x)=1, so in the above case log(1000)=3 and log(10000)=4 so on and so forth. and plotting these log values will make the plot easy to read. b) Extreme data removed-It is removal of outliers Example: Marks of students range from 1 to 100 But there are values like 999,-999, etc These are extreme values which should be removed from the dataset.",311254.0
95944,406908.0,"Shayari has correctly stated when to use the log function. I want to add to when to remove extreme data? When we need to deeply study a particular range of data, we can choose that range and plot. For ex, If there are a lot of students in the range 30-80, but only few in 0-30 and 80 above, and hence the plot is quite dense around 30-80. This makes it difficult to interpret and find corelations. So we can filter the data so the plot gets zoomed, and easier to interpret.",304319.0
95944,406921.0,"Adding to what Shayari &amp; Ruchita said: Use LOG scaling when you are looking at large number range [10 : 1000], or [0.00001 : 0.01], etc The LOG scaling per say doesn't filter data using box plot, but that the data point does not fall in the area of most data points. It is just that we are using visualisation to see how are data is spread out, and what points of extremum is irrelevant to the data or even finding the values to which we need to filter data itself. Hope this helps.",312731.0
95666,405127.0,Its shows the variance of data - maximum and minimum as specifed in lecture.,318429.0
95666,404975.0,"i am considering the bar plot used in video. In this, bar plot is plotted on mean(by default) of sales across product categories. so, coming to black line, if u take Technology category, professor said that top of black line is maximum number of sales i.e out of all sales it represents highest value among all sales values, bottom of black line is minimum number of sales i.e. out of all sales it represents highest value among all sales values. so for technology, maximum value will be around some 1600, minimum value will be around some 1300. the Mean(average) value is somewhere near to 1500 which is top of green bar.",300733.0
95666,404990.0,Max and min of sales values.,320197.0
95666,405056.0,"After reading the seaborn barplot official documentation and some googling , have come to understand that the Blcck Line represents the "" Confidence Interval "" with which the aggregrated value (like mean(default), median etc) are calculated. By default, a 95% confidence interval is calculated using bootstraping method. The parameter ci can be used to modify the behaviour. More details can be found at the official documentation .",313826.0
95666,405209.0,Black line shows max and min i.e variance as box shows mean only.,318426.0
95666,407774.0,the black line represents the variance range (maximum and minimum) and top of each bar represents the mean value,310463.0
95674,405316.0,Link is working for me.,311117.0
95674,405017.0,Link is working fine for me.,313826.0
95674,405201.0,Link's working for me.,312507.0
95575,404400.0,so do we need any changes in the way command is written ? (as indicated in the warning),309211.0
95575,404812.0,There seem to be a lot of warnings when using seaborn package. Maybe for now we should ignore them.,311857.0
96007,407486.0,"Estimator in seaborn barplot has no default value since it is optional. It is a statiscal function which you need to specify. Examle : from numpy import median sns.barplot(x=""day"", y=""sales"",estimator=mean) Regarding labels, seaborn's barplot returns an axis-object (not a figure). So for setting labels you can use pyplot from matplotlib Example: import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt fig = sns.barplot(x=""day"", y=""sales"",estimator=np.mean) plt.xlabel(""Days"") plt.ylabel(""mean(Sales)"") plt.title(""Days vs Sales"") # You can comment this line out if you don't need title plt.show(fig)",311254.0
96007,407482.0,if you want to change the label name in python you can do it by plt.xlabel(the name you want to give) for tableau you can click the capsule and change the name of the bar by the aggregation that you have used.,318017.0
96007,407801.0,"The default estimator is ""Mean"". You can change the estimator by passing a different function using the ""estimator"" parameter Ex. sns.barplot(x,y,estimator=np.median). By default the x and y labels are just the column names of the dataframe being used for teh x and y axis. The labels can be changed using plt.xlabel() and plt.ylabel() functions. More information on the official documentation: https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot",313826.0
96033,407637.0,please pass product_category in quotes and check if the column is present in the file which you loaded,318017.0
97166,418383.0,"You need to update seaborn giving ""pip3 install seaborn==0.9.0 "" on anaconda prompt.Then try",300684.0
97166,415675.0,Please go through this link. This might be of help. https://stackoverflow.com/questions/51402579/module-seaborn-has-no-attribute-any-graph,317991.0
88601,366783.0,"It's not mandatory. You can do it throughout this course, even though all the study materials for R will be provided at the end of the course.",315028.0
88623,366854.0,"Hi, This is what I did. columnname &lt;- as.character(columnname) Hope this helps.",310511.0
88623,367807.0,"Hi Deval, Kindly refer below code finaldata$name &lt;- as.character(finaldata$name) The function as.character() converts the column 'name' of the dataframe 'finaldata'. $ symbol is used in extract the particular column in the data or final data. It will remove that particular column from the data or final data.",308639.0
89543,,nan,
78546,317380.0,"Suppose you define a vector as x &lt;- 1:20 and want to remove number 3 to 7. You can achieve this by using x2 &lt;- x[!x %in% 3:7] or it can be simplied to x[-(3:10)] Regards, Manish Anand",300713.0
78546,317961.0,"To delete elements from R use negation(-) infront of the element you want to delete. See the example below ab &lt;- c(1:10) 1,2,3,4,5,6,7,8,9,10 ab&lt;-ab[-3:-10] 1,2 To delete multiple elements. Use the below scenario If you want to delete element 3 and 5 ab &lt;- c(1:10) remove&lt;-c(3,5) ab&lt;-ab[!ab %in% remove]",300688.0
75927,305718.0,"It matches the string (pattern matching). you dont have to explicity call 'Stadium', just the 'St' (or any matching pattern of the word stadium ) will subset the list And also, the answer the other questions: yes, the suffix followed by $ (example : $hp or $am) always signifcy column names",301561.0
78628,318754.0,"When we use string_AsFactors = FALSE, then the factors are converted into the other form.",304692.0
76388,304788.0,"Matrix in R can be created using the matrix() function. Dimension of the matrix can be defined by passing arguments nrow and ncol. Providing value for both dimension is not necessary. Either one of the dimension can be provided, the other is calculated from data length. eg. matrix(1:6, nrow = 2, ncol = 3) Here we are specifying a matrix of 2 rows and 3 columns eg. matrix(1:6, ncol = 3) Here also we are specifying a matrx of 3 columns. The 2 rows are inferred by the length of the matrix(6) eg. matrix(1:6, nrow = 2) Here also we are specifying a matrx of 2 rows. The 3 columns are inferred by the length of the matrix(6)",300748.0
76388,305225.0,"Matrix can be created using the matrix() function. Dimension of the matrix can be defined by passing appropriate value for arguments nrow and ncol . Providing value for both dimension is not necessary. If one of the dimension is provided, the other is inferred from length of the data. eg - &gt; matrix(1:9, nrow = 3, ncol = 3) [,1] [,2] [,3] [1,] 1 4 7 [2,] 2 5 8 [3,] 3 6 9 &gt; # same result is obtained by providing only one dimension &gt; matrix(1:9, nrow = 3) [,1] [,2] [,3] [1,] 1 4 7 [2,] 2 5 8 [3,] 3 6 9",300691.0
76388,305704.0,"Rougly explained, R calculates the not given value (Row or columns). Lets take an example matrix(1:20,4,) # this will create a 4x5 matrix (4 rows and 5 col) 1. when we specify the elements of the vector, R checks for the number of elements . in this case it checks length(1:20) and the answer is 20 2. now when you call the second parameter (either row or column), it divides that value with the previoously returned value 20/4 = 5 and hence the number of column is known to R",301561.0
76388,305323.0,"Matrix Function in R infers number of rows and number of columns, if user has not specified any values. However, if user has specified either of them and data is insuffecient to fill in the matrix , concept of recycling is used to populate the matrix. Case 1: No input on #rows and #columns Case 2: #Columns inferred by R with suffecient data [ ceil( 9/3 ) is 3. #columns = 3] Case 3: #Columns inferred by R with recycling of data [ ceil( 4/3 ) is 2. #columns = 2] Hence, matrices in R can be looked as an array of arrays where, each array is of same length. #rows or #columns are inferred and matrix is populated completely using recycling of data, if necessary.",300730.0
78207,317003.0,Is the file in the same directory? Try looking at the current working directory. You can figure that out by using getwd().,306733.0
78207,317009.0,"There could be multiple reasons. 1. is the file on your machine / cloud? 2. is it present in your Working Directory or is the path correct? 3. you can also mention the delimiter (comma separated or tab separated or any other delimiter) read.table('path_____/myfile.txt', sep = ) # appropriate delimiter 4. admin rights or any protection to access the file/folder",301561.0
78214,314434.0,"To delete elements from R use negation(-) infront of the element you want to delete. See the example below ab &lt;- c(1:10) 1,2,3,4,5,6,7,8,9,10 ab&lt;-ab[-3:-10] 1,2 To delete multiple elements. Use the below scenario If you want to delete element 3 and 5 ab &lt;- c(1:10) remove&lt;-c(3,5) ab&lt;-ab[!ab %in% remove]",300688.0
78214,319223.0,or it can be for a continuos del without remove var as below: ab &lt;- ab[!ab %in% 3:10],304813.0
84160,346320.0,Since everything can be represented as character. Even Every numeric and boolean can be stored as character and futhur they can be parsed as numeric or boolean using various methods provided in R.,312746.0
84160,346328.0,"Every other datatype can be coerced into character. However, character (strictly a text) cannot always be coerced to other datatypes (logical, integer, double, etc.). So, this makes character datatype as more generic.",301561.0
84160,348531.0,char can store other data as well lets say numbers with '2' etc..but we cant store strings/chars in numbers i nay way.so if we have bunch of data with dfferent data types then char becomes the umbrella for all of them so allow each ones. !!,304813.0
75536,301210.0,Looks like the column names have a mismatch between team_data and team_data2 Can you post the complete code of how do you create team_data and team_data2,300689.0
75536,303165.0,"While rbinding two dataframes, te column names should be identical.",301276.0
75536,304963.0,"Thank you, I got my error resolved",301655.0
75130,299006.0,Just select the command in the script and click on the run button which is available in the script section.,300688.0
75130,299058.0,"Executing a Single Line To execute the line of source code where the cursor currently resides you press the Ctrl+Enter key (or use the Run toolbar button): Executing Multiple Lines There are three ways to execute multiple lines from within the editor: Select the lines and press the Ctrl+Enter key (or use the Run toolbar button); or After executing a selection of code, use the Re-Run Previous Region command (or its associated toolbar button) to run the same selection again. Note that changes to the selection including additional, removal, and modification of lines will be reflected in this subsequent run of the selection. To run the entire document press the Ctrl+Shift+Enter key (or use the Source toolbar button).",301276.0
75136,300058.0,The below link has a neat explanation of declaring variables in quotes.,301276.0
75325,301011.0,Please make sure that you have admin access and also that firewall is not blocking it. Better to install in personal laptops than official ones.,301276.0
75325,301335.0,"Hi @Sreenath Acharath, while installing swirl from CRAN,it picks automatically and installs. How do we can update it regularly as continue adding new features and content happens.",301115.0
75325,318834.0,"Hello Sreentah, While I am trying to install R studio in my personal Laptop,I am getting below error I have tried this by Run as Administrator. Please help me to resolve this issue.",306008.0
76595,305824.0,"The dice vector is a combination of karan_dice, raj_dice and ajay_dice implying the dice_vector is actually 3,1,5,7,6,1,6,1,3,4,6,1,1,1,4,2,2. This vector is given as input to matrix function where #rows is specified as 3 and byrow parameter is true. Hence, First row of matrix is as expected. Second row of matrix is as expected. However, since ajay_dice has only 5 elements - we are short of 1 data element as first two rows have 6 elements each. Therefore data passed to matrix function is recycled . Note that first element in dice_vector is 3 and the last row of matrix is populated as 1,1,4,2,2,3",300730.0
76595,306656.0,"When you call c() function over two vectors, it gets concatenated. Here, all three vectors get concatenated into one vector of length (6x3=) 24. Then you are turning them into a 3x6 matrix",301561.0
76595,312638.0,"When you execute the above expression : The first vector has 6 elements. The second vector has 6 elements again , however, the third vector has only 5 elements so R recycles the data passed to a matrix and places the first value of karan_dice i.e 3 as the last value in ajay_dice hence the elements of ajay dice are ( 1,1,4,2,2,3)",300688.0
75181,299059.0,The below link has a pretty clear explanation. https://stats.stackexchange.com/questions/3212/mode-class-and-type-of-r-objects,301276.0
75181,299775.0,"most notably that both (typeof ""integer"" and ""double"") = (mode ""numeric""); and both (typeof ""special"" and ""builtin"" = (mode ""function"") Class is based on R's object-oriented class hierarchy",301131.0
75181,344631.0,I also add to the above to use the below for more clarity ?class ?typeof,308962.0
75181,342886.0,I feel R should have made consistence in findnig the type with class and typeof. x=2 class(2) gives numeric and typeof(x) gives double.,312019.0
80286,325310.0,"Yeah, R gives a warning when you multiply vectors of unequal lengths. Its a feature, if you may. R will recycle the elements of veca in your case, and will throw a warning to inform the user that vectors are of unequal lengths. The warning will be something like: In veca * vecb : longer object length is not a multiple of shorter object length.",300748.0
80286,325305.0,"Please try the below in R-Studio veca &lt;- c(2,6,3) &gt; vecb &lt;- c(4,3,5,7) &gt; vecc &lt;- veca*vecb First time it will give you warning but when you execte the vectorC (i.e vecc) it will give you answer 8,18,15,14 R does allow to multiply of different size here vector b last element is 7 and vector a smaller set having 2 is the first element so the result of 4th element is 7*2 i.e 14. and hence the final result 8,18,15,14. Cheers!!!",307843.0
80286,325302.0,"1. Is there a question, because I didn't understand what you've said. So, can you be a little more articulative please. 2. R actually expects vectors lengths to be multiples. And, R won't complain if the vectors are of unequal lengths, given that the lengths are multiples of one another.That's why you get a warning when you try to multiply two vectors of lengths, say 5 and 7.",306733.0
80286,326769.0,"Please note, in R a ""warning"" is different from an ""error"". A warning is a result of something that gets executed but there could be a need for intervention. On the other hand, and error (Example dividing zero by zero) is not executed at all and gives error. Now, to answer your specfiic question, when A, B, C is multiplied with W X Y Z , (note, ABC and WXYZ are just a proxy to the numeric elements in the vectors. Please note, it is not a Character vector in R,. I am using only explanation purpose) it first calculates A*W B*X C*Y now, one of the vector still has an addtional element (Z in this case), so, it 'recycles' the vector to ""find a pair"" (very crude analogy) and it calcualtes A*Z. I trust this explanation helps you get more clarity.",301561.0
75467,300568.0,"In general for R '&lt;-' is the preferred syntax Both the operators '&lt;-' and '=' assign into the environment in which they are evaluated. The operator '&lt;-' can be used anywhere, Whereas the operator '=' is only allowed at the top level (e.g., in the complete expression typed at the command prompt) or as one of the subexpressions in a braced list of expressions. The '=' operator restricts the scope within function Whereas '&lt;-' makes the scope to the workspace itself",300689.0
75467,303297.0,"we only use &lt;- for assignment and = for specifying named parameters. x &lt;- rnorm(100) y &lt;- 2*x + rnorm(100) lm(formula=y~x) The above code uses both &lt;- and = symbols, but the work they do are different. &lt;- in the first two lines are used as assignment operator while = in the third line does not serves as assignment operator but an operator that specifies a named parameter formula for lm function. In other words, &lt;- evaluates the the expression on its right side ( rnorm(100) ) and assign the evaluated value to the symbol (variable) on the left side ( x ) in the current environment. = evaluates the expression on its right side ( y~x ) and set the evaluated value to the parameter of the name specified on the left side ( formula ) for a certain function.",303082.0
75559,301207.0,"Yes, you would get a warning ... but you would get the result &gt; a &lt;- c(2 ,6, 3) &gt; b &lt;- c(4, 3, 5, 7) &gt; mulab &lt;- a * b Warning message: In a * b : longer object length is not a multiple of shorter object length &gt; mulab [1] 8 18 15 14",300689.0
75559,304481.0,"Look at the line just before the warning message....it gives the answer and augements it with the warning msg. &gt; a=c(2,6,3) &gt; c=c(4,3,5,7) &gt; a*c [1] 8 18 15 14 Warning message: In a * c : longer object length is not a multiple of shorter object length",300717.0
75559,305535.0,"In R when we multiply vectors of two different sizes, R automatically cycles the values in smaller vector until it matches the size of the bigger vector. For example Assume that we are having two vectors one of size 2 and other of size 4 a&lt;-c(2,3) b&lt;-c(4,5,6,7) When a*b is executed, since a is the smaller vector, it cycles the value until it matches the size of the larger vector which is 4. In this case R assumes the value of a to be c(2,3,2,3) to match the size of b and this value is used to multiply with vector B. in this example a*b will produce a vector of c(8, 15, 12, 21). Execution from R &gt; a&lt;-c(2,3) &gt; b&lt;-c(4,5,6,7) &gt; a*b [1] 8 15 12 21 Similarly if suppose b vector has value c(4,5,6,7,8) a*b will produce c(2,3,2,3,2) * c(4,5,6,7,8) = c(8,15,12,21,16) Execution in R &gt; b&lt;- c(4,5,6,7,8) &gt; a*b [1] 8 15 12 21 16 Warning message: In a * b : longer object length is not a multiple of shorter object length You get the warning message if the size larger vector is not a multiple of size of smaller vector. In my example i didnt get the warning message in my case 1. but case 2 produced the warn message.",300708.0
75559,306969.0,Thank for all the answers. I tried some more examples and understood that in case of multiples no warning is observed. But if the vectors size are not multiples of each other then the warning message is displayed.,301114.0
76643,305668.0,"Can you elaborate your question a little more? Logical expression is one that returns either a TRUE or a FALSE value. &amp;, | and ! are logical operators.",300717.0
76643,312632.0,"Logical is derived from logic meaning the study of correct and incorrect reasoning. As far I understand I guess you are asking about logical operators. Few logical operators are And(&amp;) , Or(|), Not (!). The output of these expressions is either True or False ie 1 or 0.",300688.0
79080,319058.0,“&lt;&lt;-” and “-&gt;&gt;”: This operator assigns the variable to the parent scope. It first looks for the variable in the parent scope and if it can’t find one then it creates a new one in the parent scope. The operator is mainly used inside functions.,305838.0
79080,319075.0,It[&lt;&lt;-] is the other type of the assignment operator.,304692.0
79080,319351.0,Assignment Operator in R,306735.0
79080,319448.0,"Hello Ashish, &lt;&lt;- is basically a defines a global variable whereas &lt;- defines a local variable. As you can see in below example, gcount is a global variable and returns 1,2,3 when it called whereas lcount is a local variable and returns 1,1,1 whenever it is called gcount &lt;- 0 print(gcount) global_count &lt;- function() { gcount &lt;&lt;- gcount + 1 print(gcount) } global_count() # returns 1 global_count() # returns 2 global_count() # returns 3 #----------- lcount &lt;- 0 print(lcount) local_count &lt;- function() { lcount &lt;- lcount + 1 print(lcount) } local_count() # returns 1 local_count() # returns 1 local_count() # returns 1 Let me know if this helps",304694.0
79080,319731.0,"I'm going to be answering the second question only. Generally, you don't need to use the ""&lt;&lt;-"" operator. [Simple name: deep assignment operator.] When you change a variable's value using a function, R uses something called a ""copy-on-modify"" method to change the variable's value. This means that R creates a copy of the variable inside the function and alters the copy instead of the original variable itself. The deep assignment operator is a method to override this process. The examples in the previous answers capture this process extremely well. But, to answer your question, you don't normally use the deep assignment operator unless you're working with environments. [And, at this point in the course, I don't think that we will need to work with environments.] However, if you are interested in learning more, check out the ""Hands-On Programming with R"" book for a brief intro to the topic. And, if you want a relatively deeper understanding, check out ""Advanced R Programming"".",306733.0
79080,321066.0,"This is a ternary operator.it takes three arguments. The first argument is a comparison argument, the second is the result upon a true comparison, and the third is the result upon a false comparison.",305847.0
80587,327257.0,"1. Click on PROGRAM tab at the top. 2. Then click on, Welcome to Pre-Launch Program . 3. Then click on, Session 2: Pre-Read Topics in Mathematics -&gt; Mathematics Curated Content. You'll find the Khan Academy tutorials you require.",301652.0
80587,327407.0,"Like Abhishek Singhal mention, Try the link below:- https://learn.upgrad.com/v/course/208/session/15776/segment/79764",301561.0
80587,328103.0,"As you might have got the Mathematics section now under Introduction section All the topics are mentioned over there. If you click on any topic, you will be redirected to the Khan's Academy tutorial site. There you will get all the sections of the selected topic. Go through it. Have a good day!! Please respond if you need more information.",300686.0
80587,345239.0,"Most of the mathematical sessions are on Khan Acacemy website and very simple to understand. Though one thing I felt ,almost similar thing is repeated across multiple sessions, thus you can skip some of the sessions. Thanks!",313767.0
80587,347623.0,"Hi, I am new to Tableau and not very comfortable in going through the online tutorial. Just wandering is it the case with others or only with me? Also , I cant download the videos though it is mentioned on the tutorial. Thanks!",307494.0
80587,348448.0,"Hi, I too felt the same way. So I googled how to learn Tableau for beginners. And every answer indicated- watch the free videos and practice. I saw the following sections completely- getting started, visual analytics, dashboard and stories, calculations, why is tableau doing that, and how to. And as for practice, I just implemented whatever the video taught while watching it. (I did not fret over too much about what I did not understand.) Doing these things definitely gave me a better understanding of Tableau.",301652.0
80643,327682.0,"No it can't be the case their must be some issue with network or server you are using, I myself jump to python before R and you have to choose any one language as per course curriculum.",305838.0
80643,328217.0,Even I did python first...the modules are not sequential,308437.0
80643,328783.0,"I have tried three browsers [ Firefox 6.1.0.1, Safari Version 11.1.2 and Chrome Version 67.0.3396.99 ) and all three allow me to toggle and choose R or Python or any of the prep course. Here is what you can attempt: 1) Fire up your browser and hit the URL https://learn.upgrad.com/v/course/208 [ I presume this is standard across for everyone] 2) Click on 'Program' link on the top right below the Title bar 3) You can either click on ""Programming in R"" or ""Introduction to Python"" . 3) You can even toggle between Python and R sessions if just one course gets monotonous. Entirely upto you. You can pace yourself and complete both. Good luck!",309211.0
80643,331908.0,Even I have been moving from one R to Python to Sql and Excel it definitely allows .Please check with student mentor if you have any technical issues.,300687.0
80643,334019.0,"for the pre-course, you are advised to complete all modules to gain general understanding. This is not the final course material. As they mentioned, decision of programming language will be based on the majority selection.",312199.0
79097,319173.0,"Learning something will not harm but if you have chosen python for data science you are good to go, you can leave R if time is a constraint.",305838.0
79097,320235.0,"Nagaraju, please access your course through the 'Program' tab of the home page. Then you can directly go to python lectures. But if you click the 'Continue learning', it may tend to take you to R.",300717.0
79097,320237.0,"Oops, sorry Vicky, my response was for Nagaraju.",300717.0
79097,321068.0,"This course is basicaly,DATA SCIENCE. The tool you selected to use is PYTHON but you can also learn the basics of R as well. and learning both will be a plus point moresover.",305847.0
79097,320485.0,The main course is taught in R. The python intro module is for your reference and it is only an intro to Python. The assignment and case studies will be done using R. May be you can check with your student mentor on course content and polyglot options,301561.0
79097,322462.0,"If you have selected Python, you don't need to learn R. But if you are inclined towards learning and have the time, you can go through the Programming in R module as well. Hope this solves your confusion.",301621.0
79204,319744.0,"data.frames, matrices, arrays these are all objects belonging to a different class. Custom data objects can belong to different classes too. But, I think you're looking for non-atomic types [not classes]. The best example is the list.",306733.0
78288,321704.0,"The size of vector of larger length has to be 'equal to' / 'a multiple of' size of vector of shorter length. Only then any operation is possible i.e. addition/subtraction/multiplication. If different, warning message is displayed. But it isnt an error.",307488.0
78288,315068.0,When you multiple the two vectors you will get the answer 8 18 15 14. The first element of the vector a is multiplied with the firs5 element of vector b. In a similar way 2nd element is multiplied with 2nd element. Since vector B has 4 elements. Hence the first element of A is multiplied with the last element of B ie 7 this is because aggregation in vectors takes place in a cyclic form,300688.0
78288,316855.0,"Hi Vivek, R doesn't throw an error. It computes the result and displays a WARNING. The Process that Prakash explained is known as Recycling in R. Where the vector with smaller length recycles itself to compute the elements of the larger vector. E.g: V(2,6)*V(1,2,3,4) will behave as V(2,6,2,6)*V(1,2,3,4)",305845.0
78288,326024.0,The vector should have the same length wheather you want to multiply/divide/addition/subtraction. The vector should have a same length then it will gives the right results.,308639.0
78288,343840.0,"I also agree to Vivek. Please see the output from RStudio console as below : &gt; a = c(2 ,6, 3) &gt; b = c(4, 3, 5, 7) &gt; d&lt;-a*b Warning message: In a * b : longer object length is not a multiple of shorter object length The answer mentioned looks incorrect. Can you all please suggest the version of R you have on your system.",311729.0
78288,344023.0,"Hi Amit, Both vector have different length, and the vector a have 3 element and vector b have 4 element. kindly refers the below solution. a=c(2,6,3) where b=c(4,3,5,7), here you will observe 2*4=8 , 6*3=18, 3*5=15, and the fourth element of b is again multiply by the 1st element of a (2*7=14), because both vector have different length thats why u get the error. Now once again you can try and run it gives the same output.",308639.0
78288,344026.0,And if you want the same results than you have mentioned the same length of the vector.,308639.0
77951,312941.0,"Both are same. If you parse the objects to class(), typeof() and mode() function, you will get the same result. Since the vector is a continuous, we can write as 1:4 (without using c() function). you can consider it as a shotcut. Similarly, if you want letters, you can create like the following way letters[1:26] #lower case LETTERS[1:26] #upper case month.abb[1:12] # three letter name month.name[1:12] #complete name of the month these are also vectors",301561.0
77951,313028.0,"c()/ combine function comes handy when we cannot specify a set of numbers with sequence. -----------Eg.1 Sequence of numbers from 1 to 4----------- c &lt;- 1:4 c_combine&lt;- c(1:4) c_seq &lt;- seq(1,4) #All 3 above ways works -----------Eg.2 All odd positive numbers till 10----------- c_combine&lt;- c(1,3,5,7,9) #works c &lt;- 1,3,5,7,9 #Error, unexpected ',' c_seq &lt;- seq(1,10,2) #works # method 2 without c() fails -----------Eg.3 Some random numbers till 10----------- c_combine&lt;- c(1,3,7,8) #works c &lt;- 1,3,7,8 #Error, unexpected ',' c_seq &lt;- ???? #you cannot generate sequence of the numbers w/o any logic Hope these examples clears the concept.",304694.0
77951,316999.0,"What if you want to combine say two character elements or a set of numbers that don't occur in a sequence? That's where the concatenate function (or the ""c function"") comes into the picture. [The concatenate function is also called the combine function.]",306733.0
79246,320024.0,"In one sense you are right Prathap, it seems Vectors, as they contains regular data types, can be consider as super set. But let's not mix them and let's use different terminology. Vectors/Matrix/Dataframes are actually data structures in R which is a way of organizing the data. Here the data can be of same or different data types. Hope this helps",304694.0
79246,325343.0,"I don't think so. A data structure is an abstraction of data. It just tells you the way in which the data is stored. A tree is a data structure. Vectors are like trees. A data type is an object that can be built and represented. It describes things [objects] that have some similar data type. Again, if you had a data type called ""prime numbers"" which represented prime numbers, you could build a tree out of prime numbers. Extending the same logic, vectors are a data structure. Char, numeric / double, they're all data types. Vectors are a structure that are used to store data of a particular type. [Hope I articulated it clearly!}",306733.0
79646,322585.0,"Did you mean myvector &lt;- c(1,2,3,4) myvector[4-3:4] prints 3 2 1 ? It prints 1 in case myvector[4-3:4] and prints 3 2 1 in case myvector[4-1:4] Need clarification as I can explain the logic for later one.",304694.0
79646,322611.0,"#1. This is not related to vector. This is more how R calculates the range in combine function i.e. c #2. Below is the generalized logic to calculate the range if it has any operators in c(x:y) Note: x1 = increment/decrement of x in the range y1 = increment/decrement of y in the range General syntax: c(x operator x1:y operator y1) # Range Start Value = x (operator before x1) x1 (operator before y1) y1 # Range End Value= if x1 is available then x (operator before x1) y (operator before y1) y1 else y (operator before y1) y1 This logic works perfectly fine for all the below test cases. Please run and test it - c(5+2:7) c(5-2:7) c(5:7+2) c(5:7-2) c(5+2:7+1) c(5+2:7-1) c(5-2:7+1) c(5-2:7-1) #3. So, in your case - c(4-1:4) returns positions 3 2 1 0 So the values of the vector at these positions will return for myvector[4-1:4] i.e. 3 2 1 as counting starts with 1 in vectors #4. Not sure the examples where we can find such scenarios where in we have operators inside range Hope this helps",304694.0
78411,316858.0,Yes u can. You can even use 3-&gt;var. R has several ways of declaring a variable but the most preferred one is &lt;- symbol. Please go through below links for details understanding on their difference: https://renkun.me/2014/01/28/difference-between-assignment-operators-in-r/ https://stackoverflow.com/questions/2271575/whats-the-difference-between-and-in-r,305845.0
78411,316998.0,"You sure can. But, that's not the norm in R programming. You can refer the content in the following link to know more about the standards for writing code in R: http://adv-r.had.co.nz/Style.html",306733.0
78411,317105.0,"a1 = 10 a2 &lt;- 10 assign('a3', 10) the above three options are common. Though using equal to sign is possible, it is not considrered as 'coding standards'. If you use this, you will not get any penalty in this course, but it is a common understanding to use the standard assignment operation (&lt;-) in R. shortcut : ALT + -",301561.0
87422,363917.0,I believe that you can find the answer to your question by reading the answers here .,306733.0
87422,360974.0,"Quoting from the R help documentation(got by running &gt; ?""&lt;&lt;-"" at the R prompt) : ""The operators &lt;&lt;- and -&gt;&gt; are normally only used in functions, and cause a search to be made through parent environments for an existing definition of the variable being assigned. If such a variable is found (and its binding is not locked) then its value is redefined, otherwise assignment takes place in the global environment. Note that their semantics differ from that in the S language, but are useful in conjunction with the scoping rules of R. See ‘The R Language Definition’ manual for further details and examples."" What I make of the above is, that the ""&lt;&lt;-"" and ""-&gt;&gt;"" work on global environment variable , if one exists, else create a global environment variable. See the examples below: ls() : provides a list of objects in the current environment. 1. The variable x_squared gets created in the environment 2. In the below example, the variable x_sq does not get created in the environment",313826.0
80405,326168.0,Can you try this? This is independent of what your working directory is set to. read.csv(choose.files()) I am assuming there is no error with your text/csv file.,304694.0
80405,326233.0,"It is giving below warning message when I am choosing the .txt file.It does not give any error or warning for .csv in my any of the case Warning message: In read.table(file = file, header = header, sep = sep, quote = quote, : incomplete final line found by readTableHeader on 'C:\My_Folder\Hadoop\Upgrad\R\Work\my_file.txt' The content of the .csv file is given below ""Player_name"" ""Total_runs"" ""Strike_rate"" ""Player_age"" ""hit_six"" ""1"" ""Rohit"" 5000 ""84.22"" 26 230 ""2"" ""Kohli"" 7200 ""89.12"" 25 123 ""3"" ""dhoni"" 8900 ""87"" 37 179",307843.0
80405,326596.0,"As per the error message, the end of line is missing in txt file. The work around is to open the file with a Text-editor and add a line at the end of the file (aka press return key) Also, you can try read.delim(file.choose()) if you are reading txt file. Let me know if this works, else we need to check txt file itself.",304694.0
80405,326932.0,"The error /warning is due to not explicitly passing argument for 'delimiter'. If you are reading a csv file, you can do either use read.csv() function without passing any argument for delimiter or you can use read.table() with a delimiter. rescpective examples below: read.csv(""c:/some_folder/some_other_folder/file_name.csv"") read.table(""c:/some_folder/some_other_folder/file_name.csv"", sep = ',')",301561.0
80405,334101.0,I jst replicated your issue on my local system. Your file is missing end of line character. So just edit the file by pressing enter in the last line of your file and save your file. Run your scirpt again. It should work.,312746.0
82560,338511.0,"'class' is a property assigned to an object that determines how generic functions operate with it . Type of clarifies more on the attributes just think of Matrix even though in matrix there are list so Matrix in bigger way is class and the list is the type of for the same. Sometimes typeof and matrix is same when the class and attribute is both element i.e it can't be further classified like integer , it is both class and type of if is also element.",307843.0
82560,338642.0,"The below link has a pretty clear explanation. https://stats.stackexchange.com/questions/3212/mode-class-and-type-of-r-objects Difference between typeof() and class() function class(): used to identify what ""type"" an object is from the point of view of object-oriented programming in R. Example: x &lt;- 2 ; class(x) || Output: "" numeric "" typeof(): gives the ""type"" of an object from R's point of view, Example: typeof(x) || Output: ""double"" Bit more detailed: Mode, class, typeof() In R every ""object"" has a mode and a class. Mode: represents how an object is stored in memory (numeric, character, list and function) Class: represents its abstract type. Example: d &lt;- data.frame(V1=c(1,2)) class(d) || Output: ""data.frame"" mode(d) || Output: ""list"" typeof(d) Reason: This is because data frames are stored in memory as list, but they are wrapped into data.frameobjects. On the contrary, . typeof() will usually give the same information as mode except for few isntances Example: typeof(c(1,2)) Output: [1] ""double"" mode(c(1,2)) Output: [1] ""numeric"" Reason: The R specific function typeof returns the type of an R object, while function mode gives information about the mode of an object in the sense of Becker, Chambers &amp; Wilks (1988), and is more compatible with other implementations of the S language",301561.0
86921,357873.0,"Hello Prashanth, It is not mandatory because you will be using python only throughout PGDDS. You can go through video for learning purpose.",320195.0
86921,357961.0,"Its better to do so, also in brochure, R is mentioned in many places.. Also its never bad to have extra knowledge.",305129.0
86921,357761.0,"Hi Prashanth, No , its not mandatory to complete R if you have opted for Python. This was stated in the starting itself on one of the pages. So, you just need to complete rest of the modules before 30th September and you can leave the R module.",318355.0
86962,357941.0,"In the given example, there exists a vector by the name ""Total_runs"" whihc is used in creation of the dataframe ""Complete_team"". sum(Total_runs) would give sum of the elements of the vector Total_runs sum(Complete_team$Total_runs) would give the sum of the elements of column Total_runs belonging to Complete_team dataframe. Hence, the correct answer is sum(Complete_team$Total_runs) . Please find below screenshot.",313826.0
83151,341382.0,"1- R console tries to match all opening and closing braces. If closing braces are less, it assumes that you are going to provide more braces to complete the match. That's why it gives you ""+"" sign to complete your expression. 2- you are giving two cos statement in one statment which is wrong. it is expecting one more closing bracket "")"" after second ""+"" sign. but you are providing cos statment which is wrong. plz look at the screenshot below:-",312746.0
83151,341397.0,"What I underderstand you are getting + sign after you run the code, this is because your code is in error and the console does not provided the output so console make you understand your code has error. Now to resolve this issue press ESC button and now you are ready to agian do coding and + sign got vanished, you can try in your console. Hope this help.",307843.0
83151,341554.0,"To add to Alok, the + sign indicates the line of code is not complete. It is not an error but it indicates you need to add additional information to your line of code. If you look at the screenshot below, I am trying to subtract one value from another. When my code is incomplete (first five lines), it gives me + sign asking me to relook at my code. When i enter 5, and the result is available, the 'incomplete loop' is broken and value is displayed.",301561.0
86474,357047.0,"This is just a prep module. Only, once your course starts you will get graded questions. These questions are for your practice.",319721.0
86474,361589.0,This are practice questions and will be avialable on 30th Sep.,318332.0
88824,368025.0,"Quoting from the R help documentation(got by running &gt; ?""&lt;&lt;-"" at the R prompt) : ""The operators &lt;&lt;- and -&gt;&gt; are normally only used in functions, and cause a search to be made through parent environments for an existing definition of the variable being assigned. If such a variable is found (and its binding is not locked) then its value is redefined, otherwise assignment takes place in the global environment. Note that their semantics differ from that in the S language, but are useful in conjunction with the scoping rules of R. See ‘The R Language Definition’ manual for further details and examples."" What I make of the above is, that the ""&lt;&lt;-"" and ""-&gt;&gt;"" work on global environment variable , if one exists, else create a global environment variable. See the examples below: ls() : provides a list of objects in the current environment. 1. The variable x_squared gets created in the environment 2. In the below example, the variable x_sq does not get created in the environment Hope this helps.",313826.0
87349,360135.0,You can use both ‘=’ and ‘&lt;-’ as assignment operators. Please check here - https://www.r-bloggers.com/assignment-operators-in-r-%E2%80%98%E2%80%99-vs-%E2%80%98-%E2%80%99/. Thank you.,311502.0
87349,360139.0,"""="" and ""&lt;-"" can be used interchangebly in almost all cases. However, there are some differences in the way these operators work mainly w.r.t the scope of the objects. Please check the below links for some insights https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-in-r https://www.r-bloggers.com/assignment-operators-in-r-%E2%80%98%E2%80%99-vs-%E2%80%98-%E2%80%99/",313826.0
87351,360149.0,"A string can be created using a pair of matching single quotes or double quotes. I think the behavior is similar to that of python, where in we can use a backslash to escape special characters.",313826.0
87351,360803.0,"Hi, In R, Single and double quotes can be used interchangeably but double quotes are preferred (and character constants are printed using double quotes), so single quotes are normally only used to delimit character constants containing double quotes.",318328.0
87351,363922.0,"Refer to the style guid e to learn about the standards. As, the other answers have clarified, quotations can be used interchangeably. Use whatever you want, but stick to a single pattern.",306733.0
83643,343809.0,You have a comma missing after Tuesday. That's the reason for the error.,306733.0
83643,343856.0,use missed comma ater Tuesday. Please find the code below -,312746.0
83643,346614.0,keep comma in btw Tuesday wednesday,314612.0
83643,346885.0,Comma is very important after every string value else it will throw error.,303227.0
86051,352575.0,Its not manadatory but good to have for your learning..,312746.0
86051,352658.0,"Are you trying to install Swirl package without RStudio? If yes, try installing RStudio first &amp; then check. Go for a clean install The R studio version in the module is Version 1.1.442. Install R studio. Once it is installed, follow the below. Navigate to the Menu bar Click Tools -&gt; Install Packages-&gt; New window pop up -&gt;Type ""Swirl"" under 3rd option Packages . Refer to this link for further help, https://github.com/swirldev/swirl/wiki/Common-Errors-and-Fixes .",319721.0
86051,352788.0,Finally got swirl installed. 'Stingi' package took 2 hours to install. just kept print some wierd logs on console. Thaks you @Alok and @SB for your help,320073.0
85724,,nan,
85763,352371.0,"Yes. A good competency in both Python and R is expected for a data science role as the choice of language may sometimes be dictated by the organization,project etc. Hence, it is recommended that we do the R Prep course also.",313826.0
85763,352378.0,"Hey Manjusha, it is mandatory for you complete the python module. But R is given as an optional module. You can complete this in your free time.",319721.0
85763,352496.0,R is an optional module which is just for your knowledge gain and honds on expereience on R.Its good to go so you will basics of R as well.,317980.0
85763,352609.0,It's fine manjusha U can learn both Though python is ur language choice,308437.0
85763,353704.0,not a mandatory course if python is your language through the course. However in your free time go through R just to know how different python is.,315383.0
83765,344635.0,"Oh this was answered later in the video itself. Thanks. Feedback : When you add 2 vectors of unequal lengths, it tries to do something smart. It will take the smaller vector [which is c(2, 6) here] and the same elements will be used for adding to the larger vector until it matches the length of the larger vector. So, in this case addition of c(2,6) + c(4, 3, 5, 7) becomes equivalent to c(2,6, 2, 6) + c(4, 3, 5, 7) This concept is referred to as recycling.",308962.0
83765,344722.0,"This is not an error it is warning, this is in warning to let you know that the element in the vectors is unequal in length, as you can see below the length of vector 1 and vector 2. my_vector1 &lt;- c(1,3,5,7,11) ##--- Here lenth of element in Vector1 is 5 my_vector2 &lt;- c(2,4,6,8) ##--- Here lenth of element in Vector1 is 4 Therefore added_vector is (2,7,11,15, 13) see the bold 13 actually 11 is the 5th element in my_vector1 and there is no 5th element so it add to 1st element of my_vector2 as cycle and hence it gives you warning to let you know in advace that you are adding vector of uneqal lenth.",307843.0
83765,345878.0,Thanks Amit :),308962.0
86116,353084.0,"#------Vector Addition--- The above ansewer 6 9 8 9is for vector a + b vector &gt; veca &lt;- c(2,6,3) &gt; vecb &lt;- c(4,3,5,7) &gt; vecc &lt;- veca*vecb # Note the answer 8 18 15 14 when you multiply vectora*vectorb What will be output? longer object length is not a multiple of shorter object length # This is just warning and inform you in the begning that the two vector are different in length thats it. &gt; vecc [1] 8 18 15 14 Operations in vectors Create vectors a = c(2 ,6, 3) and b = c(4, 3, 5, 7) on R console. The answer in the assigment is below with explanation 8 18 15 14 Feedback : You can note that vector multiplication also occurs element-by-element like vector addition. R expects you to operate in vectors of similar lengths, but if you don’t, it uses the first element of vector a and multiplies it with the last remaining element (7 of vector b here). But as this is not proper recycling, R also issues a warning for this and not an error, so you may choose to ignore the warning and look at the output. I did practice in Rstudio it gives me warning too. :) it s generic",307843.0
86241,354574.0,"To sort a data frame in R, use the order( ) function. By default, sorting is ASCENDING . Prepend the sorting variable by a minus sign to indicate DESCENDING order. Here are some examples. Asceding Order example. Here 10,100,100 is ascending order so the order is 2(as 10 in 2nd place),1(100 in first place),3(1000 in 3rd place) Similarly for sting Below is for descending order use minus sign for descending order.",307843.0
84116,346063.0,"Player_name &lt;- c(""Rohit"",""Kohli"",""dhoni"") Total_runs &lt;- c(5000,7200,8900) Strike_rate &lt;- c(""84.22"",""89.12"",""87"") Elements of Strike_rate are in double quotes ("" ""), That is why it is treated as character not numeric.",312746.0
84116,346326.0,Strike rate has been kept as text on purpose. It is to show that at times the numeric data could be coerced into text when it is read into console (system). Once can coerce it back to numeric by using the as.numeric() function,301561.0
86336,358419.0,Check if you've created Team1 and Team2 correctly.. and then binding it together..,316349.0
86336,355582.0,"As per my analysis, I think that you have defined the ""team1"" dataframe's ""Total_runs"" vector as : Total_runs &lt;- c(""5000"",""7200"",""8900"") I have been able to reproduce the same error with such a definition: I deleted the ""team1"" dataframe and recreated it by making the elements of ""Total_runs"" as numeric. I could then create the ""Complete_team"" dataframe using rbind(). Please find the screenshot below. Let me know if this resolves the issue.",313826.0
87632,363688.0,"Are you in ofice network, in that case you might need to check the proxy settings. And you can try this command directly in console: install.packages(""swirl"")",318554.0
87632,363696.0,"No..Iam using my home netwrok. Error now says "" package ‘swirl’ is not available (for R version 3.5.1)"" What is the latest version of R Studio",319759.0
87632,363703.0,"I am using R 3.5.1. I have installed both 32 bit and 64 bit, whih was default. R Studio Version 1.1.456. I downloaded zipped R-Studio and then extracted.",318554.0
87632,363903.0,"The version is also dependent on your OS. If you use Linux, check if the default repo has swirl listed in it's files. Google the error. See if it means something else. Also, RStudio is just an IDE. Installing a package doesn't usually depend on RStudio. Just to be sure, try installing swirl from the command line / terminal.",306733.0
87632,364140.0,"Can you try: install.packages('swirl', dependencies=TRUE, repos='http://cran.rstudio.com/') Also if you have firewall enabled or any antivirus/internet security try to disable them before running install.packages('swirl')",318554.0
87632,365624.0,"I am using the home network and facing similar problem while installing swirl from RStudio. package ‘swirl’ successfully unpacked and MD5 sums checked The downloaded binary packages are in &lt;my local folder&gt; But when I try library(swirl), I get the following error message : Error: package or namespace load failed for ‘swirl’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘stringi’ Please help!",315797.0
87590,363022.0,"""="" and ""&lt;-"" can be used interchangebly in almost all cases. However, there are some differences in the way these operators work mainly w.r.t the scope of the objects. Please check the below links for some insights https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-in-r https://www.r-bloggers.com/assignment-operators-in-r-%E2%80%98%E2%80%99-vs-%E2%80%98-%E2%80%99/ Link to similar question : https://learn.upgrad.com/v/course/208/question/87349",313826.0
87590,363908.0,"Apart from the scope problems it might produce, there's also the interpretation problem. R users all over the world adhere to using ""&lt;-"" and not ""="" for assignment. Like how python has PEP8, R has it's own style guides. One of the most popular ones can be found here .",306733.0
87590,367442.0,"Here I provide a simple explanation to the subtle difference between &lt;- and = in R. First, let’s look at an example. x &lt;- rnorm(100) y &lt;- 2*x + rnorm(100) lm(formula=y~x) The above code uses both &lt;- and = symbols, but the work they do are different. &lt;- in the first two lines are used as assignment operator while = in the third line does not serves as assignment operator but an operator that specifies a named parameter formula for lm function. In other words, &lt;- evaluates the the expression on its right side ( rnorm(100) ) and assign the evaluated value to the symbol (variable) on the left side ( x ) in the current environment. = evaluates the expression on its right side ( y~x ) and set the evaluated value to the parameter of the name specified on the left side ( formula ) for a certain function. We know that &lt;- and = are perfectly equivalent when they are used as assignment operators. Therefore, the above code is equivalent to the following code: x = rnorm(100) y = 2*x + rnorm(100) lm(formula=y~x) Here, we only use = but for two different purposes: in the first and second lines we use = as assignment operator and in the third line we use = as a specifier of named parameter. Now let’s see what happens if we change all = symbols to &lt;- . x &lt;- rnorm(100) y &lt;- 2*x + rnorm(100) lm(formula &lt;- y~x) If you run this code, you will find that the output are similar. But if you inspect the environment, you will observe the difference: a new variable formula is defined in the environment whose value is y~x . So what happens? Actually, in the third line, two things happened: First, we introduce a new symbol (variable) formula to the environment and assign it a formula-typed value y~x . Then, the value of formula is provided to the first paramter of function lm rather than, accurately speaking, to the parameter named formula , although this time they mean the identical parameter of the function. To test it, we conduct an experiment. This time we first prepare the data. x &lt;- rnorm(100) y &lt;- 2*x+rnorm(100) z &lt;- 3*x+rnorm(100) data &lt;- data.frame(z,x,y) rm(x,y,z) Basically, we just did similar things as before except that we store all vectors in a data frame and clear those numeric vectors from the environment. We know that lm function accepts a data frame as the data source when a formula is specified. Standard usage: lm(formula=z~x+y,data=data) Working alternative where two named parameters are reordered: lm(data=data,formula=z~x+y) Working alternative with side effects that two new variable are defined: lm(formula &lt;- z~x+y, data &lt;- data) Nonworking example: lm(data &lt;- data, formula &lt;- z~x+y) The reason is exactly what I mentioned previously. We reassign data to data and give its value to the first argument ( formula ) of lm which only accepts a formula-typed value. We also try to assign z~x+y to a new variable formula and give it to the second argument ( data ) of lm which only accepts a data frame-typed value. Both types of the parameter we provide to lm are wrong, so we receive the message: Error in as.data.frame.default(data) : cannot coerce class """"formula"""" to a data.frame From the above examples and experiments, the bottom line gets clear: to reduce ambiguity, we should use either &lt;- or = as assignment operator, and only use = as named-parameter specifier for functions. In conclusion, for better readability of R code, I suggest that we only use &lt;- for assignment and = for specifying named parameters.",305847.0
87724,364162.0,I was also getting similar error about library not found. I have to install Rtools to resolve that. You can download that from below link: https://cran.r-project.org/bin/windows/Rtools/ click on first link which is for R3.3x and later,320073.0
87724,364268.0,"You can install the package from the ""Tools"" tab. Tools&gt;Install Packages&gt;*Type swirl*",319721.0
87724,364342.0,"Install Rtools, Tools&gt;Install Packages&gt;*Type swirl*",317418.0
87724,364731.0,"based on the snap you have shared, you are trying to install swril instead of swirl. Can you try with the correct package name, it should work.",319302.0
87725,364160.0,"What error you are getting while installing and how you are installing like from R studio package tab or by command "" install.packages(""swirl"") "" in r",320073.0
87725,364271.0,https://learn.upgrad.com/v/course/208/question/87724 Refer to the above question you previously asked.,319721.0
87744,364262.0,"Hi Bhaswati, No , its not mandatory to complete R if you have opted for Python. This was stated in the starting itself on one of the pages. But it's always better to have some extra knowledge, right? You can go through the R modules in your free time.",319721.0
87729,364263.0,"Hey, here is a question you asked earlier, https://learn.upgrad.com/v/course/208/question/87732 where a link has been provided. I hope you followed those steps. If it's still a problem let us know.",319721.0
76634,306970.0,"Hi, Please refer to the following answer for your query. https://learn.upgrad.com/v/course/77/question/48619",301618.0
76634,317489.0,"my_list[[2]] is called simplifying subsetting. To know the difference, type in the following code into the R / RStudio console: dummy_list &lt;- list(1:10) class(dummy_list[1]) class(dummy_list[[1]])",306733.0
83651,343897.0,"In R, else if must in the same line after closing braces. (that is the syntax requiremnt in R ) please see the screeshot for your ref -",312746.0
83651,343894.0,"R is bit tricky in using R and if not Best approach start the curly brace in the same line where condition starts like if().....{ and t o the next line where evaluation ends as print(""you dont have money left"") }",307843.0
87137,358938.0,"is.na : function used to find the elements which are having a 'NA' value in them. ' ! ' operater : Logical 'NOT' operator which takes element of a vector and give the opposite logical value. !is.na(x) : returns a TRUE only if elements of vector ' x ' does not have a value ' NA '. In the given example, !is.na(person$salary) would only return those rows for which the salary is having a value other than ' NA ' i.e., which have a valid salary.",313826.0
87148,358941.0,""" c() "" function : combines the arguments passed into a vector. https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/c",313826.0
82066,336025.0,33700 is the row no of element for which bank$age is maximum.,300690.0
82066,334968.0,Agreed.. I was looking for if it tells me in terms of A33700 (in case if I am refering back to my excel without freezing its panes and not in bank data frame in R..) But I it makes more sense to refer to sheet in R. Thank you.,310508.0
82066,334964.0,"If you're talking about getting the index of the maximum observation, which.max() does that for you by default. So, 33700 is the index of the observation with the greatest value.",306733.0
82066,335216.0,"Ranjana seems it is question of Advanced R, since we read in the tutorial about getting the functionaly on the data sets using Which function. However I am not sure about excel as it has not been taught still in the prep course to work on the exce. My guess only to read the cells of excel and apply which function could give your result.",307843.0
82066,336999.0,"which.something value for how many i.e count value, you can use which(cond) to get the row numer and column number you already know. :) which(bank$age == max_value); lets say you have already saved the max value of age in some var. --&gt;it will give row position of all who are having age equal to the max age. Hope it helps!",304813.0
88424,365894.0,Because 30th Sep is when the course starts :),310974.0
77355,308606.0,"Did you try looping and extracting what you need ? for (i in 1:nrow(moviesdataset)) { temp &lt;-moviesdataset[i, ] if ( temp$production_compaies == ""Walt Disney Picture"" ) { print temp; }",300717.0
77355,308605.0,"Did you try looping and extracting what you need ? for (i in 1:nrow(moviesdataset)) { temp &lt;-moviesdataset[i, ] if ( temp$production_compaies == ""Walt Disney Picture"" ) { print temp; }",300717.0
77355,310620.0,"Hi need a little help again. i tried the below code for (i in 1:nrow(moviesdataset)) { temp &lt;-moviesdataset[i, ] if ( temp$production_companies == ""Walt Disney Picture"" ) { print(temp$id) } } i dont see any output when i try it in R console. But in the environment pane i see one data temp. If i click that i get some other value not equivalent to walt disney pictures, that too only one record. I have 4803 records when i try nrow(moviesdataset) https://www.kaggle.com/tmdb/tmdb-movie-metadata The above is the link where i have downloaded the tmdb_5000_movies.csv file for doing this analysis.",301114.0
77355,317001.0,The data appears to be in a JSON format [I might be wrong though.] @KiranAnumalla's approach seems right to me. Use regular expressions.,306733.0
77355,312176.0,"Hi, If you want all the rows that contains ""Walt Disney Pictures"" in Production_Companies column which is in this format [{\""name\"": \""Walt Disney Pictures\"", \""id\"": 2}, {\""name\"": \""Jerry Bruckheimer Films\"", \""id\"": 130}, {\""name\"": \""Second Mate Productions\"", \""id\"": 19936}]"" then we need to use grep function which is a function for pattern matching in a character string - I have downloaded the movies dataset that you have shared and tried using below code and it worked and gave 114 rows as expected - ---------------------------------------------------------------------------------------- movies[ grep(""Walt Disney Pictures"", movies$production_companies) , ] ---------------------------------------------------------------------------------------- Let me know if this is what you are expecting. Regards, Kiran",304694.0
82312,337336.0,* Try experimenting using scipen = &lt;a big positive integer&gt; * For example:- options(scipen=100) * Positive values assigned to scipen bias towards fixed notation and negative integers bias towards scientific notation. (Check out ?options ) * Additional:- https://stackoverflow.com/questions/25946047/how-to-prevent-scientific-notation-in-r/25946211,301652.0
80164,324638.0,"Mam, I hope this free website is useful for you on R-language Datacamp. https://www.datacamp.com/courses/free-introduction-to-r",304692.0
80164,324930.0,"Hey Ruchita, I would request you to make an account on Kaggle, there you get a chance to work on real-time data. Might help you with R programming and python, just explore that website a bit. Below is the link : https://www.kaggle.com/ I hope this helps!! Happy Learning",300688.0
80164,325300.0,"INSTALLING R: Go to http://www.r-project.org/ Click to go to that site. Click on your operating system (Wi ndows, MAC, Linux) and follow directions. If Windows, click on “ base ” and then on Download R 3.4.1 for Windows INSTALLING R STUDIO: Go to http://www.rstudio.com , click on “Download RStudio” and follow the directions for you r operating system. USING R AND R STUDIO Open R Studio by clicking on the icon. You’re ready to go!",307843.0
80164,325337.0,Thank you all for the help.,304319.0
80164,325313.0,"If you're looking for tutorials, try the any of the cookbooks on R out there. There are DataCamps, Kaggle's tutorials on R. CrowdAnalytix has three projects that guide with the entire DS workflow. If you're looking for books, I whole-heartedly recommend these: R for Data Science (available as a free online book in the eponymous site) Hands-On Programming with R (This book brings you upto speed on R by guiding you through 3 projects) Practical Data Science with R (Not a great choice if you're starting out. However, it's got some amazing content if you have some working knowledge.) Then there are courses(free ones). Roger Peng has a few courses on Coursera. There are two good courses on edX by Microsoft and DataCamp. (I've audited them and they're a good place to start) If you're on Twitter, then join the R4DS comminuty. It's amazing! You can get a lot of help when it comes to DS and R.",306733.0
76886,311483.0,"This is a regular practice, we can embed R code code into a python code and run it as part of the python code run. python has packages which will help you embed and execute.",300708.0
76886,309587.0,"Well, it is possible. We can have the R running on one of the platforms like AsterR on Teradata, Hadoop or SQL 2016 has R built in. We can create a stored process that reads the R script, runs the necessary function (may be on new or existing data) and returns the result. In short, it is possible to automate R scripts using a platform.",301561.0
79124,319649.0,"Both are similar but with some changes where class gives ""numeric"" and typeof gives ""double"" and ""integer"", for builtin's class gives ""function"" and typeof gives ""closure"". Here the example:",304692.0
79124,319247.0,"TypeOf : Represents how an object is stored in memory (character, list, function, numeric) Class: Represent the abstract type of an object. (name of the class the object inherits from) eg.1 d &lt;- data.frame(V1=c(1,2)) typeof(d) # list class(d) # [1] ""data.frame"" eg.2 typeof(c(1,2)) # [1] ""double"" class(c(1,2)) # [1] ""numeric"" Here is another difference: class can be defined by the user, whereas typeof cannot. eg.: &gt; i&lt;-list(""a"",c(1,2)) &gt; class(i) [1] ""list"" &gt; # class can be user defined &gt; class(i)&lt;-""newclass"" &gt; class(i) [1] ""newclass"" &gt; typeof(i) [1] ""list"" &gt; typeof(i)&lt;-""newclass"" Error in typeof(i) &lt;- ""newclass"" : could not find function ""typeof&lt;-""",300748.0
79318,320509.0,"player_matrix[which(player_matrix$player_Age == 30),]",307016.0
79318,320363.0,"You can use below command - player_matrix[player_matrix$player_age == 30,]",304694.0
79318,321361.0,Bro Here is the Solution I hope it is useful.,304692.0
82199,336529.0,"If you read closely, the text on the red button says ""STOP"". That's a button which indicates that a program is currently being executed and that you can't execute any other commands. You can end the execution of a particular program by clicking that button.",306733.0
82199,336625.0,This is stop button used to stop the current execution in the command prompt. Sometimes it happens that not good use of loop program or bad programing the program goes on executing and not allowed the user to write the next command nor allow anything just a situation of hang so to avoid this kind of scenerio there is STOP icon button which abnormally kills all the program in running status and release the resource allocated by the prompt or command line.,307843.0
82199,336951.0,Ohkay.. Thank you..,310508.0
81746,333125.0,"Upto my knowledgge the graded assignment are not yet unlocked and all thoe swirl assignement ""Function assignment in swirl intermediate session is for practice It always asked my my name and assignment but never asked the assignment token and email address can you share the screen shot or the assignment name you are referring to .Because in intermediate session ofR the swirl assignment file name is Functions.Are you referring to this?",300687.0
81746,333124.0,"The tokens are generated to submit your progress on Coursera. The swirl() must be an assignment of some course on Coursera. But, since we have not enrolled in that course, we don't need to submit our progress. Simply select No when swirl() asks you to submit your progress on Coursera.",301652.0
81746,333120.0,"SWIRL (or Statistics With Interactive R Learning) is an open environment (or package) to learn applied statistics within R environment. Having said that, it is a Coursera intellectual property. Those who avail the R course, via Coursera can (also) participate in this program. So, swirl could assume that you could be one of such learners via Coursera who is participating in a course through their platform where you have an option to get a verified certificate at a cost. Only in such cases, you need to have a token. For all practical purposes, you don’t need a token if you are learning outside the Coursera environment (like Upgrad)",301561.0
84979,348109.0,What is the error that you see?,310974.0
84979,348296.0,Open in R Studio. That could do..,311227.0
84979,349061.0,There's no error. It's reading the data perfectly but it is not appearing on a separate window (for viewing) as it does in the video lecture. I'm using R studio though. Thanks,308962.0
80624,327499.0,"I belive you are looking for rbind() rbind() function combines vector, matrix or data frame by rows. rbind(x1,x2,...) x1,x2 :vector, matrix, data frames Note:The column of the two datasets must be same, otherwise the combination will be meaningless.",309451.0
80624,331896.0,rbind(),304813.0
80376,325850.0,"You've forgotten the ""c"" before the parenthesis. When you want to select multiple rows, you need to send the row numbers as a vector. The same is true for columns too. So instead of team1[(1, 3), ], write team1[c(1, 3), ] . And, if you want to select the second and third column, write team1[, c(2, 3)] . So, when you want to select multiple columns you need to send the indexes as a vector and not just numbers.",306733.0
80893,329769.0,"Hi Harshendra, Great to see that you resolved your issue by googling and trying various methods. keep up the good job and happy learning!",302827.0
80893,328789.0,"Ok, so here is the solution: When on MacOS High Sierra , although lot of webpages suggest Homebrew for installation, please do not select it. Go with the approach suggested below : 1) Install ""R-3.5.1.pkg"" from ""https://cran.r-project.org/bin/macosx/"" 2) Install ""RStudio-1.1.456.dmg"" from ""https://www.rstudio.com/products/rstudio/download/#download"" 3) Fire up RStudio and just add this as shown by RC in the tutorial: &gt;install.packages(""swirl"") &gt;library(""swirl"") &gt;Swirl [ Works in the latest Mac (MacOS High Sierra 10.13.6) ]",309211.0
80893,328764.0,"Thank you. I first checked StackOverflow. If solution was available I wouldn't have asked. Homebrew for Mac basically means that it resolves the dependencies before moving with installation(hence called Homebrew). The issue am facing is that the version 3.5 does not have RCurl and that seems to be the problem. I will keep looking, probably I will have to check the appropriate version where RCurl is available",309211.0
80893,328720.0,"I too was facing this problem till a while ago with my Linux distro. A non-zero exit status means that the required .pc (or whatever the file is) wasn't available in the required directory. In your case, the swirl package depends on the RCurl package for some scripts which help swirl run correctly. The problem is, those scripts are missing. What you wanna do is check the current pkg_config path and see if there is a file missing. If it is, install that package from the terminal. In Linux the command goes something like this: sudo apt-get install ""missing package name"" I'm assuming that the syntax is going to be something similar with Mac (since they're both Unix based.) Whatever the case, look up the process to install the package from the terminal, install it and then install the required R package from within R. It'll work. Check StackOverflow. You'll find the exact solution there. Hope this points you in the right direction!",306733.0
80893,341522.0,I am facing the same issue with OS X Yosemite.,314197.0
85237,349556.0,"Sum function returns the sum of elements of the list in case elements are numeric or returns type error in case the elements of list are strings. Whereas len function always return the no. of elements in the list. Eg. list = [1,2,3,4] sum(list) will give 10 (1+2_3+4) len(list) will give 4 (no. of elements in the list) Eg. 2. list= ['a','b'] sum(list) --&gt; Gives error ( TypeError: unsupported operand type(s) for +: 'int' and 'str' ) len(list) --&gt; Gives 2 (no. of elements in the list) Hope this helps!",318495.0
85237,350114.0,"In R x &lt;- c(10,20,30,40) sum(x) would give 100 while length(x) would give 4 In Python x=[10,20,30,40] sum(x) would give 100 while len(x) would give 4",300690.0
85237,349442.0,"len return the number of items in a sequence (list ,tuple) or collection, while sum() , sums up the numbers in the list,it can be anything list , tuples or dictionaries , but most importantly it should be numbers.",317822.0
85237,349448.0,"Example of Sum ## Pass several numbers to sum, and it also adds the elements. &gt; sum(1, 2, 3, 4, 5) Output: 15 Example of length x &lt;- c(1,2,5,4,6,1,22,1) # Here x is assiged to list c heving elements of length 8 &gt;length(x) Output : 8",307843.0
85237,349636.0,"tl;dr version: sum() cannot be used to count logical data instead of length(). Read on to find out why. Logical data comes last in the coercion order in R. What this means is this: If there are two data types in a homogenous data structure in R, then every element in the data structure will be coerced to the data type which is most flexible. As an example, consider a vector (a homogeneous data structure) with two elements 1, and ""a"". R's coercion order goes like this: logical &lt; int &lt; numeric &lt; char So, if you create a vector as follows: x &lt;- c(1, ""a"") Then data type of the vector will be character. You can check this by typing typeof(x). The same way, logical's are converted to numerics in R when you perform some sort of aggregation on them. A TRUE is converted to a 1 and a FALSE is converted to 0. So, if you have a vector (let's call it y), with 5 TRUEs and 5 FALSEs, sum(y) will give you 5. length() counts how long a data structure is. (Please don't use this definition. Look at the documentation for a clearer picture). So, length(y) will give you 10 as y contains 10 elements. The same can be said about mean() too. In this case mean(y) will give 0.5 (as mean(y) = sum(y) / length(y)) To finally get to the point: No, sum() cannot be used instead of length() when counting logical data because the two functions are meant for performing different operations.",306733.0
85237,350392.0,"Although, sum() can be used to get the length of logical data as shown below, but it is utterly useless and hence better to stick with the length() function. However, sum() can come in handy to find the number of TRUEs or FALSEs for specific use cases. &gt; x &lt;- c(TRUE,FALSE,TRUE,TRUE,FALSE) &gt; sum(x) #To find number of TRUEs [1] 3 &gt; sum(!x) #To find number of FALSEs [1] 2 &gt; sum(x)+sum(!x) #To find length of vector containing only logical data [1] 5 &gt; length(x) #length function to get number of elements in a vector [1] 5",313826.0
82596,338735.0,"You have to first check the string is Number or not and it returns True then only it can be converted to Intgefer See below example. as.integer(""Joe"") # This is string so it can't be conveted to number so fist check using as.integer [1] NA Warning message: NAs introduced by coercion as.integer(""3"") # This is string and can be conveted to number so fist check using as.integer [1] 3",307843.0
82596,338704.0,"You can use as.numeric() function. For more information, type ?as .numeric in Console in R studio.",301652.0
82596,338702.0,"you shall use as.numeric() function. a &lt;- 10 # i am assigning a numeric value b &lt;- as.character(a) # I am coering it into a character value, only for illustration purpose c &lt;- '20' # i create another object c, and assign a string ('20' is not numeric, but string) b_numeric = as.numeric(b) # i use as.numeric() function to coerce into numeric value c_numeric = as.numeric(c) # i use as.numeric() function to coerce into numeric value",301561.0
82596,338844.0,My suggestion would be to use the parse_double() function from the tidyverse. The error handling is way better compared to the built-in functions as.numeric or as.integer.,306733.0
82596,338755.0,"you can use as.numeric(""3"") the function class() displays the datatype of the variable and you can also use the help by typing question ?as.numeric() or for anyother function to get the further details",300687.0
88283,365382.0,Related query: https://learn.upgrad.com/v/course/208/question/87137,313826.0
88283,365345.0,"f(person$housing == ""yes"" | (!is.na(person$salary) &amp; person$salary) &gt; 60000) Here Person is dataset and seems it has following attributes 1 Housing 2Salary Now person$housing == ""yes"" It means the value of person dataset having housing attibute for which the value equal to yes is evaluted. | This is Or condition (!is.na(person$salary) &amp; person$salary) &gt; 60000) Here first checking the person dataset having salary attibute should not be null if it true then salary is checked for person as &gt; 60000 if both condition is true it returns true i.e if (person has house or (person salary greater than 60000) if either of one condition is true overall evalution returns true. So coming to your question 1. (!is.na) is TRUE and person$salary= 50000, what will be the output of '&amp;' operator It will return false because and(&amp;) condition return true only if both condition is true and here salary is not greater than 60000 2. (!is.na) is FALSE and person$salary= 50000, what will be the output of '&amp;' operator? It will also return false because and(&amp;) condition return true only if both condition is true. and here both condition is false i.e (!is.na) is FALSE and also the salary is not greater than 60000.",307843.0
88283,365563.0,"Hey Ruchik, The parantheses in if(person$housing == ""yes"" | (!is.na(person$salary) &amp; person$salary) &gt; 60000) are misleading. You can refrase this like, if(person$housing == ""yes"" | (!is.na(person$salary) &amp; (person$salary &gt; 60000)) From here let's break the second segment of the code into 2 parts, 1. (!is.na(person$salary) : This will return a boolean value, correct. 2. (person$salary &gt; 60000) : This also returns a boolean value. So, a boolean value &amp; a boolean value can be the generic, T&amp;T is T or T&amp;F is F and so on. For your question on what will be the output of a numeric value &amp; a boolean value, any number other than zero is a TRUE in boolean while zero is a FALSE in boolean.",319721.0
83757,344611.0,"NA is coming for those person whose marital status is ""divorced"". Since Marital status = ""divorced"" is not dealt in the code. you are getting ""NA"" in my_decision column for those person.",312746.0
83757,344815.0,"It seems you have added one column called my_decision to the bank data set looking in the code you are assigning value yes for some condition,No for some codition similiary you must have writeen code to assign NA to my_desision for some condition, however you have not share the full code but you can check in your code.",307843.0
80627,327521.0,I have also encountered same issue. But it seems swirl validates from the set of predefined solutions that i has. But which(is.na(airquality$Solar.R)) also gives the same result and it is equally correct solution.,304694.0
80627,349198.0,"Lol, I was struggling at the same question. Was stuck for quite some time there as which(is.na(airquality$Solar.R)) was giving me the correct answer.",308962.0
80627,349201.0,It says - | The appropriate expression will be which(is.na(airquality$Solar.R)== TRUE). Hope you have leant about | the use of which() function as it is used frequently in data analysis. Can someone please explain why it is the more appropriate version than which(is.na(airquality$Solar.R))? Is it an industry practice? Thanks,308962.0
83514,343171.0,"For this piece of code, the outermost ""if..else if"" condition checks for the attribute ""person$maritial"" to be either ""married"" or ""single"" and based on this some more checks are made to derive a value for ""my_decision"" variable. However, please note that the ""maritial"" attribute has three possible values. &gt; unique(bank$marital) [1] married single divorced Levels: divorced married single There is no code to handle the ""maritial"" attribute with value ""divorced"" and hence the value ""NA"" gets inserted in ""my_decision"".",313826.0
86987,358168.0,"Shiv -See below the explation hope it will helps it is explained you can run the same in R-consolewe have data frame stored in names(Height,Weight,Age) then separate as per needs you can retrive which ever row and columns =======================code ============================================= df &lt;- data.frame( c( 183, 85, 40), c( 175, 76, 35), c( 178, 79, 38 )) names(df) &lt;- c(""Height"", ""Weight"", ""Age"") # All Rows and All Columns df[,] # First row and all columns df[1,] # First two rows and all columns df[1:2,] # First and third row and all columns df[ c(1,3), ] # First Row and 2nd and third column df[1, 2:3] # First, Second Row and Second and Third COlumn df[1:2, 2:3] # Just First Column with All rows df[, 1] # First and Third Column with All rows df[,c(1,3)]",307843.0
86987,358280.0,"The general syntax of the for loop is: for( var in seq ) { expr } where var is the iterating variable , seq is a vector of elements on which the iteration needs to be done and expr is the set of statements that need to executed for each iteration. So, to iterate over a specific number of rows of a dataframe, say in this example 5th to 10th row, then something like this could be done for (i in 5:10 ) { person &lt;- bank[i, ] &lt;some more logic&gt; }",313826.0
87596,363207.0,"There is a section on installation of swirl package under ""Introduction to R-&gt;Vectors"". Attached screenshot of the instructions. Hope it helps.",313826.0
87596,363957.0,invoke swirl by swirl(),308437.0
88228,364906.0,The code that you have written is correct. Swirl is looking for a specific code pattern which(is.na(airquality$Solar.R)== TRUE) Please refer the below link on a similar question. https://learn.upgrad.com/v/course/208/question/80627,313826.0
87187,359044.0,The code that you have written is correct. Swirl is looking for a specific code pattern which(is.na(airquality$Solar.R)== TRUE) Please refer the below link on a similar question. https://learn.upgrad.com/v/course/208/question/80627,313826.0
90873,392797.0,"As you see distinct clause used in the query, answer is straight forward isnt it :)",313228.0
90873,381008.0,Distint students will revert with number of different students by eliminating duplicates . without Distinct resultant count will be more than actual as it will be having repititions .,306738.0
90873,379372.0,There will be difference in output if there is any duplicate entries in the table for this instructor ID in the table. You need to select the best of the two based on the question. If the question asks about distinct or just the count.,301555.0
90873,381928.0,"Since Distinct word is used, therefore the duplicate enteries will not be printed. Therefore total number of distinct students is the crrect answer.",304319.0
90873,388364.0,The number of count using distinct will be less if we have any duplicate records.,310529.0
91085,381602.0,"In scenarios like this, there is no other choice to put a detailed info in one page. Because if you try to split them, it becomes more confusing, as then you cannot correlate. And hence, we had to go with this approach.",301555.0
91085,380405.0,"yes it is quite a lot of info in one picture to be honest but basically you need to match all four, so one from each: abcd 1234 I,II,III,IV ABCD so if you chose a-1-I-A -&gt; would that be correct? (for example)",300694.0
91085,380421.0,"These are match the columns. Column 1 needs to be matched to correct entries with column 2 , column 3 and column 4. The names of the database tables, the database table column names mentioned in column 2, primary key for table out of the database table column names mentioned in column 2 etc. The names in each column should evidently convey you the answer.",311729.0
91085,380739.0,"As you already know, there are many things that can be defined at the time of creation of table. So basically this question is designed in such a way that everything gets covered in one question. For ex: - If we have employee table, then what column could be there in that table, what would be primary key to identify unique rows, what could be foreign key. So in the question you have to match Table---&gt; Column(Fields)---&gt; Primary key---&gt; Foreign key Hope this helps.",317991.0
88701,367351.0,"Basically, if you get it correct in first attempt you will get 10 marks. If your 1st attempt is wrong and get it correct in second attempt, you will get 5 marks. The above is only for questions that provide you a total of 2 attempts to answer. In cases where question provides you with only 1 attempt, your correct attempt would fetch 10 marks and thereafter no marks, because no more attempts would be left. Hope that clarifies. For more information on grading, you can also refer to: https://cdn.upgrad.com/UpGrad/temp/f39088ba-dbb2-453a-891a-188e22144cc5/Student+Assessment+And+Learning+Experience+Manual_DS+C8.pdf",317998.0
88701,367429.0,"It is quite clear here. in question with 2 attempts, u will get 10 marks for the first attempt and in second u will get 5 marks. in questions with 2 attempt -when u will attempt on correct u will get 10 marks and on incorrect no marks i.e 0.",305847.0
88701,380411.0,"There are two types of questons. Some questions have only one attempt. In such questions, in order to secure complete marks i.e. 10, you need to answer these correctly in first attempt only else you loose all marks for that questions. Second type of questions carry two attempts. If answer you provided is right in first attempt, you get complete 10 mars for that question, if your answer goes wrong in firat attempt but right in second attempt, you score 5 marks out of 10. However, if it still goes wrong in second attempt as well, then no marks wil be awarded for that question. That is all the complete interpretation of the table.",311729.0
88701,380783.0,"1. For questions which have only 1 attempt you'll score 10 marks for a correct answer and 0 for wrong. 2. For questions with 2 attempts, you can score 10 marks if you have given correct answer on 1st attempt, else you'll score 5 marks if you have given correct answer on 2nd attempt, else you'll score 0 marks if you fail to answer correctly in both the attemps.",303671.0
89008,368992.0,I've given all the graded questions in sql. They don't ask us to write sql but only interpret them for the best result. We don't need actual tables for that as it is conceptual knowledge that they are testing at this moment. You will easily solve them. Good luck 🙂,310974.0
89008,369994.0,The questions are based on concepts of star schema design and examples of fact tables and dimesntion tables. So a real table / DB creation of any kind is not necessary.,300717.0
89008,380394.0,The questions are about understanding the syntex of the queries and underlying principles. There is no database extract required to solve these questions. You only need to have the understanding and interpreting the concepts of the RDBMS.,311729.0
92397,389164.0,"when an Ssn is given as input to UDF ,output should be avg distance from that Ssn house to the other employees house, call the above function for the Ssn whose (super_ssn isnull) calculate the average distance between the CEO house to others",301115.0
92397,389190.0,It should be something like Select absolute of average of hno - (select hno of employee whose superssn is null) for all the employee from employee table whise super ssn is not null,317811.0
90797,378422.0,Please go through the lecturer notes which was given in the introduction.,318804.0
90797,378387.0,Since it is a question related to assignment I can't directly answer. But what I would suggest that go head and write code for next two subtask i.e Subtask 2.5 and 2.6. At the end there is checkpoint as follows: Checkpoint 1: You might have noticed that we still have around 77% of the rows! If you are getting this checkpoint your solution for 2.4 is correct. Hope this helps.,317991.0
90797,378459.0,it is possible to have such a situation. even then you should write the desired code as instructed in the task.and move ahead. the idea is to understand how it is handled which can come handy for some another dataset.,311686.0
90797,379697.0,"We ought to check the output at respective checkpoints to validate if we missed something in previous codes. As far as the above query is concerned, i guess the idea is to check our understanding of the question and how we implement the code to find the solution. The output will confirm one way or the other that our understanding is in the right direction and if we are on the right track.",319302.0
93437,,nan,
134277,583728.0,Yes it is possible to implement window function for spark dataframe. Below links has step by step explanation as how to use window function in spark dataframe. https://knockdata.github.io/spark-window-function/ https://datacenternotes.com/2016/10/03/getting-started-with-spark-part-3-udfs-window-functions/ Hope this helps.,317991.0
133986,582920.0,I dont think the number of unique precincts would exceed 73 in the dataset given to us. Do check your calculation again. We need to count only the DISTINCT UNIQUE precincts and that would not exceed 73 in my opininon. Hope that helps.,317998.0
133986,582967.0,"@TA, please confirm if the dataset hosted on the lab isn't exactly matching with the one present in the kaggle?",311160.0
133986,583011.0,Better to use the data mentioned in assignment path '/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv',312019.0
133986,582995.0,"How does it matter? We need to perform our analysis on the dataset that is Hosted on the Lab. If at all it does matter, You can rerun the notebook with the Kaggle dataset and check if the values for each question is coming same or not. all you would need is to change the location for reading the dataset. Hope that helps.",317998.0
133986,583044.0,Please refer below discussion on similar query https://learn.upgrad.com/v/course/208/question/133517 The gist is to clearly state the assumptions and take actions accordingly.,313826.0
133913,582860.0,Value is big which int cannot handle. hence you are getting null. you need to use appropriate datatype,311254.0
133344,580933.0,Imporst isnan from sql funtions. Using this you will get the count of null values for each column. Below link will help you in detail about how to use it over the entire dataframe https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe?noredirect=1&amp;lq=1,308673.0
134212,583534.0,Renaming column names is not mandatory but can be done for ease of usage especially when running sql queries.,313826.0
134212,583538.0,"Thanks Vinay/Vipul- Actually i created a SQL table like df.createOrReplaceTempView(""df_sql""). Since it contains spaces as we know the below query wont work . can you please suggest in this case # after registering temp table you can run your sql queries spark.sql('select Issue Date from df_sql').show",314612.0
134212,583533.0,"There is no requirement mentioned in problem statement regarding change of column name. So it totally depends on you. If you feel that renaming name will make your task easier then you can rename, if not then you can leave as it is.",317991.0
134219,583557.0,You are getting the error becuase of the space in the column name . Enclose ticks ` (the one below Esc button) like `Violation Code` and then execute the query. Ex.: spark.sql('SELECT `Violation Code` FROM p2table').show(),313826.0
134234,583582.0,"Check for columns with literal string value as ""nan"".",313826.0
134234,583856.0,"No, it's fine to get 0 as the answer. You are good to go.",428646.0
134234,583747.0,"There might be several Data Quality issues that you would need to treat in order to do proper analysis. for example: we need only the records for 2017. Similarly, ""NaN"" as a string is also a data quality issue that needs to be treated appropriately. Furthermore, there are hints also mentioned in the questions and one can take cue from those as well. Also, assumptions can be made and mentioned in comments, if you are not sure how to deal with a particular data quality issue. Hope that helps.",317998.0
134238,583635.0,There's some ambiguity as on the problem statement page it says only jupyter notebook but on submission page it says jupyter notebook plus pdf/doc in a zip file. Request TA to clarify on this.,313826.0
134238,584913.0,"Yes. There is ambiguity that does we need to submit only one Jupyter notebook containing all questions, solutions, and insights OR do we need to prepare separate pdf. TA. Please clarify.",311115.0
134238,583592.0,As per the general guidelines mentioned that can be seen below we have to submit one jupyter notebook only.,317991.0
134238,583615.0,No. If we see Submission its bit different . required pdf too https://learn.upgrad.com/v/course/208/session/37079/segment/197895,312019.0
134238,585265.0,"Hi all, You need to 1. Include a code file consisting of the analysis. 2. Include a .docx/.pdf file with solutions to the questions which has subjective inferences, plots, etc. to this text file 3. Collect all the files and Zip them together.",301618.0
134059,583006.0,"I dont think any partitioning or bucketing is needed or required for this case study. Only the analysis as per the questions mentioned would suffice, in my opinion. Hope that helps.",317998.0
134059,583007.0,"For this assignment as mentioned in the problem statement we only have to perform Exploratory Data Analysis in accordance with the question given. Also if you go through Evaluation Rubric the marks alocated are 10% each for Reading Data, Submission Quality and coding guidelines and rest 70% is allocated for EDA. This also confirms that we don't have to perform anything else but Exploratory Data Analysis. Rest TA can confirm",317991.0
134059,583024.0,Only EDA.,313826.0
134254,583687.0,Please close the pyspark session and try again.,311861.0
134254,583858.0,Try stopping all active notebook by shutting the kernel. Then try again. If this didn't work contact Corestack.,428646.0
134254,583764.0,"Try restaring the kernel from jupyter and if the problem persists, stop SparkSession by spark.stop() and create a new spark session to continue.",306726.0
134782,585478.0,Only one Python notebook with all code and explanation. Apologies for confusion.,428646.0
134782,585476.0,"I agree about the confusion due to the 2 apparently conflicting answers by TA in https://learn.upgrad.com/v/course/208/question/134238 . However, I would recommend going with the more detailed answer i.e. you need to 1. Include a code file consisting of the analysis. 2. Include a .docx/.pdf file with solutions to the questions which has subjective inferences, plots, etc. to this text file 3. Collect all the files and Zip them together. More importantly, there is nothing to lose by adding the pdf/doc file when actually not needed but a lot to lose by not adding the pdf/doc file when it was actually needed. So better have it added.",318762.0
134288,583765.0,"Quoting the TA twice: ""Perform Data cleaning wherever asked in the question. Nothing more. Other than that everything is fine if properly mentioned in the notebook."" ""As I said whatever you are getting in your analysis you can report them. Since this is an assignment I can help you with the explanation. The question asked is to check and find a way to treat nulls, you can answer accordingly."" You can refer to the following TA verified answer and TA's answer: https://learn.upgrad.com/v/course/208/question/134096 Hope that helps.",317998.0
134288,583767.0,"You need to perform handling of any data quality issues as and when mentioned in the question. For ex: For Aggregation task Ques 3 part 2 - it is mentioned that 'Issuer Precinct' (This is the precinct that issued the ticket.) Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts. ( Hint : Print the top six entries after sorting.) So you need to take care of Violating Precinct' or 'Issuing Precinct' as '0' while performing this task only. This is mentioned and verified by TA in below link https://learn.upgrad.com/v/course/208/question/134096 Hope this helps.",317991.0
134367,583938.0,Hi Shubham Please refer to. the TA verified answers and method that can be adopted. https://learn.upgrad.com/v/course/208/question/133624,301121.0
134388,583987.0,"Hi Arpit, Just hint to group then accordingly and sort to find the appropriate answer. Hope this helps!",318427.0
134388,583967.0,"Since this is an assignment related query, so cannot provide direct answer. Hint would be to sort the result and also look at option to choose only few rows from the output.",313826.0
134388,585045.0,"Have a look at PySpark live session by Sumit, he has explained the steps.",318448.0
134422,584085.0,"Please refer to the live session by TA yesterday(a mail with link was sent in this regard), where in he had provided hints on how to approach this problem. Basically, you need to find a pattern, then extract hour of the day keeping in mind AM / PM and finally bin them into 6 groups.",313826.0
134422,584084.0,"Similar question has been answered and approach to convert ""violation time"" has been mentioned in below links: https://learn.upgrad.com/v/course/208/question/133624 https://learn.upgrad.com/v/course/208/question/134367 https://learn.upgrad.com/v/course/208/question/134337",317991.0
134222,583852.0,"1. Getting no nulls is fine. 2. Check your violation time conversion, if that is correct. I don't think Unix time conversion will be able to convert it. The way you are converting the time column is not correct. Please try some alternative way.",428646.0
134222,583793.0,"I would suggest you delete this post as you are giving away the logic. As for the nulls post conversion, you can drop them.",310974.0
134677,585102.0,"I got this resolved. Thanks for the other blog. https://learn.upgrad.com/v/course/208/question/134324 We need to make sure there shouldn't be show while storing dataframe into other dataframe. If we have show while storing into other dataframe, then the subsequent functions used on the new dataframe will have null pointer exceptions as NoneType..",310518.0
134447,584168.0,"If you are updating a dataframe with this code then try with removing .show(5) from the end of the code i.e. df = df.withColumn(""Mon"", month(to_date(df(""date_in_dateFormat""),""yyyy-mm-dd""))) I hope this will help.",317991.0
134447,584169.0,"Refer below link on a similar query https://learn.upgrad.com/v/course/208/question/134324 Please check if df is still a pyspark dataframe. My guess is that you may have assigned output from some previous line of code with .show() back to df, thus making it a 'NoneType' object.",313826.0
134453,584179.0,"Problem is with this piece of code : df2=df1.withColumn(""month"", date_format(to_date(""date_in_dateFormat"", ""yyyy-mm-dd""),""M"")).show(5) Instead, execute without .show(5) like df2=df1.withColumn(""month"", date_format(to_date(""date_in_dateFormat"", ""yyyy-mm-dd""),""M""))",313826.0
134453,584814.0,Use collect () instead of show () while creating a new df,428646.0
134453,584180.0,"When you are updating a dataframe, don't use show(). For ex: df = df.withColumn(....some condition).show() is probably the cause of error. Instead use df = df.withColumn(....some condition). And then to check you can use show() df.show(5)",317991.0
134442,584155.0,Yes your understanding is correct,311254.0
134442,584159.0,"As mentioned in the problem statement: ""The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups."" Whatever data cleanup you are performing while following this process make sure to add it into assumption. For value of 1235A == 0035A, you can follow any approach here and mention it in comments.",317991.0
134442,584166.0,"Depending on the assumptions that you would have made, this could be either right or wrong. State the assumptions clearly and any actions based on such assumptions are valid.",313826.0
134337,583861.0,Can you refer to my live session that I took yesterday? I have explained the approach there. Ask your mentor for the link to the session.,428646.0
133586,581578.0,"As per frequency defination "" Frequency is the number of occurrences of a repeating event per unit of time"" But as mentioned in statement ""The purpose of this case study is to conduct an exploratory data analysis that will help you understand the data."" Whichever frequency you feel gives more insight about data or can reveal any pattern in the data, you can select that one. So it all depends on you to decide which frequency term you want to use.",317991.0
133586,581573.0,My understanding is that we are supposed to take the frequencies based on count. Rest TA can confirm.,313826.0
134119,583367.0,"For that after you aggregate based on a function, you can sort them (ascending or descending as per your choice) and then use the SHOW() function to display the top two (or any number) of rows like .show(2) . for example: clean_df.groupby(clean_df['Registration State']).count().sort(desc('count')).show(2) Hope that helps.",317998.0
134119,583510.0,This is very hard and I don't think I have worked on such use case. Offering this question to others to answer.,428646.0
134119,583480.0,"as I said to the same qn on Whatsapp - I spent a long time on this Previous assignments I had used the nlargest(n) function but with pyspark that is not workig; so I did a workaround and sorted by the count and derived conclusions with that I have seen approaches with resetting the index, specifying index levels, etc etc - but could not get it to work and my workaround gave me the info I needed so I did not investigate further",300694.0
134119,583319.0,"Thanks for your response. However, i want to now find out the top two rows based on some aggregate function for each combination of grouping",312096.0
134119,583309.0,"Grouping by two columns will mean that you are grouping by the 1st column as the primary column. Then the groups formed by those columns will be grouped by the second column . For example, the 1st column has A,B,C distinct/unique values and 2nd column has X and Y as the unique values. Then you will get the following groups: ( if you groupby the two columns. 1st col being primary) A X A Y B X B Y C X C Y You can also refer to the following for more insight on the syntax and how to use groupby in pyspark: https://stackoverflow.com/questions/36251004/pyspark-aggregation-on-multiple-columns Hope that helps.",317998.0
134119,584511.0,you can use windows functions - I have managed to get it working perfectly with that,300694.0
133094,580439.0,"I checked the data placed under the path '/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv' on Hue File browser and found that the dataset has only 10 columns as mentioned by you. Since we have been asked to use this data for analyses, I believe we just need to go with whatever columns that have been provided here. Below is a snapshot of the data as seen from the hue file browser",313826.0
134487,584317.0,"Try closing the Pyspark file, Main console running tab, Shutdown all your Pyspark files. then logout and login and try again.",317514.0
134487,584438.0,"As suggested by Rajesh, shutdown all running notebooks from ""Running"" tab on the jupyter notebook home page, then login freshly and try again. If issue still persists, raise issue with corestack support.",313826.0
134487,584307.0,"As you have mentioned that you have tried restarting jupyter and kernel, and after that also it is not working. So try to logout from the corestack platform completely, wait for sometime and login again and check if it is working or not. If it still does not work then I would suggest to raise an issue with corestack team.",317991.0
134225,583563.0,The problem could be because of inactivity of jupyter notebook You can restart notebook by logout and login again in notebook Or You can use Kernel -&gt; Restart and Run All or Restart and Clear output without logout from jupyter notebook. Hope this helps.,317991.0
134744,585309.0,"Agree that there's a lot of ambiguity. However, I would suggest the following : 1. Create a well commented jupyter notebook. 2. A pdf/doc file with all the questions and answers answers including any inferences, insights, plots etc. 3. Zip files 1&amp;2 and submit.",313826.0
134744,585328.0,"I too agree about the confusion due to the 2 apparently conflicting answers by TA in https://learn.upgrad.com/v/course/208/question/134238 . However, I would recommend going with the more detailed answer i.e. you need to 1. Include a code file consisting of the analysis. 2. Include a .docx/.pdf file with solutions to the questions which has subjective inferences, plots, etc. to this text file 3. Collect all the files and Zip them together. More importantly, there is nothing to lose by adding the pdf/doc file when actually not needed but a lot to lose by not adding the pdf/doc file when it was actually needed. So better have it added.",318762.0
134744,585344.0,"@ TA Please confirm, as some of us would have already submitted the single Python Notebook as told by one of the TA previously. This is really very disappointing!",318355.0
134744,585490.0,"One Python notebook with all results, inferences and conclusion. If you wish to create a separate PDF with inferences and conclusions you may submit a PDF file too but it is completely optional.",428646.0
134744,585444.0,"I want to bring some more attention on this topic of submitting how many &amp; what all files. This is really becoming an assignment in iteself to decide how many files are required to submit. The post which I saw was : https://learn.upgrad.com/v/course/208/question/134238 As seen below, this question was asked 3 days ago i.e. 21st June, 2019 And then one of the candidate answers the question on the same day &amp; request the TA to verify. To which the TA replies that we need to submit only .ipynb file as shown in the screenshot below: And then finally a TA answers this question today i.e. after 3 days &amp; rather on the day when assignment needs to submitted redirecting all of us in a new direction. The same can be seen in the screenshot below. There might be some sort of confustion but in the ideal case, as most of us including me have already submitted the one python notebook as mentioned by one of the TAs. How can one expect that now we should submit a doc file as well. We're working professionals &amp; its hard to find time from our daily routines especially into entirlely different domains. One cannot expect that now we'll create the doc file &amp; resubmit.",318355.0
134501,584335.0,"You can take fiscal year data i.e. from July 2016-June 2017 or you can take year data i.e of 2017, it is upto you. You can make any assumption and state it in comments. Also I would suggest you to go through the live session recording on Apache Spark, the approach to perform this assignment was mentioned clearly in that session. Hope this will help.",317991.0
134501,584434.0,"As per the live session by TA, you could take the entire dataset as belonging to 2017 or filter data only for year 2017 (Jan-2017 to Dec-2017) or take fiscal year 2071 (Jul-2016 to Jun-2017) as long as you have stated your assumption clearly.",313826.0
134501,584818.0,Consider the complete data set as 2017.,428646.0
134501,584532.0,"Based on reading the problem statement given for the assignment, and breaking down your question into two parts: 1. do we need to only take 2017 data? This has been answered several times already in the discussion forum. Yes 2. FY17 - where does it say FY anywhere in the problem statement? Are you sure the given data is only for fiscal 2017?? Have you check if there is data for each month of 2017? I still struggle to understand why you would introduce the concept of financial year into the assignment",300694.0
134501,585043.0,"You can take the data for a fiscal year, i.e. date&gt;=2016-07-1 and date&lt;=2017-06-30.",318448.0
134501,585292.0,"Based on the live session with our TA, these is no need to filter date for 2017. We can consider entire dataset for the analysis",305650.0
134501,585252.0,I filtered the dataset to contain only 2017 data and proceeded accordingly,319759.0
134093,583324.0,"You can use when from pyspark as follows: from pyspark.sql.functions import when targetDf = df.withColumn(""timestamp1"", when(value to be replaced, new value).otherwise(df[""timestamp1""])) Hope this helps.",317991.0
134093,583246.0,"Here, even 99 seems to be a string. Try df.Series.replace('99', 'to the desired value')",311160.0
134093,583291.0,"You can use the replace() function of dataframes as follows: dataframe.replace([Value that is to be replaced] , [Replacing Value], [Column Names in which to replace]) Note: one can provide the parameters as lists also. The values would be replaced respectively in only the subset of the columns in the list (3rd Parameter). Hope that helps.",317998.0
134093,583809.0,"One easy way to achieve it is as follows : from pyspark.sql.functions import when new_dF= df.withColumn(""colName"", when(df[""colName""]==""oldValue"",""newValue"").otherwise(df[""colName""]))",306726.0
134093,583835.0,"Hi, I have tried both replace and using functions. Not working. new_df = df1.withColumn(""`Registration State`"", F.when(F.col(""`Registration State`"") == '99' ,'R').otherwise(F.col(""`Registration State`""))) This is not working. After creating temp table when I am checking the values, 99 still exits",301643.0
134046,583325.0,Do the data cleaning as per described in the questions. Since the data cleaning part can change the results. Please do it wherever asked. Also of you have taken any assumptions do mention it clearly. Don't remove NULL values if not asked.,428646.0
134046,583052.0,"If any data is not conforming to the values a particular attributes can have, then you may classify such entries as data quality issues and take appropriate actions, provided you mention the assumptions clearly.",313826.0
134046,582972.0,"If you go by evaluation rubric it is mentioned that we have to perform data cleaning as mentioned in the question. So wherever data cleaning required, it is mentioned in the question itself. Also if you find any other data quality issues you can treat as you want. You can mention the assumption.",317991.0
134046,582963.0,"Since this is a Graded assignment, cant elaborate in detail, But, There might be several Data Quality issues that you would need to treat in order to do proper analysis. for example: we need only the records for 2017. furthermore, there are hints also mentioned in the questions and one can take cue from those as well. also, assumptions can be made and mentioned in comments, if you are not sure how to deal with a particular data quality issue. Hope that helps.",317998.0
134069,583057.0,"Analyses can be done in pyspark dataframes, spark.sql or both depending on convenience.",313826.0
134069,583045.0,"The hint provided are general guidelines we can follow. It is mentioned that ""The analysis should be performed on PySpark mounted on your CoreStack cluster, using the PySpark library."" Both the approach i.e. pyspark or sql approach is fine, and it depends on you that which approach you want to follow.",317991.0
134069,583035.0,"I dont think there is any such relation between ""mentioning of hints"" and using ""SQL approach"" in the questions. The questions are well articulated in terms of when dataframes are to be used and when SQL is to be used. The hints mentioned just help in the direction we need to follow. Hope that helps.",317998.0
133837,582438.0,Please refer below link on similar query: https://learn.upgrad.com/v/course/208/question/133708,313826.0
133837,582597.0,You can access data dictionary from the link provided by Vinay. Alternatively to know which column has unique value you can count the distinct value of the columns and check against total rows of dataset. Since the number of column is less you can follow this approach for this assignment.,317991.0
134089,583355.0,this is where we can learn which piece of code works faster and learn to execute it by comparing the cost of execution in both.,301115.0
134089,583129.0,"That is the precise beauty of Spark! You can use dataframes like those in python or you can even use sql like that of mysql. If someone is very comfortable with mysql and has been using it on small data, then SPARK allows to continue using mysql style on BIG DATA. Similarly for dataframes. So, basically it is your choice and whichever method you feel more comfortable with . Similarly Spark also supports other languages such as R. Still, to get more insight on the comparison of performance and speed when using spark.sql and dataframes, you can go through the following article . They have very nicely and graphically depicted the performance of both methods for the same query. https://community.hortonworks.com/articles/42027/rdd-vs-dataframe-vs-sparksql.html Hope that helps.",317998.0
133517,581505.0,You can check actual precincts of New York and values other than that can be treated as erroneous. But just mention same in your assumption.,320103.0
133517,581789.0,"Hey, Check other values too. If you feel that the value is a error value do mention it in your analysis. Remember any assumption taken is valid until you mention it clearly in your notebook.",428646.0
133517,581620.0,You need to exclude erronous value 0. Hint provided was - Print the top six entries after sorting Alternatively in where clause you can exclude 0,317514.0
134096,583304.0,the nan you mention is not a tru NaN/Null - it is a string,300694.0
134096,583295.0,"There might be several Data Quality issues that you would need to treat in order to do proper analysis. for example: we need only the records for 2017. Similarly, ""NaN"" as a string is also a data quality issue that needs to be treated appropriately. Furthermore, there are hints also mentioned in the questions and one can take cue from those as well. Also, assumptions can be made and mentioned in comments, if you are not sure how to deal with a particular data quality issue. Hope that helps.",317998.0
134096,583337.0,Perform Data cleaning wherever asked in the question. Nothing more. Other than that everything is fine if properly mentioned in the notebook.,428646.0
134096,583255.0,"For Violation time, what I have observed is 'nan' is string value, not a true Nan. As such the inbuilt functions are unable to capture NULL or NaN.",318438.0
133538,581465.0,"In case of erroneous data in any dataset, the recommended step is to remove those erroneous data. But its upto you to delete it or leave as it is. Just make sure whatever assumption you make, do mention it in comments.",317991.0
134542,584550.0,Check the missing values of all columns by which you will be able to decide whether to delete that record or to impute that value with adequate explanation. I hope this helps.,301121.0
134542,584513.0,"we have been dealing with missing values and invalid values for almost a year in our previous assignments. By now, we should now what it means to 'deal with missing values' and how to go about it",300694.0
134542,584939.0,The values may also contain invalid formats.,306248.0
134542,584747.0,"Specifically for Ques 5. as mentioned in statement ""Find out the properties of parking violations across different times of the day: Find a way to deal with missing values, if any."" We have to consider those columns related to parking violations and check if there are any null values and if there are any we have to deal with missing value. Hope this wiill help.",317991.0
134542,585296.0,There are no null values in the dataset. NA written in any of the column can be treated as string,305650.0
134542,585254.0,Rows with Missing values can be dropped and with invalid values be imputed with appropiate ones,319759.0
134097,584429.0,You can refer https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm for detailed understanding of SparkContext,316147.0
134097,583175.0,"Prior to Spark2.0.0 sparkContext was used as a channel to access all spark functionality. SPARK 2.0.0 onwards, SparkSession provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with Dataframe and Dataset APIs. All the functionality available with sparkContext are also available in sparkSession. So I would suggest to use SparkSession instead of SparkContext. To initialize SparkSession spark = SparkSession.builder.appName(""AppName"").getOrCreate() If you want to use SparkContext, to initialize SparkContext from pyspark import SparkContext sc = SparkContext(""local"", ""simple app"") Source: https://stackoverflow.com/questions/24996302/setting-sparkcontext-for-pyspark Hope this helps",317991.0
134097,583303.0,"this is already given to us in sample files provided by Upgrad/IIT .. something like spark = SparkSession.builder.appName(""yourAppName"").getOrCreate()",300694.0
134556,584709.0,Check if there are any null values in the columns that you are using for aggregration.,313826.0
134556,585352.0,Thanks. That helped.,305653.0
134559,584555.0,Hi Rambalu It was suggested live session that you can work on the entire data without filtering. Happy coding!,301121.0
134559,584584.0,"Hi Rambabu, Filter on: The data needs to be filtered only by the year i.e. 2017 as provided in the assignment details. Not to filter on : First of all, there is no column in the given dataset on which we can filter by state. 'Registration State' might seem the column to filter upon but the registration state is the state where the vehicle was registered and the vehicle actually violated parking rules in New York City. Therefore, the dataset contains records with differenct registration state as well. If we do the filtering on registration state as NY, we will loose out the actual data. Hope this helps!",318355.0
134559,584938.0,Filtering on states not required,306248.0
134559,584751.0,"No need to filter the data for NewYork state. Also TA has mentioned that ""Perform Data cleaning wherever asked in the question. Nothing more. Other than that everything is fine if properly mentioned in the notebook."" Link: https://learn.upgrad.com/v/course/208/question/134096 Hope this helps.",317991.0
134565,584591.0,"Hi Suja, According to the given question, we need to find the count for tickets for each of the Vehicle body type &amp; Vehicle make individually. These are two separate question, no need to find the count of tickets of vehicle make in relation to vehicle body type. However, you can do it as additional effort.",318355.0
134565,584937.0,Use 2 different queries to find the result.,306248.0
134568,584643.0,"To add on to this question, its mentioned that we need to submit only the Jupyter Notebook in the General Guidelines as shown in the screenshot below: I request the TA to please answer this as soon as possible.",318355.0
134568,584720.0,Need to submit only a well commented jupyter notebook (.ipynb). Refer below link on related query: https://learn.upgrad.com/v/course/208/question/134238,313826.0
134568,584936.0,jupyter notebook,306248.0
134568,585315.0,"As answered by TA in https://learn.upgrad.com/v/course/208/question/134238, you need to 1. Include a code file consisting of the analysis. 2. Include a .docx/.pdf file with solutions to the questions which has subjective inferences, plots, etc. to this text file 3. Collect all the files and Zip them together.",318762.0
133624,581767.0,"As per the problem statement: ""The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups."" So, the ""Violation Time"" does not contain a string which resembles known DateTime formats. We will have to analyze the various values that are present in this column and find a pattern using which we will have to do transformations to finally get the data into DateTime format. Wouldn't be able to provide any hints as this is an assignment related analysis.",313826.0
133624,581676.0,You can follow below links to change datatype from string to timestamp or date format https://stackoverflow.com/questions/38080748/convert-pyspark-string-to-date-format/38081786 https://stackoverflow.com/questions/39088473/pyspark-dataframe-convert-unusual-string-format-to-timestamp Hope this will help,317991.0
133624,581834.0,"We need to Convert the given Violation Time into proper format so that we can utilize this in our Analysis . You can convert this various options Option 1 - Via writing UDF and add a new coloumns Option 2 - Via SQL Select cases , IF Else or EXPR Hint - If time is 0143A it is 01:43 AM and If time is 0400P it is 04:00 PM . Example - https://sparkbyexamples.com/spark-when-otherwise-example/",311861.0
133897,582784.0,"""Violation Time"" has the time in a strange format and we need to find a pattern in this data to make sense of it. If you are referring to ""Issue Date"" column and trying to get the year from it , then there are few things that can be done While importing data into dataframe, use . option(""inferSchema"", ""true"") so that the column is imported correctly as DateTime. Once the table has been imported with correct datatype, we can use the standard year() function to extract year from date. The same function can be used in both pyspark dataframe and TempView sql.",313826.0
133897,582675.0,"Similar kind of question has been asked, answered and verified by TA in below link https://learn.upgrad.com/v/course/208/question/133624 Hope this will help",317991.0
134111,583245.0,"Although sql view way is bit easier than dataframe way. You can follow below approach: - While importing use inferSchema = True, this will automatically select datatype for the columns as per the csv file itself. - Once the correct datatype is selected, you can use year() function to get year from it. Hope this helps.",317991.0
134111,583302.0,it can be done by importing pyspark.sql.functions (which is needed anyway for other operations) and then using where and rlike to look for a string .. say '2017-' in the Issue date field,300694.0
134111,583281.0,Please check out below link on a similar query https://learn.upgrad.com/v/course/208/question/133897,313826.0
132888,579600.0,"It seems this session was originally done for Spark R , but now for our batch it has been modified to PySpark.So theere are still some earlier mentioned of Spark R. We can ignore that and focus on PySpark. Let's wait for TA Confirmation ..",311861.0
132888,579548.0,"I believe this has been put in by mistake, since our entire course has been done using python till now. TA - Please verify and have it corrected. The section "" Questions to Be Answered in the Analysis "" appears twice - once with SparkR and second time with PySpark. Link : https://learn.upgrad.com/v/course/208/session/37079/segment/197894",313826.0
132888,579941.0,It's PySpark only guys. Apologies for the confusion.,428646.0
133985,582932.0,"For this assignment, we have been asked to do analyses on a subset of the NYC Parking violations dataset which is present at '/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv' Further, only 2017 records are in-scope for this assignment. As confirmed by TA in other queries, we can filter out in-scope records and proceed with the analyses.",313826.0
133985,583247.0,"If I take June 30, 2017, as upper Limit than values for Seasonality analysis will be Gone since I have divided a whole Year into American seasons, Summer, Winter, Fall, and Spring as the questioned Mentioned OVER 2017 not ONLY 2017 . SO WHAT Seasonality INTERVAL SHOULD I PICK assuming only for the Fiscal Year 2017 i.e for only 6 Months? Any Suggestions",317984.0
133985,582916.0,"Although Kaggle site contain data as per fiscal year i.e. July 2016 to June 2017, But for this assignment we need to focus on parking tickets over the year 2017 . Similar sort of question has been answered and verified by TA in below link https://learn.upgrad.com/v/course/208/question/133484 Hope that helps.",317998.0
133985,582924.0,"For the scope of this analysis, we have to analyse the parking tickets over the year 2017. So you can follow any of two approach mentioned below: 1. Filter data for 2017 data without deleting remaining data as suggested by TA in below link https://learn.upgrad.com/v/course/208/question/133718 2. On kaggle site it is mentioned that ""The files are roughly organized by fiscal year (July 1 - June 30)"" so you can take June 30 as upper limit for year 2017 and delete rest of the data. Just make sure whichever approach you follows do mention it in comments. Hope this will help.",317991.0
134627,584804.0,"You can use below code to concatenate column in dataframe using pyspark df = df.withColumn('joined_column', sf.concat(sf.col('colname1'), sf.col('colname2'))) Source: https://stackoverflow.com/questions/31450846/concatenate-columns-in-apache-spark-dataframe",317991.0
134627,584824.0,"I don't think there is a need of merging. Try to complete this task without merging, it will be easy.",428646.0
134627,585449.0,"For example: # find top 3 violation precinct violation_pre_freq = df.groupBy(""Violation Precinct"").agg(F.countDistinct(F.col('Summons Number')).alias('ticket_count')) violation_pre_freq = violation_pre_freq.orderBy(F.col('ticket_count').desc()).limit(3) violation_pre_freq = violation_pre_freq.select('Violation Precinct') violation_pre_freq.show(100, truncate=False)",319759.0
134627,584827.0,"Using Pyspark Dataframe: df = df.withColumn('col_name),when(some_condition1).when(some_condition2)..otherwise(default_condition) Refer this link for more clarity : https://stackoverflow.com/a/51565873 Using sql: df = spark.sql(""select *, &lt;logic for creating bucket&gt; from table"")",313826.0
134666,585463.0,"you can assume the time is given in hhmmA (for AM) and hhmmP (for PM) and delete the few rows which doesnt follow this format. It is mentioned in problem statement that you can make few assumptions , document it and proceed with data processing.",319759.0
134666,585071.0,"These violation_time does not reflect valid time, so we can ignore these values. As suggested by TA, don't filter it out, just ignore these values. Also make sure whatever assumption you make , do mention them in comments.",317991.0
134380,583950.0,Check what is the output of nyc_prk.show(5),428646.0
134380,583937.0,"Hey Please, refer this link https://spark.apache.org/docs/preview/sql-programming-guide.html Re-run it there must be just a small mistake otherwise I can't find any! df.createOrReplaceTempView(""your_data_name"") Are you using this command line by line, is there any other piece of code between this which you have mentoned?",311466.0
134380,584119.0,"Hi Surendra, I tried to run the code that you have written here. (exact copy paste) and it runs perfectly well. (attaching the screenshot). So, my best guess and suggestion for you issue would be to a. Restart the session, kernel and check again. b. To run the different lines of code in different cells. You might be running all of them at once and that could be causing the error. So, try and run it once like in the screenshot I have attached and see if it works or not. Hope that helps.",317998.0
133484,581228.0,We can always have our assumptions and can consider this as Out of Scope or In scope based on that .,311861.0
133484,581699.0,"The question states ' For the scope of this analysis, we will analyse the parking tickets over the year 2017 .' So only focus on that .. much easier ;-)",300694.0
133484,581179.0,Scope of the analyses are parking tickets over the year 2017. Any data that does not confirm to this (ticket issued in the year 2017) should be treated as Out-of-Scope and dealt with appropriately.,313826.0
133711,582892.0,"Scope of the analyses are parking tickets over the year 2017. Note: 2018 data is also present, So discard data less than 2017's",317984.0
133711,582043.0,Scope of the analyses are parking tickets over the year 2017. Any data that does not confirm to this (ticket issued in the year 2017) should be treated as Out-of-Scope and dealt with appropriately.,313826.0
133711,581988.0,"Although Kaggle site contain data as per fiscal year i.e. July 2016 to June 2017, But for this assignment we need to focus on parking tickets over the year 2017 . Similar sort of question has been answered and verified by TA in below link https://learn.upgrad.com/v/course/208/question/133484 Hope this will help.",317991.0
133824,582547.0,"this is a smaller subset of the data -&gt; using HUE File browser to view the input file we see only the following columns: Summons Number,Plate ID,Registration State,Issue Date,Violation Code,Vehicle Body Type,Vehicle Make,Violation Precinct,Issuer Precinct,Violation Time",300694.0
133824,582407.0,The dataset given contains that many column only which you are seeing i.e. only 10 column data . You can cross verify the number of columns from the file at below location in HDFS '/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv' Also this has been verified by TA in below link: https://learn.upgrad.com/v/course/208/question/133094 Hope this will help.,317991.0
134057,583004.0,"Lets say there are total 5 precincts. Namely 1,2,3,4 and 5. Now, suppose a there were NO violations that occured in precinct 4 and 5 in the year 2017 . Then violator precinct column would have the values 1,2 and 3 only as distinct values. (total 3 distinct values) And suppose, NO ticket was issued by precinct 4 in the year 2017 . Then, issuer precinct column would have values 1,2,3 and 5 as distinct values. (total 4 distinct values) . This is how it is possible that the number of distinct values for precincts is not matching for the violator precinct and issuer precinct columns. Hope that helps.",317998.0
133708,582042.0,"Metadata can be found on the below link: https://data.cityofnewyork.us/City-Government/Parking-Violations-Issued-Fiscal-Year-2017/2bnn-yakx Scroll down to "" About this Dataset "" and click on the "" OpenDataPortalMonthlyIssuance20160920.xlsx "" link under "" Attachments "" to open an excel containing the metadata details.",313826.0
133708,581995.0,"Although data dictionary is not provided for the assignment by kaggle, but I think the information given i.e. column name and what type of data it contain is sufficient to do this assignment. Also the name of the column itself reveal what it is for. So I think without the data dictionary also we can do our analysis for this assignment. Rest TA can give more insight.",317991.0
134266,583698.0,"""' 'Issuer Precinct' (This is the precinct that issued the ticket.) Here, you would have noticed that the dataframe has the'Violating Precinct' or 'Issuing Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts. ( Hint : Print the top six entries after sorting.) "" As per the above mentioned statement on the problem statement page, '0' value in violation and/or issuer precinct are erroneous values and we are supposed to provide top 5 correct precincts. You may just display top 5 correct precincts or filter out erroneous precincts or any other approach is fine, as long as the approach and assumptions are mentioned clearly.",313826.0
134266,583686.0,"It depends on your assumptions. The zero value is a valid value , that’s why it has been mentioned to show top 6 rows. But of course you can drop 0 records and state your assumptions.",311861.0
134266,583749.0,"There might be several Data Quality issues that you would need to treat in order to do proper analysis. for example: we need only the records for 2017. Similarly, Violation Precinct &amp; Issuer Precinct having 0 value for some rows is also a data quality issue that needs to be noticed (as is also mentioned in the hints). That is the reason why we are asked to display the TOP 6 instead of TOP 5. Furthermore, there are hints also mentioned in the questions and one can take cue from those as well. Also, assumptions can be made and mentioned in comments, if you are not sure how to deal with a particular data quality issue. Hope that helps.",317998.0
134297,583789.0,"It seems ""nyc_prk"" is not a dataframe but a list. Do check if by mistake you are converting ""nyc_prk"" into list. Because createOrReplaceTempView works on Spark Dataframe, so you need to convert your nyc_prk object into dataframe then you can create view from it.",317991.0
134297,583787.0,"Your vairable ""nyc_prk"" is most probably a List datatype. Do check it's datatype using dtype function. Tempview can be created from spark dataframes and the syntax is as follows: df.createOrReplaceTempView(""table1"") df2 = spark.sql(""SELECT field1 AS f1, field2 as f2 from table1"") df2.collect() where df is a dataframe. You can refer to the following official documentation for more insight on how to create tempviews: https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html Hope that helps.",317998.0
134668,585488.0,One Python notebook with all the details and inferences. If you want to create a separate document with the results and conclusion submit it in PDF format. Remember submitting a PDF with inferences is optional.,428646.0
134668,585313.0,"As answered by TA in https://learn.upgrad.com/v/course/208/question/134238, you need to 1. Include a code file consisting of the analysis. 2. Include a .docx/.pdf file with solutions to the questions which has subjective inferences, plots, etc. to this text file 3. Collect all the files and Zip them together.",318762.0
134668,585072.0,Only 1 jupyter notebook is required to submit for this assignment. Similar kind of question has been answered and verified by TA in below link https://learn.upgrad.com/v/course/208/question/134238,317991.0
134668,585312.0,"I would suggest the following : 1. Create a well commented jupyter notebook. 2. A pdf/doc file with all the questions and answers answers including any inferences, insights, plots etc. 3. Zip files 1&amp;2 and submit.",313826.0
134071,583059.0,"Analyses can be done in pyspark dataframes, spark.sql or both depending on convenience.",313826.0
134071,583054.0,"The hint provided are general guidelines we can follow. It is mentioned that ""The analysis should be performed on PySpark mounted on your CoreStack cluster, using the PySpark library."" And both approach require use of Pyspark library so I think both the approach i.e. pyspark or sql approach should be fine. But yes TA can give more insight.",317991.0
134071,583252.0,"It's totally up to your comfort level. But since I have tried both ways, what I have observed is some questions especially the ones involving CASE or Grouping are much easier to do with SQL rather than PYSPARK udf's. The main reason being Pyspark functions are significantly different from Pandas (Which I am comfortable with) and thus I had to keep checking Documentation for various functionalities and parameters. But yeah for the sake of simplicity, I have Created multiple columns using PySpark and performed ANALYSIS instead of writing CASE Statement and Then grouping.",317984.0
134044,582960.0,"Since this is a Graded assignment, cant elaborate in detail, But, There might be several Data Quality issues that you would need to treat in order to do proper analysis. for example: we need only the records for 2017. furthermore, there are hints also mentioned in the questions and one can take cue from those as well. also, assumptions can be made and mentioned in comments, if you are not sure how to deal with a particular data quality issue. hope that helps.",317998.0
134693,585941.0,"Be careful while doing this, there are several rows with special characters which you need to account for.",319357.0
134693,585460.0,"Other way is you can assume the time is given in hhmmA (for AM) and hhmmP (for PM) and delete the few rows which doesnt follow this format. It is mentioned in problem statement that you can make few assumptions , document it and proceed with data processing.",319759.0
134693,585220.0,"Hi Shubham, If the time is in HH:mm format &amp; even if its in String type, we can use hour() function to get the hour out of the time column. The hour() function would return integer i.e. value between 0 to 23 depending upon the values in the time column Once, we get the values as integer, it would be quite easy to divide them into different bins. Hope this helps!",318355.0
134694,585457.0,"Other way is you can assume the time is given in hhmmA (for AM) and hhmmP (for PM) and delete the few rows which doesnt follow this format. It is mentioned in problem statement that you can make few assumptions , document it and proceed with data processing.",319759.0
134694,585285.0,You can use the substr function to extract AM or PM from the format and use it in conjuction with the CASE WHEN functionality of SQL to convert them appropriately to the 24 hr format. Hope that helps.,317998.0
134694,585237.0,"Hi Madhusudhan, A simple solution to this problem is: Converting the AM/PM column into 24-hr datetime format Extracting the time from the 24-hr datetime format Eventually, we'll get the time in 24-hr time format Hope this helps! Hope this helps!",318355.0
134324,583839.0,"The .show() method just displays the output and returns back an object of 'NoneType' . Hence, after executing your code, df is no longer a dataframe but a 'NoneType' Object and hence the error. Try the below code instead (without show() ): park2 = park2.withColumn(""Time"", concat(col(""Violation Time""),lit(""M"")))",313826.0
134324,583834.0,The error is coming because you are trying to use lit at row level. Lit can only be used at column level. Source: https://stackoverflow.com/questions/47552309/withcolumn-with-udf-yields-attributeerror-nonetype-object-has-no-attribute/47552629 Hope this help.,317991.0
134324,585100.0,Thank you so much.. I resolve other issue with Nonetype. We should remove show where ever the assignment to store into other dataframe.,310518.0
133686,581865.0,Similar query answered by TA earlier https://learn.upgrad.com/v/course/208/question/132888,313826.0
133686,581868.0,Thanks - content not fixed yet though!,308633.0
134780,585470.0,Pyspark Notebook along with PDF file is required. I had already submitted my notebook but now checked with my mentor if i can send the pdf separately. My mentor has asked me to mail him the zipped file containing pyspark notebook and pdf file.You too plz prepare a pdf and contact your mentor,319759.0
134780,585477.0,"I agree about the confusion due to the 2 apparently conflicting answers by TA in https://learn.upgrad.com/v/course/208/question/134238 . However, I would recommend going with the more detailed answer i.e. you need to 1. Include a code file consisting of the analysis. 2. Include a .docx/.pdf file with solutions to the questions which has subjective inferences, plots, etc. to this text file 3. Collect all the files and Zip them together. More importantly, there is nothing to lose by adding the pdf/doc file when actually not needed but a lot to lose by not adding the pdf/doc file when it was actually needed. So better have it added.",318762.0
134780,585479.0,Got the confirmation from my Student mentor that submitting the Python Notebook alone would work if we've included our subjective observations in it.,318355.0
134780,585491.0,"Final clarification provided by TA is ""Only one Python notebook with all code and explanation. "" Please refer https://learn.upgrad.com/v/course/208/question/134782",318762.0
133541,582875.0,"Best way i am following in local Anaconda/Jupyter notebook installed pyspark and download csv file from assginment and started. this will be faster. later at once code can be shifted to corestack jupyter pySpark and checked. Othewise ending up the issues like auto logoff, server slow issue etc. I have one more doubt, do we have corestack only this week or next week too ? Not sure 6 weeks over ?",312019.0
133541,581476.0,"Once logged in to jupyter, check if there are any notebooks already running by clicking on the ""Running"" tab. Shutdown if you find any. Then try launching a fresh notebook.",313826.0
133541,581779.0,Raise an issue with CoreStack if nothing works.,428646.0
133541,581514.0,"Hi Ratandeep, Whenver you face any issue regarding connection, kindly try to refresh the page. It would prompt to login. Once logged in again, the issue would be resolved. I've noticed that If we leave the notebook for some time (may be 2 hours), we need to login again to access the notebooks. Hope this helps!",318355.0
133718,582033.0,"The best practice would be to clean up the data and then proceed with further analyses. Since the scope of analysis is tickets for year 2017 alone, tickets with any other date can be looked at as out of scope data and removed accordingly. Along with this, we need to do other data quality checks also.",313826.0
133718,582548.0,Use the filter for 2017 data. No need to delete it.,428646.0
133718,582549.0,I would jut filter the dataframe into a new dataframe (val df2 = df.filter ..),300694.0
133909,583248.0,"They are not the same, they are Similar. As both mention area.",317984.0
133909,582787.0,"Violation Precinct is the precinct in which the violation happened and Issuer Precinct is the precinct to which the officer who issued the ticket belongs to. Hence, it is natural that these columns would have similar values.",313826.0
133909,582721.0,"Yes, the columns Violation Precinct and Issuer Precinct contains similar sort of data.",317991.0
134795,585694.0,"Extract hours, minutes and AM/PM fields from the string in Violation_Time and then build logic to convert the data into a 24-hour format. This would then help you in binning the hours into 6 groups.",313826.0
134795,585581.0,The example that you have mentioned has 'P' as the last character of the string and also the conversion seems to be correct. What's the issue that you are facing?,313826.0
134795,585594.0,You can extract the hour and do a mod 12 to get the hour between 0 and 11.,318762.0
133577,581529.0,Similar query answered by TA earlier https://learn.upgrad.com/v/course/208/question/132888,313826.0
133577,581526.0,"Since our entire course has been based on Python, the implicit expectation is to do analysis in PySpark and submit code in form of Jupyter notebook. The reference to RStudio is a copy paste error. We cannot be expected to analyze in R when we havent learnt it in the course.",318762.0
133975,583493.0,"I didn't find any ""nan"" strings or nulls in the column ""Violation Time"". Don't know if you are seeing this before filtering 2017 data.",310974.0
133975,582912.0,"The columns may also have the literal string ""nan"" to represent a null value and hence may be causing this behavior.",313826.0
113932,491678.0,"As per me, it is advissible to do all data cleanup including outlier analysis before performing any analysis. Same goes for PCA. If we perform PCA and do the outlier analysis, it will be very difficult to understand the business value of that component we are performing outlier analysis.",319319.0
113932,491445.0,"It depends on your modelling task. In this case our final modelling task was to do clustering, thereby we do the outlier analysis just before that.You can also do it before applying PCA as well. If your final model was the PCA one only, then it is both necessary and required as well that you do the analysis previously.",313517.0
113932,491629.0,"is it like, PCA formed by data containing outliers will also have outliers and we need to do an outlier analysis on the data formed after PCA? if that is the case, why can't we do the outlier analysis after performing PCA everytime",317073.0
112117,483294.0,"You could use following syntax 1.originaldataframe= dataframe.reset_index() 2.dataframefinal= originaldataframe.merge(PCA_dataframe,on='index',how='inner')",318732.0
112780,486533.0,"I have done the merge and got the countries from the clusters, but was not sure if there is an easier to know what each cluster represents, so that I pick the right cluster without checking its contents. I'm now having to check the countries in each cluster and then make the determination which cluster to use to obtain the countries.",312491.0
112780,486518.0,"Hey Ravi, Please refer - https://learn.upgrad.com/v/course/208/question/112378 Hope this discussion might help.",302742.0
112780,486663.0,You need to merge the data frames . The one with the cluster numbers and the original data set.,318340.0
112780,486553.0,"Find out the mean of each of the PCs ans the important features with respect to Cluster id. And plot the barplot of the means. The cluster with high child mortality, low income, low health etc. will the right cluster.",304319.0
112113,483297.0,Go through this one https://learn.upgrad.com/v/course/208/question/112117,318732.0
112113,483666.0,"Hey Minerva, Those 0,1,2 columns are nothing but your Principal Components (PC) You can rename them as PC1,PC2 and so on Link mentioned below will help you to understand as to how we can rename columns in pandas dataframes : https://stackoverflow.com/questions/11346283/renaming-columns-in-pandas Hope this helps you out!",301655.0
112113,483329.0,"After PCA, you get principal components as the output. So those are principal components that you got. rename them as PC1, PC2,..so on for your convenience.",302738.0
111868,481888.0,"GDPP is calculated as the Total GDP divided by the total population. Total GDP is not given, neither total population in the dataset. Also, imports and exports are given as %age of the Total GDP Since Total GDP is not given, how will we modify these columns. TA please confirm, whether we should modify or not.",311254.0
111868,482103.0,https://learn.upgrad.com/v/course/208/question/111169,318451.0
111868,482145.0,"Hi Ravindra, The conversion step falls in the basic EDA part. Please do analyze whether conversion is needed or not. Doing PCA and checking fitment using both conversion and non-conversion may help determine which way to proceed.",334535.0
112132,483436.0,I think we need to create scatter plot of Prinicpal Components like PC1 vs PC2 etc. Rest TA's can clarify.,317991.0
112132,483443.0,we are expected to plot PC1 against PC2. It is called a biplot. PCA tries to project thedata onto a new set of dimensions where the variances in the data are captured such that one can cluster them visually or by using a simple algorithm.Biplot is the projection of the data on the first two principal components (where the variances are the highest),302738.0
112132,483610.0,"Hey Vinay, I think the link below can help you out : https://towardsdatascience.com/dive-into-pca-principal-component-analysis-with-python-43ded13ead21",301655.0
112132,483698.0,This is just a preliminary graph that you do before you go to clustering. The scatterplot would have PC1 and PC2 as the axes and the remaining countries post outlier analysis as the data points. This helps you check the spread of the data to see if any clusters are directly visible initially or not before you begin the clustering process [No need to do any biplot graph here].,313517.0
112132,483672.0,"Agree with Naseem. Just to extend while plotting PC1 vs PC2 (or PC1 vs PC3, PC2 vs PC3) you can use color dimension on clusters to check the logical grouping of clusters.",317514.0
112019,482898.0,yes even i have the same doubt,308495.0
112019,483111.0,"Hi Bhargav, Rashmi, Yes, as Bhargav mentioned, PCA clustered needs to be joined back/merged with original data.",334535.0
112126,483391.0,You must execute the following command. hopkins(deta_frame) You would have got an array from PCA which needs to be converted to this dataframe. It looks as though you had not converted np.array to a dataframe.,301121.0
112126,483498.0,"Say the dataframe after you drop the country is named as countrydf. You will apply standardscaler function and do pca on this. Next you should again convert it back to dataframe, so do pd.DataFrame(countrydf) This you need to pass the argument to Hopkins function. Do not add the country before passing it to Hopkins function.",301114.0
112126,483451.0,check if you have only numerical data in the dataframe that you are giving as an input to the hopkins function. Remove the country name and try again.,302738.0
112126,483711.0,From the error AttributeError: 'numpy.ndarray' object has no attribute 'values' We can infer from this error is that 'df_kmeans' you passed to hopkins() function is not a dataframe. So you need to make this a dataframe and again run the function. Hope this helps.,317991.0
112021,482778.0,I am confused.. how do I identify the names of PCs from the array?,314221.0
112021,482770.0,pd.concat() can be used for concatenation. Ensure that you convert the pca array to a dataframe and also take care of the indexes of the dataframes.,313826.0
112021,482804.0,You can go through the PCA part of telecom churn example given during video lectures. I am sure it will help.,317991.0
112021,482942.0,"Guys, dropped non PCA feature should be concatenated to chosen PC's Dataframe or to the orignial data set. There is a little bit confusion here, the main goal of PCA is to find the ideal no of clusters. shouldn't it be added to original data set and start with outlier treatment &amp; clustering using the optimum no of K's we found using the PCA ? Any help is greatly appreciated.",306735.0
112021,483107.0,"Team, Check out below thread for this. https://learn.upgrad.com/v/course/208/question/112041",334535.0
112262,484553.0,"Hello, You need to use linkage (single or complete) with cluster cut functions to get the cluster id's. Follow the steps in retail example python notebook from lectures , you should be able to draft the cluster ids with those steps.",305652.0
112209,484022.0,"First step is to read country.csv as a dataframe, remove the countryname and do normallizing for entire dataframe (not field by field) by basic calculations. This worked.",301121.0
112209,484021.0,It happened with me too. Somehow gdpp(in your case) has converted to string. Use type conversion (df['gdpp'] = df['gdpp'].astype('float64')) on the string columns and then scale again. I guess that should work.,301652.0
112323,485503.0,its normal to have clusters varying.,317981.0
112323,484846.0,"It does change. The reason is that K-Means is non-deterministic. By the virtue of basic algorithm of K-Means cluster label will change ,",301121.0
112323,484862.0,There is a random state feature in the clustering function which you can explore for this. As said before clustering being an unsupervised probability based method the iterations will bring in different results.,301644.0
112323,484616.0,"Hello, It's normal when you run the code multiple times , cluster id labeling will be changed. You can count the number of rows corresponding to cluster id with below sample code. dataframe.groupby('clusterid_column').count()",305652.0
112323,485288.0,"Hi, the final grouping results on the initial selection of points (lets say for k=5) its how the first 5 points are selected. then based on that it perform the operation till it finds convergence or till it reached max iteration. So its absolutely normal. Regards Darshan",311032.0
112323,485667.0,"Hi, It's normal to have varying cluster ids labelling on multiple execution. Reason : In every execution K-Means will start with random choice of cluster centers and that's why it may result in different clustering results on different runs.",300700.0
110724,476684.0,"Yes, you are correct, the reason why this might happen in certain cases is that the number of data points is low(167 in this case). In such cases you can follow one approach: Do the outlier analysis and find the final clusters. Then using these cluster centres, reassign the outlier values to the respective clusters based on which cluster centre the country is closest to. Check if this resolves the issue. You can follow this or any other logically thought out approach. You'll be getting due recognition for either of them.",313517.0
111296,479288.0,first do pca to find the number of cluster required then do clustering on the data tell which countries needs money,318017.0
111296,479372.0,Please refer to my response on a similar discussion: https://learn.upgrad.com/v/course/208/question/111230,313826.0
111296,479296.0,"Hey Arundeep, The approach has been discussed in the various discussion on the forum. ""As per the discussion that has been going in, we may follow the following steps: Remove OUtliers PCA Clustering Assign the outlier removed in step 1 to the clusters based on whichever cluster they are nearest to."" Please refer this - https://learn.upgrad.com/v/course/208/question/111230 Hope this helps.",302742.0
111601,480763.0,"Use any scaler like StandardScaler() ( or MinMaxScaler() ) from sklearn.preprocessing to fit_transform() the inlier data. Later, transform() the outlier data using the same scaler.",313826.0
111608,480752.0,You have to use pd.concat command. The result of PCA is in np.array. Ensure to convert np.array to dataframe first before using pd.concat and it works.,301121.0
111608,480723.0,what is the error ?,318017.0
111608,480776.0,"Once you remove the ""Country"" column from the dataframe, you can reset the index usig dataframe.reset_index(drop = True). After this step, you can perform the PCA and then add back this country column using a simple hstack (np.hstack).",316202.0
111961,482348.0,"hi, you can follow the approach shared in RFM example i.e. taking each component one by one and removing outliers for each component sequentially.",311686.0
111961,482398.0,you should do it individually on each PC because removing outliers data for one PC does affect the other PC's.. so better do it for each PC see the effect on other PC's and then proceed ahead if need to be for other PC's as well.. i don't think it is a good idea to remove outlier on a complete dataframe in a one go!,316349.0
111961,482555.0,"Hi Harsha, You need to do remove outliers on each of the 5 PC seperately. And then perform clustering. In case not alreday, do review the overall suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
112231,484427.0,I used predict function only. I don't see any harm of using it. TAs may confirm.,301652.0
112231,484507.0,"Yes, you can use that.",313517.0
112940,487045.0,use pd.concat to concatinate the country names back. make sure to reset the index before concatinating. check out below thread for TA varified answer: https://learn.upgrad.com/v/course/208/question/112041,302738.0
112940,487109.0,"Use pd.concat with axis=0 country_data= pd.concat([country_data,country],axis=0) You can refer this link for more understanding: https://www.analyticsvidhya.com/blog/2016/06/9-challenges-data-merging-subsetting-r-python-beginner/",311117.0
111230,478979.0,"As per the discussion that have been going in, we may follow the following steps: Remove OUtliers PCA Clustering Assign the outlier removed in step 1 to the clusters based on whichever cluster they are nearest to. Please checkout some other discussion on this topic: https://learn.upgrad.com/v/course/208/question/110724 https://learn.upgrad.com/v/course/208/question/111214 https://learn.upgrad.com/v/course/208/question/110955/answer/477857",313826.0
111230,489389.0,please refer following thread : https://learn.upgrad.com/v/course/208/question/111624,318772.0
111311,479362.0,The same has been discussed here : https://learn.upgrad.com/v/course/208/question/111300 Maybe helpful.,311117.0
111311,479515.0,Remove the country column and then perform PCA. Once done concat the country column again,314313.0
111820,481905.0,"The above link you have stated in not the right approach. After having session on 21st Feb, the correct approach/requirement is given in below link: https://learn.upgrad.com/v/course/208/question/111624, its clearly explained and also in ppt, you will get clear idea what to do.",318429.0
111820,481916.0,"I believe that statement was changed a while back- "" show the clusters on those components as well as a few selected original variables(the socio-economic factors mentioned above) "". You don't have to do 4 independent cluster models. You need to do 2 only. Once PCA is applied, and you have transformed the data to your principal components, do clustering on them- both K-means and Hierarchical Once the clusters have been formed in each case, visualise them on the PCs (any 2 of them, since you would be plotting on a 2D surface only). Now map the above clusters to the original variables and visualise on any two of them. For example, you took the original clusters and plotted it on say gdpp and income. Essentially you need to create a scatterplot of all the countries keeping 2 PCs on X-Y axes and differentiate them on the basis of the clusters. Similarly, do the same thing keeping any of the 2 original variables on X-Y axes.",313517.0
110776,477024.0,"you can show upto 3 d using x,y,z if you have pc more then 3 then its not possible to visualise.",318017.0
110776,477034.0,"I think Visualization here means, plotting the summarized values of all PCs against each cluster like the way they showed in the jupyter notebook used for PCA in the course content.",310974.0
110776,477943.0,"I have seen such visualizations as pair plots of pc n and pc n+1... (pc1 vs pc2, pc2 vs pc3 and so on)",308637.0
110776,478518.0,"I got the exact doubt as Chandan, and got the exact solution as Ram. Though I am still not very sure whether we all are in correct direction. After PCA's dimensions are coming out to be 5 ( which explains 95% variance), so question of chosing 2 Dimensions I suppose is out of question. Now once we get 5 dimensions, then we can say end up preparing 6 clusters. So, in the end, do we need to prepare 5 bar graphs ( one for each dimension) showing the spread out in each cluster?",304814.0
110893,477623.0,The clustering assignment is not to build regression model. Why do you think you need multi-regression model?,310974.0
110893,477414.0,"try doing the type casting. to float. sm.OLS(y, X.astype(float)).fit()",317689.0
110906,477850.0,We need not do the Regression model for this assignment. The objective of this is to create clusters using K-Means and Hierarchical using the Dimension reduction techniques.,316202.0
110906,477610.0,"Hi Koustav, Wondering why you are using sm.OLS in this assignment. This is not a Supervised Learning (i.e Regression) problem. This is Unlabeled Data and we just need to do Clustering after doing PCA.",311686.0
110950,477716.0,"To find the optimum number of K, we have to measure silhoutte scores and squared distances for each value of K. So, I assume that is what is meant by different values of K. Visualizations, My understanding is that it should be done on final clusters of K-means and Hierarchial but not all values of K.",318329.0
111287,479215.0,KMeans is for clustering not prediction. Go through the sample notebook provided in the course content.,310974.0
111287,479262.0,"For this assignment, it is not required to predict, as the objective is to do the clusterring and dimension reduction using PCA. I think you are trying to apply the sample code from the session for this assignment. The Sample code is applied for the Logistic Regression problem. We dont need to do the Train Test split for this.",316202.0
111514,480819.0,"Scree plot is based on cummulative explained variance ratio, use pca.explained_variance_ratio_ instead of explained_variance_",318438.0
111514,480352.0,This looks incorrect. Code you're using is correct. Please check if you've standardized the data and referring the correct dataframe on which you want to run PCA.,316349.0
111107,478441.0,"If the values you obtain from these are high, your PCA didn't go well since there is still correlations between the Principal Components. If the values are very close to 0 then they are not correlated which is what is expected from the o/p of PCA. Basically, it is a measure of the strength of PCA.",310974.0
111107,479962.0,"Before PCA it helps us to understand how many of the independent variables(assuming supervised learning is to be done) are correlated. It can also been visualized by heatmap. Just that the exact correlation values may be more comprehensible. Also, you can compare this with the PCA correlation matrix later to see how these variables are transformed into uncorrelated PCs. Hope this helps",311857.0
111805,481787.0,we were shown this for the telecom churn example - create a dataframe using the dataframe constructor -&gt; you can name the columns as necessary and also add additinal columns if needed (e.g. country),300694.0
111805,481661.0,convert the array to dataframe and then concat with the original dataframe.,311254.0
111805,481666.0,yes you will get an array after performing PCA. convert the numpy array you got after fit_transform to a dataframe using pd.DataFrame and use it for further analysis.,302738.0
110933,477606.0,"Hi Anmol, This is not a Supervised Learning (i.e, Classification or Regression) problem. So there is no Response Variable at all. This is Unlabeled Data. Also, we don't need to split this data set as there is no 'test' involved to find whether our model is 'predicting' accurately or not.",311686.0
110955,477652.0,"Hi Nagaraju, Please check this thread: https://learn.upgrad.com/v/course/208/question/110724. It has discussion about outlier treatment procedure for clustering assignment. Thanks.",305652.0
110955,478442.0,"Even if the outliers are natural, they have to treated and the best way of treatment in this case is to impute them.",310974.0
110955,477857.0,"The outliers always try to pull the centroid of the cluster towards themselves and hence may cause creation of sub-optimum clusters. Hence, outlier treatment is necessary I believe. TAs could please confirm this.",313826.0
110944,477618.0,Do the summary analysis of Principal Components for each cluster just like the way it is done for Original Variables in the practice jupyter notebook provided in the course content.,310974.0
110944,477673.0,"does it mean have to cluster 2 times - once on original variables without PCA, and another after doing PCA?",310509.0
109905,473939.0,"Though I have not yet gone through this assignment, but as per my understanding, t he outliers shall be treated if there are much differences in the values of "" 50%, 75%, and max "" values while checking the statistical aspect "" .describe( )"". If there are much difference and we would not treat these outliers, then it would impact our output.",311117.0
109905,474398.0,I would suggest not to do outlier treatment. The purpose of the assignment is to group countries on their socio-economic factors. We are doing this for an NGO who want to allocate their funds to poor countries if we do outlier treatment than we can lose the important countries which are very low on socioeconomic factors. Each entry in the dataset is a country drop 25% means losing that much amount of countries.,318451.0
109905,474722.0,"See you cannot do outlier treatment here unless you have an exceptionally high or low value. I understand your problem of getting weird (not intuitive) clusters, one thing that might help you is you can CAP the values above and below a certain value. the capping value is what you will decide. I will explain this giving an example, like in an exam if a student score 2 marks and one scores 20 marks it doesn't matter as both of them need to work almost equally hard, you can CAP all the marks below 20 as 20 only.",318344.0
109905,476603.0,Wondering why TAs have not verified/suggested any answer on this query. This is an improtant question. Requesting some TA to respond asap.,311686.0
109905,477933.0,Are the countries we are looking for not outliers? I think we need to be judicious about which outliers to treat... TA verified Chandan's repostof this question... Please check answer...,308637.0
109905,480675.0,"Outlier treatment doesnot necessarily mean delete those records too high or too low. You can impute them to the highest or lowest value you consider, that way you are not eliminating that country and also your model might not get skewed",317514.0
111902,482160.0,"Hi Rajani, 1) For outliner analysis using percentile , please go through the below link: https://stackoverflow.com/questions/34782063/how-to-use-pandas-filter-with-iqr 2) I guess RFM is a customer value segregation method rather outlier detection one.",305652.0
111902,482151.0,You can use the quantile() function to select records based on percentiles. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.quantile.html,313826.0
111835,481973.0,"Hi Siddhant, Check out code example at https://www.programcreek.com/python/example/78158/scipy.cluster.vq.vq",334535.0
110341,475379.0,"No, it will not. It was specific to that particular data. In Clustering &amp; PCA assignment the data is different. RFM (recency, frequency, monetary) analysis is a marketing technique used to determine quantitatively which customers are the best ones by examining how recently a customer has purchased (recency), how often they purchase (frequency), and how much the customer spends (monetary).",318451.0
110341,477934.0,RFM is used for transactional data only.,308637.0
112145,483589.0,"Hey Vinod, I think we should leave those columns as it is and not perform any calculations on them.",301655.0
112145,483582.0,"I think we should not do anything on that, as there were not much data columns to perform the above task.",301648.0
112145,483705.0,What I identified that in the present dataset we don't have enough information to convert current variables into absolute one. So you can skip this step.,317991.0
112145,483826.0,"Here's a hint: The variables that we have should reflect that country's property accurately. That is why we require per capita information for certain values wherever we can because they depict the true picture of that country. For example, India has a higher GDP than Singapore because of its much higher population. But, when we calculate its GDP per capita it is far far lower and indeed Singapore is far more developed than India. Use this logic to see if those percentages that you have give an accurate picture for that country or not and then proceed. You should be able to get the answer now of what to do.",313517.0
112041,482893.0,use pd.concat to concatinate the country names back. make sure to reset the index before concatinating.,302738.0
112041,482918.0,Thankyou. Should reset be done on both the datasets? And how do i verify that the country names have been mapped properly after concat?,312096.0
112041,483039.0,Regarding your 2nd question. The order of rows will not change. The transformed PCA data will keep the datapoints in the same order in which it was in the original dataframe.,301652.0
111129,478531.0,"Yes, that is what I have understood too",308435.0
111129,478537.0,Tats what even i understand hopefully the understanding is correct,344598.0
111129,478538.0,Lets hope so folks....thanks for boosting my confidence :-),304814.0
111129,478539.0,I wish TA looks into it and confirms,304814.0
111129,478581.0,"Basically, you need to perform PCA on the dataset in order to obtain the Principal Components, perform Clustering on the countries using a suitable number of components and show the clusters on those components as well as a few selected variables. Here, what about few selected variables ? Totally Confused.",317070.0
111129,479461.0,"Basically, you need to perform PCA on the dataset in order to obtain the Principal Components, perform Clustering on the countries using a suitable number of components and show the clusters on those components as well as a few selected variables. I'm also totally confused by the assignment language. Please TA explain the assignment properly.",311466.0
111129,480129.0,"Here is the explanation of what you need to do: The original dataset has variables like gdpp, exports, child_mort, etc. (Check the data dictionary for what these variables mean) First, do the PCA on the original dataset and get a reduced number of dimensions. Let's say you finally used 3 PCs for the clustering part. These 3 PCs are equivalent to the RFM columns that you created in the online retail dataset. Then you need to perform the clustering exercise using these 3 PCs as columns. Now once the clusters have been created, you need to create a scatterplot to show the various clusters that have formed using any 2 of the above 3 PCs (since you can only use two dimensions to display the countries on a scatterplot). Similarly, plot the same clusters using a scatterplot having any 2 of the original variables(socio-economic variables mentioned above as well as in the question) to show how the clusters vary. For example, let's say after performing clustering on the 3 PCs mentioned previously, I got 4 clusters. Now using a scatterplot, you'll be showing how the clusters have formed using PC1 and PC2 as the x-axis and y-axis ( or any other combination as per your choice). Basically, plot all the countries on these 2 PCs and then differentiate them on the basis of the clusters that were formed. Now choose any of the two original variables( like gdpp or child_mort). Plot a scatterplot again using these 2 as the axes and differentiate them on the basis of the clusters. This will give you an idea as to how the clusters vary on the basis of the given PCs as well as some of the original variables as well. You can also do the bar chart analysis as you mentioned on the respective variables as well.",313517.0
112096,483193.0,"Hello , There is no need to interpre the PC columns , please verify this thread: https://learn.upgrad.com/v/course/208/question/112087",305652.0
112096,483281.0,"Not need to interpret the principle components, Just use the output dataframe in PCA as input to the clustering and do analysis , after that merge with original raw dataset . You will get clear view.",318732.0
112096,483437.0,"As you performed PCA, you got principal components as the output which are linear combinations of the variables we’ve got. so these numbers that you got are PC1, PC2..etc. you can rename them to PC1,PC2,PC3.. for your convenience.",302738.0
111143,478587.0,"Extract the country column from the dataset before applying the clustering and PCA techniques. Once you are done with the clustering, you can append the country column to the dataframe",316202.0
112046,482961.0,It is better to add the plots in the ppt whatever is created in the python notebook,311254.0
112046,483034.0,I have plotted the barplots of the clustering method that worked best for me.,301652.0
112153,483587.0,"Hey Rajeev, I was also scratching my head on the same thing since yesterday morning. But the only way I think by which we can map them to original columns is by using index of both the dataframes. Add those columns which have matching indexes in both the dataframes. I think this might work out for you!",301655.0
112153,483796.0,"Hi Rajeev, Yes , you can map the dataframes using index number as they are not altered with row modifications. Please refer the threads: https://learn.upgrad.com/v/course/208/question/112041 https://learn.upgrad.com/v/course/208/question/112081",305652.0
111151,479841.0,"Clustering is grouping up of similar data. So, There is no need for splitting the data into train and test Just consider the whole data as one data object.",304692.0
111151,478772.0,"We need not split the data in to Train and Test. Since the objective is to do the clustering and also perform the PCA, splitting is not required.",316202.0
111151,478665.0,i dont think we need to split data in training and test data as there is no prediction involved,310509.0
111151,478797.0,"Firstly, PCA is not to train data. It is for dimensionality reduction. Here's an interesting debate on whether we need to split the data https://stats.stackexchange.com/questions/268934/do-we-need-to-set-training-set-and-testing-set-for-clustering",310974.0
112335,484701.0,No it's not necessary. You can do it again if you want to.,313517.0
112335,484672.0,"In my openion it should not be required, as I am performing clustering on PCs which are already scaled. Correct me If I am wrong.",300721.0
112335,485416.0,"Once you standardize the data frame, then use the same data frame for PCA/Clustering, the you would not have to re standardize anything",304814.0
112047,482912.0,you are applying it in a ndarray instead of a dataframe.,301648.0
112047,483032.0,"I guess you have transformed the original data frame by using fit_transform() function and got the output as 'NGO_data_PCA'. But note that this is not a pandas dataframe, but a numpy array. Note that the columns in this two-dimensional numpy array will be the principal components. So you can first convert the numpy array to a pandas dataframe by using pd.DataFrame(NGO_data_PCA, columns=['PC1', 'PC2', 'PC3', ''PC4']). The choice of column name depends on you. I have assumed you have selected 4 principal components and the 2D numpy array has 4 columns only.",301652.0
112047,483303.0,Convert numpy to dataframme using following syntax : df= pd.DataFrame(df_numpy),318732.0
111205,479217.0,1) PCA. 2) Hierarchical Clustering to determine ideal number of clusters. 3) Attempt two K Means models with different K and compare them with each other.,310974.0
111538,480458.0,"You can use pd.concat([pca_df, original_df['country']], axis = 1).",311160.0
111538,480688.0,All of this is shown in the sample notebook in the course content. You can refer that.,310974.0
111538,480891.0,"Once you have completed doing the PCA, you would have got few Principal components. Now you can append the country column by using a simple np.hstack. It is there in the sample model taught in the session. Now, you will have a dataframe which can be used for the clustering. Finally the cluster labels obtained by clustering technique will be appended to the original dataframe for the plotting.",316202.0
111585,480616.0,"Once PCA is done and the number od pronicpal components are identified(in your case 5), y ou need to use these 5 Principal Components further for clustering.",313826.0
109856,474109.0,"Hi Rajarshi, You understanding is correct.",334535.0
109856,476633.0,"Hi Rajarshi, This question of yours calrified a big doubt. Thanks. Can you also pls guide what you have understood from this statement mentioned in Evaluation Rubric : 'K-means algorithm is utilised properly and at least two iterations are done on different K' ? Specially in context of the clarification that we have to build 4 models.",311686.0
109856,476683.0,"My understanding regarding this has changed a bit while actually working on the assignment: I do not think we need to build separate models based on original variables any more. We build clusters (couple of iterations) on the PCAs and then represent the dataset broken into those clusters. Can provide visualizations as well. It doesnt make any sense in doing PCA if we are clustering based on original variables. Regarding the rubric part, I would say, it means we need to create multiple sets of clusters (K-means and Hierarchical) based on PCA and and select one amongst them as our final clustering model and draw visualizations to represent the dataset and draw inference.",310511.0
109856,478717.0,i still have a doubt - in the expected results they have expicitly asked for original variables- The clusters must be visualised on both the Principal Components and some of the original variables. Does this mean we should do K- Means before PCA then perform PCA and again perform K- means and highlight the difference ?,300731.0
109856,480672.0,1. You are provided with x variables. You can delete variables with high collinearity. You are left with some variables. You do clustering (k-Mean &amp; Hierarchical) and do your analysis 2. Use PCA to reduce the number fo variables from x to say y (where y &lt;x) components which have no correlation. Do clustering (k-Mean &amp; Hierarchical) and demonstrate that you are getting similar results. 3. Easier to demonstrate your case to stakeholder with fewer components or fewer themes. These themes might be dependent on all of your variables y but might have high influence by some variables. Struggling to interpret the theme,317514.0
109856,481917.0,"I believe that statement was changed a while back- "" show the clusters on those components as well as a few selected original variables(the socio-economic factors mentioned above) "". You don't have to do 4 independent cluster models. You need to do 2 only. Once PCA is applied, and you have transformed the data to your principal components, do clustering on them- both K-means and Hierarchical Once the clusters have been formed in each case, visualise them on the PCs (any 2 of them, since you would be plotting on a 2D surface only). Now map the above clusters to the original variables and visualise on any two of them. For example, you took the original clusters and plotted it on say gdpp and income. Essentially you need to create a scatterplot of all the countries keeping 2 PCs on X-Y axes and differentiate them on the basis of the clusters. Similarly, do the same thing keeping any of the 2 original variables on X-Y axes.",313517.0
111236,479019.0,No. Both are different countries . There is no duplicacy. We will have to take them separately.,311117.0
111236,479064.0,"No, both are different countries, you can do a quick google search to find out the difference. PFB the wiki page (s) for further clarity. https://en.wikipedia.org/wiki/Republic_of_the_Congo https://en.wikipedia.org/wiki/Democratic_Republic_of_the_Congo",311032.0
111236,479366.0,"No ,both are different",320687.0
111236,479238.0,both r different :),318446.0
113418,489309.0,"PCA is not used for prediction. Hence, train-test split is not required .",311254.0
113418,489403.0,"Train- test splitting is not required in PCA assignment. Step by Step procedure is attached here, you can follow: https://learn.upgrad.com/v/course/208/question/111624",311117.0
111935,482191.0,Please refer to link below on the same topic TA approved https://learn.upgrad.com/v/course/208/question/110829,301121.0
112162,483790.0,"Hi Swathi, You need to merge the dataset on the basis of row indexes (row index will not change after rows modification). Please refer the following threads for the steps. https://learn.upgrad.com/v/course/208/question/112041 https://learn.upgrad.com/v/course/208/question/112081",305652.0
111312,479373.0,Please refer to my response on a similar discussion: https://learn.upgrad.com/v/course/208/question/111230,313826.0
111312,479334.0,yes you can or rather you should. please refer this for TA varified answer https://learn.upgrad.com/v/course/208/question/111214,302738.0
111312,479360.0,"I have replied the same here : https://learn.upgrad.com/v/course/208/question/111313 You can follow this: As per the discussion, we may follow these steps: Remove OUtliers PCA Clustering Assign the outlier removed in step 1 to the clusters based on whichever cluster they are nearest to.""",311117.0
111312,479444.0,Ok..but which method to remove outliers is best here ? except 2 or 3 columns all have outliers..,310501.0
111300,479285.0,you can use the percentile method for outlier removal like keep only those values which has 95 percentile. and if you put the mean value in place of outlier it will not be correct as you are using a fake value and that might make some analysis about the varibles which is not true at all,318017.0
111300,479967.0,"Once you have the cluster assignment for the non-outlier countries, look at the means of the clusters and assign the outliers to one closest. If you are not satisfied with any cluster you may even decide to keep them separate/independent. The point of clustering is to apply a uniform strategy specific to a cluster instead of one per country. As long as you can explain the outlier cluster assignment and provide a corresponding strategy your approach is good.",311857.0
111300,481048.0,How do we assign the outlier to any cluster in python. Do we need to use logistic regression?,304319.0
111321,479389.0,you might be getting the above error because you did not remove the country column. We can not do Clustering on categorical variables hence you need to remove the country column from the dataframe. refer this for the steps: https://learn.upgrad.com/v/course/208/question/111143,302738.0
111321,479379.0,please remove the country column brfore doing pca as its a string and pca would work only for numerical data only.,318017.0
111318,479542.0,"Yes, you need to drop the ""Country"" column before doing the PCA and clustering. As PCA is not possible with categorical variables.",316202.0
111318,479380.0,you can remove the country column as it would not be helpful in PCA and later on you can add the country column once the pca and other analysis is done.,318017.0
111331,479549.0,"A way to figure out what each principal component signifies would be to look at the magnitude of the coefficient of each principal component and how that changes with changes in features. You can access this by seeing pca.components_ . You can go here for a good explanation: http://www.nxn.se/valent/loadings-with-scikit-learn-pca That said, you don't really need to do this. The best thing would be to run the clustering algorithm with the PCs and the see how the clusters were formed.",319357.0
111331,479599.0,I hope the below link will resolve your query. please let me know if it doesn't help. https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn thank you,310481.0
111331,479395.0,"The ""Principal Components"" generated after PCA are a combination of various ""Features"" from the original dataframe in varying magnitude . Hence, a mapping of the Principall Components to Features is not direct mapping.",313826.0
111347,479530.0,"It is possible to get such plots. Significance: It signifies that those Cluster's Child_mort values are less than the mean value of the Child_mort. When you Standardize, you would be subtracting the values with its mean and hence the behavior.",311160.0
111347,479540.0,"Yes, its possible. I would suggest to create the plots using the original data set before standardising. Once you have created the clusters using the standardised data, you can append these clusters to the original data for plotting. This helps in deriving insights from the charts.",316202.0
111347,479473.0,Are you plotting after standardisation? If yes then maybe that is the reason you are getting negative plots,302738.0
111361,479532.0,"Yes, we dont need to do the Test Train Split, as the objective is to do the clustering and dimension reduction. You can remove the ""Country""from the data set and perform the PCA and clustering. Once this is done, you can add back the country.",316202.0
111361,479543.0,"Sham is right, you will need to remove country to do PCA. Test-Train split should only be done if you are looking to predict a certain value, never for clustering.",319357.0
111685,481116.0,K-means is a non-deterministic algorithm. That's the nature of the algorithm and nothing to be fixed.,311857.0
111685,481345.0,"Set the ""Random_state""= 250(approx). We generally dont look that part while running the code everytime. and once it crosses the limit, cluster sizes gets changed.",311117.0
111685,481130.0,Make sure that you set the KMeans() funciton's parameter random_state to a specific value like random_state=123.,313826.0
111691,481155.0,concatinate the country names to the cluster dataframe that you got after performing K-Means algorithm using pd.concat. this way you'll get the countries in their corresponding clusters.,302738.0
111691,481476.0,"Team, The approaches found by you and suggested by your peers are good suggestions. However, looking at the number of queries asking for merging back outliers, want to suggest that give less weightage to this part. Focus on the PCA and k-Means part. You may do outlier mergeback and analysis at end; but focus more on PCA, k-Means and hierarchical Also, review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
111716,481172.0,"I am under the impression that it is not a good idea to replace the outliers with the mean for all outliers as it may end up leading to misinterpretation by PCA. Just to check the impact, you can do this on the copy of the code written, keeping the original code safe.",301121.0
111716,481213.0,On many similar type of questions TA has explicitly told to remove the outliers and then impute them at the end by assigning them to the nearest cluster they belong. So I would suggest you to go by how we are expected to or else you might loose marks unnecessarily.,302738.0
111716,481338.0,"As suggested in today's Live session, I understand that remove the outliers, do PCA, and Clustering. and during analysis, add some outliers closure the cluster, if needed.",311117.0
112213,484051.0,"draw plots for k means and hirarchial , and compare them to make decision that to which cluster of countries are to be founded",318732.0
112213,484166.0,"As mentioned in assignment problem, find the cluster of countries which needs funds. this can be found from the plots.",311117.0
112213,485507.0,we need to find the cluster of countries in dire need of aids.,317981.0
113466,489398.0,"You can refer to this TA suggested steps to perform analysis, https://learn.upgrad.com/v/course/208/question/111624. It will be helpful.",311117.0
111738,481288.0,"Once the PCA is done, you must proceed to both the clustering based on the PCs. Live session by TA is extremely good and informative.",301121.0
111738,481327.0,Just go through the recorded session of today's Webinar. All the required steps were expained there. That would definitely help.,311117.0
111739,481325.0,"I would suggest to go through the recorded session of today's Webinar, if not attended. All these are expalined there.",311117.0
111855,481836.0,Please follow below link: https://learn.upgrad.com/v/course/208/question/111624,310419.0
111740,481321.0,"Yes. You will get the different results in both cases, i.e. K-means and Hierarchical. The same has been explained in today webinar also. And as expalined we may prioratize the K-means.",311117.0
111740,481271.0,Hello Ravindra The main reason behind why they ae different is K-Means clustering is non-deterministic. Hierarchical clustering is deterministic.,301121.0
111740,481260.0,Yes it is. You are not supposed to get exact same plots. You can refer the file used in the clustering session. There too the graphs obtained for K-Means and hierarchical clustering are different.,302738.0
111416,479974.0,"every sklearn's transform's fit() just calculates the parameters and saves them as an internal objects state. In this case it calculates the PCs. Afterwards, you can call its transform() method to apply the transformation to a particular set of data , this is when the country data is converted to PCs. fit_transform() joins these two steps. Internally, it just calls first fit() and then transform() on the same data. Use fit() to get PCs and understand how a PC is mapped to the variables etc. To get the PC transformation of the data use transform() or fit_transform()",311857.0
111925,482112.0,"Try a different approach. Have a dataframe with principal components and cluster numbers of 166 rows. Concatenate the country and other columns as you require. This way you will be in a position to group the countries with respect to cluster numbers. For the purpose of plotting cluster number against the means of different features, you can use the dataframe which you have displayed.",301121.0
111925,482159.0,"For reset_index() you will have to use inplace=True for it to take effect on the dataframe that you are working on. Once you have reset the indices, you could merge the two dataframes to get the master dataframe. Ensure that the number of rows of both the dataframes that you merge are matching.",313826.0
111946,482320.0,"Hello, No need to calculate the actual value (scaling is required thuogh). We need to compare the the same column values (means srictly speaking) after finding clusters , so analysing the given data for different clusters will do rather than the actual data.",305652.0
111946,482330.0,"Please see, https://learn.upgrad.com/v/course/208/question/111169",307495.0
111421,479984.0,""" Basically, you need to perform PCA on the dataset in order to obtain the Principal Components, perform Clustering on the countries using a suitable number of components and show the clusters on those components as well as a few selected variables. "" You don't need to perform clustering without PCA. Just do the visualization on the PCs and on original variables.",311857.0
111965,482384.0,"Hi Prakash, 2) Principle components are linear combination of the variables (they need not to be direct mapping of individual variables , instead they are combination of the variables which captures maximum variance). 3) once clustering is done (on numerical columns) , based on the index numbers join the cluster numbers to original dataframe.",305652.0
111968,482396.0,"Hello, Please use IQR method for outlier treatment (column by column), https://stackoverflow.com/questions/34782063/how-to-use-pandas-filter-with-iqr Please provide the code whcih gives you error.",305652.0
111968,482466.0,Outlier treatment would remove records based on the criteria that you choose. The columns (attributes) should remain the same after outlier treatment.,313826.0
111968,482489.0,Thanks everyone realized my mistake i added the feature column in the PCA dataframe and did the outlier analysis which was wrong.,301114.0
112050,482945.0,"Hello, you need to map kmeans.labels_ ouput to PCA data , so that each row is segregated into different cluster ids. Please refer for the retail dataset clustering python notebook for instructions.",305652.0
111408,479995.0,Try imputing the outiers instead of dropping them since the dataset is small.,310974.0
111408,479836.0,"most of the outlier treatment is done for the data set after treating a particular column , check what is the difference in the outliers after (first outlier treatment), if its small , it's good to go as the dataset here is too small.",301115.0
111408,479896.0,You will have to remove outliers as per rfm analysis for each of the columns and later check with box plot. Standardization has to be done with centering before proceeding for PCA.,301121.0
111408,480053.0,Data set we are looking at is relatively small 167 rows and 10 columns. Outliers though present may not be a good idea to drop them since your PCA might be skewed,344598.0
111408,480068.0,As per the ta answer we should remove and add the outliers to clusters later . Which column to focus on ? Becoz I see all columns have outliers . And how do we add these removed outliers to cluster centres later ?,318797.0
111408,480143.0,"Team, Please note, you will have to do combination of 1. outliers removal (permanently) and performing clustering 2. data imputation and performing clustering 3. temporary removal and assigning back after clustering. You will have to give reasoning for the approaches you took. For outlier reassignment, please check https://learn.upgrad.com/v/course/208/question/111417",334535.0
111408,480884.0,One we remove the outlier no need to add again.,319319.0
111426,479982.0,"For smaller data sets you can choose to go with PCA. When the dataset is too large putting a great load on computation and memory requirements IPCA is advised. For our requirements either is fine. Anytime, if PCA can be used then go with PCA.",311857.0
112087,483155.0,"Hello, Principle components are the linear combination of original variables , there is no need to interpret the principle components. You need to feed these PCs to clustering and then when you get the oupt labels , you need to join them with the original dataset instead of PCS. After joining labels with the original dataset , you need to perform mean analysis.",305652.0
112087,483286.0,"Interpretation of PCA is not reuired , it just processed data of original data . You need to analysis on clustered data with cluster IDs , which are main output",318732.0
111749,481352.0,"You have to do K-Means clustering and Hierarchical clustering on the same output of PCA and do some comparison even though they are bound to be different. As per TA in live session, it is not a good idea to do outlier treatment before PCA.",301121.0
111749,481471.0,"PCA should give you the number of PCs you need for a good variance. The outupt of the PCA should have fewer columns than the original dataframe. Then we need to apply clustering on the output dataframe and add the countries back to it. If you want to remove outliers, they have to be removed after PCA according to the TA and then added back after the clustering to the cluster nearest to the respective outlier. Hope this helps",316416.0
111749,481492.0,"Team, The approaches suggested by peers are good suggestions. However, review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
111749,481531.0,"On the above point, How do we remove the outlier from PCA data set and added back to the cluster during cluster?",311115.0
112378,484916.0,"Hello , You need to merge the cluster ouput to original ouput and then do mean analysis. Finally , retrieve the countries from selected cluster with python code data frame.loc[dataframe.cluster_idcolumn== &lt;number&gt;, 'country_column']",305652.0
112378,485506.0,"if you do df[df['clusterid'== selected cluster id]], it will give a list of countries for the cluster.",317981.0
112378,485042.0,"Hey Rashmi, You need to merge the dataframe which you get after performing clustering with the original dataframe and then perform mean() operation on it. Link below might help you on how to merge two dataframes: https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html Hope this helps you out!",301655.0
112008,482653.0,"Hello, I think you are referring to cluster data frame , after aggregation (mean) operation. You map the cluster dataframe (cluster_id for the PC's) to main data frame and then perform aggregation operation. Index id lookup should help here. Please refer approach : https://learn.upgrad.com/v/course/208/question/111624",305652.0
112081,483140.0,"df_kmeans=pd.concat([cont_data['country'],pcs_df_final],axis=1,ignore_index=True) try this command replace the datframe names. Outliers should be treated after the concatenating.",314197.0
112081,483151.0,"Hello , You can try this approach as well: 1) Create a new column 'index' with below command on original dataframe and pca dataframe. dataframe = dataframe.reset_index() 2) now , merge the two dataframes on the new 'index' column. 155 rows will be formed in final dataframe with required columns.",305652.0
111216,478924.0,PCA handles it very effectively.. you just need to pass the nunber of increments and PCA will convert it into almost 0 correlation.. this is one of the strength of PCA.. infact we use PCA for highly correlated inputs only..,316349.0
111451,480089.0,you can add the country df with pca as both comes in the same index value but if you have removed any data you should also check that.,318017.0
111451,480045.0,Make sure the number and order of the countries is the same as in the PCA DFbefore you removed the country. Also check the index of country it should be same as the PCA DF. You may need to reset the index of the country.,311857.0
111731,481488.0,"Hi Sudheer, Perform outlier analysis only once. After that do not do outlier removal again.",334535.0
111733,481257.0,"After PCA, try performing clustering and then append the country column. It worked for me.",302738.0
111733,481264.0,Append the Country column after creation of clusters.,313826.0
111733,481335.0,above are my clusters This is my country column Getting this error,318451.0
111733,481489.0,"Hi Tanay, Review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
111452,480087.0,you can add it in the end and see that which country falls under which cluster and what are the features of that cluster.,318017.0
111452,480091.0,"Once the PCA is done, you can use these principal components and perform the clustering. At the end, you can add these clusters to the original dataset and do the plotting.",316202.0
111454,480086.0,since you are telling your silhouette analysis suggests that between 2 to 5 you can make clusters so combine this finding with PCA and take the appropriate number of clusters.,318017.0
111454,480088.0,"Once you have the cluster id created, you can append the cluster id to the original dataset. With this, you will have all the columns and its corresponding cluster ids. You can now plot the charts (similar to the example explained in the session)",316202.0
111454,480197.0,After doing PCA you get the equivalent transformation of the dataset in terms of PCs. Use this to do clustering. Later append the country and original variables to get a complete picture. You can then use this super set to do any kind of visualizations. While appending make sure the indexing is correctly reset.,311857.0
111454,480265.0,"That is correct. 5 Principal components are good enough to take you to required variance. I presume you used the scree plot to see the precentage of variance. (Please note that cluster number starts from 0 and not 1) You can do K-Means clustering on the output of PCA (choose clusters any where between 2 to 5 preferably 5).; I did 6 just for the sake doing. Appending the countries could be tricky part, I guess.",301121.0
111458,480155.0,Thanks for sharing Deval,334535.0
111169,478779.0,"I dont think any conversion is required. Because we dont have the ""Total GDP"" data for converting the variables that are expressed as % of GDP. The dataset only has ""gdpp"" which is GDP per Capita. We dont have the ""Population"" data as well to get calculate the GDP based on GDPP metric.",316202.0
111169,478784.0,"great, thanks!",316349.0
111169,482142.0,"Hi Hemant, Team, Reviewing thie further, the conversion step falls in the basic EDA part. Please do analyze whether conversion is needed or not. Doing PCA and checking fitment using both conversion and non-conversion may help determine which way to proceed.",334535.0
111819,481708.0,"I faced similar issue. As you mentioned, it got resolved after resetting the index. I followed the below steps as a workaround 1.Add the index of df as a new column 'Column_index' 2.Reset index 3.Concatenate ClusterID 4.set the index of df as 'Column_index' Hope this helps.",310467.0
111482,480205.0,Yes - country names I will add Assign outliers to each cluster ID and then map country names to this cluster - right?,308437.0
111482,480191.0,You seem to be in the right direction. Ensure you add the cooutry Names which you had removed earlier in the final stage to know which countries are in which cluster. Please take care while restoring the countries back as there will be some permanent removal of records due to outliers. TAs can confirm,301121.0
111482,480238.0,"Yes, Your Steps are sequentially correct. You can proceed.",311117.0
111482,481174.0,How to add the country names back? Can someone please help me?,310472.0
111506,480821.0,"I disagree, PCA is NOT a blackbox. All PCA is doing in linearly scaling the original variables/features. Plot the values of first principal component, you'll see various features being weighed differently. From a business perspective, this tell what features have more weightage/variance.",318438.0
111506,480579.0,"Hi Siddhant, To a big extent, the PCA technique has got reputation of being ""blackbox"". Equally, there are good articles which try to counter the blackbox argument. Refer one such below. https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html",334535.0
111506,480321.0,"You are right, there is no interpretability with PCA but hey why do you need interpretability in clustering. Ultimately, you want to understand the properties of the clusters by understanding the summary metrics of the original variables against each cluster. That is all we need.",310974.0
111984,482576.0,"Hi Rajesh, The outlier treatment needs to be performed on Principal components. When you drop that PCI, this in a way marks the corresponding country as outlier.",334535.0
111520,480391.0,"Apart from the explanation given above by Hemant, You can also go through below link for some more explanation https://datascience.stackexchange.com/questions/6715/is-it-necessary-to-standardize-your-data-before-clustering Hope it will help",317991.0
111520,480349.0,"Standardization is to bring your data on the same scale.. This is the compulsory step which we should be carrying out before running PCA/Clustering. So, that your clustering treats each parameter on the same scale with same weightage. On the note of Outliers; they'll stay as it is even if you standardize it.. the only chage would be the scale will be less but outliers will stay as outlier only. And hence you need to treat them before standardization and entering into Clustering/PCA. Hope this helps",316349.0
111828,481742.0,you can do outliers treatment after pca model building,318017.0
111828,481920.0,This is what you need https://learn.upgrad.com/v/course/208/question/111624,319357.0
112022,482764.0,I would advise to rename these columns for ease of understanding and use later for analyses and visualizations.,313826.0
112022,483029.0,How do we know which columns PCA considered?,300727.0
111524,480689.0,Outlier treatment https://learn.upgrad.com/v/course/208/question/111417/answer/480137,310974.0
112168,483788.0,"hi, after doing PCA and creating dataframe with identified PCs we need to do outlier treatment. it needs to be done one by one for each PC and outliers for each PC to be dropped subsequently as shown in RFM example.",311686.0
111526,480770.0,Yes. fit_transform() the inlier data and later transform() the outlier data using the same scaler.,313826.0
111526,480421.0,Need verification on the below steps. For non outliers - we have to do a fit and transform for Scale as well as PCA For outliers - We have to use the same fit as non outliers and just transform for Scale and PCA.,310467.0
111526,480400.0,Standardizatioin and centering is a must before PCA. Please refer to the TA approved link https://learn.upgrad.com/v/course/208/question/110835 Regarding fit and transform refer the link below. https://learn.upgrad.com/v/course/208/question/111416,301121.0
111546,480454.0,"Please refer the discussion had in below link: https://learn.upgrad.com/v/course/208/question/111361 https://learn.upgrad.com/v/course/208/question/111541 https://learn.upgrad.com/v/course/208/question/111296 The same procedure, i am also adopting, it is helping me out.",311117.0
111546,480451.0,"After the outlier treatment you should be performing PCA. Then we need to perform clustering. Also, we shouldn't be splitting the data into Train and Test data. You shouldn't be considering the Country as output variable as this is unsupervised learning",311160.0
111591,480685.0,Outlier treatment https://learn.upgrad.com/v/course/208/question/111417/answer/480137,310974.0
111582,480632.0,pandas.Dataframe.corr() -- Directly provides the pairwise correlations between numeric columns of the dataframe. You can further use the above in a heatmap to visualize the correlations.,313826.0
111583,480574.0,"No we do not need test train split in unsupervised learning. Refer the following answer for the same question: No requirement of test- train splitting procedure. Use the procedure: Treatment of Outliers, PCA, Clustering.. It is discussed here: https://learn.upgrad.com/v/course/208/question/111296",302738.0
111583,480595.0,We usually split our data into two subsets: train data and testi data whenwe want to make predictions on the test data. The current assignment does not ask for any predictions. So for this assignment we do not need to split the data into two. Hope this will help.,317991.0
111555,480481.0,you need to visualize each cluster against the original variables through barplot to find out the variation across the clusters from each variable stand point.. this probably will give you an idea on suggestions to be made to CEO for which cluster (set of countries) to priortize your spendings on..,316349.0
111555,480637.0,Visualizations can be done as follows: 1. Barplots of Cluster vs. Original Features -- using plt.barplot() 2. Scatter plots between combinations of Principal components and/or Original features -- using sns.lmplot() or plt.scatter() .,313826.0
112003,482664.0,"Hi Aaditya, You can try this approach: 1) Map cluster id's to the outlier_treated data (provided index id column is intact) . 2) Compare the data from step:1 and original data frame using 'isin' to get the outlier data. refer the below link: https://stackoverflow.com/questions/28901683/pandas-get-rows-which-are-not-in-other-dataframe",305652.0
111587,480628.0,The Outlier Dataframe should be transformed with the same PCA model. Standardization should be done after removal of Outliers.,313826.0
111587,480686.0,Outlier treatment https://learn.upgrad.com/v/course/208/question/111417/answer/480137,310974.0
111596,480662.0,No. We should not drop any of the columns for this assignment. Instead PCA should be used to generate uncorrelated principal components which then become the basis for further clustering.,313826.0
111596,480786.0,"I agree, you should not drop the columns based on multicollinearity. PCA does this job for you.",316202.0
111596,480725.0,pca is done for this case only it will remove the multicollinearity in the data .,318017.0
111917,482133.0,This error usually comes if the column is not of numeric type (int or float). Check what is the dtype of the column: pcs_df_km.gdpp.dtype,313826.0
111567,480589.0,"I think we don't need to perform IncrementalPCA. Incremental PCA is used as a replacement for PCA, when the dataset is too large to fit in memory. So you can use PCA only. You can go through below link for more information on Incremental PCA. https://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html Hope this will help.",317991.0
111215,478918.0,"hi, this is not a supervised learning (i.e. classification or regression) problem. so no need of response variable and also no need of splitting in train and test data.",311686.0
111215,478976.0,"PCA being meant for unsupervised learning, there will not be any train and test but clustering based on principal components.",301121.0
111606,480783.0,Make sure the data is normalised before doing the PCA. It doesnt look like standardsation is done. Can you confirm.,316202.0
111606,480683.0,It is not written on stone that it should reach 95%. Take the one that gives highest variance and saturates from then on.,310974.0
111606,480779.0,"The ""pca_explained_variance_"" values mentioned by you seem to be incorrect as they all should add up to 1 (meaning 100% explanation of variance). If you are trying to display these as a percentage, then ensure that you multiply it by 100.",313826.0
111606,480920.0,,319759.0
111606,480938.0,Hi this is the explained variance and is in decreasing order. This is correct. To get 95% variance you need to find pca variance ratio. Please refer to the jupyter notebook.,311857.0
111380,479687.0,yes you are right. TA has alredy confirmed a similar query. please refer the following link: https://learn.upgrad.com/v/course/208/question/111230,302738.0
111367,479601.0,yes. you should remove outliers before proceding with PCA. refer this for TA varifies answer: https://learn.upgrad.com/v/course/208/question/111230,302738.0
111313,479358.0,"The same has been discussed here : https://learn.upgrad.com/v/course/208/question/111296 You can refer the same. As per the discussion, we may follow these steps: Remove OUtliers PCA Clustering Assign the outlier removed in step 1 to the clusters based on whichever cluster they are nearest to.""",311117.0
111313,479374.0,Please refer to my response on a similar discussion: https://learn.upgrad.com/v/course/208/question/111230,313826.0
111840,481959.0,Absolutely no issue with doing it before PCA.,310974.0
111840,482135.0,"Hi Ruchik, You need to perform outlier treatment on the PCs that you determine. Thus perform PCA first",334535.0
110631,476778.0,"Hi Khusbu, As such from a techique and algorithm point of view there is no limitation in terms of number of variables that can be considered for clustering. But from a practical point of view two point arises 1. need to consider the computational power and equally the time required for high dimensions 2. From a domain/business perpective, you need to arrive at optimum number of variables, including dropping co-related variables. The below is a good read. https://en.wikipedia.org/wiki/Curse_of_dimensionality",334535.0
111773,481469.0,You can go through TA's detailed explanation about what needs to be done. https://learn.upgrad.com/v/course/208/question/111624/answer/481465,317991.0
111773,481698.0,"Hi Vipul, the link does not work, can you please recheck the link",319876.0
112049,483121.0,"Hi Vipul, You can perform mean analysis on both. But as such, you can use the one which is giving better cluster",334535.0
111541,480447.0,"No requirement of test- train splitting procedure. Use the procedure: Treatment of Outliers, PCA, Clustering.. It is discussed here: https://learn.upgrad.com/v/course/208/question/111296",311117.0
111541,480413.0,"Hi Parnasree In this is assignment there is no train or test required as there is no prediction of dependent variable etc, It is purely PCA and clustering later. Before PCA you must perform standardization of numerical columns. Please refer the link below. https://learn.upgrad.com/v/course/208/question/111361",301121.0
111624,480859.0,"I am also facing a similar thing. Once I have pca.fit_transform() inliner data, I used the same model to transform outlier data. When I check the correlation matrix of the principal components generated for the outlier data, I see two components are highly correlated(~0.7). Can TAs please confirm if this is expected?If not, then how to deal with this? TAs please guide on this point.",313826.0
111624,480970.0,I have a question 1. How did you join outlier and non-outlier ? 2. Any specific reason to check for correlation ?,317991.0
111624,480901.0,"If the above approach is not acceptable, can we follow below steps? 1. Find PC for the entire dataset.(both outliers and non outliers) 2. Flag the PC for outliers (by comparing it against the original dataframe outliers) 3. Find the clusters for the non flagged PCs.(non -outliers) 4. Manually assign the Flagged PCs(outliers) to the nearest clusters.",310467.0
111624,481535.0,"Any inputs on step 1 above which was mentioned by Mangesh ? What conversion is needed here ? 1. Check columns and assess if any columns are expressed as % of other columns, should they be expressed in absolute/non-relative/non% basis. Accordingly do conversion",300727.0
111624,481465.0,"Hi Keerti, Team, Below is suggested approach. The ""Results Expected"" section does give hints regarding this to big extent, but clarifying it further 1. Check columns and assess if any columns are expressed as % of other columns, should they be expressed in absolute/non-relative/non% basis. Accordingly do conversion 2. Drop non PCA related columns 3. Perform Fit Transform 4. As Keerthi mentioned, find PC for the entire dataset.(both outliers and non outliers) 5. Identify optimal number of PC using scree 6. Perform pca again with optimal number of components and obtain PCA dataset. Check whether incremental PCA is beneficial here or not. 7. Add non PCA columns back to PCA dataset 8. Perform outlier analysis and discard outliers 9. Do scatter-plots of the PCs post outliers 10. Check if k-means can be performed using hopkins measure 11. Once confirmed, perform silhouette and elbow analysis to determine optimal clusters 12. Perform clustering with first K to obtain cluster id 13. Join back the clustered data with dataset post step1. Perform mean analysis for all columns per cluster 14. Find one or more cluster fitting the criteria for funding 15. Do crosscheck/analysis of few individual countries selected from each cluster selected for funding 16. Repeat step 12 to 15 with different k 17. Perform hierarchical clustering with single and complete linkage on PC dataset and obtain cluster id 18. Join back the clustered data with dataset post step1. Perform mean analysis for all columns per cluster 19. Perform manual/visual analysis on the outliers countries that were dropped. Post appropriate analysis, you can a. Keep outliers as they are, or, disregard them totally b. Group outliers and label them as one or more seperate clusters c. Merge outliers with obtained cluster Note: Looking at the number of queries asking for merging back outliers, want to suggest that give less weightage to step 19. Focus more on steps 1 to 18",334535.0
111624,481721.0,"Note : Please refer to this PPT where we have discussed the assignment approach. Please also understand that we can’t mention the details of every step explicitly as it is a graded assignment. Below is the approach as suggested by Mangesh. The ""Results Expected"" section does give hints regarding this to big extent, but clarifying it further 1. Check the dataset and do basic EDA stuff(like NA check,etc.) and see if all the columns are of correct order or not. 2. Drop non PCA related column(s) 3. Perform Fit Transform 4. As Keerthi mentioned, find PC for the entire dataset.(both outliers and non outliers) 5. Identify optimal number of PC using scree 6. Perform pca again with optimal number of components and obtain PCA dataset. Check whether incremental PCA is beneficial here or not. 7. Add non PCA column(s) from step 2 back to PCA dataset(This part is optional and you skip to Step 8. Also you don't have to add the entire data. Just create a new data frame with the PCs and the original variables to see how they are related, similar to the telecom churn demo where you saw the relationship between the original columns and the PCs just after creating them) 8. Perform outlier analysis and discard outliers 9. Do scatter-plots of the PCs post outliers 10. Check if k-means can be performed using hopkins measure 11. Once confirmed, perform silhouette and elbow analysis to determine optimal clusters 12. Perform clustering with first K to obtain cluster id 13. Join back the clustered data with dataset post step1. Perform mean analysis for all columns per cluster 14. Find one or more cluster fitting the criteria for funding 15. Do crosscheck/analysis of few individual countries selected from each cluster selected for funding 16. Repeat step 12 to 15 with different k 17. Perform hierarchical clustering with single and complete linkage on PC dataset and obtain cluster id 18. Join back the clustered data with dataset post step1. Perform mean analysis for all columns per cluster 19. Perform manual/visual analysis on the outliers countries that were dropped. Post appropriate analysis, you can a. Keep outliers as they are, or, disregard them totally b. Group outliers and label them as one or more seperate clusters c. Merge outliers with obtained cluster Note: Looking at the number of queries asking for merging back outliers, want to suggest that give less weightage to step 19. Focus more on steps 1 to 18",301619.0
111624,483233.0,"I've used a slightly different approach for solving this, just wanted to know if this approach is fine. 1. Identify the socio-economic variables influencing the countries performance. ¶ 2. On similar lines, identify the health variables 3. Perform the outlier analysis using the histogram and also since, each country is important, impute the mean of the values for outliers. 4. This way, will be using the complete data set instead of 25%-75% quartile data. 5. Using histograms, identify the right bins for each of these variables. 6. Calculate the socio economic and health scoring 7. For these scores, scale and apply hopkins. 8. Form the K-means cluster and silhoutte score 9. Perform the cluster analysis and identify the right cluster of countries which needs aid at the most. 10. This can be done by plotting the bar graphs for these clusters 11. Perform the hierarchical clustering and revalidate the cluster analysis performed using Kmean",314084.0
111624,484364.0,Hi TA's What we are expect to do in step 15 ? 15. Do crosscheck/analysis of few individual countries selected from each cluster selected for funding Do we have to visualise one particular selected country ? Or just select values and crosscheck it ?,317991.0
110639,476296.0,"Hi Sumit, Good point. Although note that Nominal GDP Growth = Inflation + Real GDP Growth Globally real Growth contributes less to the equation. Thus for simplicity Inflation has been equated to Nominal GDP Growth or GDP Growth. Now, for the purpose of this assignment, it is okay to proceed with the given definition.",334535.0
111620,480787.0,"For df_train_pca_DF, you need to pass parameter inplace=True , else the dataframe remains unchanged. Also, as mentioned by Sham, you should not split the data into train and test sets as this is not a prediction problem.",313826.0
111620,480775.0,"Once you remove the ""Country"" column from the dataframe, you can reset the index usig dataframe.reset_index(drop = True). After this step, you can perform the PCA and then add back this country column using a simple hstack (np.hstack). Also, i see that you have splitted the dataset in to train and test. This is not required, as this is just a clustering problem and not a prediction problem.",316202.0
111620,481035.0,"While resetting the index, it should be df_pca = df.pca.reset_index(drop = True) or use inplace=True as mentioned by Vinay.",304319.0
111420,479966.0,Yes. That is the ball park number of outlier records.,311857.0
111420,479899.0,"You seem to be in the right direction. Keeping outliers will disturb the mean and hence the centering and standardization; Without proper cenering and standardization, results of pfa will be awkward. TAs have advised in the similar lines.",301121.0
111420,479993.0,It is better not to lose the records if you have a very small data set like the one given for assignment. Try imputing the values.,310974.0
111417,480137.0,"Hi Meghanaa, Doing Exploratory Analysis, we need to determine whether to remove/keep-aside outliers. Once we obtain the cluster we need to plot those and also include outliers. Now using visualization techniques, we need to determine 1. Keep outliers as they are 2. Group outliers and label them as one or more seperate clusters 3. Merge outliers with cluster e.g. refer picture below The above represents the cluster which have been obtained post ouliers removal. Now reassigning outliers depends on where they are located and also the number of clusters we have. If there are multiple outliers to be assigned, then we can through code check nearness of outliers to a cluster center basis the key parameters and do assignment to respective cluster",334535.0
111481,480203.0,"It is a good you point which I am pondering about too. Can you please refer to the link given below in the discussion forum, which is on the same topic? https://learn.upgrad.com/v/course/208/question/111347",301121.0
111430,479983.0,The clusters created should be visualized on PCs say PC1 and PC2. Simliarly take original variables say income and child_mort and plot the clusters.,311857.0
111888,481991.0,"the outliers dropped can be added once the clusters are formed, that way the complete data is retained,also binning can done based on the variables which have heavy loading in PC1",301115.0
111888,482031.0,"Hi Nitesh, I fully agree on your logic. Even during the live session, TA found it equally tough to drop an outlier as it is different from a normal financial transaction. In fact, at one point, TA mentioned that we need not eliminate outliers even before implementing K-Means clustering. I did not do any outlier treatment till the completion clustering and it proves meaningful from the analysis and decision point of view, I hope TA understands the concerns,",301121.0
111888,482094.0,https://learn.upgrad.com/v/course/208/question/110724,318451.0
111888,482141.0,"The outliers cannot be a part during the clustering process as they would pull the clusters towards them and hence may generate sub-optimum clusters. On the other hand, we cannot fully ignore the ouliers as rightly pointed out by you. Hence, the suggested approach to create clusters with ""Inlier"" data and then manually assign each of the outliers to one or the other clusters depending which cluster they are closet to based on the important original features.",313826.0
112183,483865.0,"Hello, 1) you need to group the data set based on the cluster id's and then find mean for each feature. Use code like this: dataframe.groupby('clusterid').column_name.mean() 2) Visualize the data using barplots after finding mean values for the dataset.",305652.0
112183,483975.0,"I would suggest you refer to the RFM example where they have taken the mean R, F and M. It is extremely useful. You have to alter that to suit our case. Execute that first and then concatenate with column having cluster IDs This enable you create barplots between cluster ID and clusterwise mean. I hope this helps.",301121.0
112180,483832.0,first calculate PCA and after performing pca remove outliers from the principal components which cover your variance.,318017.0
112206,484138.0,Yes you can. But the Python plots should be sufficient.,313517.0
112206,484013.0,It is a good question but I presume python plots would do. Don't you think it is an overkill as we are only plotting few variables against cluster IDs. What I mean is that they are basic simple plots between original variable mean (calculated) and cluster IDs,301121.0
112208,484026.0,I guess no random_state parameter can be set in the NearestNeighbors classifier. This is why Hopkin's Statistic changes every time. I am not very sure about this.,301652.0
112208,484028.0,Hello Ayyappa Please refer to the link below. All the points on variance of Hopkins value each time is explained. https://learn.upgrad.com/v/course/208/question/111911,301121.0
111911,482076.0,"HopKins Statistics: It does vary to a certain extent. But please check it is in the range from 0.7 to 0.9 If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster. Since the clustering will be done based on principal components, Hopkins Statistics has to be done on PCA data set. I hope this clarifies.",301121.0
111911,482181.0,"Hi Amrita, 1) I guess it is becuase the way Hopkins statistics is calculated. I try to give reasoning behind this. hopkins working process: It generally checks if the data is best case for clustering , by taking 'uniformely randomly' distributed data points (lets say Y) and X original data points. 'u' = sum of the distances from random data points (Y) to nearest neighbor original data points (X) 'w' = sum of the distances from orifinal data points (X) to nearest neighbor original data points (X) ""hopkins statistics = u / u+ w"" As we are using 'uniformely randomly ' distributed points(Y) , every time hopkins value might be changing.",305652.0
111911,482315.0,After doing pca like X_std = StandardScaler().fit_transform(X) pca PCA(n_components=4) xd = pca.fit_transform(X_std) for the Hopkins function i pass xd as argument . But i get error like 'numpy.ndarray' object has no attribute 'values',301114.0
112176,483834.0,clustering should be done based on elbow curve and silhouette score by which you will get the good k value to use.,318017.0
112176,483991.0,"After finding out the oprimal number of clusters (k) from elbow curve or silhoutte score, you have to use the KMean command where it takes the dataframe or a numpy array as input gives out the clusters with cluster IDs. KMean command is given in write up between lectures. You have to ensure that input ot KMeans contains only relevant fields meant for clustering and nothing else. Later you can concatenate or map the cluster IDs for the entire data (which you would have reduced already by doing outlier treatment.)",301121.0
112200,483965.0,This should do. df = pd.DataFrame (nparray),301121.0
112200,483973.0,"I would like to add to MUTHU's answer. You can then rename the columns of the dataframe df thus created by using the following command. df.rename(columns={key:value, key2, value2}) where key is the old column name and value is the column name you want to keep. Similar question: https://learn.upgrad.com/v/course/208/question/112047/answer/483032/comment/114341",301652.0
111633,480869.0,There are few ways of doing this. Here are two I could suggest: 1. Put the entire condition you used to select the data in parenthesis and do a negation with ~. You will get whatever the condition did not select. 2. Use the isin() function and negate it with ~. You will get whatever you haven't selected.,319357.0
111633,481038.0,Just reverse the sign &lt; Q1 -1.5*IQR,304319.0
111633,481467.0,"Team, Looking at the number of queries asking for merging back outliers, want to suggest that give less weightage to this part. Focus on the PCA and k-Means part. You may do outlier mergeback and analysis at end; but focus more on PCA, k-Means and hierarchical Also, review suggested approach mentioned in https://learn.upgrad.com/v/course/208/question/111624",334535.0
111633,480875.0,"Hey Hemant, I think the link below can help you out: https://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-data-frame Hope this helps!",301655.0
111214,478925.0,TAs have responded in other queries that we need to drop such outlier records and then proceed for clustering. after clustering done we need to map these outliers to nearest centroid.,311686.0
113937,,nan,
130523,570063.0,"SerDe is short for Serializer/Deserializer. Hive uses the SerDe interface for IO. The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing. A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats. Please follow the below link or more details : https://cwiki.apache.org/confluence/display/Hive/SerDe Hope it helps.",301648.0
130524,570242.0,"The Stripe footer contains the encoding of each column and the directory of the streams as well as their location. The footer section consists of three parts viz. file metadata, file footer, and postscript. Reference: https://www.ellicium.com/orc-file-structure/",317987.0
130675,571339.0,Drop the table and create it again in a different location and then insert.,318495.0
130675,570576.0,"Hi Mukesh, The Move Task error is generally because of permission issues on the folder. This might happen if you try to execute the same code provided by UpGrad without changing the location attributes, specifically, the 'user_folder' . Also, the error will pop in data insertion phase and not while table creation phase, but it is important to note that the resolution of this error is in table creation phase with modified attributes for location. Resolution will include following steps: Delete the ORC table (using drop table &lt;table_name&gt; Modify location and make sure you change 'user_folder_orc' to something like 'my_folder_orc' Execute table creation query Execute data insertion query Hop this helps.",317987.0
129752,569261.0,"While executing insert overwrite table amazon_reviews_year_month_partition_orc partition(yr,mnth) i am getting below error Error while compiling statement: FAILED: ParseException line 2:81 cannot recognize input near '&lt;EOF&gt;' '&lt;EOF&gt;' '&lt;EOF&gt;' in statement",302877.0
129752,566777.0,"Hi Anuj, In the query - instead of location 'location '/user/hive/warehouse/your_partition_folder_name_orc' (which is at end of query) provide correct location (i.e. location of database on which you have created table and running your query). That should resolve above error. How to know your path:- 1) Go to File Browers -&gt;HDFS Brower (on top right side of the lab) 2) Go to location User/hive/warehouse and then look for your Database name (that you have created and running your query). Hope this will resolve your issue.",320103.0
130265,568683.0,click on HDFS browser. ` It will open hdfs browser and you can browse through from there.,317689.0
130459,569858.0,"Hi Naveed, Try following my answer on this question: https://learn.upgrad.com/v/course/208/question/130184 If you still get the same error, let me know.",318355.0
130459,569926.0,"Issue Cause It is possible that you failed to import the .jar file that they asked to import at the beginning of every session. It is possible that your map reduce job failed due to some other issues (many cases are possible) Resolution Try running the query in some time (maybe there are a lot of ongoing map-reduce jobs) Import the files they mentioned to import prior running queries. (The .jar file, as well as the other codes, mentioned to execute prior execution of partitioned table creation)",317987.0
130268,569406.0,"Kindly refer to the discussion here - https://learn.upgrad.com/v/course/208/question/130238 Also, yu can get the details here - http://hadooptutorial.info/hive-metastore-configuration/",329936.0
129499,565736.0,The JSON file is placed is already placed in the location in hdfs under /common-folder/bollywood. You need to create the table based on the that. The csv file is given for your understanding.,311254.0
129499,566032.0,"CREATE EXTERNAL TABLE if not exists bollywood_movies( Movie string, Lead string, Rdate date, Ocollection int, Wcollection int, Fwcollection int, Tcollection int, Verdict string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/common_folder/bollywood/' tblproperties (""skip.header.line.count""=""1"");",311857.0
129499,565661.0,You don't need upload thee csv file to Hue in this case. It is already placecd in hdfs under /common-folder/bollywood. The csv file is given to you so that you understand what data types need to be used while creating the table.,310974.0
130270,568703.0,"It seems data is not getting inserted properly. I can see one code modification required fields terminated by ' , ' -- in this line remove space before and after comma , inside quotes ' '. I hope this will help. If not then do let me know.",317991.0
131667,574795.0,The data has been added from the location you have specified. In this statement it will take the data which is stored in the hdfs folder location as a json file.,301648.0
129522,565887.0,Try the below: ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; and Rdate to string,318476.0
129522,566247.0,"If you have added the JAR file, then one possibility should be that you change the data type of ""Release date(Rdate) to ""string"" from 'date' type. this will work.",329936.0
129522,566042.0,"Try to create columns ocollection, Wcollection, fwcollection and Totalcollection to be of datatype double instead of float .",320103.0
135626,587061.0,Row Delimited: This is to say to HIve that once a 'New Line' character is seen whatever comes after that is a new record SerDe (Serializer Deserializer) This is a builtin library in HDFS Hive API; This is used when 'row format delimiter' is not defined. https://stackoverflow.com/questions/14605640/how-does-hive-stores-data-and-what-is-serde,301121.0
135626,587276.0,While doing the Hive assignments I was facing the problem that was when I typed ROW FORMAT SERDE along with the creation of new table it gave me an error but after using ROW FORMAT DELIMITER it ran fine. why didn't ROW FORMAT SERDE 'com.cloudera.hive.serde....' worked at that point of time?,311466.0
135626,587325.0,"These topics are already discussed, you can refer below links for detail -- https://learn.upgrad.com/v/course/208/question/129962 https://data-flair.training/blogs/hive-serde/",329936.0
130323,568945.0,AWS Access Key ID and Secret Access Key must be specified by setting the fs.s3.awsAccessKeyId and fs.s3.awsSecretAccessKey properties means it require login credential to access AWS account. In video they have used Amazon s3 as storage which comes up with its own credential for login. We have access only to corestack and we have not been provided with any access to Amazon s3 so that command will not work. Hope this answers your query.,317991.0
130323,569022.0,Here in the given video they have used AWS account where as here we are using HDFS browser and hence request you to change the location,318427.0
129695,566769.0,Try to run below 2 statements as well before querying the table:- SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000; Also. go through below thread which might help. https://stackoverflow.com/questions/51664932/getting-failed-execution-error-return-code-2-from-org-apache-hadoop-hive-ql-ex,320103.0
129695,566753.0,Hope this helps https://stackoverflow.com/questions/34140344/how-to-delete-files-from-the-hdfs,311254.0
129695,566889.0,Check if the table does contain any data. If not then insert some data and then run the select query.,329936.0
129695,567529.0,"I m also facing this issue , Even I try below command and yet no help . SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000; I have check my data , my Select Statement query is giving the result but next query is not giving this error . Also it seem this is a random error due to lagging or due to high meta data . We need a proper guidance of how to clear the metadata .",311861.0
130151,568193.0,"You have to set table properties to skip first row i.e. header row of csv files as follows: CREATE EXTERNAL TABLE table name (col1 datatype1...........) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/common_folder/bollywood/' tblproperties (""skip.header.line.count""=""1""); Hope this will help.",317991.0
130151,568207.0,"while creating you need to give code line ROW FORMAT DELIMITED FIELDS TERMINATED BY ' , '",305804.0
129958,567256.0,"I tried changing the order but it did not impated the data, how does it map it?",300721.0
129958,568235.0,"Mapping of the data elements in the JSON source data to the columns in the target table happen by matching object keys , or names, in the source name/value pairs to the names of columns in the target table. The matching is case-sensitive. It doesn't matter if you change the sequence of column name. for ex - 1. create external table if not exists asr1( asin string,reviewerID string, reviewerName string,helpful array&lt;int&gt;,overall double,summary string,unixReviewTime bigint) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths'="""") location '/user/grader_01/as' 2. create external table if not exists asr1( reviewerID string,asin string ,reviewerName string,helpful array&lt;int&gt;,overall double,summary string,unixReviewTime bigint) row format serde 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths'="""") location '/user/grader_01/as' In both the above case 1&amp;2 data will be properly loaded in the table.",329936.0
129482,566125.0,You need to create an external table with column names asked in the question and answer the questions.,329936.0
129482,565711.0,"As per my understanding, create table as per the file (not as per column mentioned) and then run query on top of that. This will give you correct answer.",320103.0
130221,568477.0,Login credentials are provided in the Access Information tab. You can check there.,308673.0
130221,568544.0,Got it. Thanks!,301116.0
130443,569510.0,"Try this code as it is select avg(overall) from amazon_reviews_table where (year(from_unixtime(unixreviewtime)) = 2008) and (size(split(reviewtext, ' ')) &gt;=206); And do let me know if it works or not.",317991.0
130443,569515.0,"Try to exchange where condition as follows: where (year(from_unixtime(unixreviewtime)) = 2008) and (size(split(reviewtext, ' ')) &gt;=206); And do let me know if it works or not.",317991.0
130443,569647.0,"Hi Bindu, Try the below : select avg(overall) avg_rating from amazon_reviews_table where year(from_unixtime(unixreviewtime)) = 2008 and (size(split(reviewtext, ' '))) &gt;= 206",301648.0
130443,569596.0,"Hi Bindu, closing bracket for the last line should be before ""="". Instead of (year(from_unixtime(unixreviewtime)) = 2008 ) ; try (year(from_unixtime(unixreviewtime))) = 2008 ;",320103.0
130443,569944.0,"Cause of error: Incorrect Query; There is parenthesis missing in in your query, you can figure that out. I came across multiple situations when the query is wrong (syntactically), I got the same error code. Solution: Try to fix the parenthesis as mentioned by others as well. In SQL and programming, its a good practice to firstly open and close the parenthesis and then come in between them and write code. This really helped me over time.",317987.0
130237,568552.0,Most of us are facing similar problem. One possible reason could be many user are accessing corestack at same time that increases the load on corestack servers.,317991.0
130967,571909.0,"Hi Kaushiki, I didn't face any issue like this till now. Can you please provide a screenshot of running a sample query? Most probably, the query might not be returning anything if you're using query editor instead of a Notebook.",318355.0
129767,566813.0,Check below link:- https://learn.upgrad.com/v/course/208/question/129631,320103.0
129767,566835.0,Thanks..this worked,319759.0
129776,566883.0,"Please provide semicolon (;) after the location. Also, add below to your query for table creation - so that it will skip header of dataset while populating Hive table. tblproperties (""skip.header.line.count""=""1""); If you are adding this at end then provide semicolon after this line and not after location.",320103.0
129806,566973.0,"I would suggest to try below:- 1) Drop table 'airline'. 2) create again with tblproperties (""skip.header.line.count""=""1""); at end of your create table query. 3) try to run required query and see if error occur again.",320103.0
129806,566994.0,"-- IMPORTANT: BEFORE CREATING ANY TABLE, MAKE SURE YOU RUN THIS COMMAND ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; create database bin_muki; create external table if not exists bin_muki.airline( s_no int , Year int, Month int, DayofMonth int, DepTime timestamp, CRSDepTime timestamp, ArrTime timestamp, CRSArrTime timestamp, UniqueCarrier string, FlightNum int, TailNum int, ActualElapsedTime timestamp, CRSElapsedTime timestamp, AirTime timestamp, ArrDelay timestamp, DepDelay timestamp, Origin string, Dest string, Distance string , TaxiIn int, TaxiOut int, Cancelled int, CancellationCode string, Diverted int, CarrierDelay timestamp, WeatherDelay timestamp, NASDelay timestamp, SecurityDelay timestamp, LateAircraftDelay timestamp) tblproperties (""skip.header.line.count""=""1""); ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/common_folder/airlines/'; select count(distinct UniqueCarrier) as no_of_flights from airline; with this code - i get the following error. Error while compiling statement: FAILED: ParseException line 1:0 cannot recognize input near 'ROW' 'FORMAT' 'DELIMITED'",319759.0
129806,567481.0,"You migth want to try with ""bin_muki.airlines"" table instead of just 'airlines'. As it will try to search for this table in default schema.",318368.0
129806,567169.0,"Hi Bindu, One mistake is that you've added an unnecessary ';' semi-colon after [""skip.header.line.count""=""1"")] which marks the end of the query there itself &amp; considers whatever next as different query Second thing which I'm able to notice is that the order of the statements in the last where you've the tblProperties, location &amp; row format is important &amp; is causing compilation error I've modified your query, try running the below query create external table if not exists bin_muki.airline( s_no int , Year int, Month int, DayofMonth int, DepTime timestamp, CRSDepTime timestamp, ArrTime timestamp, CRSArrTime timestamp, UniqueCarrier string, FlightNum int, TailNum int, ActualElapsedTime timestamp, CRSElapsedTime timestamp, AirTime timestamp, ArrDelay timestamp, DepDelay timestamp, Origin string, Dest string, Distance string , TaxiIn int, TaxiOut int, Cancelled int, CancellationCode string, Diverted int, CarrierDelay timestamp, WeatherDelay timestamp, NASDelay timestamp, SecurityDelay timestamp, LateAircraftDelay timestamp ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/common_folder/airlines/' tblproperties (""skip.header.line.count""=""1""); Hope this helps!",318355.0
129539,565894.0,"It is a good practice to include the exist table command before creating a table. This is to ensure that table with same name is not created twice. This practice should be followed while writing any kind of queries - SQL,HQL etc",311254.0
129539,565951.0,It's not required to drop the table if it is not existing but it table already exist Hive will throw you error so better to use drop if exist statments before create table statments in HIve,318476.0
129539,565878.0,"It's not required to drop the table if it is not existing and you are sure about that. Just for safer side, if you are unaware whether table exist or not , it's better to drop first and then create.",320103.0
130896,571632.0,SERDE can tell hive how to get columns/fields out of a row of data. You can refer below link for more info https://data-flair.training/blogs/hive-serde/,311861.0
130896,571779.0,"Hey Shubham, This is what you use: WITH SERDEPROPERTIES ('mapping.customer_id' = 'Customer ID') Source: https://stackoverflow.com/questions/30002399/json-serde-mapping-column-names Thanks!",309211.0
134331,583894.0,Maybe you could write this in the feedback that is asked at the end of every module. The Upgrad team would then surely take note of it. Hope that helps.,317998.0
134331,585933.0,"The course material and the time for the content is designed properly and taken care of that it should be completed within the time limit. Still if you think it's not enough, then concern team will look into it.",329936.0
129366,565095.0,We will be having Python with Spark,318429.0
129366,565134.0,"This cloud lab is providing all the possible/available options. As per my understanding, different platforms use different languages. But here, we will use Spark in Python. Further TA may verify.",311117.0
129369,565128.0,"It is not mandatory to change the password, but it is advisable for security purpose. Time limit : This is Hive default Limits. you can go through the ""Intervals"" topic of this link: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types You can refer this also: http://docs.1h.com/Hive_Limitations",311117.0
129369,565081.0,its not mandatory to change but advisable,318732.0
130375,569916.0,Raise a ticket to support@upgrad.com,329936.0
130376,569235.0,"Hey Raveena, Refer this answer by Vipul - https://learn.upgrad.com/v/course/208/session/37070/segment/197841 You have to set table properties to skip first row i.e. header row of csv files as follows: CREATE EXTERNAL TABLE table name (col1 datatype1...........) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/common_folder/bollywood/' tblproperties (""skip.header.line.count""=""1"");",302742.0
130376,569240.0,Thank you.,303085.0
129082,563683.0,Until the start of next module.,318429.0
129082,563878.0,It will be till the end of this course,308439.0
129082,564149.0,For 6 weeks,308636.0
129082,564997.0,This corestack available for 6 weeks not more than that,308639.0
129961,567269.0,"Hi Ayush, Can't help with the time its taking but can you please post what error are you getting?",318355.0
129961,568240.0,Kindly post what kind of error you are getting.,329936.0
129964,,nan,
129980,567347.0,describe formatted &lt;table_name&gt; command will help to know the Table Type.,312479.0
129919,567859.0,"When you drop an internal table, it drops the data, and it also drops the metadata. Internal table are like normal database table where data can be stored and queried on. On dropping these tables the data stored in them also gets deleted and data is lost forever. On dropping the external table, the data does not get deleted from HDFS. Thus it is evident that the external table are just pointers on HDFS data. Hive table, does not matter internal(managed) or external can have location in S3 or HDFS. There is no connection between table(managed or external) type and storage(S3, HDFS). The difference between managed and external table is that when you delete table or partition, it's location with files also will be deleted. When you delete external table or it's partition, location will remain. Only metadata will be deleted. You can access data in S3 or HDFS location by reading files directly or by creating Hive table or partition and specifying a location. You even can create more than one tables pointing to same location, does not matter, S3 or HDFS.",329936.0
129919,567271.0,"Please go through below links, it has detailed explanation about why data doesn't get deleted from external table but it gets deleted from internal table. And eventually it will answer your question. https://stackoverflow.com/questions/17038414/difference-between-hive-internal-tables-and-external-tables https://stackoverflow.com/questions/16897778/can-i-point-multiple-location-to-same-hive-external-table https://stackoverflow.com/questions/29277584/where-does-hive-stores-its-table?noredirect=1&amp;lq=1 Hope this will help.",317991.0
129265,564586.0,"click on the Auth URL, the URL should be opened into different tab, login with the given credential details. it should work.",318778.0
129265,564622.0,"In Access Information (the first page after you login), just click on that application that you would like to use. You will immediately be getting Username,Password and Auth URL below. Copy that Auth URL and paste it in a new window and access the application upon entering that Username &amp; Password. Here i have clicked Hue. You can see that Username , Password and Auth Url have popped up after my click.",318435.0
130238,569393.0,You should refer to the default jars in the hive manual as given in link below https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL Also sometimes vendors release different patched jars because of customizations etc and you need to refer to the vedor documentation to know further details about the same. You can also define your custom UDF in the jars and use those by first adding that jar to the hive path as given in the link below https://dzone.com/articles/writing-custom-hive-udf-andudaf,329936.0
129327,564876.0,On corestack.io platform once you login.....you have to click on lab access detail. You will get below screen Click on Auth URL button on right side shown using red arrow Once you click you will get below tab Use Webconsole Credentials provided in 2nd image i.e. username and password to login in above webconsole. Hope this will help.,317991.0
129413,566039.0,"Hi Sahil, Use below query to create table:- CREATE EXTERNAL TABLE table name (col1 datatype1...........) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/common_folder/airlines/' tblproperties (""skip.header.line.count""=""1"");",320103.0
129413,565285.0,"Please check datatype assigned for each column. Also, number of column in table should be equal to number of column of dataset and sequence should be same.",320103.0
129413,565408.0,"The steps to be followed: 1. ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; 2. create external table if not exists bollywood_movies(Movie string, Lead string, Rdate int, Ocollection int, Wcollection int, Fwcollection int, Tcollection int, Verdict bigint) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths' = '') location '/common_folder/bollywood'; 3. select Movie, lead_actor, release_date, total_collection, Verdict from bollywood_movie limit 25; You can change the limit. It will work. Maybe helpful.",311117.0
129413,565985.0,Hi tried below commands as suggest and getting below error. Its same as yesterday's. No able to troubleshoot it. Any Idea as to why tis error is occuring.?,312033.0
129413,566437.0,Thank you khusbu. This worked.,304319.0
129448,565527.0,"If the purpose is to maintain the individuality of your work, then a separate personalized database should be created.",305655.0
129448,565735.0,"Default DB is accessible to everyone. So there is high chance of your database objects getting overwritten. To prevent that, please create your own database which is accsible to you and your database objects will remain intact.",311254.0
129448,565510.0,Yes it is recommended to create new db so that it separates other user work from your work.,317991.0
129448,565438.0,"Yes, it is always advisabe to create your own new DB and work on that only (i.e creating tables and modifying the etc). If you are using default and let's say someone else changes something in table, then you will not get correct result for your query. So its always advisable to create you tables inside your own DB rather than default one.",320103.0
129483,565639.0,"The processing of data over HDFS is done through MapReduce codes which are written in Java. The purpose of Hive is to provide an SQL environment which can execute the SQL code written by us, using the appropriate Java Archive(JAR) which are the aggregate of Java Files requires to process the SQL code into appropriate MapReduce code that is, in reality, the code in Java. ' hive- hcatalog -core-1.1.0-cdh5.11.2.jar ' is appointed for the same reason",305655.0
129631,566259.0,In select query the table name is wrong. You created a table by the name airlines but in the select query you are selecting from table1,311254.0
129631,566329.0,Please drop the table and try to create again. I didn't face any issue and your query seems to be correct.,320103.0
129631,566340.0,"Do not use - ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( ""seperatorChar"" = "","", ""quoteChar"" = ""\"""" ) instead use ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' **** This is known limitation of CSVSerDe serde. CSVSerDe treats all columns to be of type String. Even if you create a table with non-string column types using this SerDe, the DESCRIBE TABLE output would show string column type. The type information is retrieved from the SerDe. To convert columns to the desired type in a table, you can create a view over the table that does the CAST to the desired type.",329936.0
129472,565671.0,"It was temporary issue, but now its working fine now. May be happened due to huge network load.",318429.0
129472,565532.0,Please follow the step by step instructions given in the link attached below. It should work http://drive.google.com/file/d/1YK-n7iueLFSHJk6BoQRm3y-e-_73FRC8/view?usp=sharing,305655.0
129693,566887.0,The table does not contain any data. First insert some data into it then run the query.,329936.0
129693,566756.0,Try to run below 2 statements as well before querying the table:- SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000; Also. go through below thread which might help. https://stackoverflow.com/questions/51664932/getting-failed-execution-error-return-code-2-from-org-apache-hadoop-hive-ql-ex,320103.0
129574,566076.0,Got it.Thankyou,300684.0
129574,566057.0,Table is created but theres problem fetching the data,300684.0
129574,566043.0,While creating table just create 1st column as sl_no with datatype of int .,320103.0
129553,566038.0,hive- hcatalog -core-1.1.0-cdh5.11.2.jar converts your HQL(Hive query language) code into JAVA which is understood by map-reduce.,300684.0
129553,565947.0,Its answered in the below link : https://learn.upgrad.com/v/course/208/question/129483,318476.0
130369,569604.0,Can you mention query used?,320103.0
129962,567265.0,You can visit below links for detailed explanation on JsonSerDe. https://docs.aws.amazon.com/athena/latest/ug/json.html https://blog.cloudera.com/blog/2012/12/how-to-use-a-serde-in-apache-hive/,317991.0
129962,567261.0,"https://cwiki.apache.org/confluence/display/Hive/SerDe SerDe is short for Serializer/Deserializer. Hive uses the SerDe interface for IO. The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing. A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats.",300713.0
130048,567620.0,TA can help us in this but right now you can copy the password too by the copy icon next to the password .,318476.0
130048,567630.0,I think these credentials (Username and Password) are provided by corestack and we should not change these credentials. Rest TA can give more insight.,317991.0
130142,568185.0,Similar kind of question is answered in below Discussion Forum link https://learn.upgrad.com/v/course/208/question/129483,317991.0
130052,567618.0,The 504 Gateway Timeout error is an HTTP status code that means that one server did not receive a timely response from another server that it was accessing while attempting to load the web page or fill another request by the browser. So yes it is the server issue. Try to run after sometime.,317991.0
129582,566144.0,"Hi Sahil, The first column of table should be sl_no and not year. Please recreate table with 1st column as sl_no (or any column name as per your convenience) and then try the query again. It will fetch required data correctly.",320103.0
129586,566127.0,"Hi Naga, Use below query to create table:- CREATE EXTERNAL TABLE table name (col1 datatype1...........) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/common_folder/bollywood/' tblproperties (""skip.header.line.count""=""1"");",320103.0
129586,566118.0,"The Bollywood dataset has been placed in your Big Data lab in the path '/common_folder/bollywood/' Where can i see. I have upload the bollywood.csv into my /usr/xyz_gmail Then try to create in my db, shows exceptions how to create from the uploaded csv pls give some steps",312019.0
129586,566089.0,"I did below lines my created db -- CREATE EXTERNAL TABLE create external table if not exists amazon_reviews_table(reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths' = '') location '/common_folder/amazon_reviews'; -- RUN QUERY ON THIS TABLE select reviewerid, reviewername, asin, unixreviewtime from amazon_reviews_table limit 10; But finally shows results as 0",312019.0
129586,566132.0,"create external table if not exists bollywood_movies( Movie string,Lead string,Rdate date,Ocollection double,Wcollection double,Fwcollection double,Tcollection double,Verdict string) location '/common_folder/bollywood'; select * from bollywood_movies; complete row goes to first column, any format to set for create table . comma is not enough ?",312019.0
129586,566133.0,"Hi Naga, Please include below bold part of query as well in your query:- CREATE EXTERNAL TABLE table name (col1 datatype1...........) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION '/common_folder/bollywood/' tblproperties (""skip.header.line.count""=""1"");",320103.0
129586,566240.0,"Thankyou. I used to create one case like below Its creating with all columns as string types and not allowing to modify any clus create external table if not exists fem_semi_conductor( `Sample` int,`ecc` double,`N` double,`gammaG` double,`Esoil` double,`Econc` double,`Dbot` double,`H1` double,`H2` double,`H3` double,`Mr_t` double,`Mt_t` double,`Mr_c` double,`Mt_c` double) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( ""separatorChar"" = "","", ""quoteChar"" = ""\"""" ) location '/user/nagalprasad_gmail/fem_data' tblproperties (""skip.header.line.count""=""1""); not allowing to change the column type alter table fem_semi_conductor modify sample bigint; //or not Error while compiling statement: FAILED: ParseException line 2:0 cannot recognize input near 'modify' 'sample' 'bigint' in alter table statement",312019.0
129294,565216.0,Hi Babita I also faced same issue. Then I found Sqoop user name is different from other credential. Thus it is not working. use username babita_parida_gmail For my case it is fname_lname insetad of fname.lname. Also after -p don't give any space. I am 100% sure it will work.,311404.0
129294,564836.0,"Type below command first on webconsole. mysql -h sqoopdb.upg.cloudlab.com -u your_use_name -p And then when prompted, enter your password.",318368.0
129294,564977.0,Here is a way when the same command will work. mysql -h sqoopdb.upg.cloudlab.com -u your_user_name -pyour_password,301121.0
129294,565066.0,Use the password without space using the below command. mysql -h sqoopdb.upg.cloudlab.com -u your_user_name -p your_password,317689.0
130114,567879.0,Got the answer . Thanks.,312093.0
130114,567941.0,I am also facing the same error.. How did u resolve it?,308644.0
130114,569810.0,"I am still having the same error, the Logs say: ""{""RemoteException"":{""exception"":""NotFoundException"",""message"":""java.lang.Exception: job, job_1558926375940_14820, is not found"",""javaClassName"":""org.apache.hadoop.yarn.webapp.NotFoundException""}} (error 404)"" totally clueless.",300734.0
131282,,nan,
131554,574192.0,"The airline dataset is already present in the folder ""/common_folder/airlines/"" Please go through the videos present in the below link to understans on how to create tables using this dataset: https://learn.upgrad.com/v/course/208/session/37070/segment/197843",313826.0
130150,568113.0,There is option to check the map reduce logs for the hive jobs submitted from the hue interface. Also you can use the application id for map reduce job and check the logs through resource manager.,318476.0
130150,568270.0,You can also view the logs in the job browser in Hue.,329936.0
132396,578055.0,"You can load the data from ""common_folder"" in your table but you do not have write permission in this folder.",329936.0
132396,578297.0,"Then, how to load the data from 'common_folder'?",318772.0
136810,591996.0,Thank you Ankur,302742.0
136810,591765.0,We don't hvae Summary of Lecture notes seperately for Big Data Ingestion with Hive and Sqoop . But you can refer below code snippets link which can help us in summarization https://learn.upgrad.com/v/course/208/session/37070/segment/197841 https://learn.upgrad.com/v/course/208/session/37071/segment/197849 https://learn.upgrad.com/v/course/208/session/37072/segment/197855 https://learn.upgrad.com/v/course/208/session/37073/segment/197858 https://learn.upgrad.com/v/course/208/session/37069/segment/197834 https://learn.upgrad.com/v/course/208/session/37069/segment/197831,311861.0
130308,568947.0,"Hi, I didn't get this message till now. Can you please post the screenshot of the webconsole where you ran the query?",318355.0
130308,569428.0,It means either the MySQL server is not installed/running. In this case may be server was out or overloaded. You can try after sometime with your correct credentials.,329936.0
132919,579679.0,This is already discussed and answered. Please refer https://learn.upgrad.com/v/course/208/question/130160/answer/568291,318762.0
132919,579709.0,Iris data is supposed to be in hdfs. Upload the iris.csv in a folder iris_data in your directory in HDFS and then execute the export command,318762.0
130181,568280.0,Follow these steps to see database and tables. 1.mysql -h sqoopdb.upg.cloudlab.com -u username -p password 2. show databases; (Now you should see the data base created by you.) 3. use your databse name (ex.- use naga_db) 4. show tables; (this will show all the tables created by you in your database.),329936.0
130568,570361.0,Already discussed at - https://learn.upgrad.com/v/course/208/question/130160/answer/568291/comment/128234,329936.0
130362,569446.0,"In the command you ran sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/ pknayak0707_job_gmail (database name) --username pknayak0707.job_gmail (username) --password pknayak0707.job_gmailuco7l (password) --table iris --export-dir iris_data/*. What I have observed that credentials provided to us has same name for database name and username. But in the command you executed I can see different database name ( pknayak0707_job_gmail) and username ( pknayak0707.job_gmail). Make sure while executing the command you give correct credentials for db_name, username and password. Hope this will help.",317991.0
130362,569552.0,"You can rfer this link also, https://learn.upgrad.com/v/course/208/question/130160/answer/568291",329936.0
130362,570358.0,"If usename contains ""."" (dot ) then replace it with ""underscore"". Then try again",308636.0
130212,568396.0,"Using specialized connectors, Sqoop can connect with external systems that have optimized import and export facilities, or do not support native JDBC. Connectors are plugin components based on Sqoop’s extension framework and can be added to any existing Sqoop installation. Once a connector is installed, Sqoop can use it to efficiently transfer data between Hadoop and the external store supported by the connector. By default Sqoop includes connectors for various popular databases such as MySQL, PostgreSQL, Oracle, SQL Server and DB2. It also includes fast-path connectors for MySQL and PostgreSQL databases. Source: https://blogs.apache.org/sqoop/entry/apache_sqoop_overview You can also visit below link to get more insight on databases supported by Sqoop. https://data-flair.training/blogs/sqoop-supported-databases/ Hope this will solve your query.",317991.0
130212,568462.0,"Sqoop can be generally used with Oracle, MySQL, SQL Server and other relational databases. Sqoop is generally used for ingestion from relational DB from Hadoop and exporting data from Hadoop to relational db as well.",317689.0
130212,568556.0,"Sqoop exports data from different external sources such as Oracle, MySQL, MSSQL, Teradata, Postgres etc and imports to Hadoop technologies like HDFS, Hive, HBase etc......(and vice versa) You can consider it as an ETL (Extract Transform Load) tool which fetches the committed transactions from SQL databases and imports into Hadoop for analysing the data. It can also capture the data from non-SQL sources. Sqoop cannot be used for real-time data ingestion for replicating the transaction across the cloud sites. It is because of delay. The real-time replication system must replicate the records in fractions of a second which depends upon the hardware speed.",312479.0
130213,568467.0,"Sqooping is not a good solution for real-time requirements. It can work if the data volume is not that huge and we can work with 5-10 mins of delay. But for near real time applications we can leverage Kafka, flume and other applications as per the requirement.",317689.0
130213,568557.0,"Sqoop exports data from different external sources such as Oracle, MySQL, MSSQL, Teradata, Postgres etc and imports to Hadoop technologies like HDFS, Hive, HBase etc......(and vice versa) You can consider it as an ETL (Extract Transform Load) tool which fetches the committed transactions from SQL databases and imports into Hadoop for analysing the data. It can also capture the data from non-SQL sources. Sqoop cannot be used for real-time data ingestion for replicating the transaction across the cloud sites. It is because of delay. The real-time replication system must replicate the records in fractions of a second which depends upon the hardware speed. Oracle GoldenGate has this capabilities. At present Oracle GoldenGate is a market leader which enables the continuous, real-time capture, routing, transformation, and delivery of transactional data across heterogeneous databases and hadoop.",312479.0
130246,570241.0,"Well, in such cases, as you can clearly see, the data in the table we created is not present. We did try to import the data but seems like it failed (could be various reasons here). I am going to mention a few common reasons so that you can verify and try again: The difference in Table Schema (most common reason): The data loading part fails if the data you are trying to load is not as per the table description. A good example is any date column, which we see as a date in our data. Chances are that date is not in SQL compatible date format or the format of date varies. With human reading, we may skip such minute things but it will cause data loading failure. The solution could be usage of VARCHAR(255) where-ever we are not very sure of datatype and once we are sure, then convert the datatype. The sqoop query might have some error. Minute details such as double underscores etc must be checked. While looking at some cases online, I came to know that there are different delimiter values we might need to set for certain character delimited files. Similarly, we might need to specify line ending in the sqoop query in certain files.",317987.0
130246,568611.0,"In my opinion, your sqoop command did not complete successfully. I can see some expections before making a connection with mysql in your output image. sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/database_name -username your_username --password your_password --table iris --export-dir iris_data/* Just check if you can see iris_data directory in hadoop fs. Please see : https://learn.upgrad.com/v/course/208/question/129473 better to go with bollywood_movies dataset. - https://learn.upgrad.com/v/course/208/question/130160",312479.0
130246,568704.0,You need to first upload the iris dataset to your HDFS directory. You can readily download it from kaggle or someother site as it is pretty common dataset. Post that run export: sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/database_name --username your_username --p assword your_password --table iris --export-dir filepath database_name = database name you created in the mysql db. filepath = file path in hdfs where you placed the iris dataset.,317689.0
130508,570010.0,"Hi Ruchita, I also faced this error. I tried creating the table in MySQL with column types as varchar &amp; it solved the issue of exporting data. This happens because by default Sqoop tries to export data as Varchar, in order to avoid exporting as Varchar, there is some property that needs to be set up. Hope this helps!",318355.0
130508,570251.0,"Well, in such cases, as you can clearly see, the data in the table we created is not present. We did try to import the data but seems like it failed (could be various reasons here). I am going to mention a few common reasons so that you can verify and try again: The difference in Table Schema (most common reason): The data loading part fails if the data you are trying to load is not as per the table description. A good example is any date column, which we see as a date in our data. Chances are that date is not in SQL compatible date format or the format of date varies. With human reading, we may skip such minute things but it will cause data loading failure. The solution could be usage of VARCHAR(255) where-ever we are not very sure of datatype and once we are sure, then convert the datatype. The sqoop query might have some error. Minute details such as double underscores etc must be checked. While looking at some cases online, I came to know that there are different delimiter values we might need to set for certain character delimited files. Similarly, we might need to specify line ending in the sqoop query in certain files.",317987.0
130508,570353.0,"Hi Ruchita, kindly refer to the similar question asked earlier - https://learn.upgrad.com/v/course/208/question/130160/answer/568291/comment/128234",329936.0
129813,567824.0,Follow these steps to see database and tables. 1.mysql -h sqoopdb.upg.cloudlab.com -u username -p password 2. show databases; (Now you should see the data base created by you.) 3. use your databse name ex.- use naga_db) 4. show tables; (this will show all the tables created by you in your database.),329936.0
130826,571950.0,"Hi, Following is my understanding of your question: In Hive You're creating a new database say 'test_database' A new table inside this database say 'test_table' In MySQL You want to import the Hive table i.e. 'test_database.test_table' into the MySQL using SQOOP Please rectify me if I'm wrong so that I don't end up posting a tutorial for something which you were not looking for.",318355.0
130826,572260.0,"Hi Naga, Try to understand the reply from corestack and I would suggest you to revist the ""Apache Sqoop"" course and try to understand how ""import/export"" command works and what is the purpose of using the commands. Also, you can go to this link how to work with export command. for example :- https://learn.upgrad.com/v/course/208/question/130160/answer/568291/comment/128234",329936.0
130668,570582.0,"Hi, The import operation will bring the data present in your RDBMS to HDFS, and hence most of the details will come from the RDBMS only. Username : your sqoop username Password : your sqoop password connect jdbc:mysql://database.upg.cloudlab.com/sqoopdb : change the last /sqoopdb to your RDBMS (MySQL) database name. Table name : the table within the above selected database which you want to import to HDFS. (The table must exist before you try importing to HDFS) target-dir : the folder on HDFS where you want to import the table and its data. You must have read as well as write access to this folder in order to import data successfully, hence use your folder in HDFS (open HDFS browser, the folder it shows inotially is most probably the one you have read as well as write access to). For more info, you can refer to the introductory live session where they told which folder permission do we have.",317987.0
129942,567864.0,show tables; command will list the tables inside your database. for more detail refer - https://learn.upgrad.com/v/course/208/question/129813,329936.0
130367,569564.0,"Note that there should be no space between '-p' and your password. You should type your password just after '-p'. (-ppassword). use ""-p"" instead of ""-password""",329936.0
130367,570599.0,@Apurva Deshpande Please post command which resolved this issue. I am getting the same error while executing import command.,308636.0
130364,588547.0,Please post the solution here,311803.0
130364,570556.0,@Babita Parida Roy It would be great if you post your solution here. I am getting the same error.,308636.0
130364,569185.0,Please ignore my question. Issue is resolved now.,307494.0
129765,566801.0,Refer to my answer at following: https://learn.upgrad.com/v/course/208/question/129294,317689.0
129765,574424.0,"I had same issue. I tried password without space, it worked. mysql -h sqoopdb.upg.cloudlab.com -u your_user_name -p your_password",320074.0
130365,569557.0,"Refer this, already discussed question - https://learn.upgrad.com/v/course/208/question/130160/answer/568291",329936.0
130365,574482.0,I still have the same issue with iris data. 19/06/09 06:07:56 ERROR tool.ExportTool: Encountered IOException running export job: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern hdfs://nameservice1/user/sridhar1987_outlook/iris_data/* matches 0 files at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323) at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265) at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51) at org.apache.sqoop.mapreduce.ExportInputFormat.getSplits(ExportInputFormat.java:64) at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:305) at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:322) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:200) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304),320074.0
130160,568291.0,"Hi Ranip, kindly refer to your similar question asked earlier,Hope you have already created the table ""bollywood_movies"" in mysql. As I have successfully able to export the table using the below command/steps - 1. login to mysql 2. change to your database 3.create table bollywood( Movie varchar(20), Lead varchar(20), Release_date varchar(20), Ocollection varchar(20), Wcollection varchar(20), Fcollection varchar(20), Tcollection varchar(20), Verdict varchar(20)); 4. exit from mysql 5. sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/grader_01 --username grader_01 --password ****** --table bollywood --export-dir /common_folder/bollywood/* 6. step 1 &amp;2 7. select * from bollywood; successfully get the data.",329936.0
130664,570584.0,"Hi, The import operation will bring the data present in your RDBMS to HDFS, and hence most of the details will come from the RDBMS only. Username : your sqoop username Password : your sqoop password connect jdbc:mysql://database.upg.cloudlab.com/sqoopdb : change the last /sqoopdb to your RDBMS (MySQL) database name. Table name : the table within the above selected database which you want to import to HDFS. (The table must exist before you try importing to HDFS) target-dir : the folder on HDFS where you want to import the table and its data. You must have read as well as write access to this folder in order to import data successfully, hence use your folder in HDFS (open HDFS browser, the folder it shows inotially is most probably the one you have read as well as write access to). For more info, you can refer to the introductory live session where they told which folder permission do we have.",317987.0
130664,570783.0,"Found mistake in import query- In --connect parameter link, need to give database name i.e. connect jdbc:mysql://sqoopdb.upg.cloudlab.com/shalaka_mh_yahoo",308636.0
129473,566134.0,"try again with this command, I think you missed double ""--"" in username is the problem, hope it helps. sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/johnykrrish_gmail --username johnykrrish_gmail --password ****** --table iris --export-dir iris_data/*",329936.0
129473,570957.0,Checked HDFS file structure from Namenode Web UI. I executed it for bollywood data as file is present in common folder.,308636.0
129473,571015.0,"I did these.....still getting an error. Can anyone point out what i am missing here? -- creating a table in the database create table bollywood_movies(Movie varchar(20), Lead varchar(20), Rdate date, Ocollection float, Wcollection date, Fwcollection date, Tcollection float, Verdict varchar(20)); -- select the data from the created table select * from bollywood_movies; -- exiting from mysql exit -- exporting data from HDFS into mysql table sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/pvgarde_gmail -username pvgarde_gmail -- password ********** --table bollywood_movies --export-dir /common_folder/bollywood/bollywood.csv",311857.0
129473,567776.0,"Yes, when I dig deeper I found your assumption is correct. iris data is supposed to be in hdfs. I uploaded the iris.csv in a folder iris_data in my directory in HDFS . then ran the export command , and it's working.",329936.0
129473,568598.0,I dont see any folder named iris_data directory in hdfs and the file iris. csv and I am getting InvalidInputException as 0 files found. Can you please provide the absolute path for the iris files in hdfs.,311115.0
132853,579476.0,"I think in this case, for every unique identifier one spark task will be created. No shuffle will take place. Every unique identifier will have a separate key,value.",308673.0
130271,568714.0,"Similar type of questions has been answered in below discussion forum links, have a look into it: https://learn.upgrad.com/v/course/208/question/130184 https://learn.upgrad.com/v/course/208/question/130114 -- Go through comments also https://learn.upgrad.com/v/course/208/question/130072 Hope this will help.",317991.0
130128,568001.0,Please check the location of the table where you created it. It should in the database which you created,311254.0
130128,568179.0,If you have created separate database (which is recommended) then check that database whether the table is there or not. If its not there then there is possibility that you haven't not selected the database which you have created in this case check outside your database.,317991.0
130306,568857.0,"Check where you have created the table. - If you have created separate database (which is recommended) then check that database whether the table is there or not. - If its not there then there is possibility that you haven't selected the database which you have created, In that case check outside your database.",317991.0
130306,569052.0,"To elaborate on Vipul's answer, at some point before executing your query you should have done the following : -- CREATE EXTERNAL TABLE create external table if not exists amazon_reviews_table(..... -- Then create external table with partition create external table if not exists amazon_reviews_year_month_partitioned(... -- Then insert the data in the partition table insert overwrite table amazon_reviews_year_month_partitioned partition(yr, mnth)... -- Then create ORC table create external table if not exists amazon_reviews_year_month_partition_orc(... -- Then, write data from partition table into ORC table insert overwrite table amazon_reviews_year_month_partition_orc partition(yr , mnth)... Whichever database you used for the above tables that is where you should execute the ""select"" query.",311857.0
130306,568956.0,"Hi Bindu, In order to see the location of any table you're able to access. Just run the query 'show create table tablename' For example: Executing 'show create table amazon_reviews_year_month_partition_orc' query would provide the whole create statement which was used to create the table. On scrolling down to the last few statements of the results, there is a location attribute where the location of the files corresponding to the table would be mentioned. running 'hdfs dfs -ls location_of_table' in the webconsole would enable you to see the files corresponding to the table Also, you may move to the directory using HDFS Browser in the HUE application window I would have liked to add screenshots but there is some issue with Hue. Hope this helps!",318355.0
130338,569021.0,"Hi Neha, Can you please provide the screenshot? From the details, it seems that some service restart is happening at the backend.",318355.0
130338,569026.0,"Hi Neha, Can you please provide the screenshot? From the details, it seems that some service restart is happening at the backend.",318427.0
130338,569072.0,Run the ADD Jar command once again before executing the query.,304319.0
130338,569518.0,"Some of the possibility and solution ;- Issues with meta store related with concurrency - Run the ADD Jar command once again When you start hive server and keep it running for days and then run your code, it is possible that your connection is broker to server and you will get exact same error. If you reconnect to server and this error will go away. This is only because after some time wait_time expired and disconnect happen. - logout and login again Try the above steps, hope it helps.",329936.0
130292,568839.0,"size() - will give you the length of reviewtext for each row . However, avg() and variance() are the aggregate functions. They will return the single value for all the rows combined together. One solution to this is to group by statement, something like this: select size(split(reviewtext, ' ')) as words, avg(overall) as avg_rating from amazon_reviews_year_month_partition_orc where yr=2013 and mnth=1 group by size(split(reviewtext, ' '))",318730.0
131553,574185.0,If the intention is to select only those rows for which all the 3 variables are non-zero then you need to use and instead of the or condition like and (passenger_count &lt;&gt; 0 and trip_distance &lt;&gt; 0 and fare_amount &lt;&gt; 0 );,313826.0
130310,568878.0,"Similar type of questions has been answered in below discussion forum links, have a look into it: https://learn.upgrad.com/v/course/208/question/130184 https://learn.upgrad.com/v/course/208/question/130114 -- Go through comments also https://learn.upgrad.com/v/course/208/question/130072 Hope this will help.",317991.0
130310,569492.0,Kindly post full code you are using to create partitioned table and inserting data.,329936.0
130310,569547.0,"Query being executed: insert overwrite table amazon_reviews_year_month_partitioned partition(yr, mnth) select reviewerid, asin, reviewername, helpful, reviewtext, overall, summary, unixreviewtime, year(from_unixtime(unixreviewtime)) as yr, month(from_unixtime(unixreviewtime)) as mnth from amazon_reviews_table;",310210.0
130805,571676.0,show databases; commond in MYSQL will show you the database you created with your login ID.,329936.0
130244,568572.0,"Autocompleter is enabled by default. To manually enable or disable, use the Enable Autocompleter flag . Log on to Hue and go to either the Hive or Impala editor. Place your cursor in the editor window. Open the Autocompleter settings panel with the shortcut, command-, (Mac) or Ctrl-, (Windows). Do not miss the comma. Tip: Type ? (anywhere but the active editor) to open a menu of Editor keyboard shortcuts . To Enable Autocompleter, check the box. To disable, uncheck the box. To Enable Live Autocompletion, check the box. To disable, uncheck the box. Source: https://www.cloudera.com/documentation/enterprise/5-9-x/topics/hue_use_autocompleter_enable.html Hope this will help.",317991.0
130244,568574.0,Thanks for such quick response,318386.0
130254,,nan,
130377,569307.0,"Please follow below steps only in your created database since you are are directly creating the ORC table therefore it has no rows, in Short first create a table and insert rows from HDFS, then do the partioned and insert rows from original table and finally create the bucketing then ORC and insert rows below ttied to explain. create external table if not exists amazon_reviews_table using below statement. 1. create external table if not exists amazon_reviews_table 2.Then create the Partioned table and insert rows from the Original Table created in step 1. 3. Similarly Create the Bucketing or cluster on the Partion Table using below statement and insert rows as below, Note the bucket folder i have used is /user/hive/warehouse/amit_bucketing_folder insert overwrite table amazon_reviews_year_month_partitioned_clustered partition(yr, mnth) select reviewerid, asin, reviewername, helpful, reviewtext, overall, summary, unixreviewtime, year(from_unixtime(unixreviewtime)) as yr, month(from_unixtime(unixreviewtime)) as mnth from amazon_reviews_year_month_partitioned; 4. Finally Create the ORC table using your stateemnt and use below statement to insert rows. insert overwrite table amazon_reviews_year_month_partition_orc partition(yr , mnth) select * from amazon_reviews_year_month_partitioned;",307843.0
130377,569358.0,"This is the code snippet from this module ... where we have "" select * from amazon_reviews_year_month_partitioned;"" . The question is where this table :""amazon_reviews_year_month_partitioned"" comes from ? is it typo ? -- IMPORTANT: BEFORE CREATING ANY TABLE, MAKE SURE YOU RUN THIS COMMAND ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; -- ORC FILE FORMAT -- This format improves query performance -- First, create ORC table create external table if not exists amazon_reviews_year_month_partition_orc (reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) partitioned by (yr int, mnth int) stored as orc location '/user/hive/warehouse/your_partition_folder_name_orc' tblproperties (""orc.compress""=""SNAPPY""); -- Then, write data from partition table into ORC table insert overwrite table amazon_reviews_year_month_partition_orc partition(yr , mnth) select * from amazon_reviews_year_month_partitioned;",302877.0
130377,569354.0,"@ Amit - Still getting error at step 2 Usug the below query and i have created external table (amazon_reviews_table;) and partition table (amazon_reviews_year_month_partitioned) within my DB insert overwrite table amazon_reviews_year_month_partitioned partition(yr, mnth) select reviewerid, asin, reviewername, helpful, reviewtext, overall, summary, unixreviewtime, year(from_unixtime(unixreviewtime)) as yr, month(from_unixtime(unixreviewtime)) as mnth from amazon_reviews_table; Do you have the entire snippet so that i can execute?",302877.0
130377,569468.0,"What I can observe from the above mentioned error is that your table amazon_reviews_year_month_partitioned does not have data. - This is the table we created and populated from amazon_reviews_table - Once you will have data in amazon_reviews_year_month_partitioned table then run your query insert overwrite table amazon_reviews_year_month_partition_orc partition (yr , mnth) select * from amazon_reviews_year_month_partitioned; to populate amazon_reviews_year_month_partition_orc table. I hope this will help. If not do let me know.",317991.0
130377,569904.0,"Firstly, there are 2 SQL statements in your code provided. There are the following cases: 1. If the error comes at first query itself, the second query would also give you an error. 2. If you are able to successfully create the orc table and getting no data in it, chances are your insert command did not execute fine. There could be following cases for this: Insert query gave an error: The second query relies on data from a pre-existing table. Please check if the partitioned table exists or not. Also, the table structure for partitioned and orc table must exactly match. No error came but there is no data in orc table: This could be due to the possibility that you have no data in the partitioned table itself. Verify it using the command select * from amazon_reviews_year_month_partitioned; Hope this resolves your issue. These are not the exhaustive list f situations but common ones which we might have done, being a beginner.",317987.0
130857,572890.0,Check if the table is having data for the yr 2004.,329936.0
130857,571630.0,"Looks like the output of the function size(split(reviewtext, ' ')) is producing null values for you. Please check the output of : select size(split(reviewtext, ' ')) from amazon_reviews where year(from_unixtime(unixreviewtime)) = 2004 If it's not null then you should get the expected output.",318495.0
130327,568933.0,"You can use below command to find corr between two columns. corr(col1, col2) - Returns the Pearson coefficient of correlation of a pair of a numeric columns in the group.",317991.0
130327,568959.0,The query got executed but i didnt get any result displayed.,319759.0
130494,569910.0,"Issue Overview This is actually some CSS and front end to browser compatibility issue. The red popup is actually an error message which should have ideally came along with an error text, but here we see mostly HTML. Since I had a similar issue, I tried to look into the error text and it says Map Reduce Job fails. Issue Cause It is possible that you failed to import the .jar file that they asked to import at the beginning of every session. It is possible that your map reduce job failed due to some other issues (many cases are possible) Resolution Try running the query in some time (maybe there are a lot of ongoing map-reduce jobs) Import the files they mentioned to import prior running queries. (The .jar file, as well as the other codes, mentioned to execute prior execution of partitioned table creation)",317987.0
130494,569851.0,Figured it out.,302742.0
130333,569018.0,"Hi Keshav, As this is a graded question, I won't be able to tell you much but can explain the basics of Corr function. Corr function takes two arguments. For example: corr(Column1, Column2) You need to input helpfulness according to the given logic in the question as one of the columns. Hope this helps",318355.0
130333,569085.0,each data point in the helpful is an single dimension array having just 2 elements. so you can access element of each datapoint using simple array method i.e. a[i] or a[j]. use it get the desires element and perform mathematical operation as asked in the question using HQL query. how to use CORR in query is already given in one of the provided examples in the course.,314431.0
130333,569328.0,"Hi Keshav, 1. you need to identify the positive reviews and total reviews from the helpful column. 2. Then apply corr function using those derived column with length of reviews",301648.0
130333,569023.0,"Hi Keshav, As this is a graded question, I won't be able to tell you much but can explain the basics of Corr function. Corr function takes two arguments. For example: corr(Column1, Column2) You need to input helpfulness according to the given logic in the question as one of the columns. Hope this helps",318427.0
130689,571240.0,"i used following queires and got 2 different answers:- select stddev_pop( size(split(reviewtext, ' ')) ) as stddev_length from amazon_reviews_year_month_partition_orc where yr = 2004; -- select stddev_pop(size(split(reviewtext, ' '))) from amazon_reviews_table where year(from_unixtime(unixreviewtime)) = 2004;",318791.0
130689,570858.0,"Hi, I verified the same and I am able to get the very same result in both the cases. Checks: Check if the two tables have the same number of records. select count(*) from &lt;tablename&gt;; Check if the query is exactly the same in both the cases except table_name.",317987.0
130689,570671.0,Ideally the answer for main table and ORC table should be the same. Here is the answer I got: Can you share the query you are running ?,317991.0
130391,569339.0,"You can visit below links where similar type of questions has been answered in discussion forum, have a look into it: https://learn.upgrad.com/v/course/208/question/130184 https://learn.upgrad.com/v/course/208/question/130114 -- Go through comments also https://learn.upgrad.com/v/course/208/question/130072",317991.0
130391,569951.0,"Cause of Error: In all the cases I faced with this particular error, it was the folder permissions in which I was performing the operation. Possible Solutions: Check if you have read as well as write access for the folder you are working in If you are trying to execute the query provided, you might not have updated your 'user_name' or 'user_folder' or similar text used in various queries of table creation",317987.0
131392,573454.0,"Please restart your login and try again. If problem still persists,can you tell which command you are running.",311861.0
131700,575010.0,Add and execute the jar file at the start ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; Then run your select script.,317991.0
130007,567627.0,hadoop fs -rm &lt;path to ur folder &gt; will remove the folder from hdfs.,318476.0
130007,567929.0,"Sambit , but where to run this command , can you help us ? Is there any GUI , where we can click or select the files to do the same operation ?",311861.0
130007,568247.0,You can open hdfs browser in HUE and can delete/update any file/dir under your user name.,329936.0
130007,568255.0,"Hi, I tried deleting the folders using the editor but its a bit tricky. If you try to simply delete using trash button it won't work. Please follow the steps below in order to delete anything from HDFS. Open Hue interface &amp; click on the icon as shown in the screenshot. This will lead you to the HDFS browser as shown in the below screenshot. Check mark the folder which you want to delete &amp; click on the drop down as shown in the screenshot. Now, choose delete forever from the options provided as shown in the screenshot below. Click on Yes, a process would be initiated to delete the requested folders. Thank you!",318355.0
130184,569077.0,"You also need to supply the compression type as : create external table if not exists amazon_reviews_year_month_partition_samp (reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) partitioned by (yr int, mnth int) stored as orc location '/user/hive/warehouse/001_ORC' tblproperties(""orc.compress"" = ""SNAPPY"");",304319.0
130184,568259.0,"Hi, The error code 1 should mean that you need to clear up your HDFS as taught in debugging video. Please provide logs so that we can be assure on this point. Here's the screenshot from the debugging video: Also, I would recommend you to follow my answer on this question: https://learn.upgrad.com/v/course/208/question/130007 to clear up your HDFS. Let me know if you're still facing issues.",318355.0
130184,568281.0,"Can you replace this value - "" your_partition_folder_name_orc"" in the query with any unique name like 001_ORC and try again? create external table if not exists amazon_reviews_year_month_partition_samp (reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) partitioned by (yr int, mnth int) stored as orc location '/user/hive/warehouse/001_ORC';",311502.0
130050,567650.0,Getting error even while executing simple select count * query.,300718.0
130050,567645.0,I am also getting gateway timeout error while inserting data from partitioned table to ORC table.,300718.0
130050,567622.0,"Causes of 504 Gateway Timeout Errors Most of the time, a 504 Gateway Timeout error means that whatever other server is taking so long that it's ""timing out,"" is probably down or not working properly. Since this error is usually a network error between servers on the internet or an issue with an actual server, the problem probably is not with your computer, device, or internet connection. That said, there are a few things you can try, just in case: How to Fix the 504 Gateway Timeout Error Retry the web page by clicking the refresh/reload button, pressing F5 , or trying the URL from the address bar again. Even though the 504 Gateway Timeout error is reporting an error outside of your control, the error might only be temporary. Simply retrying the page is a quick and easy thing to try. Restart all of your network devices . Temporary problems with your modem, router , switches , or other networking hardware could be causing the 504 Gateway Timeout issue you're seeing. Just restarting these devices could help. While the order you turn off these devices isn't important, the order that you turn them back on is. In general, you want to turn devices on from the outside-in. If you're not sure what that means, check out the link at the beginning of this step for a complete tutorial. Source: https://www.lifewire.com/504-gateway-timeout-error-explained-2622941 Hope this will help.",317991.0
130072,567817.0,"Usually, if you re-try it after a while, it will go through. Happened with me.",310974.0
130072,567943.0,"even i keep getting the below error while trying to Insertthe data : Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask did you get the resolution?",316036.0
130072,567978.0,1) Ensure these 3 command are executed before you create the table: ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000; 2) Ensure the partitioned table is created in the same database which you have created i.e give the location as /user/hive/warehouse/your_folder_name,311254.0
130072,568078.0,There was a yntax error in my case. fixng that solved the problem.,316211.0
130134,568229.0,"Hi Sahil, I checked once again through the course videos for confirmation that Debugging of Error code 2 is taught or not. It would be great if you can post the logs when you run the above commands. Possibly the debugging can be done after looking at the logs.",318355.0
130134,568264.0,Kindly post the log from the job browser for this command. It will be easy to understand the issue.,329936.0
136934,592685.0,You can refer these videos -- https://www.youtube.com/watch?v=cfDZLVrixdg https://www.youtube.com/watch?v=jQ1YDRW-rKE,329936.0
129643,566321.0,We are given only HDFS file system not S3. S3 is the file system provided by AWS. Use HDFS equivalent files in your sqls.,310974.0
131607,575422.0,"I would suggest not using () and combining conditions like we do in other languages like python. Try using below instead: insert overwrite table taxi_data_year_month_part_orc partition(mnth,yr) select * from taxi_data_year_month_part_orc WHERE mnth in (11,12) and yr=2017 and passenger_count &gt; 0 and trip_distance &gt; 0 and fare_amount &gt; 0;",313515.0
131607,574522.0,Since it is an assignment question I can only give hint. Try to fix this portion of the code (mnth=11 and yr=2017) or (mnth=12 and yr=2017) Once you fix this code you will be get your desired result. Hope this will help.,317991.0
129504,565708.0,"While creating table, we need to read CSV/JSON etc file and store it is proper format so that we can write query to do further analysis. So the location mentioned in the create table is location of file. once table is created, it is stored in default location - /use/hive/warehouse which is directory in HDFS.",320103.0
129504,566743.0,I still did not understand this. What if I want to create the partitioned table and read the data from raw files in one go? How can I write such query to read a file from a location and create the paritions in other location?,304814.0
129504,565769.0,"The create table command contains the location of the file (csv/json) based on which the table would be created. The table created is stored in hdfs i.e /user/hive/warehouse The partition table created in the session is based on the non partition table i.e inserting data from the non partitioned table. So the create partition table command contains the location where the table should be stored i.e /user/hive/warehouse Had we created the partition table based on the csv/json file, we have to put the location of the file in the create partition table and by default it would be stored in /user/hive/warehouse location",311254.0
130155,568111.0,Buckets are not equally distributed .It depends on the hashing algo used and key/columns values used for bucketing. if the columns values are equally distribute then the size of each bucket are same.,318476.0
130155,568274.0,"Bucketing is another technique for decomposing data sets into more manageable parts. For example, suppose a table using date as the top-level partition and employee_id as the second-level partition leads to too many small partitions. Instead, if we bucket the employee table and use employee_id as the bucketing column, the value of this column will be hashed by a user-defined number into buckets. Records with the same employee_id will always be stored in the same bucket. Assuming the number of employee_id is much greater than the number of buckets, each bucket will have many employee_id . While creating table you can specify like CLUSTERED BY (employee_id) INTO XX BUCKETS; where XX is the number of buckets . Bucketing has several advantages. The number of buckets is fixed so it does not fluctuate with data. If two tables are bucketed by employee_id , Hive can create a logically correct sampling. Bucketing also aids in doing efficient map-side joins etc.",329936.0
129416,565312.0,"Below run below queries before creatingpartitioned tables:- General Instruction : Before creating any table and partitioning any table, make sure you run these commands. These commands are prerequisites for running the code in the lab without error: ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive- hcatalog -core-1.1.0-cdh5.11.2.jar; SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000;",320103.0
129416,565393.0,"Hi Karthik, In the query - instead of location 'location '/user/hive/warehouse/give_your_partition_folder_name' (which is at end of query) provide correct location (i.e. location of database on which you have created table and running your query). That should resolve above error. How to know your path:- 1) Go to File Browers -&gt;HDFS Brower (on top right side of the lab) 2) Go to location User/hive/warehouse and then look for your Database name (that you have created and running your query). Hope this will resolve your issue.",320103.0
129416,567360.0,"Hello, I am still facing the same issue after following the above step. Can you please help. Thanks, Upendra",308965.0
131743,575851.0,Can you mention query which is causing above error?,320103.0
131743,577856.0,Kindly refer similar issue - https://learn.upgrad.com/v/course/208/question/131722,329936.0
130229,,nan,
129726,566767.0,yes,304692.0
129726,566737.0,"Did you add the jar statement before creating the table. Please add the jar statement General Instruction : Before creating any table, make sure you run this command: ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive- hcatalog -core-1.1.0-cdh5.11.2.jar;",311254.0
129726,566772.0,Try to read document from provided location - location '/common_folder/airlines/' instead from your own location.,320103.0
129726,567049.0,"I am getting the below error while insrting the rows from non-partitioned table to partition table Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask",314092.0
130349,569529.0,"Check the column names are correct. As this error says that it is expecting ""serial"" instead of ""SNo"".",329936.0
129928,567205.0,https://learn.upgrad.com/v/course/208/question/129643,312019.0
129928,567208.0,S3 is object store offering by AWS. Similar offering is provided as Blob storage by Azure and GCS as Google Cloud Platform. Where as HDFS is distributed file system which is part of the physical cluster which can be hosted on-premise or on cloud. HDFS. Main difference being HDFS is a file store where as S3 is an object store. Refer to following links for more info. https://www.netapp.com/us/info/what-is-object-storage.aspx https://cloudian.com/blog/object-storage-vs-file-storage/,317689.0
130359,569211.0,"You can visit below links where similar type of questions has been answered in discussion forum, have a look into it: https://learn.upgrad.com/v/course/208/question/130184 https://learn.upgrad.com/v/course/208/question/130114 -- Go through comments also https://learn.upgrad.com/v/course/208/question/130072",317991.0
129922,567257.0,"I am also getting the same error, unable to find any solution on google as well. Can someone please help.",306725.0
129922,567280.0,Go through below link with some answers and it also has reply from TA. https://learn.upgrad.com/v/course/208/question/129695,317991.0
129922,567845.0,"Check the error log in the job browser , that have the exact error details. Also, try running it again.",329936.0
129978,567510.0,"Hi, I think we can store the data in HDFS and then we can process the same unstructured dataset using hive. here are multiple ways to import unstructured data into Hadoop, like below . 1. Using HDFS shell commands such as put or copyFromLocal to move flat files into HDFS. 2. Using Apache Flume",301648.0
129611,566150.0,"""External"" means external to the default directory that Hive is using to store data . It doesn't mean it is on the local filesystem - it must be on HDFS, on the same Hadoop cluster that Hive is pointing to. Since it is HDFS data, Hive queries on it is treated exactly the same as if you had written a mapreduce job operating on that data directly. That is to say, it is not copied to /apps/hive/warehouse before hive will operate on it. Functionally the only difference is that if you DROP TABLE an external table, the data is not deleted from HDFS. Other than that, everything else works exactly the same for internal vs external tables. The data is not replicated when you make an external table. So, when dropping an EXTERNAL table, data in the table is not deleted from the file system.",329936.0
132120,577880.0,"FYR, - https://learn.upgrad.com/v/course/208/question/131722",329936.0
132120,577260.0,"Hi Bharati, Use the command "" show create table nytlcdatatable_orc "" to see the create statement used for creating this table.The table mentioned by you does not look to be bucketed. Below is the output : show create table nytlcdatatable_orc; CREATE EXTERNAL TABLE `nytlcdatatable_orc`( `vendorid` int, `tpep_pickup_datetime` timestamp, `tpep_dropoff_datetime` timestamp, `passenger_count` int, `trip_distance` double, `ratecodeid` int, `store_and_fwd_flag` varchar(2), `pulocationid` int, `dolocationid` int, `payment_type` int, `fare_amount` double, `extra` double, `mta_tax` double, `tip_amount` double, `tolls_amount` double, `improvement_surcharge` double, `total_amount` double, `extra_fare` double) PARTITIONED BY ( `yr_pickup` int, `mnth_pickup` int) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' LOCATION 'hdfs://nameservice1/user/hive/warehouse/Bharati_Partition_table_folder_orc' TBLPROPERTIES ( 'last_modified_by'='pandabharati2017_gmail', 'last_modified_time'='1560312596', 'orc.compress'='SNAPPY', 'transactional'='true', 'transient_lastDdlTime'='1560312596')",313826.0
132120,577261.0,Check if the ORC table is in ACID form. Use show create table_name to understand the table properties. Or buckets have not been created properly.,308673.0
130054,571454.0,"You can right click on the location in your hue notebook. It will open up a pop up window, where you can navigate.",318495.0
130054,567619.0,do hadoop fs - ls &lt;path to your partition&gt; from the web console or dfs -ls &lt;path to your partition &gt; in the hive query,318476.0
130054,568256.0,You can view the partition details in the Metastore manager. In HUE right click on the partiton table and go to metastore where you can find the details about partition.,329936.0
131549,574172.0,Check out below link on similar query https://learn.upgrad.com/v/course/208/question/130676,313826.0
131549,574312.0,To do delete/update operations in Hive table File format must be in ORC file format with TBLPROPERTIES(‘transactional’=’true’) Table must be CLUSTERED BY with some Buckets. Note: if table is not bucketed then you will get FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table that does not use an AcidOutputFormat or is not bucketed. https://learn.upgrad.com/v/course/208/question/130787,317991.0
131570,574305.0,"To add or drop partitions for a table , the table must already be partitioned (that is, created with a PARTITIONED BY clause). The partition is a physical directory in HDFS, with a name that encodes a particular column value (the partition key ). Example: alter table partition_t add if not exists partition (y=2000); Source: https://www.cloudera.com/documentation/enterprise/5-8-x/topics/impala_alter_table.html https://data-flair.training/forums/topic/how-to-add-partition-in-existing-without-partitin-table/",317991.0
131570,574258.0,yes once you create an external or internal table after that you can make a partition.,301648.0
130447,569601.0,Can u please mention what query u have run which is throwing this error?,320103.0
130447,569603.0,Check below link:- https://learn.upgrad.com/v/course/208/question/129416,320103.0
130447,569936.0,"As far as I can see from your error logs, you are trying to perform some write operation on the data stored in /common folder/airlines Please note that this folder is provided to all of the learners for read-only operations. We do not have write access to this folder, neither we should try doing so, as discussed in the live session. You can share the complete query and the task you are trying to accomplish for further help.",317987.0
125493,,nan,
125781,549269.0,"Hi, You can try, but you can get best result using inbuild functon in the model algorithms. Like : LogisticRegression(class_weight={0:0.1, 1: 0.9})",344894.0
124702,542598.0,"Hi, Please work with original churn. I will suggest do not use for class imbalanced.",344894.0
124712,542594.0,They have provided date_of_last_rech and last_day_rch_amt which mean - when was the last recharge done in that month &amp; of what amount,316147.0
124716,543460.0,"Hi, Please follow the below process 1. calculate total incoming and outgoing minutes of usage 2. calculate 2g and 3g data consumption 3. create churn variable: those who have not used either calls or internet in the month of September are customers who have churned Thanks",344894.0
124717,543649.0,"Hi, As per my understanding, Sachet and Monthly are service schemes. Numbers 0,1, .. stands for time period. I suppose, In case of Monthly it is 0 month, 1 month etc.. For Sachet it is number of days 0 days, 1 day, 2 days etc. Thanks, Kiran",306736.0
124717,544718.0,"The dictionary has defined the columns as 'KPI' which in my understanding refers to ""Key performance indicator"" that rates the customers based on their usage of the scheme.",305655.0
124717,545469.0,"Monthly column represents a package for n months (values are around 0..4) Sachet column represents a package for n days (values are around 0...30, and also some up to 42 days, etc)",300694.0
125512,547399.0,"You can refer to the following TA verified answers, it has been explained well. https://learn.upgrad.com/v/course/208/question/124433 https://learn.upgrad.com/v/course/208/question/124154 Also go through the comments for each answer for a better understanding. Hope that helps.",317998.0
125512,547562.0,This has been answered several times on the forum already,300694.0
122726,533942.0,"Hi, You need to create Churn variable based on problem statement. Yes, you need to run multiple model",344894.0
122726,539006.0,"yes multiple models are needed but start with the most basic model first i.e. Logistic Regression , use this model as a benchmark or basis or other models to be built. This will help you clear your doubts about other models which you will build.",315560.0
124739,543432.0,As Date Columns are having high number of Null Values(NaN) - how you are imputing the NaN values?,311502.0
124739,542894.0,"This is good question , same I encounter when comparing two date columns I did all the date column converted to one format and now able to proceed further this date comparision I did it for the deriving new attributes. Also check the data types all dates column. TA please verify. Below are the ways we can convert date format. https://stackoverflow.com/questions/38067704/how-to-change-the-datetime-format-in-pandas",307843.0
124739,545464.0,it has been said that we can drop the date columns,300694.0
124739,545645.0,"Drop data columns, not required",304814.0
124739,545707.0,Date columns is not necessary for analysis simply you can drop all the date columns.,308639.0
124759,543721.0,"yes, more than equal to. It is mentioned in problem statement.",301643.0
124759,543096.0,more than or equal to 70th Percentile X&gt;=70,300694.0
124090,544364.0,TA .. please clarify,319759.0
124090,539247.0,The average of av_rech_amt_data_6 and av_rech_amt_data_7 can be used to find out the high value customers. They are the top 30 percentile.,304319.0
124090,540800.0,av_rech_amt_data_6 and av_rech_amt_data_7.. these 2 fields are for data recharges,318791.0
124090,540945.0,"total_rech_amt should be inclusive of av_rech_amt_data. I dont think we need to consider av_rech_amt_data seperatly. Moreover, av_rech_amt_data is average whereas total_rech_amt is total. TA, Please confirm",304814.0
124090,540965.0,"Yes, even I think we should use total_rech_amt",316255.0
124090,542723.0,"In the same context can TA please suggest if the number of recharges done both for data and calls also need to be used to calulate the average recharge in good phase so as to get the value customers. Else or otherwise please suggest if the average rechage to find the value customer means average monthly recharge. e.g. consider a case June -&gt; 2 times recharge -&gt; recharge values 20 and 30 = total monthly recharge = 50 July -&gt; 3 times recharge -&gt; recharge values 10,30 and 40 = total monthly recharge = 80 the average recharge in this case is (20+30+10+30+40)/(2+3) = 26 and average monthly recharge = (50+80)/2 = 65 Which among above should be taken as the basis for evaluating the value customers?",311729.0
125808,548720.0,"1. Use of incremental PCA depends on how many number of Principal Component you get after performing PCA on entire data. 2. The number of Principal Component (PC) that explain around 90-95% (depends on ones choice) would be the total number of PC after PCA. 3. Before PCA, yes normalization is required for PCA. Hope this helps.",317991.0
125222,546064.0,churn needs to be decided using the *_9 column mentioned in the description. After adding the chrun column based on the field mention *_9 column this columns should be deleted .,318476.0
125222,546124.0,"We have given four month window from 6,7,8,9. The attributes containing 6, 7, 8, 9 as suffixes imply that those correspond to the months 6, 7, 8, 9 respectively. For ex:- vol_2g_mb_9 column contain volume of 2g used in the month of 9(September) The 'Churn' phase is the fourth month i.e. September (9th) month. We have to tag 'Churn' using some attributes mentioned in the problem statement. And once the 'churn' is tagged will have to remove columns containing '_9' in column name. Hope this will help.",317991.0
125811,548782.0,yes,301648.0
125811,548738.0,Yes you can drop as mentioned by TA in one of the similar question. Below is the link: https://learn.upgrad.com/v/course/208/question/125004 https://learn.upgrad.com/v/course/208/question/125724 Hope this helps.,317991.0
125811,548951.0,Yes you can drop the date columns as mention by TA they are not helpful for analysis https://learn.upgrad.com/v/course/208/question/125004,306010.0
125811,548986.0,yes it has been mentioned several times on the forums that we can drop the date columns,300694.0
125811,549036.0,"we can use data column to consider when the customer last recharged ,,,,for customer who churned the gap will be higher",318005.0
122941,534733.0,"Hi, Please follow the link to show all list https://stackoverflow.com/questions/16424493/pandas-setting-no-of-max-rows aa = (100*df.isnull().sum()/df.shape[0]).sort_values(ascending=False) aa[aa !=0]",344894.0
122941,536566.0,"I usually do it like this: df = pd.DataFrame({‘value’:round(100*(df.isnull().sum()/len(df.index)),2)}) print(df.loc[df[‘value’]>0])",315022.0
122941,536982.0,"This is what I follow which gives only non zero % - 100*df.loc[:,df.isnull().any()].isnull().sum()/len(df.index)",304694.0
122941,539004.0,"Use these three commands before df.isnull() cmmnd : pd.set_option('display.max_rows', 1200) pd.set_option('display.max_columns', 1200) pd.set_option('display.width', 1500)",315560.0
122941,539257.0,"You can also use this command : with pd.option_context('display.max_rows', None, 'display.max_columns', None): display(hv_data.isnull().sum())",304319.0
125841,548898.0,"Yes, variety of model needs to be trained on PCA features to predict the churn.",317991.0
125841,548875.0,yes,318429.0
125841,548928.0,I believe no -&gt; we are creating two models: 1. with PCA and something else to predict 2. without PCA -&gt; logistic regression or one of the tree models to find most influential features as others have said 'yes' above .. it would be interesting for me to see what TA has to say,300694.0
125841,549950.0,"Well I Don't think it is if you are using RF for Feature Importance but if it is your Primary Model for Prediction, than Yes, You can. The main reasons of Using RF are to reduce, Multicollinearity(RF are not affected by that) and Reduce the number of Features(Reduces time and Calculation Cost).",317984.0
125846,548901.0,This question has been discussed in detail. You can visit below links for that: https://learn.upgrad.com/v/course/208/question/124433 https://learn.upgrad.com/v/course/208/question/123194 https://learn.upgrad.com/v/course/208/question/124154 Go through the comments also Hope this helps.,317991.0
125846,548902.0,Create the columns like total_rech_amt_call_n_data_6 = total_rech_amt_6 + (total_rech_data_6 * av_rech_amt_data_6) total_rech_amt_call_n_data_7 = total_rech_amt_7 + (total_rech_data_7 * av_rech_amt_data_7) total_rech_amt_call_n_data_8 = total_rech_amt_8 + (total_rech_data_8 * av_rech_amt_data_8) total_rech_amt_call_n_data_9 = total_rech_amt_9 + (total_rech_data_9 * av_rech_amt_data_9) and create an average column from _6 and _7,318329.0
125834,548849.0,This topic has been discussed in the forum Please refer to the following links: https://learn.upgrad.com/v/course/208/question/125020 https://learn.upgrad.com/v/course/208/question/125499,313691.0
125834,548931.0,"most of the modelling functions accept a special paramter to balance the imbalance. For example: class_weight={0:0.1, 1: 0.9}",300694.0
126872,558268.0,"Generally in the case of decision trees, it is simpler to generate the image of the decision tree and then look at the top of the tree for identifying the most important variables. Theoretically, the most important variables are the ones which can demarcate the variables into different sections simultaneously ensuring that the sections are as homogeneous as possible(i.e each section has only one kind of class label). Mathematically, if we are to find the variables that matter the most, then the variables with the highest Gini Index/Information Gain on splitting are the most important ones.",312063.0
126872,552827.0,"Hello Pradnya Websites given above can provide you a way out to find the order of importance features Here comes the theory; If a big split is possible , then that should be the first split. For example if it is 0.5/0.5 then that is the best split.; 0.7/0.3 is a better split than 0.9/0.1 and so on. Whichever attribute gives you the best information gain, that is the first split and subsequent splits folllow in the same way. Increase in information gain results in decrease in entropy.",301121.0
126872,552650.0,You can use ' feature_importances_' of the decision trees to get the important features. It will give you the list of features along with the importance score. Based on that score you can deduce which feaures are the most important. (higher score means more important.) You can refer to the following link for more insight and workings of it. (along with the outputs) https://datascience.stackexchange.com/questions/31406/tree-decisiontree-feature-importances-numbers-correspond-to-how-features https://towardsdatascience.com/running-random-forests-inspect-the-feature-importances-with-this-code-2b00dd72b92e Hope that helps.,317998.0
126872,553920.0,feature_importances_ parameter is way to go for your case. It contains information about all features and respective importance score.,318368.0
125499,547663.0,It is fine to use over/under sampling for balancing dataset. There are other methods as well. You can explore them and use whichever gives best results for you.,301560.0
125499,547636.0,"Hi Rajarshi, Any progress ? Which technique you are following.? We are also facing same issue. So far no clearity has been provided on this issue by TA. I hope TA will respond to this question soon.",317991.0
125499,547707.0,"Not sure of the reason, but I did not perform any explicit resampling (just class_weight='balanced'), but I am getting a pretty decent scores on test data for both LR and RF.",318438.0
125499,547840.0,Do we need to use SMOTE technique ?,302877.0
125860,548982.0,"I guess it should be before train - test split, sampling techniques should be applied. My Reason: We try to maintain 30% test data &amp; if we apply sampling techniques after train test split, then test data will not be equal to 30% of the total data. Test data percentage would either decrease or increase drastically. If this is the case there will high chances that model overfitting might occur",312093.0
125860,549311.0,"Hi, It should be after train test split. It should be done only on train data, not on test data.",344894.0
125871,549001.0,no you should keep them apart as they define consumption in different months and if you merge them you wont get data for good phase and bad phase so dont merge,318017.0
125871,549037.0,we merged some of the columns to create derived variables - but we did not drop the original columns,300694.0
125872,548998.0,Outliers are something which can make your model corrupt by bending on one side for predictions so better remove outliers if you feel they are present and do check box plot for the distribution of data to check outliers but in the case study outliers are not that much and outliers can help you find the good customer behaviour,318017.0
125873,549019.0,We have to build two different model 1. For predict Churn PCA-&gt; LR -- Here no need to perform RFE. 2. For important variable --&gt; LR/ Any Tree Model -- Here you can perform RFE on LR. Hope this helps.,317991.0
125873,549004.0,after pca you can run logistics on the pca data set and get the required set of data set with low p value and low vif,318017.0
125874,549003.0,you sholud use PC and churn column rest colums are not needed but merge the churn column to pca,318017.0
125418,546926.0,"You would need to use MORE than 2 columns. Exempli Gratia, For the month of June you have a column which gives the Total Recharge Amount done by that customer. Then there is another column which gives the avg data recharge done by that customer. another column gives the number of times data recharge has been done. So, if you multiply the last 2 columns you will get total amount of data recharge done by that customer. Now, if you add this to the Total Recharge Amount, you will get the sum of recharge done by for calls and data together. Similarly you need to calculate for July and then take average of these 2 months. Thereafter take the 70th percentile and so on. Hope that helps.",317998.0
125418,546872.0,Please refer to the following links: https://learn.upgrad.com/v/course/208/question/124433 https://learn.upgrad.com/v/course/208/question/124154 This topic is extensively discussed in the above links. Also go through the comments for each answer for a better understanding.,313691.0
125418,547560.0,there are many more than 2 columns. It is mentioned exactly in several posts in the forum,300694.0
127035,554782.0,Few of the articles to have a look at and also to evaluate how much valuable AI is to HealthCare : https://towardsdatascience.com/is-artificial-intelligence-the-future-of-health-analytics-3d54c11e2fe6 https://www.fingent.com/blog/5-ways-big-data-is-changing-the-healthcare-industry,318741.0
127035,557082.0,Decision tree one of the algorithms of machine learning...!!!,305847.0
127035,557952.0,"Yes , machine learning algorithms can predict the conditions of patients in advance,which can give a boon to health care industry... Nowadays many organisations like Fitbit and Apple Watch are doing lot of research and building the monitoring tools like watches which is monitoring our data and in case of any abnormally it can take preventive actions.",311861.0
127035,558198.0,"Analytics and technology could help dramatically cut costly and unnecessary hospitalizations while improving outcomes for patients. For example, by preventing hospitalizations in cases of just two widespread chronic illnesses — heart disease and diabetes — a country can save billions per year. Using the famous Framingham Heart Study , a doctor assesses the patient’s age, cholesterol, weight, blood pressure, and several other factors to arrive at the individual’s chances of developing cardiovascular disease over the next 10 years. Using this study, one can predict hospitalizations with an accuracy of about 56%, which is substantially lower. Using the entirety of a patient’s EHR (which can contain as much as 200 factors) instead of just a few key factors leads to superior prediction results. What’s more, an algorithmic approach can easily be scaled so it can be applied to a very large number of patients — something that is impossible with human monitors only. Based on a study of a year’s worth of hospital admissions, the U.S. Agency for Healthcare Research and Quality (AHRQ) estimated that 4.4 million of those admissions in the United States, totaling $30.8 billion in costs, could have been prevented. Of that $30.8 billion, $9 billion was for patients with heart diseases and $5.8 billion for patients with complications from diabetes. That’s half of all unnecessary hospitalizations. We are on the cusp of major changes in health monitoring and care. Google , Apple and other companies with lots of experience in collecting and learning from data have already stepped into this domain . A myriad of technologies, from implantable medical devices (such as defibrillators and pacemakers) to fit trackers, smart watches, and smartphones already capture our health data and lifestyle choices . Our credit-card and electronic-payment systems know our purchase history and the type of food we consume. The result is the emergence of a rich personal health record we carry in our pockets. If we can now predict future hospitalizations with more than 80% accuracy using medical records alone, imagine what is possible if we can tap into this trove of personal data. Recommender systems could be used to nudge us to adopt healthier eating habits and behaviors. The holy grail of heading off the emergence of conditions by keeping people well could be realized.",300713.0
127035,558739.0,"Yes, it will change the health industry in terms of predictions...i.e. predictive analytics can be used. Not just heart it can also help in finding cancer!",318446.0
127035,559590.0,"The International Congestive Heart Failure study has shown that India alone accounts for 23 % or 2.1 million deaths due to heart ailments annually. Data shows there is only one Cardiac Specialist for every 3 lakh patients in India. Although secondary and tertiary health care providers are engaged in performing ECGs and analyzing heart health conditions, none of these existing solutions can detect and diagnose high risk cases in time. Timely intervention is the need of the hour to reduce the time span between critical heart health to stable heart health. With the use of Data Science Machine Learning solutions on clinical records, we can have the ability to provide improved healthcare solutions at all places and enable on time diagnosis to reduce the number of deaths. Many researchers are leveraging Big Data machine learning algorithms to predict the heart anomalies in a fast, affordable and accurate way without using complex invasive methods. The most common publicly available UCI dataset consisting of 300 records is used for many academic studies. Classification algorithms of Support Vector Machine, Naïve Bayes, Random Forest, Neural Network have shown high risk heart conditions prediction with accuracy levels of 76% or higher. These accuracy levels are reported to be higher than the standard accuracy seen with the Heart Risk Score that cardiac specialists use to diagnose critical heart patients. In India there are medical institutions as well as tech startups who have started building AI-enabled products for diagnosing heart conditions using the power of data science machine learning. Apollo Hospital has partnered with Microsoft to use AI and cloud tech to predict India-specific Heart Risk Score based on Indian patient data derived from lifestyle, diet, and clinical records. The dataset used comprises of 22 parameters collected from 400,000 patients across India. Sigtuple, an India healthtech startup uses MRI scans and ECG images to diagnose the heart health for patients. They are offering their AI-enabled product to cardiac specialists and medical institutions for a faster smarter diagnosis and preventive care. Another Indian startup, iKure works with rural India population to bring the last mile medical support on heart diagnosis using IBM Watsons AI technology. There is immense scope for using Data Science to predict heart conditions for India and other countries. Similarly, Data Science is being extensively used by tech giant’s like Google, Apple, IBM, Microsoft for faster and more accurate diagnosis of other critical illnesses like cancer, diabetes, etc. References for Further Read: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5380334/ https://www.ijrte.org/wp-content/uploads/papers/v7i4s/E2046017519.pdf https://pdfs.semanticscholar.org/5bb8/9958f57a7f0a0efe0c4de76bb19857dae273.pdf https://cio.economictimes.indiatimes.com/news/strategy-and-management/heres-how-apollo-hospitals-is-leveraging-ai-to-predict-heart-disease/63874473 https://www.thehindu.com/sci-tech/ai-beats-doctors-at-predicting-heart-disease-deaths/article24872914.ece https://www.sciencedaily.com/releases/2019/05/190513081412.htm https://www.analyticsindiamag.com/googles-ai-algorithm-predict-heart-disease-scan-eyes/ https://www.theverge.com/2018/2/19/17027902/google-verily-ai-algorithm-eye-scan-heart-disease-cardiovascular-risk",301644.0
127035,553461.0,"yes we can if we have data by ages, and their blood pressure, sex, chest Pain type and cholesterol level using these attributes we can build a model and to determine and predict the likeness of the patient going to suffering an attack",320606.0
127035,553776.0,"Hi, Please go throw the below links for healthcare with data science. https://medium.com/activewizards-machine-learning-company/top-7-data-science-use-cases-in-healthcare-cddfa82fd9e3 https://www.altexsoft.com/blog/datascience/7-ways-data-science-is-reshaping-healthcare/ Thanks",344894.0
126118,549690.0,Just out of interest.. Why is rfe too lengthy.. You can set it to 15 variables and that will minimise the number of manual iterations needed to get to a stable model. Or yeah go for RF but be weary of the hyper parameter tuning required,300694.0
126118,550230.0,Any approch is ok but make sure when u are doing RFE + logistic correlated feature should we elimainted using VIF and other methods,318476.0
125495,547172.0,Mobile number and index are unique columns and do not have any predictive power. You should drop them along with any other feature you determine do not provide any insights into the pattern.,318438.0
125495,547203.0,All the data cleaning must be done before PCA. Since mobile number is not important for analysis you can drop it. So it should be done before PCA.,317991.0
125495,547559.0,we dropped the column,300694.0
125495,548034.0,"yes we can store it in some var and drop,,but as it is defining uniwue customer,we can put it with predictions later",318005.0
125495,548088.0,cirlce_id also we can drop ? not useful?,312019.0
125495,548240.0,Yes we can drop circle id...as all values have same id 109,308638.0
125915,549043.0,GLM doesn't have 'class_weight' parameter but LogisticRegression() does have 'class_weight' parameter. You can use that to handle class imbalance. Or you can use oversampling / undersampling technique. Hope this helps.,317991.0
125915,549164.0,"that is part of the logistic regression part Sudheer -&gt; p-values, etc .. I don't think that is needed .. that is only if you are doing pure LR; if doing PCA+LR then not needed - check the PCA python file given to us during that time of the lectures",300694.0
125777,548461.0,"Hi, No, you do not need to scale the data again after PCA.",344894.0
125813,548971.0,"From around 160-170 original columns, it came down to 84 variables, then that should be a good sign as half of them were reduced just by applying PCA",312093.0
125813,548823.0,I think its normal because I've got 75 variables when I chose 95% as explained variance.,318329.0
125813,548910.0,"Each PC components explain variance of variables. So the number of PC might depend upon the data cleaning you perform i.e. how many relevant variable you keep and how many you dropped. Larger number of column retained may lead to higher number of PC components. And its perfectly fine if you are getting 84 PC. What matter is the evaluation metric i.e. accuracy/sens/spec/precision/recall whatever metric you chose, it should come up to the mark. Hope this helps.",317991.0
125813,548984.0,"it depends on the hyperparameters you choose. If you choose a very low variance representation then less PCs will be need to represent that variance; if you choose a high variance then more PCs will be needed, for example",300694.0
125813,549033.0,"with 90 % var ,it can drop to 55-56 components",318005.0
123194,536631.0,"Hi, 1. calculate average recharge done by customer in June and July (Topup Recharge + Data Recharge ) 2. look at the 70th percentile recharge amount",344894.0
123194,549583.0,"Hi Premnath, You already must have completed the case study but this might help someone struggling on the same logic. Your approach to get the high valued customer is perfectly correct as far as we're concerned with the 29.9K rows but following the TA approach might give us a different set of rows although the same number. I read various discussions on this, the TA's are emphasizing on using both the top-up recharge &amp; data-recharges. Following are my keypoints on both the approaches Filtering top 30% based upon the data+topup would give us 5500 rows i.e. due to huge number of NaNs in the columns involved in the calculation. Workaround for this is to fill the NaN values with 0 for the colums involved in the calculation of total amount spent by the customer. Doing this, would fetch 29.9K rows. Hope this helps someone!",318355.0
124174,539834.0,"Please, refere to the following links : https://learn.upgrad.com/v/course/208/question/124090 https://learn.upgrad.com/v/course/208/question/124154",315560.0
125671,548116.0,Hint is given in the problem statement Evaluate the models using appropriate evaluation metrics. Note that is is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.,301115.0
125671,547979.0,"In my opinion, one should pick an evaluation metric based on the business understanding of the problem statement and then evaluate the model based on it. As we have been taught that ""accuracy"" is not necessarily the best metric to evaluate a model on. Thus, metrics such as precision,sensitivity or specificity could be used based on your understanding as to which is important from business perspective. And for that confusion matrix would certainly help. Hope that helps.",317998.0
125671,549294.0,Hint: Did you chekc for class imbalance. Evaluate the models using appropriate evaluation metrics.,302735.0
126003,549060.0,"Since problem statement doesn't specifically mention about SVM, you can choose to select it or not. If you think that you are getting good evaluation metrics from LR or Tree models then no need. But if you think SVM could perform better then you can build SVM also.",317991.0
126003,549507.0,"As discussed in previous module,in general, we should start with a logistic regression model. Then, we should build a decision tree model. . If we not satisfied with the model performance mentioned so far and we have sufficient time and resources in hand, we can go ahead and build more complex model support vector machines. If evaluation metrics are giving promisable values better not to go with SVM If not we can build using SVM also",308638.0
126003,550301.0,a model was not dictated in the problem statement -&gt; so you are free to choose how to predict churn,300694.0
126003,550556.0,But we need to try out multiple models and compare the metrics and choose the best one.,304319.0
126003,551169.0,We have to try out multiple models and compare the metrics and choose the best one.,305650.0
126344,550624.0,"I would proceed as follows: Step 1: Test Train Split. (70,30) Step 2: Build PCA using the train set. (ie, Fit+Transform) Step 3: Build Logistic Regression Model (or any other model for that matter) using the transformed df of step2. Step 4: Transform Test set to the new dimensions. (Only Transform here, using the PCA built in step 2) Step 5: Use the LR model built in step 3 to make predictions on transformed dataframe made in step 4. Once we get the predictions, we can do anything with them. (merge concat with df etc.) Hope that helps.",317998.0
125287,546610.0,"You can refer to the following link: https://www.dataschool.io/introduction-to-feature-engineering/ If you are still facing problems, do let us know.",420665.0
125289,546361.0,"In the discussion below the TA have given suggestion on how to deal with Class Imbalance : https://learn.upgrad.com/v/course/208/question/125020 For Logistic Regression &gt;&gt; LogisticRegression(class_weight='balanced') Random Forest &gt;&gt; RandomForestClassifier(class_weight={0:0.1, 1: 0.9}) So its safe to assume that you need to do class imbalance after the PCA.",318476.0
125289,547361.0,Imbalance in the dependent variable (y) will not impact PCA as you would run PCA only on the independent variables (X),305653.0
125291,546413.0,"Thanks for raising the question, I have the same doubt",314048.0
125291,546897.0,Take a look at this - https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb,301557.0
125291,547615.0,in my case introducing class_weight significantly improved my model (SVM),300694.0
125206,546388.0,"Hi Ruchita, You should evaluate your results on best Sensitivity, Specificity and AUC score. We are looking all matrics because our business objective is customers at high risk of churn and identify the main indicators of churn. So at the end you also need to explain about the feature coefficient.",344894.0
125206,547480.0,"@Paras, 'Sensitivity/Specificity' and 'Precision/Recall' are different ways of looking at the same thing. Is there any reason why I should consider 'sensitivity/Specificity' over 'precision-recall', which in my opinion is more relatable to the business?",318438.0
125206,547429.0,"Hi, paras. with respect to parameter tuning, since the main Goal is to Predict possible Churners, I am tuning wrt recall score on positive Class in order to get best recall possible while maintaining a decent Accuracy. Am I going in right Direction with the Specified business objective? For evaluation purpose, I will use other metrics as well.",317984.0
125206,547527.0,"specificity and Recall are the same i.e Of all the People who churned how many our Model was successfully able to predict. Thus a high Recall will mean our model is good at predicting Churns which I think is the primary Objective for this Case Study. while Specificity or TNR measures the proportion of Non-Churns(0) that are correctly classified and precision is, of all the P+(1) our model predicted how many are actually true. thus a high precision classifier is capable of flagging the positives at a better rate(which i think we can bargain for a better recall for this case study), and a high specificity classifier is better at detecting the negatives.",317984.0
125303,546506.0,perform the filtering initially so you have a smaller dataset to work with,318438.0
125303,546708.0,"Quoting you, ""Should Data cleaning and EDA be performed on the entire dataset and then be filtered for high value customers"". what would be the point of EDA and cleaning on the entire dataset when you know you will NOT be using the entire dataset? In my opinion, Yes filtering should be done initially so that you have only about 29.9k remaining to do EDA and cleaning. This would be more convenient. Hope that helps.",317998.0
125303,546818.0,Follow the order in rubrics.,301557.0
125303,547613.0,pretty much in the start -&gt; perhaps after you have done data cleansing and imputations but before EDA and modelling ofcourse,300694.0
124316,540960.0,"av means average , therefore av_rech_amt_data_6 = average recharge amount for data in June It is already average, so I dont think you need to divide it again",316255.0
124316,544228.0,av_rech_amt_data_6 means total recharge amount of data in month 6. It cannot be average as then the avg cannot be greater than the max recharge for that month.But in many cases the value is greater than the max recharge. If the total_rech_data is 8 and max-recharge data is 1255 means that 8 times recharge has been done but the recharge value for each recharge can range from 1-1255. av_rech_amt_data (for this eg) is 2553 which is the sum total of all recharges. Not clear why they have used the word av,304319.0
124316,543594.0,"average recharge amount data means total recharge data amount for a month divided by total recharge data, so in our case we have average value and total recharge data and we can calculate the total recharge data amount.",318448.0
125319,546604.0,you can find a variable which can help you do find the percentile of the good phase so try finding that variable rather then adding each variable,318017.0
125319,546726.0,"If according to your business understanding of the problem statement you feel certain columns can be combined into a single column, to create new derived feautres, and as a result the number of feaures reduce, then I dont see a reason why it should not be done. So, Yes it can certainly help in reducing number of features. Just my opinion. :)",317998.0
124215,540151.0,"If we think the date values are significant, then they can be converted to numerical values by extracting the day using the datetime library as the month and year are same in the column.",304319.0
124215,542710.0,We can also calculate the days with the dates of different months and use the same as difference number of days in modelling.,311729.0
124215,545821.0,I dont think date values are significant in this case study as we are more interested in why of the churn instead of when of the churning.,317689.0
126008,,nan,
121405,527599.0,"Hi, No, you need to calculate total recharge amount for June and July which will include both recharge amount and data amount.",344894.0
121405,536986.0,It should be the average of the months June and July rather than just taking total of these 2 months.. Your orignial thinking seems to be correct. TA could you again please verify.,304694.0
126010,549110.0,You can go through below link which explained complete separation error and workaround for this https://blog.minitab.com/blog/starting-out-with-statistical-software/what-is-complete-separation-in-binary-logistic-regression Hope this helps.,317991.0
126105,550077.0,"A good way to estimate the better C to be used is to perform a grid search before the final training. Suppose you are using RBF-SVM, then you have two parameters to tune: C and gamma (the radius of RBF). The method to perform a good grid search is described here: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf Also, you should actually start with a high value at the beginning and then move downwards so you can get the best C value. I believe that a good C value is some what larger than that. Try a two digit number first and move down to a single digit value.",329936.0
125004,545004.0,"Hi, Not useful for analysis, You can drop date columns.",344894.0
125004,545173.0,Thanks Paras,311386.0
125020,545037.0,https://medium.com/coinmonks/smote-and-adasyn-handling-imbalanced-data-set-34f5223e167 This is a good overview of how to handle class imbalance.,318438.0
125020,545120.0,I think if using random forest we can use class weight parameter to balanced for managing imbalance.,306009.0
125020,545651.0,"Hi, For LOgistic Regression &gt;&gt; LogisticRegression(class_weight='balanced') Random Forest &gt;&gt; RandomForestClassifier(class_weight={0:0.1, 1: 0.9}) Similarly, you can do with other algo. Thanks",344894.0
125020,546448.0,"class_weight: balanced, balanced subsample or some self-defined weight, anyone can be picked .. It depends on the Metric you Choose to Evaluate the Model. and accordingly, pass the weights to param_grid in GridSearch",317984.0
125014,544960.0,It depends on the type of the column if it's categorical and non the existing category make sense then you can create a new category like -1 or not defined . In case of int or float column the mean or median value with help.,318476.0
125014,545463.0,"it depends on the data. For example say a very important column has a value of 0 -&gt; one could impute with the mean of the column, or perform a complex calculation, or impute with 0. My personal approach was to impute with 0 because here I took null to mean 0; I did not think it was appropriate to impute with the mean",300694.0
125021,545044.0,"PC1 and PC2 will have the feature weights in descending order. Create a new dataframe to join PC1, PC2.. PC(n) along with your feature list and you should be able to identify the feature that weigh more in each PC.",318438.0
125021,545124.0,you can check wheather how many principal components explain the data with say 90 or 95 percent of variance.,318017.0
125021,546377.0,Draw the scree plot with which you can decide the no. of Principal Components to be taken for 95% variance. With that many PCs you can proceed for modelling.,304319.0
123329,537140.0,"Hi, total_recharge_number(total_rech_num) = This is the count of recharge max_recharge_amount(max_rech_amt) = Max amount of tiopup recharge max_recharge_data (max_rech_data) = Max amount of data Average revenue per user for Jun means for jun month only not previous month. Thanks",344894.0
124411,541022.0,Please refer this - https://learn.upgrad.com/v/course/208/question/124154 .,311502.0
125546,547561.0,"I believe we had to build two different models. One with PCA + say logistic regression -&gt; this would be used to predict churn Then another model would be used (e.g. decision or random forest) to understand which features are the most important. So regarding low accuracy (and the other metrics are also bad) are you sure you have followed all the steps and done things necessary for using PCA (e.g. look at corelation, scaling, etc)?",300694.0
125546,548031.0,"accuracy will be good (above 90) if we follow all steps, data cleaning,standarization,pca",318005.0
125546,548032.0,"for business perspective ,we will hv to choose a proper metric",318005.0
125546,550596.0,"In PCA, you have to choose the no. of components according to the variance you want your model to cover. 25 in this case study is too small a no. For 95% variance no. of PCs should be arnd 60-70. This might be the case of your low metrics.",304319.0
124425,541065.0,"I think, total_tech_amt is only for Voice, There is another column for data services - total_rech_data_6/7/8/9. TA please confirm or correct.",311502.0
124425,541290.0,Please look to the discussion below: https://learn.upgrad.com/v/course/208/question/124154 Its has comments to how to calculate the total recharge amount.Also looks to the comment section which have information on the which columns we should take.,318476.0
125053,545245.0,Data contains same circle 's data. i it can be removed.Also columns which are non relivant and does not containsvariances can be removed.,320689.0
125053,545460.0,our approach was to remvoe columns that had no variance. ie. only had one value for all rows,300694.0
125053,545194.0,'circle_id' contains only one values for entire column so it can be removed. Also the columns which are not relevant or you think doesn't contain variance can be removed.,317991.0
126217,550156.0,"We are analysing the top 20 percentile of the recharges, so not needed in this case.",318438.0
126217,550307.0,it is always good to do outlier analysis as part of your EDA. Whether treatment is required or not depends on the business case and outcome of the outlier analysis. In our case our approach was not to do any treatment but it was important for us to show that we did the analysis.,300694.0
126217,550271.0,"In my opinion, Outliers can affect your model by bending the predictions on one side. Thus it is always better and advisable to observe the presence of outliers through EDA (using Boxplots or other graphs) and treat them accordingly. You can refer to the following TA verified answer for more insight. https://learn.upgrad.com/v/course/208/question/125872 Hope that helps.",317998.0
124154,539586.0,It should be the average of the four columns you have chosen. Imputing data is necessary before your proceed with the calculations,311254.0
124154,540261.0,"Hi, 1. calculate the total data recharge amount for June and July 2. calculate total recharge amount for June and July 3. calculate average recharge done by customer in June and July 4. Calculate the 70th percentile recharge amount",344894.0
124154,540802.0,I think Data recharge is already part of total Recharge amount,318791.0
124154,540925.0,"Hi Paras, Isnt total total_rech_amt inclusive of data and voice?. If that is the case then why do we need to take the av_rech_amt_data into consideration?",304814.0
124154,541799.0,"We got 29,956 records in the high value customers by perfoming this, - (total_rech_amt_6 + total_rech_amt_7)/2 = Average of total recharges for 6th and 7th months(good phase) - 70th percentile of average for all records of ""average total recharge amount for 6th and 7th months"" - then compare the average from step1 &gt;= 70th percentile from step2 - the count output from above is 29,956 records.",316215.0
124154,542090.0,You need to consider Data recharge amount separately too.. as i did some math and concluded that data recharge amount is not part of total recharge amount,318791.0
124154,542411.0,"Based on the input from Paras Mal we should also consider data recharge amount. This can be calculated total_data_amt = number of recharges(total_rech_data) * av_rech_amt_data so that updated steps: - (total_rech_amt_6 + total_rech_amt_7+total_data_amt_6+total_data_amt_7)/4 = Average of total recharges for 6th and 7th months(good phase) - 70th percentile of average for all records of ""average total recharge amount for 6th and 7th months"" - then compare the average from step1 &gt;= 70th percentile from step1",316215.0
124154,542724.0,"In the same context can TA please suggest if the number of recharges done both for data and calls also need to be used to calulate the average recharge in good phase so as to get the value customers. Else or otherwise please suggest if the average rechage to find the value customer means average monthly recharge. e.g. consider a case June -&gt; 2 times recharge -&gt; recharge values 20 and 30 = total monthly recharge = 50 July -&gt; 3 times recharge -&gt; recharge values 10,30 and 40 = total monthly recharge = 80 the average recharge in this case is (20+30+10+30+40)/(2+3) = 26 and average monthly recharge = (50+80)/2 = 65 Which among above should be taken as the basis for evaluating the value customers?",311729.0
124154,544048.0,"From the discussion above, It seems the total_rech_data appears to be the number of data recharges (as the values from 1 to 60 odd ) and the amount, av_rech_amt_data. So as from input from Paras, can we consider the total recharge amount, an average of both total recharge amount ( 6 and 7). and data amount (6 and 7) ( i.e.,e number of recharges(total_rech_data) * av_rech_amt_data). TA please confirm.",311115.0
126050,,nan,
125127,545665.0,"Hi, Please follow the below links https://learn.upgrad.com/v/course/208/question/125020 https://learn.upgrad.com/v/course/208/question/124702",344894.0
126052,550047.0,"I suppose ""X_mju"" is the majority class , create majority class undersampled data frame from training data properly also extract features and labels correctly.",329936.0
125142,545732.0,You can follow below links with TA verified answers for similar type of question https://learn.upgrad.com/v/course/208/question/125020 https://learn.upgrad.com/v/course/208/question/124702,317991.0
125142,546040.0,Thanks for this Case study will choose class weights. But I wanted to know in General. Is using Synthetic Data point really Feasible and helps in improving the Model?,317984.0
125142,546359.0,"Using SMOTE, the train and test AUC_ROC score improves considerably.",304319.0
124433,541793.0,"Hi, 1. total recharge amount for June and July --&gt; call recharge amount (total_rech_amt)+ data recharge amount 2. For call &gt;&gt;&gt; total_rech_amt &amp; for data total_data_rech 3. one average together for both",344894.0
124433,541796.0,"If we use the average of all the four columns . then only 5575 rows are filtered. However, on using total_rech_amt , 30k rows are filtered",311864.0
124433,541841.0,"@Paras, I do not agree with explanation of recharge amount for 'data': 2. For call &gt;&gt;&gt; total_rech_amt &amp; for data total_data_rech If you look at the values for 'total_data_rech_6', it ranges from 1-61, whereas the 'av_rech_amt_data_6' has much higher values. How is that possible? Please review the dataset and dictionary and provide a detailed explanation (along with units) of the terms used.",318438.0
124433,542725.0,"In the same context can TA please suggest if the number of recharges done both for data and calls also need to be used to calulate the average recharge in good phase so as to get the value customers. Else or otherwise please suggest if the average rechage to find the value customer means average monthly recharge. e.g. consider a case June -&gt; 2 times recharge -&gt; recharge values 20 and 30 = total monthly recharge = 50 July -&gt; 3 times recharge -&gt; recharge values 10,30 and 40 = total monthly recharge = 80 the average recharge in this case is (20+30+10+30+40)/(2+3) = 26 and average monthly recharge = (50+80)/2 = 65 Which among above should be taken as the basis for evaluating the value customers?",311729.0
126055,549283.0,"hi, I'm not sure if i got your question corrctly. But the idea is to create some PCA columns, tag those new PCA columns against each observation (row) so that you have the 'Churn' variable and then use this new dataframe for modelling. Basically you need to create a new Dataframe having PCA columns and orginal 'Churn' variable.",311686.0
126055,549326.0,"Hi, You can build pipeline for model building https://dzone.com/articles/how-to-build-a-simple-machine-learning-pipeline pipe_lr = Pipeline([('minmax', MinMaxScaler()),('pca', your_pca_moel), ('lr', LogisticRegression())])",344894.0
125148,545823.0,Impute with zero for such columns there are also many thread discussion on the ssame.,307843.0
125148,546146.0,So are we not dropping the columns with 75% of null values?,300727.0
125148,546157.0,You can drop columns having 75% of null values.,329936.0
126280,550223.0,yes you will get an array only you can convert it to a dataframe and merge it with the original data frame.,318017.0
126280,550253.0,"While performing prediction using 'model.predict (some column)' , it will always gives an array. You need to explicitly convert it into dataframe using pd.DataFrame() method.",317991.0
125724,548096.0,You can drop these columns as mentioned by TA in below link https://learn.upgrad.com/v/course/208/question/125004 Hope this will help.,317991.0
125724,548291.0,"If you can use them to generate some new ""derived features"", then well and good. Otherwise, I dont think it is of any use for analysis purpose and thus could be dropped. hope that helps.",317998.0
126066,558311.0,"PCA is a dmensionality reduction technique which seeks to reduce the number of features from a large pool of features so that the model is simplified and also issues like multicollinearity are solved. After PCA, it is generally advisable to create a Scree plot which would give us the ideal number of variables(PCs) which would explain a certain amount of variance(say 90%). PCA creates PCs(Principle Components), each of which is a linear combination of the original feature variables . If a dataset has p features, each PC is a p dimensional vector. So, identifying the exact original features can get difficult. But we can get a sense of things once we use explained_variance_ratio since that will give us the importance given to each original feature by each PC.",312063.0
126066,549358.0,Make a dataframe of all the PCs n the churn variable and use this DF for modelling Refer https://learn.upgrad.com/v/course/208/question/125021,310009.0
126066,549357.0,there are ways to do this - but the question is .. do you NEED to do this? the PCA part is only used for prediction and not for determining which features are important .. so it is not necessary to map back the important features from the PCA part,300694.0
126066,549496.0,Once PCA is done proceed building random forest model . Identifying features is difficult after PCA You have to build logistic regression model without PCA or tree model to identify the features,308638.0
126066,549956.0,"PCA is dimensionality reduction, i.e. describing relationships in your data using fewer dimensions than are actually present. The number of components can be decided on the Explained Variance % metric. Ideally we should have higher explained variance % with as minimal components as possible. calculate the explained variance for each attribute , and plot the result. Decide a threshold of explained variance and pick the Number of Components accordingly",317984.0
126065,549288.0,I also used the same condition and got the Churn indicator as 1 where all the 4 columns have zero value. I guess the output matches with the request.,314197.0
126065,549444.0,All the four columns should be zero for the person to churn.,313691.0
126065,549360.0,I believe it is all OR so you basically sum all those columns and if sum =0 then they have churned,300694.0
126065,549449.0,It should be logical AND for all four attributes. ( total_ic_mou_9 == 0 &amp; total_og_mou_9 == 0) &amp; ( vol_2g_mb_9 == 0 &amp; vol_3g_mb_9 == 0),318438.0
124576,542305.0,"Hi Minerva, I don't know whether I can share the screenshots of the data here but following are my observations from the dataset: total_rech_data -&gt; The number of recharges done for data total_rech_num -&gt; The number of recharges done for both data &amp; non-data We can multiply only the average data amount with the total_rech_data to get the total amount spend on data recharges. Hope this helps!",318355.0
124576,542714.0,In the same context can TA please suggest if the number of recharges done both for data and calls need to be used to calulate the average recharge in good phase so as to get the value customers. Else or otherwise please suggest if the average rechage to find the value customer means average monthly recharge.,311729.0
124477,541377.0,we have to model based on the first 3 months. The behaviour of the customer in the first 3 months will determine the high risk customer.,304319.0
124588,542064.0,"Since you have followed the steps as per TA instructions, You are on right path. Meanwhile for number of rows concern - it depends on 70th percentile you get. If you take round off value of 70th percentile you will get approx 29990 rows. I don't think there should be any strict criteria that we should only get 29900 rows. 100-200 rows more or less could be fine. For this TA can suggest.",317991.0
124588,546251.0,Hi Shijesh Please sugest how you have arrived at the 30019 number as the explantion i snot clear in problem statement,300725.0
124594,542091.0,"Of-Course volume based cost is stong attribute to gurantee for those customers using this with high volume are high rated customer category, those customer which is using more than avg high based cost likely to very less chance to get churn and also you can verify using the data set there score for the attributes which leads to actually churn is very less.",307843.0
124594,542204.0,"No, Please follow the below question https://learn.upgrad.com/v/course/208/question/124154/answer/540261/comment/122923",344894.0
124631,542073.0,Since dataset cannot be changed even if it has skewness therefore as taught we can randomly swap range of rows to avoid skewness in the data set this will also help for training and test dataset as after swapping of rows like taking any random set of rows and swap and then prepare the training and test data this helps to balance the baisness of the data present if any.,307843.0
124631,545814.0,This needs to be handled implicitly using techniques which can handle class imbalance. You can use class Weight parameter in different algos like random forest etc. to get this tuned.,317689.0
124631,542103.0,"We cannot avoid the skewness or more appropriate word is class imbalance in dataset. We can handle this class imbalance, there are various techniques for it. And same is mentioned in the problem description to You can find many such technique in below link https://datascience.stackexchange.com/questions/11404/python-handling-imbalance-classes-in-python-machine-learning Hope this will help.",317991.0
125196,545956.0,Probably not. Some good insight into ARPU: https://blog.hubspot.com/service/arpu,318438.0
125196,546068.0,the case study is based on figure out high value customer and there churn. once you filler out the high value customer you might not find negative revenue for this customer,318476.0
123645,537946.0,I think yes. Problem statement mentions two models and then it says prepaid is used in Asia/India and this is a case study for Asia/india. Pretty clear to me :),313515.0
123645,538017.0,"Correct.. there is data of only prepaid subs in the dataset... there is no specific flag though.. but there no fields related to rental, monthly bill etc",318791.0
123645,537981.0,"yes, it contains only prepaid customers. If there were postpaid customers, we'd have a column monthly rental for those customers which is not there in the dataset. Also remember, postpaid and prepaid customers are treated separately as Postpaid customers are the major and regular revenue generating customers for a telecom company.",307176.0
125759,548314.0,"Imputing the column having more than 60% null values might not be a good idea, because it may make your analysis and prediction biased. So you can drop those columns.",317991.0
125759,548342.0,Rather than dropping you can impute with zero value. As in this data set value zero is as equal as not using the service. Dropping might lead to data loss,312093.0
125759,548844.0,"imputing doesn't make sense , if column having no variance to present doesn't make sense to have it better drop it",311386.0
125759,549039.0,"if you have such high null values, then yes dropping might be best solution",300694.0
125759,549292.0,"True imputing columns with higher number of null values dosn't make sense. But fb_user and one more can be used as categorical variable. It's either 1 or zero. You can create a new class for null values and use thes three classes for in your analysis By deleting null values, a lot of information can get removed. But thats what I think. Please refer: https://stackoverflow.com/questions/34095777/handling-unassigned-null-values-of-categorical-features-in-regression-machine",302735.0
125759,549561.0,"We should not impute even with 0 because even if 0 means the person is not using that particular service but in actual it might be the case that the person is using the service. So, imputing with 0 would make your dataset different from the actual but unrecorded value(just as nulls here). Rather than making the dataset incorrect, dropping the columns with high null values would keep us on the safer side.",318355.0
126337,550257.0,"1. For your question - is tuning is required ? Yes tuning of hyperparameter is definately required. And it is clearly mentioned in the problem description and evaluation rubrics. 2. Why we perform tuning In machine learning , hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned. Source: https://en.wikipedia.org/wiki/Hyperparameter_optimization Hope this helps.",317991.0
126337,550278.0,"Yes, Hyperparameter tuning is certainly required and necessary while building and machine learning model. Why? because, as its name suggest, we fine TUNE the model to get generate the Optimum Model. For example, in Decision Trees you fine tune the 'Max Depth','Minimum Sample Split' etc parameters. ie, based on Cross validation results you graphically see what could be the opitimum values for these parameters such that your Model DOES NOT OVERFIT and Performs Decently well on the Test Set. Therefore, you fine TUNE these parameters to generate the optimum model and thus Hyperparameter tuning becomes necessary to build a good model. Hope that helps.",317998.0
126337,550302.0,"tuning is definiltey required .. that is DS 101 :-) we always perform tuning because a model is built initially using some default values (C, gamma, RFE features, etc) -&gt; we need to use the business drivers and our particular dataset feature space to best tune the model to apply hyperparameters that work best for us just by drawing one of many graphs (scree, accuracy/specificity/sensitivity curve) we can see how different hyperparameters are giving us slightly different outputs",300694.0
124647,542228.0,"Hi Anuj, It is okay. You can proceed ahead.",344894.0
124647,545481.0,we also got 30k - I guess 29.9k or 30k doesn't matter as there are big fish to fry (I hope),300694.0
125763,548429.0,"As we need to focus on correctly identifying the Churn customers, so we should consider recall/sensitivity or AUC rather than focusing on increasing the overall accuracy of the model.",304815.0
125763,549038.0,"as said above, accuracy might not be the best scoring; better would be sensitivity, specificity, or something else based on your team's metrics for evaluation",300694.0
125766,548355.0,You can go ahead with 30K rows. Try to use round() while calculating the 70% percentile. This will gives you 29.9K rows.,311502.0
125766,548354.0,There is no strict criteria to get exactly 29.9k high-value customers. It depends on the 70th percentile. Similar kind of question has been answered and verified by TA. Below are the links: https://learn.upgrad.com/v/course/208/question/124588 https://learn.upgrad.com/v/course/208/question/124759 https://learn.upgrad.com/v/course/208/question/124647 https://learn.upgrad.com/v/course/208/question/124588 Hope this helps.,317991.0
125766,548388.0,You can continue with 30K rows .,301648.0
140089,605938.0,Interest calculation may vary from bank to bank and there are banks who charge interest from purchase date or from statement date. You are right that interest charges will reflect in the next month statement.,301560.0
140589,608238.0,1st formula is for simple interest and 2nd is for compound interest.,301560.0
140091,604739.0,they are using formula for compound interest calculation. The calculation through which you got 0.09 is simple interest calculation,317996.0
140133,605709.0,"Net credit margin is net interest income minus net credit losses, as a percentage of average managed outstanding balances.So it's the cost incurred if the loss is more.",318476.0
140133,605855.0,"That's correct. Even I dont think it should be part of cost. Net Credit Margin is (difference in Interest earned and Credit losses)/outstanding loan balance. It is nothing but (Revenue - Cost) in percentage or dollar term. I will call it as net profit. (neither revenue, nor cost)",318458.0
140153,605963.0,You can check investopedia. They have good explanation for all financial terms which are easy to understand. There are other resources available online as well.,301560.0
142216,,nan,
139482,601502.0,You can read the following article on fake review: https://ecommerce4im.com/fake-reviews-might-not-only-be-unethical-they-may-prove-illegal/,300713.0
138495,597904.0,"Hi Naga, I believe the course name is ""Domain Elective"" and it will have total of around 5 modules. At the moment, there is only 1 module (""Introduction to E-commerce"") added (more modules will be added as was mentioned in the first video). The module """"Introduction to E-commerce"" has only 2 sessions which are just introductory and hence not too long and can be finished in about 1 hr. More modules will be there after this and capstone project would be post completion of those modules. Hope that helps.",317998.0
138495,600313.0,"Hi Naga, There are 4 modules in 'Domain Elective' for course-6.First module is ' Introduction to E-commerce"". It s published right now only. This week, we 've to complete this module only. Rest of the modules will publish gradually. Hopt ,this information will help you.",320689.0
138416,597477.0,"You cannot just reduce warehouse space as the warehouses are rented/ purchased based on the square footage. Possible solutions can be either reducing the area to be used inside the warehouse and use the remaining area for other purposes. Decreasing space can actually mean keepking a lower inventory which can be managed by reducing the number of employees working there. Other than this one practical solution can instead of having one giant high capacity warehouse, they have have a few warehouses of small capacity and additional labour can be hired on the basis of seasonality.",317689.0
138416,599363.0,"Hi Utkarsh, Warehouse spaces are mostly rental spaces taken by logistics at strategic locations within a region. However, some companies do own their warehouse, but this becomes less flexible to change. Initially, an e-commerce company may start selling goods and predict the size of the warehouse needed to ensure timely delivery of products. During this stage, the company may vary its warehouse size to fit the need. If the warehouse is owned by themselves only, they might vary the amount of inventory accordingly. However, in such a case, the extra available space remains un-utilized and thus is not the most optimized way. Thus, if the companies are having the option of altering the size of the warehouse based on the predicted demands (incorporating things like seasonality, etc), this would overall optimize their business. An example could be the post-Diwali season since most of the sales take place during the festive season, e-commerce companies do increase warehouse size to accommodate inventory for Diwali, but the same must be reduced post-Diwali since sales generally drop.",317987.0
138585,602158.0,"Yes, there are only two sessions. Capstone project will start after we complete the modules.",318770.0
138585,597902.0,"Hi Arpit, I believe the course name is ""Domain Elective"" and it will have total of around 5 modules. At the moment, there is only 1 module (""Introduction to E-commerce"") added (more modules will be added as was mentioned in the first video). The module """"Introduction to E-commerce"" has only 2 sessions which are just introductory and hence not too long and can be finished in about 1 hr. More modules will be there after this and capstone project would be post completion of those modules. Hope that helps.",317998.0
139110,600809.0,I can see it now e-commerce module,317982.0
139110,600328.0,"Hi Chandan, Even I had received that email, so you are not the only one. I believe module 2 will be released in a couple of days and then we can proceed to it. If it does not, then it will surely be released by Monday (15th July) as they always do and then we can proceed on completing it by 22nd July. Hope that helps.",317998.0
139110,602155.0,It has been published and visible now.,318770.0
140856,609082.0,Test and train will have same indexes as you have formatted them into the pivot format.,329936.0
140856,609053.0,Train test split is done before pivoting the data. one User_id has multiple entries in the table before pivoting. Hence the same user might come in both train and test while splitting.,310467.0
119208,516936.0,"Hi, Please follow the below link http://joelcarlson.github.io/2016/05/14/RvsPython-GridSearch/",344894.0
118738,513502.0,"Hi, It will not work well, because using PCA you are transforming the m-features to 2 PCA component, which contain the m-features variance. Based on that you can not say how original data was separated.",344894.0
120557,522625.0,"It depends on the data. For example, if the data set can be mostly(not entirely ) classified by linear hyper plane and have a partial non-linearity then the accuracy in linear model will be as high as 90% and under non-linear it could be low like 11%. When it is highly non-linear, then Kernel trick or RBF will come in to play giving rise to more accuracy in non-linear models. I hope this clarifies.",301121.0
120557,523133.0,The case seems theoretically possible but if you use it in a model it is highly improbable that such a value will arise. Anything linear can also be explained with a non-linear model albeit with a lesser accuracy. For eg: 90 linear and 88 non-linear seems likely more than 90 linear and 11 non-linear.,318335.0
120557,523689.0,It is possible when the non linear model overfits the train data and performs badly on the test data.,304319.0
119544,518363.0,"You design a model for a specific business case, e.g. digit recogniztion, sales prediction, house price prediction. Model is trained on historical data, which provides with a picture of how these systems behave in real world. Any future data would be from the same systems and include data points/features that the model already knows. The values of these features can vary and that what the model accuracy is all about. Typicaly the historical data will give you insights into how the model should be designed, linear or non-linear. If for some reason, your future state completely changes, your model will be rendered useless, and that's where you will need to re-evalluate and re-train your model. Not sure if that answers your question, but the model training is based on historical data, where you will consider all the hyperparameters available at your disposal (specific to model), get the best performance (accuracy, roc/auc etc). and keep monitoring the model once it's deployed",318438.0
119544,521036.0,"Just to add to what Ashish hs said, we try to keep the model as simple as possible so that the model is generic and does not overfit to the train data. A generic model has higher chances of getting a better accuracy on the future data.",304319.0
120010,520638.0,For multiclass classification one vs all(rest) method is used by algorithms. below link can give you some more insight into this methodology. http://mlwiki.org/index.php/One-vs-All_Classification,320073.0
120010,521345.0,Good question! I was also thinking the same. May be because we have not covered any multi class classification methods yet.,318585.0
120010,527062.0,"Hi, the question doesn't have a correct answer yet. Can the TA look into it please? thanks in advance",305839.0
119290,558831.0,Please mark this as an issue.,301912.0
119600,518197.0,"Due to number of iterations, it does take fairly longer time than others but it does complete. It must be fine.",301121.0
119600,518550.0,I am in the same boat and can confirm in took a little under 20 minutes for me,315022.0
119600,518223.0,"for me it took 20 mins, and this is what I did, so yeah it will eventually finish :-)",304814.0
119326,517991.0,"Hi, It looks like overfitting. Try with rbf kernel, also change the value of gamma.",344894.0
119333,515841.0,"For a 2/3 dimensional case, you could plot the features to get a feel of spread. But this is not very practical in real life problems, where the features could be more. Your best bet would be to run a cross-validation test with different kernels and see which one produces the best model. https://stats.stackexchange.com/questions/18030/how-to-select-kernel-for-svm",318438.0
119333,516190.0,"Please go through the ""Example of a linear regression model"" and ""Example of a nonlinear regression model""and ""Comparing the Regression Models and Making a Choice"" in the attached link: https://statisticsbyjim.com/regression/choose-linear-nonlinear-regression/ May be helpful.",311117.0
119305,515657.0,"Hi Prateek, The below link is explained in a simpler language. https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine",308673.0
119305,516293.0,"C is for misclassfication and gamma is for non linearity ,,,high gamma means more non linearity and low C( python) means more misclassification",318005.0
119305,515692.0,"Gamma controls the non-linearity of the model. The higher the values of gamma, the more non-linear the model comes out to be. Whereas C in python implementation denotes the penalty imposed to misclassifications. The higher the value of C, the less the misclassifications. Less value of C means, that your model is fine with more misclassifications. Therefore, higher the values of C and Gamma, the more complex the model is and may lead to overfitting.",304319.0
119305,516477.0,"Gamma controls the amount of non-linearity in the model which means as gamma increases,the model becomes more non linear and complexity increases. This leads to overfitting. C controls the misclassifications. C explained in theory is linked with regularizing. It is explained below If C value is high, model is simple which can lead to high number of misclassifications. In other words it would be High Bias Low Variance. If C value is low or close to zero, low number of misclassification is observed since the model will be complex.In other words it would be Low Bias High Variance C in Python is inverse to C explained in theory. C in Python is linked with error term If C value is high, there is low number of misclassification. In other words it would be Low Bias High Variance. If C value is low or close to zero, high number of misclassification is observed .In other words it would be High Bias Low Variance",311254.0
119305,515666.0,"Good Read: https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html From: https://chrisalbon.com/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/ Gamma gamma is a parameter of the RBF kernel and can be thought of as the ‘spread’ of the kernel and therefore the decision region. When gamma is low, the ‘curve’ of the decision boundary is very low and thus the decision region is very broad. When gamma is high, the ‘curve’ of the decision boundary is high, which creates islands of decision-boundaries around data points. We will see this very clearly below. C C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance).",318438.0
119057,514138.0,"In GridSearchCV , cv is cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a (Stratified)KFold , - CV splitter - An iterable yielding (train, test) splits as arrays of indices. Source:- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html Now, cv = folds where folds = KFold(n_splits = 5, shuffle = True, random_state = 4) and cv = n_folds where n_folds = 1 are two different ways to generate value for CV. Hope this will help.",317991.0
119292,515708.0,"Based on the EDA and experience, we might get some hint on which kind of model is required. But for beginers like us, I feel It is always best to try all linear, poly and RBF kernels and check the accuracy. Then the hyperparameters can be tuned on the kernel which gives the best accuracy as explained in the Letter Recognition notebook provided with the lectures.",304319.0
119292,516298.0,"we first try with linear model and then with non linear ,if accuracy is not much improved ,linear model will do the job",318005.0
119292,515679.0,"Q) Is it right to say Kernel RBF should be used for multi-class classification always if SVM is used ? A) Not exactly. Choice of kernel should depend on the complexity of data pattern. You can have linearly separable multi-class data or inseparable binary class data. The end goal of the SVM algorithm is to segregate the data cleanly. If linear separation is not possible, we try to project it to higher dimension to make this happen. This projection is called the kernel trick, which can be RBF, sigmoid, poly etc. Q) How does one Vs All approach work Let's say you have 10 classes, while One vs all will do binary classification like class1 is one class and all other classes will be assumed as other class For more details please follow the below link https://courses.media.mit.edu/2006fall/mas622j/Projects/aisen-project/",318438.0
119323,515780.0,"For 2 and 3 dimensional data, you can plot the decision bondary. See: https://scikit-learn.org/stable/auto_examples/svm/plot_iris.html https://chrisalbon.com/machine_learning/support_vector_machines/plot_support_vector_classifier_hyperplane/",318438.0
119052,514101.0,"Hi, Please use below code import pandas as pd import numpy as np import matplotlib.pyplot as plt x=np.random.uniform(low=10, high=35, size=(200,)) + np.random.normal(loc=2, scale=0.5, size=200) y = np.sqrt(400 - (x-20)**2)+ np.random.normal(loc=20, scale=0.5, size=200) plt.scatter(x,y,color='green') plt.figure(figsize=(20,20)) p = (x-5)**2 q = (y-5)**2 plt.scatter(p,q,color='red')",344894.0
118662,512091.0,"I guess the choice of kernel also becomes a hyperparameter, something that you choose based on either your past experiene, or problem at hand. You can use GridSearchCV to pass a list of kernels along with other hyperparameters (like C and gamma) and find the best combination of kernel/C/Gamma that suits your problem (accuracy/precision/recall etc).",318438.0
118662,512165.0,"Agreed with Ashish. We can pass various kernel Parameters in GridSearch alongwith crossValidation and than pick the best parameter depending on our Classification metric, CrossValidation Mean score. if two kernel Parameters(Gamma here) have similar Scores than between those, we can follow OCCAM's Razor and pick the one which is more simple and less computationally expensive.",317984.0
118662,513026.0,"it can be done by trying non-linear model and plotting a graph and based on the accuracy, we can decide if non-linear is reqd or linear is must",318358.0
120276,521713.0,"gamma determines the polynomial quality of the hyperplane that is getting generated by the model. a low gamma corresponds to straight line, as you increase the gamma, the polynomial curves gets introduced and more pronounced in the hyperplane. that is why as you increase gamma, the nonlinearity and hence, complexity of the model increases as well.",305839.0
120682,523509.0,"It would be a good idea to do barplot 'label' against some of the fields 784 field just to get a feel of the data. It is nothing but to get some kind of visualization how some columns are with respect to label. Probably from the point of view of image processeing, it may throw some light, I presume. Image processing is not part of the assignment though. Heatmap does not make any sense in this case.",301121.0
119643,519509.0,"While maximising the margin, the mathematical formulation requires two major constraints one of which summation of squares of coefficients is equal to 1. That is due to standardization! The standardisation of coefficients such that the summation of the square of the coefficients of all the attributes is equal to 1.",301121.0
119360,516733.0,"Like in 2d it is line(1D), For 3D it is a plane(2D), in the same way for n dimensions hyperplanes should be (n-1)D,",316041.0
119360,516000.0,"The logic goes likes this. For 2D data, the hyperplane is a straight line that separates 2 classes which is basically 1D. For 3D data, the hyperplane is a ""plane"" which is 2D that can separate 2 classes. With the same logic, for nD data, the hyperplane should be of n-1 dimensions.",301121.0
119360,517050.0,for 2D we can calculate (no.of features-1) then it becomes 1D. for 3D we can calculate (no.of features-1) then it becomes 2D,306996.0
119361,515995.0,"Point (2,1) is at the bottom left of the graph. Actually you do not see the origin and probably that's why you missed that.",301121.0
119361,516440.0,"The point enclsed in red circle is the point (2,1) Hope this help",317991.0
119361,517353.0,"Bro you know the closes point to the hyperplane i.e. (2,1)and also the coordinates of the point on hyperplabe corresponding to the closes point. just find the cartesian distance and you'll get the distance",302735.0
118910,513563.0,"yes. In pre-processing you need to use dummies of the categorical features for SVM to work. Check the below link for more details, implementation and analysis of the same https://stats.libretexts.org/Bookshelves/Advanced_Statistics_Computing/RTG%3A_Classification_Methods/4%3A_Numerical_Experiments_and_Real_Data_Analysis/Preprocessing_of_categorical_predictors_in_SVM%2C_KNN_and_KDC_(contributed_by_Xi_Cheng)",305839.0
118910,513879.0,"Yes,you need to convert categorical variables to numeric form.This can be done through dummy variable creation.And then those variables can be used in SVM since they would be in numeric form.",311254.0
118910,515633.0,"Yes, create dummy variables for categorical and you can use them in SVM.",318368.0
118910,516862.0,"yes,you need to convert categorical variable to numeric form.so this can be done by dummy variable creation.so those variable can be used in svm because there would be numeric form",306996.0
118551,511320.0,"It looks correct to me. Points above the line (class1) will be positive and below the line (class 2) will be negative. If you are comparing it against the previous example, the slope of the line in the previous exampel is negative where as it is positive in the question.",310467.0
118551,511321.0,"Let's consider the equation W1*X1-X2+W0 = 0 When Y=1, then W1*X1-X2+W0 &gt; 0 When Y=-1, then W1*X1-X2+W0 &lt; 0 In both the cases, (W1*X1-X2+W0) * Y will be positive always. I do not see a problem.",318329.0
118551,511323.0,"The origin(0,0) should be classified as class 1. if you put the x1 and x2 in the euqtion it would be less than zero(w0 being negative). Now Y label is 1. So the product is negative.",311857.0
118551,511660.0,Option B represents correct value as component ​(b−w1a−w0) &gt; 0 and component (q−w1p−w0) &lt; 0 . So their product should be -ve,317156.0
118852,513050.0,What is the need for multiplication?,320687.0
118852,513038.0,Suppose C1 class is negative and C2 is positive. then (b−w1a−w0)&lt;0 (negative) and (q−w1p−w0)&gt;0 (positive) negative* positive is always negative. Hence (b−w1a−w0)∗(q−w1p−w0)&lt;0,310467.0
118852,515107.0,"Hi Deepesh, the need for multiplication can be clearly seen on the next page of this session, when we formulate the dot product. Hope it helps!",318355.0
119022,513992.0,https://learn.upgrad.com/v/course/208/question/118893,318438.0
119022,514020.0,you can use a dot product of the point with the best fit line.,318017.0
119022,514190.0,"The margin is the distance of the closest point from the hyperplane. If you know the point [x,y], then just do a dot product with the normalized equation of the hyperplane. [a,b], which is nothing but simple addition of the product. a*x + by .",315028.0
119022,515012.0,the dot product of the transpose of point vectors and the vector of hyperplane coffients,306248.0
119022,517354.0,"Bro you know the closes point to the hyperplane i.e. (2,1)and also the coordinates of the point on hyperplabe corresponding to the closes point. just find the cartesian distance and you'll get the distance. for cartesian distance you can refer to http://mathsfirst.massey.ac.nz/Algebra/PythagorasTheorem/pythapp.htm",302735.0
118808,512999.0,"This was explained by the TA. If the sum of the square of the coefficients is 1, the formula for the distance of a point from the hyperplane would come down to just the dot product of the coefficient vector and the data point which makes it easy for interpreting the cost function.",310974.0
118808,513170.0,"this basically ensures that the vector is an unit vector, ie, a vector when scaled up or down (multiplied by a constant) DO NOT change it's direction, hence the distance calculation becomes easier with the denominator of the distance formula is reduced to 1",305839.0
118808,525524.0,"a unit vector should be of unit length i.e if a vector v is a unit vector, then ||v|| = 1. if not, to find a unit vector g along the vector v , we divide the coffecients of v by ||v||. For example to make 1x +2y +3 = 0. vector a unit vector in eucliedean norm , you would divide the coffecients by sqrt(1+4+9) = sqrt(13) let sqrt(13) be denoted as k Now the coffecients are 1/k*x + 2/k*y + 3/k = 0, which is a unit vector., because (1/k)^2 + (2/k)^2 +(3/k)^2 = 1",318019.0
121042,526343.0,Thanks Rajat for the help,305650.0
121042,526071.0,"there is small correction, in this code, before this code block there is function center_extent, modify below code as int. offsetX = int((eW - image.shape[1]) / 2) offsetY = int((eH - image.shape[0]) / 2)",318429.0
118663,512038.0,"I think for Logistic regression, we have the cost function log of odds represented as a sigmoid curve (maximum likelihood)) is continuous and also represented as a linear combination, hence it is a regression. Though it is being continuous, based on probability we classify them binary, based on certain optimum value ( based sensitivity, specificity, etc,) it is classification.",311115.0
118663,512032.0,"Yeah this makes sense, but how different it is from clustering? In this case we just want to have two clusters; aren't we?",304814.0
118663,512020.0,"Logistic regression is used for classification, but model produces output - probability which is continuous numeric value Hence it is also a regression method. The probability is used to decide binary type using logit function. Hence it is a classification method. Even once you read SVM, you will see its output is continuous numeric values ( hyperplane). So SVM is also used for regression.",318458.0
118663,512732.0,"In logistic regression module we get the probability values of likelihood. Based on that probability values we set up threshold values using accuracy, precision ROC etc to define the classification. So, this classification might be binary but underlying model is doing regression.",317689.0
118663,512896.0,"In logistic regression, we are finding the probability of the outcome. Further, the log of outcome is a linear equation of the attributes. Hence, it is usually referred to as generalised linear model. Checkout the wikipedia page for more details : https://en.wikipedia.org/wiki/Logistic_regression Once we have the probabilities of each of the data points, we then use various techniques to derive the optimal probability for classifying the outcome as 1 or 0 ( Spam/Ham).",313826.0
119096,514459.0,"Hi Ashish, I feel this could probably help you: https://stats.stackexchange.com/questions/353552/support-vector-machine-calculate-w-by-hand",318355.0
119096,514587.0,"I have read this article, but even this determines the initial weights by visual inspection. My question is given a set of points and respective labels, how can I calculate W and b",318438.0
119096,516852.0,"Hi, Please go through the below links https://www.svm-tutorial.com/2015/06/svm-understanding-math-part-3/",344894.0
118717,,nan,
119239,515245.0,Suppose equation of line is 3x+4y=0 sqrt(3*3+4*4)=5 Normalised form of line 3/5 x+4/5 y=0 sum of square of coefficients= 3/5*3/5 + 4/5*4/5 =1,310467.0
119239,515983.0,x + y + 1 = 0 Normalized form of the line - x/√3 + y/√3 + 1/√3 = 0 Sum of square of coffecients including the constant = 1/3 + 1/3 + 1/3 = 1,314730.0
119239,516296.0,"because summation of sqauares of unit vectors is 1 according to vector mechanics,,,,we had normalized coefficients by dividing each coeffients with square root of summation of squares of coefficients",318005.0
118897,513222.0,I have the same doubt. I think for Y=-1 product should be negative.,320687.0
118897,513537.0,Why are we multiplying Y and the equation in the first place?,318791.0
118897,513664.0,"Hi Jayashree, If the point lies below the line, the expression will be negative so will be the label (Y ), So the product will be positive. If the point lies above the line the expression will be positive for the line so will be the label. If the point lies on the line, the expression will be zero. Hence, the product will always be positive or zero no matter where the point lies.",315028.0
118897,517008.0,if the point lies below the line then that expression will be negative. so the product will be positive if the point lies above the line then that expression will be positive,306996.0
120879,524713.0,"hi, hope you are using only 3 values for both the hyperparmeters. you can also try passing n_jobs= -1 in your GridSearchCV command as a parameter. if even this doesn't help then using Colab/Kaggle platform instead of local machine can be of help.",311686.0
120879,524876.0,"Hi Parna, Try using all cores of your CPU and reduce the processing time. Using concurrent.futures module you can significantly improve the processing time. Check the article below for reference. https://towardsdatascience.com/heres-how-you-can-get-a-2-6x-speed-up-on-your-data-pre-processing-with-python-847887e63be5",344353.0
120879,525186.0,Thanks ...it finally worked took close to 50minutes but worked.:-),301644.0
118893,513203.0,"Hi Rambabu, The below links have good explanation on the topic. https://www.toppr.com/guides/maths/three-dimensional-geometry/distance-of-a-point-from-a-plane/",308673.0
118893,514191.0,"The margin is the distance of the closest point from the hyperplane. If you know the point [x,y], then just do a dot product with the normalized equation of the hyperplane. [a,b], which is nothing but simple addition of the product. a*x + by .",315028.0
118893,517357.0,"Bro you know the closes point to the hyperplane i.e. (2,1)and also the coordinates of the point on hyperplabe corresponding to the closes point. just find the cartesian distance and you'll get the distance. for cartesian distance you can refer to http://mathsfirst.massey.ac.nz/Algebra/PythagorasTheorem/pythapp.htm",302735.0
138141,596955.0,"Hi, Please follow the below discussion https://www.reddit.com/r/MachineLearning/comments/6j8n86/d_why_is_svm_memory_efficient/",344894.0
138141,599368.0,"As far as I could gather from various sources, SVM will have 2 components i.e. kernel and the classification algorithm. The process of conversion of multiple features to higher dimensions and thus making them linearly separable is a CPU as well as a memory-intensive process. We would require to store a lot of feature vectors (the transformed ones).",317987.0
119495,517152.0,"In machine learning, a “kernel” is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem. It helps in transforming linearly inseparable data to linearly separable ones . The kernel function is what is applied on each data instance to map the original non-linear observations into a higher-dimensional space in which they become separable.",311254.0
119495,517323.0,"Kernel is a way of computing the dot product of two vectors xx and yy in some (possibly very high dimensional) feature space, which is why kernel functions are sometimes called ""generalized dot product"". Suppose we have a mapping φ:Rn→Rmφ:Rn→Rm that brings our vectors in RnRn to some feature space RmRm. Then the dot product of xx and yy in this space is φ(x)Tφ(y)φ(x)Tφ(y). A kernel is a function kk that corresponds to this dot product, i.e. k(x,y)=φ(x)Tφ(y)k(x,y)=φ(x)Tφ(y). Explained well here - https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is It transforms Non -Linear problem using a a linear classifier. Kernel functions or Kernel Tricks use this fact to bypass the explicit transformation process from the attribute space to the feature space, and rather do it implicitly . The benefit of implicit transformation is that now you do not need to: Manually find the mathematical transformation needed to convert a nonlinear to a linear feature space Perform computationally heavy transformations https://learn.upgrad.com/v/course/208/session/31158/segment/164610",300691.0
119495,517408.0,"A kernel is the core component of an operating system. Using interprocess communication and system calls, it acts as a bridge between applications and the data processing performed at the hardware level https://www.techopedia.com/definition/3277/kernel",312357.0
119495,519146.0,"To train the kernel SVM, we use the same SVC class of the Scikit-Learn's svm library. The difference lies in the value for the kernel parameter of the SVC class. In the case of the simple SVM we used ""linear"" as the value for the kernel parameter. However, for kernel SVM you can use Gaussian, polynomial, sigmoid, or computable kernel.",301646.0
119495,517351.0,"Kernel methods are a class of algorithms for pattern analysis , whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters , rankings , principal components , correlations , classifications ) in datasets. In its simplest form, the kernel trick means transforming data into another dimension that has a clear dividing margin between classes of data. [1] For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map : in contrast, kernel methods require only a user-specified kernel , i.e., a similarity function over pairs of data points in raw representation. Kernel methods owe their name to the use of kernel functions , which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the "" kernel trick Source:- https://en.wikipedia.org/wiki/Kernel_method",317991.0
119077,514647.0,"I don't think the class imbalance determines the model/algorithm to use. 40% is spam just signifies that this is not a skewed/imbalanced dataset. We could have used any classification algorithm for this. However, in case the dataset is skewed/imbalanced, there are additional steps (sub-sampling) that would have to be taken to ensure an effiient model. See https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/",318438.0
119077,514787.0,"As mentioned by the professor, we have to just make sure that the training or the test data is not skewed towards one of the classifier. Because it will make it biased and the model might not learn properly.",315028.0
119383,516147.0,"On one safe side beyond Margin, slack variable takes the value of zero, irrespective of the distance from hyperplane. Between margin and hyper plane it can be any value between 0 and 1 Beyond hyperplane on the other side slack variable can go upto infinity, theoretically. I hope this clarifies.",301121.0
119383,517390.0,"if a data point is incorrectly classified (i.e. it violates the hyperplane), the value of epsilon (ϵ) &gt; 1, it could be anything greater than 1, could go upto Infinity. On the other hand, if a data point is correctly classified but falls inside the margin (or violates the margin), then the value of its slack ϵ is between 0 and 1. For points which are at a distance of more than M, i.e. at a safe distance from the hyperplane, the value of the slack variable is 0.",300691.0
118978,513770.0,Here spam is the positive class and ham is negative class. The dataset given has the field spam. The field value =1 if the email is spam and 0 if the email is ham. Hence spam is positive class and ham is negative class.,310467.0
118978,514784.0,Spam is mentioned to be Positive here. As per the problem the Good Mail(Negative) shouldn't be marked as Spam. i.e Positive. Hence we need to get the Negative Success rate High. i.e Specificity.,315028.0
118278,,nan,
119492,517437.0,"Hi, We can look at python lab lectures and it is easy to find out. Regards, Kiran",306736.0
118280,511311.0,epsilon fo such cases would be 0. If correctly classfied and within margin would be between 0 and 1,318438.0
118280,512593.0,"It will be 0 , correctly specified means the the point is well located far above or below the hyperplane and identified rightly whether it is Spam or ham",315560.0
118280,512668.0,"For a point that is correctly classified, its distance from the hyperplan would be &gt;= M. so for below equation: Ix(W.Y)&gt;= M(1-ei) we will have ei=0 so that the distance from hyperplan is &gt;=M Hope this helps.",306725.0
118855,513064.0,"SVMs lends well to binary classifications. One way to use SVM for more than 2 classes is the one-vs-all. For ex: Consider K classes data. Here we fit K SMV’s, each time comparing one of the K classes to the remaining K - 1 classes. We then assign the observation to the class for which has the highest amount of confidence that it belongs to the kth class rather than any other classes. Refer to https://stats.stackexchange.com/questions/162377/svm-for-more-than-2-classes https://discuss.analyticsvidhya.com/t/what-happen-when-svm-has-to-predict-more-than-two-class/8442/2",311857.0
118857,513120.0,They are immune to outlier. Only the support vectors determine the hyperplane and margins.,318438.0
118857,513265.0,"While learning the maximal separating hyper plane for the given data, only those data points are considered that are between +margin width and - margin width. These points are aptly called support vectors.The separating hyper plane is only dependent on these support vectors and is not impacted by the points which are far away(outliers).",311254.0
118857,513127.0,"Yes, SVC are relatively immune to outliers. SVC s are decided by support vector points(points that are the closest to SVC). It can be changed only if there are any changes in the support vector points and it is not impacted by the far away points(which ae outliers). Hence we say that SVC are relatively immune to outliers.",310467.0
118857,513307.0,I got the point. I misjudged the question itself. Thanks all,311404.0
118857,514417.0,"In the future videos, the points that are misclassified are also termed as outliers. These ponts would definitely play a role in determining the hyperplane. Then how are the SVC immune to the outliers?",304319.0
118857,516303.0,because svm depends on nearest points only,318005.0
119494,516962.0,I dont thik we need outliers treatment as they are immune to outliers. These values will be far away from the hypeplane when it comes to classification problems.,306735.0
119494,518089.0,"SVM is immune to overall data outliers and hence does not require any outlier treatment. Infact, such outliers will be clearly classified to one or the other class and will have epsilon value of 0. On the other hand, class outliers may or may not be classified correctly and that would depend on the Cost C.",313826.0
119494,518174.0,SVM are immune to outlier as far as you are considering the support vectors points.,301648.0
119494,518316.0,"My understanding is as follows : 1. There are certain datapoints which will be viewed as outliers when the entire fata spread is considered. Let's call these as Overall Data Outliers. SVM is immune to such outliers. Infact, these datapoints will be clearly classified as belonging to one kr the class and their epsilon value would be zero. 2. On the other hand, there would be certain datapoints which will be considered as outliers with respect to the class they belong to. Let's call these as Class Outliers. Here comes the notion of cist function C. Such datapoints would be very close to or on the other side of the classifier hyperplane and their epsilon value would be greater than 0. If it is between 0 and 1, then they are rightly classified and if it is greater than 1 then they are misclassified.",313826.0
118719,512616.0,"It worked well for me without any issues. When i run email_rec.info() on the dataset, spam column has int64 datatype. Is it float for you by any chance?? In that case converting it into int and then running again may help.",317996.0
119176,514939.0,"In SVM formulation, c is the cost parameter. This is the total error that one is willing to accept. You pass c to the optimization routine and it return the epsilon and w. If value of c very small, you are pushing the optimization routine towards finding a strict separator. If c value is very large you are letting your model very lose and you will be signaling that you are willing to accommodate lot of errors, it will produce a generic model. You need to find optimal value of c. On the other hand, in the SVC() implementation of python that you will use, the hyperparameter C is analogous to the penalty imposed for misclassification, i.e. a higher C will force the model to classify most (training) data points correctly (and thus, overfit). In sklearn, high value of C implies a high cost of making errors or misclassifications. While a low value will allow misclassification in the SVC() parameter",317514.0
119470,517194.0,"The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.",311254.0
119470,517361.0,Please go through below link for more explanation about the significance / influence of C in SVM. https://medium.com/@pushkarmandot/what-is-the-significance-of-c-value-in-support-vector-machine-28224e852c5a https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel Hope this will help.,317991.0
119470,517372.0,"The Soft Margin Classifier overcomes the drawbacks of the Maximal Margin Classifier by allowing certain points to be misclassified. You control the amount of misclassifications using the cost of misclassification 'C', where C is the maximum value of the summation of the slack variable epsilon( ϵ ), i.e. ∑ϵi≤C. If C is high , a higher number of points are allowed to be misclassified or violate the margin. In this case, the model is flexible, more generalisable, and less likely to overfit . In other words, it has a high bias. On the other hand, if C is low , a lesser numer of points are allowed to be misclassified or violate the margin. In this case, the model is less flexible, less generalisable, and more likely to overfit. In other words, it has a high variance. So, C represents the 'liberty of misclassification' that you provide to the model. https://learn.upgrad.com/v/course/208/session/31159/segment/164623",300691.0
119069,514420.0,No. they are not normalised. But we are not expected to normalise it. It couldn't understand the intent of the coding question.,304319.0
119069,514700.0,Looks like the data has been normalized by comparing the data provided under description and the one which is present in the csv.,313826.0
119069,514785.0,"Yes, If you print the data, you will find the testing data. Compare it. It's normalized. You just need to create an SVC object and then use all columns except the ID and prediction from the test to predict.",315028.0
119069,515009.0,"No, data is not normalized",306248.0
119069,515023.0,"Thanks Vinay. As you said, comparing the actual csv with the data in the description seems that data is cleaned and normalized.",311857.0
119069,515622.0,Data . is not normalized.,313526.0
118871,514110.0,"Hi, The live execution set the columns in alphabetical order. Thanks",344894.0
119606,517936.0,"Yes! C is a hyperparameter. Moreover the reason being that C is not learning from the model, makes 'C' a hyperparameter.",301121.0
119606,518168.0,Yes C is a hyperparameter in SVM.,301648.0
119119,515964.0,I have reported this as an error in the question. Waiting for TA response. I believe the plot should clearly show some data points on the wrong side to consider it as a soft margin classifier.,314730.0
119119,514630.0,"If you look A,B,C there are same number of bule and red dot after classification by hyper plane.In figure D you will find few more dot bule which might be due to misclassification due to soft margin. Also the main reason is margin between the data points is high in D which means higher value for C and allowing misclassification",318476.0
119119,514858.0,"We need to pick the line that clearly separates most of the data points on either side. Basically, we are finding the separator which tries to maximize the margin while keeping the no. of misclassification as low as it can.",317514.0
119119,515316.0,"It is not mandatory to have misclassification to qualify to become Soft classifier. If you look at A, B and C the line are too close to the points. When you do try with those line on your test data, there would be high amount of misclassification. While in case of D, the line is well separated from the nearest point both sides.",317514.0
119119,515013.0,Yes Ashish....completely agree...there is no misclassification in plot D. how they are comsidering D as Soft classifier ? @TA please confirm or change the responses,305650.0
141333,610575.0,replace nan by 0 using fillna(0),318005.0
140328,,nan,
140341,606923.0,"In the video lecture, they specified about Pearson correlation and cosine similarity , What are the more measure that we can use ?",311466.0
140341,606479.0,"Example for User-based Collaborative Filtering is explained in the video lecture, https://learn.upgrad.com/v/course/208/session/41986/segment/227699 However, you can find the detailed elaboration with calculation along with a matrix example in the below link: https://towardsdatascience.com/collaborative-filtering-based-recommendation-systems-exemplified-ecbffe1c20b1 Hope, this will help.",311117.0
140435,607588.0,You have NaN values in your raw.dat file. Drop that column or replace it with 0s.,329936.0
139491,601508.0,"You can use any data you want as per your need. you can try data under ""recommended for education and development""",329936.0
139491,611409.0,well personally I would go for the 1Mb file- can you imagine trying to process the next smallest CURRENT file - 190MB .. or the 3.1gb file 8-),300694.0
139491,606433.0,Dataset at grouplens.org changes with time. So there is diiference between dataset which was on site when video was recorded and data which is available now.,318770.0
140377,,nan,
141435,,nan,
140451,607594.0,"Try using fill_value=0 instead of fillna(0) inside the bracket. df_movie_features = train.pivot(index= 'UserId' , columns = 'movieId' , value = 'ratings' , fill_value=0 ) hope it works.",329936.0
140451,609231.0,,318335.0
140138,611419.0,it is mentioned around the 45th second of the third video,300694.0
140138,606170.0,"Purpose of Normalization - some user gives higher rating to bad movies and some users has given lower rating to good movies, for this kind of scenario we normalise the ratings. A user will rate in the range of 1-3 star for the worst to best movie, whereas a different user will rate in the range of 1-5 star for the same set of movies. To bring both the users on par, the ratings are normalised using adjusted cosine similarity. How is normalization working - We first calculate the NaN which is done by calculainge the means of all the places where there is no NaN and subtract it from movie features. why are we subtracting Correlation Matrix from 1 - explained in detail in the given link here https://www.quora.com/Why-is-one-minus-cosine-similarity-equal-to-cosine-distance",329936.0
140380,609109.0,Error !!! pivot() got an unexpected keyword argument 'fill_value',311466.0
140380,607573.0,"If the User_Id is having NaN then this problem happens, which convert all values to integer so possible solution is add parameter fill_value=0 df_mov = df.pivot(index= 'UserId' , columns = 'movieId' , value = 'ratings' , fill_value=0 ) hope it works.",329936.0
141027,609239.0,This happens as you transfer the data in pivot format,329936.0
141027,609366.0,"The ratings data frame is split into train and test, rows in both the data frame have different indexes. Dummy train and test are created with movie id and user id as the indexes. And we have multiple values for user id as well as movie id and hence you can see the same indexes in both the frames. But if you notice the rows are different",315028.0
141027,611392.0,because many users hv rated multiple time,318005.0
141027,610952.0,can a TA elaborate more on this? I am still unable to grasp what happened here.,314536.0
140332,607409.0,The first question is related to what we call Cold Start and could be handeled by content base filtering. For more detail refer these links - https://www.yuspify.com/blog/cold-start-problem-recommender-systems/ https://kojinoshiba.com/recsys-cold-start/ For the 2nd question - Use dimensionality reduction. Remove unnecessary users and products from where we are not learning much and reduce sparsity of user-item rating matrix. For detail refer - https://medium.com/@rabinpoudyal1995/challenges-in-building-recommendation-systems-719a4d3cf5b2,329936.0
141453,611686.0,I would ask you to go through the below link. It has been discussed already. https://learn.upgrad.com/v/course/208/question/139491,315661.0
140249,606064.0,"Above will be content based as the algorithm is recommending items that a user himself has liked/browsed in past. In collborative, it recommends items to the user based on similar user (what he liked).",320103.0
140249,606437.0,Here in sentence the concern is about similiar items which user has chosen in past. It is not concerned about preference other users. That is why colloborative filtering is wrong.,318770.0
140249,605734.0,"In Collaborative filtering, we find Users who have similar taste as our user and use their behavior/likes/preferences to recommend items to our user while in content-based filtering we monitor our user's behavior/likes/preference and recommend items similar to that.",317984.0
140249,607283.0,"Because in Collaborative filtering, we find similar user's on the basis of their likes and dislikes. Whereas in content-based filtering we consider only the past behaviour of a particular user.",318427.0
141180,609794.0,https://learn.upgrad.com/v/course/208/question/140704 This has the answer to your question and check for duplicate values also before creating pivot table,311466.0
140725,608601.0,"Suppose your array has 3 values= [1,2,3] and df after transpose has 4 rows 3 column d e f a 3 6 10 b 4 7 11 c 5 8 12 d 6 9 13 So substraction is happening in the following manner d e f a 3-1 6 -2 10-3 b 4 -1 7-2 11-3 c 5 -1 8-2 12-3 d 6 -1 9 -2 13-3 Transpose of the substracted matrix a b c d d 2 3 4 5 e 4 5 6 7 f 7 8 9 10 Hope this helps!",311254.0
140704,608741.0,"Try using "" fill_value = 0"" inside bracket instead of .fillna(0)",329936.0
140704,609124.0,"We can use ""fill_value = 0"" in pivot table. print (countrydf.pivot_table(index='germanCName', columns='indicator.id', values='value', fill_value=0)) PS: Please give correct answers TA and its not for the first time !",311466.0
140704,609128.0,"For anyone who is still interested in the difference between pivot and pivot_table , there are mainly two differences: pivot_table is a generalization of pivot that can handle duplicate values for one pivoted index/column pair. Specifically, you can give pivot_table a list of aggregation functions using keyword argument aggfunc . The default aggfunc of pivot_table is numpy.mean pivot_table also supports using multiple columns for the index and column of the pivoted table. A hierarchical index will be automatically generated for you. Source : https://stackoverflow.com/questions/30960338/pandas-difference-between-pivot-and-pivot-table-why-is-only-pivot-table-workin",311466.0
140704,609132.0,This is the error after changing the code.,310505.0
140704,609232.0,"You can try, as we do not have to do any aggregation here. df_movie_features1 = train.pivot_table( index='userId', columns='movieId', values='rating', fill_value=0)",329936.0
140970,609237.0,Find the detail difference and usage here https://nikgrozev.com/2015/07/01/reshaping-in-pandas-pivot-pivot-table-stack-and-unstack-explained-with-pictures/,329936.0
141654,613337.0,"If you apply the MinMax scalar earlier, the extremes (Max value = 5) would tend to be taken up by the movies that are already seen by the user. Later, after you remove those values, you might end up with values that are not propery distributed over the desired range of 1-5. Your second question is not very clear. Elaboration of the question might help.",318762.0
140058,605296.0,"I think it is a typo. As per our curriculum, ,entire course is design in Python nor R. Hope,it will help you.",320689.0
140058,607192.0,"The content must be updated according to what they are teaching, that would give learners a better learning experience rather than getting stuck in these type of issues.",318355.0
140058,606431.0,This course is for Python and not for R. You can check code in last sections.,318770.0
140058,604484.0,In the given course you will learn the implementation of recommendation system in python.,329936.0
141449,612051.0,Understood. thanks a lot !,314536.0
141449,610997.0,"Movie data is split before pivoting. Please refer the following example. In this example, if we create dummy_train the movies already rated by user should not come again as recommendation to the user since they have already watched those movies. Hence 0 if the user has reviewed a movie, else 1. However while evaluating the model, we need only those movies rated by user to check the accuracy of prediction. Hence its opposite 1 if the user has reviewed the movie else 0. If you apply this logic in the above example, you will get the values as follows. Hope this helps!",310467.0
141975,614353.0,Thanks Uddhav!,319357.0
141975,613330.0,"In the IBCF approach, the movie ratings are normalized so that we can obtain the correlation values by applying cosine metriic. With normalized data, correlation is the same as cosine.",318762.0
109235,471414.0,just do scalling on the req variable each variable doesnt need scalling,318017.0
109235,471424.0,Please apply to scale on the required variable which is mentioned in the question. After that apply your k means algorithm.,301648.0
109235,471421.0,"It is clearly said in problem, use only ""Ave"" and ""SR"" as a variable. So we will have to do scaling on these variables only.",311117.0
109235,471662.0,"As mentioned in the question, you need to perform scaling only on the average(Ave) and SR columns in the given data. We dont need to perform scaling on the other columns such as HS which contains string values.",318788.0
109235,471449.0,"Hey Naveed, As mentioned in the question, you need not clean data because they have told that you need to cluster on the basis of two factors which are AVE and SR. So you can group the dataset on the basis of these two factors and then perform the required scaling operations. Hope this helps!",301655.0
109327,471639.0,"Yes, it is better to exclude them as it would impact the clustering by pulling the center of one of the clusters (say, Cluster A) towards it leaving the other valid data points (which would get assigned to different cluster) which in reality should have belonged to the clsuter A.",311160.0
109327,471668.0,"Yes, we should treat the outliers, as they may impact our outcome. A very good discussion is there in stackexchange.com in the below link: Does it make sense to remove the outliers after clustering the dataset?: https://stats.stackexchange.com/questions/328109/k-means-does-it-make-sense-to-remove-the-outliers-after-clustering-the-datasets",311117.0
109327,471671.0,"Yes, while performing k means it is better to exclude the outliers as it can cause impact on the original centroids and its effect can ignore valid data points. K means is sensitive to outliers.",318788.0
109327,498536.0,"Outlier treatment will need to be done. As we saw during the session, an outlier and pull the centroid and make the cluster not cohesive.",319302.0
109368,471820.0,Could you please mention the specific part you are talking about? maybe I can help.,311117.0
109368,471965.0,See this video once and go through the lecture after that. you will easily relate all the concepts. https://www.youtube.com/watch?v=4b5d3muPQmA,318802.0
109237,471536.0,Thanks for sharing Jayashree,334535.0
110006,474304.0,"Extract from Kaggle discussion: You can proceed with principal components for categorical variables using a multiple correspondence analysis (MCA), which will give principal components, and you can get them to do a separate PCA for the numerical variables, and use the combined as input into your clustering. You can further refer this link: https://www.kaggle.com/general/19741 and https://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data/24#24",311117.0
110006,474744.0,"Hey Anuj, Thanks. The links somewhat helped with the query. I tried working on converting the maximum number of categorical variables into numeric and then tried to test the Hopkins statistic, but i interrupted the kernel as it was more than 24 hours in running. Can you please help me understand, is Jupyter considered as a best practice to use in an organiztion. I could see there are lot of issues while working on data with a 2-3 million records. It takes lot of time. I am sure there must be some alternative available. Thanks again for your support.",317418.0
110006,474310.0,"Hey Ashish, I think, the discussions given in the link below can be helpful to you, please do check: https://discuss.analyticsvidhya.com/t/clustering-technique-for-mixed-numeric-and-categorical-variables/6753 The easiest technique would be to convert Categorical variables into numeric variables with magnitude similar to Numeric values and then perform clustering. Hope this helps you!",301655.0
109145,471175.0,We might need to do scaling if the units are not comparable as that would also hep us to understand the results better and compare different end clusters in a better way.Standardization and scaling was discussed in linear regression module.,305655.0
109145,471176.0,"Centering and Scaling : These are both forms of preprocessing numerical data, that is, data consisting of numbers, as opposed to categories or strings, for example; centering a variable is subtracting the mean of the variable from each data point so that the new variable's mean is 0; scaling a variable is multiplying. For knowing why we do it, please refer this link: https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia Method for finding Standard deviation has already been tought as, The standard deviation is given by the formula: s means 'standard deviation'. Now, subtract the mean individually from each of the numbers given and square the result.",311117.0
109420,472043.0,Thanks for sharing Kunwar,334535.0
109420,472328.0,Thanks for sharing,320687.0
109152,471164.0,"This is the minimisation of WCSS (within-cluster sums of squares), and necessarily the maximisation of the distance between clusters . Maybe you can get more understanding from here: https://stats.stackexchange.com/questions/158210/k-means-why-minimizing-wcss-is-maximizing-distance-between-clusters Why are we calculating Z here? K-means clustering distinguishes itself from Hierarchical since it creates K random centroids scattered throughout the data . You can refer this also : http://www.learnbymarketing.com/methods/k-means-clustering/",311117.0
109449,473048.0,"When the centroids stops to update itself after the assignment iterations, then we have reached the final centroid.",314431.0
109449,472068.0,"The same has been replied here: https://learn.upgrad.com/v/course/208/question/109144 The very notion of ""good clustering"" is relative, it depends on the quality measures and what you want to evaluate. Few extractions from research gate for understanding: For instance, the Davies-Bouldin Index evaluates intra-cluster similarity and inter-cluster differences. If you consider these to be good criteria, go for the Davies-Bouldin. The Silhouette Index measure the distance between each data point, the centroid of the cluster it was assigned to and the closest centroid belonging to another cluster. If you consider that this is a good criterion, go for the silhouette index. For details, please go through the link: https://www.researchgate.net/post/How_can_we_say_that_a_clustering_quality_measure_is_good Further, it has been excellently explained on analyticsvdhya also, you can refer: https://www.analyticsvidhya.com/blog/2013/11/getting-clustering-right/",311117.0
108878,470464.0,I have replied here : https://learn.upgrad.com/v/course/208/question/108825. Please go through. Hpoe it will help.,311117.0
108878,470450.0,Hi refer to the image below. This is how after the several repeated iterations the pattern of clustering changes.. Iteration #1 with this formula Dist_C1 &amp; Dist_c2: =SQRT(((C6-$H$6)^2+(D6-$I$6)^2)) Iteration #1 with this formula Dist_C1 &amp; Dist_c2: =SQRT(((C22-$H$22)^2+(D22-$I$22)^2)) There is If else condition in column KLMN.. please look at it you will understand how the clusters are getting reformed... Hope this helps...,310508.0
108490,469263.0,"Hey Anuj, I think, this link will help you out in a much better way https://stats.stackexchange.com/questions/89926/what-do-you-do-when-a-centroid-doesnt-attract-any-points-k-means-empty-cluste Hope this helps!",301655.0
108490,469260.0,First plot the scatter plot for the data set and then assigned the two centroids . Randomly selecting one centroids away from the data set will not select any points for that center . But this also if you getting one centroids then yes your assumption is correct .,318476.0
108490,469415.0,"Firstly, we may want to avoid getting into such situations by choosing initial centroids closer to the data set range. Your aim is to create 'K' clusters so in this case you may still want to keep 2 clusters but update the center of the empty cluster. Define your own workaround for ex., the closest point in the dataset from this empty cluster center. Hope this helps.",311857.0
109512,472403.0,"Thank you for the reference. Will surely go through it. So, does it mean that the information on the other page is incorrect? Can any of the TAs confirm? Because this has been shared on the Upgrad platform itself.",315471.0
109512,472397.0,"Yes. The data points within a cluster should be tightly bound which is called as intra-segment homogeneity. The data points of different clusters should show very different properties to each other which is called as inter-segment heterogeneity. You can refer the point no. 8.1.1 (page. 4/82), in the attached pdf file: https://www-users.cs.umn.edu/~kumar001/dmbook/ch8.pdf",311117.0
108587,469518.0,Xi is the ith data element. X1 would be first and Xn would be nth. K is the number of clusters to be formed. μ i is the center of the cluster i. i is a generic number For X it is 1 to n for μ it is 1 to k. Hope this helps,311857.0
108587,469908.0,Xi - Data points in given population. K - No of clusters ui -Cluster point location,318370.0
109586,472603.0,Clustering is to be performed on a dataset where you wish to perform your analysis and not any other data point. The steps are as below: 1. Randomly pick any number clustering and then they become the centroid. 2. Make sure to miminizes the distance between the centroid and the data points 3. Recompute the center of each cluster which the update process(i.e. new locations for the centroid) to an optimised cluster. Repeat steps 2 and 3. To get a more idea please check out this link which has a visual representation of clustering http://www.naftaliharris.com/blog/visualizing-k-means-clustering/ Hope this helps,312259.0
109586,472618.0,"What are some good ways to choose an efficient set of initial centroids?: An approach that yields more consistent results is K-means++ . This approach acknowledges that there is probably a better choice of initial centroid locations than simple random assignment. The method is as follows: Choose one of your data points at random as an initial centroid. Calculate D(x), the distance between your initial centroid and all other data points, x. Choose your next centroid from the remaining datapoints with probability proportional to D(x)2. Repeat until all centroids have been assigned. You can refer this link also: https://datascience.stackexchange.com/questions/5656/k-means-what-are-some-good-ways-to-choose-an-efficient-set-of-initial-centroids",311117.0
109596,472649.0,"This has been discussed here , https://learn.upgrad.com/v/course/208/question/109586 You can refer this, may be helpful: An approach that yields more consistent results is K-means++ . This approach acknowledges that there is probably a better choice of initial centroid locations than simple random assignment. The method is as follows: Choose one of your data points at random as an initial centroid. Calculate D(x), the distance between your initial centroid and all other data points, x. Choose your next centroid from the remaining datapoints with probability proportional to D(x)2. Repeat until all centroids have been assigned. You can refer this link also: https://datascience.stackexchange.com/questions/5656/k-means-what-are-some-good-ways-to-choose-an-efficient-set-of-initial-centroids",311117.0
108641,469610.0,Please find the like below explain more on Global Minimum and local minima and method to coverage them: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5039165/#!po=3.00000,318476.0
108641,470038.0,"Hi Rajani, Refer to the following image. Sharing the link as well for your ref.. https://math.stackexchange.com/questions/1591520/what-is-difference-between-maxima-or-minima-and-global-maxima-or-minima Hope you find it useful.. Thanks",310508.0
108641,469630.0,"A local minima of a function (typically a cost function in machine learning, which is something we want to minimize based on empirical data) is a point in the domain of a function that has the following property: the function evaluates to a greater value at every other point in a neighbourhood around the local minimum than the local minimum itself. A global minimum of a function minimizes the function on its entire domain, and not just on a neighbourhood of the minimum. In other words, the function evaluated at the global minimum is less than or equal to the function evaluated at any other point . For further detailed explanation, please follow the link: https://www.quora.com/What-is-the-local-minimum-and-global-minimum-in-machine-learning-Why-are-these-important-in-machine-learning",311117.0
108817,470197.0,"Hey! Refer below - https://medium.com/datadriveninvestor/k-means-clustering-b89d349e98e6 K-Means Clustering Algorithm: 1. Choose a value of k, number of clusters to be formed. 2. Randomly select k data points from the data set as the intital cluster centeroids/centers 3. For each datapoint: a. Compute the distance between the datapoint and the cluster centroid b. Assign the datapoint to the closest centroid 4. For each cluster calculate the new mean based on the datapoints in the cluster. 5. Repeat 3 &amp; 4 steps until mean of the clusters stops changing or maximum number of iterations reached. Hope this is helpful :)",300691.0
108817,471952.0,"go through this video, Its really easy to understand after this https://www.youtube.com/watch?v=4b5d3muPQmA",318802.0
108822,470234.0,"Xi refers to (Xi1,Xi2), so Xi2 essentially is a reference to y-axis",306736.0
108822,470218.0,"Xi refers to two dimensions points (x,y) also MUi refers to the centroids with (x,y) values in the video",318476.0
108822,470284.0,"This is for cost function for the K-Means algorithm: Prof calculated the below formula: J=∑n i=1||Xi−μk(i)||2 For ith data point which is a 2d object and μ which is again a 2d object, we compute the distance between these two, this is given by d(xi, μki) where k is the number of clusters.",311117.0
108973,470752.0,"I have replied and attached the calculated sheet along with the formula here : https://learn.upgrad.com/v/course/208/question/108825 &amp; https://learn.upgrad.com/v/course/208/question/108878. Please go through this, again attaching here. Hope you will get it.",311117.0
108669,469792.0,"K-means converges after 20-50 iterations in all practical situations, even on high dimensional datasets as they tested. For further, can please refer the link: https://stats.stackexchange.com/questions/261836/k-means-how-many-iterations-in-practical-situations",311117.0
108669,469745.0,Maximum Iterations. Limits the number of iterations in the k -means algorithm. Iteration stops after this many iterations even if the convergence criterion is not satisfied. This number must be between 1 and 999,318476.0
108669,469787.0,"Hey! Refer below - https://medium.com/datadriveninvestor/k-means-clustering-b89d349e98e6 K-Means Clustering Algorithm: 1. Choose a value of k, number of clusters to be formed. 2. Randomly select k data points from the data set as the intital cluster centeroids/centers 3. For each datapoint: a. Compute the distance between the datapoint and the cluster centroid b. Assign the datapoint to the closest centroid 4. For each cluster calculate the new mean based on the datapoints in the cluster. 5. Repeat 3 &amp; 4 steps until mean of the clusters stops changing or maximum number of iterations reached.",300691.0
108669,470339.0,The iteration limit for k means algorithm is till the clusters stop changing. While performing iterations we can see a certain change in the clusters and after performing certain number of iterations the clusters stop changing.,318788.0
109144,471128.0,"We need to stop the iteration when Centroid of each cluster does not change. It also, means the data points would no more move to other Clusters",311160.0
109144,471145.0,"The very notion of ""good clustering"" is relative, it depends on the quality measures and what you want to evaluate. Few extractions from research gate for understanding: For instance, the Davies-Bouldin Index evaluates intra-cluster similarity and inter-cluster differences. If you consider these to be good criteria, go for the Davies-Bouldin. The Silhouette Index measure the distance between each data point, the centroid of the cluster it was assigned to and the closest centroid belonging to another cluster. If you consider that this is a good criterion, go for the silhouette index. For details, please go through the link: https://www.researchgate.net/post/How_can_we_say_that_a_clustering_quality_measure_is_good Further, it has been excellently explained on analyticsvdhya also, you can refer: https://www.analyticsvidhya.com/blog/2013/11/getting-clustering-right/",311117.0
109144,471666.0,"In K means, we stop the iteration once the centroids of the clusters stop changing. Thus once this parameter stops changing we can say that we have required clusters.",318788.0
109144,471613.0,"Clustering algorithms use different distance or dissimilarity measures to derive different clusters. The type of distance/dissimilarity measures plays a crucial role in the final cluster formation. Higher distance would imply that observations are dissimilar whereas higher similarity would indicate that the observations are similar. In k-means algorithm we are using Euclidean disatnace as distance measure. One majr challenge in clustering is identification of the ideal number of clusters and quality of clusters. There are many popular measures which indicates quality of cluster formed. One such is Silhuette statistics,higher this value implies better clustering. For more details on Silhouette statistic Refer this link https://www.sciencedirect.com/science/article/pii/0377042787901257 https://www.datanovia.com/en/lessons/cluster-validation-statistics-must-know-methods/ Please note that there are several such measures",308638.0
109144,471951.0,plz watch this basic video- https://www.youtube.com/watch?v=4b5d3muPQmA,318802.0
109144,471972.0,"when data points remains same or converges after some iterations, the between distance from the centroid , i.e centroid should be repeated and after some iterations with same data points respect to the clusters, means data points should not move one cluster to other cluster then we stop that process.",318322.0
108825,470274.0,Please find the answer along with formula. please try this:,311117.0
108825,470193.0,The formula you are using is a bit wrong. You are using relative column references and absolute row reference. Both should be absolute. something like this: SQRT((($H$38-C38)^2+($I$38-D38)^2)),309451.0
108825,470203.0,"Hey Prashanth, I've used this formula and its working fine for me SQRT((($H$22-C6)^2)+($I$22-D6)^2) Try it out and let me know if it helps or not!",301655.0
109124,471072.0,Thanks for sharing Sharad.,334535.0
109600,472693.0,"As we ususally use the below command: KMeans(n_clusters=2, init='random', n_init=10, max_iter=50).fit(RFM_norm1) It would be highly unlikely that all the data points would be assigned to a single cluster out of 10 initializations of the various center assignments. As the model picks up the best output of the consecutive runs it is safe to think that the optimization step would handle it.",311160.0
109600,472718.0,"Clustering can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. or cab be say, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance. The decision of which similarity measure to use is application-specific. Clustering analysis can be done on the basis of features where we try to find subgroups of samples based on features or on the basis of samples where we try to find subgroups of features based on samples. As above in most of cases, all the data points would not be covered in a single cluster.",310952.0
109600,473046.0,"Centroid is calculated using the average mean of the points. So if we choose K=2, the centroids will be chosen so that points are close to them. In that case the scenario you have mentioned should not be an issue.",314431.0
108837,470256.0,Please go through the link https://medium.com/@lachlanmiller_52885/machine-learning-week-1-cost-function-gradient-descent-and-univariate-linear-regression-8f5fe69815fd,310952.0
109386,471807.0,Here we have the unsupervised model available with pictorial view along with explanation : https://www.upgrad.com/blog/how-does-unsupervised-machine-learning-work/ and also on : https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/,311117.0
109386,471833.0,"Hey Bharati, I think the link below may help you out: https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03 Thank you!",301655.0
108431,468675.0,"Segmenting is the process of putting customers into groups based on similarities, and clustering is the process of finding similarities in customers so that they can be grouped, and therefore segmented . You can read more here https://blog.agilone.com/segmentation-vs-clustering",317845.0
108431,468833.0,The segmentation is a process to extract useful information that you need. The cluster analysis focuses on the similarity of group of objects. Maybe this link willbe useful: https://www.researchgate.net/post/Is_there_a_difference_between_cluster_analysis_and_segmentation It is also explained with image segmention example in this link: https://stackoverflow.com/questions/30807543/difference-between-segmentation-and-classification,311117.0
108431,469021.0,Simple way to explain is - Clustering is one technique to do Segmentation.,311857.0
108431,469437.0,Clustering is a technicque while Segmentation is the business requirement. We do clustering to get segmentation.,318370.0
108431,469827.0,Segmentation is the technique to carry out clustering.,310505.0
108431,470168.0,"Segmentation is basically a technique , which is to be applied on and application is basically called clustering.",318427.0
109611,472759.0,Hi Naga Use the below link https://pypi.org/project/kmodes/ Let me know if this does not help you .,311861.0
109611,472787.0,"Steps for ""Installation"" and ""Usage"" are mentioned in the attached link, maybe helpful. https://github.com/nicodv/kmodes",311117.0
108808,470151.0,"Not Exactly. K - means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster , while keeping the centroids as small as possible In this case there are 2 centroids. The very objective of thie exercise is identify 2 clusters from a set of random 10 points given. Firs we are given 2 centre points for these 10 points (just assigned... may not be right at all) As a first step identify 2 groups by measuring distance of all points and from these two centre points and make a group in such a way that center point belongs to those set of points where distance from that centre point is less than from other centre point. Once done, find centroid for that group. Later repeat the step1 to find a group.If you keep repeating that, after a while there will not be any more movemenet of these centre points. This means we have identified two clusters with one centre point for each of those originally given 10 points (which otherwise would have looked too random.)",301121.0
108808,470892.0,"Yes. got it..merely the question is, does the centroid picked in the assignment step is at random. In a later section, I noticed that it is random but any points within the range for each k-cluster and. recomputed to a new value(centroid) in the optimization step. Thanks for the details.",311115.0
108839,470267.0,"Direct methods: consists of optimizing a criterion, such as the within cluster sums of squares or the average silhouette. The corresponding methods are named elbow and silhouette methods, respectively.",318476.0
108839,470254.0,Refer below for cluster - https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/,310952.0
108839,471954.0,"Go through this video, really easy to understand- https://www.youtube.com/watch?v=4b5d3muPQmA",318802.0
108839,470286.0,"How to decide on number of clusters? The optimal number of clusters can be defined as: Compute clustering algorithm (e.g., k-means clustering) for different values of k. For each k, calculate the total within-cluster sum of square (wss). Plot the curve of wss according to the number of clusters k. You can follow this link for additional details: https://stats.stackexchange.com/questions/23472/how-to-decide-on-the-correct-number-of-clusters",311117.0
115642,,nan,
115643,499247.0,"Hello Praveen, Both have got some purposes and not to prefer one for the other. Silhouette criterion is one of internal clustering validation criterions. ""Elbow"" is not a criterion but is a decision method/rule (while contemplating a plot of a criterion values). It can be used with many criterions, including the silhouette. Finally it is a good idea to arrive at final number of clusters by trial and error. (Please bear in mind that you have to take care of the outliers since they introduce unnecessary clusters)",301121.0
115643,500612.0,"Hey Praveen, I think Elbow Curve Method is the most efficient one for this problem! But, still you can refer this link given below: https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/ Hope this helps you out!",301655.0
115645,499755.0,"K means cost function is a non-convex function, which means the coordinate descent is not guaranteed to converge to the global minima and the cost function can converge to local minima.",301648.0
115645,499202.0,"Hi, So K-means can’t handle non-convex sets. Convex sets means , In Euclidean space, an object is convex if for every pair of points within the object, every point on the straight line segment that joins them is also within the object. The k-means algorithm, in its basic form, is like making little circular paper cutouts and using them to cover the data points. We can change the quantity and size and position of our paper cut-outs, but they are still round and, thus, these non-convex shapes evade us. More descriptive here https://pafnuty.wordpress.com/2013/08/14/non-convex-sets-with-k-means-and-hierarchical-clustering/",301646.0
115645,500171.0,One of the weaknesses of k - means clustering is that it is sensitive to initialization. ... The sensitivity to initialization is due to the non- convex optimization problem underlying k - means clustering There are lot of articles on this topic. Have a look at this links https://hal.archives-ouvertes.fr/hal-01504799v3/document https://math.stackexchange.com/questions/463453/how-to-see-that-k-means-objective-is-convex,301121.0
109096,471090.0,1. After randomly choosing first cluster we calculated distance and distance square. 2. Based on the largest distance square we selected the second cluster. 3. Since we have 2 clusters now we assign the datapoints to the nearest cluster and calculate the di and di^2. From this pool of new collection the third cluster center will be selected based on the largest distance square. (We choose second and third clusters with probabilitty which is proportional to di^2),318370.0
109096,470948.0,"Here the steps of the algorithm are: 1. Start by choosing K random points the initial cluster centres. 2. Assign each data point to their nearest cluster centre. The most common way of measuring the distance between the points is the Euclidean distance. 3. For each cluster, compute the new cluster centre which will be the mean of all clustermembers. 4. Now re-assign all the data points to the diffrent clusters by taking into account the new cluster centres. 5. Keep iterating through the step 3 &amp; 4 until there are no further changes possible",318476.0
109096,470944.0,"You can refer this, where all the 3 steps(Initialization, cluster assignment, moving the centroid ) has been explained: https://towardsdatascience.com/clustering-using-k-means-algorithm-81da00f156f6 maybe helpful.",311117.0
108925,470594.0,"Euclidean distance is similar to Pythagorean distance in a single plane: For example, length of hypotneuse (which is basically square root of sum of squares of size of sides) Similary whenever you want to find the distance between two points, you need to find the size of sides. In this case it is y2-y1 and x2-x1. Now you have to find the distance between (x1,y1) and (x2,y2) which is nothing but a hypotneuse for right angled triangled having sides of values y2-y1 and x2-x1. Hence hypotneuse = sqrt ((y2-y1)^2 + (x2-x1)^2) = distance beween two points of coordinates (x1,y1) and (x2,y2).",301121.0
108925,470640.0,Euclidean distance: The Euclidean distance between two points in either the plane or 3-dimensional space measures the length of a segment connecting the two points. It is the most obvious way of representing distance between two points. It is explained with an example and pictorial view on page no 2-8 of the attached pdf file in the link: http://www.econ.upf.edu/~michael/stanford/maeb4.pdf,311117.0
108925,470592.0,"The Euclidean distance or Euclidean metric is the ""ordinary"" straight-line distance between two points in Euclidean space. With this distance, Euclidean space becomes a metric space. The associated norm is called the Euclidean norm. Older literature refers to the metric as the Pythagorean metric. A generalized term for the Euclidean norm is the L2 norm or L2 distance. https://www.khanacademy.org/math/basic-geo/basic-geometry-pythagorean-theorem/pythagorean-theorem-distance/v/distance-formula Hope this helps.",300691.0
108925,471246.0,The Euclidean distance between two points in either the plane or 3-dimensional space measures the length of a segment connecting the two points. It is the most obvious way of representing distance between two points. Please refer the below link: https://people.revoledu.com/kardi/tutorial/Similarity/EuclideanDistance.html,314183.0
109687,473147.0,"so in short, the data point which has the longest d(xi)^2 they have the more probability to be picked up as next cluster center.",301646.0
109687,473076.0,"Please go through these steps, you will understand what is probability proportional to d(xi)^2: The exact algorithm for K-means++ is as follows: Choose one center uniformly at random from among the data points. For each data point x, compute D(x), the distance between x and the nearest center that has already been chosen. Choose one new data point at random as a new center, using a weighted probability distribution where a point x is chosen with probability proportional to D(x)2. Repeat Steps 2 and 3 until k centers have been chosen. Now that the initial centers have been chosen, proceed using standard k-means clustering. and what is probability proportional? The next centroid x I choose must the distance. D(x) = D(x)^2 / summation of all distances from all data points square. For more understanding, please go through this: https://datascience.stackexchange.com/questions/9541/details-of-the-k-means-algorithm-that-is-used-to-seed-k-means",311117.0
109009,470747.0,"For determining the K-values or the number of clusters: you will get to know with this Elbow Method: Start with K=2, and keep increasing it in each step by 1, calculating your clusters and the cost that comes with the training. At some value for K the cost drops dramatically, and after that it reaches a plateau when you increase it further. This is the K value you want. Please go through the discussion in the attached link, it will be clarified, its very easy: https://stats.stackexchange.com/questions/23472/how-to-decide-on-the-correct-number-of-clusters",311117.0
109009,470733.0,"Hey Koustav, This link might help you: https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/ Hope this helped you understand the concept in a better way!",301655.0
109009,471259.0,This image might help you:,314183.0
109147,471168.0,"The K-Elbow visualizer implements the “elbow” method of selecting the optimal number of clusters for K-means clustering. The elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and then for each value of k computes an average score for all clusters. For example , please go through this: http://www.scikit-yb.org/en/latest/api/cluster/elbow.html Silhouette refers to a method of interpretation and validation of consistency within cluster of data. The technique provides a succinct graphical representation of how well each object in the cluster. The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters. It may give some sight: https://en.wikipedia.org/wiki/Silhouette_(clustering)",311117.0
110015,474318.0,"DBSCAN is a popular clustering algorithm which is fundamentally very different from k-means. k-means requires specifying the number of clusters, 'k'. DBSCAN does not, but does require specifying two parameters which influence the decision of whether two nearby points should be linked into the same cluster. How DBSCAN works and why should we use it: You can go through this link for clear understanding: https://towardsdatascience.com/how-dbscan-works-and-why-should-i-use-it-443b4a191c80 and https://algorithmicthoughts.wordpress.com/2013/05/29/machine-learning-dbscan/",311117.0
108452,468864.0,Got it.. Apologies for posting :(,306011.0
113975,491680.0,Only the cluster id changes but the set of data for the particular cluster always remain the same.,319319.0
113975,491651.0,K-means is not a deterministic algorithm. Please refer this article. https://stackoverflow.com/questions/25921762/changes-of-clustering-results-after-each-time-run-in-python-scikit-learn,310617.0
113975,491686.0,"Pass any integer to random_state so that the same number is used for centroid initialization every time. Thus the elements of the clusters will be the same every time, however the cluster numbers may vary. Again, you can identify the clusters themselves by certain conditions like mean of an original variable being the lowest for some cluster or highest for some cluster, and categorize accordingly so that the change in cluster number is taken care of.",313826.0
113975,492302.0,use random_state = (any integer value) with kmeans() so that the same initial centroids are taken everytime. This way the clusters formed everytime you run will be the same,304319.0
108650,469712.0,"Please go through the solution: You will see that the solution converges really fast. After just the first iteration, the solution converges. After the first iteration, observation 1,2,3 get assigned to cluster 1 and observation 4,5,6 get assigned to cluster 2. The initial cluster centroids were (2.3,3.3) and (3.3,1.3) and the distances from these is calculated to assign the clusters.",311117.0
108650,469727.0,just plot the points over the graph you will clearly say a pattern being made which is dividing the points in clustering with k as 2,318017.0
108650,469868.0,Here the X1 and X2 are similar to X and Y of a 2-d point. A pair of X1 and X2 defines one data point. Assign datapoints to cluster 1 or 2 as per the question. Now find the average of the X1s and X2s for cluster 1 and similarly for cluster 2. these give the new centroids for the clusters. Calculate the distance for each point from the centroid1 and then centroid2. Assign point to the cluster with min distance. repeat the previous two steps with the new clusters. till the cluster numbers don't change,311857.0
110080,482400.0,"K means clustering is an iterative process where we continuously repeat assignment and optimisation steps till any data point is not assigned a new cluster in the assignment step (this is known a convergence). Practically, data is huge and we cannot reach to the point of absolute convergence, hence we specify a value of iterations to be made, we can change the the no of iterations as per the desired results.",318344.0
110080,474587.0,"For clustering, using the priinciple of K-Means, we have to choose the value of K first - number of clusters (Optimum number of clusters have to be chosen by certain methods like Hopkins etc) Once a K-value (number of clusters) is chosen, we must go through the iteration as given below. The process of assignment and optimisation is repeated until there is no change in the clusters or possibly until the algorithm converges. This process continues till the centroid no longer changes, i.e. the solution converges. By that sentence it means you cannot say for sure, in the beginning, that how many iterations it would take for the process to converge, After every iteration, you need to check if it has converged. Please note that K-Means is non-deterministic.",301121.0
109153,471150.0,"What is this Convex Clustering ? The solution paths generated by convex clustering reveal relationships between clusters that are hidden by static methods such as k-means clustering. The current paper derives and tests a novel proximal distance algorithm for minimizing the objective function of convex clustering. What is its uses? The purpose of cluster analysis is to place objects into groups, or clusters, suggested by the data, not defined a priori, such that objects in a given cluster tend to be similar to each other in some sense, and objects in different clusters tend to be dissimilar. You can refer this link for additional details: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004228 Why is it required to find C onvex solution for K-means? How to see that K-means objective is convex, please go through the link, you will get good understanding: https://math.stackexchange.com/questions/463453/how-to-see-that-k-means-objective-is-convex",311117.0
111989,482723.0,TA has replied the similar question. May be helpful: https://learn.upgrad.com/v/course/208/question/111961 https://learn.upgrad.com/v/course/208/question/111624,311117.0
111989,482671.0,"Hello, You need to feed the outlier_teated PCA data to k-means clustering to get the respective cluster id's. Before that you need to perform hopkins test , Silhouette analysis,elbow curve analysis to get the cluster tendency and optimum number of clusters. Please follow sample K-Means algorithm on retail data from lectures. For the assignment approach: https://learn.upgrad.com/v/course/208/question/111624",305652.0
111989,482874.0,just treat the PCAs like you would normal data columns - look at the RFM example that we were given and follow all the steps for clustering (K-means and hierarchical clustering is demonstrated via Python in the RFM example),300694.0
114777,495457.0,"The answer is ""It depends"" First of all, one has to check if the data can be clustered or not by using Hopkins which will reveal if the data is tending to clusterization Imagine a theoretical situation where all the points are so unique and cannot be clustered at all. If the data is so uniform and can not be clustered at all, then maximum number of possible clusters will be equal to total number of points, theoretically. I hope this will make the concepts clear. But Optimal number of clusters can be obtained by different methods like Elbow Curve etc, whcih are taught in UpGrad sessions.",301121.0
114777,495622.0,"Rahul, to play along your question, as explained above, you can check the various methods to determine the optimal number of clusters. but, the TOTAL number of POSSIBLE clusters will be &lt;= n. :-) where each element forms 1 cluster. it's impractical, but a theoretical possibility.",305839.0
114777,495340.0,"Hey Rahul, There are different methods of identifying the optimal number of clusters. Elbow method Average silhouette method Gap statistic method Computing the number of clusters using R You can go through the link to have a better understanding - https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/ Hope this Helps",302742.0
114777,496257.0,"If we have n data points then maximum clusters possible is n wherein each data point is a cluster in itself. Now, depending on the method we use for clustering(K Means or Hierarchical) the choice of number of clusters can be made at the start itself(KMeans) or else it can be done post completion of clustering operation(Hierarchical clustering). Here in order to help us with the technicality of selecting the number of clusters, we can use the Elbow curve or the Silhouette Score. But in most cases, selecting the number of clusters purely depends on the Business scenario that is being worked upon.",312063.0
115647,500611.0,"Hey Praveen, I think the link below can help you understand in a better way: https://www.dummies.com/programming/big-data/data-science/how-to-visualize-the-clusters-in-a-k-means-unsupervised-learning-model/ Hope this helps!",301655.0
115647,499479.0,"If you recall the assignment on clusters for ""Country"" we clusterized the countries and grouped them as A, B, C and D and labelled them. If your idea is to get codes for different type of visualization of clusters as labelled group, here is the link which could be useful. 7 ways to label a cluster plot in Python https://nikkimarinsek.com/blog/7-ways-to-label-a-cluster-plot-python",301121.0
108570,469474.0,"Hey Chetan, Here is a link with some interesting comments on the question which you've asked: https://www.researchgate.net/post/Could_anyone_explain_the_difference_between_Exploratory_Data_Analysis_and_Information_Visualisation Hope this helps you out!",301655.0
108570,469517.0,Hi! Please refer link and attached image - http://www.colaberry.com/data-science-predictive-analytics-pipeline/ ​​​​​​​,300691.0
108570,470407.0,"Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations. Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters. Please refer this link: https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/",314183.0
108369,469158.0,Unsupervised learning is the training of an artificial intelligence (AI) algorithm using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance. ... Subjecting a system to unsupervised learning is one way of testing AI. please refer the link below: https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/,314183.0
108369,468292.0,"When we dont have predefined lables and we want to see how the data can be grouped . Unsupervised learning is the training of an artificial intelligence ( AI ) algorithm using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance eg: You have a bunch of photos of 6 people but without information about who is on which one and you want to divide this dataset into 6 piles, each with the photos of one individual. You have molecules, part of them are drugs and part are not but you do not know which are which and you want the algorithm to discover the drugs. links:https://www.mathworks.com/discovery/unsupervised-learning.html",317822.0
108369,469418.0,"In unsupervised learning, you are not interested in prediction because you do not have a target or outcome variable. The objective is to discover interesting patterns in the data, e.g. are there any subgroups or ‘clusters’ among the bank’s customers?",310419.0
108369,469590.0,"Here we don't have the training set as we had in previous modules. In Unsupervised learning, instead of expecting a desired outcome, we try to explore data and see any interesting pattern emerges out of those.",300685.0
108561,469425.0,"Hi! Labeled data typically takes a set of unlabeled data and augments each piece of that unlabeled data with some sort of meaningful ""tag,"" ""label,"" or ""class"" that is somehow informative or desirable to know. :)",300691.0
108561,469422.0,"Hey Jyotishri, Yes, labelled data is referred to the headers of excel",301655.0
108561,469428.0,As said by both Anuj and Kanika. It can be referred to the headers of excel or the column names of the dataframe since we some times add or divide columnns based on our requirement.,318370.0
108561,469429.0,"Labeled data is a group of samples with one specific meaning or tag. Imagine that you have a dataset consist of the information related to 5 different patients. For each patient you have some different symptoms, and you also have the result of specific cancer test. Based on the result of cancer test, you can set a tag on each patient and specify if she/he has negative or positive test. For each patient &gt; group of various symptoms &gt; one cancer test tag that tag is also the label of your dataset.",318476.0
108561,470559.0,"I think labelled data should not be confused with Excel header/ column names. In supervised learning, you train your data on a train set, where you have a target, which has a label for each entry. After training you fit the model on test data set. This model can be used to predict outcomes. in unsupervised learning you don't have a target variable, you cluster data based on some business parameter. The cluster you find out does not have any specific label. these cluster on close examination can give similar properties of the segment. Data will always have column headers for supervised and unsupervised learning.",318344.0
108580,469526.0,"If we have n data points (11 in this case and provided in the sheet) and k clusters (2 in this case). We have been given the initial centers assigned. We need to find the distance between the each data point and the center-1 and populate it on the empty cells from S6:S16 and between each data point and the center-2 and populate on the empty cells from T6:T16 The formula for the Euclidean distance between (X1,Y1) and (x1,y1) (should be the center in this case) co-ordinates is SQRT(((X1-x1)^2) + (Y1-y1)^2)). The further centers, in fact centroids are calculated based on the previous populated values and continue the iterations",311160.0
108580,470052.0,Hi Rahul refer to the image below. This is how after the several repeated iterations the pattern of clustering changes.. There is If else condition in column KLMN.. please look at it you will understand how the clusters are getting reformed... Hope this helps...,310508.0
108580,469563.0,"For 1st iteration, use this formula for Dist_C1 &amp; Dist_c2: =SQRT(((C6-$H$6)^2+(D6-$I$6)^2)) and =SQRT(((C6-$H$7)^2+(D6-$I$7)^2)).. For 2nd iteration, use this formula for Dist_C1 &amp; Dist_c2: =SQRT(((C22-$H$22)^2+(D22-$I$22)^2)) and =SQRT(((C22-$H$23)^2+(D22-$I$23)^2)).. and so on till SSE becomes fixed (unchanged). Here, in this example, it fixed in 4th iteration, where you can find the centroid. Hope it will help.",311117.0
108903,470523.0,"You can get the procedure and examples here,please follow the link: https://yourbusiness.azcentral.com/attitudinal-segmentation-24107.html",311117.0
108903,470504.0,"In business, we should know about the attitude of customers towards the service or products. Conducting this kind of segmentation allows business to understand existing and potential customers in more detail. Example: 1) Core Customer You should ask your core customers what they like about your products or services. Their answers will tell you what you should emphasize in your advertising and marketing to this group. Your main focus in growing your business should be in marketing your positively perceived products and services. 2) Negative Customer If you find customers who have a negative view of your company, ask them what would change their minds. You may have to develop a marketing campaign just for customers who don't like your company. Your other choice is to count this market segment out of your considerations and spend your money on more likely potential customers. 3) Lifestyle Customers Evaluate whether the customers who like you tend to share a lifestyle. You can target your marketing and advertising toward publications, websites and regional areas where this lifestyle is prominent. By analyzing the segment of the market that has favorable attitudes, you may discover a strong direction to take your company. If customers perceive you as vital to their lifestyle, you could have very loyal customers. 4) Demographic Overlay Examine your favorable customers to see if they share a demographic. You may find a target audience for your marketing based on age, sex, race, sexual orientation or other factors. This will help you hone your message and attract more of these customers.",311254.0
108911,470723.0,Thanks for sharing Srishti.,334535.0
113271,488509.0,"please refer this link , clear analysis is given in both mathematical and logically https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization",318732.0
113271,488514.0,"Standardization (or Z-score normalization ) is the process of rescaling the features so that they’ll have the properties of a Gaussian distribution with μ =0 and σ=1 where μ is the mean and σ is the standard deviation from the mean; standard scores (also called z scores) of the samples are calculated as follows: Normalization often also simply called Min-Max scaling basically shrinks the range of the data such that the range is fixed between 0 and 1 (or -1 to 1 if there are negative values). It works better for cases in which the standardization might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better. Rescaled Value X(i)=X(Original Value)- Min(X)/Max(X)-Min(X) where Min(X) is the minimum value of feature X and Max(X) is the maximum value of feature X In statistics, Standardization is the subtraction of the mean and then dividing by its standard deviation. In Algebra, Normalization is the process of dividing of a vector by its length and it transforms your data into a range between 0 and 1.",311254.0
109378,471809.0,"No. Not happened like this. I finished the module without any interruption. Please refresh the page, if you are facing any difficulty. TA may suggest.",311117.0
109378,471842.0,"Hi Prakash, It is resolved somehow now.",301121.0
109378,471840.0,"Hi, I face the same problem. I am not able to move further. Refreshing the page did not help. This is the page I am in at present. https://learn.upgrad.com/v/course/208/session/25595/segment/131633 Quick support from TA is appreciated.",301121.0
110086,474656.0,"Hey Nirav, I think there really isn't any solid proof answer to this thing. However, you can go through the links given below to concretize your understanding about some other specifics of hierarchical clustering: https://stats.stackexchange.com/questions/3685/where-to-cut-a-dendrogram https://stats.stackexchange.com/questions/2597/what-stop-criteria-for-agglomerative-hierarchical-clustering-are-used-in-practice Hope this helps you!",301655.0
110086,474673.0,"With Statistical convention: The dendrogram can be cut where the difference is most significant. Another technique is to use the square root of the number of individuals. Another technique is to use at least 70% of the distance between the two groups.The next method is to use the function discrimination and classification based on discrimination function. For more deep knowledge, yuo can refer the below link: https://www.researchgate.net/post/How_to_select_the_best_cut_in_dendrograms_of_hierarchical_cluster_analysis",311117.0
109474,472190.0,"Having some variables with a very different range/scale can often create problems, most of the “results” may be driven by a few large values. To avoid such issues, one has to consider to standardize the data by scaling them. Scaling first then clustering will help us to visualize the overall trends in expression levels when comparing different groups. Further details can be referred from these links: https://inseaddataanalytics.github.io/INSEADAnalytics/CourseSessions/Sessions45/ClusterAnalysisReading.html https://www.biostars.org/p/85527/",311117.0
109474,472189.0,"It depends on the type of data you have. For some types of well defined data, there may be no need to scale and center. A good example is geolocation data (longitudes and latitudes). If you were seeking to cluster towns, you wouldn't need to scale and center their locations. For data that is of different physical measurements or units, its probably a good idea to scale and center. For example, when clustering vehicles, the data may contain attributes such as number of wheels, number of doors, miles per gallon, horsepower etc. In this case it may be a better idea to scale and center since you are unsure of the relationship between each attribute. The intuition behind that is that since many clustering algorithms require some definition of distance, if you do not scale and center your data, you may give attributes which have larger magnitudes more importance. You can read more here https://stats.stackexchange.com/questions/30317/reason-to-normalize-in-euclidean-distance-measures-in-hierarchical-clustering",317845.0
109542,472465.0,"Pick the variables one by one from ""Scale data"" and ""Clustering Method"", and Compare the different linkages which gives a well-separated dendrogram.",311117.0
109542,473045.0,You have play around with tha parameters given and see how the clusters are forming. You also need to apply some of your Indian states knowlegde to see which cluster is giving a more realistic picture of the data.,314431.0
109542,472544.0,"After selecting the desired variable and clustering method, number of clusters can be increased by sliding the the pointer from left to right..(from 2 to 20) Dendrogram may not look much complicated when the number of clusters is set at 2. As the number of clusters increase you will find dendrogram gets clumsier for the lnkages Single Linkage and Average Linkage whereas for ""complete linkage"" it is much neater compared to the other two.",301121.0
109388,471832.0,"Hey Khusbu, Here is a link of creating a dendogram in excel: https://help.xlstat.com/customer/en/portal/articles/2062226-running-an-agglomerative-hierarchical-clustering-ahc-with-xlstat?b_id=9283 and, here is another link for creating the same in python: https://plot.ly/python/dendrogram/ Hope that these may help you!",301655.0
109388,471828.0,"How to create Dendogram in excel file: please follow the steps, it is explained here: http://excelgraphs.blogspot.com/2013/04/plotting-dendogram-of-cluster-analysis.html It is excellently explained in our course with python code: https://learn.upgrad.com/v/course/208/session/25595/segment/131632 Draw dendrogram in python manually: Please go through this link: https://stackoverflow.com/questions/6484399/draw-dendrogram-in-python-manually",311117.0
109388,472057.0,Dendograms can be created sby analysing the points and forming clusters. Then the clusters can be grouped in form of slabs forming dendograms,318427.0
109148,471117.0,I am not facing this issue.completed it yesterday only,317982.0
109148,471140.0,"Same here Premnath. It works for me. Can you try CTRL + F5. Also try from another browser, mobile and/or laptop",334535.0
109077,470927.0,This is good thanks.,311857.0
109603,472688.0,kindly check out the below link: https://stackoverflow.com/questions/7404035/how-to-plot-dendrograms-with-large-datasets,311160.0
109603,472749.0,You can cut your dendrogram at different levels to get a better view. You can analyze a particular branch if you use cut smartly.,318344.0
109603,472786.0,"Hierarchical Clustering of 1 million objects(for an example): Can follow two possible approaches: build a hierarchical tree from say 15k points, then add the rest one by one: time ~ 1M * tree depth. first build 100 or 1000 flat clusters, then build your hierarchical tree of the 100 or 1000 cluster centers. You can refer this link also: https://stackoverflow.com/questions/9156961/hierarchical-clustering-of-1-million-objects",311117.0
108853,470321.0,Go to link https://www.clusteranalysis4marketing.com/technical-aspects-cluster-analysis/how-to-run-cluster-analysis-in-excel/,310952.0
108853,470395.0,"Have look on the topic ""Agglomerative Hierarchical Clustering"" in attached link, it will help. https://newonlinecourses.science.psu.edu/stat555/node/86/ Further, you can go through this link: https://stackoverflow.com/questions/18483172/distance-matrix-calculation-and-hierarchical-clustering-for-large-number-of-obse",311117.0
108853,470359.0,"Hey Karthik, I think these links to clustering may help you out : https://people.revoledu.com/kardi/tutorial/Clustering/Distance%20Matrix.htm https://help.xlstat.com/customer/en/portal/articles/2062227-k-means-clustering-in-excel-tutorial?b_id=9283 Hope this helps you out in a much better way!",301655.0
108853,470505.0,"Yes the Euclidean distance of each point from every other point is calculated in Clustering_Activity excel. So distance of point A from B,C,D,E and F is calculated. Similar calculation is done for all other points.",311254.0
108853,470850.0,"it's basically a matrix of distance of each point from each other.. so if we have 5 points, we will have5*5 matrix. like we create correlation matrix wrt to each variable ..",317982.0
108853,471463.0,Apply the following formula in excel to calulate the Euclidean distance between two points =((B4-B5)^2+(C4-C5)^2)^0.5 Where cells B4 and B5 are X1 and X2 cordinates and cells C4 and C5 are Y1 and Y2 coordinates,317514.0
108853,471674.0,"The distance is the euclidean distance calculated by the formula SQRT(((Y2-Y1)^2)+((X2-X1)^2)) where (X1,Y1) and (X2,Y2) are the coordinates of the two points.",318788.0
109666,473211.0,"Hi Jyotishri B, Generally, there are n clusters. In each iteration, the number of clusters gets reduced by 1. Hence, the number of iterations required is n-1. Here, n = number of points = 6. First Iteration= 6-1= 5 Second Iteration= 5-1= 4 Thus, 4 is the correct answer. Regards,",314048.0
109666,472978.0,"Here, we have 6 Point levels, and after every iteration, no. of clusters decreased by (n-1). Hope you will understand this.",311117.0
109666,473051.0,"In Hierarchical clustering (agglomerative) we start with n clusters where n is the number of points, and end with 1 cluster (bottom up tree sturcture). After each iteration, the number of clusters decreases by 1. So here we have 6 points at the beginning which means 6 clusters. so after 1st iteration it decreases by 1 i.e. 6-1 = 5. So, after second iteration, it becomes 5-1 = 4.",314431.0
109666,473053.0,"I understand what you have missed to count. In hierarchical clustering, all the individual items are considered as clusters initially. You seemed to have missed out to count those individual items which are still clusters (cluster with one item only.) After first iteration it would be 5 and after 2nd it would be 4.",301121.0
109732,473287.0,As it is a graded question I do not want to get into much details. Try visualizing the patterns of each cluster if a particular cluster has grouped players with higher average and slightly lower strike rate or lower average and amazing strike rates. You can always merge your player names with the cluster output dataframe to check out which player falls under what Cluster and perform groupby cluster operation,311160.0
109729,473312.0,"df = pd.concat([pd.Series([0,1,2,3,4]), km_clusters_amount, km_clusters_frequency, km_clusters_recency], axis=1) The above line of code is written just to plot the means of RFM WRT clusters. Also, the matching would be completely fine as the km_clusters_amount, km_clusters_frequency, km_clusters_recency were calculated by grouping the clusters and it would clearly group by 0, 1, 2, 3 clusters to find its corresponding RFM means per cluster.",311160.0
109668,474146.0,"Hi Tez, When you have 3 variables, 2-axes scatter plots cannot be used for all 3 variables in one go. You will have to plot two at a time and change to next variable in next plot. Below is a link with good examples. https://rfm.rsquaredacademy.com/",334535.0
109668,473017.0,"Hi, As we know frequency, recency, monitory are extracted information. In K-means, they are individually plotted against cluster variable , which is a bar graph as shown in lecturers.These graphs are analyzed together to understand the behaviour of the target .(Eg, for a retial store it would be customer). In Heirarchial clustering, dendrogram is used ( linkage ), which shows, which data points group together in which cluster at what distance. Finally all points becomare part of a signle cluster Thanks, Kiran",306736.0
109702,473194.0,"You can refer "" Hierarchical Clustering -- Question fun section "", where you can pick the variables one by one from ""Scale data"" and ""Clustering Method"", and can increase the number of clusters by sliding the pointer from left to right(from 2 to 20). Compare the different linkages, where you may get the cleaner data in "" Complete Linkage - Maximum Distance "". Hope this will give some understanding.",311117.0
109702,473105.0,"To find a distance of betweeen matrix of clusters, manually it would take long time and there are methods available in Python as well as Excel to dendrgoram. But if you would like to know the principle, there are three possible linkages Maximum (maximum distance between points in that cluster). Minimum (minimum distance between cluster points) and Average (Average distance between cluster points); A neater version would be Maximum linkage. I hope this clarifies.",301121.0
109702,473331.0,"For computing the distance matrix you can use: from scipy.spatial import distance_matrix pd.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index) But this function will compute the distances using Minkowski distance method. To computes the distance matrix using Euclidean distance , there are multiple ways. One of the way could be: In [12]: df Out[12]: CITY LATITUDE LONGITUDE 0 A 40.745392 -73.978364 1 B 42.562786 -114.460503 2 C 37.227928 -77.401924 3 D 41.245708 -75.881241 4 E 41.308273 -72.927887 In [13]: from scipy.spatial.distance import squareform, pdist In [14]: pd.DataFrame(squareform(pdist(df.iloc[:, 1:])), columns=df.CITY.unique(), index=df.CITY.unique()) Out[14]: A B C D E A 0.000000 40.522913 4.908494 1.967551 1.191779 B 40.522913 0.000000 37.440606 38.601738 41.551558 C 4.908494 37.440606 0.000000 4.295932 6.055264 D 1.967551 38.601738 4.295932 0.000000 2.954017 E 1.191779 41.551558 6.055264 2.954017 0.000000 source : https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance_matrix.html https://stackoverflow.com/a/39205919",318730.0
109211,,nan,
109216,471657.0,"Every iteration reduces the cluster size by 1. So according to me it should form 8 clusters. But in case two clusters are of same distance from the other cluster, the cluster size may reduce accordingly.",318788.0
109216,471364.0,For Every iteration the number of cluster reduce by n-1. so in this case this case it will be 8 cluster.,318476.0
109216,471419.0,"Initially, n=10, Since after every iteration, the cluster reduced to (n-1), hence after 1st iteration, it will be, n= (10-1)=9, and then after 2nd iteration, it will be, n= (9-1)= 8.",311117.0
109216,471437.0,"Hi Darshna, If you go with the algorithm, you will be able to devise that for each step we merge clusters in a greedy manner on the basis of Euclidean distance i.e. clusters which are closest to each other are merged to form a single cluster. For every step, you will notice that clusters reduce to (n-1). But in the special case of minimum distance of two clusters being equal, this may not hold true.",318344.0
109216,472527.0,For each iteration number of cluster rdecrease by n-1.So for n-10 it will gives you 8 cluster.,308639.0
112179,483833.0,you might have done wrong calculation on mean,318017.0
115388,498852.0,linkage is applicable when there are multiple data points with in a cluster. this is not applicable for the first cluster formation,300725.0
115388,497392.0,"Hi Nihal, It is an interesting question. The following is the feedback given in UPGRAD, when you answer the first question on Complete Linkaage in the link given below. https://learn.upgrad.com/v/course/208/session/25595/segment/131633 In the complete linkage, inter cluster distance is calculated as the maximum distance between 2 points (one in each cluster). However, the point is assigned to a new cluster basis it’s minimum distance from the clusters.",301121.0
109349,471725.0,"Earlier it was used for marketing related data, but as per a study found: ""In recent years, data mining applications based on RFM concepts have also been proposed for different areas such as for the computer security, for automobile industry and for the electronics industry . Research cases of data mining with RFM variables include different data mining techniques such as neural network and decision tree , rough set theory, self organizing map, CHAID, genetic algorithm and sequential pattern mining"". For more details, please go through the pdf of below-mentioned li nk: http://cdn.intechopen.com/pdfs/13162/InTech-Data_mining_using_rfm_analysis.pdf Another good information.",311117.0
109349,472887.0,"We have practically implemented RFM analysis on our data of supplier and their spend in Banking domain. It was useful in negotiating the prices of the services procured, basis RFM technique and it really helped the organization to bring down the cost.",308439.0
108830,470215.0,"If you see in notebook, it has standardized all the parameters after outlier treatment. # standardise all parameters RFM_norm1 = RFM.drop(""CustomerID"", axis=1)",311117.0
108830,470221.0,"#Actually its standardized : RFM_norm1 = RFM.drop(""CustomerID"", axis=1) RFM_norm1.Recency = RFM_norm1.Recency.dt.days from sklearn.preprocessing import StandardScaler standard_scaler = StandardScaler() RFM_norm1 = standard_scaler.fit_transform(RFM_norm1)",318476.0
112264,484527.0,"Please note that first of all cluster_ID is not numerical. at all. It would be a good idea to plot cluster vs cluster-wise mean of original variable by which you will be able to make some meaning out of that. The list you have has already grouped the countries.as clusters based on some criteria, whatever you have done in your code. You will be able to make out from the countreis in each cluster, whether it really meant to be in that cluster.. For example, if Afghanistan and Norway are in the same cluster, then there is something wrong . I hope this clarifies.",301121.0
112264,485673.0,"By the looks of it, this is a scatter plot. However, for this assignment, a bar plot between cluster ID and mean values of variables could be shown. If you have that, it should suffice.",308435.0
112435,485247.0,"RFM dataframe would have undergone the outlier treatment and the index of it wouldn't in order. something like 0,2,3,4,6,8 etc if row # 1,5,7 have been dropped as part of outlier treatment. len(RFM.index) would give a length of the RFM dataframe (total number of rows present in the current DF) RFM.index = pd.RangeIndex(len(RFM.index)) would get the index range back in order. (For the above example the o/p would change to 0,1,2,3,4,5 This would help you in not getting NaNs because the pd.Series(model_clus5.labels_) would have the same index and can be concatenated without NaNs",311160.0
109324,471601.0,"Hey Kaushiki, CustomerID column contains the ID of each and every customer which is unique to the dataset. So, if we standardize that particular column we will ruin the unique data which is present in that column. It is not just a number in this case, it is a customer's personal unique identity number. Hope this helps you out!",301655.0
109324,471646.0,"CustomerID basically comes under the category of "" Character "", not in the ""Numeric"" category. It is just like name etc. And in our ""RFM"" analysis, we have to standardize the ""Frequency, Amount and Recency"", which are Numerical values. Hence, we are dropping this. Hope this will help to understand.",311117.0
109322,471603.0,"Hey Kaushiki, I had also asked the same question sometime ago. Here, is the link of the same question and various answers posted by our cohort: https://learn.upgrad.com/v/course/208/question/108872 Hope this helps you, to understand the concept in a better way!",301655.0
109322,471641.0,"I have already replied the same here : https://learn.upgrad.com/v/course/208/question/108872 IQR= interquartile range ( IQR is one of the myriad of methods to spot an outlier ). For details, please refer the link: https://stattrek.com/statistics/dictionary.aspx?definition=interquartile%20range Why it is multiplied by 1.5?: The 1.5 multiplier is so that a certain proportion of the sample in a normal population will be outside it. You can find the discussion on this in below link: https://stats.stackexchange.com/questions/73324/when-finding-outliers-from-the-interquartile-range-why-i-have-to-multiply-by-1-5",311117.0
109328,471660.0,"max_iter: Maximum number of iterations of the k-means algorithm for a single run in case it doesn't converge on its own. that means the K-means algorithm will definitely converge in to fit in 50 iterations in this case. and n_clusters=3 , init='random', n_init=10 , shows that we have 3 clusters here, and n_init in 10 consecutive runs. As "" init "" is "" random "" here, so it may go beyond 10 also. K-means: How to use max_iteration, please go through this link for more understanding: https://stackoverflow.com/questions/40907775/scikit-learn-kmeans-how-to-use-max-iter",311117.0
109328,471635.0,"As you are aware that K-Means functions by re-assigning the Center to the point where the data point's centroid within that cluster is resulted and the data points would change/remain to the cluster based on the distance from itself to all the other centers. This process is continued untill the centers would no more change (meaning, it is converged). max_iter = 50 is assigning 50 as the cap to this kind of iteration. If the model converges before 50 it stops then and there. In case it does not, it stops after the 50th iteration.",311160.0
108872,470409.0,"IQR= interquartile range ( IQR is one of the myriad of methods to spot an outlier ). For details, please refer the link: https://stattrek.com/statistics/dictionary.aspx?definition=interquartile%20range Why it is multiplied by 1.5?: The 1.5 multiplier is so that a certain proportion of the sample in a normal population will be outside it. You can find the discussion on this in below link: https://stats.stackexchange.com/questions/73324/when-finding-outliers-from-the-interquartile-range-why-i-have-to-multiply-by-1-5",311117.0
108872,470408.0,"IQR Meaning When a data set has outliers or extreme values, we summarize a typical value using the median as opposed to the mean. When a data set has outliers, variability is often summarized by a statistic called the interquartile range , which is the difference between the first and third quartiles. The first quartile, denoted Q 1 , is the value in the data set that holds 25% of the values below it. The third quartile, denoted Q 3 , is the value in the data set that holds 25% of the values above it. The quartiles can be determined following the same approach that we used to determine the median, but we now consider each half of the data set separately. The interquartile range is defined as follows: Interquartile Range = Q 3 -Q 1 With an Even Sample Size: For the sample (n=10) the median diastolic blood pressure is 71 (50% of the values are above 71, and 50% are below). The quartiles can be determined in the same way we determined the median, except we consider each half of the data set separately. Figure 9 - Interquartile Range with Even Sample Size There are 5 values below the median (lower half), the middle value is 64 which is the first quartile. There are 5 values above the median (upper half), the middle value is 77 which is the third quartile. The interquartile range is 77 – 64 = 13; the interquartile range is the range of the middle 50% of the data. ---------------------------------------------------------------------------------------------------------------------------------------------------------------- With an Odd Sample Size: When the sample size is odd, the median and quartiles are determined in the same way. Suppose in the previous example, the lowest value (62) were excluded, and the sample size was n=9. The median and quartiles are indicated below. Figure 10 - Interquartile Range with Odd Sample Size When the sample size is 9, the median is the middle number 72. The quartiles are determined in the same way looking at the lower and upper halves, respectively. There are 4 values in the lower half, the first quartile is the mean of the 2 middle values in the lower half ((64+64)/2=64). The same approach is used in the upper half to determine the third quartile ((77+81)/2=79). The Reason for chosing 1.5 IRQ is to remove the outlier as in K-meean Clustering the centroids changes due to the preseence of outliers.",318476.0
109696,473320.0,"Try merging the player name column from the loaded dataframe with the clustered dataframe to see the corresponding player name against the generated clusters. Else, after merging player name column from the loaded dataframe with the clustered dataframe, extract the the resulting dataframe to csv using .to_csv and apply filters on the cluster ID column.",311160.0
109696,473177.0,please check the class sample file which was covered in the k mean cluster. you will find out that after the cluster for formed we have played their mean. Instead of mean u can print all the element of the cluster. hope this helps.,318476.0
109351,471720.0,"Robotic process automation (RPA) implementation is a process with defined steps. Some critical junctures that are required to achieve process automation implementation in any organization. The industry currently follows an Agile methodology to implement RPA, with some customization depending on business needs. Critical Juncture steps are as follows in the link: https://www.cutter.com/article/4-critical-stages-rpa-implementation-496651 RPF Analysis : RPF analysis is basically used to develop a Proposal Plan to execute the process. You can refer this link : https://info.aockeysolutions.com/blog/articles/how-to-develop-a-proposal-plan-part-2-rfp-analysis Its a good topic raised for new knowledge. I also learned something new.",311117.0
109659,472970.0,"The Hopkins statistic is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution. How it is Implemented : We can conduct the Hopkins Statistic test iteratively, using 0.5 as the threshold to reject the alternative hypothesis. i.e. if H &lt; 0.5, then it is unlikely that datset has statistically significant clusters. If the value of Hopkins statistic is close to 1, then we can reject the null hypothesis and conclude that dataset is significantly a clusterable data. You can refer this link also: https://stats.stackexchange.com/questions/332651/validating-cluster-tendency-using-hopkins-statistic",311117.0
109659,472919.0,"Hi , The defintion goes this way The Hopkins statistic (Lawson and Jurs 1990) is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution. In other words, it tests the spatial randomness of the data. link for the same https://www.datanovia.com/en/lessons/assessing-clustering-tendency/ Wiki answers It acts as a staticistical test, where null hypothesis is such that data is uniformly distributed( but random). Hence, 1) A value close to 1 tends to indicate the data is highly clustered, 2) random data will tend to result in values around 0.5, and 3) uniformly distributed data will tend to result in values close to 0. Thanks, Kiran",306736.0
109329,471633.0,"Yes, The number of clusters should be same as the number of centroids. However, n_int = 10 does not mean that 10 points are assigned as initial centroids. It is the number of times (10 in our case) the K-Means algorithm would be run by assigning different centroids (3 in our case) each time it runs. Eventually it would stick to the iteration where it had got the best centre (with least Sum of Squared distance between all the points to corresponding center).",311160.0
109329,471651.0,"Centroid is the average of data points in a cluster . Hence, the number of clusters should be same. In our case, n_cluster = 3 , so centroids will be 3 in numbers. and, n_init is clearly saying "" the number of time the k-means algorithm will be run with different centroid seeds"". Hence, the final results will be the best output of n_init in 10 consecutive runs.",311117.0
108395,469117.0,"Hi, Kindly share the error screenshot orelse connect to your mentor. Thanks!",319006.0
108395,469324.0,It goes to the page which says following: The page you are looking for does not exist.,317689.0
108395,469330.0,You can download the same dataset (Online Retail) used in logistic regression module in industry example. This dataset is the same.,317689.0
108395,469769.0,i was looking for its answer. thanks,317156.0
108395,470057.0,The python file is given on the next page.,311857.0
108395,470955.0,It is redirecting to home page. Not downloading the dataset,301108.0
109223,471363.0,As mentioned earlier part of the session first we need to download the kmode packages to anaconda. By default it's not present in the anaconda . Use conda install kmode or pip install kmode .,318476.0
109223,471415.0,you have the module on k modes its optional one just after hierarchical clustering.,318017.0
109223,471420.0,"For downloading kmode in Anaconda , you need to click on the round green circle close to the '+' to deactivate it before the package can be found in the list.",311117.0
109223,471451.0,"Hey Ranjana, I think it is an error popped up by the Python because you must not have installed that particular package/library in your system. You can do that by running the following command on your system: pip install kmodes Hope this helps you out!",301655.0
111686,481133.0,Select based on the value of the cluster df[df['Cluster'] == 1] The above will give you all the records with Cluster Number equal to 1.,313826.0
111440,480095.0,You can find the Python Notebook containing all the commands used in the session in the following link: https://learn.upgrad.com/v/course/208/session/25594/segment/131625,302738.0
111440,480151.0,Please check the link given below especially for K-means clustering after PCA which may give you an idea. https://www.kaggle.com/kkooijm an/pca-and-kmeans Hope this is helpful.,301121.0
108806,470230.0,A good link for data preparation and outlier analysis: https://www.kdnuggets.com/2017/06/7-steps-mastering-data-preparation-python.html/2,318476.0
111857,481911.0,"In general, if you have a dataframd df with columns col1,col2 and col3, to apply standardscaler only to col1 you could do the following from sklearn.preprocessing import StandardScaler scaler = StandardScaler() df[['col1'] = scaler.fit_transform(df[['col1']])",313826.0
111857,481877.0,What about the column Player Name? It is a required column. How do I handle that?,303085.0
111857,481869.0,"Since the data is of string type, you will not be able to apply standardization. Either clean the column to have only numeric values or drop the column and then run the standardization.",313826.0
115399,497479.0,"Your question is from RFM # Finding max data maximum = max(recency.InvoiceDate) # Adding one more day to the max data, so that the max date will have 1 as the difference and not zero. maximum = maximum + pd.DateOffset(days=1) recency['diff'] = maximum - recency.InvoiceDate recency.head() Please note that the value of maximum is in the date format The idea here is to find the difference between Maximum and recency.Invoice.Date for each row. If you have simply done this without this step maximum = maximum + pd.DateOffset(days=1), you will get zero for many rows where 'invoice date' is same as maximum. To do that you can not simply add ""1"" to maximum since maximum is in the date format. Hence pd.DateOffset(days=1) is used instead of just 1. A new column 'diff' is created in the dataframe recency recency['diff'] = maximum - recency.InvoiceDate CustomerID InvoiceDate diff 17850.0 2010-12-01 08:26:00 374 days 04:24:00 I hope this clarifies.",301121.0
109455,,nan,
114455,494525.0,"Hi Soumik, Looks like, mathematically too it works, and otherwise you won't be able to run the algo on categocial data. Below is an example regarding this. However, silhouette is generally used more with k-means. The usage for categorical data may vary depending on use case and domain in question. https://yurongfan.wordpress.com/2017/02/04/a-summary-of-different-clustering-methods/ This is an additional link. https://www.listendata.com/2016/01/cluster-analysis-with-r.html",334535.0
110727,476694.0,Please go through this link for the time being. We are planning a live session on coming Wednesday where we can go through this specific example. http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm,313517.0
114860,496791.0,"I think K-means being easy to understand and more intuitive is used to demonstrate clustering. In future projects, we can use the K- prototype as well. Mathematics underlying K-prototype is not that easy to understand.",318344.0
112456,485226.0,"As confirmed by TAs, We'll have to perform PCA first and then carry out Outlier Treatment on the PCA components. K Means clustering and Hierarchical Clustering should be performed on PCA generated DF as PCA would reduce the dimensionality and the correlation. Additionally, you can also check the below response from TA: https://learn.upgrad.com/v/course/208/question/111624",311160.0
115318,496826.0,"Hi, Not sure how to ascertain it directly, but the one way I used was using ""inverse_transform"" &gt;&gt;&gt; le.transform([""A"", ""A"", ""B""]) array([2, 2, 1]...) &gt;&gt;&gt; list(le.inverse_transform([2, 2, 1])) ['A', 'A', 'B'] This way you can get a map of your encoded values. -Soumik",305839.0
115318,496867.0,Checkout the below stackoverflow answer https://stackoverflow.com/a/42889046,313826.0
131744,575212.0,"Yes, We need to consider records for Nov and Dec 2017 only for the analyses.",313826.0
131744,575224.0,Yes we need to consider only record in Nov and Dec 2017. Any record outside this time period should be filltered.,318476.0
131645,574946.0,Put a check in the where condtion trip end time is greater that trip start time and then do the speed calculation,318476.0
131705,,nan,
131754,575367.0,"Hi Bharti, As suggested by Vipul, you must have changes made to the table in order to delete a record. Alternate solution without changing table properties: I can suggest an alternate method as well since we generally create the table and load data into it. Consider loading only the filtered data in the fresh table (by using where clause).",317987.0
131754,575280.0,Make sure you follow below steps: To do delete/update operations in Hive table File format must be in ORC file format with TBLPROPERTIES(‘transactional’=’true’) Table must be CLUSTERED BY with some Buckets. Note: if table is not bucketed then you will get FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table that does not use an AcidOutputFormat or is not bucketed. You can also visit below links to get more insight https://learn.upgrad.com/v/course/208/question/131116 - go through comments also,317991.0
131451,573782.0,I dont' think there is one. I manually copied the code to a text file and submitted it.,310974.0
131451,575018.0,"Nope there isn't any. Corestack forces you to download the file in JSON format (see the screenshot) when you save your query. so just copy all the commands on the query editor on to text file. You can use Online SQL formatter such as ""http://www.dpriver.com/pp/sqlformat.htm"" to remove clumsiness in the code and format it to be readable",309211.0
131451,573988.0,"On corestack, so far I haven't seen any option to download hive notebook to .txt file. There is no better way but to copy paste all the code into text file. And if it is there then only corestack guys can tell.",317991.0
131016,572069.0,"Hi Vipul, Thanks for the quick reply. I am mentioned the columns as per the given in csv file. Table is created successully. But while checking the data getting error. Below is my samle code -- create external table if not exists ny_taxi_trip_tabl (VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, ..... ) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths' = '') location '/common_folder/nyc_taxi_data/'; -- table created successfully Getting error while selecting data using below query -- select * from ny_taxi_trip_tabl limit 5;",304812.0
131016,572037.0,"Check wether you have performed below steps correctly or not : 1. While creating table make sure the sequence of column in create table and data file is same. And also do not use Json Serde since the file is in csv format. 2. After creating table check whether the data is populated correctly or not. 3. Then while running any queries make sure the column names, where clause condition or any other condition are correctly written. Also look out for opening or closing brackets () and quotes ''. Once you follow all these correctly, I think your problem will be solved. If problem remains there then do let me know.",317991.0
131016,572102.0,"Hi Vivek, As Vipul mentioned, since this is a CSV file, writing ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths' = '') would definitely cause errors on the read. We need to write the query for source as CSV &amp; not a JSON Hope this helps!",318355.0
131016,572107.0,oh..got it.. Thanks a lot Aditya..,304812.0
131436,573619.0,Please ignore. Solved the issue. Thanks.,304812.0
131436,573604.0,You can create table at below location while mentioning location in create table statement. location '/user/hive/warehouse/give_your_partition_folder_name' In place of give_your_partition_folder_name you can specify any name. Hope this will help.,317991.0
131739,575149.0,Agree these values cannot be negative,317514.0
131739,575233.0,Yes ..You are correct,318476.0
131316,573202.0,"This problem is coming because you have not skipped the header row. This can be done using below code in create table statement: tblproperties (""skip.header.line.count""=""1""); Hope this will help.",317991.0
130572,570363.0,"Yes, I faced the same issue. Corestack platform certainly appears to have some issues when more learners logon to the platform. You will have query history wherein the same query would have been successful. Raise a Support ticket and don't forget to attach appropriate screenshots about your issue. Few tips based on my experience so far with Corestack: 1) Whenever support tickets are raised, I get a feedback : "" This is a query related issue. We do not support any query related issue. Please contact your trainer. There is no infra related issue and the environment is stable ."" However, such response was obtained even after attaching appropriate screenshots from the same platform wherein query had ""successfully executed earlier"" and ""erroring out during peak periods when more learners had logged on"". I had attached both these screenshots and still got to see the comment as given above. In such instances, please attach appropriate screenshots and copy your mentor if problem persists and no resolution provided by support team at Corestack. If issue is resolved and your query executes fine (and if yet you're told that your query is incorrect then ignore their comments) 2) Alternatively try out your query on single node cluster distribution from Cloudera/other distributions which it appears that you did. If query runs successfully, test out your query on Corestack when your query runs ""Reasonably faster"" during offf-peak hours on weekdays. I can't think of a better way of testing it out on Corestack during holidays / peak hours as their env does not appear to be well scaled to handle lot of learners at once",309211.0
130676,575181.0,"i tried both bucketed and orc, still iam unable to delete",300725.0
130676,570568.0,"As per https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions, table must be bucketed and in ORC format to be able to delete the data. Is the data bucketed and in ORC format? source: https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions",309211.0
130999,573065.0,"Yes, took me a while to figure out almost lost a day.",311857.0
130999,571989.0,Thanks for confirming. I guess Upgrad team should take a note of this and ensure that people are not struggling with this assignment due to wrong data dictionary,306250.0
130999,571985.0,Yes the sequence of columns in Data dictionary and the data file uploaded on HDFS is different. To create table in correct sequence I would suggest follow the sequence present in the csv (data) file. By doing this you will not face sequence mismatch problem. Hope this will help.,317991.0
130999,571953.0,"Hi Ranjay, I just verfied the columns sequence in the csv file &amp; the data dictionary. You're absolutely correct that the sequence in the data dictionary is different from the data present in the csv file. Yes, the Hive table needs to created as per the sequence of the columns in the CSV file. Creating the table with incorrect sequence would totally make the analysis incorrect. I would say always create the table as per the sequence of the columns in the data file. This would definitely eliminate this type of error. Hope this helps!",318355.0
131399,574644.0,"Yeah, worked for me too. Have to logout from the login page and hue platform.",318481.0
131399,573491.0,"Most of us facing same problem, since day after tomorrow is deadline so many of us trying to access corestack lab at same time and server is crashing because of this. And we all are seeing this error.",317991.0
131399,573549.0,Log out from corestack login page and hue platform and login in again to both. I had also faced the same error and this ersolved it for me.,313826.0
131727,575378.0,"Hi Sahil, Alternate solution without changing table properties: I can suggest an alternate method as well since we generally create the table and load data into it. Consider loading only the filtered data in the fresh table (by using where clause).",317987.0
131727,575157.0,"You can directly delete rows in hive, but you can do update operation",318429.0
131638,575156.0,Trips where extra&gt;0 / Total no of trips,317514.0
131638,575586.0,"For this question, we need to explore the variable 'Extra' and write queries to find what is the fraction of all the trips for which an 'Extra' Charge was levied. note: Extra Charge amount can NOT be negative. Hope that helps.",317998.0
131691,574831.0,"Error while compiling statement: FAILED: SemanticException [Error 10014]: Line 3:111 Wrong arguments 'tpep_pickup_datetime': No matching method for class org.apache.hadoop.hive.ql.udf.UDFFromUnixTime with (timestamp). Possible choices: _FUNC_(bigint) _FUNC_(bigint, string) _FUNC_(int) _FUNC_(int, string) This is the insert query I am using.",303228.0
131691,574844.0,is tpep_pickup_datetime declared as string or timestamp?? if string then check for the UDF which accepsts string to give unix time.,301115.0
131029,573802.0,"try switching decimal(10,2) to double. This helped me :)",308962.0
131029,572049.0,"Issue looks at the end part of the code, same error happened with me when I executed only the comments",301115.0
131029,572097.0,"Hi Bindu, Found the solution to this problem. I did some runs, below are the screenshots: First, with the error message Without any error message So, to conclude if I write comment as: -- This is a comment Would work well &amp; won't give any error --This is a comment Would not work &amp; give the same error that we're getting in the above screenshots Always write comments with space after '--'. Hope this helps!",318355.0
131029,572399.0,"What Aditya explained is one of the reason. Also, try giving "" ; "" at the end of the query.",329936.0
131029,572417.0,Missing ; at end.,318495.0
131030,573187.0,I get 1174570 as row count.,310210.0
131030,572055.0,"Yes, the total number of records are 1174569 only.",317991.0
131039,572269.0,Try Bucketing.,301648.0
131040,572117.0,"IF pick up happens in midnight then drop of will be in next day only,so its still valid , correct travel time can be doubted if its way beyond the usual hrs/min for a ususal ride (more than half a day)",301115.0
131040,572416.0,Yes this will always give correct answer provided dropoff time is greater than pickup time.. else there might be some error with the data. Hope that helps.,318495.0
131644,575185.0,"Log in and log out, issue will resolve",318429.0
131644,574712.0,The corestack platform was offline then and it has now started working.,313691.0
131593,574392.0,Remove double from the percentile_approx function,311254.0
131593,575605.0,"The guide lines just give a Hint as to which direction to go in. It is not the exact syntax. The ""DOUBLE"" mentioned in percentile_approx(DOUBLE col, p) means that the column 'col' should have the datatype of DOUBLE. and 'p' implies the percentile that needs to be found. A syntactical example would be as: round(percentile_approx(tip_amount,0.25),3) which would give the 25th percentile of the column 'tip_amount' which has a Datatype of Double. Thus, the only erroneous part in your syntax is the word 'Double', which should not be there. Hope that helps.",317998.0
131740,575155.0,"Yes this is one of the data quality issue, and you need to find out more with EDA",318429.0
131740,575147.0,Need to report all unusal and erroneous rows in the dataset,317514.0
131068,572804.0,what checks required for removal of rows ? we should remove if passenger_count is 0 and any other checks do we needs to do ?,312019.0
131068,572272.0,"No, I got 98% data rentention after removing erroneous data.",310974.0
131068,572404.0,"It should be more than 50%. You also need to identify and explain the erroneous values. so, be cautious while deleting them.",329936.0
131068,572412.0,This is not as expected.. remember during our initial sessions we were taught that we should at least retain about 70% of the data after clean up. Also the provided data will be having approx 1-2% of erroneous data. Kindly check again your assumptions.,318495.0
131817,575898.0,"I would suggest you mention your criteria on various columns how you draw your conclusion; There is nothing like absolute right or wrong answer but your reasoning would take you to the right direction,",301121.0
131817,575633.0,We just have to make conclusion that which vendor is doing bad job. I think there is no need to create separate table or column.,317991.0
131387,573467.0,First you can find co- relation between passengers count and tip amount. Then find the average of tip amount for solo passengers and non solo passengers. You can use windowing functions for this activity if you want to do in multiple steps...,311861.0
131387,573500.0,"I think I will just go with calculating Correlation( passenger_count , tip_amount) that includes all the passengers (1,2,3,4,5,6,7 , 9) .. and then calculate Correlation that ""excludes"" solo passengers. Difference of the two would provide some value. If Corr (all passengers, tip amount) - Corr (all passengers except solo, tip amount) is positive then solo passengers don't tip much (does not have impact on tip amount).... if the value is negative then solo passengers tip more ( and their non-presence is impacting the Correlation). TA please confirm if fthe understanding is correct?",309211.0
130787,571666.0,To do delete/update operations in Hive table File format must be in ORC file format with TBLPROPERTIES(‘transactional’=’true’) Table must be CLUSTERED BY with some Buckets. Note: if table is not bucketed then you will get FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table that does not use an AcidOutputFormat or is not bucketed.,329936.0
130787,571783.0,"Hive 1.1.0 supports ACID properties , so deletion is possible. An alternate approach would be not consider these rows (or exclude erroneous rows) while creating partition table.",309211.0
131722,575379.0,"Hi Tanay, Alternate solution without changing table properties: I can suggest an alternate method as well since we generally create the table and load data into it. Consider loading only the filtered data in the fresh table (by using where clause).",317987.0
131722,575006.0,Make sure you follow below steps: To do delete/update operations in Hive table File format must be in ORC file format with TBLPROPERTIES(‘transactional’=’true’) Table must be CLUSTERED BY with some Buckets. Note: if table is not bucketed then you will get FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table that does not use an AcidOutputFormat or is not bucketed. You can also visit below links to get more insight https://learn.upgrad.com/v/course/208/question/131116 - go through comments also,317991.0
131293,573155.0,It would be better to create new column based on logic (to calculate speed) and then use that column if needed.,320103.0
131293,574197.0,"no need to create new column ,run hive query",318005.0
131293,573119.0,"Segregation and speed can be calculated using hive queries. For these calculations, new columns need not be added to the table. However, if you find adding columns and then calculating the values easy , you can add the columns to the table first and then do the calculations.",311254.0
131293,575394.0,"No need to create a new column, it can be selected in hive query by dividing the total_distance/total_time(which is nothing but difference between dropoff time and pickup time) as speed.",318448.0
131103,572343.0,The csv file is already present in the common folder. You need to import the data from there into the table you created.,311254.0
131103,572407.0,Yes we have to Create the hive table using the data from the Excel file which can be found in common folder as mentioned in the assignment instructions. Thereafter you have to apply EDA and further analysis using that table. Hope this helps!,318495.0
131425,575131.0,"yes,, if one vendor contribute 100 records and 50 are erroneous that is worse than the other vendor which conteibute 200 records and 60 erroneous ,,i only had this query",318005.0
131425,574366.0,It's upto you to decide how you do comparision.But you need to mention how you did.,329936.0
131116,572634.0,"Hi Surendra, In order to make it easier for everbody. Follow the steps below for performaing UPDATE/DELETE on a HIVE table. Creating a table Inserting values into the table Updating values in the table Hope this helps!",318355.0
131116,572396.0,"Hi Surendra, In Hive, earlier Delete &amp; Update were not supported. But in the latest release this was made available. In order to use delete/update query on your table the table must be bucketed. Hope this helps!",318355.0
131116,572405.0,Table must be in orc format and also bucketed in order to use the delete/update queries. Please follow the above steps and then try the delete command.. it should work!,318495.0
131701,574876.0,We need to provide the queries through which we are able to answer to the questions.,313826.0
131701,575400.0,We need to provide queries along with its explaination/output in comments in a .txt file.,318448.0
131723,575159.0,yr _pickup you need to derive from pickup timestamp and you cannot use this directly.,318429.0
130907,571571.0,Make sure the order in which the columns appear in the csv file is same as how you give in the create table statement.,310974.0
130907,571614.0,Kindly refer Excel file/ Data dictionary for correct sequence of columns and create table with the columns in same order skipping the first row (header).,318495.0
130907,571780.0,Yes I did and here is what I did to factor the same: Go to common folder location and see how csv ffile is oriented. Just clicking on the filename should show you first couple of rows. Make sure that DDL of the table that you read is aligned correctly with the order of the column name in CSV file,309211.0
130907,572113.0,"Still facing the same issue even the columns are mentioned in same order. ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; create external table if not exists NYC_taxi_table(vendorid int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count string, trip_distance double, RatecodeID int, store_and_fwd_flag string,PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' location '/common_folder/nyc_taxi_data/' tblproperties (""skip.header.line.count""=""1"");",312756.0
130501,569947.0,"It is comma separated file (csv), so you need to use ""row format delimited fields terminated by ','"" in your create table query.",314730.0
130501,570253.0,"As mentioned by Vinod, you are trying to load a CSV file and the code you have is to load a JSON file. Sample code for loading CSV file: CREATE EXTERNAL TABLE IF NOT EXISTS &lt;table_name&gt; (col_1 STRING, col_2 STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION 'location_to_file'; Reference: https://bigdataprogrammers.com/load-csv-file-into-hive-orc-table/",317987.0
130663,571420.0,"Hi Harshendra, Kindly check if the filter for the passenger count greater than zero resolves your problem or not.",317362.0
130663,570977.0,"Hi Abhigyan, still unclear. I have already been excluding Cash/ 0$ tip amount , still seeing NaN while performing Corr ( tip_amount, passenger_count). whenever I use values across two or three groups (say passenger_count 1, 2 , 3 ) , I do not get NaN. When passenger count = 1 and when I run Pearson coeffficient on the rows ""Excluding"" Cash tip or 0$ tips (basically some value in the tip &gt; 0$), I am seeing ""NaN"" indicating data has no variance. what am I doing wrong?",309211.0
130663,574370.0,"I think you can just calculate the co-relation value for tip amount and passenger count. (Excluding the cash payment type). If the value is positive, it means when no if travelers increase the tip amount will also increase else it decreases with multiple travelers.",315028.0
130663,570919.0,"Here we are trying to perform a correlation study between the tip_amount and number of passengers. -- Since this study will be directly impacted with the magnitude value of tip_amount and our dataset encodes tip_amount as $0 for all trips that are paid with Cash or with [payment_type=2] irrespective of the number of passengers. This will distort the correlation value. Therefore, we need to exclude the records with payment_type=2 for this query. Solo travellers = passengers who travel alone.",329936.0
130663,573509.0,"I think I will just go with calculating Correlation( passenger_count , tip_amount) that includes all the passengers (1,2,3,4,5,6,7 , 9) .. and then calculate Correlation that ""excludes"" solo passengers. Difference of the two would provide some value. If Corr (all passengers, tip amount) - Corr (all passengers except solo, tip amount) is positive then solo passengers don't tip much (does not have impact on tip amount).... if the value is negative then solo passengers tip more ( and their non-presence is impacting the Correlation).",309211.0
130505,569921.0,"The partition should be done on the column which helps the dataset to be divided into partitions, such that a frequently used query uses only one or few partitions. Partitioning is done to improve the query performance so that a query can execute without going through each and every record of the entire dataset. Example: If we frequently query our reviews dataset for specific years and months, it will be efficient to partition data based on year and month, and thus our query would execute upon that specific year and month partition and not on the entire dataset. This will make our queries much faster. Note: Upon partitioning, we must keep in mind that we do not end up creating a very large number of partitions (&gt;1000) since it will make the metadata huge and degrade the performance.",317987.0
130505,570367.0,"As per assignment, ""While performing this analysis, you will inevitably make several assumptions. As long as you state these assumptions, you will be awarded marks."" so this applies to partitioning as well. Choose the columns that will get you the results for your analysis queries faster (i,e. without degrading query performance). Also mention why you chose that column for partitioning in comments",309211.0
131885,575978.0,"The error you mentioned FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask occurs because the query you wrote is not correct. Make sure your query is correct, then this error will not come.",317991.0
131885,577833.0,"If you are sure about the query being correct, then try logging iut and logging in freshly on both corestack and hue platforms. Had faced similar issues and this helped.",313826.0
130583,570424.0,"Hi Revati, As we know that amount can never be negative. If the user had some offer &amp; the amount paid is 0 then that would be fine but negative amoun must be treated as erroneous data. Moreover, this could be stated as one of the assumptions as we need to report the assumptions that we make, so that our analysis is interpreted in a correct way. I would request TA to confirm on this. Hope this helps!",318355.0
130583,570451.0,"Whatever assumptions you make, please state it before writing out the query. If you consider to have the negative values for amount within your dataset (that would be an assumption and call it out with appropriate comments mentioning whatever you just did in your question). If you decide not to have these rows in your dataset , that would be an assumption too and your results will be based on that assumption. In my opinion there is ""no right "" or ""wrong"" analysis derivatives since there are couple of variants in the answer and all the answers are based on assumptions you make. Just make sure to mention the assumption you made along with your query. I think this is why they made this aspect very clear : ""While performing this analysis, you will inevitably make several assumptions. As long as you state these assumptions, you will be awarded marks.""",309211.0
130583,571468.0,Negative numbers should be removed.,318756.0
130583,571377.0,"Even with coupons the amount cannot be negative, at the least it could be 0 as mentioned by Aditya. Think about it........negative implies the driver gave the money to the ride.",311857.0
131394,574184.0,Refresh again and again it would run.,308639.0
131394,573473.0,Try logging out from corestack and hue platform and logging in again.,313826.0
131394,573492.0,"Most of us facing same problem, since day after tomorrow is deadline so many of us trying to access corestack lab at same time and server is crashing because of this. And we all are seeing this error.",317991.0
131394,575371.0,"Yes, we do face this error at times. Reason: It's a Gateway Timeout Error. You will see this error mostly when the platform is either down for maintenance, or is overloaded due to many users.",317987.0
131150,572636.0,"Hi Sourodeep, I would recommend you to follow my answer on this : https://learn.upgrad.com/v/course/208/question/131116 Its mentioned in detail on how to perform ACID operations in HIVE. Hope this helps!",318355.0
131150,572609.0,"The format above you used to create table is only for bucketing. To create table in ORC format you need to specify two things in create table statement: stored as orc location '.............' tblproperties (""orc.compress""=""SNAPPY""); You can also go through the module once again to create ORC partitioned table. https://learn.upgrad.com/v/course/208/session/37072/segment/197855 Hope this will help.",317991.0
131150,575372.0,"Hi, Alternate solution without changing table properties: I can suggest an alternate method as well since we generally create the table and load data into it. Consider loading only the filtered data in the fresh table (by using where clause).",317987.0
130851,571438.0,You should run the analysis on the partitioned one as the original table was having erroneous data and taking those in consideration will beat your purpose of partitioning and thus the wrong output/analysis.,318495.0
130851,573293.0,"the percentage or fraction of total trips have an extra charge is levied needs to be calculated directly using a query, or can be done manually offline after getting the counts from query?",316036.0
131161,572669.0,"tblproperties (""skip.header.line.count""=""2""); It should be 1 instead of 2 as we need to skip header which is just 1 row and not 2.",320103.0
131161,572642.0,"Hi Bindu, The part of the query which you have shared seems absolutely fine. However, there seems to be an issue with the whole query. It would be great if you can share the whole query or match the query format with the one mentioned below: CREATE EXTERNAL TABLE table_name( column1 columnType, column2 columnType, column3 columnType, column4 columnType ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 'file location' tblproperties (""skip.header.line.count""=""1""); Hope this helps!",318355.0
131305,573371.0,"You can check for the difference values and there count , you have for these extras. You'll get an idea as what to keep and what not. Dropping 50% of data is not at all advisable but keeping unnecessary data is also not the solution. As this is graded assignment can't help you much but yes consider 0 as well if that helps and if you keeping any data other than the mentioned values in dictionary make sure you are stating that as a comment since that's an assumption. Hope this helps.",318495.0
131308,573205.0,It depends whether you want to consider those data as erroneous or not erroneous. Just make sure whatever assumption you are making do mention it in comments.,317991.0
131308,573368.0,Well logically that trip never happened. But yes as Vipul said it's totally up to you to consider this as error data or not. Make sure you state your assumptions to be on safe side.,318495.0
130885,572750.0,The question asked is average fare amount per trip so we have to do (total fare_amount / total number of trips) for a month.,314730.0
130885,571776.0,"Fare amount is the time and distance fare calculated by meter. It however does not include tax, tip etc that adds on to the Total amount. ""My Assumption"" or interpretation for this question is to compute this average upon the total amount (i.e cost incurred by a customer). You can assume anything and derive your analysis as long as you mention that assumption in comments",309211.0
130885,571573.0,I think you should go with fare_amount column. TA should confirm though.,310974.0
130885,571618.0,"Since they haven't mentioned which one needs to be considered, my guess is we can take anyone is them but since it's an assumption so better to put this as remark/comment in the notebook. I'll prefer to go with fare_amount. But it would be great if TAs can confirm on the same.",318495.0
130888,,nan,
131317,,nan,
130655,570502.0,"Don't match count with excel sheet as the data is large which excel can't support, so excel is not displaying all the records and count will not match.",320103.0
130655,570508.0,"Hi Nitu, Even I checked, the following are my findings: Created the table on the file in the common folder -&gt; Record count: 1174569 Create the table by uploading the file to my folder -&gt; Record count: 1174569 Count in the Microsoft excel of the given CSV file -&gt; 1048575 In conclusion, the record count is not matching. Someone, please verify the count of records.",318355.0
130655,570530.0,"Hi Harshendra, I tried running the command provided but its giving the error as shown in the screenshot below: It would be great if you can help on this. Thank you!",318355.0
130655,570522.0,"Hi Nitu, Best way if you have Unix/ Mac , go to command line and the folder where file is present and run this : (base) C02VR17PHTD6:Downloads harshendrapiriyapatt$ wc -l yellow_tripdata_2017.csv &lt;&lt; &gt;&gt;&gt; if you have WINDOWS, DOS prompt should tell you the number of lines using this (replace file.txt with complete path) : findstr /R /N ""^"" file .txt [ source : http://brett.batie.com/scripting/count-number-of-lines-in-a-file-using-dos/ ] Note: Do consider that there is a header line as well in the file when you are checking the total number of lines , record count would be total_lines - 1",309211.0
130655,570535.0,"Hi Nitu, The number of rows in the csv file are same as we're getting in our Hive tables. Thank you so much Harshendra &amp; Khusbu for the information. Below is the screenshot of the record count in the csv file with command provided by Harshendra. The total count including the header is coming out to 1174570 which means we have exactly 1174569 records. Hope this helps!",318355.0
131199,572791.0,There is some environment setting problem. Earlier the same query was running on other table like bolloywood is also now not working . Do anybody face similar problem and able to resolve it?,318585.0
131199,574247.0,"Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask Followed the steps to clear the trash but no success. Definitely there is some environmental issue.",312199.0
131199,572808.0,"Similar question has been answered in below links. Have a look into these links and apply the solution given in these link, https://learn.upgrad.com/v/course/208/question/130114 - Go through comment also https://learn.upgrad.com/v/course/208/question/130007 https://learn.upgrad.com/v/course/208/question/130184 https://learn.upgrad.com/v/course/208/question/130459 Hope this will help",317991.0
130665,570857.0,"As per problem statement, "" Before answering the below questions, you need to create a clean, ORC partitioned table for analysis. Remove all the erroneous rows."" so there you have it. You need to create a clean, ORC partitioned table for analysis and ""exclude"" erroneous rows from your analysis. You can either induct ""all of the data"" and then perform delete on it [ I just tried deleting value from bucketed ORC format and it is possible. More details on wiki page : https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions, table must be bucketed and in ORC format to be able to delete the data ] or not induct bad data at all [ and avoid bad data right at the source] . The former is roundabout route (induct and delete) but entirely possible as ACID transactions are supported from Hive 0.14 . [ Corestack runs Hive 1.1.0 ]",309211.0
130665,572755.0,"Hi Naga, In order to add on to the Null check. Kindly follow the steps mentioned in the below screenshots. Checking if column 'lead' contains null. To check the ful table you might remove the limit. Instead I would suggest getting the count of null values rather than the rows because getting count would take much less time than getting the rows. As we see in the below screenshot, the results returned are 0. Therefore there are no null values Example where nulls are present in the column 'rdate' Getting the count of number of null values in the column 'rdate' Hope this helps!",318355.0
130665,570510.0,"Hi Shubham, In my opinion the data cleaning procedures must be same irrespective of where the data is actually residing. In order to remove the invalid rows, we can: Create a new table with cleaned data Put the conditions in the where clause of our query to filter out the correct data",318355.0
130688,570853.0,"Hi Vinod, Same line also asks us to check if data is consistent as per what is specified (month of November/December only). If on performing EDA you identify there are rows for other months or other years, that is data quality issue and should be cleaned up The data provided is for months November and December only . Check whether the data is consistent, and if not, identify the data quality issues. Mention all data quality issues in comments This line mentions it too ; Before answering the below questions, you need to create a clean, ORC partitioned table for analysis. Remove all the erroneous rows. it says ""Remove all erroneous rows"" . So on performing EDA if erroneous rows observed (not just for this column but any column, then it is data quality issue and needs to be dealt with and removed before proceeding with your analysis",309211.0
130688,571336.0,"We can also look for pickup month and drop off month to be November and December. Something like this : select count(*) from nyc_taxi_table where month(tpep_pickup_datetime) in (11,12) and month(tpep_dropoff_datetime) in (11,12) ;",318741.0
130688,570667.0,"Hi Vinod, To add on to your question. The data is present since 2003 to 2018 as seen in the screenshots below: I think we need to filter data for the November &amp; December 2017. I request the TA to verify this.",318355.0
130688,571974.0,"Data is mixed of month October,November and December &amp; year is starting from 2003 to 2018. Should we filter by month of Nov,dec 2017 during analysis. TA please confirmed.",320689.0
130853,571575.0,"Make sure you are writing to the directory that your user owns. If that is correct, try it again later?",310974.0
130853,571612.0,"As per snapshot, you can see that you are creating an Orc table in common location. You need to give the location where you have access. We all have one folder with our name ,please create over there.",311861.0
130853,571785.0,I see schema name missing. I saw such errors when I missed schema name. Please prefix table name with schema name and the error disappears.,309211.0
130853,572428.0,"As confirmed by TA, we do not have write permission in common folder. https://learn.upgrad.com/v/course/208/question/130796",318448.0
130861,571429.0,"Get the average tip using avg function and compare it with the tip at 25th percentile, 50th percentile etc. (these can be obtained from UDFs) And you have to suggest if average tip is the right parameter for Central tendency or not. Hope it clarifies.",318495.0
130653,570453.0,"Although I am not a TA, but it is clearly mentioned that "" Certain results may be affected if this is not followed and in that case, marks will not be awarded ."" I don't see any harm in doing so. So I would suggest to follow the guidlines that have been provided in the statement.",317991.0
130653,570492.0,"Hi Vipul, Thanks for your response! Yes, I read the guidelines provided and appreciate you of reminding me the same. We should not just blindly choose the datatype of the columns and that's making me think that if I mark a column which would contain values ranging from 2 to 40 as TINYINT (the range of TINYINT is -128 to +127), how would that affect the results? There are many more similar examples. According to me, these assignments must be done to grab learning from every aspect &amp; I would not end up choosing what is blindly mentioned at least to the extent I can explore. I hope the TA answers this question.",318355.0
130653,570590.0,"I agree with Aditya. We must use data types carefully. However, we must also have some scope of accomodation of future data, especially in Big Data scenario. Example : After considering an ID column, one can decide that we can go with integer. However it is very likely that the ID might exceed the int range in future, and thus using BIGINT is good option. Similarly, if we know that there is a column called dating and its value will always lie between 0 to 10, using TINYINT is good option.",317987.0
130382,569342.0,Shubham - Try Using timestamp instead.,302740.0
130382,569953.0,"If nothing works, try using VARCHAR(255) and have a look at the data once it is loaded. It might be possible that the data itself is not clean or non-standard, in such cases, VARCHAR helps.",317987.0
130382,569965.0,"Try using ""String""",329936.0
130382,569966.0,timestamp worked for me..,314730.0
130382,570365.0,Timestamp works fine for importing the DateTime field,309211.0
130709,571335.0,"Check each column for basic data checks using count and distinct queries to see the values. After that filter out the erroreneous rows. Check for the values of all the charges you might get something unusual there. As mentioned by Harshendra, there could be one or many columns having such problems that causes erroneous rows to get into the analysis and distort the analysis. Exploratory data analytics ( EDA ) is the method to identify such data inconsistencies and tackling the erroreneous data..",318495.0
130709,570862.0,"One of the hint is given in point 3 of "" Basic Data Quality Checks"" ""3) You might have encountered unusual or erroneous rows in the dataset. Can you conclude which vendor is doing a bad job in providing the records using different columns of the dataset? Summarise your conclusions based on every column where these errors are present. For example, There are unusual passenger count, i.e. 0 which is unusual."" there could be one or many columns having such problems that causes erroneous rows to get into the analysis and distort the analysis. Exploratory data analytics (EDA) is the method to identiffy such data inconsistencies.",309211.0
131504,574274.0,"When I ran your query, I am getting same result as it should be First query properly gives distinct values of PULocationID present in the table Second query properly shows min and max PULocationID which can be confirmed from the result of first query. Since you are getting different result then the reason could be that data is not inserted properly in the table that leads to result mismatch. Do cross check wether the data is inserted correctly or not. If you are varifying the result with csv datafile then don't compare it, because csv file is too big and it is not showing all the data. Hope this will help",317991.0
131504,573878.0,The order issue is because of the order of data present on the hdfs . Here below i am attaching the dataset screenshot for your reference. Please make your query in this order. Hope this helps!,318427.0
131681,574943.0,"Hi Jay, The ratecode vaild values are (1 to 6) any values outside this rate code can be consdired as error in the data. Yes its help in the trip fare but we don't need to find this relation as part of any analysis qusetion asked.",318476.0
130679,570580.0,"Please ignore the same. I found the answer myself on Stack Exchange. Source: https://stackoverflow.com/questions/44043932/what-is-the-difference-between-and-in-hive The = and == operators have the same functionality. They are implemented by the exact same class. system.registerGenericUDF(""="", GenericUDFOPEqual.class); system.registerGenericUDF(""=="", GenericUDFOPEqual.class);",309211.0
130679,571342.0,"Even in SQL, '=' is used for comparison with a where clause but can be used as assignment in UPDATE TABLE SET clause.",304319.0
131719,574984.0,It depends on the assumptions that you have made regarding this attribute and various other attributes. Clearly mention the assumptions as comments and take actions accordingly.,313826.0
131719,575021.0,"ratecodeid = 99 is not present as per dictionary, exclude it ; fare amount &lt; 0 is also very unusual ; please mention these point as assumptions if you plan to exclude it. if you include it, it will skew the analysis and be sure to mention your assumption there as well as to why the result skewed",309211.0
131752,575375.0,"Though this could be done using HIVE, by writing commands in HUE Notebook, there are certain constraints associated with it: The execution will take place parallelly across blocks of Notebook, and since a query might depend upon other, many queries might fail (for reasons like a table does not exist, etc.) Within a block, the execution takes place sequentially, however, the execution would pause if you have a query like select query, since it will display the output of a select query and wait for you to explicitly tell to proceed to next query. It would be wise to execute the individual queries and prepare a text file along with that for submission/record keeping purpose.",317987.0
130929,571611.0,No.. this doesn't seem correct.. you might have misplaced columns. Please refer data dictionary/ Excel file for the correct sequence of columns and create table accordingly.,318495.0
130932,571623.0,The range of values shows there is error in the data. Since imputing/updating values for the erroneous data will not be wise..we should remove/filter out those values and proceed with the rows having only the accepted(possible) values. Here the EDA part comes in the play (check the no. of rows having incorrect data and compare with total rows and proceed accordingly) Hope this helps.,318495.0
130932,573025.0,"I think the values should be 0.5, 1 and above values as the data dictionary states extra includes 0.5, 1 and overnight surcharges.",314730.0
130932,571777.0,"Basic data quality checks in the problem statement also mentions that there are data quality issues ""Check whether the data is consistent, and if not, identify the data quality issues. Mention all data quality issues in comment"". My understanding is that this is a data quality issue and needs to be cleaned before proceeding with your analysis.",309211.0
131544,574170.0,"Use hour(),minute(),second() functions to extract the hour,minute and seconds individually from a timestamp field. Alternatively, use substr() function (ex. substr(tpep_pickup_datetime,12) ) to extract the hour:minute:second format.",313826.0
131716,574994.0,Check which datatype you have used while creating table for tpep_pickup_datetime column. According to that datatype you have to use inbuilt function for tpep_pickup_datetime column.,317991.0
131716,574977.0,"Please provide the query which is throwing this error. Also, provide the datatype of the columns that you are using in the query.",313826.0
131874,575950.0,"Do not use: ROW FORMAT serde 'org.apache.hive.hcatalog.data.JsonSerDe' We are importing and processing data from csv file. Use: ROW FORMAT DELIMITED FIELDS TERMINATED BY ','",318730.0
131307,573176.0,"getting the error as ""Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask""",302741.0
131307,573181.0,"Hi Surendra, You need to change the folder name. It appears that you get write permission for only the folders you create and moving data needs write permission, thus it makes sense to change the folder name to something similar to the one as you used in previous create the query. stored as orc location '/user/hive/warehouse/ your_partition_folder_name_orc '",317987.0
131307,575384.0,"I think it is some syntax issue, insert command is written twice :insert overwrite table nyc_taxi_part_orc_suri partition(yr, mnth)insert into nyc_taxi_part_orc_suri partition(yr, mnth). Try inserting it using only one insert command and it might be successful.",318448.0
131307,573207.0,"Similar question has been answered in below links. Have a look into these links and apply the solution given in these link, https://learn.upgrad.com/v/course/208/question/130114 - Go through comment also https://learn.upgrad.com/v/course/208/question/130007 https://learn.upgrad.com/v/course/208/question/130184 https://learn.upgrad.com/v/course/208/question/130459 Hope this will help",317991.0
131307,573862.0,"Why do you have insert two times? insert overwrite table nyc_taxi_part_orc_suri partition(yr, mnth) insert into nyc_taxi_part_orc_suri partition(yr, mnth)",318007.0
131746,575207.0,Yes. A clean ORC Partition table (without any erroneous records as found during EDA) should be created before proceeding with further analyses.,313826.0
131746,575397.0,It is better to create ORC table with where conditions on initial table which should remove erroneous data.,318448.0
131746,575216.0,Yes ..We need to remove the rows or records which are having error example not the time period mentioned like Nov and Dec 2017. Insert to the partition orc table after filtering the record.,318476.0
131746,575377.0,"Hi Jay, Alternate solution without changing table properties: I can suggest an alternate method as well since we generally create the table and load data into it. Consider loading only the filtered data in the fresh table (by using where clause).",317987.0
131411,573543.0,"First row in CSV file is header so while importing it into database we need to skip that row. Probably because of this you are getting null. Use below code to skip header row. tblproperties (""skip.header.line.count""=""1""); Hope this will help. Do let me know if it works or not.",317991.0
131411,573539.0,"The first row of the data has names of the columns like VendorID,tpep_pickup_datetime.. You need to include the below table properties entry as the last line of the tabel creation query to skip the first row tblproperties (""skip.header.line.count""=""1"");",313826.0
131819,575643.0,"Use tblproperties (""skip.header.line.count""=""1"") while creating table to skip header line.",317991.0
131312,573353.0,"Hi Surendra, Both the queries seem perfectly fine. Can you please share the screenshot of the error you're getting while running the query without using 'limit 10'?",318355.0
131443,573783.0,Make sure you add the jar.,310974.0
130735,570969.0,Answered in https://learn.upgrad.com/v/course/208/question/130676,309211.0
130735,571331.0,"I'll recommend to filter out error free rows in a new table using where command instead of deleting from the same table. Still you want to delete the rows , then ensure that you have table in ORC format and have done bucketing/clustering and then try the delete command.",318495.0
130735,571263.0,You wouldn't even need to delete rows if you create the partitioned table filtering out the erroneous data during insert.,310974.0
130735,571458.0,Create the partitioned table and filter the erroneous records while inserting the data using where clause.,318756.0
130735,570885.0,Hi Is your data bucket and in ORC format ? Then only you can delete the records . Also you can create another table and put where clause to di-select the required data.,311861.0
130991,571900.0,it looks like your table was not populated with data. 1.check if you have used correct query to create a table 2.check if you have used correct query to import data from csv.,301648.0
130991,572038.0,"Check wether you have performed below steps correctly or not : 1. While creating table make sure the sequence of column in create table and data file is same. And also do not use Json Serde since the file is in csv format. 2. After creating table check whether the data is populated correctly or not. 3. Then while running any queries make sure the column names, where clause condition or any other condition are correctly written. Also look out for opening or closing brackets () and quotes ''. Once you follow all these correctly, I think your problem will be solved. If problem remains there then do let me know.",317991.0
133049,580213.0,can you paste query in which you are facing this error ?,317991.0
133049,580601.0,"Could you plz share the details, where and when you are getting this error.",329936.0
133023,580210.0,"As mentioned by Vinay, We do not have write permission to "" /common_folder/nyc_taxi_data "" which you are trying to access. We can only read data from this location. You can use following path to store file: /user/hive/warehouse/&lt;foldername&gt;/",317991.0
133023,580100.0,Looks like you are trying to write to a file on which you do not have permissions. Change the file path to the location where you have write permissions. It is usually /user/&lt;your_id&gt;/&lt;file_name&gt; Would be helpful if you can put in the commands which are throwing the error.,313826.0
132807,579269.0,"Hi Aswini, It would help if you could share screenshot of the code with stacktrace. If its a graded question that you're working upon, try creating a demo version of your code.",318355.0
132807,579328.0,This seems to be an issue due to non availability of resources at corestack and it could be nothing to with your Select statement. I suggest you try the same at different times and it would gor through. Please let me know if it works as it is!,301121.0
131459,,nan,
131260,572942.0,Also as part of compare what we need to do?,320103.0
131260,572954.0,We need to compare with the overall average speed with the most happening days. So first check what is the overall average speed for the month we are considering together and then check for the most happening days. Then we can compare accordingly. Hope this helps!,301648.0
131260,572955.0,"Overall Speed - Meaning the average speed for the entire dataset under consideration for the analysis. You need to give inferences like whether the speed on these most happening days is similar , low , high etc and insights that you may derive out of such inferences.",313826.0
131260,575389.0,"You can create 3 columns, one for average speed on New year eve, one for average speed on christmas eve and last for overall average speed. Then compare which is highest and find the difference of both New year and Christmas from Overall Average. Hope this helps.",318448.0
131858,575843.0,Try to run again. It might be environmental issue.,320103.0
131478,573779.0,Even I tried to find one but I don't think there is such an option. I manually copied the code to a notepad and submitted.,310974.0
131187,572883.0,here we can use simple case statement to form the required cluster.,301648.0
131187,573100.0,"It is clearly mention in the question ""Segregate the data into five segments of ‘tip paid’: [0-5), [5-10), [10-15) , [15-20) and &gt;=20. And Calculate the percentage share of each bucket (i.e. the fraction of trips falling in each bucket).",329936.0
131187,575387.0,This can be done using case statement : select * from (select columns using case statement ),318448.0
131187,574652.0,You Use CASE for adding condition inside the count function. Return 1 if the condition fulfills else 0 and then you can add to get the count for the no of trips. Select. sum(CASE WHEN amount &lt;5 THEN 1 ELSE 0 END) /count(*) as fraction from table,315028.0
131415,573554.0,Checkout similar query: https://learn.upgrad.com/v/course/208/question/131199,313826.0
131415,574513.0,"Check the incoming data/files for inconsistency in the structuring. Also check the files getting loaded, may be it is non-compliant with the structure provided for the base table(table being used to insert data into partitioned table).",329936.0
131415,573692.0,Same error unable to find answer.,318007.0
131484,573776.0,Check this -&gt; https://www.cloudera.com/documentation/enterprise/5-9-x/topics/hue_use_autocompleter_enable.html,310974.0
132303,577786.0,One of the ways would be to use case statements.,313826.0
131489,574266.0,"To apply any Insert/Update/Delete operation in Hive, table must be in orc format and also bucketed . Similar kind of question has been answered in below lnks, you can visit these links for solution. https://learn.upgrad.com/v/course/208/question/131116 https://learn.upgrad.com/v/course/208/question/130787 Hope this will help",317991.0
131489,577802.0,"Unfortunately you can't use delete directly on a partitioned database.For you to completely remove a row you'd have to read, modify and write all the data down again. For an example on how to achieve this, see the wiki: http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables#1.3.5_Modifying_Partitioned_Tables Your table should be in ACID format (As mentioned in apache hive doc about ACID limitation, for ACID support table should be in ORC format plus non sorted and bucket enabled.) to updte/delete records. for further info refer.-- https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-Limitations",329936.0
131491,573806.0,"Hi Chandan We need to do bucketing . There are multiple ways of doing this ... One of the way is to use if clause and sum of that segment divided by total count , then take a percentage of it. https://www.folkstalk.com/2011/11/conditional-functions-in-hive.html Above link will help you to write if conditions.. You can use if statement for each of the bucket range .",311861.0
131491,575169.0,"use ""case""",317514.0
131491,574680.0,"No, we don't have to create any new bucketed table. Use the already created orc table to calculate %share using query itself.",317991.0
131525,574193.0,"problem solved , was taking &gt; instead of &gt;= in condition",318005.0
131608,575163.0,Multiples of 1$ and 0.5$,317514.0
131608,575402.0,"We need to consider only 0, 0.5$ and 1$.",318448.0
131608,574439.0,We need to consider only those values $0.50 and $1.,301648.0
131608,574505.0,You need to consider only 1$ on 0.5$ no I'm between values as they are fixed charges Also if you are planning to consider values other than data dictionary make sure that you mention them as assumption in your final submission.,318495.0
131608,574441.0,It depends on the assumption you make. I would suggest to validate the assumption with data dictionary and make sure to mention all the assumption in comments as directed during problem statement.,317991.0
132314,577783.0,"It all depends on the assumptions you have made regarding the records that you would want to consider for the analyses. Would suggest that you find the counts around such edge cases so as to make an informed decision. Also, do spell out the assumptions clearly.",313826.0
131454,573780.0,I would recommend you to create the partitioned table using select with multiple where conditions filtering out the erroneous data.,310974.0
131454,574276.0,Similar kind of question is answered and you can follow instructions provided by TA in below link to delete data from table https://learn.upgrad.com/v/course/208/question/130787 Also you can think of the approach what Ram has mentioned above. Hope this will help.,317991.0
131753,575278.0,Got the solution. Please ignore,303228.0
131796,575537.0,"Are the number of columns selected matching the number of columns required for the insert operation? Please note that since you are using partition on year and month, even these columns have to be provided seperately to the insert query. Please refer the module on Partition and Bucketing.",313826.0
131796,575508.0,"Before inserting into the table try selecting the data alone. If it fails, try reducing the conditions and it would be easier to debug the problem this way",318329.0
131851,575810.0,"Yes, I have imported time as timestamp. To get the duration of the trip, you can use something like this: unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime)",318730.0
131831,575889.0,"After the initial data clean up, if you still have such errors, it is better to revisit data cleaning steps to get rid of those rows by which you will be able to do a fair analysis . EDA is never a single step phenomenon and few revisits would be required later too as and when we come to know such erroneous data.",301121.0
131831,575954.0,"such rows are not many ,about 200 ,and avg speed depends on trip distance and time ,,i did data cleaning for both ,,, data dictionary had no mention of limit of trip distance and time ,,we cant assume of a particular limit of time ,,,only avg speed can be assumed as car speed cant be very much",318005.0
131831,575734.0,Assume a reasonable limit and mention it in comments,318329.0
131357,574194.0,yes skip header,318005.0
131357,575368.0,"Hi Jayashree, Alternate solution without changing table properties: I can suggest an alternate method as well since we generally create the table and load data into it. Consider loading only the filtered data in the fresh table (by using where clause).",317987.0
131357,573358.0,"Hi Jayashree, Following are the steps we need to follow: Create an external table (which seems that you've already done) Create another partitioned table (orc format) (bucketed as well if you want to perform DELETE/UPDATE/INSERT operations) For Inserting the data there are multiple ways which are as follows: Insert into the orc table by using 'where' clause in the select statement For example: INSERT INTO demoTable select a,b,c where a is not null The above query will filter where a is not null &amp; then insert the data Insert the whole data into the orc table &amp; then delete the rows from here which are not required DELETE/UPDATE/INSERT would work only if the table is bucketed &amp; partitioned &amp; stored in orc format Hope this helps!",318355.0
131357,573366.0,Further to add you are getting a wrong count of rows..kindly check your data.. you might have included the header in the data.. You can checkout discussion forum for more.,318495.0
131357,573364.0,You can do it anyway..before or after creating ORC table. If you want to filter out the erroneous data from the current external table then use where query to filter out the data. Else create a bucketed ORC table and delete the erroneous rows. One note: Many people has faced issues while using delete query as the table was not in orc format and was not bucketed. So if you want to delete the erroneous data then table must be bucketed ORC . Hope this helps.,318495.0
130783,571068.0,"Since it is an assignment related question , can't give direct answer but I can give hint. In this assignment we need to check two things 1) Use data dictionary to validate the data and identify erroneous rows. 2) If you are making assumption make sure to add them into comment. For ex: In MTA_tax=0 case using data dictionary you can make either of the assumtion that the rows MTA_tax=0 are erroneous or they are not. In both cases you have to mention the assumption in comments. So it is upto you what assumption you makes. Hope this hint will help in this assignment.",317991.0
130783,571326.0,MTA_tax will either be levied or not. If levied it should be $0.5 else nothing.Same for Improvement_surcharge. Hope this helps. Although this is not assumption and is a fact but yes it would be advisable to mention the same as comments in notepad to be on safe side.,318495.0
130783,571528.0,"As per problem statement itself, there are many assumptions you can make while performing EDA for data cleanup. Please make sure to write and call out your assumption; this is in line with the problem statement: ""While performing this analysis, you will inevitably make several assumptions. As long as you state these assumptions, you will be awarded marks.""",309211.0
131361,573436.0,It's more on the values of the data in each column and EDA. Many columns will be having values different from the values mentioned in data dictionary. So basically we have to check all the columns for the erroneous data and remove the same.,318495.0
131361,573426.0,"Yes, we need to perform exploratory data analysis on the dataset. Refer to the data dictionary and check which fields data are not adhering to it.Mention your assumptions for those records which you are considering erroneous For example, as mentioned in the asssignment passenger count is 0 which is unusual. So rows which has 0 passenger count are consisdered erroneous. Similarly you need to perform analysis on each and every field.",311254.0
131364,,nan,
131286,573159.0,"As per my understanding, no need to delete rows where pickup location and drop location are same. It might have happened that trip was started from point A but then ended at same point as rider didn't want to proceed with trip due to some reason. It again depend on your assumption and you can mention same.",320103.0
131286,573384.0,Well logically that trip never happened. But let's take a practical example.. say you book an ola ride and at the end moment you decide to cancel the ride.. you will still be charged some amount. But now since it's an open question you are free to have your assumptions but make sure you state the same in your submission. Also a note.. if you are planning to drop the records then check the percentage of rows you are dropping.. since dropping a huge amount of rows is not at all advisable. Hope this helps.,318495.0
131368,573791.0,Encapsulate case with count and you can calculate % in a single line of code for each bucket.,310974.0
131433,573607.0,"Considering passenger_count = 0 as erroneous is already mentioned as an example in problem statement. For passenger_count &gt; 6 i.e. 7,8,9, its upto you to decide whether it can be consider as erroneous or not. Whatever assumption you do, make sure to mention it in comments. Hope this will help.",317991.0
131559,575404.0,Put a condition where dropoff time &gt; pickup time.,318448.0
131559,575614.0,"This should have been noted in the EDA phase and treated while cleaning the Data. Some records have a Drop Off time BEFORE Pick Up time. Since this is Practically NOT possible , it can be understood to be a Data Quality issue. And while cleaning the data, Entries having Pickup Time &lt; Drop Off time should only be retained. Thus, the situation of time difference being 0 would not crop up. Hope that helps.",317998.0
130796,571269.0,"I'm not getting this error, while using insert overwrite table statement. Kindly recheck your query once.",318429.0
130796,571286.0,a simple query is also giving me this error INSERT OVERWRITE TABLE NewYorkCityTaxi SELECT * FROM NewYorkCityTaxi;,316036.0
130796,571616.0,Please make sure you are creating the files in the directory where you have access.it seems you are creating a table where you do not have access.,311861.0
130796,573997.0,"Even I am getting the same error while loading data into partitioned table , did u guys got any solution.",319056.0
130796,571669.0,"You do not have write permission in ""/common_folder""",329936.0
130796,571774.0,"Hey Prateek, Can you please prefix Schema name to the table? I see Schema name missing. I saw errors while I was using common schema (without schema name) Thank you",309211.0
130796,572943.0,1. Created source table without partition. Uploaded data from CSV into this table. 2. Created ORC (STORED AS ORC) table with partition. 3. Loaded data into ORC table from Source table.,308636.0
130796,572395.0,I am also receiving the same error. Can anybody suggest what is the issue here.,314730.0
130796,573502.0,Did u resolve that error Prateek?? Created external table with location as LOCATION '/common_folder/nyc_taxi_data/' Now want to overwrite it. As per the comments in understand i dont have permission to that comon_folder so what needs to be done. how can we overwrite the table?,301114.0
130798,571260.0,As per data dictionary yes. But you are free to assume other conditions as well and accordingly perform analysis. I would prefer sticking to the data dictionary conditions.,310974.0
130798,571322.0,Yes MTA can be $0 or $0.5 only and any other values will be erroneous. If you are making any assumptions then you should mention the same. I'd suggest to consider the data cleaning as per data dictionary and only to make assumptions wherever inevitable.,318495.0
130801,571168.0,"I think we should consider pickup field to derive Month, since there are overnight trip's data too in the dataset provided. Analysis of the most happening days of the year would be more meaningful if we do it based on pickup date. If pickup date is 31st dec, drop can be either 31st dec or 1st Jan --&gt; More meaningful insights If dropoff=31st dec, pickup would be either 30th dec or 31st dec. It would be better if TA can verify this.",310467.0
130801,571340.0,"So there are only around max to max 200 such records, which is just miniscule of the actual data present. We can simply ignore all the pickup and drop date in October and January.",318741.0
130801,571526.0,"You can logically make any assumption but please make sure to write the assumptions down as comments in line with problem statement : ""While performing this analysis, you will inevitably make several assumptions. As long as you state these assumptions, you will be awarded marks.""",309211.0
131622,574497.0,That is up to you for consideration. You can drop them as well as keep them. According to me 6 is the maximum no. of passengers you should take in consideration but make sure you mention the same in the comments. You can refer below link for Max allowable count in cab in NYC: https://www1.nyc.gov/nyc-resources/faq/484/how-many-passengers-are-allowed-in-a-taxi,318495.0
131579,,nan,
131580,574294.0,You can follow the steps explained in the video and take reference from the code provided in the module in below link https://learn.upgrad.com/v/course/208/session/37071/segment/197849 Hope this will help.,317991.0
131597,575594.0,"I dont think there is any need to create new columns and store the minute, hour etc details. Reason being, the unix_timestamp function could be used to extract that information from while writing the query. For example, you can use something like: unix_timestamp(tpep_pickup_datetime) &gt; unix_timestamp(tpep_dropoff_datetime) to compare two different time. Similiarly you could subtract them to get the duration of the trip. Now, since distance is known, we can divide distance and the duration to get the speed. Do note the units of time would be in seconds. So it should be converted to hours to calculate the speed. Hope that helps.",317998.0
131597,574416.0,use unix_timestamp to convert your dates and then subtract. Result will be in seconds. Process further to derive speed,318438.0
133347,581088.0,"Check the table creation statement using below command: SHOW CREATE TABLE &lt;your_table_name&gt;; As mentioned by Vipul, the table must be storing data in ORC format and further it should be clusteresd and bucketed. Alternatively, you could only insert the required data into the table by using : "" Insert into table ... select ...where ... "" kind of construct where in you select only the data that is necessary for further use.",313826.0
133347,580877.0,Make sure you follow below steps: To do delete/update operations in Hive table File format must be in ORC file format with TBLPROPERTIES(‘transactional’=’true’) Table must be CLUSTERED BY with some Buckets. Note: if table is not bucketed then you will get FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table that does not use an AcidOutputFormat or is not bucketed. You can also visit below links to get more insight https://learn.upgrad.com/v/course/208/question/131116 - go through comments also,317991.0
131482,573778.0,"Hi, I faced similar issue. It was like a deadlock situation where bucketing should be done for deleting and deleting is not support for bucketed tables. Hence followed below approach. I created one table for partition TBLPROPERTIES (""orc.compress""=""SNAPPY""); and another for clustered with TBLPROPERTIES ('transactional' = 'true'). After which delete operation worked. TA can help further. https://community.cloudera.com/t5/Batch-SQL-Apache-Hive/Hive-update-delete-and-insert-ERROR-in-cdh-5-4-2/td-p/29485 Thanks, Kiran",306736.0
131482,573867.0,Hi Kiran . I am also facing the same issues. Even after splitting into two tables i am running into the same problem. Could you please help me out with this. steps followed: 1)create external table tablex.... 2)create table if not exists taxi_partition ....partitioned by .... 3)insert overwrite table taxi_partition partition..from tablex 4)create table if not exists taxi_partition_clustered..clustered by.... 5)insert overwrite table taxi_partition_clustered partition...from taxi_partition 6)delete from taxi_partition_clustered where ..... error: Error while compiling statement: FAILED: SemanticException [Error 10122]: Bucketized tables do not support INSERT INTO: Table: nyc_taxi_jaya.taxi_partition_clustered And if i try to perform delete on orc table i get error Error while compiling statement: FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table nyc_taxi_jaya.taxi_partition_orc that does not use an AcidOutputFormat or is not bucketed,301114.0
131482,577800.0,"Unfortunately you can't use delete directly on a partitioned database.For you to completely remove a row you'd have to read, modify and write all the data down again. For an example on how to achieve this, see the wiki: http://code.kx.com/wiki/JB:KdbplusForMortals/partitioned_tables#1.3.5_Modifying_Partitioned_Tables Your table should be in ACID format (As mentioned in apache hive doc about ACID limitation, for ACID support table should be in ORC format plus non sorted and bucket enabled.) to updte/delete records. for further info refer.-- https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-Limitations",329936.0
131891,577065.0,"When you try to find the correlation only for passener count = 1, the passenger count column would only have values of 1 i.e., no variance and hence this would result is the correlation being displayed as NaN. If there is no variance in any one of the columns that are used for checking the correlation, then the result will be a NaN.",313826.0
131891,577001.0,"i dont think u need to calculate corr for each passenger count... just do corr(passcount,tip_paid) TA pls verify",308437.0
131891,577875.0,"While finding correlation you need to take care of this condition in your code --- ""tip_amount&gt;=0 AND passenger_count&gt;0""",329936.0
131891,576361.0,"NaN implies Not A Number. Following could be the reasons for it: a. One of the columns that you are using for the correlation has NaN values in it. b. There might be some data quality issues such as 0 passenger counts or 0 tip amount paid. Do check those, it might resolve the issue. Hope that helps.",317998.0
131655,574685.0,"Problem statement has one line ""While performing this analysis, you will inevitably make several assumptions. As long as you state these assumptions, you will be awarded marks."" It is upto you to make any assumption for each of the column mentioned above. Just make sure to mention all your assumptions in comments.",317991.0
131859,575839.0,"We need to consider records only for NOV,DEC -17 from the analysis I. Prior to that we need to work on full dataset to find which all column contains erreneous data.",320103.0
131859,575852.0,"Yes, for this assignment we need to consider only the data of November and December months of year 2017. In the section - Basic Data Quality Checks , you need to make sure that your dataset only contains data from November and December months of year 2017. If there is some extra data then filter it out. The final ORC partitioned table which you will create after Basic Data Quality Checks section to perform the analysis should be clean, i.e. it should contain only the data from November 2017 and December 2017.",318730.0
131859,575844.0,Basic Data Quality checks needs to be done on the entire dataset. Analysis 1 and 2 should be done on clean data with records from Nov and Dec 2017.,313826.0
130800,571158.0,"try using the following commands while creating table. ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location '/common_folder/nyc_taxi_data'",310467.0
130800,571266.0,Try this: ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' with serdeproperties ('paths' = ''),310974.0
131502,574123.0,From the above error I can infer that you have used JSON Serde. If that is the case then don't use it since the assignment dataset is in csv file. So remove the Json Serde code and try to execute your select command. Do let me know if it helps or not.,317991.0
131502,577803.0,Kindly provide the complete code you are using to create table and selecting data.,329936.0
140584,607526.0,"Sorry my bad, it's present in yellow circles",304022.0
90899,378976.0,"Whenever you want to create new column you can do it in the following manner. Think how can you use it for your problem. df['X] = ""Some new Series"";",318368.0
90899,378982.0,Suppose X is a data frame containing 5 columns. If you want to add a new column (song) to X then the syntax is X[‘song’] = You have created the expression to add song variable so now use the above example and add it to “movies” data frame. Hope this helps,318328.0
90899,379804.0,"1. Below command will create two new columns col1 and col2 in dataframe_1 and fill these column's values from splitting the values from unplittedcolumn ie. mention the name of column and str.split will help here. dataframe_1[[col_1','col_2']]=dataframe['unplittedcolumn'].str.split('delimiter', n=1, expand=True) 2. After this you will see that first genre is in col1 but rest of the genres is in col2. 3. Now you need to keep only second genre in col_2 column for each movie, Again use above command dataframe_1['col_2']=movies['col_2'].str.split('delimiter', n=0, expand=True) 4. For movies having only one genre (i.e. having col_2 value missing), # for these movies, imputing col_1 in col_2 using df.loc and selecting rows inside where col_2 is null for col_2 column and impute col_1",317811.0
92455,389413.0,Impacted may be because one question excluded,319869.0
92455,389488.0,"One question from ""Pandas Graded Quiz: First Question"" was nullified. So it was informed that students would be scored equally. Below is the link of this information: https://learn.upgrad.com/v/course/208/question/90081 So might be possible because of this the marks are updated.",317991.0
90384,376838.0,It's a graded question and we are not supposed to put the code here. You can apply different logic to get the stat of all the team and then start doing the required operation.,301648.0
90384,376854.0,it is not a merge but rather addition of two dataframes. you can refer to one of the examples in pandas exercise notebooks on IPL to get ideas on how to do it.,318329.0
90384,376861.0,"merge is just going to merge two dataframes using the inner/outer join on the defined column whereas all other rows will simply be joined including the common rows with numeric values in it.. so, try to use some different logic &amp; code which will add the numeric values as well with these dataframes.",316349.0
90384,376919.0,"Hi Ravi, The merge function simply joins two dataframes based on the join type ie add new columns to the dataframe. But here, your requirement is to add the column values based on a common column value. So you have to look for another solution for adding numeris column values. Merge wont work here. Hope this helps. Regards, Rajarshi.",310511.0
90384,377326.0,thank you all guys!!!! Have figured out what the issue was and yes had to use add function to overcome the challenge. i was bit surprised with skewed data output i was getting with merge and thought perhaps i was doing something wrong or there was some issue with the data that I was not able to detect. cheers!!!!,312199.0
90384,376835.0,as it is a graded question i cant answer directly so to help you when you merge two data set using inner join you get a new data set with only those values which will have common team in both the data set try doing a different operation which will add the data for both the data frames,318017.0
90384,376895.0,Since it is a graded question we are not supposed to post code. Instead of it overview of concepts can be given that can solve the problem. If we want to work on multiple dataframe there are three ways: 1. Merge 2. Join 3. Concatenate for above three concepts you can go through below link to get insight into it: https://pandas.pydata.org/pandas-docs/stable/merging.html Apart from this we can also add two dataframe. Below is the link for this: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.add.html You can choose from these concept and check which concept is useful to resolve your problem Hope it helps.,317991.0
90413,376994.0,"If you have submitted the code and it is showing ""accepted"" then it is perfectly fine.",317991.0
90413,377007.0,"After you click on ""Submit' you could get one of the following statuses: Rejected - All of the non-sample test cases have failed Partially Correct - Some of the non-sample test cases have failed Accepted - All non-sample test cases have passed. Your solution code is correct. Source: https://learn.upgrad.com/v/course/208/session/15859/segment/80226",313826.0
90180,379062.0,anybody facing isuue in setting index with inplace parameter ? python goes crazy when I am putting this in,315464.0
90180,375711.0,"What is the error, you are getting?",318368.0
90180,375861.0,it answered my concern also,317990.0
90180,375762.0,This is already addressed as most of us faced same question so Uprad is going to nullify it. U must be getting worng columns,317982.0
90180,376320.0,"Hi, no the answers are correct, ipl18.iloc [0:3, [0, 6]] is wrong only..",305129.0
90180,376945.0,"i have chosen the correct one by executing in jupiiter,but still its giving error.",300716.0
90180,377112.0,"https://learn.upgrad.com/v/course/208/question/90081 please refer to the pinned post, regarding the question.",319721.0
90180,379609.0,only one answer is correct,302877.0
90719,378092.0,save and checkpoint...,319869.0
90719,378085.0,Use File --&gt; Save and Checkpoint,318084.0
90719,378350.0,"default the notebook is auto saved and checkpointed. In case you accidentally close the notebook, you can find it in Home folder as ""Untitled"" notebook, unless you have gone thru the effort of giving it a name. you can find your notebook with the date / time stamp.",312199.0
88943,368773.0,Yes i should be considered as ' It throws error on even plain editor if it picks another one. so its all ',304813.0
88943,368833.0,Yes. its a characterset encoding problem. You can report it,306248.0
88943,371197.0,"I tried but i was not able to see any such ""as"" mentioned by you. Could you update your browser ? If still facing, thn you can highlight it or report it. We will find the reason from the backend. But i do nit think there is any issue here. Please share screenshot",301555.0
90478,377396.0,In a typical MCQ Graded question when its mentioned that 'More than one option may be correct.'. It is highly probable that there are more than one right option to the question. However there is a possibility that only right answer might apply irrespective of the question being a multiple checkbox question,318085.0
90478,377453.0,Look for buttons. Round button i.e radio button means only one solution. Checkbox or square button means multiple solution possible.,317689.0
90478,377446.0,We can assume by checking whether there is checkbox or radio button placed before the option. If it is checkbox displayed in screenshot we can assume that there might be multiple correct answer. If it is radio button displayed in below screenshot only one answer will be correct: What I am assuming that you have asked because of ' Pandas Graded Quiz: First Question'. If that is the case the to inform you that question has been nullified. You can refer below link https://learn.upgrad.com/v/course/208/question/90081,317991.0
90478,377421.0,you can actually differentiate between questions having multiple solution or only one answer by the checkbox /radio button thats available. By common convention if u see radio buttons(circles)-that means only single answer u can select. If u have checkbox(squares)-then you have multiple options to chose.,309451.0
90478,377521.0,"Agreed. yes we can distinguish using radio buttons and check boxes, but a note would definitely help in not missing out.",318002.0
90478,377926.0,"Agreed, when i was donig the problem .I did'nt check all answer. I submitted . If it is mentioned there it is having more than one solution. It will be good for us.",314183.0
91033,379995.0,"Hi Navneet, temp will not store a copy of variable in this case, it will hold the reference . Because of this, the output will be different than expected. You can use .copy method on pandas dataframe to make a copy of them if you want to implement using the same logic. Hope this helps, feel free to ask if its still not clear.",317987.0
91033,380060.0,"Try temp = a[n,:].copy() a[m,:] = a[n,:] a[m,:] = temp",318329.0
91033,380153.0,This is very important question you asked as while assigning it will assign the reference so in order to use such type of multiple assignment always use copy which will not impact original reference. pls go in detail on stack overflow,319869.0
91033,380653.0,"You have to use copy() or add an extra square bracket. array[[m, n]] = array[[n, m]]",318493.0
91033,380676.0,"Another way to do this, take the copy of entire array and then perform your operation, b = a.copy() a[m] = b[n] a[n] = b[m]",319319.0
90592,377921.0,"We can convert 1987219000 to million by dividing it with, 1000000. Do the same with the column. Please ensure that you are carrying this out after cleaning the data.",319721.0
92360,389419.0,change SQL safe update mode from preference option then reconnect to DBMS..it will solve your issue.,319869.0
92360,388948.0,you've to extract the first character from the address which is house number as hno.. anyways what error are you facing?,316349.0
92360,388986.0,ALTER TABLE table_name ADD column_name INT DEFAULT 0; if u want to set deafult,317982.0
92360,389087.0,"I've found the solution, I haven't used the statement SET SQL_SAFE_UPDATES = 0; at the begin. After using this, I'm able to update all the rows at once.",318328.0
92360,389112.0,Please use SET SQL_SAFE_UPDATES = 0; It will allow you to update,317811.0
89782,373012.0,You need to submit again. Your answer in case if Partially correct wouldn't have passed all the sample test cases.,314547.0
89782,373026.0,You have advised to Submit your Answer again as it was not Passed all the Sample Test cases . Firstly Verify your answer and then Submit.,301648.0
89782,373175.0,How to know the sample inputs of failed test case? So that we can understand the error in code.,311061.0
89782,373007.0,"Partially correct mean one of the test case failed on your solution. You need to check your code with hardcoding or some condition which is failing. You will need to attemp it again as answer is not correct. DONOT use any hard coded value in the program if not necessary. After verify with provided sample output. Go to ""Input"" tab and provide few more test cases of your and run your program and see if its working.",318368.0
89782,373696.0,Even I also got partially correct and no sure why 1 test case got failed.Will there be any deduction for partially correct submission.,310529.0
89782,374382.0,Avoid usind hard coding in your program. Try some test cases of your own in the input tab and check the output. This might help to check for hidden test cases.,318009.0
89782,374464.0,"Make sure u have handled the edge case of zero. As you know, 0! = 1. I corrected my Partially correct to fully correct.I was missing this condition in my solution.",310529.0
90975,379558.0,"Yes, that's true but there is no guarentee that the graded questions has to be from the lecture notes. I know, it is tough to handle that situation but we need to figure out.",310974.0
90975,379570.0,"Yes, The basics are covered very well. We need to read addiional articles to be well versed with this and to solve the assignments.",316202.0
90975,379563.0,I believe the basics have been covered and we need to build a little more knowledge on top of that to be able to solve the assignments.,313826.0
90975,379572.0,"Yes agreed to both of you guys, we need to put more effort to gain knowledge. But Swapping values are quite common real solutions we are working on and there are many more real time commands which be very handy when we move up the scale. I feel those should be covered as well atleast as hint. Lecturer notes are fine no harm in seeing those but still few such things should also be covered. As we are getting assignments ( like real predictions ) we will come to know + we have new coders who will come to know where to apply. TA any comments on it ? I would really appreciate your views on this.",306735.0
90975,379597.0,"I struggled with the NumPy graded questions, but fortunately stack over flow had provided plenty of examples so that I could get to the correct answer before submitting. They weren't kidding when they mentioned stack overflow for additional learning.",315022.0
90975,380650.0,I think it is covered as part of material or jupyter notebook provided.,318493.0
90985,379625.0,Try reloading or clearing the cache for your browser. Some time it happens but will reload after few retries.,318368.0
90985,379650.0,"This happens to me most of the time, clear your cache, close your browsers and re-login. works !",306735.0
90985,379768.0,It is completely dependent on internet connectivity speed,317811.0
90081,375987.0,Do I still have to attempt this graded question?,319319.0
90081,376571.0,Thanks,319247.0
90081,389130.0,"Hi, It seems marks for this question was considered for final percentage, atleast for me.",318436.0
90789,378362.0,You can use temporary arrays and then swap but there is a more concise way of doing it.,310974.0
90789,379094.0,"why are we passing the values of m,n as a list when using the swap command?",318751.0
89410,372065.0,Same for me also. Got the Team in column number 8,318756.0
89410,371960.0,That's corret. Even for me the ordering of column is not right. 'Against' is column 1 and 'Team' is 8,318438.0
89410,370916.0,,318329.0
89410,370917.0,,318329.0
89410,371139.0,Still i'm getting this error,311952.0
89410,371196.0,Could you please elaborate your question if above answers have not yet been able to answer your query.,301555.0
89410,371227.0,Print your dataframe head or use describe in the console and you will be able to see column indexes. You might have Team column at first position in your dataframe. Veriy DF or provide more details for describe() or head() so that furthur deatails can be provided.,318368.0
89410,371469.0,"The same thing is happening in my case as well. Probably the column index is getting assigned based on alphabetical order. I tried thid both on jupyter notebook and in spyder but in both the cases the index is being assigned alphabetically and ""Against"" is reflecting as the first column. It would be really helpful if someone could clarify on this '",318374.0
89410,370913.0,"You are trying to reduce top 4 rows and first column (0:4 implies rows and the next 0 implies first column) Use ipl18.iloc[0:4, : ] if you are trying to retrieve all columns.",318329.0
89410,373172.0,"Issue is resolved after adding columns parameter with right order ipl18 = pd.DataFrame({'Team': ['SRH', 'CSK', 'KKR', 'RR', 'MI', 'RCB', 'KXIP', 'DD'], 'Matches': [14, 14, 14, 14, 14, 14, 14, 14], 'Won': [9, 9, 8, 7, 6, 6, 6, 5], 'Lost': [5, 5, 6, 7, 8, 8, 8, 9], 'Tied': [0, 0, 0, 0, 0, 0, 0, 0], 'N/R': [0, 0, 0, 0, 0, 0, 0, 0], 'Points': [18, 18, 16, 14, 12, 12, 12, 10], 'NRR': [0.284, 0.253, -0.070, -0.250, 0.317, 0.129, -0.502, -0.222], 'For': [2230, 2488, 2363, 2130, 2380, 2322, 2210, 2297], 'Against': [2193, 2433, 2425, 2141, 2282, 2383, 2259, 2304]}, index = range(1,9),columns=['Team','Matches','Won','Lost','Tied','N/R','Points','NRR','For','Against'] )",311952.0
89410,375091.0,Hi Please refer to this post https://learn.upgrad.com/v/course/208/question/90081,301619.0
89410,379503.0,"I am facing this issue too. When I selected only option 2, it says incorrect answer.",300730.0
90028,374854.0,Whereever there is a possibility of more than one answer correct there will be a checkbox.(square) if only 1 answer is correct there will be a radio button(circle),318358.0
90028,374848.0,It is simple to identify when the option is radio button i.e (with circle type) it means you have only one option and when there anwers options with suare bracket you have multiple correct answer.,307843.0
90028,374850.0,"Hi Rajat, There is difference in the bullets for options . For multiple answer bullets are square like in below picture For Single answer bulletes are circle like in below picture",320073.0
90363,376715.0,you need to submit it otherwise module will not be completed asaik,317982.0
90363,376714.0,You should always choose the correct answer even if its nullify.,306248.0
90363,376802.0,"Kindly read the question properly, i got it wrong as i dint read it correctly for that particular question there are two answers which we need to select,even if we select one and miss the other our answer will be considered as wrong.",300688.0
90363,376855.0,"Question was correct and there are two correct options to be selected. You can simply select both and move on. Even if you get it wrong, it will not be considered in the grading. But if you get it right it will boost your confidence.",318368.0
90857,378675.0,"np.zeros() functions need all arguments inside a separate round brackets (). Ensure that , for a function all arguments are closed inside a round brackets ().",318770.0
90857,378683.0,"There is syntax error please check below is should work, missed barcet after zero and closing bractet import numpy as np print(np.zeros((3,3), dtype=np.int)) [[0 0 0] [0 0 0] [0 0 0]]",307843.0
90857,378695.0,"First argument for zeros function is tuple and second is datatype, you can use it like below. print(np.zeros((5,5), dtype=int))",318368.0
90857,378736.0,"This issue is because np,zeros() take only one input. hence if you are adding more values separated by ',' then you have to use '( )' or '[ ]' as in tuples or lists. so the code should be, np.zeros((n,n)) or np.zeros([n,n]) Hope that was helpful",318009.0
90857,379107.0,np.zeros is a function so its arguements needs to come in a bracket. Add a bracket after zeros and close it after np.int.,311855.0
90857,379815.0,"You have misplaced parenthesis in the command. np.zeros((n,n) Correct is: print(np.zeros((n,n), dtype = np.int))",317811.0
90857,380661.0,"You have provided wrong sytax.You should include datatype(np.dtype=np.int) inside np.zeros(). It should look somthing like below. np.zeros((n,n), dtype = np.int)",318493.0
90930,379164.0,"Hello Raja, This is not a known issue. You can take it up with upgrad TA team.",320195.0
90930,380711.0,"If you answered the question and got it correct and the answer is shown correct, then that's not a problem. Don't worry.",319721.0
90930,381262.0,same i am facing the same issue,318083.0
91045,380073.0,"You may have already set the index and re-trying again could result in this error. Inspect both the dataframes to check what their indices are. Alternatively, you could start executing all the commands from the top and see it that works.",313826.0
91045,380235.0,See if you have already set the index,311803.0
91045,380442.0,"This generally happens when you have already set the index once. The best way to move forward would be: In Jupyter, Open Cell Menu, and then click on Run All. That should solve the problem. Hope that helps. :-)",318499.0
91045,380656.0,Please try using inplace=False. Inplace= TRue permanently changes the data where as inplace=false temporarily does.So it might be possible you do not have permission to permanently modify.I faced the same issue and assumed so. inplace=false worked for me,318493.0
91045,380797.0,"I ran your code and it runs perfectly fine. Your code is correct. When I ran first time it gives output: But when I again executed the same cell, I got the same error which you also faced. You can check from above screenshot that In[7] and In[8] means I ran same cell twice. The error is coming because when you run the code 1st time set_index sets the index 'Team'. And when you execute the same cell again it tries to set_index again. So you can go to menu bar at the top Kernel --&gt; Restart &amp; Clear Output. It will restart kernal and clears output. Then you can execute the cell from beginning. Or you can do as follows: or Kernel --&gt; Restart &amp; run all Hope this will help.",317991.0
91570,383329.0,"i believe your reasoning is correct. if it is your 2nd attempt and it is partially accepted then you will get partial marks out of the reduced total ( because of 2nd attempt). so, as per your example, in the 2nd attempt partial marks out of 5 would be given if the answer is partially accepted. still, can wait for an official response for the same. hope that helps.",317998.0
91570,383463.0,"1. The rubric which says, 10 for first attempt and 5 for the second attempt is for MCQs and not, coding questions, 2. Yes, you will be given partial marks for the test cases which pass. If there are two submissions. Only the latest submission counts for coding question. Your score doesn't depend on the submission which is not the latest.",319721.0
91368,381925.0,"The name of the function to use is arange() - with a single r , as in a rray range and not arrange.",313826.0
91368,382201.0,The function is arange().,317689.0
91368,382083.0,"The spelling of arange function is incorrect.It should be : array_multipleof5 = np.arange(5,55)",317460.0
90152,375546.0,"sys.stdin.read() will read input from standard input while input() will ask for input from the user. There are some conditions when you dont want to ask user for input but want to provide input in the starting of the program, you can use sys.stdin.read() function. Upgrad platform uses this feature as there are already defined Inputs in the Input pane and they dont want you to input test cases. In case of sys.stdin.read() python will read the standard input before program starts and keep it ready to use in the program.",318368.0
90152,375552.0,Thanks Kapil,320687.0
90152,375624.0,"import ast,sys input_str = sys.stdin.read() It is used to read the input from the standard input/output stream. When you're writing code in any console, you need to read the input from and write the output to the standard I/O stream. Also, note that the variable name in the second line is 'input_str'. This is because all the inputs provided to the standard I/O stream are read as a string and you need to convert them as required. For example, Let's say we needed an list and taken an input from user then it will be read as a string, 'input_str' by the system and required to be converted to a list. Hence, the following code is used to convert the read input from string to list: input_list = ast.literal_eval(input_str) Hope it helps!",317070.0
90152,375539.0,"Hi Deepesh, ""sys.stdin.read()"" is described as being able to take a newline(ie. multiple input lines from user) and finish the entry with Control+D. input() would finish your entry with the ""Enter"" key being pressed on your keyboard, so you couldn't include a newline in your data input that way. Other than that they're the same. So yes, based on the lines of input we can use either one. But in the assignments, we shouldnt change any code from the initial stuble. Regards, Rajarshi.",310511.0
90212,375779.0,"Curly braces here mean that there is variable needs to be replaced in that place which will be provided as argument to format. Here, {} of string ""Shape: {}"" will be replaced by rand_array.shape value.",318329.0
90212,375849.0,"{ } : are known as replacement field(s) and act as placeholders for values to be printed by the print statement. All the other text is printed as is , where as the { } replacement fields are replaced by arguments passed to the format() method. The arguments passed to the replacement fields through the format() method could be wither positional parameters or keyword parameters. Please see below a fwe examples: There is a whole lot of formatting that can be done by passing format specifiers. Please check out the below links for further details: https://docs.python.org/3/library/string.html#format-string-syntax https://docs.python.org/3/tutorial/inputoutput.html#fancier-output-formatting Interesting read: https://realpython.com/python-f-strings/",313826.0
90212,375804.0,"You can apply formatting on a string with template variables. {} represents formatting variable. There are multiple use case for the same. Below are few examples. # integer arguments print(""Hello {}, your balance is {}."".format(""Adam"", 230.2346)) Output Hello Adam, your balance is 230.2346 # positional arguments print(""Hello {0}, your balance is {1}."".format(""Adam"", 230.2346)) Output Hello Adam, your balance is 230.2346 # float arguments print(""The float number is:{:f}"".format(123.4567898)) Output The float number is:123.456790 # integer numbers with minimum width print(""{:5d}"".format(12)) Output 12",318368.0
90212,376010.0,Str.format method. The braces {} are replaced by whatever comes in the format section i.e () This will be handy when you have a number of values being printed and need to make a sense of them.,317149.0
90122,375323.0,what is it that you are looking for?,318329.0
90122,375382.0,"Hi Umesh, Once you submit your code, sample solution will be shown. Now once you click ""sample solution"" you will get solution for particular question. Now check at the top of commented code there you will find youtube link for video solution. Please see screenshot attached: Hope this will help.",317991.0
90122,377567.0,Hi. Video link is provied at the end when you submit the solution.,306012.0
89440,371075.0,"Hello Anand, Please find more examples for Numpy. http://www.labri.fr/perso/nrougier/teaching/numpy.100/ Explanation links for numpy: http://www.numpy.org/ Some more links are also available.you can google it. You will find better explanation.",320195.0
89440,371129.0,"I would recommend to not to go in great depth while trying to understand the dimensions as you rarely need this understanding in solving real world problems. For now, they won't help in anyway in continuing the course or answering any graded questions later on. So, continue with the rest of the content and you can always re-visit this.",310974.0
89440,371193.0,"Hello Anand, Please find more examples for Numpy with live demo or pdf material here https://www.tutorialspoint.com/numpy/numpy_indexing_and_slicing.htm . You will find better and easy explanation.",310634.0
89440,371338.0,"Forget dimensions,Consider them as parameters.you store in different values for diffeent parameters and thats when this method can be ideally used.For human its difficult to analyse anything beyond 3-dimmensions.So don't put too much pressure on your head.A little understanding of Linear algebra will also help you if you want to dig deep.",305655.0
89490,371295.0,"I used shape to confine the code to 4 lines: col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,array_2d.shape[1]-1] row_last = array_2d[array_2d.shape[0]-1,:]",310974.0
89490,371304.0,"Very true..even I thought the same seeing the solution. I tried with -1 and it works just fine col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,-1] row_last = array_2d[-1,:]",309451.0
89490,371300.0,"You can also use -1 to index the last row, or column. since negative indexing starts from the end. like this: col_last = array_2d[:,-1] row_last = array_2d[-1,:]",317998.0
89490,371305.0,"2d Array can be accessed by following syntax. array[x,y] Where array is your array and x is rows index and y is column index. Rows and column indexes can be individual value or range. Getting row values (Use indexes at x position) Individual rows, array[2,:] Range of rows array[2:3,:] All the rows array[:,:] Getting column values (Use indexes at y position) Individual columns array[:,2] Range of columns array[:,3:4] All the columns array[:,:] Based on above explaination you can easy construct indexes for your 4 problem statement. First Column : array[:,0] First Row : array[0,:] Last Column : array[:,-1] Last Row : array[-1,:]",318368.0
89490,372540.0,"Below code worked absolutely for me col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,-1] row_last = array_2d[-1,:]",302744.0
89490,374497.0,"col_first = array_2d[ : ,0] row_first = array_2d[0, : ] col_last = array_2d[ : ,-1] row_last = array_2d[-1, : ] Above code works fine. -1 indexes the last row or column.",310210.0
90424,377082.0,They have clearly written for even numbers. Oops,318372.0
90424,377085.0,"Question states that : - ""Given an even integer ‘n’, create an ‘n*n’ checkerboard matrix"". Since in question itself it is mentioned that input would be even integer, thats why the sample solution provided by upgrad doesn't cover the case for odd integer.",317991.0
90424,377084.0,"Hi, it should give you for the odd no also.. Question is given for even only but it will satisfy the odd and even both..",305129.0
88791,367844.0,"Hi, First try to unzip the file then try to open it.",314183.0
88791,367851.0,"Hi, try to download the file again.",318421.0
88791,367856.0,"Ideally, it shouldn't have any problem loading if the image is in the same directory. Did you rename or do something?",318329.0
88791,367863.0,Screenshots of the issue:,310511.0
88791,367894.0,Can you check if the cell is selected as Code or Markdown? Run after changing to Markdown - see if this works.,319302.0
88791,367895.0,"May be it has something to do with the permissions on the image files. You can give this a try : Right-click on the file and there should be a checkbox to ""Unblock"" the file. Tick that and try opening.",313826.0
88791,367973.0,"ZIP file comes with images ""2_d_array.png"" and ""numpy_axes.jpg"". First verify you have those images. Now try to open ipynb file from the location where you have these images. It is looking for the images in the local directory where you have your notebook file.",318368.0
88791,367975.0,Load the notebook after unzipping the folder. If are accessing without unzipping it dont have access to other files in the same zipped directory.,317689.0
88791,367957.0,Please find the sequence of steps to resolve the issue. 1. Right Click on the Image and copy the link for the image address. 2. For e.g http://localhost:8888/files/numpy_axes.jpg 3. See if the image loads in the browser. If not copy it to the path under the directory. Now the image should load. 4. Close the notebook and shutdown the kernel. 5. Then load the notebook again. The image would have loaded in the notebook. Thanks Vijay K,310482.0
88791,368002.0,I guess you it could also be an issue of your browser extensions not being up to date. Try updating the browser or using a different - updated browser. See if it helps,308962.0
88791,368068.0,"The idea is your .ipynb file and image should reside at the same directory. Notebooks takes the relative paths You can use both the syntaxes in markdown cells. &lt;img src=""numpy_axes.jpg"" style=""width: 600px; height: 400px""&gt; ![](numpy_axes.jpg) Check the screenshot. If need be restart the notebook service",306248.0
88791,367857.0,"Use cell as markdown type and then ![](files/picture.png) Check this video too, there are multiple ways to make sure your image shows up in notebook. Just do some changes in your cell and your image should show up. https://www.youtube.com/watch?v=xlD8FIM5biA",306248.0
89503,372001.0,"np.random.random(n) will return an array of having n elements/folating numbers within range of 0 to 1. np.random.randint(n) will return an array of having 1 element which is integer with any random value. np.random.randint(n,m) will return an array of having 1 1 element which is integer having value within range of n to m. np.random.randint(n , m, x)) will return an array of having x elements which are integers having values within range of n to m.",317811.0
89503,371347.0,Distribution is discrete uniform. Look at the link for more clarity. https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html,318368.0
89503,371388.0,"numpy.random.random returns random floats in the half-open interval [0.0, 1.0). numpy.random.randint( low , high=None , size=None , dtype='l' ) return random integers from low (inclusive) to high (exclusive).",318358.0
89503,371433.0,"HI, In randiint(), if SIZE is not entered then by default it returns as single value is retruned. That single value will be in between (low and high) value entered. ex: np.random.randint(0, 10) output will be a single value b/w 0 to 10",317410.0
89503,371572.0,"Hi, Distribution of numpy.random.random is a list of random nos from 0 to 1 Distribution of numpy.random.randint is a list of random nos between 2 nos that you provide. Hope this helps.",310511.0
89522,371481.0,"Is the default datatype for np.randon.random() is double, or do we need to mention that explicitly. Do all array have the same data type as default?",315757.0
89522,371527.0,"Numpy arrays have default type as ndarray. you can check that using simple code like this: import numpy as np x=np.array([1,2]) print(type(x)) and, np.random.random function returns float or ndarray of floats. you can refer this for more detail: https://docs.scipy.org/doc/numpy-1.10.4/reference/generated/numpy.random.random.html Hope that helps.",317998.0
89522,371654.0,Output of nd.random.random() is always float.,318368.0
88827,368141.0,"It basically repeats the pattern, pattern can be in form of 1-d array or any higher order array, in a specified dimension. for eg.",318433.0
88827,368106.0,"Tile creates a repetetive array of any existing array. For example we have an array arr = [1,2] we want to to repeat 10 times hence we set repetitions = 10. Note here repetitions can take an array as an argument as well. Following will generate an array with 10 repetitions of 'arr' np.tile(arr, repetitions)) &gt;&gt; [1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2]",317689.0
88827,368043.0,"'tile' function in Numpy simply repeats the number of elements in an array. For ex:- import numpy as np arr = [1,2] arr = np.tile(arr, 3) print(arr) o/p will be [1 2 1 2 1 2] new_ar = [1,2,3] new_ar = np.tile(new_ar, 2) print(new_ar) o/p will be - [1 2 3 1 2 3] Hope it clarify .",317991.0
88827,368062.0,"In simple words, if you want an array to be shown repeating (either one dimesional or multi dimesional) , you can use numpy.tile(). tile() takes 2 arguments. 1. takes the array which you want to repeat. 2. a single number or (rows,columns for multi dimensions) , how many time you want the repititions. Hope this clarifies. :) Enjoy learning.",313676.0
88827,368072.0,"In simple terms ,creating a new Array out of existing array for example import numpy # creating array a a=numpy.array([[1,2,3]]) print(a) #creating a new array from array a b=numpy.tile(a,[2,3]) #[2,3] means creating array a in a pattern of 2 rows and 3 columns print(b) output- [[1 2 3]] [[1 2 3 1 2 3 1 2 3] [1 2 3 1 2 3 1 2 3]]",303674.0
88827,368148.0,"Think of it like this .. the tile function basically repeats the array in a defined pattern, just like the tiles in your home repeats a pre-defined pattern. so whatever you want to repeat, just create an array for the same and use it in the tile function. Hope that helped you understand the basic concept of it :)",308962.0
88827,368250.0,"Tile function is used to repeat the previously created array as many times we want... arr = np.array([1,2,3]) print(np.tile(arr,3)) print(np.tile(arr(,3,3))",304692.0
88827,368403.0,"Just think about the drawing room tiles which are same in size and repeated multiple times. tile_in_feet = [3,3] room_size = [5,5] ------------ &gt;&gt;&gt; tile_in_feet = np.array([3,3]) &gt;&gt;&gt; print (tile_in_feet) [3 3] &gt;&gt;&gt; room_size = np.array([5,5]) &gt;&gt;&gt; print (room_size) [5 5] &gt;&gt;&gt; np.tile(tile_in_feet, room_size) array([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])",312479.0
88827,368816.0,"Tile function repeats the array A for number of times reps . Syntax as per documentation: tile(A, reps) : Construct an array by repeating A the number of times given by reps A = np.array([1,1]) print(np.tile(A,(3,3))) [[1 1 1 1 1 1] [1 1 1 1 1 1] [1 1 1 1 1 1]]",305652.0
88827,368942.0,"Hi Amith, Might be you will get from this image.",315028.0
88827,369450.0,"Hi Amith, numpy. tile () in Python constructs a new array by repeating array – 'arr', the number of times we want to repeat as per repetitions. Example: a = np.array([0, 1, 2]) np.tile(a, 2) array([0, 1, 2, 0, 1, 2]) np.tile(a, (2, 2)) array([[0, 1, 2, 0, 1, 2], [0, 1, 2, 0, 1, 2]])",314183.0
88827,369255.0,"t simply repeats the numbers of elements in an array. If you have an array, like so [1,2,3] , then np.tile([1,2,3], 2) will repeat the elements twice and make a new array. np always returns an array even if you give it a list. So explaining with some examples: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; ar = [1] &gt;&gt;&gt; np.tile(ar, 2) array([1, 1]) &gt;&gt;&gt; np.tile(ar, 3) array([1, 1, 1]) &gt;&gt;&gt; np.tile(ar, 4) array([1, 1, 1, 1]) &gt;&gt;&gt; new_ar = [1,2,3] &gt;&gt;&gt; np.tile(new_ar, 2) array([1, 2, 3, 1, 2, 3]) &gt;&gt;&gt; np.tile(new_ar, 3) array([1, 2, 3, 1, 2, 3, 1, 2, 3])",317412.0
88841,368181.0,"1.Create a 2*2 array like [0, 1], [1, 0] 2.The same array can be made checkered matrix by using tile() function. 3. Below , I am given example of n=2. You can test it for n=4 import numpy as np n = 2 x = np.array([[0, 1], [1, 0]]) num=int(n/2) check = np.tile(x, (num, num))# Print the created matrix print(check) Hope it helps. Thanks.",307494.0
88841,368166.0,"You can follow the steps: 1. Create a 2*2 array from 2 lists [0,1] &amp; [1,0]. This array is now a 2*2 checkered matrix. 2. Tile function can now be used to create checkered matrix of larger size by replicating our initial matrix more times 3. For a n*n matrix, you have to replicate it n/2 times as below: n = 2 num=int(n/2) Z = np.tile( np.array([[0,1],[1,0]]), (num,num)) print(Z) Hope this helps.",310511.0
88841,368161.0,"Cannot post the solution but for concept try this. Instead of 1x2 array, try creating a 2x2 array and then repeat it by tile function.",308962.0
88841,368190.0,"Thanks all for your answers , understood",308495.0
88841,368186.0,"I assume your question is regarding the below ; Given an even integer ‘n’, create an ‘n*n’ checkerboard matrix with the values 0 and 1, using the tile function . My approach would be much simpler( this is not a solution but a tip ) ; you create a tuple of 2 lists of [0,1] and [1,0] and you can create a checkboard matrix of the tuple using tile function with matrix dimensions of n // 2.",311115.0
88841,368149.0,"Same here, only telling you the approach and not the solution. you can consider a (2,2) matrix as a pattern, and repeat using tile function.",318433.0
88841,368124.0,"I am not posting the solution but posting the approach Approach1: You can create it using initializing an nXn array of zeros. Than traverse through each odd rows and make every even element =1. Traverse through each even row and make every odd element =1. I am not posting the solution by giving the approach Approach 2: use tile function. Tile is used to repeat an array. So, checkered matrix is an repeating pattern. Hence can be generated using tile.",317689.0
88841,368245.0,"arr = np.array([[0,1],[1,0]]) # creating an array hi=np.tile(arr,(int(n/2),int(n/2)) # and adjusting the rows and columns according to it",304692.0
88841,368314.0,think in terms of a small array that can be repeated.,318360.0
88841,368328.0,"1. First create a 2*2 array from 2 list [0,1] and [1,0]. 2. Apply np.tile() in given array which can repeat the given array n number of times. My approach is : import numpy as np n=int(input()) n_1=(n//2) # This gives matrix dimension in integer data type arr=np.array([[0,1],[1,0]]) checkerboard=np.tile(arr,(n_1,n_1)) print(checkerboard)",310419.0
88841,368379.0,"Below approach works too 1. Create an numpy array, arr=np.array([[0,1],[1,0]]) 2. Use tile function to repeat the above array and slice the obtained array to get an array of 'n' dimension, np.tile(arr,(n,n))[:n,:n]",310467.0
88841,369466.0,"Hi, My apporach is : import numpy as np Z = np.zeros((n,n),dtype=int) Z[1::2,::2] = 1 Z[::2,1::2] = 1 print(Z)",314183.0
88841,370104.0,"Try following:- n = int(input()) array = ([0,1],[1,0]) import numpy as np Tile = np.tile(array, (int(n/2),int(n/2))) print(Tile)",303673.0
88841,370466.0,"Hi, My approch is: import numpy as np Z = np.zeros((n,n),dtype=int) Z[1::2,::2] = 1 Z[::2,1::2] = 1 print(Z)",314183.0
88856,368254.0,adjust the space in the second print statement there should be no spaces,304692.0
88856,368252.0,"print(""{}"".format(array_1.shape)) print("" {}"".format(array_1.ndim)) you will get the output",304692.0
88856,368276.0,"Instead of - array_1 = np.array([list_1],[list_2]) Try - array_1 = np.array([list_1,list_2])",308962.0
88856,368270.0,"The np.array() function accepts objects of array-like structure (like lists, tuples etc). In the above example, you need to pass the nested list of list_1 nad list_2 to the np.array() function. Modify the line: array_1 = np.array([list_1],[list_2]) as array_1 = np.array( [ [list_1],[list_2] ] ) Hope this helps.",313826.0
88856,368277.0,"Hi, pls note that here we are supposed to create a 2d array. The line : array_1 = np.array([list_1],[list_2]) will not work I think. Recall what professor said about creating d-dimensional arrays. Each d-1 dimensional list to be passed as component of overall d-dimensional input which in turn itself needs to be passed as an overall list.",311686.0
88856,368425.0,"Replace the statement 'array_1 = np.array([list_1],[list_2])' in ur code with the following array_1 = np.array([list_1,list_2])",311119.0
88856,368544.0,"Please use array_1 = np.array(list_1,list_2) for print statements we need to use ""format"" function print(format(array_1.shape)) print(format(array_1.ndim))",318358.0
88856,368777.0,"import numpy as np list_1 = [10,11,12,13] list_2 = [15,12,13,14] a1=np.array([list_1,list_2]) print(a.shape) print(a.ndim)",300735.0
88856,369081.0,"Hi Vinay, This is the general structure for creating Numpy array. You need to pass the lists in an array. np .array ( [ Pass all the iterable data structure(like List,tuple ) seperated by delimiter(,) ] , If you want to mention dataType use dtype = np.TheDatatype you want ) Second argument is optional.",315028.0
88856,369083.0,"follow below. array_1 = np.array([list_1,list_2]) print(""{}"".format(array_1.shape)) print(""{}"".format(array_1.ndim))",300718.0
88856,369085.0,"Hi Vinay, This is the general structure for creating Numpy array. You need to pass the lists in an array. np .array ( [ Pass all the iterable data structure&lt;like List,tuple &gt; seperated by delimiter(,) ] , ,TheDatatype you want&lt;Optional&gt; ) Second argument is optional.",315028.0
88856,369211.0,"import numpy as np array_1 = np.array([list_1],[list_2]) You have mentioned list as a list item and the entire elements should be in a list format.",318370.0
88856,369459.0,"Hi vinay, For this problem, the code will be: array_1 = np.array([list_1,list_2]) print(array_1.shape) print(array_1.ndim)",314183.0
88856,369499.0,"Hi Vinay, Creation of array using NumPy should be like np.array( [list_1, list_2] ) but you have created as np.array( [list_1], [list_2] ). so,we have to pass data structures i.e. lists or tuples in [ ] seperated by Comma. Without using format function also you will get the same output. The main error in code is in creation of NumPy array.",300733.0
88856,370758.0,"list_1 = [1,1,2,3] list_2 = [5,2,3,4] import numpy as np array_1 = np.array([[1,1,2,3],[5,2,3,4]]) print(array_1.shape) print(array_1.ndim) OR list_1 = [10,11,12,13] list_2 = [15,12,13,14] import numpy as np 'array_1 = np.array([list_1],[list_2])' print(array_1.shape) print(array_1.ndim) Both will try this.",308639.0
88856,368291.0,"The 1st parameter for np.array() should be an array like object. What you are doing is, passing the list_2 as a second parameter to the np.array function. and thus, the function assumes it to be the value for d-type parameter (data-type) and this creates a problem. you should write it as np.array( [ [list_1],[list_2] ] ) you can refer this link for more details on the parameters to be passed: https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.array.html Hope that helps.",317998.0
88856,371307.0,"Use this.. The correct code list_1 = [10,11,12,13] list_2 = [15,12,13,14] import numpy as np array_1 = np.array([list_1,list_2]) print(array_1.shape) print(array_1.ndim)",301555.0
88856,371719.0,"you need to use this code : array_1 = np.array([list_1,list_2]) as list_1 = [ 10,2,3] i.e. [ ] --- these brackets are already provided by the Upgrad console , so dont use array_1 = np.array([list_1],[list_2]) or array_1 = np.array([[list_1],[list_2]]) as barackets are already populated. however, second code : array_1 = np.array([[list_1],[list_2]]) is also right in Jupyter , but in Upgrad Console brackets for list generate automatically thats why it creates a problem.",315560.0
90376,376884.0,Try to refresh webpage. If problem still continues try 2-3 times. And if still continue try after sometime. This can happen either because of slow internet connection or server is busy.,317991.0
90376,376772.0,,318762.0
90376,376776.0,"Working fine for me. May be there was some temporary glitch, try out now.",313826.0
90376,378541.0,Try refreshing by ctrl+F5. It should work. Same thing had happened with me as well. clearing cache and refreshing worked for me.,318440.0
90376,387251.0,This got resolved automatically. Thanks for trying to help.,318762.0
88862,368294.0,I believe you are referring to time module imported in the last segment of the notebook. time is a module in python is used to handle various time related operations. You can read about it at following link for time method in time module. https://docs.python.org/3/library/time.html#time.time,317689.0
88862,368299.0,Mostly time is a package that is used to calculate the computation time there are many packages similar for knowing the date we have a date package and calendar package similar,304692.0
88862,368313.0,"Please note time is a module in python which provides functions for working with times and for converting between representations of times. Some of the methods are: time.time() - Returns the current time instant, a floating-point number of seconds since the epoch. time.sleep(secs) Suspends the calling thread for secs seconds You can go through this link to know more about the available methods under time module https://www.tutorialspoint.com/python/python_date_time.htm Hope that helps.",317998.0
89665,372231.0,"As told by the professor numpy arrays are based on C language, hence are closer to the machine and are faster. Moreover numpy arrays donot involve any for loops which are sequential execution programs and might hamper the performance while reading billions of data. As numpy arrays donot involve any for and while loops so these are run by the computer in parallelism hence are much faster.",318358.0
89665,372369.0,check this article here: https://docs.scipy.org/doc/numpy-1.13.0/reference/internals.html explains about the internal memory organization in numpy.,309451.0
89665,372542.0,"NumPy arrays are stored in RAM. Python's tuples and list uses system memory. As explained by Professor, NumPy arrays are homogeneous. They are implemented in C language. Having homogenous data inside RAM as contiguously memory makes it very fast and allows it to be vectorized easily. Reference: https://ipython-books.github.io/",320074.0
90444,377208.0,"Since in 2Darrays the data is arranged in multiple rows and columns, to extract the particular element we need to specify its index in the matrix which pulls it out.. if you specify only index, i guess python assumes it as row index and brings all the corresponding elements from the colums for that row.. hope it helps..",316349.0
90444,377206.0,These are valid approaches. Are you getting an error in a specific question?,318085.0
90444,449155.0,It's all same .,306147.0
90444,377209.0,We can directly use -1 to find the last row/column and no specific need to calculate the last row / column using the shape of the array.,313826.0
88861,368301.0,"Yup, I also faced the similar issue. It just shows Submitted and nothing else. I think we should be good as it is Non graded.",314547.0
88861,368309.0,"Some of the coding questions do not have any non-sample test cases hence the correctness for those particular questions cannot be computed. Because of this reason, the question's final state remains at Submitted state. Hope that helps. Go to the FAQs section for more details/",317998.0
88861,368306.0,There's nothing wrong with it. Please go through the FAQs.,308962.0
88861,368308.0,Checkout the FAQ Section at the bottom of the page. https://learn.upgrad.com/v/course/208/session/19861/segment/101047,313826.0
88861,368303.0,That's not a problem because they answered the same question where you got the problem module just go to the bottom of the module FAQ's,304692.0
88861,369119.0,I also observed the same. But after reading FAQ. My doubt was cleared.,311952.0
88861,369479.0,"Hi Darshna, I have facing the same issue. But i have cheked the FAQs. I got the answer.",314183.0
88861,375526.0,I'm also facing this problem.,320687.0
88868,368390.0,"Many application areas I would say and I will pick just two broad topics: Gaming : I would typically use the random numbers when I would develop a gaming application, say Dice or Roulette wheel. When we use this function and generate an output random number Target/Control group in marketing campaigns : In marketing Campaigns, I would typically use Target/Control group of customers ; I would typically use the random number to arrive at a Random sampling of target group / control group; typically I will send the marketing message to target group and measure the effectiveness of Campaign using the control group; I will use random sampling to select the list of customers I want to funnel through. I will not cover specific syntax as it is already present in a lot of places.",309211.0
88868,368404.0,np.random.random(size) returns float of numbers between 0-1 . the size here is the number of float numbers we want to generate. too many examples to practice https://www.programcreek.com/python/example/5160/numpy.random.random,317982.0
88868,368465.0,"Use of array.random.random() : It generates random numbers between 0 &amp; 1. By default float. Significance: There may be scenarios in which we require n dimensional arrays to be created. If create these array using np.array([list1,list2,.....listn]) it is waste of time. To solve this problem array.random.random() helps.For instance array.random.random(1000,300) - it creates array of random numbers of 1000 rows &amp; 300 columns.",311119.0
88868,368385.0,"np.random.random() function is used to return numbers sampled uniformly from the interval [0,1) . (half open) I think, It is significant to learn because it helps to generate random data which can then be used for analysis purpose. Although, do note that the numbers generated are not purely random. you can go through this link for more insight. https://plot.ly/numpy/random/ hope that helps.",317998.0
88884,368449.0,"This is the example structured arrays Structured arrays are ndarrays whose datatype is a composition of simpler datatypes organized as a sequence of named fields. For example, &gt;&gt;&gt; &gt;&gt;&gt; x = np.array([('Rex', 9, 81.0), ('Fido', 3, 27.0)], ... dtype=[('name', 'U10'), ('age', 'i4'), ('weight', 'f4')]) &gt;&gt;&gt; x array([('Rex', 9, 81.0), ('Fido', 3, 27.0)], dtype=[('name', 'S10'), ('age', '&lt;i4'), ('weight', '&lt;f4')]) Here x is a one-dimensional array of length two whose datatype is a structure with three fields: 1. A string of length 10 or less named ‘name’, 2. a 32-bit integer named ‘age’, 3. a 32-bit float named ‘weight’. In your example ""i4"" is 32-bit signed integer If you want understand in details, then you need to learn structure arrays.",318319.0
88884,368430.0,"This will create a zero array of dimension 2 X 1 with each element having custom datatype of 32-bit signed integer pair (0,0). So, the number of variable you give in dtype will be the number of columns this will generate. for example: print(np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')])) &gt;&gt;[(0, 0) (0, 0)] print(np.zeros((2,), dtype=[('x', 'i4'),('y', 'i4'), ('z', 'i4')])) &gt;&gt; [(0, 0, 0) (0, 0, 0) (0, 0, 0) (0, 0, 0) (0, 0, 0)] So, to conclude this is generating an array in which each element is of dtype being custom defined and dimension defined in first argument. Similarly following will generate; print(np.zeros((2,3), dtype=[('x', 'i4'),('y', 'i4'), ('z', 'i4')])) &gt;&gt;[[(0, 0, 0) (0, 0, 0) (0, 0, 0)] [(0, 0, 0) (0, 0, 0) (0, 0, 0)]] this is 2 X 3 array with each element (0,0,0) having 32-bit signed integer.",317689.0
89531,371549.0,"You need to create a np.array() . Try the below code and then check a.shape and a.T. import numpy as np a=np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])",313826.0
89531,371564.0,"Hi, In your code, a is a list, not an array. To convert to an array, you need to enclose it inside np.array() function. a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]]) Hope this helps.",310511.0
89531,371641.0,"List has no shape attribute. If you want to use shape property, you will need to convert array to shape first. You can use below code snippet for the same. import numpy as np np.array([1,2],[3,4]).shape",318368.0
89531,371772.0,You have created a as a list. Try converting it to a numpy array and these would work. np.array(a).shape,317689.0
89531,371555.0,"Your are trying to execute method on list which are meant for numpy array. Here are the steps to follow: 1. Make a list and convert it into numpy array. Example: a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) 2. Now you can apply shape and T method you want to on 'a' which is now a numpy array. Hope this will help",317991.0
89532,371553.0,"You have defined the variable a as a list. you need to make it a numpy array, by doing this: array=np.array(a)",317998.0
89532,371562.0,"Hi, In your code, a is a list, not an array. To convert to an array, you need to enclose it inside np.array() function. a = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]]) Hope this helps.",310511.0
89532,371554.0,"Your are trying to execute method on list which are meant for numpy array. Here are the steps to follow: 1. Make a list and convert it into numpy array. Example: a = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]]) 2. Now you can apply slicing or any method you want to on this a which is now a numpy array. Hope this will help",317991.0
89532,371563.0,"The method of slicing you are attempting can be done on objects of type np.array where object a is an object of type list. Try the below code to first convert the list to an nparray and then do slicing: a = [[1,2,3,4], [5,6,7,8], [9,10,11,12]] col_1 = np.array(a)[:,0]",313826.0
89532,371639.0,"In the list you can only access elements using there indeces. if you want to access element 3rd of 2nd element. you can write a[2][3] But if you slice rows and columns. You will need to create array from the list. import numpy as np a = np.array([[1,2],[3,4]]) col = a[:,0] print(col)",318368.0
89532,371643.0,"First, convert the array into list np.array(a) and then perform the same operation.",304692.0
89532,372040.0,Its a list. not array. Convert list into array using np.array(list_name) and then perform operations.,311404.0
89532,372777.0,"'a' is a nested list in your example. If you want to print the first column, below are the steps. a = [[1,2,3,4], [5,6,7,8], [9,10,11,12]] # converting the nested list into array arr1 = np.array(a) # Retrieving the first column col_1 = arr1[:,0] print(col_1) Output: [1 5 9] Hope this helps.",314730.0
89533,371861.0,https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html You can go through these links to find how they are used. Hope it helps,310585.0
89533,371559.0,"These are used to initialize arrays. Afterwards, they can be used in binary array calculations like addition, mutiplication or matrix operations.",310511.0
89533,371637.0,"These will be useful when you will be dealing with large data set or you want to perform operations (Algorithm) on your matrix. Once you start with data analytics and data manipulation, you will find these functions very useful. Most of the time these functions are also used in scientific programming and solutions. You can find few easy usecases below. http://www.scipy-lectures.org/intro/numpy/numpy.html",318368.0
89533,371565.0,"tile function will repeat a numpy array for a specified number of times. for eg, a=np.array([0,1,2]) is a numpy array and you do and you do np.tile(a,2) you will get an array like [0,1,2,0,1,2]. ie, the values will be repeated n number of times, where n is the 2nd parameter passed to the tile function. np.full creates a numpy array where all the elements are equal to the element passed in the parameter. for eg, np.full((m,n),5) will create an m*n matrix where each of the values would be 5. np.eye() will return An array where all elements are equal to zero, except for the k -th diagonal, whose values are equal to one. for eg, np.eye(2) will give [[1,0], [0,1]] as output. random.randint(m,n) will generate a random integer between m and n hope that helps.",317998.0
89534,371632.0,"As error suggests ""Attribute Error: 'module' object has no attribute 'arrange'"" It means arrange is not available on np (numpy) library. Actual function is np.arange(start,stop,step)",318368.0
89534,372083.0,Spelling mistake brother ! Try arange(),303082.0
89534,371570.0,It is arange not arrange. Please try and let us know.,318554.0
89534,371571.0,Its just spelling mistake. It is arange() not arrange().,317991.0
89534,371575.0,"the function is np.arange(start,stop,step) and returns evenly spaced values within the open interval [start,end) Do note the, it is spelled with a single r and not double r. ie, not arrange, but arange",317998.0
89534,371647.0,"Actual function in numpy is np.arange(start,stop,step)",318358.0
89534,371979.0,"It is spelling mistake instead of arrange, please remove additional 'r and use arange as given below:- np.arange(0,n)",317811.0
89534,372739.0,As the others have suggested - try arange(),319302.0
89534,372789.0,"Whenver these kind of error occured , check for the colour of keyword . The Colour of the keywords are usually green or blue etc but not blank if spelling is correct .",310562.0
89534,373256.0,"Look like syntax error... Here is some important fact about arrange in Python NumPy numpy.arange() in Python arange([start,] stop[, step,][, dtype]) : Returns an array with evenly spaced elements as per the interval. The interval mentioned is half opened i.e. [Start, Stop) Example import numpy as geek print(""A\n"", geek.arange(4).reshape(2, 2), ""\n"") print(""A\n"", geek.arange(4, 10), ""\n"") print(""A\n"", geek.arange(4, 20, 3), ""\n"") Output : A [[0 1] [2 3]] A [4 5 6 7 8 9] A [ 4 7 10 13 16 19] For more detail please refer the below link... https://www.geeksforgeeks.org/numpy-arange-python/amp/",319319.0
89536,371630.0,"There are two ways to get the input in python. 1) Using command line argument while you run the program. import sys print (sys.argv) 2) Using raw input from user. text = int(input(""Enter a number: "")) Note: Above will through an exception if input is number. You will need to handle that exception and again ask user for valid int input. https://docs.python.org/2/library/functions.html#raw_input",318368.0
89536,371591.0,"This can be done by using input function e.g: For the case of integer as input a = int(input(""enter a integer"") Now, value of integer will be stored in ""a"".",318328.0
89536,371650.0,"Use the below: n = int(input(""Enter the number""))",318358.0
89536,371713.0,"In Python 2, you have a built-in function raw_input(), whereas, in Python 3, you have input(). The program will resume once the user presses the ENTER or RETURN key. Look at this example to get input from the keyboard using Python 3 in the interactive mode. Your output is displayed in quotes once you hit the ENTER key &gt;&gt;&gt; input() I am learning Python. 'I am learning Python.' More on using input To capture the input in your program, you will need a variable. A variable is a container to hold data. You can take the input and assign it to a variable. This is done using the = operator before the input keyword and then putting the variable name before the = operator. For example, when you give a sentence ""generic input"" as the input, this gets assigned to a variable, say, my_var. You can then print the value stored in my_var. Let us understand this with the following example: &gt;&gt;&gt; # take an input and assign it to a variable &gt;&gt;&gt; number = input() # The data you key in the next line which is 3 will be assigned to a number 3 &gt;&gt;&gt; print(number) # the next line will print the value in number after you press enter or return '3' Hope this explains better.",301648.0
90660,378031.0,"Othwerwise all other topics like , excel , SQL etc was very good . Only I cant work Python without basics learning .",319969.0
90660,378027.0,"How the indentation , space , {, : // \\ [ ; etc used and in python . Im totally stucked . Although I understand what is required in the question . It is like I can speak english but cant write ... Pl. help me",319969.0
90660,377997.0,Please go through following links. These link have basics of Python with integrated coding console. https://www.tutorialspoint.com/python/python_basic_syntax.htm https://www.w3schools.com/python/ You can start from basics from these links.,317991.0
90660,378009.0,"Pyhton basic alphbhets required to understands Pyhton ... becasue Im totally new like one electrical person is working in finance balance sheet preparation .. For me I need from the basics , I will read evrything but first I need info.",319969.0
90660,378016.0,Pyhton video Teaching was not understable to me . Because that was 2 or 4 level high then my existing IT learning skill,319969.0
90660,378020.0,Can you arrange same type of video lecture in Python basics .,319969.0
90660,378101.0,It is best that you refer to the official Python documentation from the beginning. You'll find an elaborate tutorial on this page with topics from the very beginning: https://docs.python.org/3/tutorial/index.html,306040.0
90660,378247.0,THANKS FRIENDS,319969.0
90660,378529.0,There is a course on Basic Python bootcamp on Udemy. It proved a lot helpful and the course is affordable like 700 bucks,318009.0
89548,371804.0,"Arrays can be accessed using notation array[x,y] In your case you are not passing two values for column and row. Row and column indexes must be separated by comma(,) i.e. col_first = array_2d[:,0] and so on....",318368.0
89548,372072.0,"Try the bellow code import numpy as np Base_array=np.array([[0,1],[1,0]]) print(np.tile(Base_array,n//2,n//2)",302744.0
89548,371732.0,"For checkboard matrix u need to take input array as below: np.array([[0,1], [1,0]]) Also take n as half while creating matrix using tile function",318358.0
89548,371973.0,"You posted snapshot of different question and mentioned description of other question. Please use add details option while posting questions on Discussion forum. It seems your question was Extract all the border rows and columns from a 2-D array. Format: Input: A 2-D Python list Output: Four NumPy arrays - First column of the input array, first row of the input array, last column of the input array, last row of the input array respectively. rows = len(array_2d[:, 0]) cols = len(array_2d[0, :]) col_1 = array_2d[:, 0] row_1 = array_2d[0, :] col_last = array_2d[:, cols-1] row_last = array_2d[rows-1, :]",317811.0
89548,371975.0,"Another Solution is: a= str(array_2d.shape) b=a.lstrip('(').rstrip(')') c=b.split(', ') ro=int(c[0]) co=int(c[1]) col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,int(co)-1] row_last = array_2d[int(ro)-1,:]",317811.0
89548,371976.0,"If you question was checked matrix: n = int(input()) import numpy as np list1=[0,1] list2=[1,0] array_1 = np.array([list1,list2]) print(np.tile(array_1,(int(n/2),int(n/2))))",317811.0
89548,373422.0,"Try using tile function twice- First one to create base array and tile function with (base array,(n//2,1)).",301118.0
89548,374195.0,"you need to put all the values in a single array like : np.array([f_r,s_r,t_r,f_r])",318017.0
89548,374545.0,"M not sure if you want clarification for checker board problem(The subject) or extracting border values(screenshot) So i'll reply for both. Checkerboard problem : n = int(input()) import numpy as np array_1 = np.array([[0,1],[1,0]]) print(np.tile(array_1,(n//2,n//2))) Border Values problem: array_2d[:,n] where n can be 0(first element) or -1(last element) Suggestion: Add question in correct section and provide correct screenshot. :)",318495.0
89908,374134.0,"Python has lot of Module and each one has different purpose. So, Numpy is one of the Python Module which can be used for Data Analysis",318804.0
89908,374159.0,Python3 is a language and Numpy is a scientific library available in Python3 for scientific/numerical computation.,318368.0
89908,374169.0,"In a Data science setup, there are two different team - Data Engineering and Data Science. The Data Engineering team will be responsible for setting up the Big Data environment for the Data Science team to explore the data and run machine learning algorithms. I believe basic Python framework will be used by Data Engineering Team and other numerical and scientific libraries like NumPy, Pandas will be used by Data Science Team for machine learning algorithms. This is my assumption. Maybe some experts/TAs can confirm this.",318084.0
89908,374236.0,yes both can work at the same time Numpy is a library to store data in the form of array to do various analysis on it. and python 3 is a language that we are using to answer almost every question.,318017.0
89908,374720.0,Thanks Deval Yadav. So Numpy is just a library and part of Python3?,311169.0
89908,374811.0,"One more way to interpret according to me is that basic Python provides Object Oriented Programming framework, whereas libraries like Numpy, Pandas etc give it a flavour of functional programming. This leads to my assumption that Object Oriented programming is more useful for setting up the Big Data environment by the Data Engineering team, which can later be used by the Data Scientists for executing the machine learning algorithms. Requesting TAs or other experts to validate my understanding.",318084.0
89908,374814.0,We will always need Python to be able to use Numpy. Numpy is a library used within Python.,319302.0
89908,374815.0,"Python3 is an overview or execution engine you can say. it supports basic functions like import. Numpy is a package which has functions and methods written. You need to import the packages like Numpy, pandas etc to actually do any operation. In any company, Operations or Installation Team will take care of python3 installation and also installing packages. Data Engineers and Data Scientist will be using numpy, scipy, pandas, tensorflow, matplotlib and many more packages for their implementation.",301555.0
89762,372860.0,"FYKI : In question a note could have been added to use Shape or NOT to use hardcorded valuse. I see I'm not the only one who encountred this, few more as well. Appericiate if you could do anything about this.",306735.0
89762,372859.0,"your code is got rejecting because use hardcoded values for col_last and row_last. you have to provide generic solution like for any mXn matrix. you can use below method array_2d.shape[0] - will provide the number of rows array_2d.shape[1] - will provide the number of columns col_last = array_2d[:,array_2d.shape[1]-1] row_last = array_2d[array_2d.shape[0]-1,:]",320073.0
89762,372876.0,"As Bhanu pratap mentioned your code is getting rejected as it is hardcoded. i also end up doing that most of the time but any code when we write should not be restricted to a particular input value hence it should be generic and you can take that as default criteria for all the coding questions. Using the shape function is optinal here . col_1 = array_2d[:, 0] row_1 = array_2d[0, :] col_last = array_2d[:, -1] row_last = array_2d[-1, :] or using shape function try the below row,col=array_2d.shape(its assigning the array row and column values to the variable even this step is optional if you want to skip this then instead col-1 and row-1 you need to mention array_2d.shape-1 ) col_last = array_2d[:,col-1] row_last = array_2d[row-1,:]",300687.0
89762,373447.0,Try not to hardcode anywhere in the code. It should be more or less generic,301555.0
89917,374174.0,I found below link very usefule to get clear understanding and difference between Numpy and Tensorflow. Please go thorugh it. https://www.quora.com/What-is-the-difference-between-Numpy-and-TensorFlow,317991.0
89917,374206.0,Numpy is a Python library for n-d-array computiation TensorFlow is a framework for Machine Learning &amp; Nueral Network with has plugins for C++/Python/Java etc.,306248.0
89917,374234.0,"Numpy is used to store data in array form and do various analysis on it where as TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google",318017.0
89917,374355.0,"Numpy is mathematical computation library in Python. TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.",318368.0
89948,374472.0,"a[:, 1] is used to print all rows for 2nd column a[:,1:2] will also do the same",318358.0
89948,374484.0,"a[:,1:2] means all rows and columns starting from column 2 until column 3 but not including column 3..this is same as a[:,1] which also gives all rows and column 2",314547.0
89948,374502.0,"Hi Vikas, Ideally, there is no difference between the two commands as both of them perform the same operations Just my observation (Anything, before the comma is to extract the rows and anything after the comma, is to extract the columns).",300688.0
89948,374547.0,"Suppose, a is a 2D np.array whose value is [[1 2 3] [4 5 6] [7 8 9]] a[:,1] --&gt; This returns a 1-D array of the elements present in the column with index 1 &gt;&gt;&gt; a[:,1] array([2, 5, 8]) &gt;&gt;&gt; a[:,1].shape (3,) a[:,1:2] --&gt; This returns a 2-D array, again of the elements present in the column with index 1 &gt;&gt;&gt; a[:,1:2] array([[2], [5], [8]]) &gt;&gt;&gt; a[:,1:2].shape (3, 1) a[:.1:2] returns a 2-D array because the moment we provide a : in the index , the numpy assumes that we are selecting multiple colums/rows and hence prepares a multi-dimensional array. Try out a[1:2,1:2]. Even though we are technically selecting only a single item i.e., the item at the row with index 1 and column with 1 (same as a[1,1]), it still returns a 2-D array with a single itme. &gt;&gt;&gt; a[1:2,1:2] array([[5]]) &gt;&gt;&gt; a[1:,1:2].shape (2, 1) Hope this answers your query.",313826.0
89948,374617.0,"While accessing array syntax is array[x,y]. x and y can be indices , ranges or list of indices . In first case a[:, 1] -&gt; Get all the rows and only 1st column. In second case a[:, 1:2] -&gt; Get all the rows and get column number 1 to 2 (1 included and 2 excluded). Syntatically both are different but doing the same thing.",318368.0
89948,375066.0,"If you check the screenshot, both the ways is giving me the same value of the result. If you check the data type, they both are of array dtaa type. Now , i did a reshape of y array for reshape to 5,1 and 5,2. while 5,2 aborted, 5,1 worked fine which clearly says that this is also a numpy array of (5,1) though showing as (5,) when we trying the shape command. With above screenshot, it is clear that both the commands are same and creates a numpy array of same dimensions.",301555.0
89960,374609.0,First statement will create array and tile function will repeat that array for the provided dimension. Creating array is pretty straight forward and you will get x * y array once you create it. Tile will repeat that array n * m times. Think of tile as a function that create array of n * m for the element (array) you provide. If array is X and you repeat if 2 * 2 times you will get. [[X X] [X X]] Now you imaging to replace X with your original array and you will understand you result.,318368.0
89960,374585.0,"This code first creates an 2-D array(matrix) with 2rows and 2columns and stores in x [ [0 1] [1 0] ] Now tile(A,(b,c)) repeats array A b times vertically(creates b row iterations of A) and c times vertically. ex: tile(x,(2,3)) would give 1 2 3 times [[ 0 1 0 1 0 1 ] 1 [ 1 0 1 0 1 0 ] [ 0 1 0 1 0 1 ] 2 [ 1 0 1 0 1 0 ]] times so basically as the x is 2x2 matrix, tile(x,2,3) gives a 4(2x2) x 6(2x3) checkerboard matrix. Thus tile(x,n//2,n//2) would give a nxn checkerboard matrix. We awould give re using n//2 to give a integer value as n/2 would give float value. ex: 4/2 = 2.0 while 4//2 = 2",317995.0
89960,374562.0,Checkout the solutuon video which explains very clearly the logic https://www.youtube.com/watch?v=cak6Y6yN4AU&amp;feature=youtu.be,313826.0
89960,375536.0,"This is the link from geekforgeeks for understanding checkboard matrix. In this, we need to understand how does slicing works in array. https://www.geeksforgeeks.org/python-program-print-checkerboard-pattern-nxn-using-numpy/",301649.0
88846,368195.0,"you have hardcoded the value of col and row, you should use shape attribute of array to determine the last row and col.",318433.0
88846,368200.0,"In the last two statements, where you have hard-coded 3 and 2, just give -1. It will then take the last column/row respectively.",314547.0
88846,368220.0,"Don't hardcode your col_last, row_last as per sample input as the input while submission can be of different dimension. So, in order to make it generic we can use -1 index to refer to last element.",317689.0
88846,368229.0,"In your code, you are assuming the dimensions of the array to be 2*4 always. 2*4 is just given as a sample input array. other arrays might not have the same dimensions. Although, the code for getting the first row and colum would be same, the code for getting the last row and column willl depend upon the dimensions of the array. To get the dimensions you can use array.shape which will return a tuple [m,n] where m is the number of rows and n is the number of columns. Thus, to get the last row you need to do row_last = array_2d[array_2d.shape[0]-1 , :] and to get the last column, you can do: col_last = array_2d[:,array_2d.shape[1]-1] Hope that helps.",317998.0
88846,369652.0,"The index of first item is 0 and The index of last item of an array is -1. Please use this to find last row and last column and first row and first column. col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,-1] row_last = array_2d[-1,:]",301643.0
88846,370626.0,"try usin -1 for thr last row or column eg: print(array_2d[:,0]) print(array_2d[0,:]) print(array_2d[:,-1]) print(array_2d[-1,:])",318009.0
88846,371229.0,"To reference the last column or row use the reverse numbering i.e -1 for last, -2 for second last etc.. For this question it should be [:, -1] for column and [-1,:] for Row.",318404.0
88846,371397.0,Don't hardcode the values for last column and last row.let the program figure it out itself by extractin the values out of the shape Tuple shape1=array_2d.shape m=shape1[0] n=shape1[1],305655.0
88846,372543.0,"use below code col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,-1] row_last = array_2d[-1,:]",302744.0
88846,376804.0,"Use index 0 for fist element and index -1 for last element and print the result, else it won't accept col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,-1] row_last = array_2d[-1,:] print(col_first) print(row_first) print(col_last) print(row_last)",317577.0
89018,369039.0,"U need to take the array as: array_1 = np.array([0,1],[1,0]) To create a checkboard matrix use np.tile() function Also in tile() function use the input n as half as we have already declared array containg 2 lists",318358.0
89018,369158.0,This is a brainteaser question. n can be even or odd number. Create a checker matrix using numPy.,318458.0
89018,369468.0,"Hi, My approch is: import numpy as np Z = np.zeros((n,n),dtype=int) Z[1::2,::2] = 1 Z[::2,1::2] = 1 print(Z)",314183.0
89018,369808.0,"Nidhi - Good approach. It can be consolidtaed in one line as below - import numpy as np n=7 ar = (np.arange(n**2).reshape(1,n,n)[0]%2) if(n%2==1) else (np.tile(([0,1],[1,0]),(n//2,n//2))) print(ar) What do you say?",318458.0
89018,371130.0,"import numpy as np list1=[0,1] list2=[1,0] array_1 = np.array([list1,list2]) print(np.tile(array_1,(int(n/2),int(n/2))))",317811.0
89021,369077.0,Yes. Even I observed that output is same for both the methods.,311119.0
89021,369104.0,Yeah this is same. In first you are calling a function np.shape() where as in second you are calling a method on numpy array by array_1.shape. These both will return same.,317689.0
89021,369564.0,Output is same. Multiple ways of doing same thing.,301555.0
89040,369208.0,"Thats great to find our own error in the code. Adding to that the code is asked to have the dtype by default, numpy creates data type = float64 so we can provide the datatype using the dtype array_x=np,full(rows_m,cols_n),int_x,dtype=int) Try the below in jupytr np.ones((5, 3)) np.ones((5,3),dtype=int)",300687.0
89040,369117.0,found the reason...its because of print in assigning array_x,300716.0
89040,369491.0,You've nested print() statements. Check this answer .,306733.0
89738,372755.0,"you are using np.ones, this will create an array consisting of only 1's. You have to use something generic. array_x = np.full((rows_m,cols_n), x) this will create an array with whatever x will be.",309451.0
89738,372757.0,"Thanks Madanjit! I got it,I have to use np.full to resolve it.",300699.0
89738,372751.0,U didnot multiply ur array with x .... Here u r only creating array of ones...Please multiply with x this might solve ur problem,318358.0
89738,372764.0,"Hello Nirmal, If the question is asking for a matrix with a specified integer value, better go for ""np.full"" For reference : https://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html",320195.0
89738,374538.0,"As question is asking for array having all values as x but your code is giving array with all values 1. In order to get desired output, you can do 2 things: 1) multiply the array with int_x and then print it 2) use np.full ( refer np.full )",318495.0
89738,375277.0,"You are using np.ones(),and so this will create an array consisting of only 1's. Use: array_x = np.full((rows_m,cols_n),int_x) print(array_x) for specified result as we want to get the input from user and not provide values as 1's in all places.",318427.0
89060,369235.0,You can use reshape() which is a built-in functions,306248.0
89060,369276.0,"You can also see the use of meshgrid for that purpose. Like this x = np.arange(-5, 5.1, 0.5) y = np.arange(-5, 5.1, 0.5) X,Y = np.meshgrid(x,y) print(X) Reference: https://stackoverflow.com/questions/32208359/is-there-a-multi-dimensional-version-of-arange-linspace-in-numpy p.s: I am not that well versed with python to know the intricacies but found some useful information, relating to your question, in the above link. hence sharing., Hope it helps.",317998.0
89060,369250.0,you can also use np.random.rand(). Please go through this link https://www.google.co.in/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://docs.scipy.org/doc/numpy-1.14.1/reference/generated/numpy.random.rand.html&amp;ved=2ahUKEwjozIXq9-ndAhXEo48KHSxSBHAQFjAAegQIBBAB&amp;usg=AOvVaw2EDczbaEYClWSZcJJm3opb you can also check for np.random.random() in this link https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.random.html hope it helps,310585.0
89060,370148.0,You can use range with dim list = [ [ [ '' for i in range(dim0) ] for j in range(dim1) ] for k in range(dim2) ],318846.0
89060,370199.0,"You Can use the reshape() fucntion to chnage the 1D array generated by arange to a multidimentional array. np.arange(27).reshape(3,3,3) Above statement would generate a 3 dimentional array using numbers 0 to 26 as below: array([[[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8]], [[ 9, 10, 11], [12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23], [24, 25, 26]]])",306725.0
89060,370265.0,"If above comments are not what you are looking for, can you rephrase and put down your question so i can share the solution.",301555.0
89060,370623.0,"You can use numpy's random function np.random.random((5,5))",318368.0
89110,369448.0,All the examples of the lectures are provided as Notebook in the beginning of the session. Download that and execute it with the sessions.,317689.0
89110,369576.0,"The entire Python has so many books or online word tutorials, but still a classroom course is better to learn, because someone explains what is being done and why. For the same reason, the code is provided, and also explained in videos, the only intention is , toprovide more clarity and explanation and not just the code. It is also found that this way, students remember the code better then by just providing the code.",301555.0
89061,369230.0,"Z=np.arange(1,11)*5 In arange function, specify the start. Syntax: arange(start,stop,step) Refer: https://www.geeksforgeeks.org/numpy-arange-python/",314547.0
89061,369238.0,"Specify the start parameter as 1. numpy. arange ([ start , ] stop , [ step , ] dtype=None ) The arange() function can take 3 parameters i.e., start , stop and step . The parameters start and step default to zero and 1 respectively, meaning if only one parameter is provided, then it is assumed as the stop parameter with start=0 and step=1. Read the complete details on the link below : https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html#numpy.arange",313826.0
89061,369246.0,"the issue is happening because of the improper range that you've provided. As the range starts from 0, as per your code 0*5 would be 0 not 5 so please start the iteration from 1. please go through the link that Vinay has provided, it would be helpful.",310585.0
89061,369262.0,"Apart from rectifying the range you are taking. (like everyone has stated above), you can also selectively print your final output list, like this: print(Z[1:]) Although, correcting the range would be more prudent.",317998.0
89061,369290.0,"numpy. arange ([ start , ] stop , [ step , ] dtype=None ) So for the list to start from 5, start=5 so the above statement can be updated as numpy. arange (5,55,5)",311119.0
89061,370557.0,"give the range (1,11) in np..arange",318009.0
89135,369557.0,"The outermost is for 0-axis. inside that is for 1-axis. this is for a 2d array. in other terms, 0-axis is your rows and 1-axis is your columns for 2d array. similarly for n dimensional array, the brackets will continue till (n-1) th axis. this is the way indexing is done in the dimensional arrays, so that we may access each element. hope that helps.",317998.0
89135,369562.0,"In 2D array is basically we are adding one more dimension to a 1D array. Now, 1D is represented by one square bracet [1, 2, 3] In 2D array for the second dimension for each value you will have another 1D array [ [ 1,2,3], [2,4,5] ] That is why 2 square brackets are required. But if there is only one value (1 D Array) in the 2d Array it will look like [ [1,2,3] ] Hence, [[1,2,3]] in concise form.",318554.0
89146,369626.0,"i believe it is correct. it is basically slicing the array. the syntax for that is array[ start:stop:step] since the step part is absent in your example..it will take default value, ie, 1. for start also nothing is mentioned so it will take default value of 0. and stop is 3. so, this will slice the array from [0,3) (open interval) ie. values at indexes 0,1,2 will be the output. ie, 1,2,3 will be the output. you can refer this for more insight https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html hope that helps",317998.0
89146,369633.0,"print(array_1[:3]) will print from 0(no start index, so default 0) index to index 3-1 means o/p will be: [1 2 3] Index concept is same on 1D array as lists.",318319.0
89146,369637.0,"Yes, as you are expecting, it should be comma separated values inside array. [1 2 3] should give you invalid syntax. [1,2,3] is the correct syntax",318554.0
89146,369643.0,It will give error while you try to run following: array_1 = [1 2 3 5 4 6 7 8 5 3 2] You have to specify commas between elements of the list be it integer or char.,317689.0
89146,369644.0,"This question is tricky and twiky in terms of language. Given an array array_1 = [1 2 3 5 4 6 7 8 5 3 2] it is equivalent to below statement array_2 = np.array([1 ,2, 3, 5, 4, 6, 7, 8, 5, 3, 2]) and now you apply your logic print(array_1[:3]) it will work. Hope it helps!!",307843.0
89146,369758.0,"This is correct. [:3] means it starts at index 0 and goes upto index (3-1)=2, i.e. index 0, 1, 2",317811.0
89146,369646.0,"Hi Bhaswati, Indexing is clear from the above comments. I think you are not declaring numpy array correctly. That's why you might be getting the error. If you are trying to create array in your case , you can create using the list like array_1 = np.array([1,2,3,5,4,6,7,8,5,3,2]) print(array_1[:3]) There are 5 general mechanisms for creating arrays: Conversion from other Python structures (e.g., lists, tuples) -- &gt; Eg. np.array([2,3,1,0]) Intrinsic numpy array creation objects (e.g., arange, ones, zeros, etc.) -- &gt; Eg. np.zeros((2, 3)) Reading arrays from disk, either from standard or custom formats Creating arrays from raw bytes through the use of strings or buffers Use of special library functions (e.g., random)",315028.0
89146,369795.0,"I have compiled your code and it is working. Please check the screenshot. Here array_1 = [1 2 3 5 4 6 7 8 5 3 2] has been represented as ""array_1 = np . array ( [ 1 , 2 , 3 , 5 , 4 , 6 , 7 , 8 , 5 , 3 , 2 ] ) "" because the array can't be directly given. I believe you might have made a mistake here only. Coming to the next line "" print(array_1[:3]) "". This takes the values of indexes of array_1 starting from 0(left of colon) and ending with 2 (Note: 3 is excluding and 0 is including). As per the code the elements of array_1 with indexes 0,1 and 2 are 1,2 and 3 respectively because of which [1 2 3] are printed. Hope it helps!",310585.0
93866,395538.0,"yeah thanks harish, it was jupyter notebook printing the array when i used print comman the commas disappeared",308495.0
93866,395526.0,"I've seen all the arrays with comma(,) in between the elements.. let it be np.ones(5) or np.arange(4) or anything.. Please check the type() of the functions to confirm if they're arrays..",316349.0
93866,395524.0,"Both of them DO NOT have commas in between elements. Try running following print(np.array([1,1,1,1,1])) print(np.ones((5,), dtype=int)) You will se no difference in output. You might have seen commas because you did not print the output and thus, jupyter notebook prints them with its own interpretation. hope that helps.",317998.0
89192,369829.0,"See, here they have made an array x with a dimension 2*2 and having values [[0,1],[1,0]] this is done because this is the smallest unit that will be repeating itself. so, for eg, if your input was 10. then you just need to expand your 2d array x, 5 times (half of input n) to the right and 5 times downwards . or, basically your 2d array x was repeated 25 times. so, to do this, the tile function is being used. the parameters being passed is x (the array to be repeated), the next parameter is the number of times it is to be repeated column wise (ie, towards right). so that is given as n//2. the third parameter is the number of times it is to be repeated row wise (ie, downwards). that also we need to do n//2 times. hence both parameters are n//2. This will then essentially create your checkered box of n*n dimension. Also, do note you need the 2nd and 3rd parameters being passed in the tile function to be i ntegers and that is why n//2 is done instead of n/2. cuz n/2 returns floar and n//2 makes it integer. Hope that clarifies..",317998.0
89192,370003.0,"Lets say we have been asked to create a 4*4 array using the tile finction and desired out as below We create a 2*2 array X as below: X = if you look at below image you will see the 2*2 array X is being repetaed in form of tiles: Now using X i can generate my desired array using tile function. As tile would repeat the given array and X is already a 2*2 array, my desired array wouild now be of dimention (4//2)*(4//2) (where // operater gives integer value for 4/2). Hence the expression for tile function becomes: np.tile( X,((4//2) , (4//2)) ) Or we can say np.tile( X,((n//2) , (n//2)) ) Hope this helps.",306725.0
89198,369873.0,"Hi Adarsh, ""Prof says, that in a d-dimensional array we ask python to create an outer list, that consist of d-1 dimensional list? This does not make sense"" &lt;= You're right, even I was puzzled by this because this doesn't apply to dimensions more than 2. We will have more than one outer list to represent arrays with &gt; 2 dimensions. No where in the videos a 3-d array has been created manually. Prof always created 2-d ir 1-d arrays by giving the values explicitly to that np.array() method. And, np.array([[1,2,3], [4,5,6], [7,8,9]]) this is a 2-d array which has the shape of 3x3 but that doesn't make it a 3d array.",310974.0
89198,369888.0,"Consider 2d array as point in XY plane. So for each X value there will be an Y value The example you gave, you can consider as 9 points in XY plane. a = array([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) So at X = 0 and Y = 2, the value is 3, a[0,2] or a[0][2] Now consider a 3d array, as points in XYZ plane np.arange(1,28).reshape((3,3,3)) np.arange(1,28).reshape((3,3,3)) a = array([[[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]]) Now point at coordinate 1,2,2 will be, a[1,2,2] = 18 So here at a[1] means it is a YZ plane(not plane but points projetions on that plane) cutting X axis at x=1. Hence this will give points in YZ plane array([[10, 11, 12], [13, 14, 15], [16, 17, 18]]) Then you are going a[1,2], this will give you all the [16,17,18] Little convolutelu you can think 3d array is 1d array of 2d arrays.",318554.0
89198,369942.0,"Dimenion of an array is different than the shape of the array. np.array([[1,2,3], [4,5,6], [7,8,9]]) This array has shape (3,3) where as its dimension is 2. So, when you are creating an n dimension array it consists of a list of n-1 dimension array. This is what was intended in the lecutures. So, if you take a closer look at your array it is a list having 3 elements of dimension 1. So, if you want to create a 3-d array from above you need to define it as following: np.zeros((2, 3, 4)) Here it creates an array of dimension = 3 and shape = (2, 3, 4). Even if you reshape the above array to (3,4,2) its dimension remains 3. But if you change it to (3,8) than its dimension becomes 2. So, dont get confused between dimension and shape.",317689.0
89198,371730.0,https://realpython.com/numpy-array-programming/,315455.0
89198,371728.0,gothrough this link will get more insights,315455.0
89199,369886.0,"import numpy as np list1 = input_list[0] list2 = input_list[1] list3 = input_list[2] array_1 = np.array([list1, list2, list3])",304692.0
89199,369933.0,The first few lines of code which are already written are taking input while running the code from system and that input is taken in string format and than converted into list object which can be used. input_str = sys.stdin.read() # A string is read. input_list = ast.literal_eval(input_str) #String is converted to list. list_1 = input_list[0] #list 1 list_2 = input_list[1] #list 2 list_3 = input_list[2] # list 3 Now use these above lists to create your numpy array.,317689.0
89199,369911.0,"Your concern is somewhat correct that question gives the impression that we have to create array using specified list. ""Create a 3*3 array using list_1 = [1,2,3] list_2 = [4,5,6] list_3 = [7,8,9]"" But main problem comes here is that coding console has two type of test case 1. Sample test cases 2. Non-sample test cases which is hidden. When you write code keep in mind that code should be generic enough to pass both sample and non-sample test case. Here the list1,list2,list3 are given for references so that you get clear picture of question. Avoid hard coding while writing code. Hope that clarifies your concern.",317991.0
89199,369871.0,"The question clearly states ""Create a 3*3 array using list_1 = [1,2,3] list_2 = [4,5,6] list_3 = [7,8,9]"". The input in the answer that is partially provided in the coding console is being taken from the arguments passed to the module, you need it think it that way and proceed with the solution: input_str = sys.stdin.read() &lt;= This is where the input is being read input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] list_3 = input_list[2] import numpy as np array_1 = np.array([list_1, list_2, list_3])",310974.0
89817,373306.0,"np.ones and np.zeros returns an Array not list Comma is only while printing without print function. If you print the data using print function you wont see any comma like print(np.ones(5 ,int))",318368.0
89817,373316.0,This is due to Jupyter Notebook behavior as this displays data in rich text format. When you use Print funtion you wont see any commas and when you dont use print funtion it just displays with commas. ( this works only in Jupyter notebook not in IDE's),306735.0
89817,373391.0,"All np.array, np.ones, np.zeros return ndarray as output. Below is the explanation of that: np.array - https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.array.html np.ones - https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html np.zeros - https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html Below is the example of that. You can see the type of np.array, np.ones and np.zero in example provided in below screenshot",317991.0
89817,373336.0,"arrays returned by np.ones &amp; np.zeros are not same lists. arrays are stored in RAM , as contigous homogenous data. Where as lists are stored in system memory . As far as comma is concerned, to see the result of executed array, use a print function. Check the FAQ provided in ""Subset, Slice, Index and Iterate through Arrays"" section in ""Introduction to Py"". Upgrad team have explained, how print works.",320074.0
89817,375261.0,np.ones() or np.zeros() just creates an array .. when you print the array using print command you will get an output exactly as an normal array.,318427.0
89817,373775.0,The same answer is explained in one of the modules. I have gone through that module just now. Thanks guys for your help &amp; sorry if my was question looked redundant,312093.0
89817,374535.0,"np.ones &amp; np.zeros are not creating a list and has the commas in between the values. np.ones((m,n)) --&gt; returns all the values along with the type of the object.(Notice array is written in front of the printed values with commas) print(np.ones((m,n))) --&gt; will print elements of the array(without commas). Hope this clarifies the confusion.",318495.0
89817,375785.0,np.ones() and np.zeroes() always return an array.,317811.0
90067,375044.0,Install extension for Jupiter notebook which will help you in auto complete and syntax related things https://github.com/Jupyter-contrib/jupyter_nbextensions_configurator#installation,318329.0
90067,375024.0,I don't know the answer yet but found a link on similar lines. Hope this helps : https://stackoverflow.com/questions/33700112/ipython-code-completion-intellisense-on-dot-possible,311686.0
90067,375268.0,You can also try going for VSCode Editor. It has got a couple of interesting tools including code completion.,318499.0
89276,370161.0,"The problem in your code lies in below piece of code, because of it you are getting error. array_1 = np.array([[list_1],[list_2],[list_3]]) You can infer from the screenshot you shared. The expected output is [ [ 1 2 3 4 5 6 7 8 9]] But your output has one more dimension in it which is failing the test cases. [ [ [ 1 2 3 4 5 6 7 8 9] ] ] Re-write the code as per the syntax and your code will work perfectly fine.",317991.0
89276,370150.0,"The space is coming because you are creating a 3dimensional array instead of a 2 dimensional array. You should remove the square brackets used for individual lists, inside the np.array function. because, at the moment your code is meaning that, each of the elements of the 2d array is a list. thereby making it a 3d array. Try this, array_1 = np.array([list_1,list_2,list_3]) this will create a 2d array, and then you can print. it should work. hope that helps.",317998.0
89276,370180.0,"Hi, The expected output is a 2d array. But you are creating a 3d array. array_1 = np.array([[list_1],[list_2],[list_3]]) This is a common confusion. How I have taught myself is as below: The above code can be written as below by putting the values of list1,list2, and list3. array_1 = np.array([[[1,2,3]],[[4,5,6]],[[7,8,9]]]) because list1 = [1,2,3] and so on... So the thumb rule here is no of starting square brackets inside np.array() is equal to the dimension of the array. Thus the above code creates a 3d array. To create a 2d array, you need to write the code such that the no of square brackets are 2. The hint is there in the input. Try it out. Hope this helps.",310511.0
89276,370299.0,"The dimension of your array is higher than what is expected in the output. They are expecting an array of dimension =2 and shape = 3 X3. you are generating an array of dimention = 3 and shape = 3 X 1 X 3. Try writing as array_1 = np.array([list_1,list_2,list_3]) instead of array_1 = np.array([[list_1],[list_2],[list_3]]) As with above code you are adding 1 more dimension by adding additional square brackets.",317689.0
90755,378192.0,Make sure you don't get confused with Reload page and Reload frame. Clicking Reload frame works for me Good luck,300704.0
90755,378188.0,On the frames that are not loading -&gt; right click -&gt; then click on Reload frame Good luck.,300704.0
89391,370803.0,"You can use np.ones((shape))*x np.fill as others mentioned But as the question suggests in your screenshot you have to use tile method The syntax is, numpy. tile ( A , reps ) Construct an array by repeating A the number of times given by reps. This is the process of using tile function. Important thing to notice here: A can be an array or single number(ie array of 1 dimension of size 1) And reps can be tuple representing shape ie (m,n) Believe last 2 points will help",318554.0
89391,370796.0,"Hi Ranjana, I believe the screenshot you have shows the code of the sample solution provided by them. is that not working or the code you wrote was not working? Anyway, this is how i did it, you can compare it with your code and see where the problem arises. import numpy as np array_x = np.full((rows_m,cols_n),int_x,dtype=np.int) print(array_x) or, please share your code, it would be easier to debug",317998.0
89391,370798.0,use np.full method,318360.0
89391,370887.0,"The problem is in below code np.ones((3,3),dtype=int) You hardcoded the number of rows and column as(3,3) in np.ones() which will only work for (3,3) matrix. Test case passed because it has no of rows = 3 and no. of columns = 3. But when you submit code it will run for some non-sample test cases which would have no. of rows and columns other than (3,3), thats why your code fails. We are supposed to make code generic enough to work for any no. of rows and column. Below solution is already given in screenshot array_x = int_x*np.ones((rows_m, cols_n),dtype=np.int) Hope it clarifies your doubt.",317991.0
89391,370881.0,"The screenshot you attached show that this is sample solution. In that two probable solution is already mentioned in it. At the top of screenshot you can see there is video link https://youtu.be/VeXximV1FVc This video has the explanation for two solution mentioned in screenshot. Now, what problem you are facing can be addressed if you can share your code.",317991.0
89391,370860.0,"If the question is asking for a matrix with a specified integer value, better go for ""np.full"" For reference : https://docs.scipy.org/doc/numpy/reference/generated/numpy.full.html Apart from passing the int value that has to be populated, we can pass the data type and also the order. Hope it helps!",317981.0
89391,370888.0,"The problem is in below code np.ones((3,3),dtype=int) You hardcoded the number of rows and column as(3,3) in np.ones() which will only work for (3,3) matrix. Test case passed because it has no of rows = 3 and no. of columns = 3. But when you submit code it will run for some non-sample test cases which would have no. of rows and columns other than (3,3), thats why your code fails. We are supposed to make code generic enough to work for any no. of rows and column. Below solution is already given in screenshot array_x = int_x*np.ones((rows_m, cols_n),dtype=np.int) Hope it clarifies your doubt.",317991.0
89391,371372.0,"array_x = np.ones((rows_m,cols_n)*int_x , dtype=np.int) use this , working perfectly. took me a little time to make this code but its simple",315560.0
89391,372073.0,"Try below code which is simple. import numpy as np array_x = np.full((rows_m,cols_n), int_x) print(array_x)",302744.0
89391,377514.0,Try np.full here,310529.0
89691,372386.0,You can use help function. help(np.full),318368.0
89691,372429.0,"Yes. Python has inbulit functionality to check syntax and other parameters of any function. help() # Starts an interactive help help(""topics"") # Outputs the list of help topics help(""OPERATORS"") # Shows help on the topic of operators help(""len"") # Shows help on len function help(""re"") # Shows help on re module help(""re.sub"") # Shows help on sub function from re module help(len) # Shows help on the object passed, the len function For np.full just type help(np.full)",317991.0
89395,370829.0,"In your code, to extract the col_first, row_first, you have mentioned array_2d.shape[1]-4 For suppose, if we consider 6X6 array, then the value of it will be (6-4) i.e., -2 which means you are extracting array_2d[:,-2] which will not be a last column in the array It is the same case for row_first. So i would suggest to use col_first = array_2d[:,0] row_first = array_2d[0,:] Hope this helps",318804.0
89395,370846.0,"This is what I did: col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,array_2d.shape[1]-1] row_last = array_2d[array_2d.shape[0]-1,:]",310974.0
89395,370872.0,"The question here is simply asking first and last (rows and columns); so it would be better if we simply use 0 for the first item and -1 for the last item. Getting the shape and measuring the length of Rows and Columns can be suggested if there a relevant scenario for it. (Like find the middle Row or Column). So, for the given scenario array[:, 0], array[0,:], array[-1,:], array[:,-1] would be good.",317981.0
89395,370868.0,"Don't hard code indexes anywhere. Your solution should work for all 2-d matrices. Try using following: col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,-1] row_last = array_2d[-1,:]",317689.0
89395,370804.0,"For first row and first column you could directly use the index as 0. Similarly, for last column and last row you could use the index as -1. The way I have done it is as below: col_first = array_2d[:,0] row_first = array_2d[0] col_last = array_2d[:,-1] row_last = array_2d[-1] Ofcourse, you could do using the shape attributes as well. row,col = array_2d.shape col_last = array_2d[:, col-1] row_last = array_2d[row-1, :] Hope this helps.",313826.0
89091,369347.0,"Since you are using hardcoded lists to create the array, the same array would be printed for all the testcases and hence the rejection. Make use of list_1, list_2, list_3 and try again.",318329.0
89091,369338.0,The list is hardcoded please use the variables of the list while submitting please never hardcode the variable values.,300687.0
89091,369325.0,"Here the lists that are used to create the array have been hardcoded. array_1 = np.array([ [1,2,3],[4,5,6],[7,8,9] ]) As a result, no matter what the input is, the array_1 will always be created using these hardcoded lists and hence fail when the test cases pass some other input. Remove the hardcoing and instead use the variables in which the input lists are being stored from the input. Hope this helps.",313826.0
89091,369377.0,"The problem lies in this part of your code: array_1 = np.array([[1,2,3],[4,5,6],[7,8,9]]) Here you hardcoded the list element to np.array , while you were supposed to give the input from input_list. Make the required changes in the above code and you will get your solution working fine.",317991.0
89091,369384.0,"You need to create your array from the lists which are being passed when the code is running. Please remove following code and replace it with list_1, list_2,list_3. array_1 = np.array([[1,2,3],[4,5,6],[7,8,9]]) With above line you have hardcoded your array_1 to this particular value which is same as verfiy test case hence it is passing. Whereas it is failing while submitting. array_1 = np.array([list_1,list_2,list_3])",317689.0
89091,369715.0,As a programmer avoid hardcoding in any sense. hardcoding is taking the literal values given in the question and using that in your code. The output will always be the one you have written.,315383.0
89091,369726.0,"You are using hardcoded list.You are supposed to give the input from input_list. Please remove the hardcoded list and replace it variables with list_1, list_2,list_3. array_1 = np.array([list_1,list_2,list_3])",316041.0
89091,370184.0,"Use list 1 list 2 list 3 in place of 1,2,3... which u used in np.array",308437.0
89091,370069.0,"As a python developer we need to avoid writing the thing in complicated way. It will look confuse you if write like this. We can write the conde in the following way also: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) import numpy as np array_1 = np.array(input_list)",318461.0
89091,370147.0,"your code will work only for given test case, it will fail if input is changed. Make it more generic by using variables so that it works for different input(test cases)",316323.0
89091,370484.0,you've done hardcoding (in a way you have used constants instead of variables while declaring your array).,305655.0
89091,370620.0,"Youe 1st test case pass becuase it always verifies your program with hardcoded values as thats the input for 1st test case. When it actually run agaist other test cases, it will fail as 1st test case's values are static/hardcoded in the program. Replace the hardcoding with the variable defined above to resolve the problem.",318368.0
89091,371329.0,"you can use the following code to validate import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] import numpy as np array_1 = np.array(list_1) #Type your answer here array_2 = np.array(list_2) #Type your answer here array_3 = array_1 * array_2#Type your answer here print(list(array_3))",314313.0
89091,371793.0,"I also realized this error after seeing the final syntax for np.array, which includes two brackets; one curved one and other one squared. array=np.array([your_lists])- both brackets are must here and all lists need to be inside the square brackets only",313767.0
89705,372463.0,"-1 means an unknown dimension (we are not sure of that particular dimension) and we would want numpy to figure out that dimension. Obviously, we can provide only unknown dimension. In the given example, we are saying that we are sure that we want the reshaped array to have 3 rows,but not sure of the number of columns. WIth the given elements, numpy calculates that the number of columns as 2 and provides a 3by2 matrix.",313826.0
89705,372752.0,"Thanks for the responses. Just found that reshape method is explained in the operations topic in Numpy session 2. In this statement ""np.reshape(a, (3,-1))"", NumPy will automatically determines the number of columns if we mention -1 as one of the dimensions based on the size of the original array. In my example, since I condered a 2x3 as original array, when trying to reshape using (3, -1) size, Numpy will identify the number of columsn as 2.",314730.0
89705,372474.0,As the second argument is -1 it means that python has to determine the no of columns so that the array is reshape equally into 3 rows(1st argument is 3) You can refer the below link as well https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape Hope this helps,308635.0
90275,376242.0,if you use method like this it will be confusing when you enter into some nested elements.,318017.0
90275,376187.0,"The professor is trying to tell that the ""ideal"" way to access the elements of an array is by following the standard convention of accessing a matrix. However, it is still possible to access it the way we would access a nested list.",313826.0
89706,372480.0,Ref to below link for better understanding https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html,318368.0
89706,373137.0,"Hi vinod , This is about about efficency and programming style:- -when we refer to fast changeing index u can think of it as the largest index and the index we are most likely to iterate over. -in c the most fast changeing index is stored last while in fortran it is stored first -by default it follows c styple that is in a shape(a,b,c) the c index changes the fastest. - f you have to iterate over an axis, which one are you most likely to iterate over ,it is usually the last index(axis) - As far as efficiency goes, this assumes C-order. But in python even if u chouse ""C"" Or ""F"", as At the memory level, access to numpy arrays is treated as C-ordered regardless of the memory layout. -Please check these links out:- - https://stackoverflow.com/questions/27606209/order-of-indexes-in-a-numpy-multidimensional-array - https://www.visitusers.org/index.php?title=C_vs_Fortran_memory_order",317822.0
89424,370966.0,Ofcourse you can:,310974.0
89424,371050.0,Checkout the below link. It gives some ideas creating an array whose elements themselves resemble the structure of a tuple by setting the dtype attribute as object or some other custom dtype. https://stackoverflow.com/questions/26634579/convert-array-of-lists-to-array-of-tuples-triple,313826.0
89708,372485.0,"Below book is good: Python for Data Analysis Data Wrangling with Pandas, NumPy, and IPython By William McKinney Publisher: O'Reilly Media",303083.0
89708,372475.0,Use below links for Numpy http://www.labri.fr/perso/nrougier/from-python-to-numpy/ http://www.labri.fr/perso/nrougier/teaching/numpy/numpy.html,318368.0
89708,372506.0,Thanks Kapil Dave and Syed Jahangir.,311745.0
89708,372716.0,You have basic understanding of Numpy and Pandas through the lectures and material on of this course. So what I would suggest is get some dataset online and try numpy and pandas concepts that you learnt. And if you get stuck somewhere post your query here on discussion forum and you can check solution on stackoverflow . Believe me you will learn much more than by just reading or going through book. And also it is bit difficult to complete whole book and you might lose interest of learning it. Just a suggestion though.,317991.0
89708,372753.0,"Thank you Vipul Shrivastava for your suggestion. As I am completely new in this field , can you please suggest me some links from where I can get some datasets?",311745.0
89708,373015.0,few links fro datasets https://data.gov.in https://www.datacamp.com https://www.kaggle.com/datasets,317982.0
89708,373109.0,Thank you Srishti Agrawal,311745.0
89708,373405.0,"there are mny. be choosy, many books are available as free pdf, you need not buy",301555.0
89708,374014.0,Learning with Python by Allen Downey is considered to be a good book :),316416.0
89708,376902.0,Python for Data Analysis from foudner of Pandas : https://github.com/mravendi/data-science-machine-learning-ai-resources/blob/master/books/Python%20for%20Data%20Analysis.pdf,317577.0
89719,372582.0,"Ideally, all the imports should be done at the start of the program. However, the program would still run successfully even if the imports are done at any location in the program but ofcourse you have to import before using any of the attributes,functions,etc. belonging to the library are used.",313826.0
89719,372601.0,The compiler works properly until it get the package. The order deos not matter.,306248.0
89719,372756.0,Writing import statements at the top is not mandatory but writing import statements at the top make the code quite readable and we get a fair idea of whats coming ahead,318358.0
89719,372784.0,easy for coders but not good for reviewers or examiners.,304813.0
89719,373052.0,"It is always required to import library if you are using modules. So, every language has built in function. In python you have print(), len() etc. BUt some time you need to use complicated modules like numpy. For these we make import .",303082.0
89709,372473.0,"sys.stdin.readlines() is to read input from standard input and in jupyter are not providing any standard input to your code. You can try below code instead to provide sys.stdin in jupyter. (DONOT use same in assignment, its only for jupyter.) sys.stdin = io.StringIO(""line 1\nline 2\nline 3"") https://www.reddit.com/r/Python/comments/24b3nv/programmatically_providing_input_to/",318368.0
89709,372619.0,"Problem is with the data type. lines are not of int datatype. print( type(lines[0]) ) gives output as &lt;class 'str'&gt; which is clearly not an int as expected for the full() Correcting your code to below should solve the problem np.full((rows_m, cols_n), int_x)",320074.0
89709,375278.0,"Either provide the values as int_x , rows_m ,cols_n instead of lines[0],lines[1],lines[2] or provide the specific data type like : int(lines[0], int(lines[1] and int(lines[2]",318427.0
90287,376266.0,I believe you code passed through Verify and Run Code. Looks like your code should give the correct answer. But may be the code checker is expecting the cod to be written in different way.,318554.0
90287,376273.0,"Hello Nitish, In the last two statements, col_last = array_2d[:,-1] row_last = array_2d[-1,:] or col_last = print(array_2d[:,array_2d.shape[1]-1]) row_last = print(array_2d[array_2d.shape[0]-1 ,:])",320195.0
90287,376307.0,"You need to print col_first, row_first, col_last and row_last on seperate lines. I hope you are doing that, else it will get rejected.",313826.0
90287,376537.0,"The code works correctly. If you are still having the issue, please send in the details.",317149.0
90287,376805.0,"Use 0 for first element and -1 for last element and print the result as well : col_first = array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,-1] row_last = array_2d[-1,:] print(col_first) print(row_first) print(col_last) print(row_last)",317577.0
90287,377017.0,"Mayank has provided the right solution. Before you submit the code, you need to make sure that your solution works for different inputs not just for a provided input. In addition to the verify code, provide different possible inputs and check if it gives the expected output.",312376.0
90287,377198.0,"The problem in your code is the '(' and ')' i.e. the parenthesis used in the index place. Remove those extra parentheses and your code will work fine as mentioned by Nishan Patel as well. Reason : array_2d[ expects integer value while you are passing ( ) ] However , in Python, '- 1th' index refers to the last element, and hence you can directly use: array_2d[:, -1] #Meaning all rows and last column array_2d[-1, :] #Meaning last row and all columns The same is valid for other datatypes like list etc. in Python.",317987.0
91664,383779.0,"In the last print statement, you are missing one closing bracket print (type(array_3D))",318084.0
91664,383788.0,"I see a few errors here: Hardcoding in the below line. Remove hardcoding and use the 3 lists from the input nested lists: Change list_1 = input_list[1,2,3] to list_1 = input_list[0] . Do the same for the other 2 lists. You need to use np.array() function to create the numpy arrayand assign it back to your output array. Syntax is np.array([list1,list2,list3,...]) Do not print additional details which have not been asked in the problem statement. The comparison is done based on the output produced by your code against expected output, and any additional prints in your code would not match the expected output and hence fail. Hope this helps.",313826.0
90485,377716.0,Extract the csv file from zip folder and open the file in Jupyter notebook. Then read csv with only 'file name.csv'.,301644.0
90485,377452.0,Unzip than open.,317689.0
90485,377499.0,Jupyter doesn't understands .zip files. Just unzip your file and then open the .ipyb file.,306247.0
90485,377409.0,you are trying to open a .zip file. You need to unzip the file and then use the ipynb file in jupyter notebook.,309451.0
90485,377410.0,"You can't open ZIP file. First you need to unzip using any free software. Unzip, Unrar or winzip anything. Once you unzip it, you will get .ipyb file, which you can import to jupyter.",318368.0
90485,377424.0,Thanks a lot,308638.0
90485,377884.0,It is a zipped file. extract it and then you have open in jupyter notebook,314183.0
90488,377462.0,"Vectorization is a type of parallel processing. It enables more computer hardware to be devoted to performing the computation, so the computation is done faster. Many numerical problems, especially solution of partial differential equations, require the same calculation to be performed for a large number of data. Vectorization performs the calculation for many data elements in parallel. you can read more about python vectorised code performance here https://hackernoon.com/speeding-up-your-code-2-vectorizing-the-loops-with-numpy-e380e939bed3",317845.0
90488,377443.0,"Vectorization is nothing but making use of multiple cores for parallel execution of the similar tasks. For example, when you square a normal list, you might have to iterate over all the elements of the list sqaring each number while Squaring a numpy array is very simple in syntax and faster processing happens in the background because multiple elements of the array can be squared at the same time as they are independent.",318329.0
90488,377490.0,"Python for loops are inherently slower than their C counterpart. This is why numpy offers vectorized actions on numpy arrays. It pushes the for loop you would usually do in Python down to the C level, which is much faster. numpy offers vectorized (""C level for loop"") alternatives to things that otherwise would need to be done in an element-wise manner (""Python level for loop). import numpy as np from timeit import Timer li = list(range(500000)) nump_arr = np.array(li) def python_for(): return [num + 1 for num in li] def numpy_add(): return nump_arr + 1 print(min(Timer(python_for).repeat(10, 10))) print(min(Timer(numpy_add).repeat(10, 10))) # 0.725692612368003 # 0.010465986942008954 The numpy vectorized addition was x70 times faster. Source:- https://stackoverflow.com/questions/47755442/what-is-vectorization Hope this clarifies your doubt.",317991.0
90488,377777.0,"In a plain English language Vectorization is the term for converting a scalar program to a vector program. Vectorized programs can run multiple operations from a single instruction, whereas scalar can only operate on pairs of operands at once. https://en.m.wikipedia.org/wiki/Vectorization",310522.0
90481,377469.0,"Yes, definitely you can create your own custom package and use in code. You can read more about creating package and module here https://www.learnpython.org/en/Modules_and_Packages UpGrad console is managed by UpGrad team, I don't think you can import your custom library in UpGrad console.",317845.0
90481,377411.0,"I think it should be possible. We have imported numpy, pandas and functools package(for reduce function) in coding exercises So, there is a precedent of importing external packages",318085.0
90481,381903.0,"Array Arrange Function problem: Create an array of first 10 multiples of 5 using the 'arange' function. Am getting a message File ""/code/source.py3"", line 3 array_multipleof5 = np.arrange(5:55) ^ SyntaxError: invalid syntax Whats the error here please?",314329.0
88887,368519.0,"Yes actually refreshing the brower resolves this problem, please try that out.",306011.0
88887,368468.0,"Hello Harshendra , Yes we are also facing simmilar problem but refreshing the page will resolve this issue. It might be having cookies issue in browser and you are also getting good speed. Try to clear all the cookies because it work for me.",320195.0
88887,368408.0,Try refreshing the page it gets resolved.,317689.0
88887,368420.0,Try with some other Web Browser. It can also be due to the Internet Speed. Restart your Router.,314547.0
88887,368538.0,Though we are recommended to use Chrome for upgrad but the loading is much faster in Microsoft Edge...Please try this browser,318358.0
88887,368971.0,"Check out the below link from the ""Help"" section. http://help.upgrad.com/coding-console/the-coding-question-does-not-load In case the issue is still not resolved, then you may want to report the issue using the ""Report a mistake"" link.",313826.0
88887,380154.0,Here is my take on this issue for different browsers Chrome - The issue still remain even after taking all the necessary measures stated by Upgrad. IE - The layout is not good when you open in IE. Though theere are no issues in loading question stub Firefox(I would recommend) - Work like a charm . Layout is same as Chrome and no issues in loading question stub,301122.0
88931,368605.0,i think when you write 1:2 it is interpreted as from 2nd column upto 3rd column not inclusive of the latter. it is the same as just writing 1 because you are basically referring only to one column (the 2nd). as mentioned in your query if you increase the value after the colon ie changing it from 1:2 to 1:3 you include two last columns of your 3*3 matrix. hope this helps,319898.0
88931,368601.0,"2 here is uptill which column you want to fetch the value like in ur code the column starts from 1 that is the second element and 2 is the third element here but while slicing third element minus 1 is the element uptill which the column gets printed if u remove 2 from there, all columns starting from the column number before colon : gets printed",318358.0
88931,368628.0,"a[:, 1: 2 ] means all rows in the array and all columns from column index no 1 to 2 ie column no 2. Thus the answer is [[2] [5] [8]]",310511.0
88931,368658.0,"1st 2nd 3rd Columns [[ 1 2 3] - 0th row [ 4 5 6] - 1st row [ 7 8 9]] - 2nd row Here a[:, 1: 2 ] means slice the array . 1st option ':' means include all the rows from 0th to 2nd. And '1:2' means within column get the element whose index start with 1 but element at index 2 is not included. So in this case for each row 2nd element of each column is selected as output. Now comes to your example if we change 2 to 1 i.e a[:, 1:1] it will give [] as output which you can figure out why. And if we change 2 to 10 i.e a[:, 1:10] it will select every row and element from index 1 to index 9 in all columns that why the output will be [ [ 2 3 4] [6 7 8] [10 11 12]] Hope this helps.",317991.0
88931,368722.0,"Significance of 2 in a[:, 1:2] : The index of array starts from 0,1,2. Here 1:2 refers to a column with index 1 excluding column with index 2 . 2 is for up to which column index to look.It is the same as it was in the list.",311119.0
88931,368781.0,"short answer if you give [:, 1:2] it will preserve the dimension, if you just pass [:, 1] then will conver the result into an 1-D array. hope it helps!",304813.0
90577,,nan,
89257,370079.0,"Use np.full((m,n), constant)",318084.0
89257,370096.0,"Simpler way np.full( (m, n), 1, dtype=int ) More complex way x = np.arange(m*n, dtype=int).reshape(m, n) np.full_like(x, 1)",306248.0
89257,370092.0,"To create a 2 X 2 array with constant value 10 np.full((2, 2), 10) &gt;&gt; array([[10, 10], [10, 10]])",317689.0
89257,370893.0,"Yet another way, import numpy as np arr = np.tile(1, (m,n)) print(arr)",301652.0
89257,371266.0,"np.full((m,n),1,dtype=np.int) will work for you as universal function when you want to populate postions of your matrix with a single constant",305655.0
89257,371573.0,"sol_arr=np.full((m,n),1) in np.full function - there is no necessity to mention the number as int. it automatically adrresseses itself as Int.",300723.0
89257,371885.0,"np.full((3,3),1) a 3X3 array filled with 1s",313767.0
90999,379788.0,"Hi, Please go through below documnets for better understanding. https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html The basic slice syntax is i:j:k where i is the starting index, j is the stopping index, and k is the step ( ). This selects the m elements (in the corresponding dimension) with index values i , i + k , ..., i + (m - 1) k where and q and r are the quotient and remainder obtained by dividing j - i by k : j - i = q k + r , so that i + (m - 1) k &lt; j . Example &gt;&gt;&gt; x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) &gt;&gt;&gt; x[1:7:2] array([1, 3, 5]) Negative i and j are interpreted as n + i and n + j where n is the number of elements in the corresponding dimension. Negative k makes stepping go towards smaller indices. Example &gt;&gt;&gt; x[-2:10] array([8, 9]) &gt;&gt;&gt; x[-3:3:-1] array([7, 6, 5, 4])",320195.0
90999,379759.0,"arr[1::2,::2] 1: :2 before comma refers to the row part of a 2 dimensional numpy array 1 refers to the starting index of row : : refers that it will go upto last row as nothing is mentioned in between. 2 refers that the step will be 2 i.e. if the starting row is 1, then next rows be be 3,5,7,...so on upto last row. ******************************************************************************************************************* : :2 after comma refers to the column part of a 2 dimensional numpy array Before : nothing is mentioned which refers to the starting index of column will be 0. : : refers that it will go upto last column as nothing is mentioned in between. 2 refers that the step will be 2 i.e. if the starting column is 0, then next rows be be 2,4,6,...so on upto last column.",317811.0
89195,369970.0,"Your code works correctly on the upgrad coding console, I just tried it.",300717.0
89195,369844.0,"Please share the error that you are getting. Since this is a graded question, I would advise you to not share the code.",313826.0
89195,369841.0,can you share the code? would be easy to debug then.,317998.0
89195,370011.0,"No, it worked fine for me.",318458.0
89195,370117.0,"Thanks alot guys , here is my code , i ran it in Jupyter notebook and was getting the correct out put but here it does not give me any output import ast,sys import numpy as np n = int(input()) x = [0,1] y = [1,0] arr1 = np.array(x) arr2 = np.array(y) np.tile((x[0:],y[0:]),(n//2,n//2))",308782.0
89195,370128.0,"Hi Ahmed, I think the bug is in your last line of the code. Try using, print (np.array(np.tile((x[0:],y[0:]),(n//2,n//2)))) instead of the last line and see if it works.",317998.0
89195,370213.0,"you can try print( np.tile((arr1,arr2),(n/2,n/2)) if you say x[0],y[0] it would take the element of the array and may be in jupytr if you give the input 2 it might be working but in python console it has to work for all the input its just my understanding correct me if i am wrong",300687.0
89195,369874.0,"Hey, this is not a graded question. It is in module question. And, Ahmed, this is what I did, cross check with your code, import numpy as np arr01 = np.array([[0,1],[1,0]]) charray = np.tile(arr01, (n//2,n//2)) print(charray)",310974.0
91353,381850.0,"no, they are not the same. there are a few differences such as..size flexibility and number of different attributes associated with them. you can refer this for a detailed insight. https://stackoverflow.com/questions/38155039/what-is-the-difference-between-native-int-type-and-the-numpy-int-types",317998.0
91353,382846.0,there are some difference but at the most basic level it is like the difference between char and varchar if you choose a variable to be int - then the size of the storage unit increases and decreases based on the value of the variable; with np.int the size is pre-determined so the maximum value a variable can take is then limited to the size. so int32 will be limited to a variable having a value less than 2^32,300694.0
89736,372750.0,"Since your 2 submission out of 2 is rejected below is the modification you could have done in your code. col_last = array_2d[:,-1] row_last = array_2d[-1,:] And no need to extract number of rows and columns of array because to access last element of array we can use [-1] as index.",317991.0
89736,372747.0,"Hello PavanKumar, In the last two statements, where you have hard-coded 3 and 2, just give -1. It will then take the last column/row respectively. you should use shape attribute of array to determine the last row and col.",320195.0
89736,372763.0,In your code you have hardcoded the values for col_last and row_last Do not hard code the value as it will pass for the given sample test but not pass the hidden test cases which are tested when you actually submit the code. You can check the testcase for which your code is failing in the details section. The solution provided is the generic one which will pass all test cases.,308635.0
89736,372880.0,"AS most of them mentioned In the last two statements, where you have hard-coded 3 and 2, just give -1. It will then take the last column/row respectively. i could see you are using the len function to extract the number of rows and column but you have not used it anywhere. using the shape function will help you in extracting the row nad column values row,col=array_2d.shape col_last = array_2d[:,col-1] row_last = array_2d[row-1,:]",300687.0
89736,373289.0,"Hi Pavan , as per the first line of question , we were suppose to extract row and column and use those value to print last column and last row value as you have hard coded these value as per input given in first test cases so your solution output got match with test case 1 expected output so testcase got passed but it was not generic solution that’s why answer got rejected . We have to be careful while doing graded question . We have to think about generic solution as well rather than just focusing on test cases example input and output .",310562.0
89736,373473.0,"Hardcoding the row or column number has made the output specific to a given test case/s only. As some have suggested above, to generalize the code, you would need to alter the code with -1 to return the last row/column values. Using the above you would not need to check for the length of the array before looking for respective values. Both ways are correct - checking for length and without - however, depending on the method you choose, the code to return the respective values will change.",319302.0
89736,374210.0,"try this : col_first = print(array_2d[:,0]) row_first =print(array_2d[0,:]) col_last = print(array_2d[:,-1]) row_last = print(array_2d[-1,:])",318017.0
90876,378798.0,There is a brief introduction on how to use the jupyter notebook in the prepratory course. Please check out the below link: https://learn.upgrad.com/v/course/208/session/15860/segment/91791,313826.0
90876,378977.0,"Go to specific folder where you want to save the codes. Click on ""New"" drop down on Top Right. Select Python 3. A new page will open click on the box and write you code. execute using ""Shift"" + Enter. Basic stuff: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Notebook%20Basics.html",318436.0
90511,377562.0,the code that you make should be generic try with different inputs if it gives the desired result report the issue with report button else you should change your code so that it checks all the test cases.,318017.0
90511,377568.0,There are two types of test cases: 1. Sample test cases - which run against the input when you click on verify. 2. Non-sample test cases - which run against the input when you click on submit. Code gets 'verified' doesn't guarantee that your code is correct. It means for sample cases your code is working fine. But when you click submit code gets tested against non-sample(hidden) test cases also. So your code should be generic enough to pass both types of test cases. Why sbumission get rejected can be found in below link https://learn.upgrad.com/v/course/208/question/89590,317991.0
90511,377669.0,"Check for any hard-coding in your code. The problem description says that you need to accepts 3 inputs i.e., x , m and n, use these in creating the array. The code stub for accepting these variables is already provided in the console. Just make sure that you are making use of these variables to create your array.",313826.0
90511,378183.0,"Hope below helps someone making an attempt to correct the submission. Please read again, the questioin emphasizes on format ""Output"" to be ""last column and last row"" Think of a non-sample test case that has array_2d with input 4 rows, 4 column. As the number of rows, column change in the input test case - the generic code must be able to identify the last row, last column in the array_2d instead of harcoded position. Good luck.",300704.0
95132,402045.0,I request you to see graphival represntation of linspace with endpoint=True and False. It is very simple function - https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html,318458.0
95132,402044.0,"Consider f=np.linspace(0,1,6, endpoint=True), output: array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]) Here if endpoint = True means, the numbers will be formed with equal intervals and also the last number in the array will be the stop value i.e., 1. By default, the endpoint is set to True.",318804.0
95132,402007.0,"numpy. linspace ( start , stop , num=50 , endpoint=True , retstep=False , dtype=None ). endpoint: bool, optional IF True, stop is the last sample. Otherwise, it is not included. The default is True. Here if you mentioned endpoint as true then it will include 1 as per your example, IF you mentioned endpoint as False then it will not include 1 and will do the linspace accordingly.",301648.0
102498,438525.0,mostly in data structures,317982.0
102498,438606.0,"You mostly use some of the functions in NumPy in data analysis but the main use of the NumPy arrays, as you have asked will be relevant when you study the modules in machine learning.",306040.0
103003,441043.0,"https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html Broadcasting is explained beautifully here in the documentation. Broadcasting is simply a set of rules for applying binary ufuncs (addition, subtraction, multiplication, etc.) on arrays of different sizes. By different size, I mean of different dimensions.",301649.0
103003,440681.0,"NumPy operations are usually done on pairs of arrays on an element-by-element basis. In the simplest case, the two arrays must have exactly the same shape, as in the following example: &gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0]) &gt;&gt;&gt; b = np.array([2.0, 2.0, 2.0]) &gt;&gt;&gt; a * b array([ 2., 4., 6.]) NumPy’s broadcasting rule relaxes this constraint when the arrays’ shapes meet certain constraints. The simplest broadcasting example occurs when an array and a scalar value are combined in an operation: &gt;&gt;&gt; a = np.array([1.0, 2.0, 3.0]) &gt;&gt;&gt; b = 2.0 &gt;&gt;&gt; a * b array([ 2., 4., 6.]) For more detail refer this link - https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html",318451.0
103003,446272.0,"Arithmetic operations of arrays are usually done on corresponding elements. If the array is of same dimension, operations are performed smoothly, but when the array(s) are not of same dimension. Then the broadcasting functionality comes into picture. The smaller array is broadcasted to the bigger array so they have a compactable shape. for further reading a tutorial link is also attached. https://www.tutorialspoint.com/numpy/numpy_broadcasting.htm",311032.0
90151,375531.0,"Please find below solution for installing pymysql in Jupyter notebook. 1) Open your Jupyter notebook. 2) In the left handside panel you will see a ""Commands"" tab. Click on that. 3) After opening commands tab. Search for ""New Terminal"" in the top search bar. 4) Select/Click on ""New Terminal"" option. 5) Type below command to install pymysql. conda install -c anaconda pymysql",318368.0
90151,375574.0,"You can install pymysql using anaconda navigator Steps: 1. Open Anaconda navigator 2. Click on the environments 3. drop down select all and search for pymsql 4. Click on apply Now pymysql has been installed, you can start using in Jupyter notebook",317845.0
90135,375440.0,"axis=1 does the calculation column wise. when the you do a df.isnull().sum(axis=1) that means you are calculating the null values for each columns if mention sum=0, that means you are calculating null values for each row",318804.0
90135,375460.0,Thank guys :),308962.0
90135,375455.0,Yes you are correct. They behave differently though documentation mentions axis =0 as row always.,318368.0
90135,375447.0,"Following are 2 summary for axis. And you are right. 1. Axis 0 will act on all the ROWS in each COLUMN 2. Axis 1 will act on all the COLUMNS in each ROW for ex. when we want to drop a column, we want to remove it from all rows so axis= 0",320073.0
90863,378781.0,"NaN : means 0/0 -- Stands for Not a Number NA : is generally interpreted as a missing, does not exist NULL : is for empty object.",318017.0
90863,378714.0,"Yes, These are same function but from different libraries. To detect NaN values numpy uses np.isnan() . To detect NaN values pandas uses either .isna() or .isnull() . The NaN values are inherited from the fact that pandas is built on top of numpy, while the two functions' names originate from R's DataFrames, whose structure and functionality pandas tried to mimic.",318368.0
90863,378997.0,"Almost..""isnan"" cannot be used for object arrays.",318436.0
90863,379313.0,"Hi Lopamudra, np.isnan() and pd.isnull() are different and the usage of pd.isnull() would be better for Pandas Series and Dataframes. np.isnan() cannot be used for objects, however, we may use objects in Pandas Series or Dataframes which would cause an error in certain cases.",317987.0
90863,379507.0,"pandas. isnull ( obj ) [source] Detect missing values for an array-like object. This function takes a scalar or array-like object and indictates whether values are missing ( NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike). numpy.isnan numpy. isnan ( x , / , out=None , * , where=True , casting='same_kind' , order='K' , dtype=None , subok=True [, signature , extobj ]) = &lt;ufunc 'isnan'&gt; Test element-wise for NaN and return result as a boolean array.",318007.0
96340,410205.0,Try the following command from the conda prompt pip install pydataset,313826.0
90924,379082.0,"check with this: print(round(100*(df.isnull().sum()/len(df.index)),2)) Looks like some mis with the brackets",309451.0
90924,379282.0,"Hi, Instead of print(round (100 * df.isnull().sum()/len(df.index)), 2) Try print(round (100 * df.isnull().sum()/len(df.index), 2)) Explanation: round accepts both 1 and 2 parameters but you need to pass 2 parameters in your case, however, you are passing just one.",317987.0
90924,379276.0,"It's a clear case of improper bracketing. After 100 you have applied multiplication operator. As you might be knowing BODMAS rule, according to which division is given more preference than multiplication. That's why you are getting unexpected results. please bracket properly so that bracket gets read as top priority use this to get expected values (100*(df.isnull().sum()/len(df.index)).",310585.0
90897,379308.0,"Hi, You can print(soup) in both these examples to see that the website is actually being scrapped. However, as mentioned by Kapil, websites do change over time, thus, this code won't work as expected.",317987.0
90897,380310.0,"Thus Web Scrapping is not a permanent solution and always risky, if something gets changed in website the code needs an update.",301646.0
90897,378980.0,"Google has changed the way this document was rendered previously. Not this class "" review-body"" doesn't exists on the page. So you will not be able to use this any more. You can use any other website to do the same thing, Find code below to scrap trip advisor website with the same code. import requests, bs4 # getting HTML from the Google Play web page url = ""https://www.tripadvisor.in/Restaurant_Review-g297628-d7009948-Reviews-Bangalore_Belly-Bengaluru_Bangalore_District_Karnataka.html"" req = requests.get(url) # create a bs4 object # To avoid warnings, provide ""html5lib"" explicitly soup = bs4.BeautifulSoup(req.text, ""html5lib"") # getting all the text inside class = ""review-body"" reviews = soup.select('.partial_entry') print(type(reviews)) print(len(reviews)) print(""\n"")# printing an element of the reviews list print(reviews[5])",318368.0
93739,395046.0,"Kindly go through the link, it is explained over there:- https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f",303673.0
90887,378881.0,"once dataframe is grouped based on genre1,genre2 with mean on gross,then sort desc to get 5 popular movies",301115.0
90887,378986.0,Most popular genres can be extracted using sorting them on the column mentioned in the assignment.,318368.0
90398,377897.0,Thanks for sharing!,319721.0
90416,377079.0,503 Error means - Service Unavailable. It can happen if there is some eror on the server. Or server is too busy handeling other request. Just try few more time and you should get response 200 (Success).,318368.0
90416,377028.0,import requests statement is missing,318084.0
90416,377035.0,"Since you have used requests in code req = requests.get(url) You first need to import ""requests"" module at the top. import requests Hope this helps.",317991.0
90416,377270.0,"503 response code is nothing to do with imports, its http service unable and the reasons could be - Load or some issue on server (which shouldn't be the case here since its amazon.in) - Malformed URL, you should check for URL-encoding properly.",306248.0
90416,377779.0,"unfortunately issue is still not resolved.. I have checked the URL, included ""import requests"" Continue to get 503 response. Same URL opened from browser works fine.",321435.0
90148,375541.0,axis=0 is for means row wise Operation axis =1 means Column wise Operation here we r doing column wise Operation,317982.0
90148,375549.0,"Hi Venkatesulu, Axis 1 will act on all the COLUMNS in each ROW for ex. when we want to drop a column, we want to remove it from all rows so axis= 1 Hope this helps. Regards, Rajarshi",310511.0
90148,375553.0,"axis=1 , signify rows we drop the data in all rows for that particular column. say' BuildingArea' if we say axis=0 we drop the data in all columns of that particular row, say 2 or 4 hope that helps!",308495.0
90148,375697.0,"df = df.drop('BuildingArea', axis=1) Here axis=1 is used to refer all rows in Building area column which needs to be dropped.",317811.0
90739,378159.0,df = df[~np.isnan(df['Landsize'])] Lets split the code to understand what it means np.isnan(df['Landsize']) will check if there are NaN values for the Landsize column in the dataframe. df[~np.isnan(df['Landsize'])] --&gt; the ~ before the np.isnan(df['Landsize']) basically excludes the NaN values So basically the whole statement means that the dataframe will exclude all the NaN values for Landsize column. Hope it is clear now.,318084.0
90739,378169.0,np.isnan(df['Landsize']) returns true values for those cells which has NaN in 'Landsize' column. ~np.isnan(df['Landsize']) negates it and returns true values for those cells which are not NaN and df[~np.isnan(df['Landsize'])] returns all the rows which are non NaN in 'Landsize' column.,318329.0
90674,378057.0,"go to the csv file, right click on the properties and check the path of the file and give it. Please use '/' as the separator instead of '\'",318084.0
90674,378078.0,"Here is the step to follow to get file location: 1. Go to the file .csv reside on desktop. 2. Right click on file --&gt; Click on properties. 3. Check for 'Location' --&gt; you will find something like C:\Users\.......\Desktop . This will be the actual location of your file. So the file location for read_csv will be ""C:\Users\....\Desktop\&lt;File_name&gt;.csv""",317991.0
90548,377687.0,encoding is needed when you want to save the data from a website to your local machine or from a database to local machine for analysis,318017.0
89502,371414.0,"Your syntax is wrong. You are missing the column name before = . Please refer below: df.loc[np.isnan(df['Product_Base_Margin']), ['Product_Base_Margin']] = df['Product_Base_Margin'].mean() print(round(100*(df.isnull().sum()/len(df.index)),2))",314547.0
89502,371491.0,"Hi, You are missing out the column in the df.loc approach of selecting data from a dataframe. df.loc[[rows],[column]] - for a specific element df.loc[[rows],] - for a specific row &amp; df.loc[:,[column]] - for a specific column So here if you want to select all rows where 'Product base margin' is null, your code will be df.loc[np.isnan(df['Product_Base_Margin']), ['Product_Base_Margin']] Hope this helps.",310511.0
89502,371423.0,"Focus on the loc concept. df.loc[ [rows], [cols] ] = value Now your rows = np.isnan(df['Product_Base_Margin']) // all the rows having null values And your cols = ['Product_Base_Margin'] // entire Product_Base_Margin coloumn df.loc[ np.isnan(df['Product_Base_Margin']), ['Product_Base_Margin'] ] = df['Product_Base_Margin'].mean()",306248.0
90304,376340.0,df.loc accepts two parameters one is row and other is column.. here you're capturing all the missing value rows for column named 'Product_base_margin' and then imputing the missing values only in all the rows for the same column by mean value.. first argument is just bringing all the missing value rows.. hope it helps.. if still confusing please ask,316349.0
90304,376398.0,"There are several ways, using .loc which is label based indexing, to refer to rows and columns One of the ways is using a list of booleans - this is what isnan is returning - a list of booleans to refer to the rows of the df. so for example df.loc[[True,True,False],:] returns all the coliums of the first and second row, but not the third df.loc[[True,True,False], ['Product_Base_Margin'] ] returns the 1st and 2nd row/cells of the Product_Base_Margin column that is what isnan is doing -&gt; it is creating a list of booleans referring to the the cells/rows of the Product_base_margin column and whether they are NaN or not And then the entire line that you have mentioned is reassigning those to the mean of the Product_Base_Margin series/column.",300694.0
90304,376522.0,"Here the idea is, we update all the rows with NaN with the product margin mean. And we are using loc to assign the values. Hence the row label used (bold in the code below), selects the rows which returns true for Product Base Margin is NaN while the column label reads 'Product_Base_Margin’ column. df.loc[ np.isnan(df['Product_Base_Margin']), ['Product_Base_Margin']] = df['Product_Base_Margin'].mean() Hope this helps",317149.0
90304,376663.0,"It needs to identify the row and column for which you need to replace the value. So, in order to identify the rows which are having Product_Base_Margin as NaN it is using np.isnan(df['Product_Base_Margin']). So, loc function is using above to identify the row and than column is identified using secong argument ['Product_Base_Margin'] So, this loc function returns a series which is having above columns as NaN. So, we would replace all such values with another series df['Product_Base_Margin'].mean()",317689.0
90304,376698.0,"Try to recollect how dataframes are accessed with loc df.loc[ [row_index1, row_index1], [""col1"", ""col2""] ] Now replace list of rows with the numpy array where you Product_Base_Margin is NaN, and list of cols with Product_Base_Margin. Asign this to df['Product_Base_Margin'].mean()",306248.0
90304,377647.0,"If any body running same problem, in upgrade code adding print its fully successful print(round(100*(df.isnull().sum()/len(df.index)), 2))",312019.0
90304,377260.0,"Still below code fails even the expected out is matching when i check in jupyter Not sure why this problem #Impute the mean value at all the missing values of the column 'Product_Base_Margin' and then print #the percentage of missing values in each column. import numpy as np import pandas as pd df = pd.read_csv('https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0') df.loc[np.isnan(df['Product_Base_Margin']), ['Product_Base_Margin']] = df['Product_Base_Margin'].mean() round(100*(df.isnull().sum()/len(df.index)), 2)",312019.0
90452,377249.0,"Please go through the below link to write pandas dataframe to a file. https://datatofish.com/export-dataframe-to-csv/ For ex:- raw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], 'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'], 'age': [42, 52, 36, 24, 73], 'preTestScore': [4, 24, 31, 2, 3], 'postTestScore': [25, 94, 57, 62, 70]} df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'preTestScore', 'postTestScore']) df.to_csv('Path where you want to store the exported CSV file\file_name.csv')",317991.0
90452,377365.0,You can use dataframe.to_csv() function to write to a csv file,318358.0
91070,380279.0,I think your code is keeping (instead of removing) those rows where 5 or more values are missing. you have to use negation to get the desired output.,311686.0
91070,380438.0,Or you could probably change the &gt;5 to &lt;=5 instead of using negation. Both methods would work.,318499.0
91070,380706.0,"In question it is required to ""Remove the missing values from the rows having greater than 5 missing values"" And your code instead of removing it is selecting those rows which has greater than 5 missing values. df[df.isnull().sum(axis=1) &gt; 5] So to get desired output use &lt;=5 as follows: df[df.isnull().sum(axis=1) &lt;= 5] Also I would suggest to go through videos of this link to get better understanding https://learn.upgrad.com/v/course/208/session/19864/segment/101071 Hope it will help.",317991.0
91070,381222.0,df = df[df.isnull().sum(axis=1) &lt;= 5] try this it should work.,306010.0
89897,374007.0,"Categoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values ( categories ; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales. You can convert your columns to categorical columns and do analysis. In above case we are try to replace null values in car column and we need to know what are the categories of car and what is the fequence. To do so you can convert it to category data and use value_counts function to do a quick analysis. Once you do analysis as category data you will know more about data and then you can replace with appropriate values.",318368.0
89897,374800.0,"You may find it difficult to understand the usage, but it has to be ensured that you know all these methods. In end to end DS case studies or projects, you might need thesesteps and hence it is imp.",301555.0
89750,372947.0,"As we need to remove rows having more than 5 missing values, so U can use the below code to do save only the rows having less than or equal to 5 missing values df = df[df.isnull().sum(axis=1) &lt;=5] As we are assigning it to a dataframe variable df, it will only containing those rows having missing values less than or equal to 5",318358.0
89750,372821.0,you can use below code for removing the rows having more than 5 nan values. df=df[df.isnull().sum(axis = 1) &lt; =5] now new df has only rows which has less than or equal to 5 nan values.,320073.0
89752,373444.0,"best way to read csv file:- import pandas as pd pd.read_csv(r""FilePath complete with /"")",301555.0
89752,372825.0,"While reading a file you need to have file and file location. Example as follows: Suppose you have file name companies.csv and it is kept under below location. C:\PGDDS\getting_data\companies.csv then inside read_csv() you need to specify the above location of file like as follows: df = pd.read(""C:\PGDDS\getting_data\companies.csv"") For reading text and other files sometimes you need to specify separator also as - sep=""\t"" Hope it helps.",317991.0
89752,372831.0,"Please replace ? with the Complete path of the flat file, in this case *.csv. Ex : pd.read_csv(""c:\temp\master.csv"") In case of above example, you will get an error message. The error message can be resolved in following three ways - 1. use pd.read_csv(r""c:\temp\master.csv""). Here r is extra character and it converts normal string to raw string: 2: pandas.read_csv(""C:/Users/DeePak/Desktop/myac.csv"") 3: pandas.read_csv(""C:\\Users\\DeePak\\Desktop\\myac.csv"") Source - https://stackoverflow.com/questions/37400974/unicode-error-unicodeescape-codec-cant-decode-bytes-in-position-2-3-trunca",311502.0
89752,372832.0,"Hi, pd.read_csv() has many arguments which we can while reading a file. The sample syntax is following read_csv(filepath, sep=', ', delimiter=None, header='infer', names=None, index_col=None, dtype=None, skipinitialspace=False, skiprows=None) It has only handful important argument. You can refer to below link for more information on this. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html",320073.0
89752,372854.0,It is showing the following error.,300690.0
89752,372953.0,Please use the function pd.read_csv() as you are using CSV in caps and you need to know the path of csv file which sholud be given in circular brackets of the function,318358.0
89752,373014.0,Path where the file is located in your system.,314547.0
89752,373498.0,"You have used ""CSV"" in the code snippet shared. As we know that python is case sensitive and syntax to read csv file is pd.read_csv, changing your code to the correct case should return the required output.",319302.0
89752,373736.0,"if u trying all the ways and still facing error try this simple 2 ways which worked for me. open the csv file once in Jupyter then go and execute the command in notebook copy the .csv file from global data folder to the folder where the notebook exsists and change the code to df = pd.read_csv(""market_fact.csv"")",301115.0
89752,374217.0,you should give the path of the file where you have feeded it in your system: df.read_csv('file_location.csv'),318017.0
89752,374793.0,use check the current working directory and change it if you want,300716.0
89752,375777.0,"import pandas as pd pd.read_csv(""CompleteFilePath"")",317811.0
90237,376106.0,"if you have some idea on CSS, you can make use of the browser developer tools such as inspect element to know the id's of different elements or sections of a web page.",318329.0
90237,376551.0,The page has been rewritten in Kotlin now I believe. And looks more of a webapp. The class names look dynamic. Not a good site for doing basic web scrapping. There must be other library for scrapping dynamic sites as this.,309451.0
90237,376568.0,"I tried a lot of different variations of soup.select() mentioned in the Python Notebook. Like, soup.select(class &gt; jscontroller), soup.select(span &gt; jsname), soup.select(div &gt; class), etc., but I get an empty list. @Nagaraju Gunda: Can you please elaborate a little, how to extract the reviews.",301652.0
90455,378354.0,"change the path seperator character from ""\"" to ""/"" in the path and should work",312199.0
90455,377303.0,are you sure the separator is tab?,318329.0
90455,377310.0,"The problem is with the string "" C:\Users\ 302001419\.......\Movies+Assignment+Data.csv"" Here, \U starts an eight-character Unicode escape, such as '\U00014321`. In your code, the escape is followed by the character 's', which is invalid. You either need to duplicate all backslashes, or prefix the string with r (to produce a raw string). Source: - https://stackoverflow.com/questions/1347791/unicode-error-unicodeescape-codec-cant-decode-bytes-cannot-open-text-file",317991.0
90455,377312.0,"If you are trying to load the assignment data, then the file is txt and not csv while it is comma seperated and not tab seperated. hello =pd.read_csv(""path to the file.txt"") should suffice.",318329.0
90455,377273.0,"Try using small r before the ("") eg : (r""C:\Users......) Use the below link might be helpful https://python-forum.io/Thread-unicode-error-unicodeescape-codec-can-t-decode-bytes-in-position-2-3-truncated",300688.0
90455,377328.0,"Try using forward slash ""/"" instead of backslash ""\"" in the path.",313826.0
90455,377353.0,"If your file is in the Jupyter, give the file name and you can directly open from there.",314678.0
90455,377671.0,I opened the file in Jupyter. Then entered file name.csv ..it worked. Reading file from elsewhere did not work.,301644.0
90559,377734.0,"Yes it can be done. The columns argument accepts a list of columns that need to be dropped. Additionally, drop() also has the inplace argument which can be set to True for inplace operation. The code would look like this: df.drop(columns=['BuildingArea','YearBuilt','CouncilArea'] , axis=1 , inplace=True)",313826.0
90559,377742.0,"Multiple columns can be deleted/dropped by doing something like below. It worked for me. please try. df = df.drop(['BuildingArea','YearBuilt','CouncilArea'], axis=1)",318328.0
90559,377862.0,"df = df.drop(columns=['BuildingArea', 'YearBuilt', 'CouncilArea']) In this case you do not have to specify axis which is quite confusing for me as it is different in case of sum().",318329.0
90234,377315.0,"axis=0 is for means row wise Operation axis =1 means Column wise Operation Axis 1 will act on all the COLUMNS in each ROW for ex. when we want to drop a column, we want to remove it from all rows so axis= 1",319721.0
90234,375925.0,In dataframes when we are retrieving values we use axis=1 for rows and when we drop columns in dataframe we use axis=1 for columns,318358.0
90234,375933.0,"yes, it's little confusing but try to read the code df.isnull.any(axis=1) as trying to find any rows with null values from all corresponding columns..",316349.0
90234,375938.0,I found below link which describe what is axis mean in pandas. Have a look into it. I hope it will clarify your doubt. https://stackoverflow.com/questions/22149584/what-does-axis-in-pandas-mean,317991.0
90572,377820.0,"Try below it will work there is syntax error xyz.loc[np.isnan(xyz[language'],inplace=true)] = movies['French'] where xyz is data frame location based on xyz data frame and seaching for NAN vlue using numpy on the column Language and keep inplace is true.",307843.0
90572,377812.0,you need to put language in square bracket xyz['language'],316349.0
90606,377978.0,"If it's regarding the assignment, you can't really copy paste them because there are unique problem statements in it. But for other questions, you can understand the logic and use them in the graded questions. But copy-pasting the code directly is not advised.",319721.0
89915,375038.0,"use pd.read_csv(""filepath"")",318005.0
89915,374974.0,"Python is case sensitive and as you have used ""CSV"" it is returning an error. pd.read_csv should work considering correct file path is provided.",319302.0
89915,374171.0,"Just modify your code as follows: df = pd.read_csv(""File Path"") Note: On place of File Path: put location of melbourne.csv file. Hope it will help",317991.0
89915,374207.0,Please use pd.read_csv('abc.csv') here,318358.0
89915,374208.0,"df = pd.read_csv(""/path/to/the/csv/file"")",306248.0
89915,374230.0,"df=pd.read.CSV(""melbourne.CSV"") This is incorrect way to use read function. Correct: pd.read_csv(""FULLPATHOFFILE"")",317811.0
89915,374358.0,"Function is pd.read_csv(""file_name"")",318368.0
89915,374395.0,"Please use the function pd.read_csv(""file path"")",303673.0
89915,374410.0,read_csv,314547.0
89915,374821.0,"df = pd.read_csv(r""/path/to/the/csv/file"")",301555.0
89921,374343.0,maybe you don't have imported pandas as pd,318426.0
89921,374228.0,"use code like this : df=pd.read_csv(""location of file where it is saved in your system.csv"")",318017.0
89921,374342.0,Python is case sensitive. You need to write read_csv instead of read_CSV.,318368.0
89921,374373.0,Please use the correct syntax as pd.read_csv() as python is case sensitive,318358.0
89921,374246.0,pls note that Python is case sensitive. read_csv and read_CSV are two different things and read_CSV is thus incorrect.,311686.0
90453,377702.0,Removing the df= from your program will work. Other than that your code is perfect,318454.0
90453,377259.0,"Since you are imputing the value only to the 'Product_Base_Margin' column with the mean value of the values present in this column, the correct code would be : df.loc[np.isnan(df['Product_Base_Margin']), ['Product_Base_Margin']] = df['Product_Base_Margin'].mean() The error is because you are assigning it back to dataframe df .",313826.0
90729,378129.0,Please go through the session on cleaning data.,319721.0
90729,378134.0,"You can drop multiple rows by either giving their row name or by row number. For eg: df.drop(['Europe', 'Asia']) --&gt; europe, asia are row index names. df.drop(df.index[:2], inplace=True) --&gt; europe, asia are in first 2 rows",318328.0
90729,378290.0,For removing the rows from your dataframe; 1. You need to filter rows with greater than 5 null values thrrough using a condition with '&gt;' sign in your code 2. Then you check the statistics of the data to see if the less than 5 null value rows can be imputed with some number like mean or anything 3. Still for some null value rows you categorize the data and then imoute it again in the missing value columns these are the procedural steps which helps in getting rid of missing value rows. Code you can refer in the Jupyter file. Try to relate it will the steps and it should be easy in understanding...,316349.0
89945,375784.0,Axis 0 which is bydefault is for index/rows and axis=1 is for columns needs to be explicitly mentioned if refrring to columns. import numpy as np To find the total number of missing values in each column e.g. np.isnull().sum(axis=1),317811.0
89945,374465.0,axis=1 refers to column axis=0 refers to rows As we are dropping a column we use axis=1 in drop function,318358.0
89945,374458.0,because axis 0 refers to rows and axis 1 refers to columns.,306248.0
89945,374457.0,If you don't specify the axis then it takes default value as 0 0: Index 1: column As we have to drop a specific column we should specify axis = 1 Refer the below link for details https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html Hope this helps,308635.0
89945,374622.0,"Default axis is 0. Axis can be 0 for rows and 1 for column. Examples. &gt;&gt;&gt; df = pd.DataFrame(np.arange(12).reshape(3,4), ... columns=['A', 'B', 'C', 'D']) &gt;&gt;&gt; df A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 Drop columns &gt;&gt;&gt; df.drop(['B', 'C'], axis=1) A D 0 0 3 1 4 7 2 8 11 &gt;&gt;&gt; df.drop(columns=['B', 'C']) A D 0 0 3 1 4 7 2 8 11 Drop a row by index &gt;&gt;&gt; df.drop([0, 1]) A B C D 2 8 9 10 11 Source : https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html",318368.0
89949,374468.0,~ refers to negation that is exactly opposite of a statement will be taken that follows this symbol. isnan is a function in numpy so we use np.isnan here and isnan and isnull functions of numpy can both take column_name as an input,318358.0
89949,374614.0,"You can pass a array of boolean values in df to remove rows. i.e. if my dataframe has 3 rows and I want to remove row 1, I can write df[[True, False. True]] -&gt; This means I want to retain row 0 and 2 but delete row 2. Now np.isnan(df['Price']) -&gt; This will produce a boolean array of all the entry in df which has ""Price"" as null. And ~(np.isnan(df['Price'])) -&gt; putting ""~"" in starting will reverse that complete boolean array and that will mean all the ""Price"" values which has values. So that those can be retained and other rows can be dropped.",318368.0
89949,374540.0,"To remove rows which contain 'NaN' values , we will have to first identify such rows.This can be done using the np.isnan() function belonging to the numpy library and hence the use of numpy library. Further, np.isnan(df['Price']) : would give the row number of the rows which contain some 'Nan' values. By negating this statement, we will get the rows which do not have any 'NaN' values. This can be done using ~np.isnan(df['Price']) : where ~ negates the value provide by np.isnan(df['Price']). df = df[~np.isnan(df['Price'])] : would select all the rows which do not have 'Nan' value in any of the columns and assign it back to df dataframe. Hope this answers your query. PS: Checkout the FAQ Section at the bottom of this link https://learn.upgrad.com/v/course/208/session/19864/segment/101071",313826.0
89595,371905.0,"Google has changed the way this document was rendered previously. Not this class "" review-body"" doesn't exists on the page. So you will not be able to use this any more. You can use any other website to do the same thing, Find code below to scrap trip advisor website with the same code. import requests, bs4 # getting HTML from the Google Play web page url = ""https://www.tripadvisor.in/Restaurant_Review-g297628-d7009948-Reviews-Bangalore_Belly-Bengaluru_Bangalore_District_Karnataka.html"" req = requests.get(url) # create a bs4 object # To avoid warnings, provide ""html5lib"" explicitly soup = bs4.BeautifulSoup(req.text, ""html5lib"") # getting all the text inside class = ""review-body"" reviews = soup.select('.partial_entry') print(type(reviews)) print(len(reviews)) print(""\n"")# printing an element of the reviews list print(reviews[5])",318368.0
89595,372151.0,"Thanks for the help. Yes, it is working with trip advisor's Restaurant review.",311115.0
89360,370781.0,pip install PyMySQL You can use pip for installing it. You may also need to ajdust paths if Python3 is not set as default in your machine.,306248.0
89360,370606.0,Open Anaconda Prompt. run following command conda install PyMySQL It will ask for permissions. Please provide Y. Post installation it will work.,317689.0
89360,370849.0,"Hello All, I'm able to install PyMySQL. Below are the steps, Click on 'Start'-&gt; type Anaconda Prompt-&gt; you will see the below options And then type below command on prompt and type Y to proceed,",311004.0
89613,371942.0,"While reading the web content from any URL, python will try to parse provided url's content. Python provides default ""html.parser"" which is fast and efficient for simple use cases. But in many cases content will be in with advance HTML functionality and creating a valid HTML document, as browser does, is necessary and at that time html5lib can be used. Below are option availble as parser. 1) html.parser 2) html5lib 3) lxml 4) html5-parser 5) AdvancedHTMLParser Please refer to link for better understanding. https://www.crummy.com/software/BeautifulSoup/bs4/doc/ https://tomassetti.me/parsing-html/#html5lib",318368.0
90499,377512.0,Reload the data to get the dropped column. There is no other way.,318084.0
90499,377565.0,"Column from the dataframe dropped using ""inplace=True"" in drop() cannot be brought back. So you can use ""inplace=False"" which is by default option in drop() and put it in new dataframe while dropping column. In above example: I have used df_1 as a temp dataframe. You can notice that original dataframe df doesnot get affected. But when we use ""inplace=True"" original dataframe gets modified and we cannot bring back deleted column. Can see in below screenshot: So to retain column 1. Either use ""inplace=False"" in drop() while dropping column or 2. Perform all operations again that were performed before dropping the column.",317991.0
90499,377515.0,"I think when you apply the drop function on columns/rows, you have modified the dataframe structure permanently. So I dont think you can go back to a previous state like an undo option. You only have two options: Start all over again by reimporting the data and applying the previous data operations. OR copy the data to a temp dataframe. If something goes wrong, you can reassign the structure and values from the temp dataframe",318085.0
90982,379603.0,Under Cell -&gt; Rull All will run all the cells at once !,306735.0
90982,379626.0,Select all the cells you want to execute. CTRL + A (For all cells) and then press Shift + Enter to execute.,318368.0
90982,379662.0,you can by run all the icon is like &gt;&gt;,318017.0
90982,379806.0,Select the cells that you would like to execute by pressing shift and select the cells. Once selected you can press shift + enter.,316202.0
90982,380332.0,Enjoy all shortcuts by navigating to Help-&gt; keyboard Shortcuts,301646.0
92686,,nan,
89984,374711.0,"Please go through the following links, they clearly explained dropna() with good example. You will get good understanding of how dropna() works. https://www.kaggle.com/aliendev/example-of-pandas-dropna https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html I hope it will help you.",317991.0
89984,374723.0,"Dropna wil remove rows and columns based on NA values. There are multiple properties to chagne the behaviour according to your need. &gt;&gt;&gt; df = pd.DataFrame({""name"": ['Alfred', 'Batman', 'Catwoman'], ... ""toy"": [np.nan, 'Batmobile', 'Bullwhip'], ... ""born"": [pd.NaT, pd.Timestamp(""1940-04-25""), ... pd.NaT]}) &gt;&gt;&gt; df name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Drop the rows where at least one element is missing. &gt;&gt;&gt; df.dropna() name toy born 1 Batman Batmobile 1940-04-25 Drop the columns where at least one element is missing. &gt;&gt;&gt; df.dropna(axis='columns') name 0 Alfred 1 Batman 2 Catwoman Drop the rows where all elements are missing. &gt;&gt;&gt; df.dropna(how='all') name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Keep only the rows with at least 2 non-NA values. &gt;&gt;&gt; df.dropna(thresh=2) name toy born 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Define in which columns to look for missing values. &gt;&gt;&gt; df.dropna(subset=['name', 'born']) name toy born 1 Batman Batmobile 1940-04-25 Keep the DataFrame with valid entries in the same variable. &gt;&gt;&gt; df.dropna(inplace=True) &gt;&gt;&gt; df name toy born 1 Batman Batmobile 1940-04-25 Source - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html",318368.0
89638,372052.0,"JSON stands for JavaScript Object Notation JSON is a lightweight data-interchange format Since the JSON format is text only, it can easily be sent to and from a server, and used as a data format by any programming language. Example of json object: myObj = {name: ""John"", age: 31, city: ""New York""};",318358.0
89638,372054.0,There is very good explanation for JSON in below link. Have a look into it. https://www.w3schools.com/Js/js_json_intro.asp,317991.0
89638,372056.0,"Hi Ayush, JavaScript Object Notation is a schema-less, text-based representation of structured data that is based on key-value pairs and ordered lists. Although JSON is derived from JavaScript, it is supported either natively or through libraries in most major programming languages. JSON is commonly, but not exclusively, used to exchange information between web clients and web servers. The JSON format is often used for serializing and transmitting structured data over a network connection. It is used primarily to transmit data between a server and web application, serving as an alternative to XML. The popularity of JSON has also resulted in native JSON support by many databases. Relational databases like PostgreSQL and MySQL now ship with native support for storing and querying JSON data. NoSQL databases like MongoDB and Neo4j also support JSON, though MongoDB uses a slightly modified, binary version of JSON behind the scenes. Below we’ll take a quick look at JSON and discuss where it came from, its advantages over XML, its drawbacks, when you should use it, and when you should consider alternatives. Let’s start with an example: { “firstName”: “Jonathan”, “lastName”: “Freeman”, “loginCount”: 4, “isWriter”: true, “worksWith”: [“Spantree Technology Group”, “InfoWorld”], “pets”: [ { “name”: “Lilly”, “type”: “Raccoon” } ] } The structure above clearly defines some attributes of a person. It includes a first and last name, the number of times the person has logged in, whether this person is a writer, a list of companies the person works with, and a list of the person’s pets (only one, in this case). A structure like the one above may be passed from a server to a web browser or a mobile application, which will then perform some action such as displaying the data or saving it for later reference. JSON is a generic data format with a minimal number of value types: strings, numbers, booleans, lists, objects, and null. Although the notation is a subset of JavaScript, these types are represented in all common programming languages, making JSON a good candidate to transmit data across language gaps.",320195.0
89638,373112.0,"read this ink.It has all sorts of explainination of JSON, how json uses key-value pair to storre data, how arrays are stored using json etc https://developers.squarespace.com/what-is-json",317982.0
89638,372181.0,"Previously we used to communicate data in programming using XML. The more lightweight JSON (Javascript object notation) has become a popular alternative to XML for various reasons. A couple obvious ones are: Less verbose-XML uses more words than necessary. JSON is faster- Parsing XML software is slow and cumbersome. Basically JSON is data format for transferring and storing data in key value pair. A example could be { ""glossary"": { ""title"": ""example glossary"", ""GlossDiv"": { ""title"": ""S"", ""GlossList"": { ""GlossEntry"": { ""ID"": ""SGML"", ""SortAs"": ""SGML"", ""GlossTerm"": ""Standard Generalized Markup Language"", ""Acronym"": ""SGML"", ""Abbrev"": ""ISO 8879:1986"", ""GlossDef"": { ""para"": ""A meta-markup language, used to create markup languages such as DocBook."", ""GlossSeeAlso"": [""GML"", ""XML""] }, ""GlossSee"": ""markup"" } } } } }",318368.0
89804,373185.0,"1) Open your jupiter notebook. 2) In the left handside panel you will see a ""Commands"" tab. Click on that. 3) After opening commands tab. Search for Terminal on the top search bar. 4) Select/Click on ""New Terminal"" option. 5) Type below command to install pymysql. conda install -c anaconda pymysql",318368.0
89804,373820.0,"in Windows: 1. In Windows search , search for ""Anaconda Prompt"". Click on it and open it 2. type ""conda install pymysql"" or ""pip install pymysql"" 3. it shold get installed. then type Jupyter Notebook and your jupyter notebook will open.",301555.0
89804,373792.0,"Navinage to the directory where you have Python installed, run the following comand Python -m pip install pymysql",306725.0
89804,375433.0,Kapil Dave helped me resolve this I installed the package via cmd It worked Thanks a lot 😊,308437.0
89988,374736.0,df = df[df.isnull().sum(axis=1) &lt;= 5] --&gt; This means select only those rows where the number of null values per row is less than 5 and assign all such rows back to the original dataframe df. This implicitly means we are removing all the rows in which the number of null values per row is more than 5.,313826.0
89988,374735.0,"if you consider df = df[df.isnull().sum(axis=1) &lt;= 5], here we are filtering the rows which have null values &lt;=5 and assigning it back to the Dataframe (df) as we need to proceed with other manipulations on the Data. But df = df[df.isnull().sum(axis=1) &gt;= 5], then we are collecting the data which are having null values &gt;=5 and assigning it back to the DataFrame(df), but this data is actually not useful.",318804.0
89988,377253.0,We are making new data set with only &lt;=5. Means its removed &gt;5 missing values.,312019.0
89809,,nan,
90844,378715.0,You are missing a square bracket in the syntax. df['Product_Base_Margin'.mean() Should be df['Product_Base_Margin'].mean(),318368.0
90844,378633.0,"There is syntax error in your code for df['Product_Base_Margin'].mean() Please execute below import pandas as pd df = pd.read_csv('https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0') df.loc[np.isnan(df['Product_Base_Margin']),['Product_Base_Margin']]=df['Product_Base_Margin'].mean() print(round(100*((df.isnull().sum())/len(df.index)),2)) Ord_id 0.00 Prod_id 0.00 Ship_id 0.00 Cust_id 0.00 Sales 0.24 Discount 0.65 Order_Quantity 0.65 Profit 0.65 Shipping_Cost 0.65 Product_Base_Margin 0.00 dtype: float64",307843.0
90979,379602.0,"Solution : No Module named 'Pymysql' in Jupyter Notebook. Please find below solution for installing pymysql in Jupyter notebook. 1) Open your Jupyter notebook. 2) In the left handside panel you will see a ""Commands"" tab. Click on that. 3) After opening commands tab. Search for ""New Terminal"" in the top search bar. 4) Select/Click on ""New Terminal"" option. 5) Type below command to install pymysql. conda install -c anaconda pymysql",320195.0
90979,379627.0,Ref to my solution here. https://learn.upgrad.com/v/course/208/question/89695,318368.0
90979,380324.0,Please use this conda install -c anaconda PyPDF2 or from python cli use pip install PyPDF2,301646.0
90977,379595.0,"I've tried the below method and it worked fine. We can specify the directory and then call pfd file from there. ""Python"" is the directory inside my working directory.",318328.0
89279,370261.0,"A Python program can be executed either in Standalone mode or through a Module call. A special variable __name__ is used to distinguish between these two types of calls. Standalone Call : __name__ == __main__ Module Call : __name__ == &lt;name of the calling Module&gt; It is a general practise to use the statement ""if __name__ == __main__ :"". This gives the power to execute certain statements/functions etc. only when run in Standalone mode. In the link mentioned by you, the main() gets executed when the program is executed in Standalone mode. Check out the below link which explains the concept with a simple example: https://www.slashroot.in/what-if-name-main-python Hope this helps.",313826.0
89279,370178.0,"It is because of declared the call function ""if__name__== ""__main__"". -When Python interpreter reads a source file, it will execute all the code found in it. -When Python runs the ""source file"" as the main program, it sets the special variable (__name__) to have a value (""__main__""). -When you execute the main function, it will then read the ""if"" statement and checks whether __name__ does equal to __main__. -In Python ""if__name__== ""__main__"" allows you to run the Python files either as reusable modules or standalone programs. Like C, Python uses == for comparison while = for assignment. Python interpreter uses the main function in two ways import: __name__= module's filename if statement==false, and the script in __main__ will not be executed direct run:__name__=__main__ if statement == True, and the script in _main_will be executed -So when the code is executed, it will check for module name with ""if."" It is important that after defining the main function, you call the code by if__name__== ""__main__"" and then run the code, only then you will get the required output. Hope, it is helpful",300691.0
89279,370179.0,"Here the main method is being executed by if __name__ == ""__main__"": # calling the main function main() the codes within the 'if' block will be executed only when the code runs directly. Here 'directly' means 'not imported' when you are running like python myscript . py If the python interpreter is running that module (the source file) as the main program, it sets the special __name__ variable to have a value “__main__”. If this file is being imported from another module, __name__ will be set to the module’s name You can read more on below links: https://www.bogotobogo.com/python/python_if__name__equals__main__.php https://www.geeksforgeeks.org/what-does-the-if-__name__-__main__-do",317845.0
89837,373436.0,"Please donot use req.text in quotation marks otherwise it would be treated as a string use soup = bs.BeautifulSoup(req.text,""html5lib"")",318358.0
89837,373882.0,"It should be written as following: soup = bs.BeautifulSoup(req.text,""html5lib"")",317689.0
90774,378284.0,Try to run the command in terminal: conda install -c anaconda pymysql,317991.0
90774,380091.0,Refer to the second FAQ on the following page: https://learn.upgrad.com/v/course/208/session/19864/segment/101067,306040.0
90828,378535.0,"According to the Course, it's asking greater than 5 missing values. in your code import pandas as pd df = pd.read_csv('https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0') df = df[df.isnull().sum(axis=1) &lt; 5]#Type your code here. print(round(100*((df.isnull().sum(axis = 0))/len(df.index)),2))#Type the code for taking the percentage of missing values in each column))#Round off to 2 decimal places. You have to change df = df[df.isnull().sum(axis=1) &gt; 5]",317845.0
90828,378539.0,The question asks us to remove rows having greater than 5 null values . df = df[df.isnull().sum(axis=1) &lt; 5] # This would remove rows having greater than 4 null values since you are checking only for lesser tahn 5. You need to change the condition to df = df[df.isnull().sum(axis=1) &lt;= 5] # to remove rows having greater than 5 null values. Hope it helps.,313826.0
90995,379760.0,"You can round up using round(100*df.isnull().sum()/len(df.index),2) If you can provide a snippet of code, it will be easy to identify the issue.",317845.0
90995,379766.0,"The correct is round(100 * (df.isnull().sum() / len(df.index)), M ) You must mention M when using round function to tell upto how many digits you want to round off the result.",317811.0
90995,379846.0,The code seems to be correct and would round up to 1 decimal place if you are working on an object of type dataframe. Please check if df is indeed a dataframe. Please post a snippet of the code to investigate further.,313826.0
90995,380186.0,provide full code of yours and put 2 in the end of you want to round same to two decimal places,319869.0
89323,370428.0,See if this helps https://github.com/pandas-dev/pandas/issues/14467,310974.0
89323,371312.0,"columns=['Employee_ID','Employee_Name','Company','City'] argument have 4 parmas but it seems the tuple the all_rows have 9 values in it . use the cloumns name in select statmets to get the values .Example: all_rows:select employee_id,employee_name,company,city from emp_table; and then do the below: df = pd.DataFrame(list(all_rows), columns=['Employee_ID','Employee_Name','Company','City'])",318476.0
90084,375210.0,You can use chardet library for finding the encoding of any file. Link below https://pypi.org/project/chardet/,318368.0
90084,375106.0,chardet has a function named detect which takes a non unicode string and returns two values. The first one being the encoding and second one is the confidence level of recognition between 0 and 1.,318329.0
90084,375142.0,Use chardet. https://pypi.org/project/chardet/,317689.0
89677,372373.0,After I install working at CLI...but not working in Jupyter Notebook... --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-1-8748c87b3097&gt; in &lt;module&gt;() ----&gt; 1 import pymysql ModuleNotFoundError: No module named 'pymysql',318846.0
89677,372405.0,"1) Open your jupiter notebook. 2) In the left handside panel you will see a ""Commands"" tab. Click on that. 3) After opening commands tab. Search for Terminal on the top search bar. 4) Select/Click on ""New Terminal"" option. 5) Type below command to install pymysql. conda install -c anaconda pymysql",318368.0
89677,372428.0,"Currently, the system I am using have anaconda installed but not mySQL Server. If I understand, the system we are trying to import pymysql, should have mySQL istalled in the system? Is it because of this, I get the error as ""ModuleNotFoundError: No module named 'pymysql'"" Please confirm on this one, I could have raised another question, thought of posting here, as the error I am facing is same.",313676.0
89677,372450.0,Please follow the above steps @Kapil Dave .Working fine,318846.0
89677,373697.0,"If by chance anyone is using some normal code editor and not jupyter notebook to solve coding questions, then you can try the following command in your terminal: pip install pymysql and after installing this, try importing the package again",301655.0
89677,373782.0,"@Rajani, Kindly check folder name from where u invoked "" conda.exe"" May possbile that in place of Anaconda3 , the folder is named as Miniconda3 .",318770.0
89677,373799.0,"People who have installed Anaconda3 in Windows10, the process is as follows : 1) Open your jupiter notebook. 2) In the Right handside panel you will see a ""New"" dropdown . Click on that. 3)Click on Terminal and in separate window a command line prompt will open. 4) Type below command to install pymysql. conda install -c anaconda pymysql",318770.0
89677,374197.0,install anaconda: condo install -c anaconda pymsql,318017.0
89677,374702.0,@Shubham Execution failed for some reason. Please check SS.,308962.0
89677,375614.0,I am getting the below error while installing,310509.0
89677,375615.0,i tried to run the commant 3 to 4 times but facing the same error,310509.0
89677,376160.0,,310509.0
89677,378295.0,"Thanks @Rajani Madasu, it worked.",310518.0
89825,373350.0,Can you provide you are using for resolution.,318368.0
89825,373392.0,"I think output should be 315,549,948,...",301651.0
90790,378361.0,You have to install pymysql first before importing it. Go through the below for details: https://pypi.org/project/PyMySQL/,310974.0
90790,378375.0,"Module is not found error means pymysql is not installed. You have to install it. Here are the steps: 1. Open your jupiter notebook. 2. In the left handside panel you will see a ""Commands"" tab. Click on that. 3. After opening commands tab. Search for Terminal on the top search bar. 4. Click on ""New Terminal"" option. 5. Type below command conda install -c anaconda pymysql Hope this will help.",317991.0
90790,379318.0,"Hi Damini, Here are the steps to follow: Press ' WIN ' + ' R ' keys Type ' cmd ' and press enter key Type ' conda install -c anaconda pymysql' and press enter key This would resolve the issue. Note: you would need to have working internet connection as it will download the necessary modules and install them automatically.",317987.0
89233,370116.0,"Hi Rashmi, Following is my take with regards to your question, 1. It is expected time of 2hrs, so it need not be true that everyone will be able to finish it within 2 hrs. It is just expected time based on previous data they might have used. The actual time will ofcourse vary from individual to individual. 2. I havent yet reached that section, but are you sure there is no detailed document given? like you said, there were detailed documents in the previous modules, so should be the case with this as well. are you sure you checked thoroughly? if it is indeed not there, you can raise a concern regarding the same, from the help/support section. or, you can even contact your student mentor for the same. 3. Meanwhile, There are lots of tutorials available on the net to help in the intsallation. Here is one: https://o7planning.org/en/11463/connecting-mysql-database-in-python-using-pymysql you can refer them for guidance on how to install, (could save your time) or wait for TAs for some official clarification on the same. Hope that helps.",317998.0
89233,370172.0,"Thanks for the reply Harsith I had checked they had mentioned in Jupyter notebook. "" Let' work through an example using PyMySQL. You can install it using pip install pymysql ."" But no clue on how to install, where to write the above command ? the link you sent helped as it stated the same comand in anaconda prompt. I could install beautiful soup as well. Though it throws now operational error in getting data from rd: as below. OperationalError Traceback (most recent call last) &lt;ipython-input-17-ffda25a7ab0b&gt; in &lt;module&gt;() 6 user=""root"", # your username, usually ""root"" for localhost 7 passwd=""yourpassword"", # your password ----&gt; 8 db=""world"") # name of the data base; world comes inbuilt with mysql 9 10 # create a cursor object c",308495.0
89233,370608.0,"Hello All,' Even i'm also stuck at this points. How to install pymysql in anaconda?? Didn't get much info on stackoverflow.",311004.0
89233,374881.0,"Hi pooja, Sorry for delay in reply. In Start button, you may see ""Anaconda Prompt "" ,,,, just type the command ""pip install pymysql"" and ""on it and run it will get installed. Similarly for beautiful soup. hope that helps",308495.0
89366,370634.0,"Dataframe has a function ""drop_duplicates"" to remove duplicate values. You can use this as df.drop_duplicates() You can also drop duplicate values based on the column. df.drop_duplicates([ 'EmployeeId' ] )",318368.0
89366,409777.0,result after using drop_duplicates method does not match with expected in the question of Filtering Data. Am I missing anything. Is there a different way to remove duplicates?,312199.0
89366,370904.0,"I found below articles useful to know ""how we can remove duplicate rows in dataframe"". Please have a look into it. https://medium.com/@kasiarachuta/dealing-with-duplicates-in-pandas-dataframe-789894a28911 http://www.datasciencemadesimple.com/delete-drop-duplicate-row-dataframe-python-pandas/ Hope it will help.",317991.0
89366,409778.0,"nevermind, found what i was doing different. Was too ambitious and included column to drop duplications, solution here was doing it with out the column being specified.",312199.0
89366,409779.0,not sure how it is useful though,312199.0
89797,373189.0,,318335.0
89797,373123.0,You need to import world database into mysql first. Find below the steps to import the same. https://dev.mysql.com/doc/world-setup/en/,318368.0
89797,376144.0,The passwd needs to be correct with the root pasword given for MySQL installation. I had made a mistake. Fixed the passwd and it now connects.,300717.0
89797,376122.0,"I still get this error, although world DB seems to be present / loading in MySQL",300717.0
89682,,nan,
89865,373818.0,It'll be provided by the service provider. There are many public APIs out there whose link is provided in the content itself. You can try them out.,310974.0
89865,373958.0,You can find list of API's here. https://github.com/toddmotto/public-apis API keys will be provided once you register on providers portal. Once you login you will be able to generate unique API keys for your account. DONOT share those API keys as that will have association with your account.,318368.0
89865,374064.0,"For GoogleAPI you can use following URL : https://developers.google.com/maps/documentation/geocoding/get-api-key?authuser=1 Similiary, other websites also provide their own API keys. The API key is bound to your account on API provider website and API queries follow certain rules as no of hits per second etc. Therefore beware of sharing your API keys with others.",318770.0
90361,376711.0,numpy isnan works only on int and float. Pandas isnull works on object types,306248.0
90361,376862.0,"These functions are from two different libraries. isnull -&gt; Its a function from Pandas and takes care of all datatypes. (Being it null, NAN, NA) etc. isnan -&gt; It a function from Numpy and takes care of all the numeric values.",318368.0
90361,376712.0,got the answer guys. I tried to impute the missing values in Bathroom using np.isnan() and it threw an error. I found out that we cannot use numpy's isnan() on categorical data types.,318479.0
89951,374550.0,"Internally boolean 'True' is represented by 1 and 'False' by zero. Hence, when we take a sum() of all the boolean values, only the 1's add up and we in turn get a count. In this case, df.isnull() will return a True (one) if the particular value is null else will retun False (zero). When we do a sum() on this, we get a count of null values in the dataframe. PS: Looks like there is a typo in your question.",313826.0
89951,374486.0,"In your question there might be some typo, cause both expresions are same. But, df.isnull() will give you dataframe with values True or False based on whether value is null or not. Then we have to do aggregate on each column to get total null values.",318554.0
89422,370962.0,Replace all the Backword slash \ with a forward slash / It will work then.,314547.0
89422,370981.0,thanks a lot its working,318723.0
89422,371153.0,"Use the below code and it will work. companies = pd.read_csv(r""C:\Eshwar\Eshwar personal\UpGrad IITB\Python\3_Getting_and_Cleaning_Data\3_Getting_and_Cleaning_Data/addresses.txt"", sep = ""\t"", encoding = ""ISO-8859-1"")",301555.0
89422,377157.0,Yes this \ issue comes for many times. Because windows path is \. For all the submission we should make / while calling pandas read_csv. but some cases it works,312019.0
90276,376178.0,Try working on jupyter notebook very convenient and easy to use because anaconda is prepopulated with it .,305838.0
90276,376180.0,Hi It looks like your connection is failing. Were you able to install packages successfully via pip previously?,317149.0
90276,376190.0,Launch a windows command prompt and try installing using the below command conda install -c anaconda pymysql,313826.0
90276,380202.0,may be your connection broken while installing .if that is not the case then try using conda command prompt to install.i shared a video earlier also on official group of PGDDS cohort group3 for same,319869.0
90362,376797.0,"Looks like you are passing incorrect ""user"" and/or ""passwd"" needed for connecting to your MySQL Database. Also, provide a screen shot of the complete error for further investigation.",313826.0
90362,376815.0,"Thanks. I opened MySQL editor, changed password and used the same password in Python. This resolved the error and got the output.",311868.0
90371,376844.0,"You can use forward slash ""/"" and try again.",318368.0
90371,376786.0,"Try giving the complete path of the location of the file with forward slash ""/"" as shown below companies = pd.read_csv( ""C:/Users/Nick.DESKTOP-H152642/Desktop/dataset/companies.txt"" , sep=""\t"", encoding = ""ISO-8859-1"") companies.head() See if this resolves the issue.",313826.0
90371,376748.0,"The error coming is ""FileNotFoundError"". It means read_csv is not able to find the file companies.txt. This error takes place when the file does not exist at the path mentioned in read_csv(). So try to give absolute location of file. For ex: I have kept my files in below location C:\PGDDS\getting_data So I would write companies = pd.read_csv(""C:\PGDDS\getting_data\companies.txt"", sep=""\t"", encoding = ""ISO-8859-1"") Hope this will help.",317991.0
90371,376752.0,please enter correct location of the file where it is stored in your local machine dont gine encoding or seperator as csv is the default file format.,318017.0
90371,376771.0,,308639.0
90371,376775.0,"If you are using r before file path remove space between r and double quotes as follows: companies = pd.read_csv( r""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset\companies.txt"" , sep=""\t"", encoding = ""ISO-8859-1"") companies.head() Let me know if it works ?",317991.0
90371,376888.0,"kindly check file and/or location of the input file, are you running jupyter in same directory else you need to give full file path",317577.0
90371,376923.0,"Try this: companies = pd.read_csv(""C:\Users\Nick.DESKTOP-H152642\Desktop\dataset \\ companies.txt"", sep=""\t"", encoding = ""ISO-8859-1"") companies.head(5) - a double dash before companies.txt to escape any weird hidden character. Worked in my case.",310511.0
89437,371131.0,"You can go through the below one to get an understanding of character encoding: https://www.w3.org/International/questions/qa-what-is-encoding Once done, refer this: http://pandaproject.net/docs/determining-the-encoding-of-a-csv-file.html",310974.0
89437,372624.0,"Generally authors tries to declare character encoding used for the document format they are working with. Note that just declaring a different encoding in your page won't change the bytes; you need to save the text in that encoding too as UTF-8, Character encoding etc... A character encoding provides a key to unlock the code. It is a set of mappings between the bytes in the computer and the characters in the character set. Without the key, the data looks like garbage.",318846.0
89722,372594.0,the path needs to be relative to the ipynb file.,306248.0
89722,372600.0,"pd.read_csv() might through error 1. if the file path isnt correct - give the entire directory structure of the file and make sure all the slashes are backword slashes (\). 2. if there is an unwanted hidden character in the file path - make sure to give 2 slashes(\\) before the file name in the file path. Eg. bank_data = pd.read_csv(""D:\Course 1 - Introduction to Data Management\Module 3 - Python for Data Science\Pandas_practice dataset\\bank-marketing_original.csv"") Hope this helps.",310511.0
89722,372596.0,"import numpy as np import pandas as pd companies=pd.read_csv(r""C:\Users\Rajini\Desktop\Data_science\py4me\3_Getting_and_Cleaning_Data\companies.txt"", sep = ""\t"", encoding = ""ISO-8859-1"") companies.head() Please use the above code.....We have to delimieter of the file in the parameter sep --by default(,) encoding is the code page type. http://jrogel.com/python-3-pandas-encoding-issues/ https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html",318846.0
89722,376107.0,"import pandas as pd import numpy as np file = pd.read_csv(r""EntirePathofTheFile\FileName.csv"") #you can use the path of file from the windows explorer",301555.0
90101,375207.0,"This will happen if Column doesnot exists in the Dataframe. Try to print the df or df.columns to see what all columns are availble. One more possible reason could be if you have already dropped a column and if you try to run the same code again, you will see that error.",318368.0
90101,375206.0,Make sure you haven't dropped it already. Inspect the data frame once.,310974.0
90483,377428.0,"Try this: print(round(100*(df.isnull().sum(axis = 0)/len(df.index)),2)) It seems that the round argument= '2' was listed outside the round function. Re-ordering the brackets should solve it.",318085.0
90483,377415.0,"you have the ending brackets misplaced. It should be print(round(100*(df.isnull().sum(axis = 0)/len(df.index)),2))",309451.0
90383,376839.0,Have you checked if that web page you are trying to have an HTML tag called reviews?,301648.0
90383,377492.0,"Web Scrapping is always risky, if something gets changed in website, you will not get any output or you will see error.",311404.0
90383,377488.0,"Now there is no class exist named review-body see current config and belos is my output where data actually exist. You can find inside of any HTML page in Chrome using F12 button and click on elements. Then you can check each and every small details. class=""XjE2Pb""&gt;Language: English&lt;/span&gt;&lt;div jsname=""Gvbqzc""&gt;&lt;span&gt;By purchasing this item, you are transacting with Google Payments and agreeing to the Google Payments &lt;a href=""https://payments.google.com/termsOfService?hl=en"" target=""_blank""&gt;Terms of Service&lt;/a&gt; and &lt;a href=""https://payments.google.com/legaldocument?family=0.privacynotice&amp;amp;hl=en"" target=""_blank""&gt;Privacy Notice&lt;/a&gt;.&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;c-data id=""i40"" jsdata="" Qv8W4;_;3""&gt;&lt;/c-data&gt;&lt;/c-wiz&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;]",311404.0
90383,376840.0,"Google has changed the way this document was rendered previously. Not this class "" review-body"" doesn't exists on the page. So you will not be able to use this any more. You can use any other website to do the same thing, Find code below to scrap trip advisor website with the same code. import requests, bs4 # getting HTML from the Google Play web page url = ""https://www.tripadvisor.in/Restaurant_Review-g297628-d7009948-Reviews-Bangalore_Belly-Bengaluru_Bangalore_District_Karnataka.html"" req = requests.get(url) # create a bs4 object # To avoid warnings, provide ""html5lib"" explicitly soup = bs4.BeautifulSoup(req.text, ""html5lib"") # getting all the text inside class = ""review-body"" reviews = soup.select('.partial_entry') print(type(reviews)) print(len(reviews)) print(""\n"")# printing an element of the reviews list print(reviews[5])",318368.0
90496,377505.0,"Yes, you can use the following code format: df.columns.get_loc('&lt;col_name&gt;') This would get you the column index within the data frame.",318085.0
90496,377546.0,"You can use ""df.columns.get_loc('col.name') to get column index of dataframe using column label. Here is the example: import pandas as pd import numpy as np df = pd.DataFrame({""pear"": [1,2,3], ""apple"": [2,3,4], ""orange"": [3,4,5]}) df.columns o/p Index(['apple', 'orange', 'pear'], dtype='object') df.columns.get_loc(""pear"") o/p 2",317991.0
90487,377435.0,you have to create a df which doednt have null values in a row greater then 5 so first you create a df by : df[df.isnull().sum(axis=1)&lt;5] this is a new dataframe now you can do various operations on this enjoy :),318017.0
90487,377436.0,You might want to remove the rows which has more than 5 missing values. df.isnull().sum(axis=1)&gt;5 specifies true for those rows which has more than 5 missing values. df.isnull().sum(axis=1) &lt;= 5 specified true for those rows which has 5 or less missing values and df[df.isnull().sum(axis=1) &lt;= 5] returns the rows which has 5 or less missing values.,318329.0
90487,377445.0,"Since this is an in-module exercise, I cant give you the direct answer. But you can follow the below approach: The criteria must be reversed and applied ito the parent dataframe like this: Dataframe = Dataframe[Reversed condition of criteria] Then print the percentage of missing values. Hope that helps :)",318085.0
90487,378294.0,"Hi Sanjida Try this also.. import pandas as pd df = pd.read_csv('https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0') df = df[df.isnull().sum(axis=1)&lt;=5] print(round(100*(df.isnull().sum(axis=0)/len(df.index)),2))",308639.0
90487,377434.0,"As far i can remember the question wants you to remove the rows having &gt;5 missing values. df[df.isnull().sum(axis=1)&gt;5] -&gt;&gt; this does the opposite, it tells to keep the rows having more than 5 missing values. Instead do this df[df.isnull().sum(axis=1)&lt;5]",309451.0
90491,377497.0,"df.info() returns the information of the dataframe ( column name,rows it contains,its data type etc.) whereas df.info will return the table itself just like an excel sheet with the values. Eg. Name Place 1 Rajat Pune 2 Swara Delhi",318495.0
90491,377473.0,"# getting the second string train.Name.str.split(',').apply(lambda x: x[1]).head()",317982.0
90491,377496.0,"df.info() returns the information of the dataframe ( column name,rows it contains,its data type etc.) whereas df.info will return the table itself just like an excel sheet with the values. Eg. Name Place 1 Rajat Pune 2 Swara Delhi",318495.0
90765,378214.0,"The code you have posted removes the rows but not columns. For removal of columns, you might have to use drop method.",318329.0
90765,378275.0,for deleting rows use something as df=df[df.isnull().sum(axis=1)&lt;5] and then for percentage use df[df.isnull().sum(axis=1)/len(df.index. multiply by 100 and use the round(),309451.0
90765,378277.0,For removing the rows from your dataframe; 1. You need to filter rows with greater than 5 null values thrrough using a condition with '&gt;' sign in your code 2. Then you check the statistics of the data to see if the less than 5 null value rows can be imputed with some number like mean or anything 3. Still for some null value rows you categorize the data and then imoute it again in the missing value columns these are the procedural steps which helps in getting rid of missing value rows. Code you can refer in the Jupyter file. Try to relate it will the steps and it should be easy in understanding...,316349.0
90470,377371.0,you're removing the rows from your dataframe which has many null values in it.. 1. You filter rows with greater than 5 null values thrrough a condition with '&gt;' sign 2. now you check the statistics of the data to see if the less than 5 null value rows can be imputed with some number like mean or anything 3. still for some null value rows you categorize the data and then imoute it again in the missing value columns this are the steps which helps in getting rid of missing value rows,316349.0
90470,377390.0,There might be times when there is a need to drop specific rows or columns based on any defined criteria. The drop function helps us to achieve that with a simple function call. Here is the documentation for this function: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html,318085.0
89368,370639.0,Error suggests that your username or password is incorrect. You need to replace username and password with username and password of your mysql installation.,318368.0
89368,370640.0,"It won't work because you are using ""yourpassword"" as password in the connection details. Replace it with the actual root password that you'd set during the installation and retry.",310974.0
89368,370750.0,"Try following options once - '127.0.0.1' instead of 'localhost' - Empty password '' if you do not have set password earlier If the above does not work you need to create mysql user and give proper grants to user, and its a lenghty process.",306248.0
90619,377938.0,your understanding is correct row = 1,307843.0
90619,377947.0,Axis = 1 denotes rows Axis = 0 denotes columns,318085.0
90619,377976.0,"Yes. Your understanding is, indeed, correct. In Pandas: Axis 0 denotes columns. Axis 1 denotes rows.",306040.0
90553,377747.0,"Engine is an argument/parameter using in ""read_csv"" function of pandas. Its tells the pandas which parser engine to use while reading csv file. It can use values like (""c"", ""python""). Ita an optional parameter. Where the differences are The C engine will naturally be faster. This is important if you are importing large datasets. The benefits of the Python parsing are a more feature rich set. This benefit may mean less if you are loading big data into memory. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html",318328.0
90113,375272.0,try this https://stackoverflow.com/questions/51257233/python-importerror-dll-load-failed-the-specified-module-could-not-be-found,318017.0
90113,375496.0,"try referrring this link for resolving your issue, if this helps --&gt; https://groups.google.com/forum/#!topic/pymssql/obSPFzBEKD8",318429.0
90113,376304.0,Please share complete screenshot. The error is not clear or showing with what you provided,301555.0
90113,378921.0,"Please check whether your mysql service is running or not. Try connecting through workbench first and then with python. If you have installed mysql as service you have to look into mysql service, if it is a zip file then it should be initialized first with D:\mysql\bin&gt;mysqld --initialize --console Here you will get the password 2018-10-13T04:56:00.460980Z 5 [Note] [MY-010454] [Server] A temporary password i s generated for root@localhost: 9%&lt;MfRazLqo) use the above generated password (this is specific to your instance, do not use above one) to change the root password (initially you have to change the password) by login in to mysql (in another command prompt may be) D:\mysql\bin&gt;mysql (Enter key) Enter above generated password use below command to change the root password ALTER USER 'root'@'localhost' IDENTIFIED BY 'MyNewPass'; Let me know if you need some more help",313241.0
90642,380775.0,"INFORMATION_SCHEMA provides access to database metadata, information about the MySQL server such as the name of a database or table, the data type of a column, or access privileges. Other terms that are sometimes used for this information are data dictionary and system catalog. Refer to the following link: https://dev.mysql.com/doc/refman/8.0/en/information-schema.html",306040.0
90360,376688.0,Try running the command in terminal: conda install -c anaconda pymysql,309451.0
90360,376864.0,"Please find below solution for installing pymysql in Jupyter notebook. 1) Open your Jupyter notebook. 2) In the left handside panel you will see a ""Commands"" tab. Click on that. 3) After opening commands tab. Search for ""New Terminal"" in the top search bar. 4) Select/Click on ""New Terminal"" option. 5) Type below command to install pymysql. conda install -c anaconda pymysql",318368.0
90862,378709.0,"What data you want to fill depends on your intuition. Median can be used but even if you use median, you will get around the same number as maximum number of people fall in same bucket. There is nothing wrong in using median when your data is skewed.",318368.0
90862,378728.0,"imputing data for missing values is completely based on the statistics and mathematical significance of your data.. you could impute the mean for car and bathroom as well but is it really logical? bathroom can never be 1.5 or 1.677 or anything decimal but rather a whole number.. and same is the case for car parking as well.. Also another factor to take into consideration is the standard deviation, the values are deviating too far for car and bathroom.. hence, we categorized the data to find maximum number of bathroom is in which category and same for car, and imputed the maximum number in the missing places..",316349.0
90862,379774.0,"Mean and Median are computed as Numerical Values (Results can be interger or floating type both). As you see in the dataset provided, the no of bathrooms can be pure intergers (not floating type) as 1.5 Bathroom wont make sense or a person cannot have 1.6 cars. Therefore, to impute missing values, Mean and Median cannot be used for cars and bathrooms column.",317811.0
89877,373943.0,You are not priting the data after assigining to all_rows. Please use print(all_rows) to print collected data.,318368.0
89877,373844.0,Do you have data in the database?,318329.0
89877,373879.0,"You might want to print (all_rows). Seems currently, the output is held within all_rows.",302740.0
89877,373948.0,"Yep, I totally missed it. The output is just held in all_rows and you have to print it.",318329.0
96468,410995.0,Please refer to a similar query: https://learn.upgrad.com/v/course/208/question/96317,313826.0
102972,441757.0,"For larger sets of python code, we can use nginx as a server and load balancer. The choice of cloud storage depends on you , you can choose Amazon Web Services(AWS), Digital Ocean,etc whichever you are comfortable with.",301655.0
116036,501858.0,Try below code and check the result:- Q1 = df.quantile(0.02) Q3 = df.quantile(0.98) IQR = Q3 - Q1 df = df[(df &gt;= Q1 - 1.5*IQR) &amp; (df &lt;= Q3 + 1.5*IQR)],310419.0
90858,378677.0,"First you will need to set index to column on which you want merging to happen. Then you can add both the dfs. df = df1.set_index('Name').add(df2.set_index('Name'), fill_value=0).reset_index()",318368.0
90858,378686.0,"Question language is ambigious. I think you want to combine two dataframes having exactly same column abel. When it combines, common values should be summed up. For this you use : Combined = df1 + df2 Combined - Resultant dataframe df1 - First dataframe df2 - Second dataframe Note: It will put NaN at those fields whichever not present in either dataframe; Considering [a+ NaN = NaN]",318770.0
90125,375352.0,"Strings passed as the on , left_on , and right_on parameters may refer to either column names or index level names. This enables merging DataFrame instances on a combination of index levels and columns without resetting indexes. Refer https://pandas.pydata.org/pandas-docs/stable/merging.html#merging-on-a-combination-of-columns-and-index-levels",318804.0
90125,375358.0,"As you knowwe can merge two dataframe using pd.merge() command whose sytax is as follows: pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None) Where each parameter has following meaning: left : A DataFrame object. right : Another DataFrame object. how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’ left: use only keys from left frame, similar to a SQL left outer join; preserve key order right: use only keys from right frame, similar to a SQL right outer join; preserve key order outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys on : Column or index level names to join on. Must be found in both the left and right DataFrame objects. If not passed and left_index and right_index are False , the intersection of the columns in the DataFrames will be inferred to be the join keys. left_on : Columns or index levels from the left DataFrame to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame. right_on : Columns or index levels from the right DataFrame to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame. Hope this helps.",317991.0
90125,375359.0,"left_on : label or list, or array-like Column or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns. right_on : label or list, or array-like Column or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns. Examples &gt;&gt;&gt; A &gt;&gt;&gt; B lkey value rkey value 0 foo 1 0 foo 5 1 bar 2 1 bar 6 2 baz 3 2 qux 7 3 foo 4 3 bar 8 &gt;&gt;&gt; A.merge(B, left_on='lkey', right_on='rkey', how='outer') lkey value_x rkey value_y 0 foo 1 foo 5 1 foo 4 foo 5 2 bar 2 bar 6 3 bar 2 bar 8 4 baz 3 NaN NaN 5 NaN NaN qux 7",318368.0
90125,375499.0,"to add further to above example ,left_on,right_on will make more meaningful/clear when using the index level names(like CategoricalIndex/MultiIndex) are used while fine tunning the code. for further more info : https://pandas.pydata.org/pandas-docs/stable/api.html#categoricalindex",301115.0
90149,376346.0,"Hi Adithya, This issue occurs we you try to attempt the same column as index. Might be in eariler statemts you made the 'Order_id' column as indexed. Please use the below command and try once agin df = df.reset_index() df.head()",318846.0
90149,375545.0,"Hi Aditya, This happens because you have already made 'Order_Id' as Index when you ran the commands sequentially. Next time, when you try to do it again, you get the error because it cant find the column 'Order_Id' again(from all remaining columns after omitting current index 'Order_id') for making index. Hope this helps.",310511.0
90149,375529.0,"Can you put complete screenshot. Can't see actual error. Most probably you are running the command second time and it will not find that column again. Just print the df again and verify, you have the column you are looking for.",318368.0
90149,375534.0,"The code is perfectly fine. It is the order in which your are executing the cell creating the problem and you are getting error. I will show you with example: I have code written in two cells Now if we execute both cell one by one starting from upper cell and then lower cell we will not get any error, below is the screen shot for same Please not in above screenshot In[1] and In[2] for subsequent execution of consecutive cells. But now if we execute the second cell again then error come KeyError: 'Order_ID' Please not in above screenshot In[1] and In[3] for subsequent execution of consecutive cells means I did not execute cell 1 which contain , I only executed second cell which has order_df.set_index() code and because I havenot executed first cell error is coming. So always run the cell first which has ""import"" statement. Hope it will help you to understand why error is coming.",317991.0
90149,375582.0,"All the code writeen by it is perfectly correct, I have also tried in my system. I also encounterd the error one time but when I interupt the kernal and restat it is always work.It suggest sequencely you execute and if by mistake you given the wrong column name it will give error so in that case again first you execute the data frame and then execute the set intex code.",307843.0
90149,375959.0,You can try by executing the read_csv command and then this command.,301890.0
90149,376007.0,"After reading the data, set_index is used to set the Order_id as the index. Then when you execute the same set_index block again on the same data, Order_id will not be found as it is already the index. Hence the error. Hope this helps .",317149.0
90157,375565.0,"Yes, you can use right_on and left_on property to specify column names for right and left df respectively. pandas.merge(df1, df2, how='left', left_on=['id_key'], right_on=['fk_key'])",318368.0
90157,375592.0,"You can also achieve it by ""renaming"" the join fields in any of the tables in the merge statment as below. For e.g: df_1 = market_df.merge(customer_df.rename(columns={'Cust_id':'Customer_id'}), how='inner')",318328.0
90157,375591.0,dataframe.merge() syntax in pandas clearly give instruction that column labels must be found in both dataframes. on : label or list Column or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames. Here is the link of pandas documenetaion. Please go through it. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html Hope it clarifies your doubt.,317991.0
90886,378844.0,"You can use the same agg function on multiple cols but can't use diff functions for diff keys at the same time as the resultant data doesn't make logical sense: df.groupby(['month','day'])['rain','wind'].mean() This is valid",310974.0
90886,378870.0,We can use the agg() function https://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html Please see an example below where I am grouping by Customer_Segment and Product_Category and getting the min &amp; max Shipping cost as well as the mean Profit.,313826.0
90327,376441.0,"Mulitple ways of doing the same. 1) Using lambda function. dfs = [df1,df2,df3,df4] df_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs) 2) Using merge nested. df_final = pd.merge(pd.merge(df1,df2,on='name'),df3,on='name') 3) Using merge serially. df_final = df1.merge(df2,on='name').merge(df3,on='name')",318368.0
90327,376447.0,Yes you can merge more than 1 data frame in line. Please go through below link https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes,317991.0
90327,376883.0,"you can megre dataframes, kindly see : https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html",317577.0
90161,375581.0,"You can set_index by using below function and you will need to do it for each df i.e. gold, silver, bronze and then perfom the add operation. gold.set_index('Country', inplace = True)",318368.0
90161,375576.0,"Try the below code: tot_medal.set_index(""Country"",inplace = True)",318084.0
90161,375580.0,"u need to use set_index DataFrame.set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False) Parameters: keys: Column name or list of column name. drop: Boolean value which drops the column used for index if True. append: Appends the column to existing index column if True. inplace: Makes the changes in the dataframe if True. verify_integrity: Checks the new index column for duplicates if True.",317982.0
90161,375636.0,The way to approach this problem is as below: Set the index of all the three dataframes to the column 'Country' Add the three dataframes. Ensure to use the fill_values argument to avoid the NaN values. Sort the combined dataframe by medals column in descending order. Hope this helps.,313826.0
90161,376013.0,"Since the Dataframes gold, silver, bronze do not have any specific index fields assigned, when .add is called, the addition happens on the default index (0,1,2) which gives you an addition of all the country values (example :USAUSAFrance). Instead if you set the country as index for the Dataframes before the .add, the addition will happen on the medals with the country as index. Hope this helps.",317149.0
90161,375693.0,"import numpy as np import pandas as pd gold = pd.DataFrame({'Country': ['USA', 'France', 'Russia'], 'Medals': [15, 13, 9]} ) silver = pd.DataFrame({'Country': ['USA', 'Germany', 'Russia'], 'Medals': [29, 20, 16]} ) bronze = pd.DataFrame({'Country': ['France', 'USA', 'UK'], 'Medals': [40, 28, 27]} ) gold.set_index('Country', inplace=True) silver.set_index('Country', inplace=True) bronze.set_index('Country', inplace=True) d=gold.add(silver, fill_value=0) e=d.add(bronze, fill_value=0) m=e.sort_values(by = (['Medals']), ascending = False) print(m)",317811.0
90161,376508.0,You should not assign inplace with True within quotes. True is supposed to be assigned as a boolean not String.,305655.0
90418,377026.0,Just check if the value of n is the same as the one that you are reading from the standard input. Apart from that I dont see anything wrong with the code.,318084.0
90418,377048.0,the code is absolutely fine but as Vinay said you have to print the series_1and that line is missing print(series_1),300687.0
90418,377030.0,"You need to explicitly print series_1 in the coding console. Instead of the last line, write the following code print(series_1)",313826.0
90418,377075.0,Just use print function to print your answer. print(series_1),318368.0
90418,388768.0,You can simplify your code by doing this: series_1= pd.Series(np.arange(n+1)) series2= series_1.apply(lambda x:x**2) print (series2[1:]),312826.0
89430,371012.0,Download the file and change the location in the Jupyter script. The corresponding Jupyter Notebook is there at the starting of every module. Do remember to change the backward slash \ with a forward slash / Example: 'C:/Docs/IIIT/global_sales_data/market_fact.csv' This will be the location that you need to give if your file is under C:\Docs\IIIT Hope this helps.,314547.0
89430,371089.0,"Issue: If you are getting the error in reading the CSV file and getting the error as below FileNotFoundError: File b'../global_sales_data/market_fact.csv' does not exist Solution: Change the path to 'global_sales_data/market_fact.csv' , and this shall work. Onservation: This is happening because of the working director, your python file location which you are currently working on, the csv files should exist under the same folders or subfolders and thus it shall be accessible.",313676.0
89430,371054.0,I don't think forward slash or backward slash make much difference. When I tried to read data from market_fact.csv using forward slash / and backward slash \ in address it works fine for both. Below is the screenshot:- What I can infer from the FileNotFoundError: File b'../global_sales_data/market_fact.csv' does not exist code is that this is not the correct address where the file exist in your computer. Make sure you enter the correct location in read_csv while reading the data from any file.,317991.0
89430,371052.0,If your file is in the same directory as the notebooks than relational path 'global_sales_data/market_fact.csv' would work. Or else specify the entire path.,317689.0
89430,371143.0,"Looks like you haven't downloaded the Package which from the introductory session of Pandas. Download the package and Extract it. You will see a folder with name global_sales_data where you can find all the .CSV files mentioned in the sessions. Also, when you are trying to read a file, my suggestion is to use relative path i.,e. global_sales_data/market_fact.csv than the absolute path",318804.0
89430,371201.0,Thank you everyone for the suggestions.,315764.0
90119,375304.0,"The columns can be selected by just passing them as a list as shown below: df_2 = df[ ['month','day','temp','area'] ] The same can be achieved using .loc,by mentioning that you want to select all the rows by using a : df_2 = df.loc[:,['month','day','temp','area']] Hope this helps.",313826.0
90119,375335.0,"Maya, loc is Label based indexing and the arguments it takes are all row indexes and columns.. for your case i guess, month, day, are all columns and not indexes (rows).. and hence it'll throw error.. Solution: df.loc[ : , ['month','day','temp','area']]. this will bring all rows corresponding to these columns.. hope it helps :)",316349.0
90119,375339.0,"Please pass them correctly in loc. You need to pass the column names as second argument in the loc as a list. df_2 = df.loc[:,['month','day','temp','area']]",317689.0
90119,375367.0,"Loc accepts two parameters. df = df.loc[:,['month','day','temp','area']] 1) Row &amp; 2) Column -&gt; If you want to use loc. Provide collon(:) in place of row, which will select all the rows. Other way of doing the same is to use columns directly on df. df[['Y','month']]",318368.0
90119,375698.0,"import pandas as pd df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') df_2 = df[['month','day','temp','area']] print(df_2.head(20))",317811.0
90119,375754.0,"Hi Maya , Loc with data frames has below syntax: df.loc[rows , columns] If you need all rows then specify : and list of columns i.e. df.loc[:, column names in list]",305652.0
90119,375989.0,"I believe following is the error you are getting: ""None of [['month', 'day', 'temp', 'area']] are in the [index]"" Loc is used to access rows and columns by labels. Here the code has been written to retrieve rows with these labels – hence the error. To return all rows with the above said as columns please use the following: YourDataFrame = DataFrame_1.loc[:,['month','day','temp','area']] Hope this helps.",317149.0
90119,376056.0,"You mention all the rows explicility by using [:, df_2 = df.loc[:,['month','day','temp','area']]",303669.0
90119,376141.0,"dataframe.loc takes two parameters at a high level (rows,columns) &gt;&gt;In the case you have mentioned df.loc[['month','day','temp','area']] will return a result in the form of a data frame only if the rows are indexed with these labels. Since these are column names/ attributes, try giving them as second parameter with a colon in first parameter to indicate all rows. df.loc[:, ['month','day','temp','area']]",311803.0
90119,376873.0,"df[['month','day','temp','area']]",317577.0
89362,370573.0,"It works as the file u r working is relative to the csv file u r importing, so u can use the relative path here...Moreover absolute path shoupd also work",318358.0
89362,370586.0,"Hi Utkarsh, Give the entire directory where your csv file is. ALso make sure that all the slashes are back slashe(\). Hope this helps.",310511.0
89362,370594.0,It is working because this global_sales_data is in the same folder as your python notebook. Hence the relative path is working. You can also give the full path.,317689.0
89409,370935.0,"I could not find a direct function for mid in pandas. you could use the following to print mid 5 rows and join all three data frames mid_index = int(len(market_df.index)/2) market_df.head().append(market_df.iloc[ mid_index-2:mid_index+3 , : ]).append(market_df.tail())",318329.0
89409,370956.0,1. for top 5 rows: df.head(n =5) (by default balue of n is 5) 2. for mid 5 rows : middle_index = df.shape[0] // 2 df.iloc[middle_index-2 : middle_index+3] 3. for last 5 rows : df.tail(n = 5) (by default n=5),318532.0
89409,372233.0,Please refer to this link it has all the ways to do this https://pandas.pydata.org/pandas-docs/stable/merging.html,317982.0
90294,376316.0,"All the csv files are part of the zip folder (under the global_sales_data directory) available at the bottom of this link: https://learn.upgrad.com/v/course/208/session/19863/segment/101059 PS: In some of the notebooks, the path to access these files may not be correct. You will have to modify the path to point to the location where you have unzipped the datasets.",313826.0
90294,376515.0,"Under Introduction to pandas at the end of the page, there is a zipped folder with all the .ipynb files and the data files for download. You can find the datafiles under 'global_sales_data'. While running the code for reading the files, please make sure it is refering the right path. Hope this is helpful",317149.0
90294,376319.0,"You can find all these dataset by going through the below session as mentioned below Intro to Data Management --&gt; Module 3 (Python for Data Science) --&gt; Session 3 (Introduction to Pandas)--&gt; Introduction You will see download link at bottom naming IPython Notebooks used in this Session , Download it, unzip it you will find global_sales_data folder. Inside that folder you will find all csv files.",317991.0
89349,370785.0,The example was only for intuition puropse and not a quantitave analysis.,306248.0
89349,370533.0,In this question in the end they are just counting the number of medals irrespective of the type. The medals are of different type earlier just to give us example to learn the technique to do dataframe add operation. No weightage is given to any medal in the final tally count.,317689.0
89349,371736.0,The example was taking the weightage of the medals into account. Just a quantitative analysis for us to strengthen our concepts. Please don't take it seriously :P,308962.0
89487,371286.0,"This was explained very clearly in the course video itself. Since you are explicitly asking pandas to index the rows using the custom defined list ""[1,2,3,4]"", it is taking that. Otherwise, the usual positional index starts from 0.",310974.0
89487,372697.0,"If you see above, when you print s_3, you get index starting from 0 and values are 1,4,9 &amp; 16. When you convert the list to series,it undertands that There are four elements and it converts them to series. Unfortunately, it starts from index 1 and 4 elements, i.e. 4, 9, 16 &amp; fourth element being NaN. Again when you do not provide the valie of index, it takes it as default which is array and hence you get all the values correctly.",301555.0
89494,371503.0,Series keyword should be in Initcaps.,310511.0
89494,371315.0,"S in series should be capital series_1 = pd.Series(['6','7','8','9','2','3','4','5'])",318368.0
89494,373990.0,"Below is the syntax and make sure that S is captial while using Series series_1 = pd.Series([6,7,8,9,2,3,4,5])",302744.0
90226,376244.0,the one which is more readable will pass the test as it would be understood by a lyman,318017.0
90226,375872.0,I have seen this behavior with 2 of my problems. It is just an observation.,301649.0
90226,375855.0,"No, that is not due to readability of the code. it is because in some questions there no furhter hidden test cases to verify your code and thus it shows submitted and not accepted. hope that helps",317998.0
90226,376053.0,Even I had a same issue for couple of times.,303669.0
90226,375929.0,"There are two types of test cases: 1. Sample test cases - which run against the input when you click on verify. 2. Non-sample test cases - which run against the input when you click on submit. When you click on submit and it shows only ""submitted"" it means there are only sample test cases and there are no non-sample test cases. And your solution has been accepted. Its perfectly fine , no need to worry if it is giving ""submitted"" after clickng submit. You can cross check this with FAQ section add the bottom of the page in below link: https://learn.upgrad.com/v/course/208/session/19861/segment/101047 Hope it clarifies your doubt.",317991.0
90225,375848.0,You don't need lambda function for this. This will do: df['XY'] = df['X']*df['Y'],310974.0
90225,375851.0,"You can do below to achive multiplication. df1['XY'] = df1.apply(lambda x: x['X'] * x['Y'],axis =1); In above can apply will work on each row of the df. and pass it to lambda function. In your lambda function you can read column and mulitply them",318368.0
90225,375866.0,You can write the code without lambda function as mentioned. df['XY'] = df['X']*df['Y'] Above code will do.,317991.0
90225,376882.0,df['XY'] = df['X']*df['Y'] or df['XY'] = df.X * df.Y,317577.0
90552,377699.0,I saw the published solution and figured out the error,312608.0
90552,377705.0,"The error is due to this piece of code range(1,(n+1))**2 Range returns a iterable and hence the error. You could modify it as: opseries = pd.Series(np.array(range(1,(n+1))) **2 ,index=range(1,n+1))",313826.0
89507,371984.0,"import numpy as np import pandas as pd list = [6,7,8,9,2,3,4,5] series_1 = pd.Series(list) series_2 = series_1.apply(lambda x:x**2) print(series_1) print(series_2)",317811.0
89507,371360.0,"Although your solution is correct for [1,2,3,4] as an input. It is failing because you have hardcoded the input. The problem is in below code: series_1=pd.Series([1,2,3,4],index=['1','2','3','4']) There are sample cases which run when you verfiy and non-sample(hidden) test cases which run when you submit. You were supposed to make a code generic enough so that it will execute fine for any input or test cases. Remove hardcoding from the code and you code will work fine. I hope my explanation helped to clear your doubt.",317991.0
89507,371361.0,"because you have hardcoded the input. you need to create a series of first n natural numbers. where n is the input. but you have hardcoded input list to 1,2,3,4 along witj indexes. chamge that and it should work.",317998.0
89507,371382.0,please use manual indexing to solve this question,318358.0
89507,371474.0,Use range function inistead of hardcoding the values. That will work.,310511.0
89507,371658.0,"DONOT hard code values in the solutions. You always should use raw input provided to you on the top. You can also change raw input in input pane and check for your possible testcases for your program. Once you hardcode the values in the program, It wont pass for any other testcase.",318368.0
89507,372609.0,"Hello, Here is what I did s = pd.Series(range(1, int(n)+1)) series1 = s.apply(lambda x:x**2) newSeries = pd.Series(series1.values, index = s.values) print(newSeries) Let me know in case there is any issue with the solution --Rajesh",300708.0
89507,373515.0,"Hello, below is how i did it. import numpy as np import pandas as pd series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x:x**2) print(series_1) print(series_2) Hope it helps:)",305129.0
89507,373790.0,"below code worked fine for me numbers=np.arange(1,n+1) numbers_to_series=pd.Series(numbers, index=[numbers]) print(numbers_to_series.apply(lambda x:x**2))",302744.0
88850,368218.0,"I got the answer by googling guys, df.sort_values(['a', 'b'], ascending=[True, False])",317993.0
88850,368421.0,Good point. I was also wondering the same. Thanks for putting the effort.,311686.0
88850,368429.0,Yeah it works ... great,308495.0
88850,368837.0,"As “by” property of sort_values function accepts array, “ascending” property also accepts array. So do the ascending for 1 attribute and descending for 1 attribute, you will need to write following df.sort_values(by=[‘month’, ‘day’], ascending=[True, False]) Hope this helps :) Happy learning :)",318368.0
88850,368996.0,"df.sort_values(by=['month','day'],ascending=[True,False])",304692.0
88850,369725.0,"ascending also accepts lists. df.sort_values(by=['month','day'],ascending=[True,False])",318756.0
90132,375436.0,"try d3 = d3.drop(['Medals_x', 'Medals_y'], axis=1)",318358.0
90132,375437.0,"Got the answer it has to do with pandas version not python. This tools uses something below pandas version 0.21 from this version column has been added as parameter for drop method. I use 0.23 above version in my local. Anyways df.drop(['col_x','col_y'], axis=1) worked.",301118.0
90439,377150.0,"Since the fourth element of 'one' is blank, the first series is taken as float.",318084.0
90439,377158.0,the fourth entry in 'one' is NaN even the other values you can see printed as 1.0 2.0 etc.. that is treated as float where as 'two' is int and all values are printed as 1 2 3 etc..,316349.0
90439,377159.0,"Since, the series ""one"" has only three elements where as series ""two"" has 4 elements, a NaN value has to be added as fourth element to the series ""one"" , which causes the co-ersion of the other elements from int to float to cater for any future values that could replace this NaN value.",313826.0
90439,377174.0,"as both, the series is the different length. It's not possible for Pandas to store NaN values in integer columns. so, this makes 'float' as the default choice for data storage. because of missing values, it's changing the first column to float type. https://stackoverflow.com/questions/38003406/pandas-why-is-default-column-type-for-numeric-float",317845.0
90953,379326.0,for iloc you are refering to the index of the row that you want to fetch so if this is the case then the data will be fetched for i-1 rows only and when you use the loc then you are refrerrimg to the name of the row index it can be possible that only rows with index 0 and 2 are only present then only those two rows will be fetched so in this case it will find all rows for the given range.,318017.0
90953,379780.0,df.iloc[0:3] -&gt; in iloc function the range works starting at the first index i.e. 0 and goes upto the ending index-1 i.e. 3-1 upto 2. df.loc[0:3] -&gt; in loc function the range works starting at the first index i.e. 0 and goes upto the ending index i.e. 3. Your observation is completely correct. This is mentioned in the module in the screen notes given before videos in Idexing and Slicing of Pandas.,317811.0
90953,379421.0,df.iloc[0:3] -&gt; All the rows where index label is 0 to 3 (3 Excluded). df.loc[0:3] -&gt; All rows from 0th to 3rd index (3 index). Both will excluded 3rd row/ 3 lable index. But if you have different indexing in the index column. Result might look a bit strange but you can first print you df to check index labels in the df.,318368.0
90953,379357.0,"iloc : iloc is used when you want to select elements by index. Now for dataframes if we do not set index explicitly then pandas api set it by default which is like [0,1,2,3,....] Now indexis is 0 based. df.iloc[0:3] will seek positional index and will select first 3 records 0th, 1st and 2nd. loc: For loc pandas expects explicit row or column index. ie by Value. So if your row index are integer then you can use 0:3 1:4 etc. But keep in mind as it is value based, it takes full range of values ie 0:3 means 0,1,2,3 But if index is something different ie 'a','b','c' etc then with loc we can not use 0:3 Now for your question, it might look confusing because you have used default index.",318554.0
90953,382032.0,"Hi , Below Image will give clear difference between them, for in-depth below article will help you https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/",318732.0
90232,376098.0,Append does not work when the no. of columns is different for two or more series. It will throw an error when coloumns is different. when column is same 2nd frame appended the below of first syntax a1.append(A2) however using concatenation you can overcome this you cocact the data frame which fills the NA when columns is different give the size of column of bigger frame. syntax is a1.concat(A2) here you can provide the axis i.e row or column wise concatenation.,307843.0
90232,376096.0,It works based on a concept called broadcasting: For reading https://stackoverflow.com/questions/29954263/what-does-the-term-broadcasting-mean-in-pandas-documentation,310974.0
90450,377248.0,"You need to use the inplace=True operation in order to sort the df dataframe and save it in df dataframe itseld. df.sort_values(by='Medals', ascending=False,inplace=True)",313826.0
89529,371540.0,"You have four rows so you need to give atleast 4 index values for the series. try adding d also pd.Series(pd_series1, index = [‘a’, ‘b’, ‘c’, 'd' ]) and see if it works.",317998.0
89529,371646.0,"There are two ways of doing what are you trying to achive. 1) Provide indexes which creating Series import pandas as pd pd_series1 = pd.Series([1,12,33,44],index = [1,2,3,4]) print(pd_series1) 2) Change index after creating Series. import pandas as pd pd_series1 = pd.Series([1,12,33,44],index = [1,2,3,4]) pd_series1.index = [1,2,4,3] print(pd_series1)",318368.0
89529,371638.0,The number of index values should be equal to number of rows in the series. So add one more value to the index and try,318804.0
89529,371901.0,"Right, but over here you have hardcoded. what if i donot know the length of the series or the values within the series and what my indexing to start from 1. wonder how others coded for the 3rd problem: Input: A natural number 'n' Output: A pandas series with the first 'n' natural numbers in the index position and their respective squares in the adjacent column. Example: Input 1: 4 Output 1: 1 1 2 4 3 9 4 16 dtype: int64",316036.0
89529,371924.0,"Hi Prateek, You can use the range function to get the indexes from 1 to n. where n it the input (length) Here is how you can do it: n = int(input()) import numpy as np import pandas as pd series=pd.Series((range(1,n+1)),index=range(1,n+1)) series= series.apply(lambda x: x**2) print(series) Hope that helps.",317998.0
88895,368439.0,no all questions are either accepted or rejected. might be some issue .. sometimes it takes time due to internet speed and everything,317982.0
88895,368450.0,Refer the FAQs on that page. That will help. This point is mentioned there.,314547.0
88895,368453.0,Please refer the FAQ present on the below page https://learn.upgrad.com/v/course/208/session/19861/segment/101047,308635.0
88895,368463.0,"Thanks, totally missed looking for it in the FAQ section.",312490.0
88895,369138.0,"Suppose If the solution does not match the rubrics, then it may go in submitted state. Even if the solution is correct and pass the test cases.",310482.0
88895,369253.0,"I ran into same issue and hence upvoted Apurva (good job Apurva! ) Since the issue is more generic i.e. it applies to many other modules/ pages I believe the question ""Submitted"" in https://learn.upgrad.com/v/course/208/session/19861/segment/101047 can be put up in a general section",309211.0
89900,375260.0,"I could see in your statement there is a comma after 21 as df_2 = df[2:21,]. If you want the rows from 2 to 20 to be printed, try df_2 = df[2:21]",314730.0
89900,374026.0,"in df.loc(x1:x2,y1:y2) indexes x1, x2, and y,1,y2 represent row and column indexes. First point is indexes start with 0. So if you need to get row 2, you will need to give index as x1 as 1. Also in x1, x2 values x1 is included and x2 is excluded. So when you provided 1 as start row number you will still need to keep x2 as 20 as that will be excluded.",318368.0
89900,374038.0,"Reproducing from the Pandas Official Documentation: ""Warning: Note that contrary to usual python slices, both the start and the stop are included"" Please find below the link of rthe same: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html",313826.0
89900,374027.0,"Yes, i also noticed the same. In loc it is a closed interval. ie, doing [a:b] would give all from a to b including a AND b. I dont know why that is the case with loc , but it is contrary to the normal indexing of arrays. you can refer this for more details https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html hope that helps",317998.0
89900,374239.0,"I understand it is because in .loc the range being passed is treated as labelled index. Suppose you label your 1st row as 5 while indexing , 2nd row as 6 and so on. .loc[5:10] will just look at those indexes and bring all of them treating these indexes as 'names' of each row and thus including the row labelled 10 as well. .iloc behaves differently and doesn't work on labelled index. it behaves as we learnt in lists and other Python sessions.",311686.0
89900,374248.0,"Label Based Indexing Pandas provides the df.loc[] functionality to index dataframes using labels . A range of labels, where row_x and row_y both are included , i.e. 'row_x':'row_y' This is the loc function feature to include both starting and end values of range.",317811.0
89900,374370.0,loc function is based on label based indexing which means both the border vakues are included while returning data iloc function on the other hand is position based indexing which include the start point border value only,318358.0
89900,376877.0,loc is location based index and hence the end points are included,317577.0
93775,395272.0,It's kind of hit and trial. You have to analyze the characters and try using the different encoding format.,317845.0
93775,395268.0,"There is no way to know the encoding of a file. You can only find the same while looking at the data. Please refer to the StackOverflow links, provided in the group case study for more information.",312758.0
93775,395266.0,"You need to look at the data and identify the encoding. There is no automated way. Based on the characters included in the file, you would end up chosing one encode",304814.0
93775,395701.0,"Abhishek, I found a link ( https://jrogel.com/python-3-pandas-encoding-issues/ ) which is mentioned in one of the Python sessions and looks like we can get an idea of the file encoding using Python commands: I have not tried it yet but it looks promising and suggested in the below session as well. Please have a look. https://learn.upgrad.com/v/course/208/session/19864/segment/101067",312758.0
88899,368479.0,"Although you might have googled and found this already, but pasting here a good link explaining what is the difference with a good example : https://stackoverflow.com/questions/27980843/python-pandas-functions-with-and-without-parentheses",311686.0
88899,368478.0,Rule: 1. parentheses executes the function. head() is function to pretty print first 5 rows of the dataframe 2. without parentheses gives the attributes. head give the output starting with angle braces &lt; &gt; Below screenshot should help you better with hello() function,306248.0
89901,374060.0,https://stackoverflow.com/questions/22646463/difference-between-and-boolean-vs-bitwise-in-python-why-difference-i Great explaination on above stackover flow.,318368.0
89901,374086.0,Also have a look into below link to get insights into boolean operators vs bitwise operators. https://stackoverflow.com/questions/3845018/boolean-operators-vs-bitwise-operators,317991.0
89901,374710.0,"Hello Khusbu, And and &amp; are used in different context. And is more a logical operator. but &amp; is more a bitwise operator. A good example of bitwise operators is given here. https://www.tutorialspoint.com/python/bitwise_operators_example.htm --Rajesh",300708.0
89901,374521.0,'&amp;' is bitwise operator .Things that can be combined in bitwise operator include True/False and Integer. 'and' is logical operator.It tests whether both operands are logically True or False.if both operands are True then result will be True else False.,320635.0
89901,374103.0,"Hi Khushbu and is a logical operator. It will return True if both the operands are Ture and it will return False if either of the operand is False. See the below example: and is frequestly used with comparison operators like &gt;,&lt;,==,&gt;=,&lt;= and !=. Refer below example: &amp; is a bitwise operator or bitwise AND. It operates at bit level (0,1). The input values are converted in binary and then bitwise AND operation is done. For example x = 5 in binary is 00000101 and y = 6 in binary is 00000110. x&amp;y operation will give 4. For the result of the operation x&amp;y please refer the below example The crux with bitwise operation is that you need to understand binary counting and operations to understand the result. Additionally, in set operations '&amp;' is used for intersection (remenber the intro for python assignment question on student playing football, hockey and criclet ?). Hope I have clarified your doubt.",318479.0
90998,379762.0,"No, not as of now.",317811.0
90998,379797.0,"Hello Bala, We are not facing any issue.You can try to clear the browser cache and Restart your Laptop.",320195.0
90668,378018.0,use the .head() function,318084.0
90668,378103.0,"df.loc[['EmpName', 'Salary']].sort_values(by=['EmpName', 'Salary'])[0:5]",318329.0
89559,371797.0,This problem occured when the required column doesn't exist in the dataframe. In you case if you are executing statement second time so when you execute statement first time it will change the column to index column and second time it wont be able to find that column. Please load the data again from scrach and you will be to execute the statement again.,318368.0
89559,371977.0,run commands in the file starting from top where dataframe is created by reading csv file and then run the sorting command.,317811.0
89559,371812.0,For the market_df dataframe u need to load it first into the memory by executing it and after that it will be able to find the column to which u r setting index,318358.0
89559,372228.0,try it in jupyter before trying here.Answers are getting submitted sometimes and you will not able to proceed,317982.0
89559,373087.0,I faced the same issue. But I figured that I'd changed the index to some other column and then tried to change it back to Ord_id. It gives the same error. Is that the case with you as well. Because Python doesn't allow that. You'll have to load the CSV again.,308962.0
89559,375180.0,That is why use inplace=False. to avoid changing the original database.Using inplace=false will only display the database without actually performing any changes in the original,310472.0
89925,374303.0,"Since you are operating on boolean values, you should use &amp; operator in this case. Refer this for more understanding https://stackoverflow.com/questions/22646463/difference-between-and-boolean-vs-bitwise-in-python-why-difference-i",310974.0
89925,374322.0,and is logical operator and will not work in case of Series. You will need to use &amp; and | -&gt; bitwise operators in this case.,318368.0
89925,374360.0,"Since you are working with boolean values, we need to use bitwise operators &amp; |",318358.0
89925,374375.0,"Code - df.loc[(df['area']&gt;0) and (df['wind']&gt;1) and (df['temp']&gt;15),:] Instead of using df.loc us df[""column name""] directly Similarly use &amp; instead of and because all the values are presented after checking the condition is boolean values . So that logical &amp; will be used. The answer for the question you asked is given bellow: df[(df[""area""]&gt;0) &amp; (df[""wind""]&gt;1) &amp; (df[""temp""]&gt;15)]",318461.0
89925,375780.0,import pandas as pd df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') df_2 = df.loc[(df.area&gt;0) &amp; (df.wind&gt;1) &amp; (df.temp&gt;15)] print(df_2.head(20)),317811.0
90728,378124.0,"You can drop multiple columns as follows: import numpy as np import pandas as pd df = pd.DataFrame(np.arange(12).reshape(3,4),columns=['A', 'B', 'C', 'D']) df.drop(['B','C'],axis=1,inplace=True) Hope this will help.",317991.0
90728,378253.0,"df.drop(['col1','col2','col3','col4',....'col15'], axis=1,inplace=True) is the way to drop multiple columns using the list format. If this does not work and you get a syntax error, then try single column one at a time. Though it is not a preferred way of coding. In single column, there is no need to have""inplace=True""",301644.0
88990,368916.0,The manual indexing question is working fine for me. Can you elaborate a little as to what error you are getting?,318374.0
88990,368943.0,"This is a series question. Series has both index and values. Check whether the output index is staring from 0 or 1. If it is starting from 0 use the below sample example present in the '1__Pandas_Basics' python notebook to set the proper index pd.Series(np.array(range(0,10))**2, index = range(0,10)) Use similar logic like index = range(1,n). Hope this solves your issue",308635.0
88990,369102.0,The manual indexiing is working fine for me. Can you elaborate your problem?,314183.0
88990,369484.0,"The index and the number has to start with 1. If you initiate the range as range(1,n) it will start from 1 but end at n-1. So, consider doing range(1,n+1)",317689.0
88990,369465.0,"You can write you code something like below For example if we consider n = 4 series = pd.Series(np.array(range(1,4+1))**2, index = range(1,4+1)) print(series) Because if we take range from (1,4) , values from 1 to 3 will be printed, here our case is to print 4 as well, try executing the above code.",312756.0
88990,377827.0,"Will this work? ans = pd.Series(np.array(range(1,5))**2, index = range(1,5)) print(ans)",310508.0
88990,369505.0,"Hi, The range of this problem will be range(1,n+1). It will worked for me.",314183.0
89955,374612.0,Function is set_index instead of sort_index for setting the index.,318368.0
89955,374504.0,"Going through the above question I can see that It is asked in question that ""Using set_index command set the column 'X' as the index of the dataset and then print the head of the dataset."" The error coming is because of this piece of code. df_2 = df.sort_index('X', inplace = False) Try the below code: df_2 = df.set_index('X', inplace = False) Hope this will help you.",317991.0
89965,374680.0,"You can go through below links that gave nice explanation for ""Pandas commands ending with parentheses and those that do not"" https://stackoverflow.com/questions/27980843/python-pandas-functions-with-and-without-parentheses https://www.ritchieng.com/pandas-parentheses/ https://python-forum.io/Thread-What-is-the-difference-between-parenthesis-and-no-parenthesis-in-head-and-tail Hope this will help.",317991.0
89965,374605.0,"Every class has some members. These members can be function or properties. df.describe() -&gt; This is a function and so parenthesis. df.column -&gt; Its a property and so no parenthesis. Example. If you create a Car, you might want to give a name to car and you will create a name property. To access that you might want to create a car which describes the car. For Example. car.name = ""BMW"". car.describe() or car.getName(); Hope this is clear.",318368.0
89961,374570.0,"You need to specify the parameter inplace=True so as to modify the dataframe on which you are working. By default this parameter is false and hence the dataframe is not modified. df.column.fillna('test' , inplace=True ) Checkout the below link for more details: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html#pandas-dataframe-fillna",313826.0
89961,374574.0,Thanks Vinay.,312491.0
89961,374608.0,"By default DF function will not change you original DF, so that you can work on it multiple time till you are not satifistied with result. Once you are satisfied, there are two ways, 1) use inplace = True df.column.fillna('column_name' , inplace=True ) 2) Assign column back to origin column. df['column'] = df.column.fillna('column_name' )",318368.0
89972,374651.0,Instead of passing wind and rain in column you should pass it in values argument as you want to show mean of these columns grouped by month and day. For more info on arguments in pivot_table refer http://pbpython.com/pandas-pivot-table-explained.html,317689.0
89972,374638.0,"Hello Nagaraju, Use square bracket when creating list. Please try below one. df_1 = df.pivot_table(values = ['rain','wind'] , index = ['month','day'] , aggfunc = 'mean') print(df_1.head(20))",320195.0
89972,374695.0,"Syntax for pivot_table is pd.pivot_table(df,index=[""Manager"",""Rep""],values=[""Price""], columns=[""Product""],aggfunc=[np.sum]) you should be passing rain and wind as values instead of columns.",318368.0
89972,376932.0,"first apply group by and then select requred column from dataframe: df_1 = df.groupby(['month','day']).mean()[['rain','wind']]",317577.0
89972,374755.0,"In above code just modify columns=('wind','rain') to values=['wind','rain'] / values=('wind','rain') both will work. Below is the screenshot of both:",317991.0
89042,369166.0,"Please sort the data frame using sort_values method for dataframe. There is sample examples present in Pandas Basic Jupiter Notebook Store this output in a seperate dataframe and apply sort_values function for eg: print(comb.sort_values( by = 'Medals', ascending = False)) where comb is the new merged data frame. Hope this helps you",308635.0
89042,369259.0,"You have to apply sort function. e.g: print(total.sort_values(by=['Medal'],ascending=False)) total is the merged data frame of three given data frame.",310419.0
89042,369345.0,"Thanks, both of you . It worked.",314183.0
89042,373866.0,"Here is what i did, 1.set the index as Country 2.add the columns and sorted values in one step.. gold.set_index('Country',inplace=True) silver.set_index('Country',inplace=True) bronze.set_index('Country',inplace=True) print(gold.add(silver.add(bronze,fill_value=0),fill_value=0).sort_values(by='Medals',ascending=False)) Hope it helps :)",305129.0
89044,369403.0,"If you do not want to edit, keep the same dir/folder structure as provided in the example Notebooks. It will not throw any error. But if you wist to have your own dir structure, then the Notebooks needs to be edited. Thanks",310482.0
89044,369602.0,"The file is in downloaded in same dir/folder structure. SInce its a zip file, when unzipped thats the file path, stil it doesnt read on given format. Regards",308495.0
89046,369313.0,"I have not tried but there is one way you can try but this will work i.e to use merge inside merge and when there is common condition then this is very good to use enve if is different it will work. df_1 = pd.merge(pd.merge(x_df,y_df),how = 'inner' , on cust_id),how_'inner','on_prod_id') Hope it helps!!",307843.0
89046,369301.0,"Yes, you can do it by using the reduce function on a list of dataframes. For eg, dfs= [df0,df1,df2,df3] df_final= reduce(lambda left,right : pd.merge(left,right,on=’name’),dfs) (Assuming the column ‘name’ to be common) similarly, you can modify it further to incorporate varying how and in Hope that helps.",317998.0
89046,369605.0,Thanks for the reply will try both ways regards,308495.0
89836,373445.0,"The filed Ord_id is a column which contains strings like ""Ord_1001"", ""Ord_1010"" etc. So, when you set index of the dataframe on column with string(dtype=object) the sorting will be done in lexicographical order meaning alphabetical order.",313826.0
89836,373439.0,"sort_index will sort the index by it natural order. If its number, data will be sorted by numerical order. If its string, it will be sorted by string sorting. In your case index is string and so its sorting based on string order.",318368.0
89836,373467.0,Here the Ord_id is a string and it is getting sorted as we look for words in a dictionary that is a lexicographically. So Ord_101 will be come after Ord_1009,318358.0
89836,373798.0,"Here index column is object data type , you can check the data types of columns with ( df.dtypes) . Note : Object data type is string or character in python pandas world. String data type index column can be sorted on numerical order ( if the column has string+numbers ) using split method (both creating new integer column or sorting on the same index column).",305652.0
89836,373885.0,"Sorting in string is different than sorting numbers. Here the field Ord_id is a string. So, in string sorting Ord_1001 will come before Ord_101. So, if you want to do sorting on ord than probably you will have to split the Ord_id in string and number part and do sorting on the number part.",317689.0
89132,369577.0,Set the index to 'Country' for all the 3 data frames using set_index method for data frames. Then merge the data frames into a separate data frame Then sort the output data frame using sort_values method for data frames Hope this helps,308635.0
89132,370050.0,"You can set the index to country and then concatenate the dataframes provided the column numbers &amp; names are same.. in that way, it will be easy to add and get total numbers of respective kind of medals / total medals. But if we merge, we will have 9 columns of medals (incase of a few same countries are present in all DFs). Then it will get clumsy to get the count of different medals of each country / total medals by a country. Basically the coding will expand.",307488.0
89132,373865.0,"Here is what i did, 1.set the index as Country 2.add the columns and sorted values in one step.. gold.set_index('Country',inplace=True) silver.set_index('Country',inplace=True) bronze.set_index('Country',inplace=True) print(gold.add(silver.add(bronze,fill_value=0),fill_value=0).sort_values(by='Medals',ascending=False)) Hope it helps :)",305129.0
89025,369058.0,Try--&gt; series_1.apply(lambda x:x**2),314547.0
89025,369062.0,"Also when you have to square, you give lambda x:x**2 and not lambda x:x*x",314547.0
89025,369088.0,"When I run your code, I am getting the following error: "" TypeError: can't multiply sequence by non-int of type 'str' "" . You are trying to square the non int type data. If you want the string '1' to be displayed a '11' or '2' as '22', you can use ""x+x"" in the lambda function. Whereas when I am changing the datatype to say int, I am able to get the desired output. FYI - x*x and x**2 gave me the same output. code1: import numpy as np import pandas as pd series_1 = pd.Series([1,2,3,4]) series_2 = series_1.apply(lambda x : x*x) print(series_1) print(series_2) o/p1: 0 1 1 2 2 3 3 4 dtype: int64 0 1 1 4 2 9 3 16 dtype: int64 code2: import numpy as np import pandas as pd series_1 = pd.Series([1,2,3,4]) series_2 = series_1.apply(lambda x : x**2) print(series_1) print(series_2) o/p2: 0 1 1 2 2 3 3 4 dtype: int64 0 1 1 4 2 9 3 16 dtype: int64 Hope this helps.",307488.0
89025,369095.0,"series_1 = pd.Series(['1','2','3','4']) Your series contains string put int value to it. series_2 = series_1 .apply(lambda x : x*x) print(series_1)",318476.0
89025,369494.0,"you are passing series_1 = pd.Series(['1','2','3','4']) Here inputs are strings not integer. Try using following instead: series_1 = pd.Series([1,2,3,4])",317689.0
89025,376808.0,"you are using character '1' instead of number 1 in your list, please refer as below : series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x:x**2) print(series_1) print(series_2)",317577.0
89025,370122.0,"Hi Bindu, 1 . Create a list of squares of numbers from 1 to n by anyone of below ways : a. Use np Array --- &gt; np.array(1, n+1)**2 ---&gt; n+1 because last value is not included. OR b. list [ x **2 for x in range(1,n+1) ] -- &gt; All squares list from 1 to n 2. Now Use the 2nd argument of the panda series which allows to provide custom indexing . We need index from 1 to n, so index = range(1,n+1) ---- &gt; Again since last no is not included So finally the series will be : x = pd.Series(np.array(range(1,n+1))**2, index = range(1,n+1))",315028.0
89138,369570.0,You need to import pandas package first in current cell if it is not added and executed to above cell. Please add import pandas as pd before your code and execute it. Let me know if problem persist.,317991.0
89138,369573.0,"Firstly, install pandas from anaconda command prompt using ""conda install pandas"" Then in Jupyter Notebook, import pandas as pd And finally your command should work",301555.0
89138,369579.0,"if you have nor imported Pandas please do it first. If you have, the error say ""Pd not defined"". May it is a case-sensitive issue and you imported pandas as 'pd' instead of 'Pd'.",319898.0
89138,369584.0,"If you have imported pandas as import pandas as pd So, this ailas pd is case sensitive. if you write Pd it will throw error.",317689.0
89138,369934.0,"first, ensure that Pandas library is installed on your system. second, make sure you import is correctly by - import pandas as pd - where pd is an alias.",308962.0
89138,370619.0,Same is happening for Numpy np.,300690.0
89140,369601.0,"inplace True modifies the dataframe itself and does not return anything. inplace False (Default behavior) will create a copy of the object and apply the intended operation and returns the new dataframe. So, inplace True will make you loose the original dataframe incase you need that in future. But on the flip side if working with large dataframe, inplace True would not consume more memory and is memory efficient. You can check the answer here: https://stackoverflow.com/questions/43893457/python-pandas-understanding-inplace-true",318554.0
89140,369610.0,"The inplace keyword determines whether this assignment will performed on the original DataFrame or return a copy with the new column. df = pd.DataFrame(dict(a=range(5), b=range(5, 10))) df.eval('c = a + b', inplace=True) print(df) # Original DataFrame is changed as per the arithmetic operation Output: a b c 0 0 5 5 1 1 6 7 2 2 7 9 3 3 8 11 4 4 9 13 When inplace is set to False, a copy of the DataFrame with the new or modified columns is returned and the original frame is unchanged. df.eval('e = a - c', inplace=False) print(df) # The original DataFrame is not changed Output: a b c 0 0 5 5 1 1 6 7 2 2 7 9 3 3 8 11 4 4 9 13 Hope this helps",318804.0
89140,369654.0,"Let's say you have a df df1. you do following: df2 = df1.apply(lambda x: x**2, inplace = False) than df2 is created as a new dataframe and df1 remains intact. but if you do inplace = True the original dataframe df1 is modified and df2 would be a reference to that modified dataframe. https://stackoverflow.com/questions/43893457/python-pandas-understanding-inplace-true",317689.0
89140,369985.0,Your question is answared in FAQ. Please check FAQs at bottom of the page.,311952.0
89140,372402.0,"Setting the 'inplace' argument to 'True' updates the dataframe inplace, i.e. you don't need to assign it to another variable. The change will permanently reflect on the dataframe it is applied to. Whereas, is you specify the 'inplace' argument to 'False', the change is temporary, i.e. the dataframe is not updated.",318429.0
89154,369811.0,"I had used the below code for creating the pivot table df_1 = df.pivot_table(values = ['rain','wind'],index = ['month','day'],aggfunc= 'mean') Hope this helps",308635.0
89113,369497.0,"When you are doing group by then within each group you can use aggregate function like sum, max, avg etc. So you can get avg(rain) and avg(wind) within a group separately. From SQL side: Select avg(col_a), avg(col_b), col_c, col_d from table tbl group by col_c, col_d I did not go through the Pandas till now, but looks like it can be used in situation like, Where you want to find avg rainfall and avg windspeed in a particular day or month over the years of data.",318554.0
89113,369523.0,"Try like this: df.groupby(['month', 'day']).agg({'rain': ""mean"",'network_type': ""mean""})",317689.0
89113,369697.0,"Even shorter way: dfg=df.groupby(['month','day']) df_1 = dfg['rain','wind'].mean() print(df_1)",318433.0
89113,370168.0,you can pass both the columns to group by and the attributes whose mean you want to find out both as a list. example:- df_1=df.groupby([column1.column2]) df_mean=df_1['attribute_!'.'attrinute_2'].mean() df_mean will be a dataframe containing the mean values for attribute_1 and attribute_2 grouped by column_1 and column_2,318374.0
89113,370333.0,"Alternatively you can use df.groupby(['month','day'])['rain','wind'].mean()",317689.0
89113,371806.0,"I broke it into 2 parts for ease of understanding df_1 = df.groupby(['month', 'day']) df_2 = df_1['rain','wind'].mean() print(df_2.head(20))",308962.0
89113,376931.0,"first apply group by and then select requred column from dataframe: df_1 = df.groupby(['month','day']).mean()[['rain','wind']]",317577.0
89628,372007.0,"Here first 'month' is sorted in ascending order and on the basis of that 'day' is sorted in ascending order. To ur question follow the below syntax:- df.sort(['A', 'B'], ascending=[1, 0]) 1 = True",318358.0
89628,372389.0,"As ""by"" property accepts array of columns. ""ascending"" property also accepts array/list of boolean to sort on columns. df.sort_values(by=[""month"",""day""],ascending=[True,False])",318368.0
89628,373429.0,"Hi Prateek, df.sort_values(by=['month', 'day'], ascending = True) one more solution",306242.0
89178,369741.0,In the path referring to csv file try using single '\' backslash instead of double '\\' backslash,318358.0
89178,369746.0,"The problem lies in read_csv(""C:\\Users\\user\AppData\\Local\\Programs\\Python\\Python37"") You used double backslash '\\'. Use single backslash '\' and try executeing it.",317991.0
89178,369981.0,"Does you csv file name is Python37 read_csv(""C:\\Users\\user\AppData\\Local\\Programs\\Python\\Python37"") Please give the full filepath with file name. I think you missed giving the filename as there is no filename with extension CSV in your code.",311952.0
89178,370230.0,"Use Single back slash(""\"" ) in your file name. And also, add ""r""before the path. E:g - market_df = pd.read_csv(r""C:\Users\kumarsh\Desktop\market_fact.csv"") What ""r"" does is that it converts the normal string to raw string. I too faced this issue and it was fine when i added ""r"" before the file name. Hope this helps.",316202.0
89178,370429.0,"write the code like this:- pd.read_csv(r""C:/ABC/Path/TO/FILE.csv"") And it will work.",301555.0
90836,378721.0,Numpy - http://www.labri.fr/perso/nrougier/from-python-to-numpy/ http://www.labri.fr/perso/nrougier/teaching/numpy/numpy.html Pandas - http://tomaugspurger.github.io/modern-1-intro.html,318368.0
90836,378602.0,"https://stackoverflow.com https://www.tutorialspoint.com/ you can find your answers on these sites. I suggest you directly type in google for a particular command or process in python, you will get tonns of options of how to implement it",318009.0
89633,372037.0,You should print Total_Medals.sort_values function and sort the values by column 'Medals' only,318358.0
89633,372076.0,I think I got the solution. I did this through Merge hence the problem in Column labelling. Now it is sorted out. Thanks.,300718.0
89633,372139.0,Set the index of all the three dataframes to the column 'Country' Add the three dataframes. Ensure to use the fill_values argument to avoid the NaN values. Sort the combined dataframe by medals column in descending order. Hope this helps.,313826.0
89633,372192.0,"You can remove the defalt index and set the index to a specific column. Currently you have index as default (0,1,2...). You can change it with any column with below syntax. df.set_index(""column_name"");",318368.0
90372,376750.0,"data_frame1=df.sort_values(by=['day','month'],ascending = True) data_frame1",318017.0
90372,376868.0,"df_2 = df.sort_values(by=['month','day'])",317577.0
90372,376870.0,"You can sort values using below notation if you have month and day column. df=df.sort_values(by=['day','month'],ascending = True) If you dont have day and month column you can create two column or split the column or convering string to datetime object then sort the values with date column. df['Date'] =pd.to_datetime(df.Date) df = df.sort_values(by=['Date'],ascending = True)",318368.0
90372,376925.0,ok.. thanks all,315757.0
90375,376789.0,"Directly putting the question and asking for an answer is not a good way to take the discussion forward. Please put forth the attempt(s) that you have made to resolve the issue or the error you are getting , so that your peers can try to help you out.",313826.0
90375,376841.0,You can use labmbda function with apply to mulitple column and create a new column in the dataframe.,318368.0
90375,376879.0,you can achive it by : df['XY'] = df['X']*df['Y'],317577.0
90375,378272.0,"import pandas as pd df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') def mult(x,y): return x*y df['XY'] = df[['X', 'Y']].apply(mult, axis=1) print(df.head(20)) Why is this not working?",318751.0
89803,373169.0,"Hi Lalitha, As per the statement you have to Create a series using list = [6,7,8,9,2,3,4,5]. series_1 = pd.Series([6,7,8,9,2,3,4,5])",320195.0
89803,373182.0,"You need to create list first which you are using in the statement. You can not use any variable which is not declared/defined. You can define the list like below list = [1,3,4,5,67,8]",318368.0
89803,373226.0,"You need to create a series using list_1 = [6,7,8,9,2,3,4,5]. series_1 = pd.Series([6,7,8,9,2,3,4,5]) Donot assign anything to variable list as it is a python function",318358.0
89803,373399.0,"Hi Lalitha, l1= [6,7,8,9,2,3,4,5] print(list(map(lambda a:a*2,l1))) try with this, hope it will work The lambda function is multiplying by 2. Need to use ** for exponentiation.",306242.0
89803,373400.0,"there are many ways of doing ,Use list comprehension as shown below list1 = [6,7,8,9,2,3,4,5] print([x**2 for x in l1])",306242.0
89803,374380.0,There is no input in the program. So you need to mention the list first.,318421.0
89803,373851.0,"In the 3rd line of your code - you have used ""list"" as the list to be converted to Series, however, ""list"" has not been defined, hence python is unable to find anything to convert.",319302.0
89218,369948.0,Numpy array contains only numberical values but pandas series contains numerical and character and datetime. So scalar values of same type means collection of values with same data type.,311952.0
89218,369952.0,"Ashish - Scalar is nothing but single value. It can be number like 1,5, 3.4 etc or it can be letters like 'a', 'b', 'c'. Oppossite to scalar in programming is Vector which is collection of scalars eg. [1, 4, 9 , 16, 25] or [a, e, i, o, u]",318458.0
89218,369931.0,"A scalar is a simple single numeric value (as in 1, 2/3, 3.14, etc.), usually integer, fixed point, or float (single or double), as opposed to an array, structure, object, complex vector (real plus imaginary or magnitude plus angle components), higher dimensional vector or matrix (etc.) data type that contains more than one single numeric value.",308962.0
89218,371148.0,"Scala means a single value. So scalar values of same type means collection of values with similar data type Ex: Char Array['a','b','c'] Int Array[1,2,3]",301555.0
91924,385401.0,"it should be Series, as python is case sensitive. In jupyter notebook after typing pd press tab to get function assistance",318554.0
91924,385369.0,Series() is a case-sensitive method. Use a capital S.,319721.0
91924,385374.0,"Instead of pd. s eries, use pd. S eries small s in series needs to be replaced by capital S.",317811.0
89815,373236.0,"those are questions where there are no test cases to verify against. therefore it will remain as submitted. nothing to worry. you can refer to the FAQs also, it is mentioned there as well",317998.0
89815,373364.0,"There are two types of test cases: 1. Sample test cases - which run against the input when you click on verify. 2. Non-sample test cases - which run against the input when you click on submit. When you click on submit and it shows only ""submitted"" it means there are only sample test cases and there are no non-sample test cases. And your solution has been accepted. Its perfectly fine , no need to worry if it is giving ""submitted"" after clickng submit.",317991.0
90749,378269.0,Query related the same code is answered.Have alook maybe it will resolve the issue.What is the error are you get is the expected output not matching? https://learn.upgrad.com/v/course/208/question/90459/answer/377293,300687.0
90749,378308.0,"Please note that you need to explicitly print using the print() function in the coding console to display the results to the standard output , so that it can be validated against the expected output.",313826.0
89656,372173.0,"You need to assign sorted df back to your original df. By defult sorting is not inplace it will create a new df. Use below syntax Final = Final.sort_values(by='Medals', ascending = False) print(Final)",318368.0
89656,372180.0,"U need to assign this to a Final variable as sort_values doesnot change the data frame internally, so we need to explicitly assign it to Final variable and then print this variable",318358.0
89656,372234.0,"Following are the ways you can print the sorted dataframe: After sorting the dataframe, assign it back to the same dataframe and then print Final = Final.sort_values(by='Medals', ascending = False) print(Final) Do an inplace sorting and then print Final.sort_values(by='Medals', ascending = False , inplace=True) print(Final) Directly print the sorted dataframe print(Final.sort_values(by='Medals', ascending = False)) Hope this helps.",313826.0
89828,373355.0,Please try to write code first which will lead you to some learning. And if you already tried then let me know the error you are facing. Asking for solution directly will take an opportunity of learning from you.,317991.0
89828,373371.0,Try using the iloc function that will help,318358.0
89828,373395.0,Try this: df[2::2].head(20),300693.0
89828,373754.0,"hello, i have done as below, df_2 = df[2::2] print(df_2.head(20)) Hope it helps:)",305129.0
89828,373363.0,Using indexing you can select only the even rows as follows: df[0::2] The general syntax of indexing is: df[row:column:setp] Here the first parameter is row which has been provided a value of zero to satrt at the zeroth row. The second parameter col has been left blank to indicate that we will to select all the columns. The third parameter step value has been set to 2 to mention that we wish to select alternate rows and hence only the even rows.,313826.0
89828,373892.0,df[2::2] starting with 2 ignoring 0 jump every 2 rows to get even rows.,306735.0
89828,373891.0,"df[row:column:step] Here you can use the step argument to define how ,many rows to jump to pick the next row. By default it is 1 i.e pick the next row.",317689.0
89828,374215.0,you can try this : as indexing in python is from 0 so df[1::2],318017.0
89828,375790.0,import pandas as pd df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') df_2 = df[2::2] print(df_2.head(20)),317811.0
89828,375179.0,dataframe_name[starting_row(even):(ending_row+1):2] Executing this will make the compiler select every alternate row until the ending row provided by the user.,310472.0
89828,376871.0,"df_2 = df.iloc[2::2], you have to start with 2 ( as 0th row excluede and step by 2)",317577.0
89829,373387.0,"df = pd.read_csv('example.csv') Please note above file should be available in the same folder where you are running the code. Also if you want to provide absolute path, thats also possible using system path of the file.",318368.0
89829,373384.0,To read a csv file use the below method: df = pd.read_csv('pandas_dataframe_importing_csv/example.csv') Give the name of the file that you want to load If you want to save dataframe into a csv file try df.to_csv(),318358.0
89829,373466.0,"While reading a file you need to have file and file location. Example as follows: Suppose you have file name companies.csv and it is kept under below location. C:\PGDDS\getting_data\companies.csv then inside read_csv() you need to specify the above location of file like as follows: df = pd.read(""C:\PGDDS\getting_data\companies.csv"") For reading text and other files sometimes you need to specify separator also as - sep=""\t"" Hope it helps.",317991.0
89829,373474.0,"Hello Surya, Please go through below link as well. You will get idea. https://learn.upgrad.com/v/course/208/question/89752",320195.0
89829,373831.0,"Hi Surya, While specifying file please don't use path with back slash ('\' ) , replace back slash with forward ('/). import pandas as pd p1 =pd.read_csv(""C:/Users/user1/Desktop/Pandas/market_fact.csv"") Back slash with cetrain characters works as escape character (example : \r , \n ,\b ,\c). or you can just add 'r' to make it as rawstring p1 =pd.read_csv(r""C:/Users/user1/Desktop/Pandas/market_fact.csv"") Hope this helps.",305652.0
89829,374804.0,"Suppose the file is stored in location= C:\Desktop\PGDDS\employee.csv df=pd.read_csv(r""C:\Desktop\PGDDS\employee.csv"")",311254.0
89829,375781.0,"import pandas as pd pd.read_csv(""CompleteFilePath"")",317811.0
89829,375991.0,"import pandas as pd variablename(To load the database) = pd.read_csv(""Path_of _the_file/FileName.csv"") This should serve your purpose.",317600.0
89275,370145.0,Can you show the screenshot of error ?,317991.0
89275,370170.0,Thanks Vipul. Its sorted out. The issue was with the backslash. Its now fine.,316202.0
89275,370182.0,"Yeah, I too faced this issue. You need to include the entire directory with backslashes.",310511.0
90778,378298.0,You can apply them in sequence as shown below: df.groupby('Customer_Segment')['Profit'].sum().sort_values(ascending=False),313826.0
90778,378302.0,"I assume you have applied the aggregation after grouping by the field that you are looking for. In continuation to the same you can call .sort_values(by=&lt;agrregated field name&gt;, ascending = &lt;False or True&gt;)",302740.0
90778,379325.0,"Hi Shalini, There is one more approach to this problem, i.e. giving a name to the column that is generated by aggregate operations. .agg({'ColumnName':'sum'}) instead of .sum() Thus, the code df.groupby('Customer_Segment')['Profit'].sum().sort_values(ascending=False) becomes df.groupby('Customer_Segment')['Profit'].agg({'ColumnName':'sum'}).sort_values('ColumnName' ,ascending=False)",317987.0
90784,378327.0,"The question reads ""Indexing Dataframes Description Print only the even numbers of rows of the dataframe 'df'. Note: Don't include the row indexed zero. "" They have put a note to exclude zeroth index, so the expected output will start from 2nd index.",313826.0
90784,378337.0,in the question itself they have specifically mentioned not to include 0th row.. that's why expected output row starts from 2.. Technically speaking you're right that 0 is even.,318495.0
90784,378334.0,even numbers are divisible by 2 and it should give interger as an output,318017.0
90022,375048.0,NaN is equivalent to null in database. It means it was not able to determine the value for that. For the 4th row there are no matching rows hence it it populating as NaN,317689.0
90022,374806.0,"Yes, if there are no values specified for those cells, you will get NaN over there. it is like a NULL value in relational database.",317998.0
90022,374816.0,"Since you are using axis=1, both the dataframes would be concatenated along the column i.e., the dataframes will be placed side-by-side. Here, df1 has 4 rows where as df2 has only 3 rows. Hence, the 4th row of df1 does not have a matching row on df2 hence we see the 'NaN' values, as any missing values during concat/merge will be represented by 'NaN'.",313826.0
89293,370232.0,Please see below hope it helps. See the last column used mask to show the date range from 01-06-2000 and 10-06-2000 using the &amp; condition and greater and less than equality you can use other also.,307843.0
89293,370291.0,Here type function is just printing the type of the dataframe created. If you want to see few elements of your data frame you can write date_series.head(),317689.0
89293,370547.0,"Hey this should help - date_series = pd.date_range(start = '11-09-2017', end = '12-12-2017') date_series print(date_series)",308962.0
89310,370328.0,"I hope you are familiar with SQL. Take example of following sql query select sum(sales),customer_segment from sales_data group by customer_segment; Here we are taking sum of sales and grouping it by customer segments. So, in order to do so using pivot_table we need to define following arguments in pivot_table. index = 'Customer_Segment' values = 'Sales' aggfunc = 'sum'. Columns basically provides an additional way to segment the data further. Revisit the notebook and lecture again for more info. Further checkout http://pbpython.com/pandas-pivot-table-explained.html",317689.0
89310,370441.0,"simple bro, index means selecting the index name simple to set_index() values mean which part you want to perform the aggfunc.",304692.0
89310,371667.0,"Always like.... On Which column we are doing aggreation function those should be in ""Values"" parameter Grouper Column should be the index ""index"" parameter agg function to use in ""aggfunc"" parameter ---If want to further rolldown the dataframe we have to use the Columns in ""Columns"" parameter Below help commands self explanatory with examples help(pd.DataFrame.pivot_table) ---used to Aggreate the dataFrame help(pd.DataFrame.pivot) ---Used to Reshape the dataframe",318846.0
89310,373856.0,This is how i have understood: index - categorical - category/categories we intend to group on values - numerical - values we want to aggregate based on the respective category columns - categorical - further filtering of category listed in the index,319302.0
89310,373064.0,"I recommend you check Pivot Tables in Excel, it'll explain the concept to you much better than any definitions. However, values : column to aggregate, where you perform your functions index : column, Grouper, array, or list of the previous. Index custom calculation gives you a picture of each value’s importance in its row and column context. aggfunc : function or list of functions, default numpy.mean Hope that helps :)",308962.0
89310,373067.0,"I recommend you check Pivot Tables in Excel, it'll explain the concept to you much better than any definitions. However, values : column to aggregate, where you perform your functions index : column, Grouper, array, or list of the previous. Index custom calculation gives you a picture of each value’s importance in its row and column context. aggfunc : function or list of functions, default numpy.mean Hope it helps :)",308962.0
90019,374791.0,All the files are in the modules itself download from there. there are many zip file once you unzip those file all the pandas related code and csv file able to pick.,307843.0
90019,374792.0,All the CSVs have been provided as part of the zip file on the session introduction page. Please scroll down on the below link to find the zip file https://learn.upgrad.com/v/course/208/session/19863/segment/101059,313826.0
90019,374802.0,Ok.. I think i missed the download part. Thanks Guys happy learning...,300708.0
89336,370409.0,Make sure the df dataframe has those columns.,310974.0
89336,370435.0,check with the columns and the data frame created or not,304692.0
89336,370446.0,Found the answer...Thankyou,318846.0
89844,373524.0,"Pandas depends upon and works on top of Numpy. If you want to work with numpy only no need to import pandas. But if you working with pandas, then you have to import numpy.",318368.0
89844,373531.0,"Both NumPy and pandas are often used together, as the pandas library relies heavily on the NumPy array for the implementation of pandas data objects and shares many of its features. In addition, pandas builds upon functionality provided by NumPy. Both libraries belong to what is known as the SciPy stack, a set of Python libraries used for scientific computing",318358.0
89844,373552.0,"Although, importing only pandas is sufficient when you are working with dataframes, more often both numpy and pandas are used for analysis. Hence, it is recommended to import both numpy and pandas.",313826.0
89844,373880.0,They are generally used together hence we are importing both. If you are not creating any arrays using numpy than you dont need to import them.,317689.0
89844,374824.0,"No, Pandas can be used independently, but sometimes some operations on pandas dataframes require numpy and hence it is always good practice to have both imported. In case, they were dependent, then while install itself pandas would have also installed numpy, which is not the case.",301555.0
90793,378359.0,Check this out https://stackoverflow.com/questions/25888207/pandas-join-dataframes-on-field-with-different-names,310974.0
90793,378383.0,"We can merge two dataframes on different column names by using the left_on/right_on parameter on the merge command. Below is the syntax. pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=True, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None) For more details refer to the link below : http://pandas.pydata.org/pandas-docs/stable/merging.html#database-style-dataframe-joining-merging",317460.0
90793,378381.0,"Suppose you have two dataframe frame_1: event_id, date, time, county_ID frame_2: countyid, state And you want to merge dataframe as: joined_dataframe: event_id, date, time, county, state Then you can use below command:- pd.merge(frame_1, frame_2, how = 'left', left_on = 'county_ID', right_on = 'countyid') Hope this will help.",317991.0
90354,376645.0,"You need to remove inplace = True. Because inplace modifes the original dataset df. You can either go with df.set_index('X' , inplace=True) print(df) Or df_2 = df.set_index('X') print(df_2)",317689.0
90354,376657.0,do not use parameter inplace=True,306248.0
90354,376881.0,"you can remove inplace=True in set_index as it modify the original dataset, intead store it in df_2",317577.0
90354,376626.0,"Problem is there in below code: df_2 = df_2 = df.set_index('X' , inplace = True) You can do it in two ways 1. Since you have mention ""inplace=True"" in set_index , then it will overwrite the actual dataframe df. So in this case you can use df.set_index('X' , inplace=True) print(df) 2. Other way is df_2 = df.set_index('X',inplace=False) print(df_2) Hope this will help.",317991.0
90440,377766.0,"how about this approach Amani: df1 = (gold.append(silver)).append(bronze).groupby(['Country']).sum().astype(float).sort_values(by = ['Medals'] , ascending = False) Feel free to post any better solution",302735.0
90440,377156.0,"The answer that you have derived is correct except for the format in which the medals tally is displayed. Looks like the person who has set this question expects that the solution be derived using the add() function using wihch the count of medals is displated as float. Following is the approach through which I resolved this problem: gold.set_index('Country',inplace=True) silver.set_index('Country',inplace=True) bronze.set_index('Country',inplace=True) total = (gold.add(silver , fill_value=0)).add(bronze , fill_value=0) print(total.sort_values(by = 'Medals', ascending = False)) Hope this helps.",313826.0
90440,377153.0,you're not defining the axis on which you want to perform the action for both concat and sort.. the other thing is you need to find the sum of medals from the grouped by dataframe.. so edit your code to include that before .sum() add column on which you want to find sum..,316349.0
90440,377160.0,"Guys, I know the following approach. gold.set_index('Country', inplace=True) silver.set_index('Country', inplace=True) bronze.set_index('Country', inplace=True) t_series=gold.add(silver, fill_value=0).add(bronze, fill_value=0) f_series=t_series.sort_values(by = (['Medals']), ascending = False) print(f_series) I would appreciate if I get the answer using group-wise sum approach.",317845.0
90440,377165.0,"You need to convert the int type to float using the astype() function. Try the beow code change in one of the lines. The rest of the code remains the same. df_add=pd.concat([gold,silver,bronze],).groupby(['Country']).sum() .astype(float)",313826.0
90440,377725.0,"Use the below code and it will work. import numpy as np import pandas as pd gold = pd.DataFrame({'Country': ['USA', 'France', 'Russia'], 'Medals': [15, 13, 9]} ) silver = pd.DataFrame({'Country': ['USA', 'Germany', 'Russia'], 'Medals': [29, 20, 16]} ) bronze = pd.DataFrame({'Country': ['France', 'USA', 'UK'], 'Medals': [40, 28, 27]} ) df_add=pd.concat([gold,silver,bronze],).groupby(['Country']).sum() .astype(float) df_final=df_add.sort_values(by = (['Medals']), ascending = False) print(df_final)",301555.0
91006,379820.0,"No, its fine for me. Might be with the connectivity issues.",316202.0
91006,379842.0,"Right-click on the question frame and select ""Reload Frame"". See if this helps in loading the question.",313826.0
91006,380155.0,Just reload the frame by right clicking on the question frame. It gets reloaded if no network issues.,301646.0
91006,382030.0,Might be connectivity issue or reload the Frame,318732.0
90795,378424.0,can you please explain what you are trying to achieve,318804.0
90795,378368.0,"Yes, it will. But you will have to pass the columns as a list in the above code. master_df[['Profit','Shipping_Cost']] = master_df[['Profit','Shipping_Cost']].apply(lambda x: round(x, 1))",301652.0
93930,395950.0,You can use this to export and can specify the delimiter suited to as per your requirement.,317689.0
93930,395999.0,"Since you asked for other way ,I searched and found that we can use tkinter module of python also. But when I go through the below link https://datatofish.com/export-dataframe-to-csv/ I found that inside that module also to_csv() is used. So we can assume this is the only way to export csv from pandas. May be others or TA's can through some light if they know any other way out.",317991.0
89348,370536.0,There is IPL example given in the notebook of this section. You can use that notebook to solve this question. Introduction to Pandas/5__Merging_Concatenating.ipynb. When you download the zip file extract all files and use jupyter notebook to access all the notebooks.,317689.0
89348,371734.0,For opening the notebook - install anaconda and jupyter. Then run 'jupyter notebook' in command prompt. You'll be able to browse your directories. Just browse to the directory where you've saved your notebooks. Open that and you'll be able to access the notebooks. For the question - try setting the index to 'Country' - then add all three Data frames with fill_value = 0. Then sort them with ascending = False. I hope that helped.,308962.0
90115,375366.0,"The error i.e NaN coming for last value coming because there is slight mistake in your code below: s = pd.Series(pd.Series(list1).apply(lambda x : x**2 ),index = list1) Just remove "" index = list1"" and your code will run fine. Hope it will help.",317991.0
90115,375295.0,"when you are doing s = pd.Series(pd.Series(list1).apply(lambda x : x**2 ),index = list1) what it is basically doing is creating a series of squares, (suppose n=4). so, it creates a series of 1,4,9,16 and their indexes are 0,1,2,3 respectively. now, when you set the index = list1. ie, 1,2,3,4 it just takes the values in the series at the position 1,2,3 and 4. since there is no value at position 4 it shows NaN. instead you could do those two steps (ie, creating the series and squaring the numbers) separately, like this: series=pd.Series((range(1,n+1)),index=range(1,n+1)) series= series.apply(lambda x: x**2) print(series) hope that helps",317998.0
90115,375384.0,"You might want to use index while creating Series not while using lambda function. Currently while applying it to lambda function you are telling program to apply lamdba function only to indexes(1,2,3,4) while indexes in your dataframe are (0,1,2,3). So you are seeing a 4 index and NAN as value. n = int(input()) import numpy as np import pandas as pd list1 =list(range(1,n+1)) s = pd.Series(pd.Series(list1, index=list1).apply(lambda x : x**2 )) print(s) Above code will first create series with indexes (1,2,3,4) itself so you have correct indexes in your Series.",318368.0
90115,375443.0,"you are using list1 =range(1,n+1) as Index but index starts from 0. Change list to list1=range(0,n+1) and code should run fine. list1 =range(0,n+1) s = pd.Series(pd.Series(list1).apply(lambda x : x**2 ),index = list1) print(s) Enter #4 0 0 1 1 2 4 3 9 4 16 dtype: int64",312033.0
90115,375708.0,"n = int(input()) import numpy as np import pandas as pd print(pd.Series(np.arange(1,n+1)**2, index= (np.arange(1,n+1)) ))",317811.0
90115,375747.0,"To use the manual indexing in your code, use index = range(1,n+1) where is n number of elements entered. Hope it helps.",301649.0
90115,376858.0,"series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x:x**2) print(series_1) print(series_2)",317577.0
89873,373975.0,"Lalitha, correct your syntax for 'groupby'; it should be 'groupby' and not 'group_by'",316349.0
89873,373848.0,"Try df_2 = df_1[('rain', 'wind')].meain()",318358.0
89873,373849.0,"You are trying to use two columns and hence you have use them as list It should be df_2 = df_1[['rain', 'wind']]",318329.0
89873,373857.0,"You might be getting 'DataFrame' object has no attribute 'group_by' Because there is no group_by method. It is groupby() Try below code: df_1 = df.groupby(['month','day']) It will work fine. Hope this helps to solve your problem.",317991.0
89873,373875.0,When you are giving multiple columns to do aggregation over pass them as a list.,317689.0
89873,373952.0,Looks fine in the the line you have pointed. Only problems I can think of is. group_by should be groupby.,318368.0
89873,374219.0,"you need to pass the index correctly like : df = df.groupby(['month','day'])",318017.0
89873,374379.0,correct the syntax of groupby it should be df.groupby() not df.group_by(),318461.0
89873,375112.0,"This code worked fine. df_1 = df.groupby(['month','day'])['rain','wind'].mean()",302744.0
90117,375299.0,"Please check whether you are printing the final sorted dataframe, as we need to explicitly print() when it come to getting the output displayed and verified on the coding console. Try the below code change : print(fa2.sort_values(by = 'Medals', ascending = False)) PS: g=gold.set_index('Country', inplace=True) is redundant as the gold dataframe itself would have the index set to 'Country'. Also, you can applythe add() function in conjunction as shown below fa2 = gold.add(silver, fill_value = 0).add(bronze, fill_value = 0)",313826.0
90117,375374.0,"There is no result as you are not printing any data, please use print to print the df. print(fa2.sort_values(by = (['Medals']), ascending = False))",318368.0
90117,375712.0,"If you are assigning dataframes to a new variable after setting index, use inplace=False.",317811.0
90117,375702.0,"s=silver.set_index('Country', inplace=True) This is incorrect. When you are assigning the dataframe to a new variable after setting the index, you should use inplace=False Correct is:- s=silver.set_index('Country', inplace=False) Another Solution: import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({'Country': ['USA', 'France', 'Russia'], 'Medals': [15, 13, 9]} ) silver = pd.DataFrame({'Country': ['USA', 'Germany', 'Russia'], 'Medals': [29, 20, 16]} ) bronze = pd.DataFrame({'Country': ['France', 'USA', 'UK'], 'Medals': [40, 28, 27]} ) gold.set_index('Country', inplace=True) silver.set_index('Country', inplace=True) bronze.set_index('Country', inplace=True) d=gold.add(silver, fill_value=0) e=d.add(bronze, fill_value=0) m=e.sort_values(by = (['Medals']), ascending = False) print(m)",317811.0
90117,375294.0,"gold.set_index('Country', inplace=True) returns an object of type= None because of the use of inplace function which will rewrite the dataframe itself. so, when you are trying to add a dataframe to ""Nonetype"", its throwing error. So, don't assign ""gold.set_index('Country', inplace=True) "" to any variable because when you use inplace then the Dataframe ""gold"" is already modified with Country as its index.",318328.0
89702,372493.0,"df.sort() got deprecated. Sorting for dataframes can be done using sort_index or sort_values As in the above example we have to sort according to Medals which is not an index use sort_values syntax : df.sort_values(by = Column , ascending = False) https://stackoverflow.com/questions/44123874/dataframe-object-has-no-attribute-sort Hope this helps",308635.0
89702,372445.0,df.sort() got deprecated. So instead of using df.sort() use df.sort_values() warning will not come.,317991.0
90268,376251.0,Use apply function on series. series_1.apply(lambda x: x**2),317689.0
90268,376151.0,"So you are using series. So when you are using Series and want to apply function on each element you need to use apply function. In your case it is not doing any modification to the series basically. Use APPLY function on the series. pd.Series(range(1,10)).apply(lambda x:x**2)",318554.0
90268,376103.0,"can you post the function, its usage and output",318329.0
90268,376112.0,is your logic correct? You can refer to check --&gt; http://zetcode.com/python/lambda/,301648.0
90268,376114.0,"functioon series_2 = (lambda x: x**2, series_1) output (&lt;function &lt;lambda&gt; at 0x7ff4e1dc2a60&gt;, 0 6 1 7 2 8 3 9 4 2 5 3 6 4 7 5 dtype: int64)",314678.0
90268,376152.0,"In question it was mentioned that ""First create the series and then using apply and lambda find the output series"". So you have to use apply() function. You can do it as follows: series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x: x**2) Hope this will help.",317991.0
90268,376440.0,"we can square the series by using lambda function along with map(). example : series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = pd.Series(map(lambda x: x**2, series_1)) print(series_2) We'll get the desired output but this is the bad way and not a good practice. Here, we are iterating through every single element of series and applying the lambda fn on it, means we're imposing looping which is performed by map function for us, but the similar functionality has been provided for series with apply() fn, that does the mapping for us and perform the specified computation iterating through the elments behind the scenes providing us complete piece of mind. like : series_2 = series_1.apply(lambda x: x**2) Better use good practices! Hope this helps! :)",317070.0
90268,376857.0,"series_1 = pd.Series([6,7,8,9,2,3,4,5]) series_2 = series_1.apply(lambda x:x**2) print(series_1) print(series_2)",317577.0
90268,376890.0,you are using apply on series so you have to uset the below format: s2 = s1.apply(lambda x:x**2),317577.0
90268,377037.0,"As you are using pandas to solve this problem. You have first create the series of element to calculate square import pandas as pd series_number=pd.Series(range(1,10)) now you can use the apply() function to square the series using lambda function series_number=series_number.apply(lambda x:x**2)",317845.0
90369,377844.0,"describe is an attribute of a dataframe object. and we use df.describe, the contents of the defined df are displayed along with the column names. When we use the method, describe() and call it, then, the mean, standard deviation, quartiles etc of the df can be displayed.",319721.0
90369,376754.0,it will give the quantitative information about your csv like how and wat kind of data it has such as mean avg and all,318017.0
90369,376848.0,Describe is a function and you should alwasy call it like. print(df.describe()) If you call it like below : print(df.describe) It will tell you that currently describe is a function of dataframe. You need to call it to describe.,318368.0
89710,372618.0,"Learn it this way... iloc stands for index-location now for your problem every true at corresponding index will fetch you the result. market_df.iloc[[True, True, False, True, True, False, True]] is equivalent to market_df.iloc[[0, 1, 3, 4, 6]] index locations 2, 5 are false and are not selected.",306248.0
89710,372617.0,"Hi Rajat, In this Example ,You can select instead of giving values of position of integer .You can also give booleans value as true &amp; false . What basically does that pull out only the row or columns corresponding the position where the position is true in the booleans array what we have passed in array. In below exmple, we are taking rows corresponding to True market_df.iloc[[True, True, False, True, True, False, True]] True : it will select rows from market_df.iloc. False: It will not select rows from market_df.iloc. True True False True,true ..... 0 Row selected T rue 1 Row selected True 2 Row not selected False 3 Row selected True I hope that i have clarified your doubts. Output: Ord_id Prod_id Ship_id Cust_id Sales Discount Order_Quantity Profit Shipping_Cost Product_Base_Margin 0 Ord_5446 Prod_16 SHP_7609 Cust_1818 136.81 0.01 23 -30.51 3.60 0.56 1 Ord_5406 Prod_13 SHP_7549 Cust_1818 42.27 0.01 13 4.56 0.93 0.54 3 Ord_5456 Prod_6 SHP_7625 Cust_1818 2337.89 0.09 43 729.34 14.30 0.37 4 Ord_5485 Prod_17 SHP_7664 Cust_1818 4233.15 0.08 35 1219.87 26.30 0.38 6 Ord_31 Prod_12 SHP_41 Cust_26 14.76 0.01 5 1.32 0.50 0.36",320195.0
89883,373939.0,inplace = True makes changes in the original df. By default its false so it you apply any changes it doesn't change your original df. And you can keep on experiementing with df. Once you are final you can use inplace to change df.,318368.0
89883,373887.0,inplace = True will make modifications in original df an inplace=False will make it temporary doesn't change the original df.,306735.0
89883,374258.0,if we give true to the inplace value then the data frame is updated according to it but if you dont give the true then it wont update the database.,318017.0
89883,374293.0,"If we mention true to the inplace value then the data frame itself is updated BUT if you dont give the true then it wont update the dataframe itself, rather it will assig the modified dataframe to a new variable.",317811.0
89883,374338.0,"market_df.set_index('ord_id',inplace=True) In this ord_id column of market_df dataframe is set as index and that will be reflect directly on the dataframe. that means there is no need to assign it to another and same dataframe. inplace=True means what ever the change that you doing on the dataframe it will update on it. inplace=False means what ever the change that you doing on the dataframe it will not update.",318461.0
89883,373874.0,"The purpose of assigning inplace = True/False is to modify the DataFrame in-place or not. Now what in-place means is whether we want to update the existing object or to create new object, If you passed inplace=False then it returns the new modified object and when you pass inplace=true None gets returned and the original object get changed, By default inplace = False, it means we don't want to modify the DataFrame inplace. For more clarification go through below links: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html https://stackoverflow.com/questions/42854598/pandas-dataframe-set-index-with-inplace-true-returns-a-nonetype-why",317991.0
89883,373871.0,inplace=True would do the change inplace i.e the original dataframe is changed. But if you don't specify it takes default value as False and a new object is returned but the original dataframe remains unchanged. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html,317689.0
89420,370975.0,"Series is a data structure with a single column whereas DataFrame has multiple columns. When you have to analyse any SQL table, Text file, PDF File or any Websites, you have to use DataFrame so that the data is loaded completely. You can't import a SQL table with multiple columns in a Series.",314547.0
89420,370970.0,"Well, you get a lot of built in functions for analysis if you have dataframe object at hand instead of a series. But I think it doesn't really make much sense if you have just one column/attribute in your dataframe as after all a dataframe is a collection of series. This can be applied to any example.",310974.0
89420,371243.0,"Dataframes and Series both share almost similar API's. Dataframes are for multidimestional/SQL type data. When you read CSV, JSON etc. those should be stored in Dataframes. Series is for one dimensional arrays. Series is used while handling individual row or column from Dataframe.",318368.0
90367,376849.0,"Redownload and import again, sometime file will not be downloaded correctly or it will have incorrect checksum values. Redownloading and importing should solve teh problem.",318368.0
89956,374528.0,"The answer that you have derived is correct except for the format in which the medals tally is displayed. Looks like the person who has set this question expects that the solution be derived using the add() function using wihch the count of medals is displated as float. Following is the approach through which I resolved this problem: gold.set_index('Country',inplace=True) silver.set_index('Country',inplace=True) bronze.set_index('Country',inplace=True) total = (gold.add(silver , fill_value=0)).add(bronze , fill_value=0) print(total.sort_values(by = 'Medals', ascending = False)) Hope this helps.",313826.0
89956,374610.0,"Hi Santosh, If you want to convert your values to float values, you can type cast the complete column like below. Final = Final['Medal'].astype(float);",318368.0
89956,374655.0,The answer needs to be in float instead of int. Please convert it from int to float.,317689.0
89718,372575.0,That was exactly what I did more than five times but it did not work. Went through entire page and watched the video lectures again to check if I missed anything. Refreshed the browser and tried again but nothing worked. But after asking question here IT WORKED. Seems like Upgrad wants me to participate more in discussion. :) Can anyone explain how did NaN got in there which is btw also part of correct output.,313515.0
89718,372564.0,df_1.append(df_2) is the correct answer. Assign this to another dataframe df_3 and print(df_3.head()) .,313826.0
89721,372625.0,"Hi, In manual indexing of a dataframe your code is used to extract rows. Incase you want to extract column values, write any of the below codes: a.column_name - output is a series a['column_name'] - output is a series a[['column']] - output is a dataframe Otherwise if you want to mention both rows and columns for extracting data, you can use iloc or loc functions. Hope this helps",310511.0
89721,372704.0,"I am getting a different output with the same data and steps and which is expected When you are accessing without loc or iloc , then index are considered as row-wise selection and strings are considered as column-wise selection. 0::2 would mean that start index is 0, select till last with increments of 2 and hence 0th, and 2nd index row would be displayed skipping the 1th row. If you would like to display 0th column, then you might have to use as following",318329.0
89620,371985.0,You need to set ascending=False for it to be descending otherwise its ascending by default the same way it is in sql .,318358.0
89620,372004.0,There is an option called report an error at the bottom of the screen in the modules. you can use that to notify the upgrad team of this error. they will update it accordingly.,317998.0
90063,375229.0,"As you have index for rows repeating, new values for id column will also repeat.",318368.0
90063,375008.0,That is because the length of the series of each of the columns is only 3 and you are trying to create a new column of length 9. That is why it is ignoring all the values apart from the first 3. Check this,310974.0
90063,376860.0,please clarify what you are looking for ?,301555.0
90732,378139.0,"I just now ran this code in my jupyter notebook, it ran perfectly fine. Below is the screenshot. Probably there is some issue in your network.",317991.0
90732,378158.0,Thank you! I tried this in the Module itself I have installed Jupyter noteboook in another laptop. Can you let me know how I can execute these in Jupyter which I have not tried so far?,301121.0
90732,379158.0,Thanks,301121.0
90732,378160.0,As you have installed Jupyter notebook it is pretty easy to execute this. 1. Firstly open jupter using Anaconda. 2. Then create new notebook bu clicking new and then clicking on Python3 (below Notebooks) as shown in screenshot. 3. Once notebook is created you will see following console: 4. Type the code and execute as follows:,317991.0
90732,380071.0,The code is getting successfully executed in time. Try reloading the page and running the code again.,306040.0
90365,376730.0,code looks good.. can you tell what exactly is the error??,316349.0
90365,376740.0,"hey Surendra, basically we're printing the complete df_1 dataset by just extracting the first 20 rows.. here, we should be assigning a right variable and printing it out.. do something like this which should give you the result as expected; df = df.groupby(['month','day']) df_1 = df['rain','wind'].mean() print(df_1.head(20))",316349.0
90365,376731.0,"Try the below code: df = df.groupby (['month','day']) df_1 = df['rain','wind'].mean () Hope this will help.",317991.0
90365,376798.0,What is the error that you are getting?,313826.0
90365,376853.0,"You need to assign your mean values (Result) to df again. df_1 = df_1['rain','wind'].mean()",318368.0
90365,376929.0,"first apply group by and then select requred column from dataframe: df_1 = df.groupby(['month','day']).mean()[['rain','wind']]",317577.0
90365,378324.0,"You are not assigning the mean value dataframe to any variable.(See the second line of the code) So your dataframe will not be altered. To alter/create the new dataframe with mean value modify the 2nd line of your code as below: df_1=df_1['rain','wind'].mean () and you'll get your answer. Other versions of the answers can be as below: 1) df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') df1=df.groupby(['month','day'])['rain','wind'].mean() print(df1.head(20)) 2) df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') df1=df.groupby(['month','day']).mean()[['rain','wind']] print(df1.head(20)) 3) df = df.groupby(['month','day']) df_1 = df['rain','wind'].mean() print(df_1.head(20)) Hope this helps :)",318495.0
90469,377360.0,"We need to write inplace = True if we want the dataframe to be updated implicitly. If you donot write inplace=True, changes will not apply to the dataframe and you need to assign the operation to dataframe",318358.0
90469,377357.0,Inplace = True is required to make the changes to the dataframe. Alternative to using inplace = True is assigning the statement you have mentioned to the same same dataframe like below: market_df = market_df.set_index('Ord_id') Hope this helps.,318084.0
90469,377404.0,"The default behaviour of set_index is to create a new dataframe object whenever a index value is changed. if this is not set to true, the index change made to the dataframe is lost. setting inplace ='True' ensures the index column for current dataframe is modified and retained. Here is the documentation for the function: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html#pandas-dataframe-set-index.",318085.0
90469,377475.0,"In Python Pandas When inplace=True is passed, the data is renamed in place (it returns nothing) orignal data frame is modified, so you'd use: df.set_index (inplace=True) When inplace=False is passed, performs the operation and returns a copy of the object, so you'd use: df = df.set_index (inplace=False)",317845.0
90469,377641.0,"Bro this flowchart might help to understand the use of inplace (T or F) if inplace is False: - Assign to a new variable ( when you are creating a new variable, you want to function to return a value, thas why FALSE, it performs the operation and returns a copy of the object ) else: - No need to assign ( When you are modifying the same df or variable, you dont want the function to return a values, thats why TRUE)",302735.0
90501,,nan,
90502,377525.0,"Sort_values : Sort_values is used when we require to sort values of a column or columns. (i.e multiple columns can be sorted at once). Order of sort can be used as an argument to the function. e.g: Order id's column values are sorted in ascending order. Sort Index : Index (either row index or column index) can be sorted. Argument used is axis. where axis = 1 sorts column lables, axis = 0 sorts row lables. e.g: in the below example, initially the data frame column lables are not sorted and after using sort index (axis=1) we can see ""Cust_id"" column has come first.",318328.0
90512,377580.0,may be because in the first case you're reading your csv file and then performing the sorting on the column month and day and that works fine.. but in the second case you're reading and performing the sorting on the mentioned columns at the same time which might be causing it to run into errors.. try to read your data first and once you've your dataframe ready you shouldn't face the same issues while running into nested/comprehended codes.. hope this helps..,316349.0
90512,377578.0,"f.sort_values(by=['month','day'], ascending=True) Here you are not refrencing this changed dataframe anywhere. And by default it is not done inplace for the original dataframe. Here you need to use inplace=True for this to work. As in above code original dataframe is not modified and in print statement you are referring to original dataframe df. So use following for the sorting to be done in the original dataframe. df.sort_values(by=['month','day'], ascending=True, inplace=True) or else write print(df.sort_values(by=['month','day'], ascending=True).head(20)) in order to see the sorted dataframe.",317689.0
90512,377667.0,"By default, df.sort_values() just sorts the dataframe df and returns a new dataframe. It does not mofidy the df dataframe on which the sort is being done. To sort and modify at the same time you need to specify the inplace=True argument. df.sort_values(by=['month','day'], ascending=True ,inplace=True )",313826.0
90512,377581.0,"When we use sort_values() , by default it has "" inplace=False"" in it. When "" inplace=False"" is passed , performs the operation and returns a copy of the object. When ""inplace=True"" is passed, the data is renamed in place (it returns nothing), i.e the changes take place in the original object itself. In first case you are taking new dataframe df_2 and assigning the sorted values in it. So it gave result. But in second case you haven't assigned sorted_values to any dataframe so it does not show intended result. You just need to put print statement to see result. print(df.sort_values(by=['month','day'], ascending=True)) Other way is put ""inplace=True"", in this case you don't need to assign the result into other dataframe. df.sort_values(by=['month','day'], ascending=True,inplace=True) df Hope this helps.",317991.0
91756,384150.0,"import pandas as pd variable_name=pd.read_csv('filename (Or) filename along with the path') Ex: DS=pd.read_csv('Student.csv') -- In this case, makesure that the Jupyter notebook and CSV file are in same folder. DS=pd.read_csv('D:\Temp\Student.csv') -- when the CSV file and jupyter notebook are in different location. Thanks.",311502.0
91756,384236.0,"Hello Shiv, Please go through below link for more information. https://learn.upgrad.com/v/course/208/question/89178",320195.0
91756,384159.0,"dataframe_df= pd.read_csv(""FullFilePathalongwithFileName.csv"") if jupyter notebook and csv file both are not in same location/path. dataframe_df= pd.read_csv(""FullFileName.csv"") if jupyter notebook and csv file both are in same location/path.",317811.0
91756,384815.0,"When you pass the path of the csv file in read_csv method, make sure to give the entire path to the csv file. If both the csv file and the ipynb file are in the same folder, you can just mention the name of the csv as the argument without giving the entire path.",314730.0
91756,384465.0,"Sample code to export data from csv into dataframe: companies = pd.read_csv(""D:\Course 1 - Introduction to Data Management\Module 3 - Python for Data Science\Introduction to Pandas\global_sales_data\companies.txt"", sep=""\t"", encoding = ""ISO-8859-1"") ""D:\Course 1 - Introduction to Data Management\Module 3 - Python for Data Science\Introduction to Pandas\global_sales_data\"" - is where my csv file is placed. The file content is delimited by tab. The the encoding used to read data is ISO-8859-1",310511.0
90576,377855.0,indeed they give the same results and they can be used interchangably as the ':' is implied in the first case,300694.0
90576,377860.0,"If you do not specify anything for the column section, by default all columns is assumed.",318329.0
90576,377904.0,if you're not specifying the columns bydefault python understands that you want the data for all the columns corresponding to the rows that you've passed.. and hence you'll get the same results with/without passing the column argument.. Note this is only when your column argument is (:) i.e. all the columns.. hope this clarifies the doubt..,316349.0
90576,378854.0,"1. Firstly, without the colon, we select all columns 2. Second, wiht the colon, we are free to choose all or some of the columns. The gives us flexibility in selecting the columns that we are interested in analysing, incontrast to having a snapshot of all columns, which is seldomly used in real-world problems.",312731.0
90584,377871.0,That's the difference between iloc and loc. iloc indexing starts from 0 and for loc since it is label based indexing it searches for the exact match in the index. This is covered already in the course.,310974.0
90584,377889.0,loc takes value upto specified argument unlike iloc or range function. loc[2:21] will take 21 as well instead of ending 1 element prior to it. in loc both the start and the stop are included. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html,317689.0
90584,377941.0,"When you use label based indexing, i.e. 'loc', pandas interprets the range passed to it as actual labels and not indices. So if you pass loc[2:21], it will take all the rows labelled from 2 to 21. Check the first FAQ on the following page to get more details: https://learn.upgrad.com/v/course/208/session/19863/segment/101061",306040.0
90584,377969.0,Thanks guys for the reponse,305650.0
90492,377472.0,suppose u want to do some complx operation then u can define a function and use it in apply,317982.0
90492,377780.0,"Apply can be used to write complex cde in one liner. You can definitely replace it with Looping and other straight code, but the complexity of writing will be rmoved using apply.",301555.0
90492,377552.0,"There will always be some cases which you won't able to get using direct operations. Example splitting a string column based on some condition. In those cases, apply () is gonna help.",318329.0
90064,375027.0,use df[column_name].astype(float),318358.0
90064,374996.0,"Use pd.to_numeric(p,downcast='float') p --&gt; pandas series that you want to convert from Int to float For converting dataframe column, use the below code: df[col] = df[col].apply(pd.to_numeric, downcast = 'float') Hope this helps.",318084.0
90064,375037.0,You can write following. assuming df is your dataframe name. it will convert it into float64. df[['Medals']] = df[['Medals']].apply(pd.to_numeric),317689.0
90064,375223.0,"You can change data type by using astype function as below df[column_name].astype(type) In above statement, type is the target data type for column_name column. That can be int, float, string etc.",318368.0
90064,375341.0,"Use following on dataframe x['Medals'] = x['Medals'].astype(float) I have checked it is working. For numeric it converts string to float or downcasts it. So, in this case it is not able to convert int to float.",317689.0
90791,378376.0,"Question states that:- ""Print only the even numbers of rows of the dataframe 'df'. Note: Don't include the row indexed zero. So in question itself they have put a note to not include the row indexed zero. That's why solution expected by upgrad console doesn't include 0. Hope this clarifies your doubt.",317991.0
90791,378851.0,"Yes Ayyappa, the question excludes the row '0' in the solution.",312731.0
90667,378121.0,Check the FAQ on the following page: https://learn.upgrad.com/v/course/208/session/19863/segment/101060,306040.0
90667,378800.0,"inplace=""True"" is used where you make changes to the dataframe itself. Wheras if inplace=False, means it returns the changed object. It is your choice to store it in a new object or overwrite the existing one. There are unlimited functions with a lot of parameters. Not possible for anyone to explain all that. We learn by experimenting and exploring on the net.",304319.0
90520,377652.0,The argument subset accepts as list of columns. Convert the result of df.columns[df.isnull().mean() &gt; .95 to a list using tolist() function and call it like this. df.dropna(subset=df.columns[df.isnull().mean() &gt; .95].tolist()) You will have to assign the result of the above code to a new dataframe or use inplace=True to modify the same dataframe. Hope this helps.,313826.0
89924,374306.0,"Since you are operating on boolean values, you should use &amp; operator in this case. Refer this for more understanding https://stackoverflow.com/questions/22646463/difference-between-and-boolean-vs-bitwise-in-python-why-difference-i",310974.0
89924,374374.0,Since we are working on boolean values we need to use bitwise operators &amp; |,318358.0
89924,375759.0,Use '&amp;' instead of 'and'..Since we are dealing with boolean values,305655.0
89339,370433.0,"Numpy is only used to manipulate the numbers only designed specifically for the numerical values, But Pandas is for both and is used to import the data from many resources mainly used for the string type data",304692.0
89339,370535.0,"NumPy is the base layer on top of which Pandas is built. But Pandas library serves a wider purpose in the sense that typical data cleansing and massaging happens using the objects in Pandas library. NumPy - Numerical Python . As the name indicates, it revolves around numeric computation Pandas - Python Data Analysis Library is superior and can be used for data analysis and scientific computing for solving a lot of data science problems. There are a lot of out of the box options available for resolving many statistical problems.",318084.0
89339,371745.0,"NumPy provides some in built functions to deal with only arithmetic operations of numeric type.When we are analyzing a large chunk of data say for example a company customer data , customer data for instance might contain heterogenous values like customer unique identification number, customer personal details,purchases of customers along with numeric figures stating profit and loss of company to whichever segment customer is computing. If we want to analyze a combination of data together we need to use numy and Pandas together.With the use Pandas in buit data frames whole customer data can be easily transformed into a table along with headers , which definitely numPy doesn't support.Manipualtions on data frames such as sorting on different columns as per our requirement and indexing with desired column helps in understanding the data in an efficient manner.",304696.0
89339,370659.0,"I found below link useful to get insight into ""NumPy vs Pandas"". Have a look into it. https://medium.com/@ericvanrees/pandas-series-objects-and-numpy-arrays-15dfe05919d7",317991.0
90870,378739.0,The input given is a LIST. Where as Series is a Pandas function. So your first task is to convert the given LIST into SERIES. Now the second task is to use the LAMBDA function on that series to create a new series giving output as its square values. Hope it is clear now,318009.0
90870,378865.0,"hello Pratik and Subarna, I understood how to solve the problem. The question was about the input list. Do we need to create a randomn list then convrt to series or use the list given in question statement ?",302735.0
90870,378755.0,You have to convert the list to a Pandas series first and also use lambda to apply the square of each number in the output series.,309451.0
89202,369884.0,According to the convince of our we can use both...,304692.0
89202,369883.0,"When you are using keyword while calling the function then it does not matter the position of the argument. Hence you can call market_df.sort_values( ascending = False, by=['Prod_id', 'Sales']) But if you do not want to use keyword then you should pass all mandatory parameters while calling the function. So, without keyword, your call will be, market_df.sort_values(['Prod_id', 'Sales'], 0, False) Below is the sort_value function syntax. DataFrame. sort_values (by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last') In your case it is giving same result both times because you are passing the mandatory argument in first place which is incidentally in the first position as per the syntax.",318554.0
89202,369869.0,"Yes, the former one is more readable than the later.",310974.0
89202,369928.0,"The format is DataFrame.sort_values (by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last') So, if you are not passing keywords than the arguments have to be in order so that python can understand what needs to be done. Else if you pass the keywords like by or inplace etc. than it can be in any format.",317689.0
89635,372044.0,Yes you can use iloc function here.Only thing is to note the difference between the two functions: iloc is used for only integers loc can be used for any datatype,318358.0
89635,372188.0,"In this case df.loc is obviously better choice as you have names for columns. df.loc[df.Sales &gt; 3000] If you go with df.iloc, you will need to put indexes of the column and rows. and you might not be aware at which position ""wind"" column is in DF. Or after merging with another DF, it might have changed the index. So better go with df.loc when you know column names.",318368.0
89635,373402.0,"Hi Nishan, You can do one thing. I tried this as an experiment. You can dfine any df.Sales&gt;3000 as a list Ex: a = list(df.Sales&gt;3000) Now use this list in iloc function as df.iloc[a] This will give you the necessary output. For 3 inputs you can crea 3 different lists a b c for area wind and temp. and put the boolean condition as df.iloc[a&amp;b&amp;c , :] try and do let me know",318009.0
90497,377511.0,"There are two things here: First, you need to print the final output dataframe Second, you can tweak the above code to add gold,silver and bronze datframes to total dataframe in a single code line rather than declaring an additional variable like Final. Here is an example​​​​​​ --&gt; c = a.add(b,fill_value=x).add( c,fill_value=x) Hope that answers your questions",318085.0
91139,381818.0,That's really nice!,319721.0
91139,381728.0,Thanks,314621.0
90779,378317.0,The column name you are passing to the set_index() function is not correct. It should be ' Country ' with capital C and not ' c ountry'. Rest of the code looks fine. Hope this helps.,313826.0
90779,378340.0,"There's a typo in your code. The logic seems perfect. The data frame which is provided is having ""Country"" (Capital C) as it's column but you are using ""country"" for indexing . Try with ""Country"" and voila you'll get your output.",318495.0
90779,378465.0,"pls note the python is case sensitive. the words Capital and Medals have been used in different cases (like lower case, Camel case etc) in the code.",311686.0
89716,372607.0,Use df.sort_values(by=['col1']) on your output pivot.,306248.0
89716,372768.0,I want it sorted like this across the margins on the total columns and rows:,310511.0
89716,373401.0,"Not sure if i understand the requirements correctly but i think following statement might work: df.sort_values(by='Total',ascending = False)",306725.0
89716,373411.0,"df.sort_values(by = ['col1','col2','col3']) Also note, iin aggregation or pivot, it willcreate groupng columns and sub-grouping columns. you need to reset the index",301555.0
89716,373551.0,"Hi Rajarshi, Here is a though, if i create a series based on condition age_group == 'Total' with booling output (Tur or Flase Values) and concatinate with the data frame as total_flag column. Then i can sort using below expression df.sort_vlaues(by=['total_flag','total'],ascending=False)",306725.0
89716,373472.0,"Hi All, Some of you have suggested the solution of sorting after pivoting, by the pivot columns or pivot margin column. df.sort_values(by='Total',ascending = False) or df.sort_values(by = ['col1','col2','col3']) will push the row Total column up when I am sorting based on Column Total. But thats not what I am expecting. I wanted to sort the rows of dataframe based on the margin Column 'Total' such that all the rows are sorted but the row 'Total' still stays at the bottom. This is possible in Pivot tables created in excel files. Please see my excel screenshot below. This will help in visualization. Hope I can make the problem clear now.",310511.0
91211,381109.0,yes absolutely - for example lamda functions can be used to return a list of booleans to be used in loc,300694.0
91211,381250.0,"The .loc[] and .iloc[] functions would work perfectly fine as long as we are providing the input to these functions in the way that they are able to understand an work upon. Take for ex. the .loc[] function , which can take boolean values as input. So, you could write a lambda function which would return a boolean value True/False based on some condition. Furthermore, you could write a custom function, which can perform a lot of checks and then return a True or False value. Please take a look at an example below:",313826.0
91211,381785.0,"Is there an option that, aggregate functions such as sum or average of the column values can be performed?",314084.0
90253,376001.0,"You have to use inplace = True in the sort statement. total1.sort_values(by='Medals', ascending=False, inplace=True)",318084.0
90253,376016.0,"You are printing total1 which gives you the unsorted dataframe. Inorder to display the sorted values, please use print(YourDataFrame.sort_values(by='Medals',ascending = False)) Or In this case, if you remove your last line 'total1', you will get the sorted dataframe displayed.",317149.0
90253,376015.0,"Following are the ways you can print the sorted dataframe: After sorting the dataframe, assign it back to the same dataframe and then print total1= total1.sort_values(by='Medals', ascending = False) print(total1) Do an inplace sorting and then print total1.sort_values(by='Medals', ascending = False , inplace=True) print(total1) Directly print the sorted dataframe print(total1.sort_values(by='Medals', ascending = False)) Hope this helps.",313826.0
90253,376247.0,"The last line of your code doesnt assign the sorted df to any variable. You have to assign it to something and then print that. gold.set_index('Country',inplace=True) silver.set_index('Country',inplace=True) bronze.set_index('Country',inplace=True) total=gold.add(silver,fill_value=0).add(bronze, fill_value=0) total=total.sort_values(by='Medals',ascending=False) total",309451.0
90253,376254.0,"Either assign it to some variable total=total.sort_values(by='Medals',ascending=False) or use inplace argument to do it in the existing dataframe and to persist it. total.sort_values(by='Medals',ascending=False, inplace='True')",317689.0
90253,376504.0,You have made a temporary change into the dataframe by not assigning inplace=True.Do that and your code will come out as right,305655.0
90253,376718.0,"In the Line of code where you are sorting, there is no inplace argument given. So by default it is false. When inplace = False, it performs the operation mentioned and returns a copy of the object. The original object on which the operation is performed remains unchanged. This is why your 'total' is not getting sorted. If you specify inplace = True, the sorted data replaces the original object and it returns nothing. The below link has some good explanation about inplace: https://stackoverflow.com/questions/43893457/python-pandas-understanding-inplace-true",301654.0
90718,378096.0,"As per my understanding, df.iloc will return the rows with indices mentioned as list only. In case you haven't explicitly indexed your rows with first (i.e. 0 index by default) as indexed 1 and so on then both loc and iloc will give same results only. Requesting TAs and others to add/correct any point if necessary.",311686.0
90718,378444.0,"loc gets rows (or columns) with particular labels from the index. iloc gets rows (or columns) at particular positions in the index (so it only takes integers). ix usually tries to behave like loc but falls back to behaving like iloc if a label is not present in the index. By default row have labels as 1,2,3 and the same index also ,by if u change the row label the row position will remain same. try running the same query after changeing the row lables links: https://stackoverflow.com/questions/31593201/pandas-iloc-vs-ix-vs-loc-explanation-how-are-they-different https://stackoverflow.com/questions/28757389/pandas-loc-vs-iloc-vs-ix-vs-at-vs-iat/47098873#47098873",317822.0
90118,375298.0,"Please check whether you are printing the final sorted dataframe, as we need to explicitly print() when it come to getting the output displayed and verified on the coding console. Try the below code change : print(Total.sort_values(by = 'Medals', ascending = False)) Hope this helps.",313826.0
90118,375342.0,"Program output wants you to explicitiy print anything which is expected to be returned as output. So, in some cases notebook might print but in solution stub you have to pass the final output to a print. print(Total.sort_values(by = 'Medals', ascending = False))",317689.0
90118,375371.0,"First you would want to do sort_values inplace or assign sorted values to anyother variable for furthur use. Then you can print the df. Total.sort_values(by = 'Medals', ascending = False,inplace=True) print(Total)",318368.0
90118,375701.0,"You did not print the last lineof your code. import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({'Country': ['USA', 'France', 'Russia'], 'Medals': [15, 13, 9]} ) silver = pd.DataFrame({'Country': ['USA', 'Germany', 'Russia'], 'Medals': [29, 20, 16]} ) bronze = pd.DataFrame({'Country': ['France', 'USA', 'UK'], 'Medals': [40, 28, 27]} ) gold.set_index('Country', inplace=True) silver.set_index('Country', inplace=True) bronze.set_index('Country', inplace=True) d=gold.add(silver, fill_value=0) e=d.add(bronze, fill_value=0) m=e.sort_values(by = (['Medals']), ascending = False) print(m)",317811.0
90915,379114.0,"under a groupby operation, you know that a function can be applied on one another column. If you have more columns where you applied same function, input these to concat function with axis=1 to create a combined dataframe against same manager_name index. concat function can take series also as inputs. The outcome of each grouped function is a series. So all these series if concantenated can get the datafram you need.",318007.0
90915,379426.0,"You can try below approach. Not sure if its efficient one but you can try. data_df = {'employee_id': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], 'employee_experience': ['1', '3', '2', '2', '4', '5', '6', '8','7', '1'], 'employee_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger'], 'manager_name': ['robert', 'charley', 'charley', 'donald', 'charley', 'robert', 'donald', 'charley', 'donald','ramesh'] } df = pd.DataFrame(data_df) df.groupby('manager_name')['employee_id'].apply(list)",318368.0
90915,379707.0,"i find this to work:: raw_data = {'employee_id': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], 'employee_experience': ['1', '3', '2', '2', '4', '5', '6', '8','7', '1'], 'employee_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger'], 'manager_name': ['robert', 'charley', 'charley', 'donald', 'charley', 'robert', 'donald', 'charley', 'donald','ramesh'] } df = pd.DataFrame(raw_data, columns = ['employee_id', 'employee_experience', 'employee_name', 'manager_name']) df['employee_experience']=df['employee_experience'].astype(float) print(""\n dataframe"") print(df) groupkey=['manager_name'] list_columns_to_analyze=list(set(df.columns.values)-set(groupkey)) grouped_by_manager=df.groupby(groupkey) l=list_columns_to_analyze print(""\n Grouped by manager_name data::"") for key,item in grouped_by_manager: print(""manager ::"",str(key)) print(grouped_by_manager.get_group(key)[l], ""\n \n "")",318005.0
90915,379956.0,My take on this. I think this is what you are trying to achieve.,313826.0
90735,378142.0,"It's not an error. You just reached the maximum number of submissions, in this case, 1. It's accepted. https://cdn.upgrad.com/UpGrad/temp/d44361c1-ac5d-46f5-a5d8-ed8a8455fd8d/Coding+Console+FAQs.pdf got through FAQs on coding console, pelase.",319721.0
90735,378144.0,"There are two types of test cases: 1. Sample test cases - which run against the input when you click on verify. 2. Non-sample test cases - which run against the input when you click on submit. When you click on submit and it shows only ""submitted"" it means there are only sample test cases and there are no non-sample test cases. And your solution has been accepted. Its perfectly fine , no need to worry if it is giving ""submitted"" after clickng submit. You can cross check this with FAQ section add the bottom of the page in below link: https://learn.upgrad.com/v/course/208/session/19861/segment/101047",317991.0
90735,378436.0,thx,315455.0
90735,378344.0,Please go through the FAQs for coding console. Note: There are few questions which don't have test cases so the go in submitted state i.e neither accepted nor rejected. So you need not worry about it. You can compare your solution with the sample solution which you get after submission to verify/compare your code,318495.0
91104,380725.0,"I can assume from the solution above that you are talking about the task in which we had to convert gross column from $ to million $. I would suggest just follow the instructions given in the task. Because in the above code you are trying to convert column with float value into string. In later stage you have to perform some manipulation on same column, at that time you would have to again remove '$' from the column. This will lead to more code and more time consumption. I would advice to follow task instruction, that would be more than sufficient. Hope it will help.",317991.0
91104,380765.0,"It might be the issue with data. Double check the data. Your code works fine for instance: def currency_millions(val): new_valstr = (str(val).replace(',','').replace('$', '')) new_value = float(new_valstr)/1000000 return ""million $ "" + str(new_value) import numpy as np import pandas as pd ipl18 = pd.DataFrame({'Team': ['SRH', 'CSK', 'KKR', 'RR', 'MI', 'RCB', 'KXIP', 'DD'], 'Matches': [14, 14, 14, 14, 14, 14, 14, 14], 'Won': [9, 9, 8, 7, 6, 6, 6, 5], 'Lost': [5, 5, 6, 7, 8, 8, 8, 9], 'Tied': [0, 0, 0, 0, 0, 0, 0, 0], 'N/R': [0, 0, 0, 0, 0, 0, 0, 0], 'Points': [18, 18, 16, 14, 12, 12, 12, 10], 'NRR': [0.284, 0.253, -0.070, -0.250, 0.317, 0.129, -0.502, -0.222], 'For': [2230, 2488, 2363, 2130, 2380, 2322, 2210, 2297], 'Against': [2193, 2433, 2425, 2141, 2282, 2383, 2259, 2304]}, index = range(1,9) ) df_3=ipl18.Points.apply(currency_millions) print(df_3) I get the below output 1 million $ 1.8e-05 2 million $ 1.8e-05 3 million $ 1.6e-05 4 million $ 1.4e-05 5 million $ 1.2e-05 6 million $ 1.2e-05 7 million $ 1.2e-05 8 million $ 1e-05 But I would say if you can do the same using just one line of code. df['columnname']=df['columnname']/ divisible",317845.0
94457,398002.0,"You can use find function which takes the string to search, start and end indices in the apply function to achieve the same.",318329.0
94457,398388.0,"You can use string replace and perform 2 sequential steps- 1. replace('0','na') 2. replace('.na','.0') # Note the dot before na and 0 This will ensure that X 2.0 is not corrupted during the replace.",318762.0
94741,,nan,
90326,376445.0,"For htack first argument should be list of arrays to be stacked. array_4=np.hstack((array_1,array_2))",318368.0
90326,376453.0,"Hi Bishnu, You can refer to help command if you encounter any issue in future for your code to check if your syntax is correct or not :-&gt; help(np.hstack) Examples -------- &gt;&gt;&gt; a = np.array((1,2,3)) &gt;&gt;&gt; b = np.array((2,3,4)) &gt;&gt;&gt; np.hstack((a,b))",318429.0
90326,376437.0,"It should be array_4 = np.hstack((array1,array2))",318804.0
90326,376542.0,"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.hstack.html hstack takes only one parameter as input. By missing the brackets, np.hstack(array_1,array_2), the error is telling you that you have passed in two input parameters. By changing the syntax to np.hstack((array_1,array_2)), it will be read a single input parameter. Hope this is helpful",317149.0
90326,376454.0,"The problem lies in below line of codes: array_4=np.hstack(array_1,array_2) array_5= np.vstack(array_3,array_4) Both hstack() and vstack() takes one parameter that can be tuple : sequence of ndarrays. But in your code you have give two parameter thats why error is coming. Just modify the above code as follows: array_4=np.hstack((array_1,array_2)) array_5= np.vstack((array_3,array_4)) Hope it will help.",317991.0
90326,376661.0,"Pass the arrays as a tuple or a list. i.e in a square bracket or normal round brackets. (array_1,array_2)",317689.0
90326,376751.0,"You need to pass the arrays as list of arrays, enclose all the arrays that you are passing in parenthesis '()' array_4=np.hstack((array_1,array_2)) This should work fine",304815.0
90326,376765.0,"I was also getting the same error until i realized that to combine 2 arrays we have to give them in brackets. So np.hstack or np.vstack takes a combined array that is just one argument while you are passing 2 arguments. So in order to fix this just enclose the passed parameters in brackets i.e np.hstack((array_1,array_2))",318495.0
90326,376806.0,"list_1 = np.array(list_1) list_2 = np.array(list_2) list_3 = np.array(list_3) # Write your code here res1 = np.hstack((list_1,list_2)) res = np.vstack((res1,list_3))",317577.0
90326,377526.0,"@bishnu agrawal Question is: Stack 1st 2 array horizontally, the result of this array should be stacked vertically with 3rd array # Write your code here array_1=np.array(list_1) #step correct array_2=np.array(list_2) #step correct array_3=np.array(list_3) #step correct array_4=np.hstack(array_1,array_2) #step correct #array_5= np.vstack(array_3,array_4)# &lt;-----Change the code here as below array_5= np.vstack(array_4,array_3)",314048.0
90326,378587.0,"The simplified code looks like this: print(np.vstack([np.hstack([list_1,list_2]),list_3])) Please correct me if i'm wrong. Cheers.",318435.0
89724,372651.0,"Hi, '/' is float division and '//' is integer division. Hope this helps.",310511.0
89724,372792.0,"// will round off the division value to floor, meaning it ignores the decimals / will give us the value with decimals Example 33/2 = 16.5 33//2 = 16 Hope it's clear now, Happy learning :)",318080.0
89724,372657.0,In Pyhton 3 division can be done in both ways '/' as well '//'. But when your using '/' you will get exact answer which can be in decimal like 5/2 will give you 2.5 where as if you will use '//' you will get an floor interger value like 5//2 will give you 2. In Pyhton 2 '//' will work same as pyhton 3 but in python 2 '/' will give you integer like 5/2 will give you 2 if you want true division in python 2 you have to import division from __future__ then you will get true division,305651.0
89724,372673.0,Division with '//' would give you the integer value ignoring the decimal part while division with '/' would give the result including the decimal part.,318329.0
89724,372748.0,/ will give a decimal part as well in the result for devision while // will give only integral part,318358.0
89724,372780.0,"// is floor division, it will give you floor integer. If the float o/p comes 2.5 then // will give us 2, if -2.5 will give -3",304813.0
89724,372795.0,"Hi, as referred to '//' it is used when we want answer in integer form removing the decimal part while case of '/' is used in float type of variable. Hope it helps :)",305129.0
89724,372895.0,"In Python , ""/' will return integer part of the divison. For example :3/2 = 1 (as integer part of 1.5 is 1) where as when 3//2 is used, it will return 1.5 (integer along with float value) Hope it helps :)",320603.0
89724,373274.0,/ -&gt; gives the floating division values (float value) // -&gt; gives the only the quotient of the divison (integer value) % -&gt; gives the remainder value (whole number) e.g. divide 7 by 2 (here 2 is divisor) 7/2 =3.5 (leaves no remainder) 7//2 = 3 (gives quotient as 2 with remainder one missed out) 7%2=1 (this is the remainder) Divison works the way as below : Dividend or actual number = quotient*divisor+remainder 7 = 3 *2 +1 or 7 = (7//2) * 2 + (7%2) Hope this will clarify you.,311729.0
89724,373453.0,"The example you have listed - the question had asked to divide each element of the array by 5, hence using ""/"" gives the actual output with decimal (float) By using ""/"", we get the output of the division including the decimal position (remainder) The way I understand the various division symbols in python: / - returns actual output including remainder, hence returns decimal output // - returns only the quotient of the division % - returns only the remainder of the division Based on what is required for a given specific problem, one or all of 3 symbols can be used to produce the results. Hope this helps.",319302.0
89724,374190.0,in python / division method return as float value and // returns a int value. for example: 13/2 = 6.5 and 13//2=6,318017.0
89724,374281.0,the diffrernce between / and // is : / is float division // is interger division for Example: 5 / 2 = 2.5 5 // 2 = 2 So based on the problem you can choose the division operator,318461.0
88828,368101.0,"The difference in speed is not due to vectorisation. The difference arises because: 1. NumPy is written in C and the C implementation does not have to go through plethora of runtime method dispatch and exception checking that a Python implementation has to go through. 2.Loop implementation in Python is inefficient. In loops, vstack is used and everytime vstack is called it completely copies all arrays passed to it. This adds to the complexity and makes python implementation slower than NumPy. Reference: https://stackoverflow.com/questions/17483042/explain-the-speed-difference-between-numpys-vectorized-function-application-vs https://hackernoon.com/speeding-up-your-code-2-vectorizing-the-loops-with-numpy-e380e939bed3 Hope that clarifies.",317998.0
88828,368560.0,"@Abhijeet, this is a special case of misunderstanding. :P First look at inverse method Your inverse() method is actually a vectorize function and not simple Python function. You are actually doing 1/ndarray which is equivalent to any other vectorize operation which is basically dividing each element of ndarray further examples of vectorize operations are ndarray * 2 ndarray + 2 ndarray ** 2 Now look at other part where you are using np.vectorize function Though you are using np.vectorize but your time complexity is increased due to use of lambda function. Lamdba function are just short hand of Python loops which comes handy to write lesser code.",306248.0
88828,371831.0,"Hi Abhijeet, if you have to compare the time of operation then compare by vectorizing the same user defined function instead of comparing it with a seperate vectorised lambda function. I have done same comparison which you did. Time taken by a vectorizing the same user defined function is lot less than a non vectorised user defined function and a vectorised lambda function.",318479.0
89602,371887.0,"Computation time for your function depends on many factors even it is running on same machine. Even if you are working on the same problem, your machine is doing lots of the thing behind and it will execute your command once it gets the resources and execution time will also depend on amount of resources provided to you for execution. Few of those factors are: 1) Number of processes running. 2) Number of threads running for each process. 3) Disk usage. Please ref to below link for more details. https://www.quora.com/What-are-important-factors-which-affect-the-running-time-of-a-program",318368.0
88851,368236.0,"You can follow the steps: 1. Create a 2*2 array from 2 lists [0,1] &amp; [1,0]. This array is now a 2*2 checkered matrix. 2. Tile function can now be used to create checkered matrix of larger size by replicating our initial matrix more times 3. For a n*n matrix, you have to replicate it n/2 times as below: n = 2 num=int(n/2) Z = np.tile( np.array([[0,1],[1,0]]), (num,num)) print(Z) To get the dimensions you can use array.shape which will return a tuple [m,n] where m is the number of rows and n is the number of columns. and you can access it by using array.shape[0] (for number of rows) array.shape[1] (for number of columns) Hope this helps.",317998.0
88851,368336.0,"Try creating an 2x2 array [[0,1],[1,0]] instead of 1x2 array of [[0,1]]. Now repeat this 2x2 array with tile function.",308962.0
88851,368364.0,"Yes, please create a 2 d array [[0,1],[1,0]] and then use tile function. x = int(n/2) a = np.array([[0,1],[1,0]]) g = np.tile(a,(x,x)) print(g)",301643.0
88851,368853.0,"import numpy as np n = int(input()) a = np.array([0, 1]) #even number b= np.array([1, 0]) #odd number nfinal = np.tile([a,b], (n//2,n//2)) print(nfinal)",304027.0
88851,368864.0,"I used the below code but getting extra dimensions a = nm.array([[0, 1],[1,0]],dtype = None,copy = True) print(nm.tile(a,(2,2)))",302741.0
88851,369006.0,"yes,you can start with 2 dimensional array [0,1][1,0] and apply tile on that for 2*2 checkboard.",319912.0
88851,369311.0,"your code is almost there but you are missing the even integer as it says we should have even integer import sys import numpy as np n = int(input()) m=int(n/2) Z = np.tile(np.array([[0,1],[1,0]]),(m,m)) print(Z)",300687.0
90438,377151.0,"That's right. If the number of elements and the first dimension are such that the second dimension cannot be derived, then we get an error.",313826.0
90438,377271.0,"Correct, the second dimension automatically adjusts when it satisfies rule (total_items % first_dimension==0), otherwise it gives an error, ie total_items should be perfectly divisible by first_dimension.",306248.0
90438,377182.0,"You are absolutely right numpy allow us to give one of new shape parameter as -1. It simply means that it is an unknown dimension and we want numpy to figure it out. And numpy will figure this by looking at the 'length of the array and remaining dimensions' and making sure it satisfies the above-mentioned criteria If you provide the first shape parameter more than the length of the array, it throws error: ValueError: cannot reshape array of size X into shape (Y,newaxis)",317845.0
90441,377187.0,"Yes, the 2nd and 3rd dimension should be the same and the 1st dimension can differ. For e.g array_1 = np.arange(16).reshape(1, 4, 4) array_2 = np.arange(48).reshape(3, 4, 4) np.vstack((array_1, array_2)) Therefore, all the input array dimensions except for the concatenation axis (i.e. 1st axis) must match exactly.",318085.0
90441,377200.0,"for your case, 'hstack' will work fine.. hstack is adding the stack array horizontally (column wise) anf hence the column number should be same.. which is there in your case.. and yes, you're correct in 3d_array, 2nd and 3rd dimension mainly decides the placement of elements.. for vstack, the row numbers should be same.. if you want to give it a try simply switch the 2nd and 3rd dimension for both the arrays.. hope it helps..:)",316349.0
90437,,nan,
88890,368474.0,np.vstack function accepts sequence of array to stack vertically. It could be a list or an array. Only requirement is to have same length for 1-D array. And same shape(Except first axis) for multi-dimentional array.,318368.0
88890,368437.0,both are working for me as well.,317982.0
88890,369027.0,"stacking is nothing but creating another array, so input should be list as we do while creating any array .",319912.0
89528,371551.0,"You need to create a np.array() . Try the below code and then check a.reshape. import numpy as np a=np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])",313826.0
89528,371648.0,"You need to create numpy array to be able to reshape property. Currenlty it is list. And list can not be reshaped. To convert list to array, you can use. array = np.array(list)",318368.0
89528,371537.0,"You need to put commas after every list while defining the array. like this: a=[list1,list2,list3] hope that helps",317998.0
89527,371538.0,"You need to put commas after every list while defining the array. like this: a=[list1,list2,list3] also, you need to create the array from this list, by using np.array() function hope that helps",317998.0
89527,371531.0,"The variable a is not able to read the input values. Separate each sub list with comma and try again. See like below - a=[[1,2,3], [4,5,6], [7,8,9]] f=np.vectorize(lambda x:x/(x+1)) print(f(a)) Thanks",311502.0
89527,371651.0,"You need to create numpy array to be able to reshape property. Currenlty it is list. And list can not be reshaped. To convert list to array, you can use. array = np.array(list)",318368.0
89527,371695.0,Currently a is a list of list on which you cannot apply vectorized functions. Convert a to numpy array say array. then pass f(array),317811.0
89544,371705.0,"The major difference is that np.array will make a copy of the original object and not edit it unless copy is not set to false. Meaning that the original object remains unchanged. np.asarray would reflect the changes in the original array like object itself. The difference can be demonstrated by this example: generate a matrix A = numpy . matrix ( np . ones (( 3 , 3 ))) A matrix ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ], [ 1. , 1. , 1. ]]) use numpy.array to modify A . Doesn't work because you are modifying a copy numpy . array ( A )[ 2 ]= 2 A matrix ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ], [ 1. , 1. , 1. ]]) use numpy.asarray to modify A . It worked because you are modifying A itself numpy . asarray ( A )[ 2 ]= 2 A matrix ([[ 1. , 1. , 1. ], [ 1. , 1. , 1. ], [ 2. , 2. , 2. ]]) ndarray is very much same as array where we have n-dimeension the array. An ndarray is a (usually fixed-size) multidimensional container of items of the same type and size For more informaton please refere internet also. Hope this helps!",307843.0
89544,371708.0,"numpy.ndarray() is a class, while numpy.array() is a method / function to create ndarray. Below is an example to understand it better : Create a list : a = [1,2,3] print(type(a)) &lt;class 'list'&gt; Create an array (from a list) using np.array a = np.array(a) print(type(a)) &lt;class 'numpy.ndarray'&gt; We can observe that type of the numpy array is numpy.ndarray. Now the difference between np.array and np.asarray np.array will make a copy of the original object and not edit it unless the copy is not set to false. Meaning that the original object remains unchanged. np.asarray would reflect the changes in the original array-like object itself. Their definition is also different. Function definition of an array: numpy.array(object, dtype=None, copy=True, order=None, subok=False, ndmin=0) Function definition of an asarray numpy.asarray(a, dtype=None, order=None) Below is an example : x = numpy.matrix(np.ones((2,2))) &gt;&gt;&gt;x matrix([[ 1., 1., 1.], [ 1., 1., 1.]]) use numpy.array to modify x. It will not work because you are modifying a copy &gt;&gt;&gt; numpy.array(x)[1]=2 &gt;&gt;&gt; x matrix([[ 1., 1., 1.], [ 1., 1., 1.] ]) use numpy.asarray to modify x. It worked because you are modifying x itself &gt;&gt;&gt; numpy.asarray(x)[1]=2 &gt;&gt;&gt; x matrix([[ 1., 1., 1.], [ 2., 2., 2.]]) I hope this explains the difference.",301648.0
88953,370389.0,"Hi Prativa you have to do a HSTACK using arr1 and 2 then VSTACK with previous result and arr3. VSTACK(HSTACK(ARR1,ARR2), ARR3) - Rajesh",300708.0
88953,368717.0,If we are converting list to array we donot use square brackets [] inside circular brackets () as we have explicitly defined these as lists ..it should be array_1 = np.array(list_1) and similar is for other two arrays,318358.0
88953,368760.0,"list1 , list2 and list3 are already of the type list. So you will not require the extra [ ] around them while creating an array. Logic is however correct :)",308962.0
88953,368768.0,"you are already passing list_1, so dont put extra [] around list_1. hope it helps!",304813.0
88953,369035.0,Here we must understand we are using two built-in functions of NumPy library 1. np.array 2.np.hstack/vstack. whenever you come across a function you must be careful what type and number of arguments must be passed to obtain the desired result. 1. Typically np.array takes one list as an argument. A list of list will give a multidimensional array. {In your case you are passing the list of list ...ie [list1]} 2. np.hstack/vstack takes one object an argument a list/tuple. (of course stackable elements shoud be present in list/tuple),318344.0
88953,369087.0,"Hi, When we are creating array using list we do not mention [list] as shown in array_1=np.array(list_1). Replace the similar part of code with the following: array_1=np.array(list_1) array_2=np.array(list_2) array_3=np.array(list_3)",311119.0
88953,370015.0,"After doing the horizantal stack, you got to do another step before doing the vertical stack. Hint : reshape",305843.0
88953,374109.0,"The correct would be array_1 = np.array(list_1) array_2 = np.array(list_2) array_3 = np.array(list_3) array_4 = np.hstack((array_1,array_2)) array_5 = np.vstack((array_4,array_3)) print(array_5)",318421.0
89923,374262.0,"Hello Indranil, Some of the coding questions do not have any non-sample test cases .Because of this reason, the question's final state remains at Submitted state. Checkout the FAQ Section at the bottom of below page. https://learn.upgrad.com/v/course/208/session/19861/segment/101047 Hope that helps.",320195.0
89923,374330.0,For some questions there are no non-sample test cases. Its its Submitted which means your local test cases passed and everything is fine.,318368.0
89923,374388.0,"There are two types of test cases: 1. Sample test cases - which run against the input when you click on verify. 2. Non-sample test cases - which run against the input when you click on submit. When you click on submit and it shows only ""submitted"" it means there are only sample test cases and there are no non-sample test cases. And your solution has been accepted. Its perfectly fine , no need to worry if it is giving ""submitted"" after clickng submit.",317991.0
89926,374339.0,Some questions don't have non-sample test case so' for them it is showing submitted rather than accepted. It's fine nothing to worry.,318426.0
89926,374335.0,"I also encountered the same situation, maybe it must be because they aren't graded questions, UpGrad just wants us to solve them so as to strengthen our core concepts which we've learnt",301655.0
89926,374356.0,This is in the case of questions that are not graded and doesnot involve difficult coding solutions verify is still there for verification of answer,318358.0
89926,374385.0,"There are two types of test cases: 1. Sample test cases - which run against the input when you click on verify. 2. Non-sample test cases - which run against the input when you click on submit. When you click on submit and it shows only ""submitted"" it means there are only sample test cases and there are no non-sample test cases. And your solution has been accepted. Its perfectly fine , no need to worry if it is giving ""submitted"" after clickng submit.",317991.0
89926,374625.0,Submitting. Mean no other test cases to check otherthan what is already verified. It mean your answer is correct for checked test cases and no need to worry.,318368.0
90242,375964.0,don't worry till the point you're able to proceed ahead.. you're not missing anything.. it's just the way few questions are designed looks like..,316349.0
90242,375963.0,"There are certain questions which do not have non-sample test cases to be evaluated. For such question, the status remainf 'Submitted', Please check out the FAQ on thi below link: https://learn.upgrad.com/v/course/208/session/19861/segment/101047",313826.0
90242,376022.0,"One point to add to the above answers, you could always cross verify using the 'Show Solution' link provided for each question.",317149.0
90242,375970.0,"more_horiz There are two types of test cases: 1. Sample test cases - which run against the input when you click on verify. 2. Non-sample test cases - which run against the input when you click on submit. When you click on submit and it shows only ""submitted"" it means there are only sample test cases and there are no non-sample test cases. And your solution has been accepted. Its perfectly fine , no need to worry if it is giving ""submitted"" after clickng submit. You can cross check this with FAQ section add the bottom of the page in below link: https://learn.upgrad.com/v/course/208/session/19861/segment/101047 Hope it clarifies your doubt.",317991.0
89015,369030.0,Please refer the FAQs in that section. This point is mentioned there. https://learn.upgrad.com/v/course/208/session/19861/segment/101047,314547.0
89015,369038.0,"I think when your code is passing Sample test cases, but failing in non-Sample test cases &amp; exceeds number of submissions allowed for a problem, then you will see this state. This way you can move forward in course.",318458.0
89015,369047.0,Please refer to the FAQ section : https://learn.upgrad.com/v/course/208/session/19861/segment/101047,301648.0
89015,376356.0,"Q. Even though my code was verified correctly, the code remains in the 'Submitted' state. Why isn't it getting accepted? The coding questions on the platform have two kinds of test-cases, i.e. sample and non-sample test-cases. The sample test-cases are the ones which are evaluated when you click on 'Verify' and the non-sample test-cases are the ones which are hidden and are evaluated when you click the 'Submit' button. Now, for some of the questions, there are no non-sample test cases. For such questions, if your answer has been verified correctly by clicking the 'Verify' button, your code is correct. And when you click on submit, it will just show the code in the 'Submitted' state, so don't worry. But be sure to always click on 'Submit' for all the questions irrespective to the test-cases, otherwise, your progress would be stuck.",305804.0
89015,369053.0,"No need to worry bcoz if there are no non-sample test cases available for the question, then if u submit, it will show as submitted only not accepted or rejected. Normally these non-sample test cases are verified against ur code when u click on sumbit button. so, the question you are attempting might not have non-sample test cases.Its fine if it shows as submitted.",300733.0
89082,369286.0,Repeat is function which is used for repeation call of the function whereas tile we use to reapat a stored list. so Repeat is bigger form of Tile where we can repeat the statement where as Tile used to repeat the list of array and comes from the family of numpy. Tile you have to import the numpy library. Hope it clears.,307843.0
89082,369556.0,"Hi Maya, numpy .repeat -- &gt;&gt; This is used to repeat each element in an array , the elements can be a list too. It has 3 parameters--&gt; numpy.repeat ( ArrayName, No of Repetitions , Axis along which you need repetitions ) Axis defines the direction where you need repetitions. Let's see some example for the clearcut idea : &gt;&gt;&gt; np.repeat(3, 4) ## Repeat 3 4 times along default axis Output :: ([3, 3, 3, 3]) &gt;&gt;&gt; x = np.array([[1,2],[3,4]]) &gt;&gt;&gt; np.repeat(x, 2) ## Repeat all the elements of the list twice. Output :: ([1, 1, 2, 2, 3, 3, 4, 4]) &gt;&gt;&gt; np.repeat(x, 3, axis=1) ## Repeat x 3 times along the axis 1. Check x is 2-D array([[1, 1, 1, 2, 2, 2], ## so the elements are repeated along the axis = 1. [3, 3, 3, 4, 4, 4]]) While for np.tile , the array is repeated along the axis by the no of times you want to repeat along directions. Check the image below. --Rahul",315028.0
89082,369451.0,"Both are different.Repeat function repeats the values one by one where as tile repeats the complete block of array. Example: arr=([1,2,3,4]) print(np.repeat(arr,3)) [1 1 1 2 2 2 3 3 3 4 4 4] print(np.tile(arr,3)) [1 2 3 4 1 2 3 4 1 2 3 4]",311952.0
89771,373234.0,You can refer to the below video for understanding matics multiplication. https://www.khanacademy.org/math/precalculus/precalc-matrices/multiplying-matrices-by-matrices/v/multiplying-a-matrix-by-a-matrix,311729.0
89771,373110.0,"Hi Richa, Hope following images would help understand matric multiplication: Please refer below links for more details: www.mathsisfun.com/algebra/matrix-multiplying.html Below is a link from Khan Accademy, this was also part of the Pre-Launch course. These videos would help you understand all about matrics: https://www.khanacademy.org/math/algebra-home/alg-matrices Hope this helps",306725.0
89771,373077.0,"Hi Richa , It is esentially the sum of product of coresponding on the row to the colunm (going across the rows and down the columns) Eg: Please refer this link: https://www.purplemath.com/modules/mtrxmult2.htm",317822.0
89619,372113.0,"what will be the solution for the below then? Given an even integer ‘n’, create an ‘n*n’ checkerboard matrix with the values 0 and 1, using the tile function. Format: Input: A single even integer 'n'. Output: An 'n*n' NumPy array in checkerboard format. Example: Input 1: 2 Output 1: [[0 1] [1 0]] Input 2: 4 Output 2: [[0 1 0 1] [1 0 1 0] [0 1 0 1] [1 0 1 0]]",302739.0
89619,371993.0,I donot understand why you are using a for loop here as the core idea of using numpy arrays is to not use a for loop as using it might hamper the performance of analysis.Numpy arrays has all the functions in their disposal to handle all operations related to arrays.,318358.0
89621,371980.0,Use a lambda function inside np.vectorise() and divide each element by 5. Related example can be seen in the video by the professor,318358.0
89621,372074.0,"Hello Lalitha, Please refer below link for good explanation of vectorise numpy. https://learn.upgrad.com/v/course/208/question/89427",320195.0
89777,373053.0,"Tuples and lists can also provided as inputs to hstack and vstack. &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; l1=[1,2];l2=[3,4];l3=[5,6,7,8] &gt;&gt;&gt; t1=(1,2);t2(3,4);t3=(5,6,7,8) &gt;&gt;&gt; np.vstack((np.hstack((t1,t2)),t3)) array([[1, 2, 3, 4], [5, 6, 7, 8]]) &gt;&gt;&gt; np.vstack((np.hstack((l1,l2)),l3)) array([[1, 2, 3, 4], [5, 6, 7, 8]]) The main condition to keep in mind is that, for horizontal stacking the number of rows should be the same and for vertical stacking, the number of columns should be the same. PS: Please do not write additional print() statements ( like print (a) in the above code snippet) in the coding console as this would print additional displays than expected and the test case would fail.",313826.0
89777,372962.0,"You need to give np.array() function as a input to hstack and vstack methods as these methods are used to stack arrays.Please try following the syntax from below example: Example 1 a = np.array((1,2,3)) b = np.array((2,3,4)) np.hstack((a,b)) Example 2: a = np.array([[1],[2],[3]]) b = np.array([[2],[3],[4]]) np.hstack((a,b)) Pleae ensure for hstack number of rows are same and for vstack number of columns are same",318358.0
89389,370773.0,"Comma "","" should be present between elements while declaring the list. a = [[1,2,3],[2,3,4]] Try and let me know if this resolves your issue.",318554.0
89389,370777.0,"Since you are trying to create a 2d array, you should separate each elements of the array with commas. for eg, array = np.array([L1 ,L2]) where, L1 and L2 can be your 2 lists. note how both are separeted by commas to indicate they are different elements. Further, you could also use, array=np.arange(1,10).reshape(3,3) to get the same result, since that would help you in avoiding such mistakes. hope that helps.",317998.0
89389,370792.0,"In the second row ie [38], it seems that you are creating a list. So each element needs to be separated by comma. Try : a = [[1,2,3], [4,5,6], [5,6,7]] Also after this convert 'a' to an array. Then to print it , we should provide the indices as list else it will throw an error. Provide it as a list like - a[[0,1]].",317460.0
89389,370871.0,"Just add comma between elements. Like: a = [[1,2,3],[2,3,4]]",317689.0
89389,370844.0,you are giving the input as the np.array that cant is given directly that is to be created by np.array(). The given input should be in the form of a list,304692.0
89780,372999.0,To understand np.vectorize please go through the below documentation: https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html,317991.0
89780,372990.0,It returns a single or tuple of numpy array as output. We cannot define it as f(a) as this is the syntax used to call out functions in python and we cannot assign value to f(a),318358.0
89780,373009.0,It return vectorized callable function. But you will need to assign it to regular variable and than you can call that regular variable as function. f = np.vectorize(lambda x: x/(x+1)) f(a),318368.0
89780,376512.0,np.vectorize function basically allows you to perfrom any user defined function on numpy arrays.. For example over here: f = np.vectorize(lambda x: x/(x+1)) f(a) here basically f(a) function when applied on any numpy array would give you values of each element as x/(x+1).,318427.0
89780,383154.0,question: how is it printing without any print statement? f = np.vectorize(lambda x: x/(x+1)) f(a),322683.0
89794,373105.0,"In the coding console, the verification will always be done by comparing the Solution Output (i.e., output of the solutuon provided by you) against the Expected Solution. Hence, it is necessary that you always provide the output by using print().",313826.0
89794,373147.0,"If you dont use print function. Python/Jupyter will print the out put but will not reseve it. To preserve the output, you will need to use print statement. Example 1: a,b,c=1,1,1 a b c Output of the above program will be 1 Example 1: a,b,c=1,1,1 print(a) print(b) print(c) Output of the above program will be 1 1 1",318368.0
90827,378608.0,"You are doing wrong dot product insteast you use below it will give you result y = np.dot(z, a) [[ 3.15251974e+15 -6.30503948e+15 3.15251974e+15] [-6.30503948e+15 1.26100790e+16 -6.30503948e+15] [ 3.15251974e+15 -6.30503948e+15 3.15251974e+15]]",307843.0
90827,378598.0,though i tried a lot to find the inverse of the matrix in range 1 to 10 of the form 3 by 3.. but the results are wrong in python.. i tried it manually to do the same.. the determinant is 0 for this matrix.. I'm wondering why even the inverse is calculated by python for this.. can you re verify if I'm finding it correctly? also another thing i noticed is python shows the results in float so try to change tge datatype to int and see your results will be close to what is expected but again not completely correct... the code though looks correct!! please verify and share your results.. dot product we can find in other way as well.. a.dot(ainv) aldo i tried converting the array to matrix as well!!,316349.0
90827,378648.0,Hemant was right. The determinant of this matrix is zero; so the inverse does not exist. Maybe the python function could be more user friendly and throw an error instead of returning a wrong result.,318762.0
89291,370216.0,"Please let me know the issue as u need to first create horizontal stack using hstack with same number of rows after which u create vertical stack as number of columns become same with the combining array , and then print the same",318358.0
89291,370250.0,What error or issues you are getting will you please elaborate. ?As it is difficult to judge the error on the basis of question.,317991.0
89291,370353.0,"I don't have any error ,please can any one tell me how to slove the above problem .",308432.0
89291,373201.0,"you can refer to this code later and make it shorte as per your convinience but try ourself first: a,b,c --- names of list np.hstack((a,b)) result = np.vstack((np.hstack((a,b)),c)) print(result)",315560.0
89291,370385.0,"Hello Arifa It wouldnt be proper for us to share the solution unless you attempt by yourself. Please go through the video lecture again and try it out. It is pretty straightforward. Please post here wiith the issue you are facing, we will help solve the issue you are facing. - Rajesh",300708.0
89333,370463.0,Makes sense now. Thanks Ram.,300708.0
89333,370424.0,"Yes, even I observed the same. The definition of the function says it accepts sequence of ndarrays though:",310974.0
89333,373127.0,"Sorry, i have not understood the above, please can you elaborate more in a simple language.. Thanks",305129.0
91581,383379.0,"In an interactive python notebook, just calling this will print it.",318084.0
91581,383383.0,"Yes, Jupyter notebook displays the output in its own structured format (according to its own interpretation) even without using the print statement. However, you need to use the print function in the upgrad console to display the output. this was even mentioned in one of the session modules. go through that for more insight. hope that helps.",317998.0
89427,375985.0,Thank you for the detailed explanation,310508.0
89427,371029.0,"No, it is not necessary to use lambda function with np.vectorize. First let's understand what vectorize does. Suppose you have a 2d array [[1,2,3],[4,5,6]]. Now, lets say you want to square each of the elements of this 2d array. You can do this in 2 ways. 1. Use a for loop to iterate over each element, take that element square it and then store its result. Note, this will be done sequentially, ie, till the time the operation on the first element is not complete, the computation for the second element wont start . Similarly, 3rd would start only after the completion of the 2nd. and so on. Now, imagine you had millions of elements (data) in the array. how time consuming this method would be!! What is the solution? 2. Vectorize . ie, do all the computation parallely. meaning, the 1st,2nd,3rd...nth all elements would be operated on simultaneously. Note: you can do this for operations where the computation of the elements are independent of each other. like in our above example of squaring, the output of any element does not depend upon the output of any other element. ie, they are all independent , and thus can be computed parallely . in cases where the elements are dependent, you will have to go sequentially (this 1st methond described above). Now, in numpy, all operations on arrays are done vectorally. (2nd method above) so, for eg, if you have 2 arrays A1 and A2. and you do something like A1+A2 then, each of the elements of A1 will be added to each of the elements of A2. This will be done parallely! Not sequentially! imagine the time this would save if you are operating on Huge Chunk of data. same is the case for other operations such as, A1*A2 or, A1*5 or, (A1)/5 etc. Now, How to use vectorize without lambda: You can define your own function: lets say, an sq function which will return the square of the parameter passed to it. def sq(x): return x**2 Now, you can vectorize this function like this: y=np.vectorize(sq) Now , when you do, array_1=np.array([[1,2,3],[4,5,6]]) print(y(array_1)) This will compute the square of each of the elements of the array vectorally, ie parallely, since you told python to vectorize this function sq. (by doing np.vectorize(sq)) Similarly, you can define your own custom functions and vectorize them and use it. Using lambda is not necessary. The only caveat is that the operations on every element should be independent. Hope that helps.",317998.0
91553,383170.0,"you dont have to deifne it. it is taking the input from the input stub. there would be a tab called input. you can see there..over there the 2 lists would have been mentioned. so, list_1=input_list[0] takes the 1st list mentioned there and list_2= input_list[1] takes the 2nd. hope that helps.",317998.0
91553,383169.0,list_1 and list_2 is pulled from the input_list.. check the first five lines of code..,316349.0
89374,370706.0,"You can consider Matrix operation as linear transformation of the vector space. Say you have a rubber sheet(n-dim space) and you are rotating or stretching it uniformly (transformation) Think linear transformation, not twisting(ie torsion). eg You can stretch a jpeg image or rotate a jpeg image. Now when you are applying this transformation basically you are multiplying with the matrix. Now when doing this transformation if it does not change the image in any one of the axis then that axis is called eigen vector. But it can stretch, that stretch amount is eigen value. So if you are rotating an image by say 45 degree, then each small pixel is rotated by 45 degree, and no eigen value or vector is present there. But say you are stretching it at 45 degree, then each square pixel will become rhombus, but X axis would not change only Y axis will be stretched/squeeshed at 45 degree based on angle. So each square unit pixel will be a parellolegram but X axis would not change. So X axis in this case will be eigen vector and eigen value is 1 cause on X axis it is not stretched. https://qph.fs.quoracdn.net/main-qimg-0c62a15bff0c8fae7cd00ae6b7fe988f.webp",318554.0
89374,370735.0,"Its a bit longer concept. You need to read it patiently over internet, but basic concept is as follows. Definetion: There is a vector A and you perform some transoframtion T such that the vector is scaled by factor λ(lambda). ie. T(A) = λ.A All such vectors A are called Eigen Vectors and the scaling factor λ is called its Eigen Value Link Now solving for Eigen Vector &amp; Eigen Value (Remember a Physics vector can always be respresented as mathematical matrix, so the word Vector &amp; Matrix can be used interchangeably). Eigen vectors also holds the equation Det( λ I - A ) = 0 true. Link where - Det is the Determinant of the matrix - I is the Identity matrix - λ is the scalar value.",306248.0
89343,370511.0,The following links give insight into the basic matrix and linear algebra operations: https://www.python-course.eu/matrix_arithmetic.php https://web.stanford.edu/class/cs231a/section/section1.pdf https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html,313691.0
89343,370509.0,"&gt;&gt;&gt; a = np.arange(1,10).reshape(3,3) &gt;&gt;&gt; a array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &gt;&gt;&gt; np.linalg.inv(a) array([[-4.50359963e+15, 9.00719925e+15, -4.50359963e+15], [ 9.00719925e+15, -1.80143985e+16, 9.00719925e+15], [-4.50359963e+15, 9.00719925e+15, -4.50359963e+15]]) &gt;&gt;&gt; np.linalg.det(a) 6.66133814775094e-16 Though the value is coming actually the determinat is very small, the exact value of determinat is 0 when you do by hand, which means it is a singular matrix. It may come because numpy implemenets numerical methods in this case LU factorization as per the documentation. And while storing it there is floating point conversion, which might give error. But looks like it is happenning in some systems. In my system I did not find this error as shown above.",318554.0
89343,370501.0,"Matrices are Invertible only if has below few properties - *A is invertible, i.e. A has an inverse, is nonsingular, or is nondegenerate. *A is row-equivalent to the n-by-n identity matrix In. *A is column-equivalent to the n-by-n identity matrix In. *A has n pivot positions. *det A ≠ 0. In general, a square matrix over a commutative ring is invertible if and only if its determinant is a unit in that ring. *A has full rank; that is, rank A = n. *The equation Ax = 0 has only the trivial solution x = 0. *Null A = {0}. *The equation Ax = b has exactly one solution for each b in Kn. *The columns of A are linearly independent. A square matrix is singular if and only if its determinant is 0, hence not invertible.",300691.0
89778,372966.0,"According to the syntax of hstack and vstack functions these take numpy arrays as input, please try giving arrays as input a = np.array([[1],[2],[3]]) b = np.array([[2],[3],[4]]) np.hstack((a,b))",318358.0
89778,372969.0,"For vstack, no of cols should be same which is not the case here:",310974.0
89778,373029.0,"Prateek, in your following statement you are tryng to vertically stack list_4 and list_2: print(np.vstack((list_4,list_2))) list_4 : [1 2 3 4] [5 6 7 8] list_2: [3 4] [7 8] As for vertical stack we need to have the same number of columns, python is giving error and unable to vertically stack there two lists. I belive what you want to do is vertically stack list_4 and list_3 list_3 : [ 9 10 11 12] So the stamenet changes to as below: print(np.vstack((list_4, list_3 ))) and you will get below output: [ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12] Hope this helps.",306725.0
89778,373049.0,"np.hstack() and np.vstack() take inputs as tuples of elements. The elements themselves can be lists, tuples or nd.array type objects. For horizontal stacking, the number of rows should be the same, while for vertical stacking, the number of columns should be the same. In the example provided, you are trying to vstack two elements in which the first element list_4 has a dimension of (2,4) and the second element list_2 has a dimenstion of (2,2). Since, there is a mismatch in the number of columns, vstack() throws an error. Instead, if you try to vstack() list_4 and list_3 , the command will be executed successfully.",313826.0
89778,373122.0,"Hi, I did it like below way, import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] list_3 = input_list[2] # Import NumPy import numpy as np array_1=np.array(list_1) array_2=np.array(list_2) array_3=np.array(list_3) a=np.hstack((array_1,array_2)) #print(a) b=np.vstack((a,array_3)) print(b) hope it helps:)",305129.0
89778,376723.0,"prateek.. pl convert your lists in array first then u can use hstack or vstack.. i used following solution.. earlier i was trying array1=np.array([list_1] ie i was using square brackets but then i removed it and it worked.. list_1 is already a list we are not suppose to use [] again. import numpy as np array1=np.array(list_1) array2=np.array(list_2) array3=np.array(list_3) harray=np.hstack((array1,array2)) varray=np.vstack((harray,array3)) print(varray)",318791.0
91550,383236.0,"array_1 = np.array([list_1, list_2, list_3]) DONT USE HARDCODED VALUES FOR INPUT LISTS.",317811.0
91550,383136.0,"that is beacuse you have hardcoded your input array. it will always have the same values in the 3*3 matrix and thus will not match with the expected output for different set of inputs. so, you should use the input lists to create your array. like this: array_1= np.array([list1,list2,list3]) hope that helps.",317998.0
91550,383140.0,"You are hard coding the list in the np.array() function. Instead you should use list1, list2 and list3. That way, no matter what the values of the lists are, you will get the coirrect output. array_1 = np.array([list_1, list_2, list_3]) Hope this helps.",310511.0
103721,450633.0,the difference lies in the how the code is exceuted and its resulting exection speed . please refer this article for a detailed understanding: https://hackernoon.com/speeding-up-your-code-2-vectorizing-the-loops-with-numpy-e380e939bed3,317822.0
103721,445197.0,"Yes, both of them give you the same answer. But np.vectorize() will be faster in giving out the results as compared to the lambda function. Because , all the loop processing in np.vectorize() is done with the help of a precompiled code written in C and python , so it takes much lesser time than lambda , whereas lambda is a simple python function which follows normal looping.",301655.0
103575,444357.0,"Hey subhin I think your mentor must be busy that's why he must not have called you. Anyways drop a mail on coaching@upgrad.com and keep your student mentor in CC, that you want to request for a rescheduling of your mentor call. I think this should solve the problem.",301655.0
102465,438235.0,Please drop an email to coaching@upgrad.com with your mentor in CC . I had some other issues related to call scheduling and this helped.,309211.0
102465,438228.0,I am facing the same,318329.0
102465,438810.0,Did you check if you have any proxy or firewall protection ? Try in mobile app in worst case.,317460.0
98659,420084.0,"thanks I have confirmed from mentor, we can book from 10th December.",318429.0
98659,420071.0,They will be open from Dec 10th I believe.,310974.0
98719,420393.0,"Hey Neha, You will be available to register from 10th of December, 2018. Refer the mail from Upgard regarding the same! ""A few important points to note: You can schedule your call anytime between 10th Dec 2018 to 6th Jan 2019. The calls must be completed by 6th Jan 2019, Sunday . NOTE: You can start booking calls only from 10th Dec, however you may refer the content of the module now. Please note that this is a mandatory step to access Career Services from UpGrad and IIIT-B and you need to complete this call within this schedule.""",302742.0
98719,420494.0,It is open only from 10th Dec,318804.0
98719,425058.0,It's 10th and still not open.,318344.0
98719,433389.0,Email coaching@upgrad.com and contact your mentor if you are still facing the issue.,309211.0
102624,438955.0,Check you student mentor about the same. They can help you rearrange the call.,318476.0
102624,439995.0,You can go to the mentorship platform and chat with the mentor. https://mentor.upgrad.com/home,300698.0
103877,446129.0,just to make it clear again - I HAVE contacted my Upgrad mentor and coaching@upgrad.com over a week ago but no response.,300694.0
103877,446384.0,"Hey Nitesh, Keep your student mentor in CC of your mailing thread to coaching@upgrad.com. I think this will help your student mentor to track the issue faced by you.",301655.0
102681,439036.0,"Same with me, unable to upload my resume. Please drop mail to coaching@upgrad.com keeping your mentor in cc.",314730.0
102681,439108.0,try it on the browser not on the app,318017.0
102681,439384.0,"from browser only it is possible, application is not supporting it.",320197.0
106808,461327.0,you have to give the equation and place all the coefficient and variable in the equation.,318017.0
106808,461797.0,"Hi, Please follow the attached links https://stackoverflow.com/questions/30336138/how-to-plot-a-multivariate-function-in-python https://datascience.stackexchange.com/questions/27740/plotting-multivariate-linear-regression https://www.learndatasci.com/tutorials/predicting-housing-prices-linear-regression-using-python-pandas-statsmodels/",344894.0
106897,461694.0,When you select the variable number 10 it picks up 10 variables that would affect the modelling most. Similarly it does the same when you select 15 or 20 and affect various paramters accordingly which is correct. It is good idea to take 15 to start with and later do manual modelling to arrive at the final result.,301121.0
106897,461841.0,RFE create a sub segment on the basis of number of varaibe you command it to retain. If you asked the RFE code to retain 20 varaibles it will pick top 20 correlated variable and create the model on basis of that. Even a addition or deletion of 1 variable will impact your model stats because each variable interacts with your model positively or negatively. The presence or absence of that varaible creates a unique model each time. Also the combination of different model will give you unique result. Also the sequence of addition and deletion will have an impact. even if you have taken the same varaibles.,318451.0
105789,456354.0,"low difference b/w R and R-sq means that there are no reduandant variables in the model, so ideally Yes difference b/w R and R-sq should be minimum.",304814.0
105789,456336.0,The critera is below: As part of that you should automatically get the difference adjusted to the lowest possible.,310974.0
105789,456377.0,adjusted r square is a better option to compare if you have high number of variables as it will encounter with the penalty as well so better compare adjusted r square,318017.0
105795,458092.0,"I think use EDA to manually remove insignificant columns, then RFE and mannual elimination to reach to the best result.",318344.0
105795,456488.0,"Since number of variables are going to be 100+, thus RFE will be the feasible option. Manual will take quite a time.",313767.0
105795,456479.0,"Hi Chetan, you can follow any of the approach- manual or RFE. But as per the videos, best approach is to use both RFE and manual.",300733.0
105795,457453.0,"As shown in the session, its better to use RFE first and then use manual option to eliminate some more variables.This will be the best approach I feel.",308967.0
105799,456521.0,"I think, we have to just consider them as it is for this assignment and not find actual meanings of each of them. Just refer the data and the data dictionary. I think that may be enough.",301655.0
105799,458062.0,"It's always better to gain some domain understanding of variables and values, but don't waste your time going in technicalities of everything, you can see how target variable price behaves with the types of engine which will give you fair idea what is the significance of different type of engines is with our model.",318344.0
105799,462513.0,dohc means Double OverHead Cam &amp; dohcv is same ohv means oberhead valve &amp; ohc means overhead cam,317980.0
106913,461761.0,"The correlation between those feature variables would be 1. Hence, the value of INF.",311160.0
106913,461833.0,"These VIFs tell you there is perfect collinearity: you have completely redundant variables. The very first thing you should do to address collinearity is to think about what the variables mean. Unless you are coding for more than two genders, for instance, including an indicator of male is completely redundant with an indicator of female. Therefore when we have only two category we do not split the variable in two new variables.",318451.0
105806,458081.0,"Hi Sahil, During analysis a constant is added to the training model, which is not the part of actual data. Maybe this is the reason you are getting this error.",318344.0
105806,456525.0,"When there is only one predictor variable we need to reshape the variable. Try this: X_train_lm = X_train_lm.values.reshape(-1,1) and try fitting the model.",311160.0
105806,456519.0,May be this link can help you out: https://stackoverflow.com/questions/28735344/pythonvalueerror-shapes-3-and-118-1-not-aligned-3-dim-0-118-dim-0,301655.0
105806,456989.0,"Hi Sahil, Your training data which you used for model training and x_train_lm shape is not equal. Please check the number of columns in both the data",344894.0
106918,461789.0,I would suggest first check if column is significant by checking if p value is less than 0.05 . And when all the variables are significant then check the vif for all those columns for multicollinearity. if vif is greater than 5 then drop those columns as well,320073.0
106918,461827.0,"p value act as the first filter - any thing above a p value of 0.05 should be inspected with VIF, VIF act as a valdiation on the p value if the VIF is also more than 5 than we should drop it. p value determines whether the variable is significant or not and VIF vouches that the variable we are looking is a reliable one.",318451.0
105817,456630.0,"Perform univariate analysis and see which seems significant. Then, pairplot to understand the multicollinearity.",310974.0
105817,456656.0,"We do not remove the features based on our visualization intuition alone. Just try to visualize few variables at a time, try to get a feel of them and go by coerce tuning using RFE and then manual feature dropping using p-value and VIF",318329.0
105817,456906.0,"If your issue is that you can read the pair plots because the charts are too small, just double-click on the pair plot. Individual plots become larger and you can not vertically and horizontally scroll them",305653.0
105814,456676.0,"It's working fine for me using map on fueltype column as follows df.fueltype.map({'gas': 'hello', 'diesel': 'world'})",318329.0
106930,461845.0,"Though the p-value in the summary is zero, it will not actually be zero. You could check all the p values and sort them in descending order using lm.params_ and drop the feature with high p-value when sorted. Try checking the the model fit. Also, try dropping based on VIF and check the model fit. Choose which gives the best statistics.",318329.0
106930,462115.0,"The ideal process is to check p-value first - - If p-value is less than 0.05 then the variable is significant and you need to check if VIF is &lt;5 or not. If it is high which is true in the above case i.e. VIF is 150 you should drop that variable despite having p-value = 0. - If p-value is more than 0.05 you can directly drop the column based on p-value, no need to check VIF. Hope this helps.",317991.0
106933,461859.0,"Yes, it suggests that any car which has fuelsystem as mpfi has higher market price.",318329.0
106933,463165.0,it also depends on the co-efficient - if it is negative then your correlation to price is negative,300694.0
106894,461649.0,yes it would bring all the data to one same scale which would be good for analysis and model,318017.0
106894,461842.0,https://learn.upgrad.com/v/course/208/question/106115,318451.0
105823,456625.0,"50? It should be more than that. Anyways, you have to choose which variables make business sense also before even considering them in the model.",310974.0
105823,456659.0,Try having all of them and reduce to 15-20 using RFE. Check the features after elimination and see if you would have missed those variables if you go by the initial business understanding.,318329.0
105823,458085.0,"I will suggest do EDA before jumping into models, maybe you will find some columns irrelevant to the target variable. Further try to classify variables with too many categorical variables into fewer categories. This will reduce the need for dummy variables. And you will be left with less columns, then you can use RFE for significant variables.",318344.0
105820,456632.0,"For a category which has 2 levels of data e;g - Yes and NO, you can map it to 1 and 0 resp. For the categorical variables which has more number of levels, use dummies function to convert it in to binary. Hope it helps.",316202.0
105820,456628.0,"As long as it is obvious to figure out the other value by looking at the 0 or 1, it should be fine. But, I would prefer creating 1 dummy variable and discard the main one and the 2nd dummy one.",310974.0
106932,461886.0,"This is my view point. Even if some rows are exaclly identical in all columns including price, it would not affect the analysis or modelling, But you will not able to varify your results on test data, because the same may be the part of train data as well as of test data. So it might improve test results . So always drop duplicate",301121.0
106938,461914.0,Drop the columns based on p-value followed by VIF,318329.0
106938,462037.0,Drop columns keeping p-values. but treat the NANs 1st.,311117.0
106938,461915.0,Start with dropping one column at a time which has vif value inf and then again fit the model and calculate the vif...and perform this process again and again until all the vif and p values are in optimal range... i would also suggest to check the data type of columns ...for nan value... all the columns should have numerical data type for vif calculation.,320073.0
106938,462059.0,"infinity columns (VIF) have to be dropped - but drop one by one and re-calc summary, VIF etc before dropping another one as each time you drop a column it changes the reln/value of all other columns in the new model",300694.0
106938,462289.0,TA Varified https://learn.upgrad.com/v/course/208/question/106881,312376.0
106947,462128.0,The adjusted R squared increases only if the new term improves the model more than would be expected by chance and it can also decrease with poor quality predictors. The predicted R-squared is a form of cross-validation and it can also decrease. Cross-validation determines how well your model generalizes to other data sets by partitioning your data. So if your predicted R square is close to the adjusted R square that means your model is robust and it is achieving the almost same R sqaure with new data also. You cannot expect the predicted one to give you the exact number till the decimal its unrealistic.,318451.0
106947,462454.0,"Hi Anuj, It is okay to get a difference of 0.03 between these two. But try to reduce this difference.",344894.0
105873,456907.0,"Yes, we need to concat the dummy variable columns created and drop the original one.",318329.0
105873,456883.0,Yes same applies to all kinds of dummy variables.,310974.0
105873,458091.0,"You can use 0,1 scheme for fuel type, i.e 0 for gas and 1 for diesel. but do mention this with your model. Otherwise, you can use dummy variable and drop one as with all other variables.",318344.0
114363,492816.0,"Train Data Set and Test Data set are expected to be identical to each other with respect to number of variables, scaling and dummy variablest etc. Either you have to do the identical steps to maintain to keep the structure identical on both train and test data sets OR merge both of them but maintaining indexing in such a way to differentiate the train set data and test set data and then do those data cleaning, scaling of numerical vairables and creation of dummy variables etc (Later, you can separate them based on index as train set and test set before you start doing further analysis) TA can pass thier opinion too.",301121.0
114363,493098.0,"Yes, all the steps performed on the 'train' dataset should also be performed on the 'test' dataset.",313826.0
114363,493783.0,"Both data set should be identical, So either you have to merge your data set. Or else you need to perfromed all the steps on both 'Train' and 'Test' data",311004.0
114363,492833.0,"To add to my second option in my response where you merge both sets first and separate later , scaling of numerical variables shall be done after separating train and test data sets. (Scaling should be done after the test-train split as what we have been taught in Linear Regression) TA can clarify,",301121.0
105875,456908.0,"car_ID is a unique number and so can be removed while you take a call for car_name. The categories present in car_name after removal of company is more than 50% of the original records count. symboling, it might be important as this signifies the risk rating",318329.0
105875,458075.0,"Car_Id is the primary key for the data set, it can be used to check data issues like ________ . Symboling can prove to be important for model building so don't drop if you don't have a solid reason to drop it. carname contains vital info for our model, its mentioned in the problem statement itself what special treatment it requires. Scaling is very important in a linear regression model ( refer prev lectures ). I will suggest scaling numerical variables before model building.",318344.0
105879,456916.0,Did you observe the number of categories present in car name after removing company name? I found it to be more than 50% of the total number of records which implies that there are 2 records per model on average. You can take a call at this stage or go ahead and check its correlation at model stage. Thoughts?,318329.0
105879,457001.0,"Hi, Always run the first Linear Regression model on all the variables, after cleaning. After that look the VIF &amp; P-value, based on that drop the variables. So keep the CarNAme, run the model, if you find it has high VIF or P-Value than remove it from model. Thanks",344894.0
105879,457748.0,We should split the CarName to Company and Model. Helps for better insights price w.r.t car company and model belongs to.,312019.0
105879,458096.0,"I don't think it's ALWAYS best to run linear regression model on all the variable, if you add just a column with random values and run regression model you may find VIF and P- values considerable to not drop that column, see with your business knowledge if it is relevant to keep the column, try to derive new column if possible from the column itself for more relevance and try to proceed, don't drop any column till you have a strong reason to drop.",318344.0
106951,462117.0,since it is numeric in nature no need to transform proceed with the data as it is. the model will read it correctly. if you want to analysis whether risk and safe will impact my price please create the variable and do the modeling.,318451.0
106951,462121.0,No need to convert the values into categorical one. You can just proceed with that variable as it is.,317991.0
105886,457004.0,"Hi, Try this https://stackoverflow.com/questions/51182025/python-very-large-correlation-heatmap-for-many-variables/51182474 https://learn.upgrad.com/v/course/208/question/105817",344894.0
105894,457039.0,No you can't as that variable have a r square of 1 which means it's good in perdicting the varibality.,318476.0
105894,457101.0,"R-squared value 1 indicates that all other variables are able to explain the current feature very well. This, there is no point in having this feature while others are able explain this one. Hence, you can drop this variable. Also, Infinity is greater than 5. You can simply drop it.",318329.0
105905,457103.0,"I tried both the options. I ended up getting cylinder in both the final models. But, I believe it makes more sense if you could tell how many cylinders will have an effect on the price. So, I think we should go with dummy variables.",310974.0
105905,457095.0,"Since the number is going to be limited, it makes sense to consider it as a categorical. But I believe the model should be able to give similar results in both of the cases. I'll try and confirm.",318329.0
105905,457112.0,"Well, in my model, if I use cylinder as dummy variable, then I get it as part of my final model. But if I replace text with number, then I dont get cylinder as part of model. I am sure both these options work diffrently !!!",304814.0
105924,457190.0,yes you can use any method but it should match your output,318017.0
105924,457204.0,"Hi, Generally, LabelEncoder is used for Ordinal Categorical variables, but you can use it instanced of OHE (Output should be decent )",344894.0
105924,457612.0,"If we use LabelEncoder , i think we can't make proper insights. Ex: To know which fuel type diesel or gas affect car price Which car_body is affects the care price. These can be done proper insights with get_dummies()",312019.0
107063,462540.0,No of columns are different . use a command like this for eg df.columns it will print all the column names.,314197.0
107063,462591.0,Here Number of columns is different from test data and what is in your final model. Hint: Try to look for const attribute in your final model and treat that properly.,301648.0
107006,462131.0,From my research proir to begin the assignment - They are correct,318451.0
107006,462517.0,"As after creating the dummy variables for FuelSystem, there will be no values for mfi and spfi. So either drop them from dataframe or you can replace as mentioned above.",318448.0
107006,462372.0,Fuel system mpi and mpfi are basically same; I had replaced mpi with mpfi as both are multi point fuel injection. Similarly spfi can be replaced by spdi Incidentally this will reduce the dummy variables due to this. Can TA clarify?,301121.0
107011,462277.0,Ideally after the model is build and in real world problem we will not have y value so based in RSME and r square value only we can justify the prediction. But it seems like a good question for the TA to answers.,318476.0
107011,462417.0,I have replied for the same question on; https://learn.upgrad.com/v/course/208/question/106938 Hope this will work.,311117.0
107012,463239.0,"Harsha, what you say is correct but there will be a problem with interpretation. As then all the n-1 cols have to move together. You cannot drop any one of them during your model analysis.",304319.0
107012,462175.0,I could resquest you to inspect the column for data quality checks. Also get.dummy command will create the exact unique value variables from that column you just have to drop any one of them.,318451.0
107012,462404.0,"The concept of Dummy Variable Creation: If the categorical variable has 'n' levels (here, 22 cars), you will need to build 'n-1' dummy variables. Example, categorical variable 'sex' can have two levels, male and female (n=2). We will need only 1 (n-1) dummy variable for indicating this (0 is male and 1 is female). That is the reason we drop the first column as well.",312376.0
107012,462413.0,"Hi Sree Harsha, It is a good point. I was also under the same impression that just the 5 columns will be sufficient to tackle 22 car names. But as were taught in these lesson, we will end up having as many columns minus 1. That is what the command ""get_dummy with drop_first - TRUE"" does.",301121.0
107015,462169.0,When you create dummy variables by dropping one variable you take care of the multi colineraity there itself. Coming to two different variables being correlated and you found out with visualisation or your domain knowledge you can drop it prior to building a model. As you can save your time and effort. For example If i have a variable like number of registration on my stall and samples distributed. When i know that the stall attendent will give sample only after registration than i will opt for only one variable between the two and get rid of multicolinearity.,318451.0
107015,462391.0,"My view - It might seem right to drop a variable during visualisation due to multicollinearity. But you need to remember that visualisation is used to just understand the data, get the feel of it and maybe derive some insights. The best practice would be to use the VIF to take care of multicollinearity. Instead of using plots/graphs for reducing the number of variables, use numbers and statistics to take care of it.",312376.0
107024,462228.0,y_train_price = lm.predict(X_train_lm) use this code but instead of writing lm before predict write the name of the final model that you got.,302738.0
107024,462269.0,It seems like you train data set have less number of colunms than the lm model is expecting .Please check the cloumns again.,318476.0
107024,462381.0,"The 'const' variable is missing. Use the X_train from your final model, and drop the 'const' only after residual analysis. TA varified answer here - https://learn.upgrad.com/v/course/208/question/106519",312376.0
107029,462285.0,Yes you can drop it before dropping only on the basis of VIF also check whether the p value is greater than 0.05 or not.,318451.0
107029,462265.0,"Yes we can drop them . Variance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. inf means denominator is zero so its highly correlated to other varibale.",318476.0
107029,462379.0,The inf means infinite VIF. There is very high multicollinearity and needs to be dropped.,312376.0
107029,463816.0,"You may drop such columns, but it is recommended to delete a column and review the change in VIF before deleting the next column. In other words, delete one column at a time.",318078.0
107031,462283.0,"Cross Validation “splits” the available data-set to create multiple data-sets , and Bootstraps “clones” or extrapolates original data-sets to create multiple data-sets.Bootstrap is not a model validation technique or it is weaker than cross validation if used for model validation. https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf",318451.0
107031,462449.0,Differences between Train-test spilt and bootstrapping to estimate the prediction error : It may give you clear sight: https://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio,311117.0
107031,462332.0,To evaluate our model we split the data into Training and Testing sets.,319006.0
107035,462315.0,Yes. we have to correct it.,319006.0
107035,462581.0,What is there to correct here?,310974.0
107035,462852.0,There's nothing to correct here. These insignificant variables need to dropped later while building &amp; refining the model,318827.0
107036,462348.0,"Finally you have to arrive at a model where all required conditions are met like p-value, R2, VIF, R2 adjusted etc. It shall also be able to predict reasonably well and error terms shall be having normal distribution.",301121.0
107036,462368.0,"Understood, Thank you!",312376.0
107036,463084.0,"That is correect - many models are ""correct"". At my first attempt I had p&lt;0.05 and VIF &lt;5 but final R score for test set was in low .8x (5 variables); so I went ahead and built another model -&gt; one in which I got final R score for test set to be almost 0.9 with 9 variables - so I kept the latter",300694.0
106015,458376.0,Because symboling data type is integer. Do we needs to make change dtype before going to dummy vairbale. Because its not creating dummy variables as tried with int dtype of symboling pls clarify,312019.0
106015,458373.0,Symboling making dummy variable multilevel ok. Incase of CarName. Split to Name and Model Do we need Name(company name). Car companay name multilevel to make dummy vairbles ? We are ending up with so many variables. Hmm,312019.0
106015,457486.0,Yes since it is multi level categorical.,310974.0
106015,457538.0,"Yes, it has to be converted in to dummy variables. It is mentioned as int in the dataset. However, as per the dictionary, we need to use this as categorical variable",316202.0
106015,457640.0,"There's nothing directly mentioned about symboling in the session page of problem statement. While, this variable is mentioned as categorical in Data Dictionary, there's no information on the categories that would be mapped to symboling. Thus, I feel we better leave it as numerical feature.",318329.0
107051,462574.0,I think you have converted Car name into two sets ( Car Company &amp; Car Model ) Car Model can have some NAN values . Car Model coloumn can be dropped .,311861.0
107051,462470.0,"Hey Anurag, There is no Nan value in the CarPrice dataset. Please re-check the dataset.",302742.0
107051,462826.0,I have done this check in my assignment file. None of the columns have null values .,318827.0
107042,462370.0,Yes. VIF and p-value are different. Both need to checked and dropped (vif&gt;5 and p-value&gt;0.05). TA Varified discussion: https://learn.upgrad.com/v/course/208/question/106881,312376.0
107047,462386.0,"Hey Sree, These VIFs tell you there is perfect collinearity, that you have completely redundant variables. Don't worry keep dropping variables one by one while checking for both p value and VIF.",302742.0
107048,462445.0,"1st look the combination of P|t| and VIF i.e if P|t| &gt; 0.05 and VIF greater than 5 column should be dropped 2nd even if p|t| is less than 0.05 then look for the VIF which is greater than 10 should be drop 3rd Also find the colinearity among the variable usin the pair plot and drop column which are highly colear among themselves even if there is chance that one of coleanear value less than 5.,just drop one by one. TA please check my answer and add your light also.",307843.0
107048,462833.0,"We should have only variables that have P <span style=""font-size:11.0pt;line-height:107%; font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-theme-font:minor-latin;mso-ansi-language:EN-IN;mso-fareast-language: EN-US;mso-bidi-language:AR-SA"">≤ 0.05. VIF value should be <span style=""font-size:11.0pt;line-height:107%; font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-theme-font:minor-latin;mso-ansi-language:EN-IN;mso-fareast-language: EN-US;mso-bidi-language:AR-SA"">≤ 5 but it can be nearly = 5 if it makes business sense to have that variable",318827.0
107021,462207.0,"Yes, p-value is important factor. It provide us the information about the varaible being significant or not. As a standard practice p-value&lt;0.05 is consider to tell that variable is significant and if it is more than 0.05 then variable is not significant. Hope this help.",317991.0
107021,462272.0,If you significant level is 5 % then any p-value&gt;0.05 means they are statistically insignificant for the model prediction. Best it to remove those columns from the models.,318476.0
106961,462457.0,"Hi, Please follow the attached link https://stackoverflow.com/questions/33833832/building-multi-regression-model-throws-error-pandas-data-cast-to-numpy-dtype-o",344894.0
106572,460315.0,I think you didn't create dummy variable properly. VIF will be applied for numeric type of variable and your train set may have still str type of variable.,301648.0
107003,462436.0,"Instead, you can create a dummy variable as well. You will have one less variable to fit in the model.",312376.0
107003,462134.0,Can you paste the code ?,318451.0
106062,457617.0,try to find the affecting variables and then do dummy convertion will help in reducing at intial steps then go to train test,317982.0
106062,457638.0,It's better not to drop any features before dummies. Use RFE to reduce the features to your desired number. Then you can depend on p-value and VIF to drop further.,318329.0
107059,463237.0,"You only need the car company,model name has to be eliminated",300684.0
107059,462573.0,"You can convert different Car names into Dummy variables via Pd.get_dummies Then you can see interesting pattern Some cars have high co-relation , based on brand name it influence price ..",311861.0
107059,462619.0,Check the categorical variables and drop them. You can eliminate CarName from the equation. Don't worry about the 22 columns.,319006.0
107059,462823.0,Loyalty can be assessed by brand names. In industry there are many brands for whom we may not know the manufacturer's name but we still buy them.,318827.0
107059,463054.0,"This question hasn't been raised because it has already been answered in the assignment brief: There is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. For example, chevrolet impala has 'chevrolet' as the car company name and 'impala' as the car model name. Y ou need to consider only company name as the independent variable for model building. This clearly says and asks you to use the company name in the model",300694.0
106068,,nan,
106069,457731.0,We can go ahead with get_dummies(). It is pretty much relevant.,311160.0
106069,458100.0,"Revisit the section where use of dummy variable has been taught, it will help.",318344.0
107061,462542.0,Yes. VIF and p-value are different. Both need to checked and dropped (vif&gt;5 and p-value&gt;0.05). TA Varified discussion: https://learn.upgrad.com/v/course/208/question/106881 Its a iterative process,314197.0
107061,462618.0,If p-values is more than 0.05 then drop one by one so that all are within 0.05. It's not wrong till you drop it to find it within 0.05. But every time you drop the column you go back and run the summary again.Do this one by one only.,319006.0
107061,463076.0,"it is an iterative process - each time after dropping a feature we need to re-build the model. We can continue to do this until no more variable can be dropped. THEN we check VIF - and if a feature/variable needs to be dropped due to high value (&gt;5, definitely if &gt;10) then we need to again rebuild the model and check p-values and keep repeating this until both VIF and pvalue are under acceptable thresholds",300694.0
107061,462812.0,"Except for constant, all P values should be <span style=""font-size:11.0pt;line-height:107%; font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-theme-font:minor-latin;mso-ansi-language:EN-IN;mso-fareast-language: EN-US;mso-bidi-language:AR-SA"">≤ 0.05. VIF should also ideally be <span style=""font-size:11.0pt;line-height:107%; font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-theme-font:minor-latin;mso-ansi-language:EN-IN;mso-fareast-language: EN-US;mso-bidi-language:AR-SA"">&lt; 5",318827.0
106076,457734.0,Please check the below discussion link: https://learn.upgrad.com/v/course/208/question/105817,311160.0
106076,457746.0,"Make prices has first column. Two heatmaps we can draw to see high correlation value to consider for analysis to start if you have 40 variables plt.figure(figsize=(16,10)) sns.heatmap(train.iloc[:,:20].corr(),annot=True,cmap=""YlGnBu"") plt.show() do next half for another hetmap. addit first col as price again This way values will be visible clearly",312019.0
106076,458045.0,"You can make the size of the heat map bigger and then increase the font size: plt.figure(figsize=(20,15)) sns.set(font_scale=1.6)",305653.0
106076,458070.0,Do these two things : sns.set(font_scale=1.6) and then double click on the plot to make them big and easily readable.,315560.0
106076,461411.0,You can draw corelation map after doing RFE as the number of columns will reduce.,314621.0
106080,457839.0,"Hi, Generally, LabelEncoder used for ordinal categorical variables. As like No. of cylinder , you can use LabelEncoder but for car model, you should not.",344894.0
106080,457944.0,"HI Paras, so even CAR company name, carbody which have high count of unique catagoligal values need to be convert ted to dummy variables?",316036.0
106099,457950.0,"Visualizing the values before building the model is to get a feel of the variables. You should not drop these before building the models, as these might be significant when combined with other independent variables. So, after visualizing, you can build a model using the RFE method to start with and then drop the insignificant variables one by one based on the model KPIs.",316202.0
106099,458007.0,"if two variables are highly corelated, then it is better to drop one of them. Just assume that if one company prepares one unique type of engine, then keeping two seperate variables in the model would not make much sense.",304814.0
106099,458011.0,"Hi Sham, I did not quite understand your answer. If two variables are highly corelated, then why can not we drop one of them? Wouldn't keeping two highly correlated variable anyways lead to high VIF in the later stage of model, so would not dropping one of them early be beneficial? Am I missing something here?",304814.0
106099,458112.0,Use VIF to drop these columns later when building model. Dropping one without analysis may lead to loss in info.,318344.0
106081,457840.0,"Hi, Please follow the attached conversation. https://learn.upgrad.com/v/course/208/question/106080",344894.0
107070,462588.0,You Should Properly Clean your data and then check for HeatMap.,301648.0
107070,462569.0,On which data set you are running the Heat Map . You may probably have not done the cleanup of data . Null vaues need to be dropped if you hvae created any new variables which are null. In the orginal data set there is no null value .,311861.0
107070,462610.0,"Hi, Unmatching IDs, if any, will get labeled as NA. You can remove them with complete Or, enforce uniqueness. NA values in the data may also create empty spaces in the heatmap, so you can set them all to zero. overall follow the data cleaning steps. Hope this will help you.",319006.0
107070,462801.0,The blank rows are occurring due to data issues. Please clean the dataset. Check for unique categorical values and fix it.,311254.0
107065,462804.0,Above are good answers for your Q. Do remember to replace the car names with correct spellings before you apply unique() function,318827.0
107065,463235.0,You can also use df.Carname.value_counts() It will also give the count of unique values.,304319.0
107065,462541.0,use this df =CarPrice['fuelsystem'].unique(),314197.0
107065,462570.0,You can try this as well df.CarName.unique() which will give you the list of Unique value associated with Car Name,311861.0
106108,458076.0,I have the same doubt. The company wants to predict prices for thier cars.In this scenario company name seems an unwanted feature,311864.0
106108,457935.0,"Hi Shubham, Yes , you're right we will not require car company column, but before dropping that have you extracted the company name out of it ? If yes, then go ahead and drop the column or else extract the company name from that column and then drop it. Hope this helps",301655.0
106108,457946.0,"I assume that you have extracted the ""Company Name"" from the ""Car Name"" column. I dont think you need to drop the ""Company Name"" from the analysis before analysing the model KPIs. If the KPIs are strong enough for this variable, you need to retain them n the model. So, please add them to your model, check the metrics (p value, t value, adjusted R2 etc..and decide.",316202.0
106108,458109.0,"""Brand Name"" plays a very important role in fixing price. I don't think you should drop it straight away with extracting some info from it.",318344.0
106108,458473.0,"Yes, you can drop it after the company name &amp; model number split has been performed. You can used .str.strip() to separate both first and then drop the Car Company. Remeber in a model building video it was recommended to make a ""lean model"" Hope this helps you..!",310508.0
106108,459022.0,I guess you need to convert Car Company into dummy variables then you can drop both CarCompany and the splitted Car Make names.,300734.0
106108,459026.0,"in my opinion Car Name or Brand name is an exogenous factor which cannot be utilised effecively by Geely company in their own predicttions. Rather they should look at the segment of the car - such as premium, std,, etc.",310509.0
107073,462586.0,"R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 – 100% scale. 100% represents a model that explains all of the variations in the response variable around its mean. Anyway, we should not depend on only R square value we should check other parameters in our final model.",301648.0
107073,462602.0,"Yes we have to calculate R squared . R - squared is a goodness-of-fit measure for linear regression models. For instance, small R - squared values are not always a problem, and high R - squared values are not necessarily good ! It’s very high at about 0.85.",319006.0
107073,462799.0,R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination. It is the percentage of the response variable variation that is explained by a linear model. Pl. go through the link: http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit,318827.0
107080,462683.0,"Yes, you should drop variable based on p values and VIF one by one. First, drop one variable and rebuild your model again and check for the updated p-value and VIF.",301648.0
107080,462708.0,"Hi Ankur, Yes, you need to drop these variables one by one and after dropping every variable rebuild the model and observe the changes",301655.0
107080,462794.0,"Yes, such &amp; more variables need to be dropped one by one untill you are left with those having P <span style=""font-size:11.0pt;line-height:107%; font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi; mso-ansi-language:EN-IN;mso-fareast-language:EN-US;mso-bidi-language:AR-SA""> <span style=""font-size:11.0pt; line-height:107%;font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin; mso-fareast-font-family:Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font: minor-latin;mso-bidi-theme-font:minor-latin;mso-ansi-language:EN-IN;mso-fareast-language: EN-US;mso-bidi-language:AR-SA"">̴ 0 &amp; VIF <span style=""font-size:11.0pt;line-height:107%; font-family:&quot;Calibri&quot;,sans-serif;mso-ascii-theme-font:minor-latin;mso-fareast-font-family: Calibri;mso-fareast-theme-font:minor-latin;mso-hansi-theme-font:minor-latin; mso-bidi-theme-font:minor-latin;mso-ansi-language:EN-IN;mso-fareast-language: EN-US;mso-bidi-language:AR-SA"">≤ 5",318827.0
106137,458107.0,I think you should scale the numerical values before building the model.,318344.0
106138,458106.0,I think you should scale the numerical values before building the model.,318344.0
106138,458174.0,You need to scale the data to bring all the column's values to same level( to normalize),311952.0
107085,462720.0,Try removing any other variable with VIF greater than 5 and check what happens,311160.0
107085,462740.0,vif should be less then 5 for good model as higher vif represent multicollinearity,318017.0
107085,463060.0,This happened to me too -&gt; dropping a feature with high VIF resulted in the adj r-squared dropping MASSIVELY - so I dropped a different feature (theone with the second highest VIF) and it resulted in the adj r-sq actually going up -so that was a good move :-) and remember &gt;10 is sure drop; &gt;5 but &lt;10 we might actually leave those features in if that is the best course of action,300694.0
106155,458910.0,"It means that your model is following the train dataset very closely if your R2 drops significantly while running on test dataset. So, try rebuilding the model by doing some tweaks.",317689.0
106155,458214.0,small r squared values means that the relationship that you have built is not that significant so change the variable you are using.,318017.0
106155,458271.0,It means that your model got trained very well but did not perform well on test set. It would not be acceptable in in real situations. Try re-visiting and see if you can build a better model.,318329.0
106181,,nan,
106174,458347.0,You can generally follow this:,310974.0
106174,458658.0,Or we can do automatic and manual with from sklearn.feature_selection import RFE Because the variables are too many.,312019.0
106174,459566.0,A high p value indicates the variable/feature is insignificant and can be eliminated.,315242.0
106175,458344.0,"Hi Deepak, Hope this link helps you clear out your doubts to some extent: https://www.researchgate.net/post/Should_one_use_regression_analysis_when_all_independent_variables_are_binary_categorical",301655.0
106175,458345.0,"There are various regression models like logistic regression which helps in classification problem. So, not all regression models are for predicting numeric values.",310974.0
106175,458426.0,"Regression models to only predict integers (instead of floating point numbers ) : For explaination, pls go through the link: https://stats.stackexchange.com/questions/160180/regression-models-to-only-predict-integers-instead-of-floating-point-numbers",311117.0
106175,458500.0,"HI Deepak, Your question is right in case where you encounter the data which is a mix of both numbers &amp; descriptions. To put it in proper way ""Quantitative""&amp; ""Qualitative"". If you recall the session named ""Dealing with Categorical Variables"", (Link https://learn.upgrad.com/v/course/208/session/24636/segment/126227 ) this was addressed in it. Regression models are used to represent the ""quntifiable""data hence there is need to convert the descriptive data for underlying program to run it and then show the plots in a more meaningfull way. I used the following link to understand it better. https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding Could help you too.. Thanks",310508.0
106175,458405.0,"No all the regression types are not limited to predict integer. Regression analysis is a form of predictive modeling technique which investigates the relationship between a dependent (target) and independent variable (s) (predictor). This technique is used for forecasting, time series modeling and finding the causal effect relationship between the variables. There are various kinds of regression techniques available to make predictions. These techniques are mostly driven by three metrics (number of independent variables, the type of dependent variables and the shape of the regression line) You can read more detail here in detail: https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/",317845.0
106179,458359.0,"From my understanding, if we assign numerical labels to categories and try to build a model, then the algorithm would consider it as continuous variable and try to build some relationship based on that which is not. Please follow the attached discussion https://learn.upgrad.com/v/course/208/question/105924/answer/457204",318329.0
107103,462862.0,the difference should be minimal verifying that your model is working fine.,318017.0
106199,458446.0,Try with both and see which model explains high variance.,310974.0
106199,458448.0,"We generally want a VIF that is less than 5. So there are clearly some variables we need to drop. This was emntioned in one of our house case study in the regression module course. But my doubt gain is how to decide dropping the varibles w.r.t to VIF and p-value, t- value etc Some can help plese ?",312019.0
106199,459586.0,"Generally, we use VIF&gt;5 method, using VIF&gt;2 will leave the management with very few independent variables to consider. The severity of the scenario needs to be taken into consideration. For the assignment i suppose we can go ahead with VIF&gt;5",320690.0
106199,458903.0,"VIF measures magnitude of multi-collinearity.Usually there will be some correlation between expanantory variables in almost all cases,so the value of VIF likely to be more than one. The threshold value for VIF is 4(few authors suggest 5 and so on) for eg VIF equals to 4 implies that t-statistic value is deflated by a factor 2 and thus there will be a significant increase in corresponding p-value You can try with both VIF &gt;5 and VIF &gt;2 and check. Usually a value of 1 indicates that there is no correlation between this independent variable and any others. VIFs between 1 and 5 suggest that there is a moderate correlation, but it is not severe enough to warrant corrective measures. VIFs greater than 5 represent critical levels of multicollinearity where the coefficients are poorly estimated, and the p-values are questionable. Please note the above are just suggestible",308638.0
107106,462858.0,if the data is not that significant that it would impact the model then you can either drop it or dont do any analysis using it . as if you drop that row some other variable might have imp data and you will end up loosing it so better keep it and correct it as spell correction.,318017.0
107111,463042.0,rfe only works on numerical values and it seems you have left audi as a value in your training set. You need to convert your categorical values into n-1 dummy variables before calling on RFE,300694.0
107111,462889.0,You should convert all the string type attribute into a categorical variable. Keep in mind that RFE only support numerical attribute and it looks like your dataset still contains string-type attribute. So work on your categorical/dummy variable creation part.,301648.0
107113,462895.0,It is good to drop the first variable after creating your dummy variable. Hint: Inside pd.get_dummies you can use drop_first in a proper way to handle this.,301648.0
107113,463039.0,as per lectures - you should drop the orignial column + 1 of the new dummy columns; in the dummy creation function you can use a parameter drop_first = True to drop one of the dummy columns automatically and thereby have n-1 columms - since one columns will be redundant,300694.0
106219,458611.0,"In the videos, the variables were dropped based on p-value value and then checked for vif to be less than 5",318329.0
106218,458559.0,There's no mandate to use MinMax scaling. You can use any scaling.,318329.0
106218,458668.0,"Both are used to transform your observations. Can use any of the methods. Normalizing typically means to transform your observations x into f(x) (where f is a measurable, typically continuous, function) such that they look normally distributed . Some examples of transformations for normalizing data are power transformations . Scaling simply means f(x)=cx, c∈R, this is, multiplying your observations by a constant c which changes the scale. That's a slight difference. Can refer for details: https://stats.stackexchange.com/questions/35591/normalization-vs-scaling",311117.0
106218,461396.0,Yes you can use any scaling.,314621.0
107138,463081.0,It is correct. You can add them to your dataframe. I hope you have transformed the column with just the company name.,311160.0
107131,463012.0,1.Basic steps to identify the co-lineariry using the heat map w.r.t to price if it is insignificant then remove it. 2 Also check he colinearity with other variable and if is is high colinearity then the variable canbe removed.,307843.0
107131,463015.0,If you think the unique ID contributes to the value/price of a car then keep it otherwise remove it .. simple :-) Personally I don't think that a randomly generated unique ID will contribute to the variance in price of a car.,300694.0
106252,458792.0,try changing the seed value while splitting,318329.0
106252,458807.0,"This shouldnt be the case. May be you can try using random.seed(0) and check, as mentioned by Nagaraju. Please Let us know how it goes.",316202.0
106252,459540.0,"Hi Puneet, Do preprocessing (dummy variable creation ) before splitting the data.",344894.0
106318,459082.0,trial is the answer here you have yo try changing varibles and find the best possible output,318017.0
107165,463225.0,"It certainly is possible. Particularly, with the datasets containing lesser data.",311160.0
107165,463435.0,It is possible and I did get that in my model.,318329.0
107168,463230.0,"We have just assigned the price series of the df_train to the y_train and the rest of the dataframe to x_train. Accordingly, y_train would be Series and x_train would become dataframe. Info is the attribute of Dataframe and doesn't work on Series.",311160.0
107168,463308.0,y_train = df_train.pop('price') X_train = df_train we used something like the above code -as you can see y-train is a series (just one column) while X_train is ALL the other columns of the dataframe,300694.0
107168,463229.0,"Y train is popped out value from DF and hence it is a series. X train remains as DF with one column dropped ( price). If you need y train to be DF , you can convert using to_frame()",316211.0
106263,458791.0,"If you are asking how to do it in python, you can use pd.get_dummies() method with drop first as true. then concat these new columns with the original df and drop the categorical features for which you created dummies",318329.0
106263,458772.0,"The thumb rule is to use n-1 dummy varible for n-collearinety variable. Suppose we have 4 level of rfeedback of produc good,bad,v.good,excellent,outstanding then we can make dummy variable as good - 0 v.good - 1 excellent -2 outstanding -3 , We can then not consider good - 0 and let it co-effients is B1 with and whenever we multiply with B1 with 0 it give this result is 0 so absence of B1 can be predicted even though it is absence as good dummy variable. You can refer details in upgrad or also below https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/",307843.0
106263,461393.0,Use get_dummies() method and you can refer python notebook provided in lecture to get an idea.,314621.0
107585,465467.0,constants can give you high vif so you might end up removing it but it should not be the case as if you remove constant your predicted line will pass through origin but its not always the case so u cam either remove constant for vif calculation or you can keep it but not remove based on vif value,318017.0
107585,465729.0,"Const term shows the value of the target variable when all the predictor variable = 0, If you think there is no meaning of having some values with predictor variable = 0 you can drop it. Otherwise, keep it.",318344.0
107585,465602.0,"Constants will highly impact by increasing the vif of all the variables if present. Hence, we must remove constant before calculating the vif.",317811.0
107586,465512.0,it means that the model you developed could explain 88% of the variance occurred between the actual and predicted car price of your training data. for the test data it could take care of 78% of the variance b/w predicted and actual values. there is no right or wrong as such. different models will have different R2 value pairs in this assignment. no problem.,311686.0
107586,465728.0,"R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. The definition of R-squared is fairly straight-forward R-squared = Explained variation / Total variation R-squared is always between 0 and 100%: 0% indicates that the model explains none of the variability of the response data around its mean. 100% indicates that the model explains all the variability of the response data around its mean. Now coming to your question of the difference between the two R sq score. First one is how much variation your model could explain in train data whereas the second one is how much variation it could explain in the test data.",318344.0
107586,465842.0,"hey Rishi, you see that difference solely because of the nature of data in train set and the test set.. you've splitted your data as 80, 20 and your model was on 80% of data which you're testing on 20% test data to evaluate it.. there will be chances that the 20% of data might have values/scenarios which were not captured in the 80%of data and hence you'll see the difference in r2 in both these datasets.. significance of r2 was as explained above in two responses that it tells you a variance explained by your model.. but I'm considering your doubt was on why the difference in r2 on both these dataset which is due to the nature of datasets..",316349.0
107174,463284.0,"You need to delete those variables which has high VIF values, though it's p value is 0.",316202.0
107174,463299.0,VIF infinite is considered a very very high value - so any variables with a high VIF value you have to drop - drop one at a time; and after each drop you need to re-generate your model to see if the p-values have changed - if you have high p-values then you need to drop variables again one at a time and check new p-values,300694.0
107174,463425.0,"Multicollinearity when indivisual regressions are significant, but VIFs are high/low. Go through the link, where it has been explained in many ways regarding how to deal when regressions are significant, but VIF's are high/low: https://stats.stackexchange.com/questions/24464/multicollinearity-when-individual-regressions-are-significant-but-vifs-are-low",311117.0
107174,463914.0,"1. Using features generate the model 2. Feature with p value > 0.05. If yes, then drop that variable and go back to Step 1 and check p value and VIF 3. Feature with high VIF ( > 5 usually is considered as high). If yes, then drop that variable and go back to Step 1 and check p value and VIF 4. Keep repeating Steps 1, adding new significant feature through 3 until you get p value and VIF for feature under permissible range",317845.0
107174,464320.0,"This needs to be repeated until and unless you have a model with low p-value i.e &lt;0.05 and low VIF i.e &lt;5 or can be &lt;10. You need to drop the columns and check for both p-value and VIF, if any of them fails.",318448.0
107175,463298.0,for multicollinearity we have to use VIF after you have shortlisted variables via RFE,300694.0
107175,463324.0,"Multicollinearity arise from the inclusion of highly correlated independent variables. Both correlation coefficient and VIF are used to verify multicollinearity In regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity exists when: One independent variable is correlated with another independent variable. One independent variable is correlated with a linear combination of two or more independent variables. For details please go through the below link : https://stattrek.com/multiple-regression/multicollinearity.aspx",319006.0
107169,463234.0,kindly check the data types of all the variables of your dataframe. They should all be numeric. Probably you would have missed converting one or more categoric variables to dummy variables.,311160.0
107169,463286.0,"This issue is relatest o the data type. As, linear model only considers numeric variables, please make sure you have converted all the variables in to numeric.",316202.0
107169,463303.0,please ensure all your values in the dataframe are numeric - there are several things that need to be done before you create a model (from encoding and dummy value creation to normalisation to ensure all numeric values are close to each other),300694.0
108357,468202.0,"Hi, well commented juypter notebook. you can upload it directly. zip is not required. Thanks",319006.0
108357,468227.0,It is mentioned in the module itself very clearly. Still a well commented juypter notebook. No ZIP.,318344.0
107181,463384.0,As per my understanding first Preference should be given to VIF value and then see its effect . So in your cases . Case 1 - First drop the value which has VIF 100 and then see the efect on Model Case 2 - First Drop the value which has VIF 100 and then see the effect on the mdoel. let me know your result with this . If Data cleaning &amp; high corelated variables are dropped prorperly you will get good result.,311861.0
107181,463402.0,generally you would drop feature with highest VIF in this case - the one with 100; but sometimes we need domain knowledge and I have gone to drop a feature with a lower VIF (Say one with 80 in your example) - and it has resulted in a much much better model - so yes you need to try a few different things if you understand the business behind the data. say you are analysing house price and according to VIF analysis area of property is showing highest but below that is number of windows .. my gut would say to drop number of windows before area - so I would try both models and see which one results in a better adjusted r-square immediately following the dropping of a feature Also - your examples all had a p value &lt; 0.05; If there were p values higher than 0.05 then those features would be dropped first by me before I look at VIF at all,300694.0
107181,463815.0,"p-value indicates the significance of a inedpendent variable in the predication of the dependent variable. A value of p &gt; 0.05 indicates that the variable is not adding any prediction power to the model and hence should be dropped from the model. VIF indicates the multicollinearity between the various independent variables of the model. If the p-value of all the variables is in permissible range (&lt; 0.05) , then we can need to drop variables with high VIF ( &gt; 5 ). However, we need to keep a close eye on the R-square and Adjusted R-square values. If by dropping certain variable, there is a drastic drop in these values, then we should look at other co-rrelated variables as contenders for dropping and try out various combinations.",313826.0
107181,463580.0,"This is how I would do: First drop the variables which are insignificant, p &gt; 0.05. If there are too many variables in the model, you can drop variables with p values between 0.04 and 0.05 as well as long as it doesn't bring down Adjusted R2 drastically. Check VIF If some variables have high VIF, remove the variable which is relatively less significant. Repeat Step 1 through 2 if this causes some variables to become insignificant. If none of the variables are insignificant (p=0.00) and you still have variables with high VIF, drop the variable with highest VIF. In doing so, make sure dropping that variable doesn't bring down Adjusted R2 drastically. If that is the case, drop the next highest and repeat the Adjusted R2 check again and move to the next one. Repeat Step 1 through 2 if this causes some variables to become insignificant. By this step, you should be left with variables which have low VIF and are significant.",310974.0
107181,465196.0,TA pls answer this question as I have similar doubt and not convinced with answers from others,308437.0
106269,458820.0,"Is there anything different that you are choosing the featues? Like a different seed value, dummies? Because, the variables that got eliminited in RFE might be different for you from them.",318329.0
106269,458928.0,"The reason why you have got VIF as INF is Correlation among those variables would be an exact 1. Maybe, you can try dropping one among them and see how it fares. We can probably take it forward from there.",311160.0
106269,458834.0,"Anuj Modelling is a very subjective process, maybe the classification, the variables you chose are different. There is no perfect model for these problems. If you find your model not that appropriate you can play around and find a good fit. For your better understanding of VIF Variance inflation factors show the degree to which a regression coefficient will be affected because of the variable's redundancy with other independent variables. As the squared multiple correlation of any predictor variable with the other predictors approaches unity, the corresponding VIF becomes infinite For any predictor orthogonal (independent) to all other predictors, the variance inflation factor is 1.0.. If the VIF's are not unusually larger than 1.0, multicollinearity is not a problem. An advantage of knowing the VIF for each variable is that it gives a tangible idea of how much of the variances of the estimated coefficients are degraded by the multicollinearity.",318344.0
106280,458907.0,yes as much as possible for the Variables which have more than two categories.,301648.0
106280,458922.0,"Even for the categories with 2 values, we can use dummy variable technique by dropping any of the other column. Does anyone see an issue with it?",311160.0
106280,459139.0,"Yes, you need to convert all the categorical variables in to numeric by using dummies function, as the linear regression will not accept any categorical variables. We will be getting a lot of variables after converting, and hence we can use RFE feature to create a model.",316202.0
106280,458882.0,"for the variable which have two values you can use 0, 1 technique and for rest you can make dummy values by n-1 technique",318017.0
106279,458884.0,t value is coefficient / standard deviation and p value gives the value of the value lying in the normal distribution curve,318017.0
106279,458917.0,Please follow the below link you will get some hints. https://www.google.com/amp/s/www.researchgate.net/post/Why_have_I_got_a_negative_T_value/amp,301648.0
106279,458976.0,"p-values are about regression coefficient estimates not variables, while T-tests can measure whether two means have a significant difference. For further details, pls go through : https://www.researchgate.net/post/P_value_in_Regression_and_T-Test_what_is_the_relation",311117.0
106285,458892.0,Now you should handle this properly by converting all the synonymous car maker name into a single group. Like Toyota or toyouta to only toyota,301648.0
106285,458881.0,yes you should correct the spelling as it would impact analysis suppose you draw box plot of make vs price it would give different plots for porcshce and porsche.,318017.0
106285,458906.0,You need to handle this data quality issue appropriately by converting incorrect names to correct names.,317689.0
106285,459602.0,Yes data cleaning has to be done. Some incorrect car names are given in the list. You can apply unique() on the carname of the dataframe. Check the carnames you have got. Replace the incorrect carname with the correct ones. Then again do dataframe.carname.unique(). This will give you an idea of all the names of the car used in the analysis.,301114.0
106303,458959.0,I think we can keep it.,301648.0
106303,458966.0,"Const term shows the value of the target variable when all the predictor variable = 0, If you think there is no meaning of having some values with predictor variable = 0 you can drop it. Otherwise, keep it.",318344.0
106303,459132.0,"You can drop the const while calculating the VIF, as you will not be analysing it for the target variable. VIF is used to check the relation between independent variables.",316202.0
106303,461385.0,Yes you should drop constant before calculating VIF.,314621.0
106295,458967.0,"Check syntax, maybe you are not passing a list to the function.",318344.0
106295,459138.0,"Not really getting this error. It is related to the indexing variable you are passing in the loop. You must have created a variable with the list of ""Columns"", which has to be scaled, and have applied the scalling function to this variable. Please check.",316202.0
106295,459550.0,"Hi, Please follow the attached links https://stackoverflow.com/questions/44917589/indexerror-only-integers-slices-ellipsis-numpy-newaxis-none?rq=1 https://stackoverflow.com/questions/34952651/only-integers-slices-ellipsis-numpy-newaxis-none-and-intege https://github.com/Mostafa-Samir/DNC-tensorflow/issues/17",344894.0
107196,463510.0,Constant should have been removed in the first place before performing VIF check. Secondly if VIF is high that feature shall be dropped even though p-value is lower than 0.05 I hope this clarifies,301121.0
107196,463828.0,"A high VIF indicates that there are other independent variables in the model which can explain the given variable to a good extent(multicollinearity). In order to keep the model lean, any such independent with high multicollimearity need to be dropped.",313826.0
107196,463916.0,A rule of thumb for interpreting the variance inflation factor: 1 = not correlated. Between 1 and 5 = moderately correlated. Greater than 5 = highly correlated. You should try dropping the variable.,317845.0
106292,458916.0,"Yes, in order to take care about the Multi-Collinearity, we need to drop each feature and check what happens next by looking at the P-values and VIF's agains",311160.0
106292,458929.0,"yes we should try that as ViF will check for Multicolinearity. A low p-value (&lt; 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable.",301648.0
106292,458968.0,"Yeah, remove one by one and check how the model behaves.",318344.0
106292,458946.0,"Yes ,in order to address multicollinearity problem. Multicollinearity causes the following The coefficient can swing wildly based on which other independent variables are in the model and coeffiecients become very sensitive to small changes in the model. Multicollinearity reduces the precision of the estimate coefficients We might not be able to trust the p-values to identify independent variables that are statistically significant. Please note that the serious impact of multicollinearity is that it can change the sign of regression coeffiecient.It is advisable to drop feature having VIF and then check for statistics again and decide upon",308638.0
106292,461089.0,It’s up to you. Choose which one you want to take as threshold. But according to my analysis I prefer 10,306738.0
106292,459573.0,Anything greatervthan 5 shud be dropped,315242.0
106292,463882.0,"I have same issue, i feel somewhere we did not tableout the categorical conversion properly or had an issue in normalizing numerical features",315242.0
107217,463698.0,"You should check the p-value of other attribute after removing constant, P value &lt; 0.05 is not appropriate for constant.",301648.0
107217,463669.0,This is the screen shot from lecture note. p value should be &lt;0.05,317845.0
107217,463787.0,constant can have o value greater but try to make it as low as possible,318017.0
107217,463836.0,"In case you have standardised the independent variables, then the constant(intercept) should be zero ( or very close to zero). Otherwise it can take any value based on the scale of the independent variables.",313826.0
107217,463808.0,"Try to have a value &lt; 0.05. Higher values mean that the column is insignificant, hence, may be dropped. There could be other reasons for getting such value. Validate using other techniques like VIF, etc.",318078.0
106305,458973.0,you can use either of the method rfe will do self model making and in non rfe you will have to make model according to the analysis made earlier,318017.0
106305,459129.0,"Would suggest to use one of the ways. RFE preferably. Because the number of variables after converting all the categorical variables will be much higher. Doing manually through non RFE, would be time consuming.",316202.0
106305,459145.0,Thanks Deval and Sham. Was thinking on the same lines.,311857.0
106314,459085.0,you are given a situation like this is hp of car this is mpg this is the engine size and this is peak rpm and the car name then putting the values in the equation you can find the price of car using the equation.,318017.0
106314,459322.0,I think if there are more categorical variable and we need to classify our analysis then logistic regression will be the best approach.,301648.0
106307,459177.0,There must be some value in your X_train_lm which is not numerical. Try it by reducing the number of column taken in the X_train_lm and creating the model.,317689.0
106307,459127.0,This is definitely related to the variable data type. Make sure that you have converted all the categorical variables in to integers. You can use pd.get_dummies for the categorical variables and convert them.,316202.0
106307,459034.0,Before building a model you need to have all your variables in numeric datatype Try running X_train.info() and check for the info. Try converting to dummy variables in case you find any Categorical type.,311160.0
106341,459211.0,"Yes. It can be left as it is, only you need to scale it.",304319.0
106341,459163.0,It can be left as is as the values are numerical. Model can take into account the positive and negative co-relations.,317689.0
106386,459403.0,Constant is just the Y intercept. It is not any variable that affects the analysis. At the end of the day you aren't telling the customer about its importance,311160.0
106385,459324.0,"First, you can use RFE to bring down predictor variable that you can handle manually, say 10-15, then proceed with manual tuning for the best model.",318344.0
106385,459472.0,Best approach will be mix of RFE and manual tuning.,301648.0
106228,458612.0,We do not delete the constant as it doesn't vary with variables.,318329.0
106403,459436.0,I've removed them since it's only couple of records and is insignificant compared to total records.,318329.0
106403,459491.0,"Though RFE selects this particular variable, the p-value of it would suggest you to drop it at some point :)",311160.0
106415,459524.0,"Hey Sourodeep, Refer - https://github.com/statsmodels/statsmodels/issues/467/ and see if the suggestion given is helpful for you.",302742.0
106423,459589.0,"zip the wrong and correct spelling. Then convert into a dict and access it. Eg. a=dict(zip('upgGrde':""UpGrade"",'bhusan':""Bhushan"")) &gt;&gt;&gt;a['bhusan'] 'Bhushan'",318780.0
106423,459650.0,"For now, we may have to look manually.",318329.0
106423,459680.0,or create a function if x==a then return b elif x==b then return c and so on.... and use apply on dataframe series,304814.0
106423,459887.0,"Instead of searching the manually in csv use print(df.columnname.unique()) You will get list of column values, now using replace() function you can standraize the column value. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html",317845.0
106423,461382.0,You can create function and apply to column.,314621.0
107149,463158.0,"You have popped the price variable out of df_train and assigned it to y_train. So, try the below code: plt.scatter(df_train.enginesize, y_train)",311160.0
107277,464183.0,u need to convert the values to array and then use inverse_tranform .. this might be useful https://datascience.stackexchange.com/questions/16548/minmaxscaler-broadcast-shapes/16741#16741,317982.0
107277,464172.0,inverse_transform Scale back the data to the original representation You can use inverse_transform using your scalery object: y_new_inverse = scalery.inverse_transform(y_new),317845.0
107277,464202.0,Yes. So i can use two different scalers right? It isnt necessary that all my numerical variables need to be scaled at one go with the same scaling variable.,312096.0
107277,464190.0,"Yes. But, my initial scaler had around 17 columns. While applying an inverse_transform, I wont have those 17 in my model. so it throws an error. Because the array dimentions wont match if i try to inverse transform price alone. So, in order to over come this, i used a different scalervariable to scale the price column alone and using this scalervariable to inverse transform. Will that have any impact on my model?",312096.0
107277,464334.0,"How to scale down a range of numbers with a known min and max values: Formula along with the program is mentioned in the link: https://stats.stackexchange.com/questions/25894/changing-the-scale-of-a-variable-to-0-100 you can try this, may be helpful.",311117.0
107269,464158.0,Please check if you are creating dummy variables for quantitative variables as they would create hundreds of new variables and may cause problem during RFE.,313826.0
106443,459841.0,"Which variables are showing Nan values in VIF. Make sure you have converted all the categorical variables in to numeric and check the VIF. Because, even with 1 row of data, VIF shouldnt show Nan.",316202.0
106443,459844.0,Yes.,318429.0
106443,459916.0,"It might be showing in ""Fuel type, Aspiration, Door number, Drive wheel or cylinder number"". Take it carefully, can convert in ""1"", ""0"". Need not to delete this, as data cleaning will not be fruitful. Though little lenghty, but refer this link for better understanding: https://www.dataquest.io/blog/machine-learning-preparing-data/",311117.0
106414,459508.0,you should replace or group all those synonymous values into one common name.,301648.0
106414,459517.0,"Hey Bhushan, You can use str.replace to replace the incorrect names with the correct one. Refer - https://learn.upgrad.com/v/course/208/question/106284 Hope this helps.",302742.0
106458,459906.0,How to get correlation between two categorical variable and a categorical variable and continuous variable? Please go through in the link attached: https://datascience.stackexchange.com/questions/893/how-to-get-correlation-between-two-categorical-variable-and-a-categorical-variab,311117.0
106458,459838.0,"Yes, you can visualise it using heatmaps as well. VIF shows the correlation between these independent variables. You need to convert the categorical variable in to integer using dummies and then apply VIF to find the correlation.",316202.0
106458,459834.0,"Yes. After building the model using RFE , you can find the correlation value. Refer code which was taught in sessions",318429.0
106461,459866.0,"While model evaluation, we need to test how well our trained model predicts. So, we draw a scatterlot between y_test and y_predict . I think you are asking for the line fitting the multiple regression plot with the taining data. One way i could think of is plot a scatterplot between X_train, Y_train and overlay a lineplot with parameters you found out in regression. Overlaying two graphs in pandas can be understood with this link. https://stackoverflow.com/questions/42948576/pandas-plot-does-not-overlay",318344.0
106461,461144.0,"Hi, Please follow the attached link https://stats.stackexchange.com/questions/89747/how-to-describe-or-visualize-a-multiple-linear-regression-model https://datascience.stackexchange.com/questions/27740/plotting-multivariate-linear-regression",344894.0
105487,454390.0,create dummy value if possible.,318017.0
105487,454574.0,Create dummies and remove the correlated ones. That would reduce the number. I see that's the only option.,318329.0
105487,454573.0,"Hi Chandan, Use LabelEncoder instance of creating dummy variables.",344894.0
105487,455300.0,My question is Do we really need to use CarName in the model in the first place?,310974.0
105487,457111.0,I think CarName not required. We need to see what factors making cars prices variance. Hope so.,312019.0
105487,458055.0,"Variables with many categorical values can be further classified in less level of categories as per business understanding. For example, If you have 100 college name in the dataset, you can classify them in teir 1, teir2, teir 3 colleges based on their accreditation score. Now you have to deal with only three categories.",318344.0
106499,460036.0,"Hey Jaikrishna, If you'll perform EDA before creating dummy variables you'll gaining a better understanding of data. Like for which categorical variable you can assign binary values and for which you need to create dummy. Hope this helps.",302742.0
106499,460142.0,"Yes, EDA is important and is valuable since it allows us to define and refine the selection of feature variables that will be used for machine learning.",310467.0
106499,460244.0,EDA is necessary to gain the better understanding of the feature variable. Here for this problem you can gain some interesting insights that will help you simplify your model.,301648.0
106501,461369.0,Yes you have to delete because high VIF value indicate Multicollinearity.,314621.0
106501,460041.0,"Yes, P value is different from VIF. If you have high VIF, it means your high VIF variables have more or less the same effect on your Outcome variable. Also, when the VIF values are high you should not be trusting your P-Values as well.",311160.0
106530,460238.0,VIF is 1/(1-R-squared). VIF Infinity means R-squared will be 1 which suggests that the model built with the variable under observation as dependent variable and remaining all as independent variables is very well built and all other variables are able to explain all about the current variable. This you can drop it. but only one at a time and rebuild.,318329.0
106530,460140.0,"It means it has extremely strong relationship with one or more variables. Yes, we need to drop one at a time and check the VIF again to make further calls",311160.0
106530,461349.0,"That variable has high corelation with other variable. Follow below steps while droping variable Build a model containing all variables Check VIF and summary Remove variables with high VIF (&gt;2 generally) and which are insignificant (p&gt;0.05), one by one If the model has variables which have a high VIF and are significant, check and remove other insignificant variables After removing the insignificant variables, the VIFs should decline If some variables still have a high VIF, remove the variable which is relatively less significant Now, variables must be significant. If the number of variables is still high, remove them in order of insignificance until you arrive at a limited number of variables that explain the model well.",314621.0
106528,460239.0,Drop one at a time and rebuild because they keep changing based on the features present.,318329.0
106528,460139.0,"No, we should not be dropping multiple High VIF columns at once. As each variable is highly correlated with the other, there is no point dropping all at once. Need to drop one at a time and check the VIF (would mostly reduce for the rest after dropping one)",311160.0
106528,461142.0,We have to drop one feature at a time n rebuild the regression model and check summary of statistics and VIF s of remaining features. Usually dropping one feature might reduce VIF with respect to others.Then decide on further,308638.0
106528,461268.0,No ..drop one varibale at a time since dropping a vraible will cause changes in vif and p values of other varibales,319759.0
106528,462052.0,no - only drop one at a time because the dropping of one will have an impact on all the other values,300694.0
106537,461140.0,"Hi Keerthi, Generally, LabelEncoder used for ordinal categorical variables. As like No. of cylinder. if we use label encoder for a categorical variable instead of dummy variables for non-ordinal categorical variable, then your model output might decrease, model will assume categorical variable as continues numeric variable.",344894.0
106535,462519.0,There is no particular order for us to assign values for levels so you can drop any,317980.0
106535,460236.0,"It doesn't matter which one you drop. Because there's no particular order for us to assign values for levels. So, you could dropfirst in all cases",318329.0
106535,460261.0,Yes you can drop_first for all the cases as there is no such rule to follow.,306010.0
106535,461315.0,"Although it does not matter for forecasting, it does matter when it comes to interpretation. If one gets a negetive value in the coeffeicent, it would make it more difficult to convey what needs to be done. For example if you drop diesel and find out that fueltype_gas has a negetive coeffient when you build the model, it would be more difficult to convey that the negation of a gasolene engine would be useful rather than that the presence of a diesel engine would be.",319357.0
106535,461345.0,You can use drop_first in all cases.,314621.0
106541,460193.0,"First of all, eliminating 69 variables manually is going to be a tedious process and suggest you to use RFE method to get the variable count down to 15 or 20. Secondly, while eliminating/dropping a variable you need to drop it from the x train DF which you are using to fit the model. Verify the size of it by using .info or .columns to see if the column has been dropped from the desired DF.",311160.0
106541,460190.0,"Hi Hari, I think the reason behind the variables popping up again and again, must be that while calculating VIF you must be dropping those variables but while building the model again you must not be dropping them, please check whether you're dropping them or not. Hope this helps!",301655.0
106541,461339.0,It looks like you are not droping variable. And doing manaul approach will be very tedious process. Try to use RFE methos.,314621.0
106511,461132.0,"Hi Ruchita, Yes, you need to check outlier, If you find the outlier than you should do outlier treatment. Thanks",344894.0
106511,460241.0,"The outliers in the current dataset makes sense as there are companies like BMW, Audi. So, it's not good to impute them in the current case.",318329.0
106519,460088.0,Check the column list in the df X_train_7. This might not the list which you used in the final model creation for lm_7.,320073.0
106519,460145.0,Add the constant back to the X_train_7 to match the shape.,311160.0
106573,460311.0,it would can be more than that it depends on how you determine which variables to be put in dummy.,301648.0
106340,459165.0,You should check for p values and VIF both. Steps are clearly mentioned in the lecture notes. Try to follow these.,317689.0
106340,459318.0,"The reason for them being getting dropped is, RFE is backward elimination technique which is based on AIC may be. during this process, it might find set of other features which explains the highly correlated variables.",318329.0
106765,461097.0,"Yes, you need to drop the variable if it is greater than .05 . If you have any other variable with higher than that value of P then try dropping that variable and re-visit the stats once again.",311160.0
106765,461118.0,"Yes, you should drop that and rebuild your model and test if that makes an impact on other variables' p-value and VIF.",301648.0
106766,461103.0,It might not affect. You are dropping the feature variable only because you know it is not going to impact the Outcome variable. Rest of the Variable's effect would still be retained by the model.,311160.0
106766,461497.0,"I didn't understand yet,",318436.0
106766,461908.0,I have the same query.. if we drop 1 coulmn from 4 column... how will my model know that there existed one more data point (column) ?,318791.0
106766,461771.0,"Hi Karthik, Let's assume two cases: Case 1 : 4 categories with four dummies Regression equation : Y= A1X1 + A2X2+ A3X3 + A4X4 + C Assume &gt;&gt; X1X2X3X4 == 0001 then Y = A4 + C which is a constant = C1 Case 2 : Drop the X4 , 4 categories with 3 dummies Regression equation : Y= A1X1 + A2X2+ A3X3 + C2 Assume &gt;&gt; X1X2X3 == 000 then Y = C2 which is a constant = C3 So from above both the case we find that only constant value changes , which will become fix after trining the model Thanks",344894.0
106766,462890.0,"As suggested by paras, dropping a variable wont affect the outcome since only constant value in the equation changes. Since the outcome has no impact from dropping the variable, for the model ,the existence of such a feature becomes irrelevant.",306736.0
106767,461263.0,Drop the constant before calculating vif all the time. Add it back for recalculating pvalue and repeat the process,311160.0
106767,461773.0,"Hi, it is not required to drop the constant variable.",344894.0
106768,461791.0,"Hi, a) Let's assume two cases: Case 1 : 4 categories with four dummies Regression equation : Y= A1X1 + A2X2+ A3X3 + A4X4 + C Assume &gt;&gt; X1X2X3X4 == 0001 then Y = A4 + C which is a constant = C1 Case 2 : Drop the X4 , 4 categories with 3 dummies Regression equation : Y= A1X1 + A2X2+ A3X3 + C2 Assume &gt;&gt; X1X2X3 == 000 then Y = C2 which is a constant = C3 So from above both the case we find that only constant value changes , which will become fix after trining the model b) Yes, the above equation mean semifurnished and furnished are not significant parameter to determine price of house c) As final model is trained on unfurnished, so while predicting we pass 1 if house is unfurnished otherwise will pass 0. d) Given the category set, it is possible to have multiple variables within the same category set(furnished and semifurnished) be part of the equation Thanks",344894.0
106777,461228.0,"As per problem statement , we have to take ""car company"" for analysis. Hence, you can drop ""car model"" as nothing to do with this here. Please recheck: https://learn.upgrad.com/v/course/208/session/26097/segment/134765",311117.0
106777,461273.0,Yes. car company does affect the price and thus the model,318329.0
106474,459902.0,you can set it for true and false also but you should not use it for diesel and gas as true and false are confusing but 0 and 1 are easy to count but in case of diesel and gas if you assign 0 or 1 to these values you will have to remey what is what .,318017.0
106474,459914.0,"hi, you can create a new column for ex fueltypeGas. then assign 0/1. if it's 1 then it's gas else diesel. however in such an approach we lose the information about the other value from the dataset. for ex a new person seeing 0 in this column will not get clarity immediately that it means diesel. but as mentioned in other answers we can go ahead with this approach for this problem.",311686.0
106474,459939.0,"Yes you can proceed . You can assign 0 for Gas n 1 for Diesel. You can go with this approach for columns having two levels,say aspiration and so on Even you can go with label encoder for all columns having levels",308638.0
106474,461063.0,Just make sure the columns are named appropriately so that you will understand what is what when u look at it later.,312376.0
106474,461090.0,"We can assign 1 ,0 for any variable which is having 2 type of inputs",306738.0
106474,459929.0,"Hi Sudheer, Yes, you can proceed ahead to assign binary values to DIESEL and GAS columns also. As they would also contribute in your model building and predictive analysis. But you'll have to make two different columns for both of these, for your model to make somewhat good predictions. Make one column Diesel where all the diesel values are stored as ones and make other column as Gas where all the gas values are stored as ones. I hope this helps you out.",301655.0
106474,461670.0,better convert it to dummy variable,315679.0
106801,462440.0,"Hi, It means, using given independent variable how good is your model. How much predicting correct or near value to real price.",344894.0
106803,461329.0,if the pvalue of the constant is way to high then you can drop it.,318017.0
106542,460187.0,It is letting you know that your model contains variables with Multicollinearity issue.,311160.0
106814,461330.0,"They would have only 0 or 1 values. So, Min-Max scaling them would leave them with the same values. There's no harm in scaling them through Min-Max",311160.0
106814,461647.0,"Well, You can do Min-Max scaling for dummy variables which are already 0 or 1. Incidentally when I tried they changed to 0.0 and 1.0.",301121.0
106819,461361.0,Applying Labelencoder on variables like CarName could impact your modelling as there are number of distinct values. The algorithm might try finding a pattern between the numbering of the Labelencoder output which in reality doesn't have any.,311160.0
106819,461828.0,"Hi Manasa, Generally, LabelEncoder used for ordinal categorical variables. As like No. of cylinder , you can use LabelEncoder",344894.0
106827,461780.0,I think we should include all the steps that we had to do. Also reason out on what basis we have excluded variables in each step.,318451.0
106829,461481.0,"Absolute values of AIC and BIC will not make any meaning but they are used to compare 2 different models of the same. For example, while arriving at a model you would have gone through the process of making different models and you can see both AIC and BIC for these. Smaller the values of these are better then the bigger ones.",301121.0
106829,461390.0,no its not always necessary but it should be minimal,318017.0
106829,461419.0,What is a good minimal value in the range of 1000s or 100s or 10s?,318084.0
106829,462053.0,AIC and BIC should be used to compare two models with each other - with smaller value of the two models being better,300694.0
106831,461389.0,go through this : http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients,318017.0
106861,461844.0,"Hi Rajat, You can drop any variable, not necessary first variable",344894.0
106861,461464.0,"Hey Rajat, Yes Ideally you should. The drop_first=True drops one column from the resulted dummy features. The purpose is to avoid multicollinearity.",302742.0
106861,461518.0,"After creating dummy variable you need to delete the first one so that you have required number of dummy variables ( 0 in all existing columns will point the presence of the first column which was deleted) Now that we have already transformed the original categorical value in to dummy variables, we need to delete the original variable. If you keep the original variable, You would end up getting error while using RFE.",301121.0
106861,461531.0,Yes you should. You can use drop_first = True.,317689.0
106863,461569.0,It is fine. Since multicollinearity is more we need to continue to update model after creating first model from RFE Ensuring VIF and p value in acceptable range is very important irrespective of number of times you updated model,308638.0
106863,461457.0,"There is no particular number of models to be created. You could limit the features to 15, 20, 25 after RFE, whatever you feel is right. And then, you can start building the models manually by dropping the features one by one. The final count, we cannot know in advance because after which model, all the coefficients will be significant and VIF's will be less than 5.",318329.0
106863,461818.0,There is no limit on how many modules you create. I completely depends on how far you want to go in model building. It is also not a mandate to use RFE you can create a all varaible model and reduce them one by one anything as per your understanding and wish. At the end the model should stand the test of p value and VIF.,318451.0
106863,463195.0,No limit on the number of models. We should check the p value. If all variables have p value less than 0.05 then check VIF for auto correlattion. VIF &lt; 5. We can drop variables based on these conditions.,301643.0
106863,463409.0,ok let's be clear here - he is talking about creating 30-40 MODELS not a model with 30-40 features. After 5 models (where you have all features being significant (all p&lt;0.05) and minimal multocollinearity (VIF &lt;5) one would be too tired to create anymore and/or you might find that there are only so many unique models you can create with 60 variables - after all you generally drop about 40 features via RFE anyway - so we are talking about perhaps 20 variables. Ofcourse up to you how many models you want to create but one has to ask themselves what is the value add after maybe 5-10 models.,300694.0
106440,459847.0,"Don't drop these variables. Include them in your model and then check the model stats and VIF before dropping the independent variables. Or entries with dohcv and rotor replace with new name like 'Other_cat', so Other_cat count will become 5.",316202.0
105824,456624.0,"Yes, that's what it means. You can discard the car model data.",310974.0
105824,456626.0,"Yes, you are right. We need to consider only the Car Name and ignore the model",316202.0
105824,456689.0,it means that we need to consider only company name of the car for analysis and not the full name. for example if it is 'audi 100ls' then only 'audi' should be considered in the model. requesting TA to clarify this point as i can see my view is different from others' answers.,311686.0
106828,461539.0,You should remove constant while calculating VIF. Add back the constant while creating another model using the previous models variables.,317689.0
106828,461600.0,"Constant is nothing but a n intercept in y-axis. A model will have a constant, but when we are in the process of finding out a model by adding or dropping variable (due to p-value or vif), constant will vary. Since we do not want constant to interfere with the calculation of VIF, we remove the constant field temporarily while calculating the VIF and add them again for other analysis as well as to arrive at the final equation. I presume this clarifies,",301121.0
106866,462431.0,"Hi, Firstly, this data will not follow the Linear regression assumption, So it will not make sense to fit linear regression this data. Secondly, If you want to fit Linear regression model on this data, then the error will be same for every regression line passing through origin",344894.0
106866,463107.0,What assumptions it'll not follow?,317070.0
106425,459649.0,You have to provide the number of features to retain as parameter to RFE. you might have given 10. Give a higher number let's say 15 or 20. you will be good to go,318329.0
106425,459662.0,RFS is all about to select TOP N Features. So You should have passed the Number as a parameter while building your model.,301648.0
106425,459716.0,"# Running RFE with the output number of the variable equal to n lm = LinearRegression() lm.fit(X_train, y_train) rfe = RFE(lm, n) # running RFE rfe = rfe.fit(X_train, y_train) In the above code replace n with your desired number of variables that you want to retain and then perform the manual elimination based on p value and VIF.",318451.0
106425,461379.0,You can change value of n to get more variables.,314621.0
106876,461814.0,Company Name variable has to incuded in the analysis as the companies which will come in the final model will be direct competition to Geely and they have to study these companies product offerings and come up with their own.,318451.0
106876,461630.0,"Hi JaiKrishna, Have you extracted the Car-make getting rid of car's model? If not, you have to do that first. Even though 'Geely' will not manufacture Audi or BMW. The cars they would manufacture will be based on the features which have impact on the price of the car. But incidentally even the brand name has quite a significant impact on the price of the car and hence can not be omitted completely. Modelling has to be completed with this scenario and see if Car-Make exist as a factor on the final equation. I request TA clarifies on thsi point.",301121.0
106045,457738.0,"Yes, Kindly check if CarPrice.CarName is getting you the results. If it throws key error, try loading the dataset again and re-run the code",311160.0
106045,457618.0,did u run this command twice.it seems that the carname is alreadydropped,317982.0
106045,457963.0,carname already dropped. the above carname command you executed twice,306996.0
106045,458063.0,Reload data set again and then try to run this command.,314621.0
106045,458911.0,It seems like you have run your code twice. It is already dropped. In order to check reload the dataset and try running above snippet again.,317689.0
106881,461609.0,"First you must drop the variables which are having high VIF and then p-values more than 0.05 one by one so that all are within 0.05. Again once you drop a variable based on VIF, you have to go back to previous summary to find the values of p-values, Please note that VIF is a figure which tells you particular variable's dependency on rest of the variable selected for modelling.",301121.0
106881,461853.0,First go for high p value more than 0.05 and High VIF i.e. mor e than 5. After you are done with elimination based on combination check for individual if only p value is high or vice versa and drop varaible one by one.,318451.0
106881,462797.0,Once we are done with elimination of variables there is const column which if we are removing at end is increasing the VIF. How shall we proceed?,320103.0
107134,463032.0,either add constant into the df or else remove from the other data frame. constant is extra column.,318017.0
107129,463026.0,Check the number of varible selected X_test_new and ls . In ls there should be one extra variable called const (this is nothing but intercept) You can verify the number ovarible in ls it should in ls.summary.,307843.0
107129,463058.0,you have probably droped the constant in one df but not the other - drop them in both the DFs and it should be fine,300694.0
107129,464335.0,"This is due to the number of columns mismatch(mainly const), try building the model again.",318448.0
107176,463300.0,"Hi, Hope you have followed all the steps properly like data cleaning ,columns Mapping. You have to drop the column one by one. Not all the columns at a time . Drop one column and again regenrate the module . Likewise do it one by one. you will definitly get the result at one point. Thanks!",319006.0
107176,463297.0,"yes you have to continuously remove features wth high P values - then when no more insignificant features, you need to check VIF values; if you drop a column due to high VIF values then you need to go back and re-generate your model (OLS regression) and re-check for high P values",300694.0
107176,464327.0,"Yes it happens sometimes, p-value changes after removing columns with high VIF. As you need to repeat the steps and remove all those columns with high p-value(&gt;0.05) and high VIF(&gt;5) and final model should not have those columns. Make sure removing the columns one by one not all together.",318448.0
107176,463839.0,"Step 1. Generate a model. Step 2. Check if there is any variable wuth p value &gt; 0.05. If yes, then drop that variable and go back to Step 1 i.e, Generate Model and monitor R-square and Adjusted R-Square. Step 3. Check if there is any variable with high VIF ( &gt; 5 usually is considered as high). If yes, then drop that variable and go back to Step 1 i.e, Generate Model and monitor R-square and Adjusted R-Square. Step 4. Keep repeating Steps 1 through 3 until you have a set of variables in permissible range wrt p-value and VIF.",313826.0
110716,476660.0,"PCA is not a clustering method, but sometimes it helps to reveal clusters. You can use PCA for dimensionality reduction as a feature extractor, and to visualize the clusters. Doing PCA before clustering analysis is also useful for dimensionality reduction as a feature extractor and visualize / reveal clusters. Doing PCA after clustering can validate the clustering algorithm. For more details, please refer the link: https://stats.stackexchange.com/questions/157621/how-would-pca-help-with-a-k-means-clustering-analysis https://www.researchgate.net/post/Which_would_you_use_first_K-Means_Clustering_or_Principal_Component_Analysis Hope it clarifies you.",311117.0
128992,563389.0,pyspark is basically python spark but for using it you might have to use sql type language hiveql and in place of r for us it will be python.,318017.0
128992,563433.0,SQL like structure would be used to write Hive queries and Python would be used along with Spark which is known as PySpark to process large data set,308673.0
128360,560304.0,"'Report an error' option on the bottom left corner is the best solution in such cases. I've tried it once, the team responds really fast.",305655.0
128414,560495.0,"These are domain related information Technologies Servises and knowledge that company establish for their business via IT, like ERP is Enterprise Resourse Planning and CRM is Customer Relation Management. Many Industry like working in Manufacting will choose the ERP as it comes as packge like module having feature Inventory,Purchasing,Paybles etc all package and IT modules comes so that it is easy for business set up using IT Similary CRM is Customer Relationship Management is used by Banking,Telemarket etc. These two are nothing in short a Information Technology Services Package which has specific domain industry support servises infrastructure.",307843.0
128414,560830.0,Hi Manasa,301121.0
128414,560842.0,"Hi Manasa I pressed the ‘Submit’ button by mistake before I entered the answer. SAP is one of the most popular ERP packages you would have heard of which is being used by most of the Fortune 500 companies. Salesforce is one of the most popular CRM packages for customer relationship management. Apart from what is mentioned in the above answer, SAP is also used as CMMS which is computerized maintenance management system. In our company we use both these packages extensively.",301121.0
127816,557723.0,"Big data ingestion is about moving data - especially unstructured data - from where it is originated, into a system where it can be stored and analyzed such as Hadoop or Spark . Data ingestion may be continuous or asynchronous, real-time or batched or both (lambda architecture) depending upon the characteristics of the source and the destination. In many scenarios, the source and the destination may not have the same data timing, format or protocol and will require some type of transformation or conversion to be usable by the destination system. We have dsitribution like Hortonworks which provides tool like Hrotonworks data flow stack which helps in data ingetion process .It you looks internal is uses varies open source technologies like Achace NiFI,Apache kaffka ,Apache Strom ect based on the ingestion requirments. Below links will help give you a overview on the data ingestion stack : https://www.predictiveanalyticstoday.com/data-ingestion-tools/ https://www.xenonstack.com/blog/ingestion-processing-big-data-iot-stream/",318476.0
127816,557959.0,Needless to mention that we also have hadoop native copy command to transfer large files to HDFS.. which can even be script'ed and cron'ed for automating the ingestion,301116.0
127816,557858.0,"In Big Data context there are tools which are used for sourcing data from its actual source to the Big Data ecosystem for processing. These tools are also called as data ingestion tools. Data ingest tools for BIG data ecosystems are classified into the following blocks: Apache Nifi : An ETL tool that takes care of loading data from different sources, passes it through a process flow for treatment, and dumps it into another source. Apache Sqoop : Bidirectional data transfer between Hadoop and an SQL databases (structured data) Apache Flume : System of ingesting semi-structured or unstructured data in streaming on HDFS or Hbase. On the other hand there are messaging systems with own ingest functions, such as: Apache KAFKA : Message intermediation system based on the publisher/subscriber model. RabbitMQ : Message Queuing System (MQ) that acts as a middleware between producers and consumers. Amazon Kinesis : Kafka’s counterpart to the Amazon Web Services infrastructure . Microsoft Azure Event Hubs : Kafka’s counterpart to the Microsoft Azure infrastructure . Google Pub/Sub : Kafka’s counterpart to Google Cloud infrastructure . Source: http://www.diegocalvo.es/en/big-data-data-ingestion-tools/ You can also visit below link for sources of big data and data ingestion https://www.allerin.com/blog/top-5-sources-of-big-data https://hortonworks.com/solutions/data-ingestion/ Hope this will help.",317991.0
127966,558408.0,"Veracity comes into picture when the data is unstructured and semi-structured. Whereas when you talk about relational databases they always deal with structured data and mostly the data is clean and already fed through many quality checks, so very little scope for veracity handling. Although in traditional Data Warehousing Techniques data cleaning is also catered and mostly the bad data is rejected as it is very small in volume and there isnt much insight to be gathered from it.",317689.0
127966,558426.0,"When we take veracity into account is better to look from a data warehouse prospective rather that from a releational database. By definition of veracity it's the unstructured data contains a significant amount of uncertain and imprecise data. For example, social media data is inherently uncertain Traditional data warehouse / business intelligence (DW/BI) architecture assumes certain and precise data pursuant to unreasonably large amounts of human capital spent on data preparation, ETL/ELT and master data management.",318476.0
127966,558968.0,"Veracity deals with usefullness of data.. in traditional systems we are aware of the data and its quality.. in traditional approach we have sense of what data will be usefull and what will not be.. it will be mostly structured whereas in big data scenario we can use variety of data, with sheer power of system we can drive useful conculsions from previously though useless data or unstructured data",318791.0
128359,560290.0,Student Mentor confirmed it on WhatsApp group that further modules will be added that will also include Assignments. They involve familiarization of cloud so most probably we will get some hands on experience on our laptops.,305655.0
128359,560323.0,"As per the course calendar we will have 2 assignment one on Big Data Storage Processing using Hive and other will be using Spark. We cannot run entire Hadoop on our laptop, so may be Upgrad and IIITB will provide some kind of access to cloud platform or something like that. Rest TA can confirm.",317991.0
128359,560723.0,"Yes, you will get assignment and access to cloud platform.",329936.0
132767,579572.0,"Hi Bindu, the VectorAssembler Transformer does not take strings. So you need to make sure that your columns match numerical, boolean, vector types. Make sure that none of the columns has StringType. To convert a column in a Spark DataFrame to another type, make it simple and use the cast() DSL function like so: val analysisData = dataframe.withColumn(""Event"", dataframe(""Event"").cast(DoubleType)) you should use StringIndexer , maybe together with OneHotEncoder . You can find more information on these two in the linked documentation.",329936.0
99634,426673.0,1) Decrese the winning amount. 2) Increase the loosing amount. 3) Reduce the probibility of winining by indroducing more blue ball or reducing red balls etc.,318368.0
99634,426450.0,You can decrease the winning amount or can introduce the more balls of different colout so that the chances of getting red balls will decrease.,318851.0
97447,417084.0,"Hi, Can you please provide the video URL and time at which this is mentioned.",320073.0
97447,417097.0,"In the video, each of 75 Upgrad folks have been asked to pick 4 balls as part of the experiment. It was shown that only 2 folks out of 75 have picked all the balls of color Blue. Accordingly, P(getting 4 balls) = 2/75 = .027.",311160.0
97447,419794.0,This is computed as 2/5 *2/5 * 2/5 *2/5 = 16/625 = 0.026 (this is the probability) 0.027 is the experimental value (2/75) which is close to the probable value,318344.0
97447,420094.0,"I agree with the question... In the video they said they are putting 2 blue balls and 3 red balls. so how can there be 4 blue ball, this is the doubt even I have....",301113.0
97447,420111.0,"The experiment is conducted in the following way: A ball is taken from the bag, its colour is noted down and then it is replaced back in the bag before the next trial. This continues for 4 trials and hence it is very much possible to get the outcomes of 4 red balls or 4 blue balls. If the ball wasn't replaced back, then it would be impossible to get 4 red balls or 4 blue balls as an outcome. Please check the FAQs for the Random variables segment. And since this is an experimental probability calculation, we just divide the number of times the given situation has occurred over the total number of trials to get the required value.",313517.0
97447,420259.0,"For your question ""Where is the representation of P(getting 4 blue balls) ? I don't think it was present in the video, how was it derived?"" P(X=0) is the answer. Probabilty of getting zero red balls is the same as probability of getting 4 blue balls.",300717.0
97447,420388.0,"0 red balls will, in turn, mean 4 blue balls. Hence the probability of getting 0 red balls will also be the probability of getting 4 blue balls.",319302.0
98317,418321.0,"We have to think in terms of the experiment conducted in case of experimental probability. When you conduct an experiment, the probability distribution will be taken from the experimental data. Another way finding distribution without experiment is binomial distribution which is in the next session of this module where we discuss about theoretical probability and how close experimental and theoretical probabilities can get. In short, you are talking about theoretical probability.",310974.0
98317,419312.0,Even i had the same confusion.,318433.0
98317,419781.0,These events are not equally likely so we can not use the formula P = Favourable cases/ Total no of cases the probabilty of X=1 (one red ball ) will be calculated as 4 * (2/5 * 2/5 * 2/5 * 3/5) = 15.3 %,318344.0
98317,420140.0,"Probability checks on possible outcome came on experiment . Here, total number of outcome is 75. 4 red balls picks 10 times in an experiment. so probability for 4 red balls is 0.133. Experiment should be repeatable procedure with a set of possible results.",304397.0
98317,420519.0,This is perhaps covered in the next module - Discrete Probability Distibutions,319302.0
98317,420749.0,Thanks everyone for the responses. It is clear now. I was mixing Probability of an events and Probability distribution,304814.0
97288,416434.0,"Hi Likayat, X = 2 means the probability of the number of people who got two balls out of four attempts. If the random variable is considered to be the picking of the balls itself, then it would have been 6/16 which is not the case.",318329.0
97288,416443.0,,304813.0
97288,416452.0,"26/75 is because 26 people among 75 people got 2 red balls.(This is experimental probability distribution and is not close to calculated probability, the more time the experiment is performed the closer it gets to the Binomial probability distribution, which is the calculated probability considering the experiment is performed for infinite times) And the reason why it can't be 6/16 is simple. The probability of getting a red ball in one trial = 3/4 The probability of getting a red ball in one trial = 1/4 Therefore, probability of 2 red and 2 blue irrespective of order = 3/4 x 3/4 x 1/4 x 1/4 =9/256 Probability Considering the order = 6x 9/256 = 54/256 = 0.2109 0.2109 is the binomial probability of getting two red balls after 4 trials",312490.0
97288,416453.0,"26/75 is because 26 people among 75 people got 2 red balls.(This is experimental probability distribution and is not close to calculated probability, the more time the experiment is performed the closer it gets to the Binomial probability distribution, which is the calculated probability considering the experiment is performed for infinite times) And the reason why it can't be 6/16 is simple. The probability of getting a red ball in one trial = 3/5 The probability of getting a red ball in one trial = 1/5 Therefore, probability of 2 red and 2 blue irrespective of order = 3/5 x 3/5 x 2/5 x 2/5 =36/625 Probability Considering the order = 6x 36/625 = 216/625 = 0.3456 0.3456 is the binomial probability of getting two red balls after 4 trials Note: Sorry, my previous response considered 4 balls, instead of 5",312490.0
97288,419788.0,"I find a lot of people having similar doubts, Okay first we need to understand why it is not 6/16. we all have studied (mugged) a formula for probability : P = No of favourable cases / Total no of cases what we forget is, it is only applicable for equally likely events. Suppose you go home and try to open your door lock with keys, as per your argument there are only two possibilities either it will open or not. so. P(lock will open) = 1/2 But practically the probability of lock opening is almost 1. Why this paradox, Haha, this is because these two events are not equally likely. So always before applying the formula first see if cases are equally likely. Drawing red balls is easier than drawing blue balls because there are 3 red balls and 2 blue balls, so all the events are not equally likely. for calculating exact probability you must learn the basics of probability and permutation and combinations and grouping. then you can understand the answers above this one.",318344.0
97288,416534.0,"sir, probability we can find in two ways, 1. mathematical approach 2. practical approach ( which we get from experiment observations) In this case we are doing, practical approach which is also called observed frequency distribution and we are calculating probabilities, i.e. 75 people they involved in the experiment based on that records we are saying probability which is (number of people chosen 2 balls=26) / (total number of people=75) that's why probability is 0.35(approx.) in mathematics approach, when there are two colours (3 red and 2 blue balls) and we are choosing 4 as sample and we are doing this process with replace and one after another ball, based on this, we can say the maximum number of possible cases is 4^2 =16 (i.e here 4 is choosing(sample size), 2 indicates no of colours) using binomial distribution p(r)=( nCr * p^r * q^(n-r)), let r represents number of red balls, p is probability of getting red ball=1/2 , q is probability of not getting red ball=1/2, n is number of balls every individual sample = choosing 4 balls. then p(getting 0 red balls)=p(r=0)= 4C0*(1/2)^0 *(1/2)^4-0 = 1 p(getting 1 red ball) = p(r=1)= 4C1*(0.5)^1 * (0.5)^(4-1) =0.25 (i.e 4/16 which represent with one red ball we can maximum 4 combination's like RBBB, BRBB, BBRB, BBBR) similarly for other possibilities of red balls also. probability ( event ) = 1 then the event is said to be 'certain event ' probability ( event)=0 then the event is said to be 'Impossible event'",318322.0
101244,435692.0,"if he/she looses the game, the money (10 rs) he/she put in to play a game will be lost and hence your lowest value is -10.. if he/she wins a game, highest value is 150.. Hence the range in which your X can lie is -10 to 150..",316349.0
98315,418365.0,Probability is the measure of the likelihood that an event will occur Probability Distribution is the distribution curve plotted of the probabilities of the random variables VS the random variables see the below link you will get an answer https://www.quora.com/What-is-the-difference-between-probability-and-probability-distribution https://www.kean.edu/~fosborne/bstat/04prob.html,318461.0
98315,419773.0,"As you have mentioned, 16 cases are there, correct. But the point is each case is not equally likely. Let's understand how, by taking an example There are two cases 4 red balls and 4 blue balls. according to you both should have 6% probability. But now think there are 3 red balls in the bag whereas only 2 blue balls so getting 4 red balls should be easier than getting 4 blue balls. So, you now must have understood that all these events are not equally likely. To find the exact probability of 4 red ball we have to multiply the probability of each time red balls is drawn that is: 3/5 * 3/5 * 3/5 * 3/5 = 81/625 That is equal to 13%. You need to learn basics about probability if you still find it confusing.",318344.0
98315,420106.0,"Probability Distribution is the distribution curve plotted of the probabilities of the random variables VS the random variables. It is useful because if you know the pdf ( probability distribution function) of an event, then you can predict the outcome an event.",314183.0
98315,420137.0,Probability is possible outcome of an experiment by total number of possible outcomes of an same experiment. Probability Distribution is distributing probalities of the random variables. Sum of all distributing probabilites of random variable should be 1.,304397.0
98315,420723.0,These are the possible combinations but the probabilities of each event occuring is different.,304319.0
98315,422282.0,Probability is the measure of an event will occur Probability Distribution form of representation that tells us the probability for all possible values of event. https://learn.upgrad.com/v/course/208/session/18002/segment/91566,310952.0
98660,,nan,
99097,422016.0,No Needs to Draw bar graph SImply follow the steps 1. Remove the % from Recovery Column or you can create a new column with value same as in recovery column but without %(NewRecoverycolumn) 2. Then apply the formula on new column(EA) = Exposure at Default*Probability of Default*(1-NewRecoverycolumn/100) 3. Then take the sum of new column(EA),305651.0
99097,422491.0,Just create a column to calculate Exposure * probablity * (1-recovery); No need of dividing by 100 as it has percentage sign. =RC[-4]*RC[-2]*(1-RC[-3]) Sum up this column and you will get the desired answer in lakhs but please note that choice is in crores,301121.0
98706,,nan,
98734,420581.0,yes you can find out the total number of outcome or total sample space with 2^n. But this will give only the total in number.,301648.0
98734,420672.0,this is related to Permutations with Repitition In Permutations with Repitition use n^r to calculate total number of outcome(Possible Outcome). Here n = different types r = number of times(number of count). Here there is two balls Red or Blue so n = 2 and Number of times we pick the balls is 4 so r=4 Hense possible outcomes = n^r = 2^4 = 2*2*2*2 = 16,310952.0
98734,420582.0,"As it falls under Permutations with Repitition, we can use n^r to calculate the total possible outcomes. Where n = different types present and r = number of times you choose them. In our case, n = 2 (Red or Blue) r= 4 (Number of times we pick the balls) Therefore, 2^4 = 16",311160.0
98734,420734.0,"Permutation and Combination is more about intuition than formula. In the example, the ball has to be taken out 4 times and each time the ball is taken out, the colour is noted and the ball is replaced back into the bag. So for each draw, there are 2 choices of the outcome, either red ball or blue ball and this is true for all the 4 cases. So the total number of outcomes 2*2*2*2 = 16 The multiplication is done here because for each outcome in 1st draw there are 2 choices for second outcome and for each outcome in 2nd draw there are 2 choices for the 3rd draw and so on and so forth. Now, for ease of calculation we can convert this intuition to a formula, for a outcome having n choices and an experiment having r repitition the total possible outcomes will be n^r. However, I would recommend to understand the intuition rather than finding out the formula.",318576.0
99148,422335.0,"That is the part of Probablity winning cost calculations.. To imagine it in the other way; 'you'll earn the max number of times the money betted on it' so, 36 is the max number and you betted 100 pounds gives you the winning cost as 3600 pounds whereas your net winning = 3600 - 100 = 3500 100 poiund is the amount you've paid to play the game which needs to be subtracted to get the net winning..",316349.0
99148,422441.0,"The winning probability of this game for single number is 1/36. i.e. If you net some X amount on any number then you will get X * 36. For Exa. If you bet $100 , then it winning amout will be 100*36 = 3600. As this winning amount is included your bet amount so we need to subtract that from winning amount i.e. 3600 - 100 = 3500 ( Which is the actual winning amount) . 1 2 3 4 5 6 7 8 9 If you bet $100 on two adjucent numbers like (5&amp;6 or 4&amp;5 or 5&amp;8 etc) then your winning probability will be 2/36 i.e. 1/18 so you will be then amoung 100 * 18 = 1800 1800 - 100 (your bet amount) = 1700 (actual winning amount) like wise if you are betting with group of number then winning probability will be change Exa. if you bet on 5,6,8,9 or 4,5,7,8 then winning probability will be 4/36 i.e. 1/9 so actual Winning amount will be: [ 100 * 9 - (bet amount) ] i.e. (100*9) - 100 = 800 an so on..",304812.0
99359,423945.0,"When you have equal number of red balls and blue balls, then probability of getting blue balls or red balls (ie. No Red and No Blue ball) is same. Else it will be different. probability is defined as = (Number of preferable outcomes)/(Number of total outcomes) So if number of red and blue balls are NOT EQUAL in basket, then prob of getting No red ball p(No red)= (Total number of blue balls in basket )/ (Total number of balls in basket) prob of getting No blue ball p(No blue)= (Total number of red balls in basket )/ (Total number of balls in basket) Hence p(no red) and p(no blue) are different. If you are repeating this experiment 4 times - prob of getting No red ball = p(No red) * p(No red ) * . p(No red) * p(No red)",318458.0
100950,433633.0,"There are already a few posts on the discussion forum. Kindly check the following. If you still have a doubt after that, please ask. https://learn.upgrad.com/v/course/208/question/99920 https://learn.upgrad.com/v/course/208/question/99736 Also, check this link: https://www.statisticshowto.datasciencecentral.com/type-i-and-type-ii-errors-definition-examples/",313517.0
99201,424532.0,Try and complete them as there are really good practice questions and few concepts on Bay'es theorem and sampling. This wlll help you in exams.,318368.0
99201,422717.0,"Facing time issue too due to office committments. I think we are all on the same boat. However, Skipping these modules would not be a good idea .Since these modules are optional so just ensure that you can skim through them some other time before we move on to the next course.",311254.0
99201,423026.0,"I would recommend to finish those module also as it contains some useful concepts like types of sampling methods, conditional probability, Bay'es theorem. In future there is possibility that you require these concepts for analysis. Now as far as time is concern try to finish it anyhow, if not before the deadline. Hope this will help.",317991.0
99201,424872.0,yes bro.. apart from probability which will be useful.. the sampling methods may be useful for future projects too,302735.0
99201,432702.0,Yes. They are in more details than the actual sessions to be honest. They would boost our knowledge to a very good level. Please read them as soon as you find time. I am sure reading them would provide you good knowledge for our upcoming exams and graded submissions as well. Hope this helps. Best of luck!,310217.0
103925,447247.0,"At a sample size of 30, the sample distributions starts to resemble a normal curve. However, just because the sample distribution is normal is a not a sufficient condition to represent any population. Let's take an example: say you want to measure mean temperature in India on 1/1/2019. You go to 30 villages in Rajasthan and collect temperature data. The distribution of this sample would be normal, but we cannot say that the average and standard deviation is representative of that of whole of India. You can go to 300 or 3000 or all 44,795 villages in Rajasthan and do the same exercise- the sample would surely be normally distributed and yet not useful to determine temperature on 1 Jan 2019 in India. The key assumption when someone says that sample size of 30 can represent a much larger population size is that the make-up of the sample is roughly similar to that of the population. So, if one picks up random samples from locations across India, then at sample size of 30, with certain (not very high) confidence level, one can say that the sample distribution represents population distribution. As you increase the number of random samples, the probability that your sample looks like the population keep going up. Beyond a point, it starts to plateau with limited marginal gain per added sample. For example, for a confidence level of 95% and confidence index of 5%, sample size plateau at a sample size of 385 (you can check here: https://www.calculator.net/sample-size-calculator.html?type=1&amp;cl=95&amp;ci=5&amp;pp=50&amp;ps=30000000000000&amp;x=108&amp;y=14 )",305653.0
103925,446533.0,n = 30 indicates that your sample is normal distribution and so the properties of normal distribution apply. But if you want your analysis to be correct and want the sample to define your true population. Than you have to make sure your sample is sufficient for the population you are testing for. So its subjective in nature. e.g. A sample of 30 is justifiable if you are predicting for a population of 300. But a sample of 30 is not that good when you want to do it for a population of 3 lacs or 3 crores. The more you increase the sample the less error you get. You can do analysis on a sample of 30 in correspond to a population of 3 lacs but you have to keep in mind the error of your analysis on this sample will be large. It’s basically a tradeoff whether you want to control the error or increase the sample size. So there is no fixed ratio in which you take sample in correspond to the population. Many a times business faces scenarios such as increasing the sample size is costly affair so they do the analysis keeping in mind that the error will be high.,318451.0
106919,461782.0,Check the column list in the X_train_lm and see if those are the one which you used to train your model lm and add the constant also.,320073.0
99248,423022.0,"The area percentage (proportion, probability) calculated using a z -score will be a decimal value between 0 and 1, and will appear in a Z -Score Table . ... A Z -Score Table , is a table that shows the percentage of values (or area percentage) to the left of a given z -score on a standard normal distribution.",318017.0
99248,423027.0,"The values in Standard Z-table are calculated using the formula given in this segment just below the table image. https://learn.upgrad.com/v/course/208/session/18004/segment/91584 Basically, since we know the mean (0) and standard deviation (1) of a standard normal distribution, we can calculate the area under the curve for the normal PDF to get the probability. If you are familiar with basic calculus, this area calculation is the same as integrating it between two values. That is how the formula in the above segment goes by in calculating the Z-values. Once the cumulative probability has been calculated, the probability between 2 different Z-values can also be easily found out by subtracting one from another.",313517.0
99248,432700.0,You can read about this and more at this link: http://www.z-table.com/,310217.0
99241,423014.0,lets take example of exit polls : suppose in Rajasthan out of 250 seats the data collected shows that cng will win 120 + or - seats so what is this + or - to find this out we consider the data which should be greater then 30 will behave like a uniform distribution in which mean is sum/ count of values. we can also find the standard deviation of this with the help of these two we can find the z score which is error margin in our case,318017.0
99241,423025.0,Could you connect the same example with confidence level,306734.0
99241,424836.0,"For example, when you find that the z-score for a given observation equals 1.96, you know that the probability of finding a value at or above that value is .025 and below is 1-.025 = .975). So, using our tables , we know that 95% of a normal distribution falls within 1.96 standard deviations of the mean.",314183.0
99241,423083.0,"Z-score is the value associated with a particular probability whereas Z* is the Z-score associated with a particular confidence level. If someone asks, what is the Z-score associated with 0.95 or 95% probability, you just have to look up the table and find the corresponding value associated with it. In this case, it would come up as 1.65(or 1.645 to be more exact). However if someone asks you what is the Z* for 95% confidence level, then first you need to find the corresponding probability associated with that confidence level. This can be easily calculated with the following formula: if y is the confidence level in percentage then the probability is given as y/100 +(1-y/100)/2. Thus for y=95, we have the probability as 95/100+(1-95/100)/2= 0.975. And the Z-score associated with 0.975 probability is 1.96. Please check the FAQ given in the page for the estimation using CLT segment.",313517.0
99471,425205.0,"Hi, Z-table gives cumulative probability till that particular Z-score. Suppose your Z-score is 2.0. By Z-table you will find the coresponding value as .9772. It means that 97.72% of the total values will lie below (or at) the Z-score of 2.0. Now if the question is P(Z&gt;2.0) then it will be 1-0.9772 which is 1-P(Z&lt;=p) only. Hope I understood your question correctly and could answer your query. You can also understand this by seeing the diagram of the Z-table PDF in the link pasted below. http://users.stat.ufl.edu/~athienit/Tables/Ztable.pdf",311686.0
99428,424902.0,Probablity has total outcomes in the denominator and hence you add both probablities in denominator.. whereas numerator has possible outcomes only..,316349.0
99428,424901.0,"Apurva think of the example in this way. When we calculate probability, we find the favorable outcome(A) and total outcome(B). Based on this the probability P = A / B. Now in the example, we have been asked to find the green ball for first bag. So, our A = selecting bag 1 probability * selecting green balls from bag 1 and our B = total cases i.e. ( selecting bag 1 * selecting green ball from 1 + selecting bag 2 * selecting green ball from 2. Anyways, we are just considering favourable outcomes to total outcome. If we dont consider the bag 2 case, then the probability will be 1, and that is highly improbable. Hope this helps",302735.0
99566,426023.0,Please go through below link to get more insight for your query https://math.stackexchange.com/questions/941150/what-is-the-difference-between-independent-and-mutually-exclusive-events Hope this will help.,317991.0
99566,425902.0,"I will try to explain will a concise example: Event : Flipping a coin twice for 4 times. Now for each individual pair of flips(for every event) : The flip 1 and flip 2 are mutually exclusive. i.e., they cannot occur at the same time. And each of the event(two flips): E1, E2, E3, E4 are independant of each other. In more simple words, single coin when flipped twice is mutually exclusive, where two coins are flipped at the same time then their results independent..",305845.0
99566,426042.0,"Mutually exclusive : Occurence of an event precludes occurence of the other. Eg. if you get a 1 on the dice excludes the possibles of getting a 2 on the same roll. Hence 1 and 2 (and 3,4,5,6) are mutually exclusive Independent : Occurence of an event has no bearing or influence on the other. Eg. if you get 1 on the first roll of the dice, it does not change or influence the probability (1/6) of getting a 2 in the second roll To dissect your example: - ""What will happen if we roll a non biased coin. There are two events H or T. One event does not affect other. "" : Actually, they do. Getting a Head precludes any chance of getting a Tail and hence are mutually exclusive - ""Event- Head in two successive flips. Is this event mutually exclusice?"": No. If you get a Head on the first flip, you still have a 50-50 chance of getting a Head or a Tail. Neither possibility (H or T) is precluded due to the first Head. Hence the two flips are not mutually exclusive. As getting a Head in the first flip, does not, in any way influence the probability of getting a Head in the second flip, they are independent",305653.0
99566,427776.0,"Let me try to explain it here: Mutually Exclusive event--&gt; If we toss a coin, either we get head or tail, if we get head we do not get tail and vice versa. Here getting head and getting tail is mutually exclusive event. P(H) = 0.5 P(T) = 0.5 and P(H intesection T) = 0. Event of getting head and event of getting tail are mutually exclusive event. Independent event--&gt; When we do two trails of flipping the coin, we can get any thing on first trail H or T, and we do not have any bearing on the second trail it may also get H or T. Therfore the first trail and second trail are independent events. P(getting H in second trail when we get H in first trail) = P(getting H in second trail) =0.5 only. Hope this will clear the doubt!",318585.0
99566,426654.0,"Key Difference Between Mutually Exclusive and Independent Events The significant differences between mutually exclusive and independent events are elaborated as under: Mutually exclusive events are those events when their occurrence is not simultaneous. When the occurrence of one event cannot control the occurrence of other, such events are called independent event. In mutually exclusive events, the occurrence of one event will result in the non-occurrence of the other. Conversely, in independent events, occurrence of one event will have no influence on the occurrence of the other. Mutually exclusive events are represented mathematically as P(A and B) = 0 while independent events are represented as P (A and B) = P(A) P(B). In a Venn diagram, the sets do not overlap each other, in the case of mutually exclusive events while if we talk about independent events the sets overlap. Get more on the link below: https://keydifferences.com/difference-between-mutually-exclusive-and-independent-events.html",318476.0
99566,432698.0,"Mutually exclusive events cannot happen at the same time. For example: when tossing a coin, the result can either be heads or tails but cannot be both. Events are independent if the occurrence of one event does not influence (and is not influenced by) the occurrence of the other(s). You can read more about this in : https://math.stackexchange.com/questions/941150/what-is-the-difference-between-independent-and-mutually-exclusive-events Probability is a mathematical concept, which has now become a full-fledged discipline and is a vital part of statistics. Random experiment in probability is a performance that generates a certain outcome, purely based on chance. The results of a random experiment are called event. In probability, there are various types of events, as in simple, compound, mutually exclusive, exhaustive, independent, dependent, equally likely, etc. When events cannot occur at the same time, they are called mutually exclusive. On the other hand, if each event is unaffected by other events, they are called independent events . Take a full read of the article presented below to have a better understanding of the difference between mutually exclusive and independent events. Reference : https://keydifferences.com/difference-between-mutually-exclusive-and-independent-events.html",310217.0
127292,,nan,
99835,427606.0,It is a given condition that 5% samples are defective. Out of those there is a sample of 10 packets where we are calculating the percentage of getting 2 packets defective.,318851.0
99835,427597.0,"Please note this is not a fair distribution in which probability of any event is same(e.g. in dice getting prob of getting ""1"" is 1/6). here getting 2 out of 10 packets defective is a known fact or outcome after testing large samples. You can just use it as a fact.",315277.0
99835,427645.0,"Attaching a screenshot of the problem statement where it is clearly mentioned that the probability is 5% i.e., 0.05",313826.0
96663,411767.0,go through the below link : https://www.quora.com/Five-students-A-B-C-D-and-E-are-given-a-problem-to-solve-The-probabilities-of-them-solving-are-1-2-1-3-1-4-1-5-and-1-6-respectively-What-is-the-probability-that-if-all-of-them-try-the-problem-would-be-solved,318017.0
96663,417687.0,"Binomial Distribution What is the probability that exactly 4 students are able to solve all the 3 questions? Hint : First you need to calculate the probability that a randomly selected student is able to solve all the 3 questions. I used the binomial distribution and calculated the probability for (p=0.85, 0.6, 0.3) for 4 people out of 10 peoples and then added them. But got the wrong answer. Is there any different method required to be used? Took the ref. given by Deval also, didn't understand this. Could anybody help?",311117.0
96663,420712.0,Refer to the answer given here. https://learn.upgrad.com/v/course/208/question/97655,304319.0
97558,417483.0,"X = 2 means that the number corresponding to the case where the picked red balls are 2. Out of 4 attempts, the total combinations of picking 2 red balls will be 6.",318329.0
97558,420065.0,"X = 2 is the number of red balls. So, you can have 2 red balls and 2 blue balls in following order as arrangement do matter here. R1,R2,B1,B2 R1,B1,R2,B2 B1,B2,R1,R2 B1, R1, B2, R2 R1,B1,B2,R2 B1,R1,R2,B2. So, total of 6 arrangements. Basically it is equal to 4C2 which is = 6.",317689.0
97558,420240.0,"X here is the numner of Red balls. In how many cases there can be 2 Red balls, you need to focus on that.If you will see that example in the Video , you will infer that there can be a total of 6 outcomes where there are 2 Red Balls(i.e. total 6 combinations of '2 R + 2 B') . Thats why factor 6 is there for X=2, again X is how many times there can be 2 red Balls.",315560.0
97558,421518.0,"X= 2 has 6 possibilities. they are RRBB BRRB BBRR RBBR RBRB BRBR so, we have 6. You know the formula of combination i.e., nCr = (n!)/(r!(n-r)!) here, n &gt; r 4C2 = 6",304397.0
97655,417820.0,"Let's first find different combinations able to solve 0 questions - 1 able to solve 1 question - 3 able to solve 2 questions - 3 able to solve 3 questions - 1 Now, probability of solving all 3 questions is 1/(1+3+3+1) = 1/8 = 0.125 Hope its clear and right",318329.0
97655,417825.0,"A student being able to solve a particular level of question has no guarrantee on the probability of whether the same student is able to solve other level of question, i.e. all the three scenarios are independent of each other. Therefore, find the probability of student being able to solve all three questions correctly through multiplication rule. That becomes the value of p and then calculate P(X=4).",311160.0
97655,417926.0,"Since all the three scenarios are independent, so find the probability accordingly and apply Binomial distribution. Hope this will helpful..",311117.0
97655,420707.0,Soving all the three questions are independent events. So P(solving all 3 questions)=P(solving 1st q) * P(solving 2nd q) * P(solving 3rd question). Now you can apply binomial probability to find out the final probability.,304319.0
97655,420766.0,"When you are going to solve this problem. You are going to need the probability to put in the Formula , for that probability you need to think , how to calculate prob of a randomly selected student. For this plz watch the first video of Binomial Distribution , you will find the answer of what is the prob of 1st red ball and scond and soon. The method to calulate prob of random student to solve this question is similar to that.",315560.0
98928,420990.0,Would like to add to Hemant's answer.,301652.0
98928,420959.0,cumulative is adding up your probablities.. so you see the curve as increasing till 1.. in your case Probablity at 65 is &lt;= 65 and at 60 is &lt;= 60 hence to find between 60 and 65 you've to subtract 65 from 60.. i hope I didn't confused you more.. :),316349.0
98928,421053.0,"The cumulative prob until 60 includes all prob until 60 and for cumulative prob. for 65 includes all prob until 65 (incl. cumulative prob until 60) What we need to find here is what is prob between these 2, hence the diff of cumulative prob between 65 and 60. In other words, if we assume that the prob of weight between is X, then we can denote it as: cumulative prob at 60 + X = cumulative prob at 65 X = cumulative prob at 65 - cumulative prob at 60",319302.0
98928,421473.0,"the cumulative probability till 60 is 0.367 and that till 65 is 0.467. So, the probability of weight bring between 60 to 65 ids 0.467-0.367=0.1. This is similar to the example that a batsman has scored 40 runs till 20 mins of batting and 60 runs between 30 mins, so the runs scored between 20 to 30 mins of batting is 60-40=20",318374.0
95373,404612.0,"Hi Karthik, Can you please proivde more details like which question you are pointing to here.",301559.0
95373,417412.0,"Hi Karthik, Probably you are asking for the picture in Question 2/2. So, for this refer the picture given in Question 1/2. Question 2 follows Question 1. Follow this Link: https://learn.upgrad.com/v/course/208/session/18003/segment/91575. This will help.",311117.0
98967,421191.0,"hi Ashish, I'm sorry, but I still didn't understand. how can we have more number of balls in the middle of the experiment than the count we started with",319876.0
98967,421165.0,,318344.0
98967,421202.0,"Hi Payal, The experiment involves placing of the ball back to the place from where you have picked. What it is: You pick one ball and note down what you have picked and put it back in the basket before you pick another What it is not: You pick one ball and put it out and pick another from the rest",311160.0
98967,421214.0,"thank you , got it now, it is like picking up one ball at a time and not all 4 !!!!",319876.0
99301,423369.0,"Now I understood, we are putting the ball again in the pocket.",301124.0
99301,423493.0,YES we are not putting them back .,301648.0
99052,421751.0,"hi Payel, it is asking what is the combined probability of a situation like this where 7 out of 10 questions could solve normal question, 5 out of 10 intermediate and 2 out of 10 high level question. there can be many situations like 8 normal - 8 intermediate- 8 high or 10-5-6 or 10-10-10 or 1-2-3 and all such cases. we need to mention the calculated probability of a 7-5-2 situation. you need to 1st find probability of 7 solving normal question, 5 solving intermediate and 2 high level. then you need to find out combined probability.",311686.0
99052,422891.0,"There are three observations and we want ALL to happen. So I would calculate individual probabilities with the given facts (7 students, 5 students, 2 students) and then do the ALL/Multiplication operation.",318007.0
99052,421820.0,"Here are important rules of probalility - Addition rule - p(A or B or C) = p(A) + p(B) + p(C) p(A and B and C) = p(A) * p(B) * p(C) so you can use that to solve this question. Each probability is binomial distribution here. eg. 7 students were able to solve the normal level question &gt;&gt; (10 C 7) * (p ^ 7) * ((1-p)^(10-7)) p is probability of solving normal level question. Regardind Theoretical probability - Considering question says ""she found that 7 students were able to solve the normal level question, 5 students were able to solve the intermediate level question and only 2 students were able to solve the high-level question"" - It means this event has already occurred. When something is already confirmed, probability of it is 1. It is also called as certain event. Theorotical probability is if it is not confirmed, what would be probability of occurance.",318458.0
99339,423669.0,would like to perform COMIN funciton of excel .. on paper .. to be more precise for my question',312199.0
99339,423802.0,"COMBIN excel formula implements the combinations formula which calculate the number of ways ""r"" elements can be chosen from a population of ""n"" objects i.e., COMBIN = nCr = n! / ( r! * (n-r)! ) where n - number of objects in the population r - The number of elements to choose from the population containing ""n"" objects. Here ! indicates factorial. For example, there are 5 varieties of pizzas offered by a restaurent and as an offer you are allowed to choose any 2 pizzas for a promotional proce of say Rs.200. The way in which you can choose 2 pizzas out of the available 5 varieties of pizzas is 5C2 = 5! / (2! * (5-2)! = 5! / (2! * (3!)) = ( 5*4*3*2*1 ) / ( 2*1 * (3*2*1)) Cancelling 3*2*1 from numerator and denominator we are left with = ( 5 * 4 ) / ( 2 * 1) = 10. So you 10 combinations of 2 pizzas from 5 varieties of pizzas. Hope this helps.",313826.0
99339,424000.0,Please go through this video = https://www.youtube.com/watch?v=NEGxh_D7yKU It explain it in very simple way. nCr = n! / [ (n-r)! * r! ],318458.0
99339,424044.0,"i'm able to apply the formula to get the result of Permutation ( nPr) or combination (nCr), what is challenging is Binomial example from Session 2: P(X=5) = 10c5(0.5)^5 (1-0.5)^5 Not clear in my mind how the result 0.246 or 24.6% I need to understand this part to help me move forward.",312199.0
99339,424705.0,HOW ? is my question. 0.5*0.5*0.5*0.5*0.5 = 0.03125 going by this math no where i'm getting result of 0.246 this is what i need to understand,312199.0
99339,424470.0,"Binomial probability is given by the formula: P(X=r) = nCr * (p)^r * (1−p)^n−r In the example, we are trying to find the probability that out of 10 cars spotted, what is the probability that the number of cars having a even numberplate is 5. Hence, the total number of cars n = 10 , number of cars with even numberplate r = 5 . Out of 10 cars , we are trying to find probability that 5 cars have even numberplate. Therefore, p = 0.5 . Plugging in the values we get P(X=5) = 10C5 * (0.5)^5 * (1−0.5)^(10-5) = 0.246 = 24.6%.",313826.0
99339,424861.0,"To calculate combinations, we will use the formula nCr = n! / r! * (n - r)!. 10C5 = 10!/5!*(10-5)! = 10*9*8*7*6*5!/5!*5! =252",314183.0
99073,421842.0,"We have 3 events. Student solves a difficult, an intermediate and easy question. These are all independent. So you can use multiplicative rule of probability here. After this, you will get probability of success. So you know, number of trials, probability of success. And hence you can solve fpr P(X=4), where, X is a random variable with two outcomes success(student solves all the questions) and failure(student will not solve any question).",301652.0
99073,421846.0,Find the individual probability of solving each level of question and then use the multiplication rule of probability.,310974.0
99073,421858.0,They are asking for prob of exactly 4 students are able to solve all the 3 questions (Difficult &amp; intermediate &amp; normal) Prob rules - P(A or B or C) = p(A) + p(B) +p(C) P(A and B and C) = p(A) * p(B) * p(C) Use appropriate rule. It will be prob of solving all 3 question. Then define random variable X as all 3 questions are correctly solve. It is a binomial random variable as it has 2 outocomes (Success and Failure) &amp; Number of experiments are also fixed. So use appropriate distribution.,318458.0
98700,420348.0,"Yeah, correct. I think this is an easier way to do this question.",318344.0
98700,420356.0,Agreed. This is one way to verify that EV is same as the probability given. If youtry with n=5 the value comes to 0.25 (both methods),311857.0
98700,422694.0,Yeah we can derive by this way also,318756.0
103863,446052.0,"In general though, the PMF is used in the context of discrete random variables (random variables that take values on a discrete set), while PDF is used in the context of continuous random variables. hope this helps 😁",318017.0
103863,446102.0,"The probability density function or PDF of a continuous random variable gives the relative likelihood of any outcome in a continuum occurring. Unlike the case of discrete random variable, for a continuous random variable any single outcome has probability zero of occurring. The probability density function gives the probability that any value in a continuous set of values might occur. Its magnitude therefore encodes the likelihood of finding a continuous random variable near a certain point. PMF or Probability Mass Function is used in the context of discrete random variables (random variables that take values on a discrete set), while PDF is used in the context of continuous random variables.",314183.0
103863,446169.0,"No we cannot use pdf for both since the nature of both the variables are completely different. Discrete distributions describe the properties of a random variable for which every individual outcome is assigned a positive probability. eg Rolling of Dice the outcome is certain 6 possible outcome and so individual probability is used. Continuous distributions describe the properties of a random variable for which individual probabilities equal zero. Positive probabilities can only be assigned to ranges of values, or intervals. Two of the most widely used discrete distributions are the binomial and the Poisson. eg probability of employees coming to office neither of their outcomes will be similar to the exact second or nanosecond so take probality here is in terms of range. I have used the defination from this site - https://www.dummies.com/education/math/statistics/discrete-and-continuous-probability-distributions/",318451.0
103863,446813.0,"No, PDF can only be used for continuous random variables",300694.0
103863,447517.0,"I agree to disagree with the others on this. One can use a distribution function to define discrete or continuous random variables. So, you can define a distribution function for discrete and continuous random variables. However, a probability density function can be used only for continuous variables (as far as I know).",306733.0
100100,428980.0,"The cost involved is less because it does not involve any buying or 3rd party involvement. The sample collection is easy as take only your example the sample collection among all students from all the scholl is costly rather using the opportunity sampling where it can coolect the sample students just by visiting. 2. Opportunity Sampling is a popular technique, especially when we don't have the time or money to select a truly random sample. 3. When making news at random opportuninty sampling is best as selected sample at particular location about the mood of people regarding the electorial party, and it gives good hypotheis in less time in less cost",307843.0
100100,429807.0,Major advantage of opportunity sampling is collecting large amount of data in short time span which will definitely result in less cost expense on sampling as now people hired for that task will not be on field for same amount data for much amount of time .It will save time and cost both. For detail go through this link: https://www.ehow.co.uk/info_8510671_advantages-disadvantages-opportunity-sampling.html,318426.0
98972,421292.0,Some of the answers I got from the video only are: 1. Sample surveymust take a minimum stipulated time. 2. All the answers should not be extreme. 3. Sample should correctly represent the cohorts. Can you list more?,318344.0
98972,421332.0,use category wise distribution of sample like age sex and demographic location,318017.0
98972,421596.0,"Trying to capture something which was metioned in the Video by Prof. Tricha; ""To see the errors in your sampling, you've to take out some samples for e.g. 5 people out of 75 upgrad team and then take the average; it varies more.. take more samples of 5 and see it will come closer, take more and more and continue doing this (no limit as mentioned).. If you plot this average data (Sampling Distribution Mean), it will be observed that the with less sample, the plot is little wide.. with increase in number of samples the plot will be be a proper bell curve.. and that should give you the closer pattern.."" Point here is "" more the number of samples, you get the closer results"" ..As far as samples concerned, they're randomly picked.. I can imagine of only this error prediction method mentioned in the video whcih sounds logical and nicely correlated.. :) I hope, I'm not diverting from your original question.. thought of sharing some light from my side.. thanks for asking this question!",316349.0
98972,422487.0,"I am assuming you have gone through the different sampling techniques mentioned in the session such as volunteer sampling, stratified sampling, etc. You can read some more sampling techniques from this link https://towardsdatascience.com/sampling-techniques-a4e34111d808. The sampling techniques that are used in any particular experiment need to be chosen carefully that they remove the bias from the experiment. It can be a combination of two or more different sampling techniques. So depending on your knowledge of the problem you are going to solve, choose the techniques wisely so that the bias is minimum and the sample data is more representative of the population. All these qualitative checks are pretty useful before doing a sampling experiment. Now, a quantitative check that you can perform on the sample data, is that you calculate the confidence interval by applying CLT and see its margin of error. (Here I am assuming that you have gone through CLT as well). Basically, you do sampling to guess the population parameters. (The entire concept of sample data being valid is required for doing population prediction only).So the final answer would be in the estimation would be in the form of the following equation. population mean = sample mean ± margin of error. So if the margin of error is small, then your sample data is statistically better in predicting the final population parameter and more representative of the population data. For example, 2.8 ±0.4 is a better estimator than 2.8 ± 1.2. The true population lies in a shorter interval in the first case(2.4-3.2) and hence it is statistically better than the second one(1.6-4.0). The margin of error depends on the two parameters. The sample size and the sample standard deviation. If the sample size is large, then it is statistically more representative of the population data. For example, if you have a population of 200 only, a sample size of 50 would give a better result than a 20 size sample. Similarly, if the standard deviation is low, then the sample that has been taken would give better results than the one with the higher standard deviation. This is all I say that is within the scope of what you've learnt in this course. All in all, you can never know if you get a 100% perfect sample, but doing some qualitative and quantitative checks can give you statistically significant answers.",313517.0
99192,422662.0,You have to find the rainfall in mm so that current infrastructure could be redesigned to avoid flood conditions. and the chance of either similar or heavire rains in future reduces to only 3%.,317991.0
99192,422634.0,"You've to find the Z value such that, your probablity distribution falls within 97% which is other way saying heavier rainfall chances are less i.e. 3% Hint on how to approach it; Using the option, try to find the Z value and you'll be able to guess the right answer..",316349.0
99205,422641.0,Try to find the Z value using Mean and standard deviation and then find the probablity from the Z table which will take you to answer in the options.. :),316349.0
99205,422757.0,it means find P(X&gt;2200). Compute Z and using Z-Table find the probability. Or use excel to find it.,304319.0
99204,422758.0,Refer to the link below for the answer: https://learn.upgrad.com/v/course/208/session/18004/segment/91582,304319.0
97677,417993.0,Can you paste the URL which contains your question?,311160.0
97677,423076.0,"<table style=""border-collapse: collapse;width:598pt"" width=""796"" cellspacing=""0"" cellpadding=""0"" border=""0""> Total days(X) 0.68(X) 0.95(X) 0.997(X) Note 5days every week 7 5 This means that 68% times bread weight will vary within one SD 20 days every 3 week 21 20 This means that 95% times bread weight will vary within two SD 364 days every year 365 364 This means that 99.7% times bread weight will vary within three SD",301643.0
97677,418146.0,"1. 5 days every week - 5/7 = .71 (very close to .68), so we can say 1 standard deviation. It is given that standard deviation is 1 gm and mean weight is 100 gm. Say that you buy bread evey day for a week. You know that their will be some variation in the weight of the bread packet. You can assume that if you take 5 days weight data out of the 7 days, you might find that the weights vary by 1 gm from a mean of 100 gm. (1 standard deviation equals 1 gm. Also 5/7 = 71%, close to 68%). Similar concept can be applied below. 2. 20 days every 3 weeks - 20/21 = .95, so we can say 2 standard deviation. 3. 364 days every year - 365/365 = .997, so we can say 3 standard deviation.",301652.0
99237,422945.0,it i a histogram based on random guess of the employee comute time having bins of employee in particular time on x axis and count of employees on y axis.,318017.0
99237,423505.0,"The exact way of the mapping the commute times is difficult to explain within the scope of this course as the concept of probability density function uses a fair bit of calculus to make complete sense. However, a basic intuitive idea can be derived in the following way. Basically, the probability density is the probability per unit random variable, to explain in layman's terms. For example, in the commute time example, the probability density for the commute time between 25-30 minutes would be P(25&lt;T&lt;30)/length of the interval = 0.20/(30-25)=0.04/min. Or we can say that on average the probability changes at 0.04/min in the range of 25-30 minutes. Similarly, if we calculate the density for all the intervals we get the following list 20-25(0.03) 25-30(0.04), 30-35(0.06), 35-40(0.04), 40-45(0.03). Now if you plot a histogram with these values and the intervals and then join the centre point of the peaks with the smooth curve, the resultant graph would be the probability density function (in a very basic way.) There is, however, a catch with this method. We have assumed that the probability changes at an equal rate within the intervals. But this is not the case all the time. It may so happen that between 25-28 the probability would be 0.15 and between 28-30 the value would be 0.05. Thus the density would change to 0.05 from 25-28 and 0.025 between 28-30 rather than 0.04 all throughout. This results in a more accurate probability density function. As we keep making these intervals smaller, we would keep getting a more accurate answer. This is where the concepts of calculus help us in making infinitely smaller intervals so that we can predict highly accurate probability density functions. Check this link if you wish to learn more about this. https://onlinecourses.science.psu.edu/stat414/node/97/ The given probability density function is only for demonstration purpose and not necessarily highly accurate. It is drawn similar to a normal distribution, to give a visual aid when you'll learn it in the future segments. It just gives a basic idea of how to calculate the area to find the probability.",313517.0
99249,423461.0,"u can understand it very easily by normal dist. curve ,,,probability at any point is area upto that point covering left ,,,,p(x&lt;=c) means area upto c ,point c here divides total prob. 1 into p(x&lt;=c)+p(x&gt;c) ,so probability p(x&gt;c) means 1-p(x&lt;=c) as total prob. is always 1.",318005.0
99249,423034.0,in a normal distribution mean divides the distribution into two halves so p(z&gt;-3) = 1-p(z&lt;-3),318017.0
99249,423041.0,"The given equation P(Z&gt;-3)= 1 - P(Z&lt;-3)holds because Z&gt;-3 and Z&lt;-3 covers all the possible values Z can take, [Actually it should be P(Z&gt;-3) = 1 - P(Z&lt;=-3) but since it's a continuous distribution P(Z=-3)=0, hence we can discount that] Thus P(Z&gt;-3)+P(Z&lt;-3)=1 and hence P(Z&gt;-3)= 1 - P(Z&lt;-3) For example in a single coin toss experiment, the only possible outcomes are heads or tails. Thus P(Heads)+P(Tails)=1 hence P(Heads)=1-P(Tails). In a similar way, Z can take a value either greater than -3 or less than -3. And since they cover all the possibilities(Again we are discounting Z=-3 for the time being), from the basic rules of probability we have that their sum is equal to 1.",313517.0
99249,423095.0,"Thanks Mahima for the explanation. I have one more query.. Q:What is the probability that the tablet that has been selected by the authority has a paracetamol level below 550 mg? In this we do P(X&lt;550) = P(Z&lt;{550-510}/20) = P(Z&lt;2) = 0.977, or 97.7%. why don't we do the same as above like P(Z&gt;2)+P(Z&lt;2)=1 in this case?",309451.0
99244,423008.0,"in a realistic conditiom you can say that time to reach office cant be zero but time can be zero while finding probability , what the data which is showed in the tutorial is random but there can be a employee who can say that the time to reach office is zero so they are keeping all the situations open",318017.0
99244,423016.0,"As per the rules of probability ""The probability of the sample space (the collection of all possible outcomes) is equal to 1"" . And here the possible outcomes are not only 20-25, 25-30,30-35,35-40 and 40-45 but 0-20 min also be possible. Thats why the probablity of 0-45 is taken as 1 Hope this will help.",317991.0
99244,423079.0,"As you can see from the table P(20&lt;Time&lt;45)=1. Now the sample space chosen here is Time&lt;45 minutes and thus (0-20) would also be included in this. But since P(0&lt;Time&lt;20)+P(20&lt;Time&lt;45)=1 and as we found out in the in-video question P(T&lt;=20) =0. Thus adding the interval of P(T&lt;=20) doesn't cause any change since P(T&lt;45) remains 1 only. Similarly, if you took the sample space as the number of people whose commute time is at most 55 minutes, P(T&lt;=55) would be equal to 1 since all the sample space needs to be accounted for while calculating the total probability. And here apart from P(T&lt;=20), P(45&lt;=T&lt;=55) would also be 0 since all the probabilities have been accounted for by the P(20&lt;=Time&lt;=45) interval.",313517.0
99251,423044.0,z table look at the .8 column for 1 z value and and .2 column for 2 z value,318017.0
99251,423048.0,"They are calculated from the Z-table . The probability against each Z-value given is taken as P(Z&lt;z). So for P(Z&lt;1.8), you need to check the value against 1.8 z and similarly for the other one, find P(Z&lt;-2.2) first and then subtract it from 1 and get the value of P(Z&gt;-2.2). [Note: That feedback would be P(Z&lt;1.8)-P(Z&lt;-2.2) instead of P(Z&lt;1.8)-P(Z&gt;-2.2)] Please go through this segment once again to gain further understanding: https://learn.upgrad.com/v/course/208/session/18004/segment/91584",313517.0
99251,423056.0,These values are taken from Z table. http://users.stat.ufl.edu/~athienit/Tables/Ztable.pdf For Z &lt; 1.8 Similarly you can find for Z &gt; -2.2,317991.0
99258,423115.0,"Since this is a graded question, I can only provide some hints. You need to calculate the probability for the interval where the error from the mean is 500 at the most, either less or more.",313517.0
99261,423126.0,Could you specify in which sessions you are facing the most difficulty? Is it only for continuous probability distributions?,313517.0
99261,423149.0,"Here are some links for normal distribution that might help you: Link1 , Link2",313517.0
99001,421440.0,Yes . you will need to calculate the Z score. your mean is 1600 SD is 400 X is 2200,317996.0
99001,421415.0,"you have to check the standard deviation here which will tell you by what value the rainfall if happens is dangerous apply 1, 2, 3 rule of normal distribution here",318017.0
99001,421465.0,All the raw Data is provided above in the statement. Like what is mean and what is your standard Deviation. Hope it helps,315560.0
99001,421487.0,"Yes. The problem asks us to calculate the z-score. The mean, sd and X are all provided. Please pay close attention to condition : "" more than 2200 mm "" in the question.",313826.0
99001,421590.0,"Yes Z score is the standard way to solve this question. But if you are confortable use can use other function like NORM.DIST(xvalue ,mean,standard_dev,true) in excel Both will give you the same result.",318476.0
99001,422258.0,"Here , Mean is 1600 mm with a standard deviation of 400 mm from this you have to calculate z score =(X-mean )/standard deviation Probability more than 2200.",314183.0
99001,421434.0,"Yes, we have to check Z score for that question by converting into Z values range by (X-mean)/standard deviation. so we have to check P( Z&gt;(X-mean)/standard deviation)) which we will get value from Z-table",300733.0
99027,421635.0,mean is total of all the output / total output which is same as average,318017.0
99027,421623.0,"Yes, 'Average' and 'Mean' is same as far as answering questions on Continous probablity distribution is concerned.. :)",316349.0
99027,421636.0,Thank you guys for clarifying.. just had a doubt thought of clarifying. :) Thanks again !,300727.0
99027,422495.0,"Yes, Mean and Average are same",314183.0
96776,412544.0,it means u need to find what will be the values in range of +-500 wrt mean,317982.0
96776,420115.0,range between -500&lt;x&lt;500,314678.0
96776,421457.0,"Calculte the value of Z at both 500+- , and calculate final value of Z by looking in Table.",315560.0
96776,422260.0,We have to calculate between -500 to +500.,314183.0
99099,422018.0,"Mean=35 and standard deviation=5 for the commute time is just an example. Usually, these two values would be given while solving the questions. If not we need to find the mean and standard deviation from the data given in order to calculate the Z value",311254.0
99099,422062.0,For a normal distribution we are goinf to have a mean value and an standard deviation. In the video is mention that for the Sample/Population under observation is having a mean of 35 mins with a standard deiation of 5 mins,318476.0
99099,422755.0,Mean value and Standard deviation are calculated on the basis of some sample data. And then they can be used to find the probability of occurence of X lying in an interval.,304319.0
98743,420636.0,"It is asking for over estimating by 2330 km. So, you are going 2330 km from mean value which is Z score of 2.33.",317689.0
98743,420870.0,"as per the question it is asking to calculate the probability of X&gt;2330 as it written 2330 kms or more. i.e P(X&gt;2330), convert it to Z score and you will get the answer.",305129.0
98743,421454.0,"Z=X−μ/σ . calcuate the value of Z according to this Formula, and look for the value of Z in the Table.",315560.0
98743,422259.0,"In Question , They are asking about more than 2330 km, so we have to calculate +ve side only.",314183.0
99159,422416.0,Approach that i used here and was able to get the answer in just 2 mins was; Verify all the options using excel by just changing values and see which option is close to 3%.. :),316349.0
99159,422406.0,you have to apply 1 2 3 rule using z score you can find the cutoff,318017.0
99159,422479.0,"Here μ= 1600 and σ= 400, So according to formula Z-value corresponding to x = (z-1600)/400 Now P(Z&gt;=z) = 0.03, and so it can be written as P(Z&gt;=z) + P(Z&lt;z) = 1 further -&gt; 1-P(Z&lt;z) =0.03 or P(Z&lt;z)= 0.97 ! Check the Z table for the value of 0.97 , By taking the value from row &amp; column, You'll get the value of 'z' z = (x-1600)/400 ( The z value which you get from the Z table ) ! You will have the desired answer by this logic ! Understand the logic behind it as in examination this kind of question might be there !",311466.0
99405,424649.0,"This is question on the sampling Here taken the any random sample of 100 employee and estimated the mean.of sampleis 36.6 Sample distribution standard deviation = S/Square root on N i.e 10/Square root of 100 = 10/10 = 1 Please note Standard deviation we use for entire population and wewe use Sample Standard Deviation is (S) Since we take a sample mean of size greater than 30, than thesample mean and population mean is equal. Now to find the sample mean (+-) we use like sample mean +1 0r 2 etc and derive the population based on the graph",307843.0
99408,424488.0,standard deviations of the population / √number of sample,318017.0
99408,424494.0,"Standard error is calculated for the standard deviation of the sample mean, . It describes accuracy as an estimate of the population mean, . When the sample size increases, standard error decreases and accuracy improves.",301644.0
98436,420754.0,"Xi is not calculated, it is part of the provided data. In this case, it is height of each student. Therefore, X1 = 1 21.92, X2 =133.21, X3 =141.34, X4 = 126.23, X5=175.74",311160.0
98436,420879.0,"Xi is the actual data. X1 = 1 21.92, X2 =133.21, X3 =141.34, X4 = 126.23, X5=175.74 X-bar is 139.69. You can manually calculate is, but i will advise to copy-paste the data in an excel, and apply AVG and STDEV formula.",318344.0
98436,422072.0,"(1 21.92 - 139.69) ^2 + ( 133.21- 139.69) ^2 +....till X5 , this is how it is calculated , you can use scietific calculator for this , either in Mobile or your computer.",315560.0
98923,434136.0,o.33 is the margin of error. this to calculate the confidence interval we need do +/- to arrive at interval related to that confidence level,315242.0
98923,420922.0,can you please explain about which question you are asking ?,318017.0
98923,421001.0,"Not only has 0.0033 been added but it is also subtracted to 50.5 get the confidence interval. (50.17, 50.83). It is added and subtracted because you want to find the interval in which the mean will lie (you are not sure). In content provided, 0.0033 is margin of error corresponding to the mean (50.5) with 90% confidence.",301652.0
98923,421087.0,"The mean and standard deviation (S) are in percentages (50.5% and 20%). For calculation of margin of error (.0033), S was taken as 0.2 i.e. as a number and not a percentage. The resulting margin of error is, therefore, also a number and percentage. Before adding/subtracting to mean (% figure), the margin of error needs to converted back to a percent. Alternatively, you can use S=20 instead of S=0.2 and this conversion between % and number would not be required",305653.0
99260,423134.0,Thank you did the same :),300727.0
99260,423125.0,Use Excel to calculate the sample's mean and standard deviation from the dataset provided.,313517.0
99395,424698.0,Refer to this discussion post : https://learn.upgrad.com/v/course/208/question/99284,313517.0
99395,424409.0,You can refer to the below . https://www.youtube.com/watch?v=sO6SjcaCzJw,307843.0
99395,447254.0,This link explians in a clean fashion http://www.math.armstrong.edu/statsonline/5/5.3.2.html,305842.0
99424,424795.0,"It depends on the question that is asked or the interval that you are claiming to have the values with the confidence level. So both the interval and the confidence level comes together. For eg : It's like I am saying this with 99% confidence that for the mean age of the complete population will lie between 28-32, so this is one claim with 99% confidence level. Your range value will change as per the confidence level you are claiming. Generally, the above confidence level (90%,95%, and 99%) is used, and you have values of Z* for these.",315028.0
99424,425727.0,Please go through the below URL. It has some very insightful answers: https://stats.stackexchange.com/questions/132536/how-to-choose-a-confidence-level,301618.0
98573,420751.0,It is difference of the value of the positive and negative value if z i.e .9772 - .0228 = .9544 95.4%,311160.0
99037,421633.0,please convert your answer to proper percentage,318017.0
99037,421627.0,It is given that sample’s standard deviation S = 0.2(20%). In your calculation you have not converted Standard deviation to percentage. (1.96* 0.2 )/SQRT(10000) Convert .00392 to percent and then add and substract to 50.5%.,301652.0
99037,422101.0,Mean estimation using CLT 95% interval (1.96*0.2)/SQRT(10000)= 0.00392 0.00392=0.39 % Range is 50.5 - 0.39= 50.11 and 50.5 + 0.39=50.89,311254.0
99037,422314.0,Mean estimation using CLT is 95% (1.96*0.2)/sqrt(10000)=0.00392=0.39% Range 50.5-0.39=50.11 50.5+0.39=50.89,306996.0
99331,423746.0,"Actually we don't need to create sampling distribution. Sampling distributions are just a theoretical exercise and you’re not actually expected to make one in real life. If you want to estimate the population mean, you will just take a sample. You will not create an entire sampling distribution. Source:- https://learn.upgrad.com/v/course/208/session/18005/segment/91591 Hope this helps.",317991.0
99331,423808.0,"I believe you are mentioning about the video present on the link https://learn.upgrad.com/v/course/208/session/18005/segment/91587 . Here, 100 random samples of 5 people were taken so that the concepts and properties related to the Sampling Distribution can be explained and understood. This exercise was done purely from a theoritical perspective. In future lectures, the Prof. has mentioned that such a practice of taking multiple samples and creating of the sampling distribution is not necessary at all due to the inehrent properties of the Central Limit Theorem. However, adequate care should be taken in sampling process which has been covered in the optional module ""Application of Sampling Methods"". I suggest you to take a look at this module when time permits.",313826.0
99063,422781.0,We can use excel formula =STDEV.P(your array of values),314183.0
99063,421800.0,"Find the ""Mean"" of the sample means(the simple average of the numbers) Then for each sample mean: subtract the ""Mean"" and square the result. sum those squared differences and divide it by nuber of samples(100) Take the square root of that.",310467.0
99063,422039.0,We can also use the excel formula =STDEV.P(your array of va;ues),318476.0
99063,422322.0,* Find the mean of sample mean (means doing avg of that numbers) * Then for each sample mean:subtract the mean and square result *sum of those square and divide by number 100 (sample) *then take square root of that,306996.0
99441,425001.0,I believe it is covered in the Session 5 (Applications of Sampling Methods) of Inferential Statistics,311160.0
99106,422071.0,"Z value is the difference between X and the mean, divided by standard deviation. Z value represents how many standard deviations away is the X value from the mean. .",310467.0
99106,422069.0,"There is a table , link : http://users.stat.ufl.edu/~athienit/Tables/Ztable.pdf Just look for the interval they have asked in the question if Z= +1.65 , look for its calue in the Table , then multiply that value by 100 , you will get percentage for Z* .",315560.0
99106,422077.0,"Thank you for the quick responses. I actually need to know what logic was used to calculate the Z* values. The most commonly used values for confidence levels have been given. (90% - 1.65, 95% - 1.96, 99% - 2.56). The actual calculations have been provided in the FAQs. However, not much has been given about how these were calculated and what was the logic behind the calculations.",315471.0
99106,422227.0,,301652.0
99106,422235.0,the below link explains it in detail. https://www.khanacademy.org/math/statistics-probability/confidence-intervals-one-sample/modal/v/confidence-interval-example,310467.0
99106,422248.0,Thank you. That answers my question,315471.0
99106,422218.0,You can use the integration yourself to calculate the Z-values. (https://learn.upgrad.com/v/course/208/session/18004/segment/91584) Z-table is prepared for standard normal distribution. A standard normal distribution is one which has mean 0 and standard deviation of 1.,301652.0
99106,423764.0,You can calculate Z* by the formula given in the answer to the question - What is the difference between Z* and Z-score? at the link given below: https://learn.upgrad.com/v/course/208/session/18005/segment/91591,304319.0
98181,420210.0,x is the mean of the complete sample which will be sum / number of values and x bar is the mean of sample which is sum of 5 value / 5,318017.0
98181,420649.0,"Hi, X bar is mean of the sample and X means normal values from sample If we have below sample with size Ex: 1,2,3 X bar is 3 X are 1,2,3 X-X bar is used in sample standard deviation formula",311952.0
98181,420878.0,X is referred to the individual values of the sample. X-bar is mean of the sample. In the vide they have assumed that X-bar is 36.6 and S-Square to be 9900.,318344.0
98181,420934.0,"Thanks everyone, None of the answers provide me an indication as to why this value is selected. I am not sure how answer is accepted and verified by TA as well. My question is that : As per video, I don't see any calculation basis on arriving at 9900 . It just says ""Let's just say we find by calculation that summation of (Xi - X(bar) square is 9900"". I know that X(bar) is 36.6 ; and I know sample size is 100 ; what is X1, X2, X3 , X4 within samples ? how is that arrived at ? is there any spreadsheet that gives us X1, X2 , X3 ....X100 values from which we subtract 36.6 and sum it up ; and square it to get 9900 ? where is it?",309211.0
98181,420974.0,My question is about the Video https://learn.upgrad.com/v/course/208/session/18005/segment/91586 @3:25,309211.0
98181,420982.0,"Yes, the value of 9900 as the (Xi-X)^2 has been assumed in the video. You are not calculating it using any previously mentioned values. Basically, the value of 9900 is chosen such that when you divide it by (n-1) which is 99, in this case, we get the value of variance as 100 and the value of standard deviation as 10 which makes for easier computations.",313517.0
98181,421575.0,"Hi , I guess you were asking about (Xi -X(bar)) ,, see Xi is the 1st sample , lets say 12 and X(bar) i.e. meani is 14 , so Xi-X(bar) is 2 and square beacomes 4 , so you need to calculte it for every value of Xi for eg. 121.92 ,133.21 etc and add those , as this is the submission and then you can find the answer, Hope it helps.",315560.0
99284,423268.0,Do let me know if any doubts exist so that I can clarify them.,313517.0
99284,424866.0,"This is a great explanation, thanks a lot Mahima!",309211.0
99284,436995.0,thanks for clearing this up,317822.0
99284,446033.0,A similar explanation only with equations this time:,310974.0
99169,422442.0,"Z value tells you the normal probablity distribution.. whereas Confidence level corresponds to the Z* value and so, you will need to extract the Z value corresponding to Z* from the Z table and that's how you get the probablity distribution..",316349.0
99169,422448.0,"suppose you have a mean of 5 and sd as 2 then 3 to 7 will have 68 percent of the distribution values , 1 to 9 will have the 95 percentage of value or you can say it has 95 percent of confidence level similarly -1 to 11 will have 99.4 percent confidence level this works acc to 1,2,3 rule of mean and sd",318017.0
99169,423188.0,"What the professor said in the video, and what is in the FAQ are both correct. In Z* you are calculating the probability of an interval, whereas in Z-score you are calculating the cumulative probability of that particular Z-value In the video, the professor found out that within 2 standard deviations of the sample mean, we have a 95.4% probability that the population mean would fall in this interval. Or P(-2&lt;Z&lt;2)=0.954. Now if you are asked to calculate the Z* for the confidence level of 0.954, you shouldn't go to the Z-table and find 0.954 and report back its Z value (which would be 1.685 in this case), because Z tables calculate probability as P(Z&lt;z)=0.954. Thus as you can see P(Z&lt;1.68)=P(-2&lt;Z&lt;2)=0.954. As you can see, you will get the incorrect Z* here as the intervals are completely different. What you need to do here is use the formula given, or use the symmetry of the normal distribution (you can use 1-2-3 rule but be careful that for 2 standard deviations the exact answer is 95.4 and not 95%) to calculate the cumulative probability value associated with 95.4% confidence level. This would come as 95.4+2.3=97.7%. And if you check the Z-table for this probability, it would come out to be 2. Hence with a margin of error of 2, we have the confidence interval of 95.4%. So, all in all, you need to make sure that you are calculating the Z* and Z value correctly because the intervals for which the probabilities are being calculated are different.",313517.0
99170,422445.0,no need to convert minutes as all the values have the same unit so it will give you correct answer,318017.0
99170,422446.0,I guess No it is not correct.. 2 mins is your Margin Error which is; = (Standard Deviation x Z*) / (Square root of sample size),316349.0
99170,422599.0,Hre 2 min is your Margen of error 2 = (Standard Deviation x Z*)/Sqrt(Sample Size) 2=(10 x Z*)/10 Z* = 2 Now as per the Z table values for cumulative Z* = 2 are 0.9772 and 0.0228 which results in a confidence percentage of 95.44% (0.9772-0.0228 ) Hope this help.,306725.0
99170,423340.0,"The standard deviation has the same unit as the mean. Hence it would be in minutes only. Thus when we say the probability of the average is within 2 standard deviations, it is the same as saying the average is within 2 minutes, since the standard deviation of the sampling distribution is 1 minute. Also, I am guessing you divided 2 by 10, which is the sample's standard deviation to get the interval, which is incorrect. You need to divide 10 by sqrt(sample size) as shown in the video to get the sampling distribution's standard deviation. After that, you can perform the calculations for the margin of error.",313517.0
99179,423755.0,"If the confidence interval is above the 50 mark, we can be confident that the party wins. Say if it is (51,55) for BJP, means that BJP would mean.",304319.0
99179,422523.0,"If they overlap, we can't conclude which one wins; For the case where A = (45,55) and B = (50,60) ===== we can't conclude which one wins If hey don't overlap then it is quite clear from the interval itself; For the case where A = (35,45) and B = (50, 60) ====== implies B wins",316349.0
99179,422593.0,"In this case each party would need atleat 50% votes to win. As 50% is in the both the part ranges, we cannot be sure which party woud win. In case the confidence interval was 45,49 and 50,55 we coud have been sure that second party would win.",306725.0
99088,422081.0,"Not actually answering your questions: 1. The 0.99 std deviation was already known I guess which was used for further calculations. 2. Even I have the same question. In all the questions and examples, the standard deviation of population is already provided. But how will that be available to us in a real world scenario..",315471.0
99088,422063.0,"Yes, in the real-world business situations we would never know the standard deviation of the entire population. Populations are often too large to estimate the value of standard deviation. For such cases, standard deviation of the sample (S) is generally used as an estimate of population standard deviation. I assume σ = 0.99 is the standard deviation of the sample that was taken in previous module.",310467.0
99088,422279.0,It is explained in later module to apporximate as S where S is the Std. Deviation for sample taken. S.E. = σ / √n = S/√n,317689.0
99088,422681.0,1) It is calculated from previous upgrad ball example. 2) In later modules it is explanned how to predict mean value for population with some error margin,314621.0
99088,422509.0,"For the first question of yours, the calculation is based on the original experiment that happened in the first session on the basics of probability. If you remember in the upgrad ball game we had taken 75 people, and the respective distributions were something like this 0(2), 1(12),2(26),3(25),4(10) [ the representation is in *number of red balls picked(total number of people getting that combination)* format]. The mean of this data came out to be X = 2.385. The standard deviation, therefore, can be calculated as follows, S = sqrt[((Xi- X )^2)/n]= sqrt[[(2*(0-2.385)^2 + 12*(1-2.385)^2+26*(2-2.385)^2+25*(3-2.385)^2+10*(4-2.385)^2)]/75]=0.99. I am assuming this exact formula wasn't used, but the S value might have been given somewhere previously. You only need it for this example here. So you don't necessarily have to go and calculate it. And for the second question, yes you would need to approximate the sample standard deviation as the population standard deviation and then proceed to calculate the sampling distribution's standard deviation. That is the procedure you need to follow in the real-life scenario as well.",313517.0
99088,424210.0,"Yes, in the real-world business situations we would never know the standard deviation of the entire population. Populations are often too large to estimate the value of standard deviation. But in such cases, we can make inferences about population mean based on the shape of sample distribution curve. As Prof. Trica explained Properties of Sampling Distributions lecture, the small SD of sample distribution curve implies that the sample mean is approximately equal to population mean, and this SD is the standard error of the sample. Please refer to attached screenshot. Hope this helps",302735.0
99340,423772.0,"According to CLT, the sampling distribution mean = population mean. Since the smapling distribution mean is given as 0.50 which means that the populaton mean is also 0.50 which means that 50% of the people have voted for BJP.",304319.0
99340,423850.0,Mean of the sampling distribution is given and standard error is given. In simple words you are asked to calculate population mean.,318328.0
105880,456945.0,I think this can help out: https://www.google.com/amp/s/www.edupristine.com/blog/addressing-multicollinearity/amp,301655.0
105880,457067.0,"Multi-collinearity means that some variables in your data can be expressed as a combination of other variables. This means that these ""dependent"" variables really don't add any new information that will increase the prediction power. For example, if you measure temperature, humidity and another variable which is an average of temperature and humidity, then the predictive power of your variables will not be diminished if you drop this third variable before applying your favorite prediction tool. In this sense, the variable doesn't contribute to increasing the predictive power -- in the same token, it doesn't decrease the predictive power either. Some times, if the relationship between the variables is very complex (think 100s of variables) having these dependent variables around may actually help you as they may lead to a simpler and more robust model in the presence of noise.",318368.0
107046,462593.0,You can check in StackOverflow : https://stackoverflow.com/questions/41599481/getting-around-valueerror-operands-could-not-be-broadcast-together?rq=1,301648.0
107046,462627.0,if you are trying to multiply two matrices (in the linear algebra sense) then you want X.dot(y) but if you are trying to broadcast scalars from matrix y onto X then you need to perform X * y.T .,319006.0
106102,457917.0,"In general, as the model complexity increased, the variance tends to increase and the squared bias tends to decrease The sweet spot for any model is the level of complexity at which the increase in bias is equivalent to the reduction in variance... If our model complexity exceeds this sweet spot, we are in effect over-fitting our model; while if our complexity falls short of the sweet spot, we are under-fitting the model. In practice, there is not an analytical way to find this location. Instead, we must use an accurate measure of prediction error and explore different levels of model complexity and then choose the complexity level that minimizes the overall error. A key to this process is the selection of an accurate error measure as often grossly inaccurate measures are used which can be deceptive You can read the more detailed discussion here https://www.researchgate.net/post/How_does_model_complexity_impact_the_bias-variance_tradeoff",317845.0
106104,457904.0,"The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors. The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not. It is always lower than the R-squared. You can read more here with example http://blog.minitab.com/blog/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables",317845.0
106104,458005.0,The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.,318017.0
106114,458053.0,"Hi, You need to pass the scaled variable into the model while doing prediction. For scaling value at same level you need to pass from same function Please follow the attached link https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html",344894.0
106115,457979.0,"Risk of not scaling down is that your higher value variables like peakrpm, curbweight will take high coefficient and will direct the model in comparison to your small variables like boreratio, stroke. So by scaling down we bring all the variable on a comparable scale. And now the model will give you the right drivers of the dependent variable.",318451.0
106142,458114.0,when the model has many variables and we try to make the model perfect then sometimes the model remembers all the points in the train data set and when the test data set is ran then the model fails terribly. if the model is giving good output with train data and bad output with test data then it has overfitted model,318017.0
106142,497879.0,"A example of Overfitting is as given by this example Let’s say you’re a new mother and your baby boy loves pasta. As the months go by, you make it a habit to feed your baby pasta with the kitchen window open because you like the breeze. Then your baby’s cousin gets him a onesie, and you start a tradition of only feeding him pasta when he’s in his special onesie. Then you adopt a dog who diligently sits beneath the baby’s highchair to catch the stray noodles while he’s eating his pasta . At this point, you only feed your baby pasta while he’s wearing the special onesie …and the kitchen window’s open …and the dog is underneath the highchair. As a new mom you naturally correlate your son’s love of pasta with all of these features: the open kitchen window, the onesie, and the dog. Right now, your mental model of the baby’s feeding habits is pretty complex! One day, you take a trip to grandma’s. You have to feed your baby dinner (pasta, of course) because you’re staying the weekend. You go into a panic because there is no window in this kitchen, you forgot his onesie at home, and the dog is with the neighbors! You freak out so much that you forget all about feeding your baby his dinner and just put him to bed. Wow. You performed pretty poorly when you were faced with a scenario you hadn’t faced before. At home you were perfect at it, though! It doesn’t make sense! After revisiting your mental model of your baby’s eating habits and disregarding all the “noise,” or things you think probably don’t contribute to your boy actually loving pasta, you realize that the only thing that really matters is that it’s cooked by you. The next night at grandma’s you feed him his beloved pasta in her windowless kitchen while he’s wearing just a diaper and there’s no dog to be seen. And everything goes fine! Your idea of why he loves pasta is a lot simpler now.",311861.0
106142,458130.0,"Overfitting a regression model occurs when you attempt to estimate too many parameters from a sample that is too small. Regression analysis uses one sample to estimate the values of the coefficients for all of the terms in the equation. The sample size limits the number of terms that you can safely include before you begin to overfit the model. The number of terms in the model includes all of the predictors, interaction effects, and polynomials terms Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples. You can read the complete discussion here https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765 https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html http://blog.minitab.com/blog/adventures-in-statistics-2/the-danger-of-overfitting-regression-models",317845.0
106184,458355.0,"We try to do EDA, it's not been used for modelling as we take all the features. But, may be it will help in supporting our analysis from model validation, You may find how the dependent variable depends on the independent variable",318329.0
104985,452241.0,"I think it is not generalizing the hyperline. It's memorizing all the points given in train dataset and instead of showing a curve or trend, it is just joining the datapoints. Hence when applying same logic to test dataset, it fails and accuracy is very low.",311404.0
104985,452291.0,"Polynomial degrees related to Overfit is explained with an example in below-mentioned link, Pls refer: https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765. Hope this will clarify.",311117.0
104985,452599.0,"In other words, The fit line is so accurate on the model that it has memorised all the points, hence the polynomial like curve in the fit. Is is bad because as explained, the model will be able to correctly predict the values it has not seen yet and will only be able to predict values that are already present in the system.",308962.0
104985,453026.0,"The degree of a polynomial is the highest degree of its monomials (individual terms) with non-zero coefficients. The degree of a term is the sum of the exponents of the variables that appear in it, and thus is a non-negative integer. When the degree keeps increasing, it is fitting the data points better. The degree can be increased to such an extent that all the points of the data fit into the equation, in which case the train data set is memorised completely. This then becomes an overfir, because this is actually not needed i.e., the overtly accurate nature of the model is unnecessary. This is because when it comes to the test data set, it will make inaccurate predictions as the model is unable to generalise.",310505.0
105010,452363.0,vif is actually the relationship between the variable which you are testing with the other variable and r sqaure gives the error value squared so to reject something we need vif and if vif is high we can say that the variable is correlated to other variable and is insignificant for model but we cant say that using r squared,318017.0
105010,452364.0,"There is a difference in what VIF and R-squared convey i.e, VIF is a metric to calculate the dependence of each independent variable on other independent variables whereas R-squaed is a metric to calculate the variability of dependent variable by independent variables. VIF doesn't at all talk about the output/dependent variable. In the example quoted in this module, The variation in sales price being explained by advertising in TV, newpapers and Radio is given by R-squared. The if values of TV differed based on the values of Newspaper and Radio then that can be quantified by VIF.",318328.0
105010,452476.0,"They both are used to measure different things. R square is a measure of how well the linear regression fits the data (in more technical terms, it is a goodness-of-fit measure): when it is equal to 1, it indicates that the fit of the regression is perfect; and the smaller it is, the worse the fit of the regression is. VIF is used to explain how much amount multicollinearity (correlation between predictors) exists in a regression analysis. Multicollinearity is dangerous because it can increase the variance of the regression coefficients. And so, VIF has to be checked in case of linear regression so the assumption of 'No multi collinearity' holds true.",303229.0
105010,452635.0,both have different uses. VIF is to depict the relationship between variables. R-squared is to depict the variance in the model.,308962.0
105010,452710.0,"Why use VIF and not R-Squared itself: A simple approach to identify collinearity among explanatory variables is the use of variance inflation factors (VIF). VIF calculations are straightforward and easily comprehensible; the higher the value, the higher the collinearity. A VIF for a single explanatory variable is obtained using the r-squared value of the regression of that variable against all other explanatory variables: For further detailed, pls follow: https://beckmw.wordpress.com/2013/02/05/collinearity-and-stepwise-vif-selection/",311117.0
105010,452659.0,"Variance Inflation Factor (VIF) is a common simple stat used to quantify multicollinearity in least squares regressions. It is calculated for each covariate in a regression, with higher values meaning that the covariate is more colinear with the other covariates. It technically measures “how much the variance (the square of the estimate’s standard deviation) of an estimated regression coefficient is increased because of collinearity.” The equation is: where R2i is from the regression of the covariate i on all the other covariates. The problem is where to draw the cutoff? Is a VIF &gt; 2.5 too high? &gt;5? or how about VIF&gt;10, all have been used as cutoffs. Here is a figure of R2 vs VIF. As you can see, a cuttoff of 2.5 is an R2 of 0.60 and 10 is 0.90! While statistically, you could perhaps get away with these high inflations, what does it mean for your particular question? If you are dealing with a relationship among covariates that is as strong as 0.90, can you really be sure that the model and your interpretations are valid?",318451.0
105010,453022.0,"R-square will not tell you anything about multi-collinearity. It only signifies how much of the variance in the data is explained. To firstly detect whether the predictor variables have multi-collinearity i.e., how much they are related with each other if at all and then deal with it for the sake of effectively interpreting the model, you need VIF. Only then you can build a better model.",310505.0
107142,463113.0,"The reason why you have got it as false is, it doesn't fall under the number which you have chosen the RFE to limit the variable count. If you feel, that particular variable is a good candidate to impact the pricing, you can certainly concatenate it to you dataframe and check the stats.",311160.0
107142,463484.0,"same I tried after concatenating these variables and found that stats was not satisfying so decided to go with RFE rank=1 columns so go with rank 1 otherwise even if you will consider the other columns with rank-2, you will end up dropping due to stats somewhere.",300735.0
106284,458920.0,We need to take care about the mis-spelled entries and replace it with the correct spelled company name.,311160.0
106284,458972.0,Correct the spelling using replace(),318344.0
105205,,nan,
106402,459429.0,Drop one at a time because the remaining values will change once you drop a variable.,318329.0
106402,459474.0,Drop one and then rebuild the model and check again VIF. Then you should drop another one delved on high VIF. In this way you will not loose significant variable.,301648.0
106402,459682.0,dont drop 2 variables in one go. Whether high p or high VIF...,304814.0
105226,453208.0,for male and female example you just need one dummy value i.e. anything between male and female and fill that value with 1 wherever its true. if you have selected male as dummy value column fill 1 in the columns where male is true and whenever its 0 in that column it means it is female value so while interpretation you should mention that count number of 0 for female and number of 1 s for male.,318017.0
105235,453226.0,"and also as explained by Rahim, we do it; 1. For the easy interpretation of data 2. For faster conversion using gradient descent method",316349.0
105235,453223.0,Scaling is required to bring all the variables into the same scale of measure so that they can be easily compared and understood. It helps the convergence of gradient descent as well. This is explained in the course content. More https://stackoverflow.com/questions/26225344/why-feature-scaling,310974.0
105235,453561.0,Scaling means that you're transforming your data so that it fits within a specific scale which helps in easier interpretation of the data,300729.0
105235,454617.0,"Apart from easier interpretation it also help in correct and accurate results for the predictions. In non-technical example. Lets say two persons are talking each other about distance between two places. First person, is habituated to measure in miles and the second one is known only kilometers. While conversation first person will talk only in the aspect of no. of miles. However the second person might interpret it in kilometeres. So, here there are two scenarios one is if he knows that the first persion said the value in miles it is difficult to convert to km and use it. second scenario if he thought the distance given by first person is in km then he follow that he might not reach where he is expected to be i.e the accuracy.",314244.0
105236,453225.0,standardization scaling transforms your data in such a way that the mean is scaled at 0 with standard deviation of 1.. whereas minmax scaling is based on the range of data.. it tries to scale and transform your data in the range of 0 to 1.. and we do this for the easy interpretation of data and faster conversion in gradient descent method..,316349.0
105236,453222.0,Standardization Brings data into a normal distribution with mean zero and standard deviation 1 Min Max Scaling Brings data in the range of 0 and 1. More https://sebastianraschka.com/Articles/2014_about_feature_scaling.html#z-score-standardization-or-min-max-scaling,310974.0
105236,453314.0,"Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1. Normalization transforms your data into a range between 0 and 1. For more details with example and graphs, pls refer link : https://stackoverflow.com/questions/32108179/linear-regression-normalization-vs-standardization",311117.0
105301,453416.0,instead of dummy variable you can use label encoder to convert it into numerical value or you can use one hot encoding to convert categorical value to numerical value.,320685.0
105301,453515.0,Number of dummy variables would be 2x1x3 = 6.,310974.0
105301,453559.0,you need to add (n-1) for all the levels and find the answer.,300729.0
105301,453536.0,"For your case: (3-1) + (2-1) + (4-1) = 6 is the correct answer. Just add n-1 for all levels. Dummy Variable is very similar to One Hot encoding except it uses one base case where all coefficients are 0 and rest cases have coefficient as 1. For example we need to encode opinion = {'yes', 'no', 'dont know'} Dummy Variables: Opinion b1 b2 'yes' 1 0 'no' 0 1 'dont know' 0 0 Here now base case is 'dont know' and n-1 variables are introduced with n = levels , as n=3, only 2 required. when both coefficients, b1 and b2 are zero then 'dont know' is implied. This helps reduce penalty on model. One Hot Encoding: Opinion 'yes' 'no' 'dont know' 'yes' 1 0 0 'no' 0 1 0 'dont know' 0 0 1 Here no base case is used and 3 variables are introduced.",313515.0
105353,453692.0,The question I think is will realtive the variable already added it's TV AND sales .Now if u add a new variable then to the TV values that will raise the Rsquare always .,318476.0
105353,453720.0,"Can R Squared increases/decrease with more variables?: The adjusted R - squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R - squared can be negative, but it's usually not. For further details, pls refer : https://en.wikipedia.org/wiki/Coefficient_of_determination",311117.0
105353,453751.0,"The R-squared for simple linear regression using 'TV' as the input variable was 0.816. When you have two variables as input - 'Newspaper' and 'TV', the R-squared gets increased to 0.836. Using 'Radio' along with 'TV' increased its value to 0.910. So it seems that adding a new variable helps explain the variance in the data better.",314183.0
105353,453946.0,"yes, there will be change in the R squared value by combining the two variables in mainly depends on the correlation between the variables, and by this increase in the R squared value we can decide which of the values are to be used to build a useful model, because the R squared value should be high.",304692.0
105353,454468.0,"Adding new variables mostly Increases the R square value because it helps us to explain the variance of the model. As seen in the example although the variables Newspaper and radio did not have a very evident linear relationship with the sales varable as seen in the scatter plot, but ading them still incresed the R square value.",311864.0
106452,459908.0,"In a multiple linear regression how to know when to drop variables? For better understanding, please go through the matter in the link attached: https://stats.stackexchange.com/questions/337487/in-a-multiple-linear-regression-how-to-know-when-to-drop-variables",311117.0
106452,459835.0,"When dropping the variables bring huge change in R-squared and adjusted R-squared value, you can stop dropping the variables.",318429.0
106452,459880.0,You can stop dropping the variable when all the variables are significant means p value is less than 0.05 and vif is less than 5 for all those variables.,320073.0
106460,459901.0,"Try this, car_temp.drop('car_column', axis=1, inplace=True) For more details, pls go through : https://stackoverflow.com/questions/38288372/unable-to-drop-a-column-from-pandas-dataframe",311117.0
106460,459864.0,Can you please paste the exact list of problematic variable names and the code used to drop them?,311160.0
106460,459878.0,"Try giving column names without quotes('' or "" "") around them. Eg. df.drop(column = [1], inplace =true)",320073.0
106460,460072.0,"X = X.drop('1', axis = 1 , inplace = True) KeyError: ""['1'] not found in axis"" Same error pops up",319759.0
106460,460150.0,"As Bhanu suggested, try below: X = X.drop(1, axis=1, inplace=True) Do not include quotes around 1",311160.0
106460,460185.0,"Hi, try this df.drop(df.columns[cols_index],axis=1,inplace=True)",344894.0
106483,460063.0,"Due to Multicollinearity, the coefficient estimates can swing wildly based on which other independent variables are in the model. Coefficients become very sensitive to small changes in the model. You cannot trust your P-Values.",311160.0
106483,460124.0,"Yes R2, F statistic, AIC, BIC aren't affected. In addition, Please follow the attached link http://blog.minitab.com/blog/adventures-in-statistics-2/what-are-the-effects-of-multicollinearity-and-when-can-i-ignore-them",310974.0
107896,466078.0,you can do it but its recommended to make a new model to compare it with the previous model performance.,318017.0
107896,466079.0,lm_1 is already a model that was built with various steps. (through RFE and running VIF and checking the significant of all the co-efficients),317993.0
107896,466107.0,""" y_train_predicted will become my independent variabale and y_train will become my dependent variable"" If I have understood it correctly, what you are suggesting is: y_train = Bo + B1*y_train_predicted Imagine if B1 was 1 (for sake of simplicity, but the argument can be generalized), the equation will become: y_train = Bo+ y_train_predicted or, y_train-y_train_predicted = Bo y_train-y_train_predicted is actually the sum of residuals (error terms). If we now divide both sides by n, we get: Mean Error = Bo/n This violates the linear regression assumption that mean error/ sum of residuals should be 0",305653.0
105385,453983.0,"Your question is more about Over-fitting than multicollinearity. Let's say you are predicting someon'e height based on someone's weight. Now let's say you add more variables to the mix Gender, Education, Birthplace. Some might be relevant, some might not. Let's say you find that from the Train data set that post graduation students have higher weight. Now logically you know there's no connection, but the machine has learnt this connection. Now when you use this algorithm on Test set the accuracy goes down. That's why it's essential to remove irrelavant and similar variables before you train the data set.",304022.0
105385,453934.0,"Higher the number of variables, there are good chances that the variables may not be totally independent. The degree of dependency between each other will vary which makes the prediction more difficult and unreliable That is the reason why we have to strike a balance while planning to increase number of variables as there is a trade off / That is the main reason they have formed technique AIC where they penalize when number of variables are increaased You need to maintain a balance between keeping the model simple and explaining the highest variance (which means that you would want to keep as many variables as possible). This can be done using the key idea that a model can be penalised for keeping a large number of predictor variables Akaike information criterion ( AIC ) is a fined technique based on in-sample fit to estimate the likelihood of a model to predict/estimate the future values. A good model is the one that has minimum AIC among all the other models. ... A lower AIC indicates a better fit.",301121.0
105385,453975.0,"For a good regression model, you want to include the variables that you are specifically testing along with other variables that affect the response in order to avoid biased results. Minitab Statistical Software offers statistical measures and procedures that help you specify your regression model. Adjusted R-squared and Predicted R-squared: Generally, you choose the models that have higher adjusted and predicted R-squared values. These statistics are designed to avoid a key problem with regular R-squared—it increases every time you add a predictor and can trick you into specifying an overly complex model. The adjusted R squared increases only if the new term improves the model more than would be expected by chance and it can also decrease with poor quality predictors. The predicted R-squared is a form of cross-validation and it can also decrease. Cross-validation determines how well your model generalizes to other data sets by partitioning your data. P-values for the predictors: In regression, low p-values indicate terms that are statistically significant. “Reducing the model” refers to the practice of including all candidate predictors in the model, and then systematically removing the term with the highest p-value one-by-one until you are left with only significant predictors. Stepwise regression and Best subsets regression: These are two automated procedures that can identify useful predictors during the exploratory stages of model building. With best subsets regression, Minitab provides Mallows’ Cp, which is a statistic specifically designed to help you manage the tradeoff between precision and bias.",318017.0
105516,454564.0,"I have only come across MinMax scaling and Standardization in the course so far. Using a common scaling across the distribution wouldn't guarantee the changed values to be within certain range. Thus, we would either need to consider max, min or the mean.",318329.0
105516,454595.0,"As per Princess Sumaya University for Technology , It is not necessary to take log (or ln) of any variable. In some cases you can try using variables at level and variables with log then compare the performance of the models and choose the one with higher performance. Concerning that some variables are stationary at level and others are at first difference. For further details, please go through the link: https://www.researchgate.net/post/When_should_I_take_log_of_variables",311117.0
105516,454853.0,There are two more scaling method I came across. You can go through below links which provide good explanation on them https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e https://en.wikipedia.org/wiki/Feature_scaling Hope this will help.,317991.0
105557,454845.0,You can go through below links where it has been explained about standardization vs MinMax scaling https://sebastianraschka.com/Articles/2014_about_feature_scaling.html https://www.quora.com/When-should-you-perform-feature-scaling-and-mean-normalization-on-the-given-data-What-are-the-advantages-of-these-techniques Hope this will help.,317991.0
105557,454830.0,MinMax Scaling can create problems when there are large outliers in your data. Suppose your max value is very high. Then rest of the data will get sqeezed into a small interval near Zero after MinMax transform. But MinMax gives you a gauranteed bounded data after transform which means all data will be within 0 and 1. Also you don't get any negative values in MinMax scaling. Some algorithms needs such bounded data and only positive values to perform better. So if you are not training some neural network that needs only positive values/ values b/w 0 &amp; 1 then you can use Standardisation always. Read more here: https://stackoverflow.com/questions/32108179/linear-regression-normalization-vs-standardization,313515.0
105582,454993.0,"Its very logical question raised: I tried to understand the logic of the question and found a bit sight from the pdf file attached from IITK (Page no. 4-5 in example), and page no 7-8 (Dummy variables versus quantitative explanatory variable). It may be helpful for you perhaps. Pls go through the link of pdf file: http://home.iitk.ac.in/~shalab/econometrics/Chapter10-Econometrics-DummyVariableModel.pdf",311117.0
105582,455051.0,"We create Dummy coloumn and populate it with 0 or 1 just to account the Categorical Variables in the fit. When we do so, we will be checking the levels the Categorical variable eventually has. As eplained in the example say we are dealing with Gender coloumn. It only has two levels. Male or Female (excluding the third gender) . In that case it's enough to create 1 dummy coloumn to account it. The logic being : If one is not a female,he is obviously a male. We dont lose anything by dropping a level.",318435.0
105582,455766.0,"In a simple term, Let’s say, we have a data set with features X is [ID, Surname, Age, Country] as follows categorical column called “Country” and its values are - [India, Germany, France] In ML regression models, predictions will do the good job if categorical values are converted into numerical (binary vectors ) values. Encoding categorical data technique to apply for the above categorical set and the values a.k.a dummy variables will become This is called - One hot encoding technique. Dummy Variable Trap : With one hot encoding conversion, we have three columns in place. By including dummy variables in a regression model, we should consider to drop a column - “Dummy variable trap” N - 1 dummy variables (It is always a good practise to use minimal set of features to apply ML techniques and create regression model.) . In the above dummy variables table, we should consider to drop any of one columns. Let’s drop the column “Germany” and the final table looks like in below Thumb Rule :- Which dummy variable column do we need drop? The answer is - we can drop any of one dummy variables column. It can predict the dropped column’s value based on other two columns. Let’s take the record no 3 in the above table, both dummy variable values are ‘0’. So obviously another dummy variable column value is ‘1’ and categorical value is ‘Germany’. Technically, the dummy variable trap is a scenario in which the independent variables are multi-collinear - two or more variables are highly correlated. https://www.quora.com/When-do-I-fall-in-the-dummy-variable-trap",318451.0
105613,,nan,
105674,455851.0,"If the p value for a variable is high, then it means that it is less significant. That's the reason we try to drop the features one by one which has high p value. You get this value from the summary of stats model",318329.0
105674,455860.0,we drop all the parameters from the model which has high value (&gt;0.05) p value since it is not significant in your model to predict the outcomes.. manually you try to do it by eliminating each such feature with higest pvalue one by one since each parameter based on its correlation value could affect the other variables pvalues.. i hope it answers your doubt..,316349.0
106802,461328.0,use 63 in place of 62.,313767.0
106802,461271.0,Are you using the same features that you scaled your X train?,318329.0
106802,461285.0,Iam using the last varibles left out after eliminating the rest.,319759.0
106802,461547.0,use transform on the same number of variables using in training dataset.,317689.0
106802,461289.0,we fit on the variables before doing. try those features,318329.0
106810,461324.0,suppose you have to decide the price of a wine then price is dependent variable and age and flavour can be independent variables but its possible that age and flavour are related to each other also so variable are independent with rest to dependent variable and related to each other.,318017.0
106810,461579.0,"Not all the variables will be totally independent of each other in many practical cases. Ideally we want them to be totally independent but there is a degree of dependency. This dependency is due to multicollinarity and all our approach in the whole session is to see to understand the degree of dependency using vairous factors, coefficients, p-value etc by manual methods as well as RFE. I hope this clarifies.",301121.0
106822,461372.0,"Yes, you should convert that to dummy variable, But I also think for category less than 3 we can simply put 1 or 0 instead of creating a dummy variable.",301648.0
106822,461353.0,You have to convert them to dummy variables as Linear Regression cannot be performed with non numeric inputs,311160.0
106822,461566.0,"Even though they look numerical, they are not numercal but categorical variables and hence dummy variable have to be created. I would suggest you to have a look in the data dictionary provided along with assignment,",301121.0
106822,461679.0,It will be good to convert them to numeric values.,315679.0
115908,501679.0,"Hi, Please find the below source code for linear regression in sklearn You can take hints from there. https://github.com/scikit-learn/scikit-learn/blob/7b136e9/sklearn/linear_model/base.py#L362",344894.0
115908,501442.0,"interesting question Rohit. Although the purview of your question is really REALLY vast. Before I answer, do tell me, if you are comfortable with the idea of an algorithm? Secondly, you want to write an algorithm that generates a model? or what is the purpose something else? Do let me know as I am really intrigued by this topic of yours :)",305839.0
122877,534362.0,Dummy variables are created from one-hot encoding scheme. Range varies between 0 and 1. You dont need to scale them.,307176.0
122877,534302.0,"No scaling is not required for dummy variables. We need scaling for variables as some of them might be on different scales for example prices can be in thousands or millions whereas other variables like height/width area of building would not be on that same scale. So, to bring everything on similar scale, we require scaling of variables. Dummy variables are in 1 or 0. So, they are already on scale. No scaling is required for them. Refer below: https://stackoverflow.com/questions/50557129/dummy-variables-is-necessary-to-standardize-them",317689.0
114072,491886.0,Yes it definitely takes care of the collinearity between the variables and you can create variables based on the type of metrics:,310974.0
114072,492054.0,In that case we can drop one variable to avoid colinearity. Moreover if you want to derive a variable from two you can approached mentioned by Ram in your priliminary EDA part.,301648.0
114072,492130.0,"Typically when two variables have high VIF, those need to be removed from the model data. Also, creation of a new variable (or derived metric) is to be done after data quality checks are over and before we start EDA or start building a model. The new variable is to be created based on understanding of the raw data given. Eg: You have a date timestamp column and want to derive the month or year out of it, then you can create a new variable 'month' or 'year'. This action does not require us to check for VIF value first. This action is to be taken much earlier in the program. Hope this clarifies!",308435.0
122876,534723.0,"Hi, If you do not scale the variable then you will not be able to explain your results. For example : Not Scaled Regression y = 10X1 + 300 X2 + constant From above equestion we can say that X2 has more importance then X1 for same data if we do scale we might get y = 500X 1+ 200X2 + constant From above equestion we can say that X1 has more importance then X2. So this true because we have all the variables in a range and we can compare but for non-scalable, we can not compare",344894.0
122876,534623.0,"Check the below link explain why we need scaling and when: “Why, How and When to Scale your Features” by Sudharsan Asaithambi https://link.medium.com/irDGWSV8aW",318476.0
122878,534622.0,"We can speed up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. check out the video on the same by Andrew NG https://youtu.be/gV5fD8Xbwgk",318476.0
115622,499231.0,"Hi, Turns out they can indeed be included in case of some special considerations. Check out the link below to learn more https://statisticalhorizons.com/multicollinearity. See, VIF indicates the inflation factor of the coeeficient - mostly it determines how much the coefficient of a predictor is inflated or, in simple terms, misleading. Mostly it is a very robust indicator to drop a variable and you should. But the above blog showcases some thoughts about when you can ignore the same and simply retain the variable. Personal opinion, remove the variables with high VIF and make your model leaner. Regards,Soumik",305839.0
115622,499055.0,"It is highly unlikely that you would be not be removing a variable with high VIF. If it is high VIF for one feature, it means feature(s) which are correlated to this variable are good enough to describe the model, hence the variable with high VIF needs to be dropped. Keeping that will distort the model.",301121.0
115622,499075.0,"HI Ramkamal, NO, there is no exception like a feature varaible with high VIF exists but still we should not remove it. We have to drop variable with high VIF bcoz the presence of that variable shows that multicollineraity exists, actually Multicollinearity is the effect of having related predictors in the multiple linear regression model. In simple terms, in a model which has been built using several independent variables, some of these variables might be interrelated, i.e. some of these variables might completely explain some other independent variable in the model due to which the presence of that variable in the model is one of main considerations of multiple linear regression is also that there should be no multicollinearity in the model to make sure model is stable one. so we have to remove variable with high VIF for sure",300733.0
114921,495942.0,"maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters.",313200.0
115339,496959.0,"They are different. If you need just the interpretation of the predictor variables, you have to choose them carefully using business understanding and multi-collinarity. If you need just the prediction, you can get rid of them using PCA and focus only on the predictive power of the model.",310974.0
134141,583529.0,"There is no attachment. Also, please give more details about the dataset being used.",313826.0
134141,583486.0,"In my opinion, variable having High correlation with another one, should be dropped. This is because the variable would become redundant in terms of helping in predicting and thus would lose its value as a predictor variable. hence should be dropped. Furthermore, the variables VIF values should also be checked, since there is high propability it would have high value because of high colinearity. variables with VIF more than 5 should be dropped as a thumb rule. Hope that helps.",317998.0
134141,583887.0,"Hi, If you do any mathematical operation on any variable, it's a correlation with another variable will not change. We do log transformation if our target variable is skewed, after doing log transformation target variable will have approximate normal distribution as well it will have the same correlation with other variables.",344894.0
105313,453547.0,"reshape is for numpy and not available to pandas Series Object. You can access underlying values in pandas Series using .values operator - Correct Code: X_train_lm = X_train_lm.values.reshape(-1,1) X_test_lm = X_test_lm.values.reshape(-1,1) Look here :https://stackoverflow.com/questions/42240376/dataframe-object-has-no-attribute-reshape",313515.0
105313,454447.0,I am also facing same issue.,301107.0
105313,454462.0,"Below code is working for me : X_train_lm.values.reshape(-1,1)",301107.0
105796,456429.0,I think you'll be provided with the sample standard error which may be equal to your standard deviation,301655.0
105796,456431.0,"Check out this link, it can be useful: https://stattrek.com/regression/slope-test.aspx",318448.0
105464,454481.0,"Root Mean Square Error (RMSE) measures how much error there is between two data sets. In other words, it compares a predicted value and an observed or known value. It's also known as Root Mean Square Deviation and is one of the most widely used statistics. For more details, please go through the link: https://en.wikipedia.org/wiki/Mean_squared_error",311117.0
105464,454630.0,MSE is average of square of errors. Larger MSE means larger error. Error is difference between observed and predicted values. The difference is squared because positive and negative values should not cancel out each other.,301114.0
105464,454875.0,"Hi, mean square error (MSE)— is the average of the square of the errors. The larger the number the larger the error. Error is this case means the difference between the observed values y1, y2, y3, … and the predicted ones pred(y1), pred(y2), pred(y3), … We square each difference (pred(yn) – yn)) ** 2 so that negative and positive values do not cancel each other out.",344894.0
106038,457526.0,The video link provided for the same did not answer much,305655.0
106038,457769.0,Go through the pdf file (pg no. 1-6) from IITK as attached in link below: http://home.iitk.ac.in/~shalab/regression/Chapter2-Regression-SimpleLinearRegressionAnalysis.pdf This may help you.,311117.0
106038,457967.0,"If the number of samples/observations are less than 30 , then T distribution is used to check significance. If the number of samples exceed 30 t disbributes approximates to normal distribution.It serves both purpose- for sample less than 30 and greater than 30",310629.0
104828,451682.0,"The question pretty much answers the question. By assuming that the line is horizontal, we are assuming that the independent variable, x has no effect on dependent variable y. B1 and so X becomes significant only if we disprove our Null Hypothesis",304022.0
104828,451711.0,"β1=0, and X1 and X2 are uncorrelated, then formally, in the population of our data, x1 is uncorrelated with y, linearly independent of y",318451.0
104828,451943.0,"As per hypothesis testing we can only successfully disprove the null hypothesis. Hence, we assume there is no correlation between x and y. therefore NULL hypothesis is β1=0",317689.0
104830,451681.0,Here the sample refers to the y co-ordinates of the presumed best fit line for a value of x and population is all possible values of y for that value of x,304022.0
104830,451709.0,Inference for the Population Intercept and Slope Refer this link to get more clarity they have explained it with an example - https://newonlinecourses.science.psu.edu/stat501/node/261/,318451.0
104830,452327.0,"What I don't understand is, if we fit a line through the data, we will know visually if the slope if 0 or not right? Why do we have to do hypothesis testing for that? Instead it should be how well the derived value of slope represents the data right?",310974.0
104830,452478.0,"Here β1 is the slope of the line. As we stated the null hypothesis is β1 =0, this means the slope of the line is zero until proven otherwise. Until we don't reject the Null hypothesis slope the line is zero. As the slope is Zero for all possible values of X, mean for the slope for all values of X is also Zero. Hope this helps.",306725.0
104854,451836.0,"If an independent variable is correlated with the error term, we can use the independent variable to predict the error term, which violates the notion that the error term represents unpredictable random error. Below is a good like explain the assumption made in a linear regression and why: http://statisticsbyjim.com/regression/ols-linear-regression-assumptions/",318476.0
104854,451821.0,Because the violation of this rule will result in distortions in the regression coefficients and / or weaker analysis / weaker fitting of the regression line.,308962.0
104854,452265.0,"To simplify the concept, let's take an example of Height and Weight. Let's say we pick all people at 60kg weight, we expect all these people's height at that wieght to follow a normal distribution, peaking let's say 5'5 ft, which is predicted by our best fit predicted line. Let's take other two examples of 65kg and 70kg. In these case we expect the height's to follow a normal distribution, i.e. by chance how it it happens in nature and all possible heights peaking at some value predicted by our line. But if we observe the pattern i.e Height's are not following a normal distribution, it's distorted towards left ot right, X weight is surely having an effect on the possible values of height and hence it's putting an undue influence and predicting height that we are force fitting, hence it's not a good model",304022.0
104854,451941.0,"This means that the one error term should not be able to predict the next error term. For example, if the error for one observation is positive and that systematically increases the probability that the following error is positive, that is a positive correlation. If the subsequent error is more likely to have the opposite sign, that is a negative correlation. This problem is known both as serial correlation and autocorrelation.",317689.0
104878,451959.0,The below will help to understand all the terms :,318476.0
104878,451976.0,Mean sum of squares: Mean sum of squares is an important factor in the analysis of variance. It is an integral part of the ANOVA table. The sum of squares is divided by the group degrees of freedom to determine the mean sum of squares (MSB). Sum of squares is a statistical approach that is used in regression analysis to determine the spread of the data points. The mean sum of squares can also be defined as the variance of the set of scores. https://www.chegg.com/homework-help/definitions/mean-sum-of-squares-31,318451.0
104878,451936.0,Mean sum of squares is a measure which tells you the variance of the data points and is obtained by (sum of squares of differences of data points from the mean) divided by the degrees of freedom. A larger MSS means that there is larger variation from mean.,318329.0
104878,451927.0,its mean sum or squares : Mean sum of squares is an important factor in the analysis of variance. It is an integral part of the ANOVA table. The sum of squares is divided by the group degrees of freedom to determine the mean sum of squares (MSB). Sum of squares is a statistical approach that is used in regression analysis to determine the spread of the data points. The mean sum of squares can also be defined as the variance of the set of scores.,318017.0
104878,453262.0,Model sum of squares. Basically MSS+RSS=TSS,300706.0
105493,454503.0,"I don't think it is just a number and probably has some significance. value of R-square , Adjusted r-square , f-statistic , t-ststistic, and coefficients change if we change random number. See below: With ""42"" , look at R-Square and Coefficient for the following statement: X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 42) with changed random_staste to 100, this is what is seen: X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)",309211.0
105493,454366.0,"It doesn't matter if the random_state is 100 or 42 or any other integer. What matters is that it should be set the same value, if you want to validate your processing over multiple runs of the code. random_state as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case. Its just a number which is sent to the random number generator function as a parameter. It generally can be anything.",318435.0
105493,454790.0,"Changing the random_state will definately change all the resulting statistics but that does not mean it has any significance. A different random_state value gives you a different split of test train data. Because underlying train data changed, so your model also changed and its statistics with it. But still you can see both models are more or less same, this difference will grow negligible if you have a large dataset. Also, when we start using cross validation later to avoid overfitting, this wont be even an issue.",313515.0
104911,452093.0,"R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots. R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data! Furthermore, if your R-squared value is low but you have statistically significant predictors, you can still draw important conclusions about how changes in the predictor values are associated with changes in the response value. Regardless of the R-squared, the significant coefficients still represent the mean change in the response for one unit of change in the predictor while holding other predictors in the model constant. Obviously, this type of information can be extremely valuable.",318017.0
104911,452145.0,"Ok I just googled out and see two blogs having answer: 1) http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit 2) : https://www.theanalysisfactor.com/small-r-squared/ If TA can validate my answer it is much appreciated. Also, if additional learning links or references are provided , that is appreciated as well. Thanks!",309211.0
104983,453180.0,"Hi Monisha, Please follow the attached link for derivation https://stats.stackexchange.com/questions/50156/are-standardized-betas-in-multiple-linear-regression-partial-correlations",344894.0
105000,452360.0,"Multiple regression is using more than 1 independent variable.. As long as you see all the points satisfied on residual error minimization, low variance in data, high value of F-statistics on both the train and test data using statsmodel or sklearn on a single independent and single dependent variable, siMple regression model gives you a good fit of model to your predictions..",316349.0
105000,453181.0,"Hi, Please follow the attached link for how to get a better linear model https://scikit-learn.org/stable/modules/linear_model.html https://www.ritchieng.com/machine-learning-evaluate-linear-regression-model/",344894.0
105015,452398.0,your calculation is correct.. but the acccuracy here depends on the number of digits that you consider after decimal.. tried with standard error of 0.0022 it comes out to be 24.77 .. may be python is considering more number of digits while actually calculating the value but the summary representation is restricted to 3 digits after decimal..,316349.0
105025,452422.0,"Ok Got it. It should be # linear regression from sklearn expects 2d array x_train_lm=x_train_lm.values.reshape(-1,1) x_test_lm=x_test_lm.values.reshape(-1,1)",312019.0
105025,452410.0,series actually doesn't have reshape. Don't know how it worked in the provided ipynb file use x_train_lm.values.reshape,318329.0
105025,452747.0,This is reported on the page as error and I think TA's will fix it. Thanks!,309211.0
105036,452495.0,"it's not very readable. but only thing I could get from this is, the p-value of the variable TV is 0.000 which means that the variable is significant.",318329.0
105036,452497.0,coef are the coefficient values for β0 = 6.9487 and β1=0.0545 std err is the standard error in the coefficient values t is the t-value calculated by coef/std err P&gt;|t| is the p-value for the above t-value [0.025 0.975] are the values for the coefficents at the confidence intervals. Hope this helps,306725.0
105036,452498.0,It tells you that beta1 is 0.0545 (which rejects the null hypothesis stating it as 0 which makes it significant) and err is 0.002 which helps in calculating T value which is .0545/.0022 gives 24.7 which is again significant makiing the p-value 0 and the confidence intervals are away from 0,311160.0
105036,452573.0,This explains the coefficients and intercept of the line equation. Ideally you would want to have the coefficient with almost 0 p value. This coeffiecent is != 0 and is having p value ~0 which means this rejects the nulll hypothesis.,317689.0
105036,452626.0,How to Interpret Regression Analysis Results: P-values and Coefficients? refer this link - http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients,318451.0
105049,452559.0,A plot is the easiest to find out but you can always calculate the Pearson correlation coefficient and find out if there is a +ve or -ve or no correlation between the variables.,310974.0
105049,452565.0,The scatter plot is the easiest way to figure out any relationship. But you can look to the correlation values also.,318476.0
105049,452571.0,Checking the correlation coefficient and plotting is the easiest way to check if there is any linear relation between variables.,317689.0
105049,452572.0,"Expressing the equation as Y+C1 = A(X+C2)+B might clarify. C1 and C2 being the noise in Y and X measurements, respectively If you have a reason to expect the relationship to be a straight line, just calculate the errors in X and Y measurements, draw the resulting rectangle for each data point and see if a straight line can be found that intersects all of them. With only 7 or 8 points it should be no big job.",318451.0
105049,452584.0,"Depends on how the data were collected or how the system was simulated. It also depends on how the regression analysis was performed. Was the regression based on a well known model, or based on ""best fit?"" What type of regression was used? There are a large number of choices here. It includes transformations of the data (log, square root, etc...). As a set of random words that could be used to describe a regression model: polynomial, ridge, segmented, repeated measures, logit, stepwise, and the list goes on. Were any of the assumptions of the model tested, or can you rely on the authors to have considered the consequences of the model failing those assumptions. The real default is that the model fails all assumptions, so the real questions are ""does that matter"" and ""was anything done to reduce the consequence of such failure?"" Are you asking about results that you have generated or interpreting results others have generated? Are you asking what conclusions you can draw about data that you have, or about figuring out if the conclusions someone else made about their data are appropriate or relevant to your work? It is really very easy to draw inappropriate conclusions from regression analysis. You have asked a harder question, so we need more details.",318017.0
105049,454016.0,A simple way to check linearity is to draw scatter plot and the other way is to find the Pearson's correlation coefficient r. This coeffiicient is used for measuring the strength and direction of linear relationship between the two continuous variables. The values of r always lie between -1 and +1 If r is excatly zero then we can say variables are not linearily related The following links explain methods of finding correlation coefficient https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/correlation-coefficient-formula/#Pearson You can download this material which explain methods with worked examples http://www.cimt.org.uk/projects/mepres/alevel/stats_ch12.pdf,308638.0
105032,452529.0,Move to a higher version of SKLearn and you might not found this error.,318476.0
105032,452490.0,"Series.reshape(*args, **kwargs) is deprecated since version 0.19.0: Calling this method will raise an error. Please call X_train_lm = X_train_lm.values.reshape(-1,1) X_test_lm = X_test_lm.values.reshape(-1,1) Reference: https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.Series.reshape.html",311160.0
105092,452685.0,It is due to you have taken only 3 digits after the decimal in Standard Error. Refer this dicussion - https://learn.upgrad.com/v/course/208/question/105015,318451.0
105092,452778.0,the calculation take .0022 in place of .002,318017.0
105137,452823.0,Interpret the key results for Correlation refer this link - https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/correlation/interpret-the-results/,318451.0
105137,452815.0,"The correlation coefficient, r , tells us about the strength and direction of the linear relationship between x and y . However, the reliability of the linear model also depends on how many observed data points are in the sample. We need to look at both the value of the correlation coefficient r and the sample size n , together. We perform a hypothesis test of the “ significance of the correlation coefficient ” to decide whether the linear relationship in the sample data is strong enough to use to model the relationship in the population. Performing the Hypothesis Test Null Hypothesis: H 0 : ρ = 0 Alternate Hypothesis: H a : ρ ≠ 0 What the Hypotheses Mean in Words Null Hypothesis H 0 : The population correlation coefficient IS NOT significantly different from zero. There IS NOT a significant linear relationship(correlation) between x and y in the population. Alternate Hypothesis H a : The population correlation coefficient IS significantly DIFFERENT FROM zero. There IS A SIGNIFICANT LINEAR RELATIONSHIP (correlation) between x and y in the population. Drawing a Conclusion There are two methods of making the decision. The two methods are equivalent and give the same result. Method 1: Using the p -value Method 2: Using a table of critical values",318476.0
105137,452935.0,refer this link - https://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/11-correlation-and-regression,310952.0
105171,452970.0,"The short answer is that as n approaches infinite (or the population total with a parameter that is distributed along a z distribution), the T distribution converges to the Z distribution.Think of Z as the abstraction and T as the real world approximation.",318451.0
105171,453139.0,"If the sample size is small, i.e. less than 30, use t-score and if You know the standard deviation of the population and your sample size should be above 30 use z-score.",310952.0
105171,453179.0,"In our analysis, the sample size is less. It could be less than 30, for which T-value analysis is considered better that z-score analysis.",317987.0
105171,453291.0,"T-score vs. z-score: When to use a t score The general rule of thumb for when to use a t score is when your sample: Has a sample size below 30, Has an unknown population standard deviation . You must know the standard deviation of the population and your sample size should be above 30 in order for you to be able to use the z-score. Otherwise, use the t-score. When to use a t-score vs. z-score. For further clarification, pls follow link: https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/hypothesis-testing/t-score-vs-z-score/",311117.0
105150,452842.0,in the calculation in place of .002 keep .0022 you will get teh same answer this is the python mistake or you can say its how it is working.,318017.0
105150,453185.0,"Hi, Please refer to attached discussion https://learn.upgrad.com/v/course/208/question/105092",344894.0
105038,452622.0,Making Predictions with Regression Analysis Ya the title me not directly relate to your question but the flow of article such that by the end your question will be solved. Hope it helps refer this link - http://statisticsbyjim.com/regression/predictions-regression/,318451.0
105038,452583.0,"Depends on how the data were collected or how the system was simulated. It also depends on how the regression analysis was performed. Was the regression based on a well known model, or based on ""best fit?"" What type of regression was used? There are a large number of choices here. It includes transformations of the data (log, square root, etc...). As a set of random words that could be used to describe a regression model: polynomial, ridge, segmented, repeated measures, logit, stepwise, and the list goes on. Were any of the assumptions of the model tested, or can you rely on the authors to have considered the consequences of the model failing those assumptions. The real default is that the model fails all assumptions, so the real questions are ""does that matter"" and ""was anything done to reduce the consequence of such failure?"" Are you asking about results that you have generated or interpreting results others have generated? Are you asking what conclusions you can draw about data that you have, or about figuring out if the conclusions someone else made about their data are appropriate or relevant to your work? It is really very easy to draw inappropriate conclusions from regression analysis. You have asked a harder question, so we need more details.",318017.0
105038,452528.0,there are multiple stats number that tell you where your model is significate or not. F stats :Shows the over all significate of the liner model. R Square : Tells % of variability in x explain by y. P value associated with the slope tell you where the slope is significate or not. Also to check the resuidual plot the check the fit.,318476.0
105229,453205.0,"R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. In statistics, the coefficient of determination, denoted R2 or r2 and pronounced ""R squared"", is the proportion of the variance in the dependent variable that is predictable from the independent variables Please follow the attached link for more information https://stats.stackexchange.com/questions/142248/difference-between-r-square-and-rmse-in-linear-regression https://www.theanalysisfactor.com/assessing-the-fit-of-regression-models/",318017.0
105161,453023.0,"For the simple linear regression model, you need to split your dataset into Train and Test dataset.. Train dataset is something on which you build your model and you test it on Test dataset.. this splitting of dataset is achieved using this function; train_test_split.. you can pass the arguments with the quantities in whcih you want that to be splitted (80,20) or (70,30) etc.. i hope this helps in understanding..",316349.0
105161,452937.0,train_test_split fucntion splits your data according to the arguments you pass to it. Like you may want to split the data into training and testing data in 70% and 30%. It basically let's you develop the model by identifying the patterns in the training data and later test the efficiency of the model on the test data. This is explained in detail in the course videos.,310974.0
105161,453187.0,"I would try to explain using the official documentation: sklearn.model_selection.train_test_split ( *arrays , **options ) --&gt; Split arrays or matrices into the random train and test subsets X_train, X_test= train_test_split(X) --&gt; Splits the list 'X' into two random lists, firsts one is training list and the second one is the testing list. By default 75% data goes in the training list and 25% goes in the testing list. X_train, X_test, y_train, y_test = train_test_split(X, y) --&gt; Splits the list 'X' and 'y' into four (2*number of lists passed) random lists, amongst the first two lists returned, firsts one is training list and the second one is the testing list for list 'X'. Similarly, amongst the next two lists returned, firsts one is training list and the second one is the testing list for list 'y'. For more details, please refer: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html",317987.0
105228,453206.0,"In statistics, the coefficient of determination, denoted R2 or r2 and pronounced ""R squared"", is the proportion of the variance in the dependent variable that is predictable from the independent variables",318017.0
105228,453385.0,"In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors —that is, the average squared difference between the estimated values and what is estimated. Further, I have replied the similar query in the attached link: https://learn.upgrad.com/v/course/208/question/105231",311117.0
105228,453719.0,"Hi, Please follow the attached link for your question https://stats.stackexchange.com/questions/118/why-square-the-difference-instead-of-taking-the-absolute-value-in-standard-devia Thanks",344894.0
105231,453203.0,error term are two one is sse which is the distance of predicted best fit line and actual data point and other is ssr which is the distance between predicted best fit line and line passing throught the mean of y value from the given points you can see the mean and draw a horizontal line . yoi can look at the image i have attached.,318017.0
105231,453227.0,"error term is the residual value which is; the distance between the actual data point and the predicted value (which lie on straight line passing through data points, when you draw a perpendicular from the actual point)..",316349.0
105231,453321.0,"An error term is a variable in a statistical or mathematical model , which is created when the model does not fully represent the actual relationship between the independent variables and the dependent variables. The error term is also known as the residual , disturbance, or remainder term . For More detialed explaination with Graph, pls refer : https://www.statisticshowto.datasciencecentral.com/error-term/",311117.0
105231,453476.0,"When we draw a linear regression line for every value of x, we get a value of y which is the predicted value. But there is an actual value of y too. The difference between the actual value of y and the predicted value is called the error term or residue.",304319.0
105308,453448.0,"Apart from what you had mentioned, in the same excel sheet you will see the straightline being fitted in excel sheet along with the equation of straight line in the form y = c + mx = B0 + B1x. Co-efficient of x is B1 and intercept in y axis is B0.(B1 is slope and B0 is the intercept) I hope this clarifies.",301121.0
105308,453556.0,"Obtaining RSS TSS are not for understanding the correlation of the two variables FYI . RSS and TSS are used to see the goodness of the fit after plugging the data into the model. B0 and B1 as obtained as in the final answer are the fit's slope and Y intercept respectively. For example: if y=3+2x is the obtained fit. Then here, 3 is B0 and 2 is B1 (which is also the slope).",318435.0
105308,453605.0,RSS/TSS calculated to find how effective/best you model is by calculating R-square value which is 1-RSS/TSS if R-square is close to 1 then model is best / effective elase not effective,315455.0
105308,453542.0,"""B0"" and ""B1"" are an ""intercept"" and ""slope"" respectively, and are an annotation like ""c"" and ""m"" in straight line equation, y = c + mx. It is just a different Countries teach different ""notation"", which is also given in the "" link"" in the topic "" Regression line "". You can go through on that link, which is mentioned below: https://www.mathsisfun.com/equation_of_line.html",311117.0
105302,453419.0,x_train and y_train are data frames which is used to train the model and head() just returns the top five rows of the data frames,320685.0
105302,453565.0,"Open the CSV file - Advertising in excel, and ctrl+f the 1st value of 213.4, so you will see 1st coulmn is the index value ( starting from 0, hence the row no is 74) 2nd column is value of X, i.e. for Column TV, row 74, i.e. 213.4 1st col, index value, 2nd col, X value of parameter hope above helps",308495.0
105302,454373.0,"For obtaining the train and test data set, we are dividing the whole data in X and y in a 70:30 ratio using the train_test_split() function: X_train, X_test, y_train, y_test = test_train_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100) X_train contains 70% and X_test contains 30% of values which were in X y_train contains 70% and y_test contains 30% of values which were in y",318334.0
105305,453454.0,"When R-Squared =1, you can fit a straightline which will go through all the set of points collected. But in real life they will be scattered and many may fall out of straight line. When R-Squared is getting lower, these points may be scattered on both sides of line. When it is very low, scattering of these points may be so much that variance can not be explained as they are not contained. For example, R-squared = 0.9, we can explain that this line represents these points with defined variance and we can explain. When R-Squared is low, we are not able to explain the variance which means it is so random it is difficult to fit a line and difficult to explain that this straight line can represent these points with contained variance. In other words, train points have so much of variance which can not be explained, when R-squared = very low.",301121.0
105305,453699.0,Refer below link for clear understanding: http://statisticsbyjim.com/regression/low-r-squared-regression/ https://stats.stackexchange.com/questions/44484/why-is-my-r-squared-so-low-when-my-t-statistics-are-so-large,311117.0
105319,453511.0,"Intercept is needed since if intercept is not there, y becomes 0 if x=0 which need not be the case. x=0 doesn't necessarily mean the absense of the variable x but it could be a value itself.",310974.0
105319,453514.0,"You think of a equation of straight line which y = c + mx In thsi case it is y = b0 + b1x where b0 is the intercept in yaxis. Without add_constant command, the python command for fitting the line, will make the line go through the origin which may not be desirable for the line to be fit with those points. (Most of the cases, it may not be a best fit at all without intercept.) If there is no intercept it will be simply y = mx which means that there is no offset at all and fitted line will go through origin,",301121.0
105319,453518.0,"Green Line - Fit using intercept Red Line - Fit without intercept. When you dont use add_constant, statsmodel assumes intercept to be Zero and therefore gives a BAD fit where a good fit should actually have an intercept.",313515.0
105319,453950.0,In the statesmodel.api we don't have any built-in intercept available so we are creating the intercept using this add_constant function to create it manually. without the intercept we can't get the best fit in the model.,304692.0
105346,453674.0,"Error between predicted and actual value is just for one value of Y at a particular point of X. In reality, there could multiple values of Y at the same point of X. The Error distribution of all such points of Y at that X should follow Normal Distribution. That's the assumption we got to make for the Linear Regression.",311160.0
105346,453949.0,"Error distribution is the error terms distribution of x and y variables, error terms is the difference between the actual y value and predicted y value. The error terms should be maximum of normal distribution.",304692.0
105381,454518.0,"Hi, Please follow the attached link for train_test_split https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html",344894.0
105381,453893.0,"You can use train_test_split of sklearn.model_selection and below is the code from sklearn.model_selection import train_test_split # We specify this so that the train and test data set always have the same rows, respectively np.random.seed(0) df_train, df_test = train_test_split(housing, train_size = 0.7, test_size = 0.3, random_state = 100) You can it in all of the sample notebooks provided after the lectures.",318329.0
105381,453939.0,"It is a function in sklearn which is used to split train and test data Train data is used to build the model. we have to train the model with this data. Test data is used to test the above created model and check whether the model is correct or not. code: train_data , test_data = train_test_split(data, train_size=0.8, test_size=0.2)",304692.0
105377,453862.0,for answer this question go below link https://learn.upgrad.com/v/course/208/session/24635/segment/131278 tscore = Beta1 / standard error as the null hypothesis is that β1 is equal to zero,310952.0
105377,454585.0,"t-score for ^β1= (^β1-0)/SE(^β1); where, β1 = 0.5, and the standard error of β1= 0.02' Hence, putting values, it comes as 25.",311117.0
105412,454142.0,Normal distribution curve is nothing but a frequency curve with some special properties like 68 percent of values lies within one standard deviation. you can plot the frequency curve of error terms (actual minus predicted) and check if it is normally distributed (bell shaped),318329.0
105412,454143.0,mean can be zero if there are positives and negatives in the distribution,318329.0
105412,454361.0,"We are generally finding the error terms and then we are making a scatter plot of them. By doing that we can visually see how the error terms are distributed. According to our assumption, the error terms should be normally distributed and should be centered at 0. We can clearly see how the error was found out in the lecture. The prof. finds the residual as follows: res=(y_train-y_train_pred) and then he plots the scatter plot of res. Thereby he checks the assumption validity.",318435.0
105396,454148.0,"Y is the dependent variable and is dependent on right side variables of the equation. Since, we are predicting the y from X, the intercept has to be added to X.",318329.0
106490,461240.0,"Slightly difference due to the presence of intercept: In statsmodels.api , you have to add a constant yourself. In statsmodels.formula.api , a constant is automatically added to your data and an intercept in fitted. For how to do this, you may go through the link: https://stackoverflow.com/questions/30650257/ols-using-statsmodel-formula-api-versus-statsmodel-api",311117.0
105542,454734.0,it's find constant by finding the value of y when x is zero,318476.0
105542,454776.0,"This is how it is done actually - ax + b = y ...... a= slope , b = intercept Put Σ on both sides of straight line equation, you get aΣxi + Σb = Σyi ...........This evaluates to equation (2) below because Σb means b + b + b + ..... n times = b X n , here n = number of datapoints. Multiplying equation (2) with xi on both sides gives equation (1) below - All terms are calulated from dataset except a and b, see in below example data how - Here n=5, X and Y column is our dataset, while XY and X2 has been calculated using X and Y columns. Using these values in equation (1) and (2) gives us below equations - 30a +10b =243 10a+5b=76 ..............This is a system of linear equation in 2 variables which can be solved easily to find a and b values. In case of MLR, there are more equations and more variables, Hence, Matrix Algebra is used to solves such large system of equations.",313515.0
105531,454694.0,https://www.socscistatistics.com/pvalues/tdistribution.aspx see the link above,318017.0
105531,454882.0,"Hi, Please follow the attached video https://www.socscistatistics.com/pvalues/tdistribution.aspx Thanks",344894.0
105568,454994.0,"Rejecting or Failing to Reject the null hypothesis completely depends on the p-value and the confidence interval. In here, p-value was mentioned to be less than 0.05 so as to obtain the 95% confidence level. Say, if you want to be 99% confident. Then you should have a p-value less than or equal to 0.01.",318435.0
105568,454909.0,if the p value is greater then .005 then the chances are there that the your variable is not reliable and if p is less then .005 then you reject the null hypothesis i.e. beta =0 meaning that your variable has an impact on the model.,318017.0
105568,454950.0,"The 0.05 is nothing but the confidence level. If p value is &gt; 0.05, you can't tell with 95% confidence that the value is significant.",310974.0
105568,455409.0,"Null hypothesis is taken as ""Beta is insignificant"". Therefore for the model fit to be reliable, you have to prove that null hypothesis is rejected. The way you prove it is by saying P-value is less than significance level of 5% which makes it fall in the rejection region. If P-value is above 0.05, it falls in acceptance region which means null hypotheis is true and hence fit is not reliable.",318079.0
105585,455044.0,"As explained in the video, Probability of F Statistic says about the overall fit of the model. P-value of the F-Statistic for 95% confidence should be less than or equal to 0.05. In the case shown,the p-value of the F-Statistic was obtained to be 1.52*10^(-52) which is very much less than 0.05. Hence one can come to a conclusion that the the overall fit is very good.",318435.0
105585,455000.0,"On the same page, It is explained that ""if the ' Prob (F-statistic)' is less than 0.05 , you can conclude that the overall model fit is significant"". and ""If it is greater than 0.05, you might need to review your model as the fit might be by chance, i.e. the line may have just luckily fit the data"". Now, the F-statistic is 1.52e-52, i.e. ""exponential power of -52 to 1.52, which is extremely less than 0.05, and is far behind to 0.05"", hence we can assume that it is 0. Hope this clarifies.",311117.0
105585,456786.0,it means 1.52 e-52 is very insignificant. Do not confuse it with other statistical terms.,311803.0
105604,455266.0,"Normal its varies on the number of observation you have : 1) If the obersvation is in higher side then (80,20) or (70,30) split is ok. 2) But if the number of observation lower then the bootstrap model looks good.",318476.0
105604,456450.0,"Basically training data has to be more than test data, as we need to do analysis on training data. Generally it is 70:30 but you can try using 75:25 or 80:20 based on how much data you have and how better you model is behaving.",318448.0
105604,455747.0,"There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage. If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive). Assuming you have enough data to do proper held-out test data (rather than cross-validation), the following is an instructive way to get a handle on variances: Split your data into training and testing (80/20 is indeed a good starting point) Split the training data into training and validation (again, 80/20 is a fair split). Subsample random selections of your training data, train the classifier with this, and record the performance on the validation set Try a series of runs with different amounts of training data: randomly sample 20% of it, say, 10 times and observe performance on the validation data, then do the same with 40%, 60%, 80%. You should see both greater performance with more data, but also lower variance across the different random samples To get a handle on variance due to the size of test data, perform the same procedure in reverse. Train on all of your training data, then randomly sample a percentage of your validation data a number of times, and observe performance. You should now find that the mean performance on small samples of your validation data is roughly the same as the performance on all the validation data, but the variance is much higher with smaller numbers of test samples https://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio",318451.0
105604,458111.0,"You decide based on the sample data, if the sample data is large you may go for 70: 30, if it is small you may reduce the proportion.",310629.0
105607,455406.0,"Yes, higher the R-Squared, better is the model fit because R-Squared=1-RSS/TSS where RSS which is residual sum of squares gives you a measure of how much error is between actual Y and predicted Y and we desire it to be as low as possible while TSS total sum of squares which is a constant quantity for a particular data because it is just a measure of RSS assuming the fit is a horizontal line passing through mean Y value. So lower the RSS, higher the R-Squared and hence better the fit. T-Score is just to determine if the best fit line we got from regression is statistically significant or not. Infact, it is P-value corresponding to the T-Score that tells us if we can reject the null hypothesis - Betas are insignificant, so that fit is reliable.",318079.0
105607,455540.0,The F stats associated with the model will tell you the overall significate of your model. Will the p values associated with the individual independent variable coficients tell there significate also with repesect to the dependent varibles.,318476.0
105607,455734.0,"P, t and standard error The t statistic is the coefficient divided by its standard error . The standard error is an estimate of the standard deviation of the coefficient, the amount it varies across cases. It can be thought of as a measure of the precision with which the regression coefficient is measured. If a coefficient is large compared to its standard error, then it is probably different from 0. How large is large? Your regression software compares the t statistic on your variable with values in the Student's t distribution to determine the P value, which is the number that you really need to be looking at. The Student's t distribution describes how the mean of a sample with a certain number of observations (your n) is expected to behave. If 95% of the t distribution is closer to the mean than the t-value on the coefficient you are looking at, then you have a P value of 5%. This is also reffered to a significance level of 5%. The P value is the probability of seeing a result as extreme as the one you are getting (a t value as large as yours) in a collection of random data in which the variable had no effect. A P of 5% or less is the generally accepted point at which to reject the null hypothesis. With a P value of 5% (or .05) there is only a 5% chance that results you are seeing would have come up in a random distribution, so you can say with a 95% probability of being correct that the variable is having some effect, assuming your model is specified correctly. The 95% confidence interval for your coefficients shown by many regression packages gives you the same information. You can be 95% confident that the real, underlying value of the coefficient that you are estimating falls somewhere in that 95% confidence interval, so if the interval does not contain 0, your P value will be .05 or less. Note that the size of the P value for a coefficient says nothing about the size of the effect that variable is having on your dependent variable - it is possible to have a highly significant result (very small P-value) for a miniscule effect. https://dss.princeton.edu/online_help/analysis/interpreting_regression.htm",318451.0
105622,455721.0,Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit? Check this link. They have also touched upon the fact that not always high R square is good and also not always low R square are bad. http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit,318451.0
105643,455903.0,"Hi Sai, It goes like this. If the error terms increase or decrease when the x changes it becomes unpredictable. If the error terms are somewhat same in all the values of x, then it is highly predictable. It all depends on the degree of error terms. It is assumed to have error terms in normal distribution to get the best fit line fairly accurately which means it is predictable. I hope this clarifies.",301121.0
105643,455707.0,Identifying Specific Problems Using Residual Plots They have explained all the possible pattern with example. Refer - https://newonlinecourses.science.psu.edu/stat501/node/279/,318451.0
111126,478509.0,"Hey Shashank, Hope this link helps: https://stackoverflow.com/questions/42191717/python-random-state-in-splitting-dataset/42197534",301655.0
111126,478524.0,"'random_state': means that everytime you run it without specifying random_state , you will get a different result, this is expected behavior. If you put 'random_state=some_numbe r' , then you can guarantee that the output of Run 1 will be equal to the output of Run 2 , i.e. your split will be always the same. It doesn't matter what the actual random_state number is. The important thing is that everytime, you will always get the same output the first time you make the split. This is useful if you want reproducible results. For more understanding, please refer: https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn",311117.0
111126,478503.0,Please checkout related discussions: https://learn.upgrad.com/v/course/208/question/110829 https://learn.upgrad.com/v/course/208/question/107575 https://learn.upgrad.com/v/course/208/question/111082,313826.0
126734,552225.0,"Hi, Please follow the below link http://reliawiki.org/index.php/Multiple_Linear_Regression_Analysis",344894.0
115366,497194.0,in adjusted r square there is a penalty of adding more number of variables to the model but no such penalty is being applied on r square when you have more number of variables go for adjusted r square for better results.,318017.0
115366,497394.0,"There is one typographical error. R-Squared; The last part of that should read as ""It will not decrease"" Apologize for the error.",301121.0
115366,497312.0,"R-Squared : By addition of more variables, this value can either increase or remain same. It will not decrease. Adjusted R-square: By additional of more variables, it can increase, remain same or even decrease. It may decrease due to possible penalty of addiong more variables.Penalty is nothing but negative value added. Reason for penalty: Even though addition of more variables give more information to the model, there is a possiblity of introduction of collinearity between variables which can distort the model and hence penalty is applied by the system whenever additional variable is added with collinearity. TA can clarify.",301121.0
115366,497619.0,"Value of R-squared increases when additional variables are added to the model even if they are not related to the output or dependent variable. But Adjusted R-squared penalises R-squared for unnecessary addition of variables. So if the variable added, does not increase the accuracy adequately , adjuated R-squared decreases although R-squared might increase.",317460.0
115366,499759.0,The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.,303083.0
115366,500691.0,"Both R2 and the adjusted R2 give you an idea of how many data points fall within the line of the regression equation. However, there is one main difference between R2 and the adjusted R2: R2 assumes that every single variable explains the variation in the dependent variable. The adjusted R2 tells you the percentage of variation explained by only the independent variables that actually affect the dependent variable.",314048.0
126755,558281.0,"The min and max accuracy depends on the business case the model is built for. Generally, an accuracy of 80-85% in considered good enough for all practical purposes. But , in some cases, we would also need to consider the sensitivity and specificity in order to have a balanced model and avoid bias in favor of one of the classes. In such cases, the accuracy can come down to accomodate a higher sensitivity and specificity. There is always a model that can be designed for 100% accuracy but the issue would be that the model would end up memorizing the entire dataset and would lead to overfitting. This would lead to bad generalizability of the model and it would perform poorly on unseen real world datasets.",312063.0
126755,552179.0,"I have asked the same question in one of the live sessions conducted by upgrad. In that also, professor told that there is no certain range of accuracy specified by anyone to make sure that our model is very good or very bad. It all depends on business requirements and situations how well ur model is performing on unseen data and how well ur model learnt about the data used for training the model.",300733.0
126755,552155.0,I think there is no range we can decide for accuracy of model. It depends on the business objective and the balance of accuracy on train and test data. For ex:- Suppose we have a model and its accuracy is 100% on train data and 75% on test data then this model is overfitting and not a good model. Whereas if we have a model giving accuracy of 90% on train data and 86% on test data then this model can be considered as good one because it is not overfitting. So it all depends on what business wants. Hope this helps.,317991.0
114966,496175.0,"beta coefficients signify the weight of the corresponding variable in the model. ie, if all the other variables remain same, if the variable's value is changed how much the output of the model will vary is signified by beta coefficient. correlation coefficient, on the other hand signifies how much a variable is dependent on another variable. A correlation coefficient of 1 or -1 signifies that 2 variables are absolutely dependent on each other. and if the value is 0 then they are least correlated",305839.0
114966,496267.0,"Consider a mulitple linear regression equation: y=(beta0) + (beta1)X1+ (beta2)X2 Here, beta0 is the intercept of the line(i.e. the value of Y when X1,X2=0). Whereas, beta1 and beta2 are the coefficients of the regression and they signify the amount of change in Y if the value of one of X1/X2 changes keeping the other constant. Simply put, betas help in interpretion of the equation. Betas can assume any value The correlation coefficients indicate the correlation between the different variables. The correlation coefficients can assume values between -1 and +1.",312063.0
114966,496207.0,"If simply put, Beta co-efficient lets you know how much it affects the dependant variable Correlation co-efficient between 'features' lets you know the collinearity. To go one step further, VIF gives you information on multicollinearity. You can remember this way easily.",301121.0
115365,497212.0,R-sqaured is the goodness-of-fit measure for the model. It indicates the percentage of the variance in the dependent variables that the independent variables explaines collectively. It is the strength of the model and the dependent variable.,301649.0
115365,497195.0,variance is the distribution of points in the plane on which the predicted line is passing and the variance should be in such a way that for all the points the distance from the predicted line should be constant.,318017.0
115365,497321.0,"Hello Praneeth, R-Squared value: When R-Squared value is exactly 1, it means all the points exists on the predicted line. As the value decreases from 1.0, it means that not all the points are on the same line and some are outside that line which is defined as variance, which is fine.. Now comes the actual explanation to your question on variance for the best possible fit line. Error terms due to variance are expected to be having normal distribution on each point of predicted line. EXTENT of variance are expected constant throughout the range of x (independent variable) - This also basically defines the stability of the model. (As an exmaple, when x=1, variance is small and x=10, variance is high and x=16 variance is in between; This scenario will make the model unstable) I hope this clarifies.",301121.0
114468,493763.0,"If by your following statement ""If X and Y follow a non-linear approach"", you mean that the curve is NOT linear then BOTH Linear as well as Non-Linear regression can be used. Linear regression can be used because it will find a Best Fit Line on the curve and thus making it a Linear regression. While non-linear regression is much more flexible in the shapes of the curves that it can fit. You can refer to the following article to get more insights into when to choose which type: https://statisticsbyjim.com/regression/choose-linear-nonlinear-regression/ Hope that helps.",317998.0
114468,494734.0,"I think nonlinear regression is much more flexible in the shapes of the curves that it can fit, so in my opinion it will be the best option.",316368.0
114468,493928.0,"multiple linear regression” is a specific type of linear regression. So, there needs to be a linear relationship between the dependent variable and each of your independent variables. There are a number of ways to check for linear relationship.For example, you can use scatterplots, and then visually check for linearity. If the relationship displayed in your scatterplot is not linear, then, you need to use non-linearregression.",311004.0
114900,495953.0,"1. Probability is expressed as a number between 0 and 1, while Odds is expressed as a ratio. 2.Probability ensures that an event will occur, but Odds is used to find out whether the event will ever occur.",313200.0
114900,496279.0,"Let's consider the equation: P=1/(1+e^(-beta0+beta1X)) where P is the probability of the event Now, the log odds is ln(P/1-P) = -beta0 + beta1X Log odds basically signifies the odds of the event happening. For example if log odds is 4, then the probability of the event happening is 4 times the probabiility of the event not happening.",312063.0
126754,552373.0,"That would come under Residual Analysis, I believe. On searching over the net, I found this article that might help with your question. https://newonlinecourses.science.psu.edu/stat501/node/277/ You can go through it (specially the 2nd part) to get some insight on the answer the panel might have been looking for. Hope that helps.",317998.0
126754,552449.0,I guess it should be a straight line cause we know this formulae error = Y Actual - y Predicted .,300721.0
126754,552806.0,"Hello Jay Kumar Very interesting hypothetical question: Let us think mathematically, Ideal Situation : Error terms are normally distributed throughout and with absolutely no variation across the domain; Situation 1: If you take error term as it is with positive and negative signs, mean would become zero and line would coincide with x-axis as the error terms are zero throughout Y-Predict Situation 2: If you take magnitute of | error terms | then it will be a horizontal line parallel to x axis with y-value equal to magnitude of error term. Average magnitude of error term shall be same through out x-axis which Y-predict.",301121.0
105884,457006.0,Which model equation?,318329.0
105884,457106.0,You know the formula of multi linear regression. You just substitute the variable names and the coefficients that you obtain from your final model into the equation.,310974.0
107014,462172.0,Model will not drop a variable unless you tell him to drop. Model will show postive or negative effect due to multicollinearity. If you not drop anyone of the dummy varaibles you have created than it surely will overpredict.,318451.0
107016,462162.0,Drop it in the descending order of the VIF one by one even if their p value is zero.,318451.0
107016,462185.0,"The ideal process is to check p-value first - - If p-value is less than 0.05 then the variable is significant and you need to check if VIF is &lt;5 or not. If it is high you should drop that variable despite having p-value = 0. - If p-value is more than 0.05 you can directly drop the column based on p-value, no need to check VIF. Hope this helps.",317991.0
107016,462338.0,I think this is where our understanding of the domain plays a big role. Technically dropping it makes senss. If the variable of business significance and including that is not significantly impacting adjusted R squared then retain it. VIF -&gt; Multicollinearity P value --&gt; significance,311803.0
105885,456972.0,You can use any one of the approaches or both,311160.0
105885,457005.0,"RFE is used to reduce the number of features. So Ideally, first we need to build a model with all variables, use RFE to reduce the features to certain number and then fine tune manually.",318329.0
105885,457015.0,"Yeah, I am also planning to use approach mentioned by NG, but having hard time to keep the track the variables :-)",304814.0
105885,457066.0,You need to use both. First use RFE and then manual approach to fine tune and reduce the variables which are insignificant.,318368.0
105885,458054.0,Use both the methods,314621.0
107109,462856.0,yes the mean should be close to zero and if the mean is not close to zero then it means it is not a best fitted model so try to change the variable that you are using,318017.0
107109,463044.0,"that is what we are doing with regression analysis first - creating the best model possible. So perhaps go through RFE, manual regression (OLS), VIF to come up with a model that is giving you mean at 0",300694.0
107137,463092.0,"It makes the train and test samples get the same value. No matter how many times you execute train_test_split function on your DF, it assigns the same samples to test and train as it did for the first time.",311160.0
107173,463301.0,we had a whole lecture on that here is another source - https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/,300694.0
107173,463429.0,"Go through the video's of Session-4 (Variable Selection using RFE), you will get clear understanding: https://learn.upgrad.com/v/course/208/session/24637/segment/126238",311117.0
107173,463382.0,"Hi, As per my knowldge ,it happens using a feature selection and search algorithm. See if this helps you .. https://en.wikipedia.org/wiki/Feature_selection",319006.0
107152,463272.0,I think RFE to some level is the kind of automated approach you are talking of. We integerate it with the manual approach so that domain knowledge can be applied.,304319.0
107152,463178.0,"Hi Nitesh, Link below may clear your doubt to some extent : https://www.automatingthefuture.com/blog/2017/5/9/making-predictions-with-simple-linear-regression-models",301655.0
107152,463856.0,What you are probably hinting at is an Automated Data Modeling Checkout these below links: https://www.youtube.com/watch?v=yzAhjinmdzk https://www.h2o.ai/blog/finding-clarity-in-the-automated-modeling-space/ https://www.datarobot.com/ https://www.kdnuggets.com/2018/07/automated-machine-learning-vs-automated-data-science.html,313826.0
107203,,nan,
107229,463786.0,the equation is formed by the use of coefficient and the variable whose coefficient is like y = mx +c here m is coefficient and x is variable.,318017.0
107229,463824.0,"lm.summary() provides a lot of details in which the co-efficients are also listed under the ""coef"" column.",313826.0
107229,464035.0,"think of it kind of like a straight line whereby the co-efficients in the summary are the 'weightage' of that particular feature in that particular model; and the constant is the y-intercept so then you have something like yi = axi + bzi+ cwi+dui+evi + g- where a,b,c,d,e are the coefficients we get from the summary, yi is the dependent variable (price in our case) and xi,zi,wi, ui,vi are the independent variables, and g is the constant as per the summary of the model",300694.0
106364,459317.0,Ram mentioned the correct approach. However in every scenario we should drop the variable one by one and rebuild the model again to check the stat summary.,301648.0
106364,459309.0,Follow this:,310974.0
107237,463822.0,You can use a box-plot to plot the categorical variable against the outcome variable to understand the associations between them. This has been covered as part of the python implementation of MLR. Please refer the 3rd video on the below link for more details : https://learn.upgrad.com/v/course/208/session/24637/segment/126233,313826.0
107237,464029.0,use box plots - it is something we did in out other assignments as well (uber and investments assignment) There are lots of examples on the net about furniture/stationary store where they use box plots to show relationship between say sales and different category of products,300694.0
106807,461326.0,This is because you didn't convert all the categorical variable to a numeric variable. Please check which attribute contains 'mpfi' and check if you have created a dummy for that.,301648.0
106429,459707.0,"To center the data (make it have zero mean and unit standard error), you subtract the mean and then divide the result by the standard deviation. x′ = x − μσ You do that on the training set of data. But then you have to apply the same transformation to your testing set (e.g. in cross-validation), or to newly obtained examples before forecast. But you have to use the same two parameters μμ and σσ (values) that you used for centering the training set. Hence, every sklearn's transform's fit() just calculates the parameters (e.g. μ and σ in case of StandardScaler ) and saves them as an internal objects state. Afterwards, you can call its transform() method to apply the transformation to a particular set of examples. fit_transform() joins these two steps and is used for the initial fitting of parameters on the training set x, but it also returns a transformed x′. Internally, it just calls first fit() and then transform() on the same data. https://datascience.stackexchange.com/questions/12321/difference-between-fit-and-fit-transform-in-scikit-learn-models",318451.0
106430,459709.0,df.pop is used to pop the column label called price from the df_test data frame. Later the resultant data frame was assigned to x_test dataset. So Basically it was used to separate the dependent variable from the predictor/feature variable.,301648.0
106430,459702.0,df_test indicates the test datframe from which we want to select the variable pop() This in-built function of Python helps to pop out elements/variable,318451.0
106430,459904.0,it gives the particular column of dataframe and delete it from the original dataframe. so you can assign the price value to Y and rest dataframe as X.,318851.0
106430,460004.0,It is to fetch (pop out) price variable from df_test.,317689.0
106430,461483.0,"we need to assign a dependent variable to Y.. So , from dataframe selecting price variable alone and assign it to Y using pop feature which removes price from dataframe. Then, all other variables are assigned to X as independent variables to see their regression methods",318454.0
106432,459907.0,Outcome variable is the predict or dependent variable which has to be predicted.whereas feature variables are the independent variable on which the outcome variable depends.,318851.0
106432,459795.0,"The outcome variable will be the variable we are trying to predict Eg. 'price' of a house. Feature variables are the variables on the basis of which we will be predicting the outcome variable. Eg. Area of house, number of bathrooms and bedrooms.",320073.0
105318,453479.0,"Hi Siddhant, Suppose you have 100 records as training data. If you develop a model on this you will consume all the data for developing the model. You won't be able to test if is it is working fine or not because you don't have any data left. And it will be not wise to start predicting values for new cases without evaluating your model. Thus a part of train data is kept separate for doing the testing and is not consumed while training the model. Hope it helps.",311686.0
105318,453487.0,"Hi Sid, When a model is built, a large chunk of the data is segregated to train the model. After the model is trained it is then tested on the remaining part of the data so that we can ensure that the model is working well. Once that is established, the model can be utilised to predict future values. If we straight away train the model on all of the data that we have, then we will never get to know the correctness of the predictions that the model makes on the new data. Hope that helps :)",308962.0
105318,453655.0,"This is also done to avoid overfitting of the model. Suppose you use up all the data for training - in that case your model can ""memorize"" the data and thus give perfect answer for figures in the dataset but fail to predict other data. To avoid this, two sets are made - one to train and one to test.",318499.0
105318,454618.0,"In ML model to predict the future we divide data as training and testing data. While building a ML model, model learns the mathematical relationship in the data using the training dataset. In order to verify whether the model is valid, we test the model with different data called testing dataset”. Follow the link for detailed explanation : https://towardsdatascience.com/data-science-simplified-key-concepts-of-statistical-learning-45648049709e",301114.0
105318,454655.0,The dataset for testind helps you test your model . With the help of trainin dataset You can build a model but how will you test if your model is good enough to actually make predictions on values that it has never seen. Also you can determine if your model fits the data values by comparin the true labels with tthe predicted labels.,311864.0
105359,453749.0,Preferably dummy variables need not be scaled. You can take a look at the following link for better understanding: https://www.quora.com/How-bad-is-it-to-standardize-dummy-variables,310505.0
105359,453901.0,"MinMax scaling doesn't have an effect on the dummy variables because dummy variables holds the values 0 and 1 and when you do a MinMax scaling, you still get the same value for each of those.",318329.0
105359,453941.0,I hope we need to apply scaling to the dummy variables as these are not much included in the original data and they are created by us.,304692.0
105359,453737.0,min max scaling sets the variable value from 0 to 1 and standardized method turns the variable value between -1 and 1 having mean at 0. min max method is preferred whenever you want value doesn't have to be in negative value.,318017.0
105389,453935.0,"Assumption for errors/residual in a liner regression is as follows: There should be no correlation between the residual (error) terms. Absence of this phenomenon is known as Autocorrelation. The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity. The error terms must have constant variance. This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity. The error terms must be normally distributed. What if these assumptions get violated ? Autocorrelation: The presence of correlation in error terms drastically reduces model’s accuracy. This usually occurs in time series models where the next instant is dependent on previous instant. If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error. Multicollinearity: This phenomenon exists when the independent variables are found to be moderately or highly correlated. In a model with correlated variables, it becomes a tough task to figure out the true relationship of a predictors with response variable. In other words, it becomes difficult to find out which variable is actually contributing to predict the response variable. Heteroskedasticity: The presence of non-constant variance in the error terms results in heteroskedasticity. Generally, non-constant variance arises in presence of outliers or extreme leverage values. Look like, these values get too much weight, thereby disproportionately influences the model’s performance. When this phenomenon occurs, the confidence interval for out of sample prediction tends to be unrealistically wide or narrow. Normal Distribution of error terms: If the error terms are non- normally distributed, confidence intervals may become too wide or narrow. Once confidence interval becomes unstable, it leads to difficulty in estimating coefficients based on minimization of least squares. Presence of non – normal distribution suggests that there are a few unusual data points which must be studied closely to make a better model.",318476.0
105436,454207.0,"It depends on the model you are building and the data which you currently have available. It will also depend on the computing power also in case data is huge. So, there would always be trade off between data size, its availability and resource availability as well. In some cases few thousands records might suffice but in some cases few million rows will be too small to predict.",317689.0
105436,455433.0,"Depending on your model, data preparation can be a one-time activity or a periodic one. Predictive analytics is an iterative process that can continue after your solution has been deployed. As new insights are revealed, it is common to experiment by adding or changing aspects of the input data. You can schedule analysis to continually add new data to your model incrementally. It is also common to periodically update your model variables and fields with new information or more focused business questions. you can go through the below link : https://help.salesforce.com/articleView?id=bi_edd_prep.htm&amp;type=5",314183.0
105436,455973.0,"Its completely depends on the type of data, prediction, industry, relevance of data, coverage of data etc. Nothing is enough if you want to make a perfect model.",318368.0
105431,454177.0,"Well, This is interesting! Since, it is not graded can you paste your code here to check if there is any issue?",311160.0
105431,454525.0,"Hi Prashant, I encountered same issue initially. It is because the of datatype mismatch between the solultion output and expected output In the expected output it is float and hence your output should be float too. I guess data type of your output is string. I hope this clarifies.",301121.0
105431,455559.0,"Thanks for responding, yes it was a same issue. I also had same finding after lot of investigating. It was string in my code :(",306011.0
105405,454023.0,,318017.0
105405,454144.0,the stats model package will give these values when you call the summary method and you can compare different models R squared or adjusted R squared values with that,318329.0
105405,454869.0,"Hi, Please follow the attached link https://www.myaccountingcourse.com/financial-ratios/r-squared Thanks",344894.0
105476,454322.0,FurnishingStatus is a categorical variable hence cannot be used in heat map to see it's impact on target variable i.e price We created the dummy variables for the FurnishingStatus category which are numeric in nature 0 or 1. These can be used in heat map and we can see it's impact on price and other variables.,311254.0
105476,455495.0,"""Creating dummy variable, we had eliminated 'Furnished' category. "" It is a catogerical data. A dummy variable is a numeric variable that represents categorical data, such as gender, race, political affiliation. Technically, dummy variables are quantitative variables. Their range of values is small; they can take on only two quantitative values. Regression results are easiest to interpret when dummy variables are limited to two specific values, 1 or 0. Typically, 1 represents the presence of a qualitative attribute, and 0 represents the absence.",314183.0
105476,455962.0,"If you want correlation, you dont drop the dummy variable and drop only after analysis. Even though we drop it, we can use other two variables for correlation.",318368.0
105499,454389.0,no you can either give it optionally or if you use dummy function it comes automatically.,318017.0
105499,454569.0,"The values need not be in the same order. You could come up with your own mapping. As you asked, yes they are dummy codes.",318329.0
105499,454938.0,The encoding is not exactly dummy encoding; it’s just manual encoding.,314183.0
105499,455959.0,"Its not necessary to take the ecoding as spefied, you can use any enconding combination for blood group. But make sure you apply same enconding for test and train both othewise model wont work.",318368.0
108666,469819.0,"Hey Varun, I think you must not be dropping the original columns from the dataset, for which you have created the dummies. And, I think this link below may help you to a certain extent: https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/ Hope this helps you out!",301655.0
108666,470042.0,Make sure that the base column is dropped after dummification.. Use only numerical variables.. Assuming that you are running RFE.. plot heatmap after dropping variables once you are done with RFE exercise. Thanks,310508.0
108666,469876.0,"Nametitle_sir and Nametitle_ the countess, might be having "" Nan "" or "" Null "" values or "" original columns "" might be present in the dataset. I had also faced such problem, after scrutiny, i got ""Nan"" value. This is additional reading for getting knowledge : Understanding Dummy Variable Traps In Regression: https://analyticstraining.com/understanding-dummy-variable-traps-regression/",311117.0
105537,454716.0,here with two columns you are able to identify the cloumns details: 00 as A 01 as B 10 as AB 11 as O the reason to drop two first column is last two columns are sufficient. the more columns you take for the complex will be you model as it's not adding any more information also,318476.0
105372,453833.0,it depends on the situation suppose the highest co relationship is .90 then .31 or .45 are not that much in these cases but if highest value is .5 the .45 is very much considerable.,318017.0
105372,453842.0,"The correlation coefficient indicates the strength of the relationship between the variables. The value ranges from -1 to +1 If it is greater than zero then we say variables are positively correlated and if coefficient is less than 0 we say variables are negatively correlated. If the coefficient is 0 then the variables are not correlated. If r=1 then there is perfect positive correlation between the variables -1 describes perfect negative correlation. The closer the coefficients are to +1 and -1 we can say variables are strongly correlated ;means greater the strength between them For convenience we can follow this : To interpret the correlation coefficient,see which of the following values your value is closer to : a. exactly 1 or -1 : then variables are perfectly correlated b. nearer to 0 .70 r -0 .70: then variables are strongly correlated c. nearer to 0.50 and - 0.50 then variables are moderately related d nearer to -0.30 and 0 .30 then the variables are weakly correlated e. exactly 0 r nearer to 0 then variables are not correlated. Please note that these are not standard ones . For e.g. if coefficient is 0.45 the we can say variables are moderately related Hope this helps you .We can see many have considered different ranges . http://www.dmstat1.com/res/TheCorrelationCoefficientDefined.html https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/correlation-coefficient-r/v/correlation-coefficient-intuition-examples www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data/scatterplots-and-correlation/a/correlation-coefficient-review statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-stat",308638.0
105656,455946.0,"You remove feature only after taking singnificance/ correlation into consideration. If a feature is not significant or has high correlation with another feature, then only we drop the feature.",318368.0
105656,455703.0,Its a type of permutation and combination thing. When you have 30 variables for example it is diffcuilt to include all and eliminate. So that time we prefer adding one variable on to another to see a gardual improve/degrading of model. This how we are able to see which variable is improve/degrading the model . You always has a choice to again impute the variable that you have taken off. When that variable is missing is its then the importance of that variable is identified.,318451.0
105656,455861.0,It is not always necessary to remove one feature. It might be meant to remove one feature at a time till you find that all the features present are signification.,318329.0
105664,455941.0,"df[['Response']] returns Series of String in apply fuction. df['Response'] returns String in apply function. map function can be applied on Series of String but not on String. So df[['Response']] = df[['Response']].apply(binary_map) works, while df['Response'] = df['Response'].apply(binary_map) will now work.",318368.0
105664,455857.0,"mapping has to be applied on a dataframe because map is method applicable for series but not str. When you do just df['Response'].apply, it will be applied on individual elements which is of type str",318329.0
105670,455854.0,We do not consider constant because Y is not varying wrt the constant.,318329.0
105670,455952.0,We drop feature based on insignificate based on hypothesis. And Based on our hypothesis (B)i becomes 0. Constant is insingificant but its not 0 and Y is not dependent on contant but intercept is.,318368.0
105670,456444.0,"Whatever will be the p-value for constant whether high or low, we do not consider it as Y is not dependent on constant.",318448.0
105670,456456.0,"The null hypothesis we start with is that the each coeff equals 0 and then we test for evidence to reject this hypothesis using p-values. So, when we say a coefficient is significant, we mean that this coefficient is significantly different from 0. If it is insignficant, it means that it is not significantly different from 0. In case of the constant, if the coefficient is insignificant, it just means we can't reject the hypothesis that the constant is 0.",305653.0
106786,461232.0,"Please go through the link, many solutions have given. This may be helpful: https://stackoverflow.com/questions/52736900/how-to-solve-the-attribute-error-float-object-has-no-attribute-split-in-pyth",311117.0
105592,455955.0,This is working. Few observation. 1) It should be HTTPS. 2) You should be hitting resource which is avaialble. Make sure URL is correct.,318368.0
106514,460081.0,"The variables which has less correlation value might be significant in the model with other variables. So dropping the variable on the correlation value is not such a goof idea. The general idea of dropping variables from a model is by looking model summary and vif. From model summary check if p value is greater than 0.05 then that variable is insignificant, you can drop the variable with the highest p value. then again create the model with rest of the variables...and repeat the whole process again and again.. After all the variables are significant then check the vif and decide which variable to drop.",320073.0
106811,461322.0,you can start with say 20 then based on the model that you made you can add or remove variable by checking p-value and vif,318017.0
106824,461358.0,"When you have more number of variables model would get trained with all of them. It could result in more chance of getting overfitted and wouldn't be dynamic enough to adapt to the data it hasn't seen. When you have Multicollinearity issues, you wouldn't know the effect of an individual variable on the model i.e, reducing/increasing the value of one variable by one unit could have double the effect on the outcome variable if the variable is highly correlated with an other feature variable.",311160.0
106824,461783.0,VARIABLE REDUCTION: AN ART AS WELL AS SCIENCE In this article they have given different techniques of variable reduction also a business scenario why to reduce variables Refer this link - https://valiancesolutions.com/variable-reduction-an-art-as-well-as-science/,318451.0
107163,463313.0,AIC and BIC were explained in one of the lectures. They are very simnple measures of the distance between a prediction and the actual y value. The difference in BIC is that it penalises you for having too many features/variables.,300694.0
107163,463211.0,"Suppose that we have a statistical model of some data. Let k be the number of estimated parametersin the model. Let L ^ {\displaystyle {\hat {L}}} be the maximum value of the likelihood function for the model. Then the AIC value of the model is the following. A I C = 2 k − 2 ln ⁡ ( L ^ ) {\displaystyle \mathrm {AIC} \,=\,2k-2\ln({\hat {L}})} . BIC is same as AIC but put mre pleanty so when AIC increase or Decreases by X then BIC will bit increase more than AIC by harder pleanty. Very Good concept is defined in below . https://methodology.psu.edu/AIC-vs-BIC",307843.0
118945,513674.0,"I'm not sure of the situation when you would want to report the coefficients. It'll mostly be the dependent variable that business would be interested in. However, if you still have to report on it (or the dependent variable), you can use the inverse_transform method .",318438.0
118945,514123.0,"Hi, Based on the coefficient you can make the business logic. As in your case you have only 1 variable so you can directly tell how independent variable is important. But if you have more than one independent variable then it should be given importance based on coefficient. Like y= 2X1 + 0.6X2 + C In above case you can say that X1 has more importance than X2 for predicting Y. If you could not done scalling then you might get equestion like y = 10X1 + 30X2 + C (Because of range of X1 &amp; X2 will be different so coefficent will come based on their value )",344894.0
104698,451267.0,Correlation means any change in X how does your Y behave it may be positive or negative. It need not be positive only. eg Positive Correlation can be observed when Y ( Sales ) and X ( Marketing Spends ) increase in marketing spends should ideally lead to increase in sales. Negative Correlation can be observed when Y ( Time ) and X ( Speed ) increase in speed will decrease the time taken to reach. Strong Positive Correlation of variable is determined how close it is to 1 (100%). Strong Negative correlation of variable is determined how close it is to -1 (-100%).,318451.0
104698,451335.0,". The word Correlation is made of Co- (meaning ""together""), and Relation When two sets of data are strongly linked together we say they have a High Correlation (Either positive or negative ) Correlation is Positive when the values increase together, and Correlation is Negative when one value decreases as the other increases Correlation can have a value: 1 is a perfect positive correlation 0 is no correlation (the values don't seem linked at all) -1 is a perfect negative correlation Correlation Is Not Causation"" A common saying is ""Correlation Is Not Causation"". What it really means is that a correlation does not prove one thing causes the other: One thing might cause the other The other might cause the first to happen They may be linked by a different thing Or it could be random chance!",318476.0
104698,451265.0,"The statistical relationship between two variables is referred to as their correlation . A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable's value increases, the other variables ' values decrease. so in your case the direction on one variable is upwards and secind is downwards",318017.0
104698,451367.0,Strong as in he means that there is a strong relation between the two variables ( Co-relation value not equal to zero). It can be anything.. positive or negative. Its the correlation value which decides whether it is positively related or it is negatively related.,318435.0
104698,451540.0,One can visulise the correlation by usig Excel. Steps are given https://www.excel-easy.com/examples/correlation.html,314511.0
104698,451814.0,strong co-relation is when the value is close to 1 or -1 and when the value is close to 0 then the co-relation is weak and the sign of co-relation value just shows whether the relation is directly proportional (if +ve) or inversely proportional (if -ve).,320685.0
104698,452283.0,"positive correlation means both variables are proportional(X increases/decreases,Y increases/decreases). negative correlation means inversely proportional(X increases/decreases,Y decreases/increases).Relationship among them is strong because they have mutual interdependence on each other.",320603.0
104693,451219.0,"Because the Independent Variable is the historical data which is used to predict the data. Dependent Variable is the data which can be predicted. Let's take an example of marketing and sales given above : After the creation of the scatter plot, we want to estimate/predict the "" Y sales "" for spending "" X marketing amount "" then X is the Independent variable and Y is dependent variable. Spending on marketing amount leads to sales, this can vice-versa sometimes.",304692.0
104693,451352.0,Dependent Variable is centre of our regression. Our aim is to understand how your dependent variable behave with an increase or decrease in independent variable. eg Y (Unemployment Rate) vs X (Inflation Rate) . Here if you want to find out whether and increase in Inflation Rate will lead to increase or decrease in Unemployment Rate this can be one relation you found. Now your aim changes and now you want to find the vice versa that is also possible. So it completed depends on what is your aim. What is the aim of my study ? Always ask this question before deciding on Y Can i control this variable ? Always ask this question before deciding on X,318451.0
104693,451266.0,independent variable is the one on x axis and deoendent is on the y axis. as the values on x axis change the value on y also changes according to x axis so we can say that the y value is dependent on the x value and that is what the linera equation also says: y = mx + c for each value of independent variable we will get a new value of dependent variable.,318017.0
104693,451344.0,"Independent vs Dependent Variables The two main variables in an experiment are the independent and dependent variable. An independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on the dependent variable . A dependent variable is the variable being tested and measured in a scientific experiment . The dependent variable is 'dependent' on the independent variable. As the experimenter changes the independent variable , the effect on the dependent variable is observed and recorded. Independent and Dependent Variable Example For example, a scientist wants to see if the brightness of light has any effect on a moth being attracted to the light. The brightness of the light is controlled by the scientist. This would be the independent variable. How the moth reacts to the different light levels (distance to light source) would be the dependent variable.",318476.0
104693,451365.0,As i understand Independent variable is the input to your algorithm. And Dependent variable is the expected output. You are trying to determine Sales( dependent variable ) based on different values of Marketing Spends( independent variable),317996.0
104693,451481.0,"It is quite clear to all that linear regression is used to predict a future values for our variable. The mathematical concepts have been explained by our colelagues pretty well on above answers. Now the question is what is dependent and what is independent variable.? In our case, since we are trying to predict whether further investment in marketing will lead to future increase in sales revenue. That is important, we are trying to figure whether sales will increase on increase the marketing budget. Therefore, our sales variable is dependent on marketing budget variable. With this logic its clear that sales is our dependent variable and marketing is our independent variable.",302735.0
104693,451393.0,"Look at the general equation of a line: y = mx+c From the above you see that y is dependent on x because when x changes, y will change and not the other way round. This x and y can take on any parameter. Let's take x = Marketing Budget, y = Sales Now when we insert the values of each in the equation of linear regression Y = β 0 + β 1 X, we see that the sales is dependent on the budget and not the other way round. So that's why we have one independent variable (X or the budget in our case) and one dependent variable (Y or the sales in our case).",310505.0
104692,451199.0,"It can be used when the response variable is a continuous value to evaluate trends and make estimates or forecasts. For example, if a company’s sales have increased steadily every month for the past few years, by conducting a linear analysis on the sales data with monthly sales, the company could forecast sales in future months.",303229.0
104692,451220.0,"Linear Regression is firstly simple and best algorithm. It can be used for the labeled and predicting numerical/ Continuous output. Eg's: Cricket Scores, Winner of the match, Market Sales etc..",304692.0
104692,451358.0,"When, why, and how the business analyst should use linear regression ? Go to the link below to read the article on the above topic. Eric Benjamin Seufert has wondefully placed his points in very concise manner and supported with examples. Refer this link - https://mobiledevmemo.com/when-why-and-how-you-should-use-linear-regression/",318451.0
104692,451268.0,go through the below link : https://www.google.com/amp/blog.minitab.com/blog/how-to-choose-the-best-regression-model%3fhs_amp=true,318017.0
104692,451342.0,"Hi Manish, Regression should be used wherever the situation is based on Cause &amp; Effect. Example - how the marketing spend (cause) effects the sales (effect). If the situation calls for segregation into different groups then you cannot use Regression.",308962.0
104692,451363.0,"As the instructor said, Regression is used when the output is continuous valued. For example, Predicting the marks, predicting the sales etc.,",318435.0
104692,451379.0,When the value to be predicted is continuous in nature you will use linear regression. This will come under supervised predictive analysis since there are labels associated with the analysis.,310505.0
104692,451486.0,"linear regression is quite similar to correlation matirx. They both tell us how a variable is affected by other factors present in a case. But in linear regression, we build a model, whenever we need to predict a future case like stock price, housing price etc or any cause and effect case study for that matter,on that degree of correlation. Now the question comes on what kind of data is regression aimed at and as Revati and Professor explained, Continuous variables where our resultant variable (dependent variable) is correlated to some other factors or variables.",302735.0
104709,451337.0,"To add a linear regression line on your scatter plot , right click on the data points, and then click add "" trend line "". Choose ""linear"" as the type of Trend /Regression. Select check boxes of ""Display Equation on chart "" and ""Display R-squared value on chart "", and then click ""OK"".",318451.0
104709,451332.0,Below are the step: 1) Select the x and y values you want to plot the scatter chart with. 2) Go to insret --&gt;Charts--&gt;Scatter plot 3) Select/Right click on the plot and add trend line in you case liner trend line .Also there are options to display the equation and r square value.,318476.0
104709,451373.0,"Once you plot the scatter plot, you can see under chart layouts an option having a trend line. Select that and you'll have a best fit line plotted for your scatter plot.",310505.0
104709,451396.0,"For adding the 'y=mx+c' equation on the trend line on the scatter plot in Excel: Pls follow the link, the complete procedure is mentioned: https://www.homeandlearn.co.uk/excel2007/excel2007s3p13.html;",311117.0
104718,451407.0,"R^2= Residuals sum of (Actual Y- Predicted Y)^2. OLS: Ordinary least squares (OLS) is the workhorse of statistics. It gives a way of taking complicated outcomes and explaining behavior (such as trends) using linearity. The simplest application of OLS is fitting a line. Residuals: Residuals are the observable errors from the estimated coefficients. In a sense, the residuals are estimates of the errors. For further detailed mathematical clarification , pls follow the below mentioned link: https://datascience.stackexchange.com/questions/11467/what-is-the-difference-between-residual-sum-of-squares-and-ordinary-least-square/11468",311117.0
104718,451383.0,"So basically to counter the dependency of value of RSS on scale of X and Y we are using R^2 (also a scale to determine fitting of our Regression Line), which is basically a value between 0 and 1 determined by the equation (1-RSS/TSS). r^2==1 means Best fit and r^2==0 means Poor FIT of our Regression Line.",317984.0
104718,451370.0,OLS is the method followed to find the best fit line. Where we minimize the sum of residuals squared (RSS). Residual is the difference between the predicted value and the actual observed value. When we sum the residuals we might end up getting a negative value or even zero. Thats why we square it before minimizing it.,318435.0
104729,451403.0,"For more detailed clarification, follow this link: https://stats.stackexchange.com/questions/277009/why-are-the-degrees-of-freedom-for-multiple-regression-n-k-1-for-linear-reg",311117.0
104729,451397.0,"It depends on where you are using it. Basically, the number of df = n - # parameters you are estimating. For example: a) Usual one-sample test of mean: estimate mu with X-bar df = n - 1 b) Estimate linear regression: estimate intercept and slop df = n - 2 c) Estimate quadratic regression: estimate intercept, slope, and curvature = &gt; df = n -3 Hope this clarifies.",311117.0
104729,452038.0,"Degree of Freedom is number of independant pieces of information available to analysis or to take decision. In linear regression : y=mx + c , Here , m and C are two variables which helps us to predict the value of y or particular equation. n is number of parameters . To get the Degree of freedom = n - 2(as we have twovariables which impacting our degree of reedom) Go through below link for indepth knowledge :https://people.richland.edu/james/ictcm/2004/multiple.html",318732.0
104729,452152.0,"Degrees of freedom can be defined as the number of ""observations"" (pieces of information) in the data that are free to vary when estimating statistical parameters. Simple example; I have to find three unknowns x,y and z such that sum of three numbers is 20 ie x+y+z=20 Among three number two can be chosen freely say let us assign x=4 and y=7 But z cannot be chosen independently. It should be now 20-11=9 Refer these videos which explian degress of freedom in a simple way. 1.Degrees of freedom -Part 1,Khan Academy-Ben Lambert https://www.youtube.com/watch?v=-4aiKmPC994.In this video, concept is explained with straight line y=mx+c 2.Degrees of freedom -Part 2,Khan Academy-Ben Lambert https://www.youtube.com/watch?v=iA2KZHHZmmg.In this video importance of degrees of freedom in stats is explained .",308638.0
104820,451707.0,"Slope = change in Y/change in X or Slope =( Y2 - Y1) divided by (X2 - X1) When you chanage mile to kilometer, The values in denominator get multiplied by 1.6. Hope this answers your question",317996.0
104820,451725.0,"Since your variable X got multiplied by 1.6, your earlier coefficients are 1.6 times more so we divide it to bring them down.",318451.0
104820,454566.0,"Since slope = change in Y/change in X , once you convert the x-axis to km it basically expands so the denominator increases in the equation of slope and thus the slope also falls.",318756.0
104821,451670.0,"As far as determining which line is the best fit, It is done by finding values for β₀ and β₁ such that RSS has the least value. This can be achieved by various ways, prominent among which is Gradient descent. Gradient descent is explained further in additional resources section.",317996.0
104821,451721.0,"A line of best fit is drawn through a scatterplot to find the direction of an association between two variables. This line of best fit can then be used to make predictions. To draw a line of best fit , balance the number of points above the line with the number of points below the line. R square is an indicator which explain who well the best line explains your data. More the R square more data points your line covers",318451.0
104821,451663.0,RSS is sum of squares of distance b/w actual value and predicted value. But RSS is an absolute value . i.e Value of RSS changes according to scale of your independent and dependent variables. In order to avoid that we make use of TSS. TSS is sum of squares of distance b/w actual value and average value of Y. R² = 1 - (RSS/TSS)²..... This is a relative value. As value of R² tends to 1 it means that line is a better fit. As value of R² tends to 0 it means that line is NOT a better fit.,317996.0
104821,451819.0,RSS is residual sum of squares which is summation of square all the terms y-y' where y is actual value and y' is the value on best fit line while R square is just a measure without any units as it is ratio It can be directly displayed through excel charts to find the best fit line or you can use Python code to find the best fit line and the algorithm automatically finds the best fit line by comparison.,320685.0
104866,451880.0,"Hi Subhin, Yes, reinforcement learning is just a type of Machine Learning where an agent learn how to behave in a environment by performing actions and seeing the results. You may refer this for more details: https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419 The link contains detailed description about Reinforcement learning.",318355.0
104866,451950.0,No.Reinforcement learning is not a classification of ML model. Below Image will help you differents :,318476.0
104866,451979.0,Reinforcement Learning and Supervised Learning: A brief comparison https://hackernoon.com/reinforcement-learning-and-supervised-learning-a-brief-comparison-1b6d68c45ffa,318451.0
104872,451928.0,you need to plot a scatter plot fir sales and independent variable and then find the best fit line for the least RSS,318017.0
104872,451931.0,"You need to build the model with train data and then predict using test data. actual minus predicted value will give you a residual series and if you square each of them and sum, you will get RSS.",318329.0
104872,451956.0,If I understand the question correctly you have sales data for last four to five year and based on which you want to predict the next quater sales.The important question to ask here is to you have x and Y values to make the prediction as time/year is might not a good paramter to predict it. Tableau prodive the future quater prediction based on time series data .Try it out: https://onlinehelp.tableau.com/current/pro/desktop/en-us/forecast_create.htm,318476.0
104908,,nan,
104909,,nan,
104903,452141.0,"As mentioned in the video, if the dependent variable, Sales in Crores of Rupees is asked to be represented in terms of Millions of Rupees the RSS values would obviously change as it is Absolute. Accordingly, it is better to be Normalized. The normalization is done by dividing the RSS with TSS which would make it Relative. When we change the calculation from Crores to Millions even the TSS values would change and the Normalized value would remain the same",311160.0
104903,452053.0,"RSS is Aboluete as it is abolute error that is difference between its normal to pedicted value. and it is directly proportional to the vaule of Y ,as Y changes the error value or RSS changes directly . as per video, R² is Relative not TSS, R² analyzed by taking the TSS also into consideration as given below R² = 1 - (RSS / TSS) So , it is ideal to take relative thing than absolute",318732.0
104903,452681.0,I was wondering to know the meaning of absolute and relative terms.Which one is Independent and which one is Dependent ? Aboslute is independent/Dependent?,318372.0
104952,452294.0,"What is the use of RSE, After calculating RSE what we have to do with it: When you calculate the Training Set RSE, you measure the deviations of the model you generated, relative to the data set used to generate that model. On the other hand, the Test Set RSE measures deviations of the model you generated with some new data, not used previously to generate this model.",311117.0
104952,452134.0,"If the RSE value is very close to to the actual outcome value, then your model fits the data well. If there is a large difference between the values, then the model does not fit the data well. A downside to RSE is that it is very contextual to your data set and thus requires a general understanding of the measurements and overall scale to understand. For example, if the RSE value was 8.0 and the actual outcome was 10.0, without any additional information, this could either be a massive difference or a very small one. What are the units? What’s the overall scale? So we use R square R² is a different measure of fit that always takes the form of a value between 0 and 1, which means it is independent of the measures of your data. In short, an R² value that is closer to 1 indicates that the regression explains a large amount of the variability, while a value closer to 0 indicates that it did not.",318451.0
106129,458116.0,you can see the videos in the last module of the last week lecture it has error graph made in it,318017.0
106129,458155.0,"The difference between what was expected and what was predicted is called the residual error. Just like the input observations themselves, the residual errors from a structure like trends, bias, and seasonality. where the residual plot shows a non-random pattern. you will have to ""transform"" the data to use a linear model with nonlinear data. You can read the more detail here: https://stattrek.com/regression/residual-analysis.aspx https://machinelearningmastery.com/model-residual-errors-correct-time-series-forecasts-python/",317845.0
104949,452091.0,coz if you are doing it for ypred then you are finding the best fit line which will have the least r square value and by that only you can predict the value of y for a particular x. y pred will help building the best fit line for you .,318017.0
104989,452262.0,Kindly look at the below link: https://learn.upgrad.com/v/course/208/question/104729,311160.0
104989,452289.0,"I have replied the same question in below-mentioned link, Pls have a look: https://learn.upgrad.com/v/course/208/question/104729. For further clarification, pls follow: https://stats.stackexchange.com/questions/204238/why-divide-rss-by-n-2-to-get-rse/278459",311117.0
104989,452292.0,"hi Prateek , i was also confused about the same but i went through google and found that if you have two points on a plane and you will join these two points you will always have a straight line and once you introduce the third poin your line might not pass through all the points and you will have to find the best fit option for line so likewise the dof for three point will be 2 and like this we can say that the dof is n-2 where n is number of points for x. if you have the same plot in 3-d then the the dof will be n-k-1 where k will be other axis of x. hope this helps 😁",318017.0
105031,452471.0,"The difference, known as the residual value or, simply, residuals, is the distance between the actual points and what the model will predict. The error term can be thought of as the composite of a number of minor influences or errors. As the number of these minor influences gets larger, the distribution of the error term tends to approach the normal distribution. This tendency is called the Central Limit Theorem. The t-test and F- test are not applicable unless the error term is normal distributed. https://www.quora.com/Why-is-the-error-term-normally-distributed",303229.0
105031,452628.0,"Here's the basic idea behind any normal probability plot: if the data follow a normal distribution with mean µ and variance σ 2 , then a plot of the theoretical percentiles of the normal distribution versus the observed sample percentiles should be approximately linear. Since we are concerned about the normality of the error terms, we create a normal probability plot of the residuals. If the resulting plot is approximately linear, we proceed assuming that the error terms are normally distributed. refer this link for more clarifcation - https://newonlinecourses.science.psu.edu/stat501/node/281/",318451.0
105031,452721.0,"Residual standard error: The error (or disturbance) of an observed value is the deviation of the observed value from the (unobservable) true value of a quantity of interest (for example, a population mean), and the residual of an observed value is the difference between the observed value and the estimated value of the quantity of interest. How this error can be normally distributed?: For this answer, pls follow the link, where it has been explained with graphs and examples: https://stats.stackexchange.com/questions/12262/what-if-residuals-are-normally-distributed-but-y-is-not",311117.0
105048,452560.0,"Yes, the squaring is to get the absolute value of the error otherwise you will have some -ves and some +ves and when you add them up you will lose data about some errors.",310974.0
105048,452620.0,Why squared error? refer this link - https://www.benkuhn.net/squared,318451.0
105048,452567.0,"There are two reasons for using a root-mean-square (RMS) deviation rather than average-absolute-value (AAV) deviation: RMS deviations are mathematically much easier to work with than AAV deviations. Partly this is because squares are better-behaved mathematically than absolute values: often statisticians will not bother to calculate the standard deviation (σσ) and do all their work using the variance (σ2σ2). Partly this is because RMS averages have a clear relationship to the classical Pythagorean means (the arithmetic, geometric, and harmonic means). Gauss demonstrated that the RMS deviation was the best estimator of variance for simple independent random trials. That was part of how he developed the Gaussian distribution: what we call the Bell Curve or Normal Distribution these days One answer is grounded in history. Least squares regression goes back to the famous mathematician Carl Friedrich Gauss in the eighteenth century. Back in the eighteenth century, computers were not available. So, in order to compute a regression line, one would have to do the calculations by hand. We can determine the minimum of the sum of the squared residuals manually because it only involves minimizing a quadratic function, which can be done by taking the first derivative.",318476.0
105048,452737.0,For more insight you can go through below link in which many have answered the same question with different perspective. https://math.stackexchange.com/questions/1860579/why-work-with-squares-of-error-in-regression-analysis,317991.0
105048,453116.0,"This is just to get the modulus value of errors, so that while doing summing up ,will not have impact due to signs of errors.",313767.0
105048,453212.0,Squaring the value removes the negatives .,300684.0
105152,452883.0,"SSR is the ""regression sum of squares"" RSS is the ""residual sum of squares"" TSS is the ""total sum of squares"" r2=SSR/TSS given ,TSS = SSR + RSS =TSS-RSS/TSS =1- RSS/TSS Please go through the following link for detailed explanation: https://newonlinecourses.science.psu.edu/stat501/node/255/ =1-SSE/SSTO",311254.0
105152,452888.0,"R-Square is calculated as ESS/TSS. Which is Explained Variation to the Total variation. From the above graph you know that TSS = ESS + RSS ESS = TSS-RSS So, R^2 = (TSS-RSS) / TSS 1 - (RSS/TSS) You can check out the below link for better understanding: https://www.riskprep.com/component/exam/?view=exam&amp;layout=detail&amp;id=131",311160.0
105155,452887.0,"RSE is a measure of lack of fit of the model to the data at hand. In simplest terms, from the authors, if the RSE value is very close to to the actual outcome value, then your model fits the data well. If there is a large difference between the values, then the model does not fit the data well (James, Witten, Hastie, &amp; Tibshirani, 2013). A downside to RSE is that it is very contextual to your data set and thus requires a general understanding of the measurements and overall scale to understand. For example, if the RSE value was 8.0 and the actual outcome was 10.0, without any additional information, this could either be a massive difference or a very small one. What are the units? What’s the overall scale? Admittedly, as a researcher you are going to know this information, but even then, you need to understand the data at the highest level to know the significance of that difference. More so, the point of this example was that it does not make interpretation of the data immediately easy for anyone else. I did find it surprisingly difficult to find further information on RSE outside of the reading. It seems like it is more or less an R exclusive term and doesn’t have a lot of additional non-technical information. https://medium.com/humansystemsdata/rse-vs-r%C2%B2-ba8fba098434",318451.0
105155,452886.0,"Relative Squared Error (RSE): It measures performance based on a comparison with a simple predictor performance. The RSE normalizes the total squared error of the tested model and divides it by the total squared error of the simple predictor. Just like the other two measures, it ranges from 0 to infinite, being 0 the best value. Reference; https://medium.com/@wilamelima/metrics-to-measure-machine-learning-model-performance-e8c963665476",316349.0
105259,453319.0,"Classification vs clusteringClassification and Clustering are the two types of learning methods which characterize objects into groups by one or more features. These processes appear to be similar, but there is a difference between them in context of data mining. The prior difference between classification and clustering is that classification is used in supervised learning technique where predefined labels are assigned to instances by properties, on the contrary, clustering is used in unsupervised learning where similar instances are grouped, based on their features or properties. Classification is the process of classifying the data with the help of class labels. On the other hand, Clustering is similar to classification but there are no predefined class labels. Classification is geared with supervised learning. As against, clustering is also known as unsupervised learning. Training sample is provided in classification method while in case of clustering training data is not provided classification and clustering are the methods used in data mining for analysing the data sets and divide them on the basis of some particular classification rules or the association between objects. Classification categorizes the data with the help of provided training data. On the other hand, clustering uses different similarity measures to categorize the data.",318017.0
105259,453330.0,"The prior difference between classification and clustering is that classification is used in supervised learning technique where predefined labels are assigned to instances by properties, on the contrary, clustering is used in unsupervised learning where similar instances are grouped, based on their features. For further clarification, pls refer lnik: https://techdifferences.com/difference-between-classification-and-clustering.html",311117.0
105259,453403.0,Classification is used in supervised learning when we have labelled data and we can train our model to classify certain things based on pre defined features . While Clustering is used in unsupervised learning when we don't have labelled data and when we can just provide that data to our model and it clusters different images based on some similarities then it is called clustering.,320685.0
105259,453651.0,"For Classification , all the training data has a label.And on the basis of these labels we build a model to clasisify future values under these labels. However in clustering the data points have no label. The data is organised into groups on the basis of certain similar features. K means clustering is a good example.",311864.0
105259,453657.0,Simple rule of thumb : If the labels for the groups are known : Classification. If the labels are not known : Clustering.,318499.0
105259,453793.0,"Classification is the process of classifying the data with the help of class labels. On the other hand, Clustering is similar to classification but there are no predefined class labels. Classification is geared with supervised learning. As against, clustering is also known as unsupervised learning.",303083.0
105272,453391.0,"Difference between Training data and Testing data: Training data is the one on which we train and fit our model basically to fit the parameters whereas test data is used only to assess performance of model. Training data's output is available to model whereas testing data is the unseendata for which predictions have to be made. For further details with example and graphs, pls refer: https://towardsdatascience.com/how-dis-similar-are-my-train-and-test-data-56af3923de9b",311117.0
105272,453402.0,Suppose you have millions of pictures and the model is built to detect whether the picture is of cat or dog here among those millions of pictures we can use 75% labelled data( training data ) as our training set and then we can use remaining 25% to check the prediction made by the model and measure it's correctness based on already those 25% labelled pictures( testing data ) . So that we can use the best model for prediction and when the new unlabelled picture is provided as input to the model it can predict properly. The same thing is carried out e-mail spam case . we are just building an accurate model based on our previous data dividing it into two parts for training and testing it's accuracy so that it can predict correctly when provided with new data .,320685.0
105272,453660.0,"Hi Jyotishri, There is no specified rule to divide the given data is a particular way. From the given dataset, you can choose randomly and divide the whole dataset into two parts in any way you like. This is done to avoid overfitting of the model. Suppose you use up all the data for training - in that case your model can ""memorize"" the data and thus give perfect answer for figures in the dataset but fail to predict other data. To avoid this, two sets are made - one to train and one to test. You can read more about overfitting here : https://en.wikipedia.org/wiki/Overfitting",318499.0
105272,491114.0,Testing data is subset of training data . So testing data is one on which you test your model's performance while training data is used to assess and train the model,303082.0
105272,492897.0,"In simple words.. when you have a large set of data say of a telecom operator i.e. recharges done and you need to predict the revenue from each customer.. so you take such similar old data where you have both recharge and revenue data..( say 10,000 records).. u divide this data in 2 parts.. 1 is called training data (7000 records) and another is called test data (3000 records).. using training data you make the model.. and then you predict the output (in this case revenue) on the test data also.. here you can check the accuracy by checking the value your model is giving and what actually the revenue for each customer is showing.. you can refine the model accordingly.. then once model is final you can make predictions for a fresh set of data where you dont have revenue output data.. So train data is for making the model.. test data is for testing the model..",318791.0
105290,453388.0,"Cost function: A mathematical formula used to predict the cost associated with a certain action or a certain level of output. Businesses use cost functions to forecast the expenses associated with production, in order to determine what pricing strategies to use in order to achieve desired profit margins. For further detailed explanation with graphs, pls refer the following link: https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220",311117.0
105290,453711.0,"In Machine learning, a cost function can be defined as a measure of how wrong the model is in terms of its ability to estimate the relationship between X and Y. It is usually expressed as a difference between the predicted value and the actual value. The cost function can be termed as error can be estimated by iteratively running the model to compare estimated predictions against “ground truth” — the known values of y . The objective of a ML model, therefore, is to find parameters, weights or a structure that minimises the cost function. Refer the below links to understand better: 1.https://medium.com/@lachlanmiller_52885/machine-learning-week-1-cost-function-gradient-descent-and-univariate-linear-regression-8f5fe69815fd In this cost function is well explained along with gradient descent and regression with example 2. https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220",308638.0
108713,470053.0,"Hey Arpit, Once you'll insert a scatter plot on the data, a Chart tools bar will be activated. In that there are various Chart layouts available. One of which helps you directly calculate simple linear regression. Hope this helps.",302742.0
108713,470044.0,The formulas for implementing Simple Linear Regression have been used directly in excel and no additional tools have been used for excel demonstartion.,313826.0
108713,470075.0,no tool was used.,301648.0
108713,470084.0,"Hey Arpit, I think the link below can help you out well https://msu.edu/course/psy/403/StatDemos/Regression/Regression.htm Hope this helps out!",301655.0
108713,470066.0,Though it has been used manually by using formulas. But you can follow this link for additional knowledge. https://www.engineerexcel.com/linear-regression-in-excel-3-alternative-methods/,311117.0
108713,487440.0,You can have reference from following site: https://www.ablebits.com/office-addins-blog/2018/08/01/linear-regression-analysis-excel/,318772.0
108713,499109.0,To perform regression you can use excel add in by using the Data Analysis add-in.,301646.0
105485,454475.0,"Please go through the attached link for pdf file, where RSS, TSS, R^2, RSE are explained thoroughly : http://users.umiacs.umd.edu/~hcorrada/IntroDataSci/lectures/Regression.pdf",311117.0
105485,490153.0,"RSS: In statistics, it is defined as the total sum of error across the whole sample. It is the measure of the difference between the expected and the actual output. A small RSS indicates a tight fit of the model to the data. TSS: It is the sum of errors of the data points from mean of response variable. R2: a number which explains what portion of the given data variation is explained by the developed model. It always takes a value between 0 &amp; 1. In general term, it provides a measure of how well actual outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model, i.e. expected outcomes. Overall, the higher the R-squared, the better the model fits your data. RSE: The residual standard deviation is simply the standard deviation of the residual values, or the difference between a set of observed and predicted values. The standard deviation of the residuals calculates how much the data points spread around the regression line.",306726.0
105485,499117.0,detailed explanation https://www.stat.washington.edu/~nehemyl/files/UW_STAT391_linear-regression.pdf,301646.0
105588,455042.0,in the given example the commands for creating best fit lines were already used,318017.0
105588,455067.0,"I know that. Hence My quetsion is, - is using excel/python formulaes/libraries, the standard way to calculate the best fit or in practical situation we need to directly apply graidient descent etc and figure out by purselves",305842.0
105588,455070.0,"Yes, Excel was automatically giving us the best fit line along with the equation of the same. And then we found out RSS,TSS and the Rsquared value using the coefficients of the line equation obtained and doing some work on the coloumns. Yes it is a standard procedure to get the best fit line. And Yes, you can find it manually as well using the methods you've mentioned in the question. When the data is huge. It will be very difficult to do it manually in Excel. That's where the standard procedure in Python using sklearn comes handy.",318435.0
105504,454576.0,"In linear regression, we would predict a continuous variable while in logistic regression, we would predict the probabilities assosciated with the actual values and predict based on a threshold by us.",318329.0
105504,454603.0,"Yes, A Classification problem can be solved with a logistic regression. And No, Logistic regression is not a type of Linear Regression. Instead you can say, Linear and Logistic are both types of regressions. Below Image can clear a lot of doubts. We take a cutoff value depending on the S curve of logistic regression. Say 0.5, for this case. If Y &lt; 0.5 for some X, then category A - Points on y = 0 line and if Y &gt; 0.5 for then category B - Points on y=1 line. Read more here: https://medium.com/greyatom/logistic-regression-89e496433063",313515.0
106405,459434.0,they both are different types. try converting any one of them to the other,318329.0
106405,459486.0,the reason behind the issue different data type you can use the following code to convert the data type media['Date'] = pd.to_datetime(media['Date']).dt.date,317845.0
106405,497623.0,Both the variables need to be converted to proper types.,317460.0
106405,498944.0,"media['Date'] = pd.to_datetime(media['Date']).dt.date This helped , thank you",311803.0
105921,457192.0,strong multicollinearity must be checked and if vif value is large then the variable must be discarded,318017.0
105921,457193.0,"Correct, but it was not checked in the case study video...",304814.0
105921,457842.0,"Hi, Please read the attached documents https://stackoverflow.com/questions/25676145/capturing-high-multi-collinearity-in-statsmodels https://groups.google.com/forum/#!msg/pystatsmodels/2werOyWj-5g/b7jKpCN9y3kJ https://stats.stackexchange.com/questions/89028/i-have-my-data-set-now-what",344894.0
105973,457307.0,"Most of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem. If left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes. Tiny Features vs Mega Features To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling. How to Scale Features There are four common methods to perform Feature Scaling. Standardisation: Standardisation replaces the values by their Z scores. This redistributes the features with their mean μ = 0 and standard deviation σ =1 . sklearn.preprocessing.scale helps us implementing standardisation in python. 2. Mean Normalisation: This distribution will have values between -1 and 1 with μ=0 . Standardisation and Mean Normalization can be used for algorithms that assumes zero centric data like Principal Component Analysis(PCA). 3. Min-Max Scaling: This scaling brings the value between 0 and 1. 4. Unit Vector: Scaling is done considering the whole feature vecture to be of unit length. Min-Max Scaling and Unit Vector techniques produces values of range [0,1]. When dealing with features with hard boundaries this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255. When to Scale Rule of thumb I follow here is any algorithm that computes distance or assumes normality, scale your features!!! Some examples of algorithms where feature scaling matters are: k-nearest neighbors with an Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally. Scaling is critical, while performing Principal Component Analysis(PCA) . PCA tries to get the features with maximum variance and the variance is high for high magnitude features. This skews the PCA towards high magnitude features. We can speed up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. Tree based models are not distance based models and can handle varying ranges of features. Hence, Scaling is not required while modelling trees. Algorithms like Linear Discriminant Analysis(LDA), Naive Bayes are by design equipped to handle this and gives weights to the features accordingly. Performing a features scaling in these algorithms may not have much effect. Hope you understood the why , how and when of feature scaling.",318017.0
105973,457405.0,Maybe this link can help you out better in understanding the difference between the two: http://rajeshmahajan.com/standard-scaler-v-min-max-scaler-machine-learning/,301655.0
106121,458169.0,"You can compare the Model-1 and Model-2, presented in video lecture of ""Model Building"". Please go through the page no .21 of Lecture notes as attached: https://cdn.upgrad.com/UpGrad/temp/a852ca8a-d1b9-4779-8f86-fad3e722c1b6/Lecture+Notes+-+Linear++Regression.pdf",311117.0
106182,458356.0,"Thinking of data as sample or population may be valid for Hypothesis or Inferential statistics. But for models, you need as much data as possible. May be you could consider the samples for validation of model but I don't think sample/population terms is valid for model building.",318329.0
106182,458398.0,All our statistical inference is based on SAMPLING from the population data. extrapolation beyond the range of the data must be treated skeptically. The reason is that in many cases extrapolation (unfortunately and unavoidably) relies on untestable assumptions about the behavior of the data beyond their observed support.,317845.0
108909,470525.0,May it will be helpful: https://www.statisticssolutions.com/multiple-linear-regression/,311117.0
108909,470572.0,"yes you can use multiple linear regression here where independent variable can be 1,2,3 and dependent can be two variables or else you can make two seperate mlr for this.",318017.0
108909,471029.0,"Hi Jayashree, In this scenario, you need to train three different models 1. Dependent1 &gt;&gt; ( Independent1 Independent2 Independent3) 2. Dependent2 &gt;&gt; ( Independent1 Independent2 Independent3) 3. Dependent3 &gt;&gt; ( Independent1 Independent2 Independent3) Because you have three targets. A model can give only one target.",344894.0
115615,498848.0,we need not handle multicolinearity if the sole intention is to make prediction,300725.0
115615,500075.0,We remove multicollinearity just to remove redundency of data. As one independent variable is collinear with other independent variable then both is impliying same thing.,305651.0
115615,499071.0,"For doing any prediction using a model, multicollinearity needs to be removed by different techniques taught, else prediction would fail letting us know that model is distorted . By prediction and inferences, if you mean prediction and projection, prediction is meant for the same range where the model was built (train and test data range) - Intrapolation projection is meant for the range outside the range of where the model was built. Extrapolation",301121.0
115615,500706.0,"In regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity exists when: One independent variable is correlated with another independent variable. One independent variable is correlated with a linear combination of two or more independent variables. Multicollinearity makes it hard to assess the relative importance of independent variables, but it does not affect the usefulness of the regression equation for prediction. Even when multicollinearity is great, the least-squares regression equation can be highly predictive. So, if you are only interested in prediction, multicollinearity is not a problem.",314048.0
105523,454720.0,No there is no direct standard model for it. the classifaction model might help you to determine if the sheet while be borken or not broken based on prior system parameters .But it will not point out exactly which system is broken.,318476.0
105523,454737.0,"seems you are right , at this moment I hv to wait to understand how classification method can help on my problem i.e. ""make a model to indentify significance vaiables of Plastic sheet breakage "". may be we can ask this question with upcoming webinar session .",319969.0
105336,453573.0,"Causation indicates a relationship between two events where one event is affected by the other. In statistics, when the value of one event, or variable, increases or decreases as a result of other events, it is said there is causation.",318017.0
105336,453654.0,"Correlation between two variables shows whether both of them are moving upwards/downwards together or in reverse direction. It doesn not tell that movement in one variable is 'causing' movement in the other. For ex. in rainy season there is an increase in sales of umbrellas. Also, there is increase in no. of accidents on road because of slippery conditions. So if someone tries to calculate a correlation b/w these two variables for this period, she might get a high value as both are increasing together. But we can not say that because of higher sales of umbrella more accidents are happening or vice versa. One is not causing the other. They are actually being caused by 'rainy season'. Thus Correlation doesn't imply Causation. Hope it helps.",311686.0
105336,453666.0,"what is causation? - In this context, One variable value is the result of occurance of the values in the other variable. One good example which I have come across is: smoking causes an increase in the risk of developing lung cancer. Where as, smoking is correlated with alcoholism, but it does not cause alcoholism Is it applicable in SLR or MLR? - Linear Regression explains Correlation but not the Causation",311160.0
105336,453798.0,"Causation is often confused with correlation , which indicates the extent to which two variables tend to increase or decrease in parallel. However, correlation by itself does not imply causation. There may be a third factor, for example, that is responsible for the fluctuations in both variables. A statistically significant correlation has been reported, for example, between yellow cars and a lower incidence of accidents. That does not indicate that yellow cars are safer, but just that fewer yellow cars are involved in accidents. A third factor, such as the personality type of the purchaser of yellow cars, is more likely to be responsible than the color of the paint itself.",303083.0
105383,454002.0,We need to add this cammand before calculating delta values. d0=pd.to_datetime(d0) In above error it is clearly shows that data type format was different so we need to convert it in pandas timestamp object to perform the arithmatic operation on that data.,311004.0
105383,454150.0,try using media.dt.date,318329.0
105383,454383.0,I changed the below line of code to solve it: d1 = media.Date to d1 = media.Date.dt.date,318598.0
105383,453989.0,"Let's use pandas Timestamp.now(): s = pd.Series('27-Sep-2018') s = pd.to_datetime(s) (s - pd.Timestamp.now()).dt.days Output: 0 15 dtype: int64 Note: The error is stating that you can't subtract object type DatetimeIndex from object 'datetime.date'. So, use pandas Timestamp to create the same object type as DateTimeIndex follow the below link : https://stackoverflow.com/questions/52278464/convert-datetimeindex-to-datetime-date-in-pandas",318017.0
105383,454471.0,"Nagaraju and Isha has suggested as easy and clear answer- Use d1 = media.Date.dt.date It serves the purpose here easily. But if you need a general approach to change the data type of values in a column to do any operations, you can use apply function with lambda on the Series like below: delta = d1.apply( lambda x: x.date() - d0 ) .dt.date() is available for pandas Series object but in 2nd approach we apply to every value in Series so we only need x.date() here. Hope this helps.",313515.0
105383,454590.0,"There is an issue in this Media+Company.ipynb # Deriving ""days since the show started"" d0=pd.to_datetime(d0) or d1 = media.Date.dt.date any of the conversion should be fine",312019.0
105383,454967.0,Thanks everyone for answer,317156.0
105383,459431.0,"Yes, this is because of difference in types od d1 &amp; d0. Converting d0 using pd.to_datetime should resolve the issue.",306726.0
105380,453898.0,"The CSV has a comma at the end of each row and pd.read_csv created a new column with NaN's because of that. Excel might not be considering that one to be a new column. Since, this column is of no use to us, it is being dropped. You can observe the dataframe after loading without dropping it. You will find the column.",318329.0
105380,454482.0,To see it more clearly you can do the following: media = pd.read_csv('mediacompany.csv') media.head() As you can see the last column is there which is droppped by this particular line of code.,313515.0
105565,454976.0,"Both adjusted R-squared and predicted R-square provide information that helps you assess the number of predictors in your model: Use the adjusted R-square to compare models with different numbers of predictors. Use the predicted R-square to determine how well the model predicts new observations and whether the model is too complicated. Please refer the link, where it is explained with examples for clear understanding: http://blog.minitab.com/blog/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables",311117.0
105565,455338.0,Thanks guys for helping !!,312479.0
105565,454907.0,"if you consider R and R-Sq and observe that there is not much difference in the values, then you can confidently say that introduction of new variable has not penalized the R-Sq, and infact this new variable is good a predictor. Ultimately we need to rely on R-Sq, but its good to view both R and R-Sq for the reason mentioned above as we dont want to introduce and un necessary variable.",304814.0
105639,455536.0,The Lag View mentioned is the 1 month lag view meaning. Month Values Lag Values 1 10000. 0 2 20000. 10000 3. 30000. 20000 And so on for other months.,318476.0
105639,455712.0,A Lag is used when you suspect that the variable will have a impact after few weeks or months. In Marketing Mix Modelling variables like sampling are laged. If a company is distributing samples than the customer will use it experience it when he again think of buying that product than he may recall the sample he used. So it easily ends up after a week or month.,318451.0
105639,456486.0,This lag is similar to lag used in the SQL assignment. It will take the previous row value for that variable.,317689.0
105639,464737.0,The reason for creating lagview is for example if you watch a tv show today and you felt it is very interesting and the probability of watch this show tomorrow by you is very high. So the expert mentioned if you take the lagviews i.e. visitors of the show in the previous day and compare with today visitors. It will help in understanding the quality of content of the TV show.,314244.0
105678,455868.0,What would you pass to the function? What are the values corresponding to the days?,318329.0
105678,456472.0,"The function input 'i' corresponds to the 'day' column which is the number of days since the show started. Since the first day was a ""Wednesday"" the weekend corresponds to 4 &amp; 5. Also the corresponding 'weekday' column shows which day of the week it is. A better/intuitive way would have been to use the 'weekday' column instead of the 'day' column to find the weekend and not a weekend",311857.0
105696,456039.0,"If it is before removing the features using VIF, there there might be multicollinearity affecting the sign of the coefficient",318329.0
105696,456489.0,"There is multicolinearity with charecter A and AD Impressions. So, when we add AD impressions charecter A become insignificant.",317689.0
105692,455984.0,,310974.0
105692,456279.0,it can be anything which can be defined with a mathematical function coz its not necessary to have a linear relationship for example it can be polynomial or log function as well,318017.0
105692,456485.0,Multiple linear regression will give you a hyper plane instead of line. We draw a line when there are total 1 dependent variable and 1 independent variable i.e. in 2D plane. When your dependent variable increases the dimension of the equation will also increase. It might be a 3D figure or n dimension figure but the relation between varaibles is linear. i.e. increase in x will have a linear increase in y. If it is non linear for example squared than y will be proptional to x^2.,317689.0
114990,496183.0,"going to the basic, it is a classification problem, where we want to predict the vote % of each party from the total pool of voters. By that line of thinking, broadly we can use logistic regression technique for this purpose. specifically, SVM is a good candidate as shown in a Stanford discussion. https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;url=http://cs229.stanford.edu/proj2017/final-reports/5232542.pdf&amp;ved=2ahUKEwio-Ibiuf_gAhXPfX0KHSO6BksQFjABegQIBhAB&amp;usg=AOvVaw1I9okTvcWb09JdyuFAWYhV",305839.0
114990,496455.0,its a logistics regression problem,318017.0
114990,497622.0,"In Election poll prediction if we are trying to predict which party will win, then it should be Classification problem ideally as linear regression is used for continuous variables.",317460.0
114990,498849.0,it is a classificatin problem,300725.0
136584,591660.0,"Hi Isha, This is got from Regression prediction of final model, doing the calculation with the budget.",344894.0
116475,503627.0,We can say a model is robust and generalisable when it is simple. Such models works well even if there are changes training data. They are immune to the specifics of training data provided . It rather picks essential characteristics that is invariant across any training datasets. Accuracy measures how accurate the model is predicting the output. We can increase the accuracy of the model by adding more features yo the model which in turn increases it's complexity. There should be a trade off between Accuracy and generalisation (robustness) of the model.,310467.0
116475,503599.0,"These are 4 unique points about using a simpler model where ever possible: A simpler model is usually more generic than a complex model. This becomes important because generic models are bound to perform better on unseen datasets. A simpler model requires less training data points. This becomes extremely important because in many cases one has to work with limited data points. A simple model is more robust and does not change significantly if the training data points undergo small changes. A simple model may make more errors in the training phase but it is bound to outperform complex models when it sees new data. This happens because of overfitting . You can refer this session lecture: https://learn.upgrad.com/v/course/208/session/31167/segment/164672 For in-depth knowledge, you can refer this: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4535087/ This may be helpful.",311117.0
116494,504083.0,It is the same as what we learned in the assumptions of linear regression:,310974.0
116494,503717.0,"Noise in your dataset are the meaning less data. There are two kind of noise 1. Class noise (label noise). This occurs when an example is incorrectly labeled. Class noise can be attributed to several causes, such as subjectivity during the labeling process, data entry errors, or inadequacy of the information used to label each example. Two types of class noise can be distinguished: Contradictory examples : In the figure placed above the two examples (0.25, red, class = positive) and (0.25, red, class = negative) are contradictory examples, since they have the same attribute values and a different class. Misclassifications: examples that are labeled as a class different from the real one. In the figure placed above the example (0.99, greee, class = negative) is a mislabeled example, since its class label is wrong, and it would be ""positive"". 2. Attribute noise. This refers to corruptions in the values of one or more attributes. Examples of attribute noise are: Erroneous attribute values. In the figure placed above, the example (1.02, green, class = positive) has its first attribute with noise, since it has wrong value. Missing or unknown attribute values. In the figure placed above, the example (2.05, ?, class = negative) has attribute noise, since we do not know the value of the second attribute. Incomplete attributes or ""do not care"" values. In the figure placed above, the example (=, green, class = positive) has attribute noise, since the value of the first attribute do not affect the rest of values of the example, including the class of the example. mean zero and variance 1 this means the Noise is normally distributed.",311254.0
116025,501748.0,"Basically it is comparison. If a model is "" overfitting "" then the chance of variance for test data, unseen data is high. If the model is simple and not overfitting, then chance of variance would be less for data other than training data too. This is what he was trying to explain comparing two models",301121.0
116025,502563.0,"For a over fitted model, the model has learnt about the inherent noise and patterns of the training data as well. Alternatively it means that whenever a data whose pattern is similar to the training data is put to test on the model, the model will do great and the variance will be low. On the other hand for actual data/test data it will not have the same noise patterns as training data, but because the model is trained using that, the variance will be very high. In other words, the model will fail to predict the outcome of the test data correctly.",318366.0
116469,503591.0,"There is no calculation required for getting the answer. Here, Y = x + 58z + log(w) , and Y = 2x + exp(w) has complex functions as having "" log "", and "" exp "", so these will be eliminated. Y = 3x + 0.005z + w , has coefficient ""0.005"", i.e. 5/1000 , which is also a not simple integer. This is also eliminated. Y = x + 3w, has simple integer coefficients. Hence, it will give the simplest model.",311117.0
116469,503551.0,"There is no calculation that needs to be done here. Its just that the correct answer Y = x + 3w , has simple integer co-efficients, only relevant features (note that z is not present) and also that no complex functions are part of the model make it the simplest model as compared to the other models.",313826.0
116469,503662.0,"You need not perform any calculation here. All we have to do is find the simpler model from the provided list. Look at the models below: Y = 3x + 0.005z + w Y = x + 58z + log(w) Y = x + 3w Y = 2x + exp(w) As professor explained, imagine that you are converting them to bits it is clear that Y = x + 3w consumes less number of bits when compared to the others. Meaning, Y = 3x + 0.005z + w has Float Y = x + 58z + log(w)= uses log Y = 2x + exp(w)= uses exp The simpler model is Y = x + 3w.",314048.0
116469,503858.0,As professor as explained we should take only simple model Y=x+3w,306996.0
116469,507243.0,Visually u can say that Y = x + 58z + log(w) uses log Y = 2x + exp(w) uses exp So definately they are complex model. Now you have to select from Y = 3x + 0.005z + w Y = x + 3w. No of bits to represent coefficient of (Y=x+3w) is less than the coefficient of (x+0.005z+w). Hence Y=x+3w i simpler.,315679.0
115820,500922.0,True for Option 2,308495.0
115820,500934.0,"If we take accuracy only in train data then obviously it doesnot explain the reason behind using machine learning model. But evn considering it for test data , accuracy in itself is not the best metric to judge the effectiveness of the model. The model effectiveness is also driven by the business use case and other metrics which vary across algorithms and business problems. However, the most general use case of machine learning model is to gain insights from unseen data by making use of the data which is available to us so when the unkown data in the domain of the same business problem is sent as input to the model/algorithm/function the output is more close to the ground truth. The option 1 is therefore less accuarte. The option 3 is exactly what you interpreting it to be.",318576.0
115820,501044.0,"Option 3 is suggesting that, given a finite amount of data we build a model to learn this data and then use the model to extrapolate these learnings on a new set of data which is similar to the kind of data that was used for learning.",313826.0
115820,501693.0,"Option 1 : How to choose models so as to achieve the highest level of accuracy Ans) Question tells us about accuracy on the test set. It is possible for a model to memorize the training data while failing to truly learn the underlying trends and patterns. On unseen data, memorizing is bound to fail.Thats why the accuracy of test sets matters. Option3 : How to extrapolate learnings from a finite amount of data to explain or predict all possible inputs of the same kind Ans) I guess, the question is talking about creating a learning algorithm. A learning model is told what needs to be done (i.e. explain possible inputs of same kind) and it figures out how it needs to be done and returns a model.",302735.0
116188,502454.0,"Hyperparameters and parameters are often used interchangeably but there is a difference between them. You call something a 'hyperparameter' if it cannot be learned within the estimator directly. However, 'parameters' is more general term. When you say 'passing the parameters to the model', it generally means a combination of hyperparameters along with some other parameters that are not directly related to your estimator but are required for your model. For example, suppose your are building an SVM classifier in sklearn: from sklearn import svm X = [[0, 0], [1, 1]] y = [0, 1] clf = svm.SVC(C =0.01, kernel ='rbf', random_state=33) clf.fit(X, y) In the above code an instance of SVM is your estimator for your model for which the hyperparameters, in this case, are C and kernel. But your model has another parameter which is not a hyperparameter and that is random_state",311254.0
116188,502492.0,"Hi Bindu, You can refer to the below link. https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/",308673.0
116188,502834.0,"A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data. They are required by the model when making predictions. They values define the skill of the model on your problem. They are estimated or learned from data. They are often not set manually by the practitioner. They are often saved as part of the learned model. A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data. They are often used in processes to help estimate model parameters. They are often specified by the practitioner. They can often be set using heuristics. They are often tuned for a given predictive modeling problem. Model parameters are estimated using an optimization algorithm, which is a type of efficient search through possible parameter values. Model hyperparameters are often referred to as model parameters which can make things confusing.",314183.0
116188,502509.0,"In this session, where we are in the infancy of principles model selection etc,what we need to understand is hyperparameters are some which are required for regularization it will control the complexity of model selection. The following sentences will make it even clearer from the conceptual point of view. The learning algorithm is given the hyperparameters as an 'input' and returns the model parameters as the output Hyperparameters are not a part of the final model output, but the model parameters are. (For example coefficients are model parameters)",301121.0
116427,503367.0,"Class of models in the sense, Linear Class of Models, non-linear Class of Models etc., They dont mean implementing different algorithms and choosing the best model.",318435.0
116427,503826.0,"Refer to the lecture by the professor giving examples of choosing a learning algorithm. For example suppos when the learning algorithm is chosen as regression (linear, logistic or SVM), then it will produce only linear class of models becase it is designed to do so. and when the learning algorithm chosen is decision tree (or regression tree) then it will produce only a tree based class of model because it is designed to do so. So, a learning algorithm can only produce models of a certain kind or class. Hence, a learning algorithm designed to produce a linear class of models will never produce tree based model or clustering model or any other model other than linear class of models. Hope the answer clarifies your doubt.",318479.0
116271,502805.0,"Hi Darshana, The tradeoff occurs depending on the model fitting. Yu can refer to the below link for detailed explanations. https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/",308673.0
116271,502789.0,One simple reason is the high model complexity.,310974.0
116271,502807.0,"Bias is basically how much error the model is likely to make on test data. Variance is how sensitive the model to the input data. If the model is learning all the train data the model prediction on the train data will be very high and the error will be very minimal leading to low bias. At the same time any change to input data, the model will have to retrain hence the variance is very high. In the example of two students, the student who was mugging up tutorial, his bias is very low as he would crack the exam the way it is. But any change in the input data, he will have to mug the whole tutorial again. Hence the variance is high.",317514.0
116271,502836.0,"Linear regression can fit the training data very well and the testing data very poorly. This is known as overfitting the data (low bias and high variance). Similarly, it could fit the training and testing data very poorly (high bias and low variance). This is known as underfitting the data.",314183.0
116271,502795.0,"Generally low bias and high variance occurs in case of overfitting. If your model is very complex and has learnt training dataset. Now, the test values would be all over the place in such cases. Refer to following link https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229",317689.0
116271,503442.0,"Linear regression can have high bias and low variance, or low bias with high variance depending on whether it is a case of underfitting or overfitting respectively. There are various parameters involved which influence how your model fitting is happening such as number of datapoints, number of features considered, ratio of number of observations to features considered. However, there are ways the model fit can be improved such as regularisation, remove features in case of an overfit scenario. Whereas, an underfit model can be improved by adding features or degree of polynomials in the feature considered.",316147.0
116271,503518.0,The goal of any supervised Machine Learning model is to achieve low bias and low variance . The reason why it is call a trade-off is because by increasing the model's complexity the variance will increase and the bias decrease whereas with more simpler models its the bias which increases and variances decreases.,319319.0
123574,537480.0,predict_proba is being provided by sklearn. But Random Forest as such is not a probabilistic classifier like Logistic Regression.,310974.0
123574,538429.0,"Hi, Please follow the below link https://stackoverflow.com/questions/30814231/using-the-predict-proba-function-of-randomforestclassifier-in-the-safe-and-rig",344894.0
116448,503450.0,Hope this like below will help: https://newonlinecourses.science.psu.edu/stat501/node/324/,318476.0
116448,503465.0,"Yes, it is possible to fit a polynomial to the data with more than one variable. Both the above models are linear. They both are the second order polynomials. First one is with just one vaiable, x. Second one is with two variables X1 and X2. Similarly, we can have any number of variables. We can easily represent one variable against the output using a 2d plot. For more than 2 variables, we need to plot a higher dimensional plot( for e.g. 3 d plot for 3 variables). But it gets difficult to plot as the number of dimension increases. As a workaround we can plot these variables separately against the output variable. The same has been explained in the optional assignment.",310467.0
116454,503437.0,I think the idea of the demo was to give example on how to use hyperparamter to the present model like RFE in feature selection pervious we used manual methods for the same . With cross validation and hyperparamter we now a one additional methods to decide on number feature selection and then use the VIF to remove correlated feature.,318476.0
116454,503726.0,"This lecture was demonstrating using k-fold with RFE to iterate over a range of features (1,,,13). VIF comes after that anyway - after you have chosen how many features you want. A As has been said before (in last semester) we want to work with around 15 features for manual fine tuning of the model (of which VIF is part of). The lectures are focusing on getting maybe 100+ features down to that set of 15 (or 10, etc).",300694.0
116669,504557.0,"I don't it would effect the output cross validation if not to a great extent. Cross validation should work ideally on any kind of data since we are going to shuffle the data. But, it is an interesting question.",310974.0
116669,504588.0,"It is advised to shuffle the data before cross-validation when data are from different sources , as some records would form part of a certain class. I suggest you refer to the interesting link below on the same topic. https://stats.stackexchange.com/questions/180827/model-construction-when-to-shuffle-data-and-when-to-sort-it/180847",301121.0
116669,504699.0,https://stats.stackexchange.com/questions/134446/does-the-order-of-input-matter-in-cross-validation-in-linear-regression,311254.0
116457,503433.0,The number of feature to be used can be a hyperparamter in the RFE model feature selection then the same feature can to used to build the OLS model. Hope I answer your query.,318476.0
116457,503663.0,"As per my understanding, the number of features are hyperparameters in the Linear regression because as the number of features used in the model increase, the complexity of the model increases. While doing regularization, we reduce the complexity using hyperparameters, in this case, using less features which define maximum variance, hence complexity is reduced.",308967.0
116457,503723.0,"'number of features' is something we decide (whether it is use RFE or some other way). And as per definition of hyperparameter (something that cannot be estimated by the model but still plays a crucial role in affecting a model). Number of features affects the model so I would think it is a hyper-parameter. Although I can see in your post you have found it not to be, and me searching via google has also brought up many articles saying feature selection is not a hyperparameter. So based on that I would say feture selection is not a hyperparameter because it is not a parameter that affects the model per se. Say if you had 100 data rows (yes rows) - but you only decided to feed 99 of them into the learning algorithm -&gt; but number of rows is not a hyperparameter - however the final model IS affected. Similarly, altering the number of columns affects the model but still isn't a hyperparameter.",300694.0
115855,501143.0,An interesting discussion here https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation,310974.0
115855,501463.0,Go through here: https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f Hope this will help you.,308639.0
115935,,nan,
115915,501405.0,In Cross validation with and without k-folds Data in each fold is different. Because data is divided into different folds randomly each time. This results in slight variation in R2 score,317996.0
115915,501368.0,"R2 changes beacuse it has been calculated on the different sets of training and test data. With k-fold and k=5, we are using 20% of the data as test set (For each iteration, a different 20%) whereas without k-fold we are splitting into 70-30. So using 30% of the data as test set.",304319.0
115915,502753.0,the data set used for calculation is different in Cross validation with and without k-folds. This results in variation in R2 score,302735.0
115947,501466.0,"Hi, There is a typo. It should transform not fit_transform.",344894.0
115947,501802.0,yes.. It should be a Transform on Test data,311004.0
115947,502045.0,We use fit_transform() on the train data so that we learn the parameters of scaling on the train data and in the same time we scale the train data. We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.,319319.0
115941,501500.0,Here we used Cross Validation for hyper parameter tuning which is only valid during the training phase and the whole idea is to not to have the learning algorithm access to the test data.,310974.0
125798,548783.0,"Yes, we can. Better to apply few more classification model and compare the result.",301648.0
116020,502214.0,Choose a class of model Choose model hyperparameters Fit the model to the training data Use the model to predict labels for new data Repeat this proces in loop.,313526.0
116020,501741.0,"This is done for hyperparameter tuning. You would want your hyperparameters perform well on test data. But if we go via conventional split of test and train they hyperparameter tuning is happening while running it on test data, so hyperparameters are being tuned as per the test data. Total Dataset is split in test, train and vaildation set. So, in order to separate the test data all together and get final performance of model on test data, hyper-parameter tuning is done on validation set. After this you test the final model with fixed hyper-parameter on test data.",317689.0
116027,501747.0,"Make a sampling (random) from your original dataset for the training, validation and test set. It could be 70%, 20%, 10% or 65%, 20%, 15%. Training dataset should not be too small otherwise your models will get bias. Validation dataset, should not be very small(e.g. 5%), otherwise your model selection can get wrong.",311254.0
116027,502642.0,"With less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage",318427.0
116027,503185.0,"I have come across varying ratios - 60-20-20, 70-20-10",319302.0
116031,501945.0,"Higher the degrees higher the complexity and hence the higher overfit. When the degree is 1 it is linear and 2 it becomes quadratic and so on. Higher the degree means higher the overfit. While doing training and testing, as we start increasing the degree we will have to check how the overfitting increases as the degrees go up. You can plot and check for visual inspection for different degrees.(in logarathimic scale if necessary) It is also a good idea to compare ""R2 value"" for train and test and arrive at a figure for the degree.",301121.0
116031,502756.0,"Koustav, i guess the to determine degree of model. We have to use Variance Bias trade off Professor said that our model should be simpler and robust. But, it doesn't mean that degree1 models will be useful in our test dataset. Please refer to attached screenshot above, it may help you with understanding the optimal model complexity. As Prof. said ""Even for the simpler model, it still need to remember some data characteristics so that we are able to generalise the model for unseen data"". You can refer to http://scott.fortmann-roe.com/docs/BiasVariance.html for more details. Hope this helps",302735.0
116053,501907.0,"Hyper parameters can be chosen with intuition and business understanding. However, we can employ Cross Validation for hyper parameter tuning to compare the results of the model for different values of the hyper parameter and take the call. Example: In clustering, business understanding can tell you that it makes sense to have 5 clusters. By looking at the data distribution, intuitively you can tell that there could be 4 clusters. A better way to decide or validate this is to perform statistical/mathematical procedures.",310974.0
116053,502017.0,"Hyperparameters are parameters that we pass on to the learning algorithm to control the complexity of the final model. In Linear regression example, the number of variables had lot of bearing on final model produced by the learning algorithm. If we had used one parameter, the model is simple while if we use all 13 variables, the model is complex. We decided based on k-fold cross validation that 10 variables provide the Optimal R2 value. We pass this Hyperparameter 10 variable to find the coefficients of the linear equation. In case of Logistic Regression, we can use the cut-off threshold as Hyperparameter. The cut-off decided has a lot of bearing on final model produced by the learning algorithm. The learning model finds the probability for each record and based on provided Hyperparameter (Cutoff=0.5) decides to classify as say Churn or Not Churn. Say we had provided Hyperparameter i.e cutoff as 0.1 vs 0.9 will have very totally different results",317514.0
116053,502751.0,"Hyper parameters are the inputs to the learning algorithm to control the final model complexity. They are the choice that the algorithm designer makes to tune the behaviour of the learning outcome. Therefore, Hyper parameters are usually fixed before the learning process begins. For ex: Number of depth or leaves of a tree. Number of clusters in a k-means clusters Learning rate of a model. Or in a lecture earilier, Initially there were 13 parameters, which the speaker mentioned as hyper parameters and the later tuned those parameters down to 10.",302735.0
116087,502232.0,PCA and Hyper parameters are two different concepts. PCA is for dimensionality reduction which is applied on the data. Hyper parameters is the parameter whose value is set before the learning process,318804.0
116087,502024.0,"On a high level, PCA is a dimensionality reduction algorithm where as Hyperparameters are the parameters that we pass to the learning algorithm ( for e.g. dimensionality reduction algorithm) to control th ecomplexity of the model it produces.",310467.0
116087,502374.0,PCA is used for dimensionality reduction of data whereas the Hyper parameter is a concept that is used to tune the parameters of an algorithm.,320687.0
116087,502743.0,"PCA provides you with principal components made out of original features which are orthogonal to each other; Each component will have certain loading factor of all original feautures which will not be easily visible to us. (By choosing limited number of principal components, you will be able to do dimensionality reduction which will make the model simpler or less complex and you will be able to get to the accuracy of 90% possibly). Hyper-parameters are parameters which we pass on to the learning algorithm to control the complexity of final model. They are the choice that algorithm designer makes to tune the behavior of learning outcome. So if we go by definition, we will first create a model using PCA (dimensionality reduction technique) and then will tune the model or decrease its complexity by tuning the hyper-parameters.",302735.0
116087,503296.0,"The main difference is that Lasso is a regularization technique (adding vars to minimize effect of large coefficients) while PCA is feature selection technique (by covariance matrix decomposition). PCA allows you to do feature selection outside of the fit and transform and therefore give more flexibility in the hyper parameter search. Whereas in lasso the ""feature selection"" is kind of done for you and therefore there is less scope of hyper parameter optimization.",314183.0
116089,502067.0,"Hyperparameters are parameters that we pass on to the learning algorithm to control the complexity of the final model. In case of linear regression, the number of input variables is a hyperparameter. The process of selecting an optimal number is done using GridsearchCV in python. Here we are trying to find the R2 score for case where we had selected 1 parameter vs 2 parameters ….vs 13 parameters. We then determine at what variable selection the model complexity is optimal",317514.0
116089,503299.0,This technique involves randomly dividing the dataset into k groups or folds of approximately equal size. The first fold is kept for testing and the model is trained on k-1 folds. The process is repeated K times and each time different fold or a different group of data points are used for validation.,314183.0
116089,502008.0,"GridSeach CV model gives the following output if there are 13 features, First row : score when features selected =1 Second row :score when features selected=2 Simialrly 13th row gives us the score when all 13 features are selected. The following link explains gridsearchcv with an example. https://www.kaggle.com/cesartrevisan/scikit-learn-and-gridsearchcv",310467.0
116033,501968.0,"You split the data into 5 fold say d1,d2,d3,d4 and d5. Iteration 1 you use d1 as test data and d2,d3,d4,d5 as train data. You compute R2. For iteration 2, you use d2 as test data and d1,d3,d4,d5 as train data. You repeat the step for 5 iterations. The number of features is the hyperparameter which you are optimizing in this case. You have range from 1 to 13 features. So first you you will train your model with 1 feature and it will be doing 5 iterations (because we have 5 folds) The second time we will run with 2 features and it will be doing 5 iterations (because we have 5 folds) We will repeat this for up to 13 features. Hence the algorithm runs for 13 * 5=65 times",317514.0
116106,502070.0,"In the example 5 was a random pick and was not derived. There is post in discussion fourm arleady on ""Thumb rule to select K for K-Folds CV?"" https://learn.upgrad.com/v/course/208/question/115855",317514.0
116138,502195.0,I think the best practice and industry standard to give n_features_to_select is up to 15-20. This is similar to RFE where we select number of output variables to be up to 15. This number is independent of total number of variables. Request TA to please correct me if I am wrong.,310522.0
116138,502286.0,"We need to give numeric range [1, 2, ...., n]. The algorithm automatically maps all the independent variables to numeric range indexes.",306248.0
116158,502369.0,pca is something which is used when you have highly correlated data given to you where as feature selection is a method which can be used to reduce the number of variables used for model making it can be possible that the features are made out of correlated variables and it might be that the features are created out of some understanding or requirements.,318017.0
116158,502396.0,"Adding to Deval's, Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and PCA is that 1.""F eature selection"" keeps a subset of the original features 2. ""PCA"" creates brand new ones which are linear combination of original features and are uncorrelated to each other.",310467.0
116158,502644.0,"There are major differences explanation will clear your doubts. PCA provides you with principal components made out of original features which are orthogonal to each other; Each component will have certain loading factor of all original feautures which will not be easily visible to us. (By choosing limited number of principal components, you will be able to do dimensionality reduction which will make the model simpler or less complex and you will be able to get to the accuracy of 90% possibly) Feature selection / elimination has been taught to us multiple linear regression model where we have used an algorithm called RFE (Recursive Feature Elimination) which does it automatically for us. It is upto us choose the number of features and generally (10-15 even though there is no strict rule) CV - Cross Validation score where we are in a position to change test data on different folds; By choosing CV (In the Python example they have chosen this as 5) you will be able to use 5 different sets of internal test data without going for any additional test data. This enables us to validate our model with internal test data. Please note that CV has nothing to do with feature elimination but RFE is.",301121.0
116301,503019.0,I also think the same and have raised an error ticket twice for that. But didn't receive the clarifications. Need clarification from TA!,318585.0
116301,503156.0,I have also raised a query in this regard and awaiting for response from TA.,313826.0
116301,503255.0,"Hi, Probability threshold in logistic regression is not a hyperparameter. Because we are not tunning our model based on the threshold. We always decide the threshold based on our business scenarios.",344894.0
116301,503365.0,Exactly Paras!! this is what we're saying and this is what StackOverflow link that i provided says. But then graded question call it as hyperparametrer. Take a look at graded question number 03 in Model Evlauation now!!,316349.0
116303,503025.0,"Lambda is propotional to Error term. If you increase Lambda, Errror term will increase and vice versa.",318368.0
116303,502900.0,"In normal regression or unregularized regression model, you try to minimize the error term resulting in arbitrary complex coefficients. In Regularized regression model or as in above equation for Ridge Regression model, you have error term and regularization term and the model tries to minimize the sum of both. Now in the above equation if lambda is close to zero then you are essentially doing normal regression or unregularized regression. So you are only worried about how you can bring the error term down and do not care about whether the model itself becomes complex. Other way round, if lambda is high, you are putting lot of premium on controlling the complexity of model. Hence Lambda is basically the balancing factor. You will typically choose value of lambda which best captures relative importance of error versus model complexity.",317514.0
116303,502963.0,"Hi Aditya, What you mentioned is correct. The only difference is that overall term is the error term now and we are trying to reduce the whole by addition of a new correction term which is hyperparameter * regularization term.to reduce the error We have to choose low lambda if we want to reduce the overall error term We have to choose higher lambda if we want to regularize the model (that is to reduce the complexity) Basically there should be a trade off. Here comes the interesting concept: If you reduce the error term as much as possible (close to zero) on the training data , it will start overfitting by increasing the complexity whereas if you increase the value of lambda then regularization will be better but error term may be significant but it will be a simpler model. Please note that we also should not create a model which is so simple that it becomes naive. .",301121.0
116307,503266.0,"Hi, No, Question is correct but the answer might not correct",344894.0
116319,502953.0,Scaling is always done after Test Train split.,317514.0
116319,503016.0,This is always done after test train split as Model should run on unseen data for testing and validation.,318368.0
116319,503061.0,"You are right on what you mentioned. Also in the same ""Cross-Validation - Linear Regression"" in the second case, they are doing after the Test but doing fit_transform on Test data instead of transform",317514.0
116319,503204.0,After the split.,319302.0
116319,503172.0,"You should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points. Don't forget that testing data points represent real-world data. For more details, please refer this link: https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data",311117.0
116319,503288.0,"19 You first need to split the data into training and test set (validation set might also be required). Testing data points represent real-world data. Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables (i.e. the mean and variance). Therefore, you should perform feature normalisation over the training data. Then perform normalisation on testing instances as well, but this time using the mean and variance of training explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points. Please refer the below link: https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data",314183.0
116343,503207.0,I think it is wrong. As per the graphs only specific options are correct. But the actual answers are different. Request TA to please clarify the same.,310522.0
116343,503430.0,"Hi Preeti, as per the TA, the question is correct, but the answers provided may not be correct. please check response from Paras (TA) in the other thread.",319876.0
116346,503094.0,Hi Kaushiki Please find below the TA approved link on the same subject. https://learn.upgrad.com/v/course/208/question/108272,301121.0
116346,503079.0,"Standardscaler: Assumes that data has normally distributed features and will scale them to zero mean and 1 standard deviation.Then all the features will be of same scale . Minmaxscaler :This shrinks your data within the range of -1 to 1(if there are negative values).This is used when distribution is not Guassian, and it is good if standard deviation is small. This is very sensitive to outliers. For more info. check this links below https://datascience.stackexchange.com/questions/43972/when-should-i-use-standardscaler-and-when-minmaxscaler http://rajeshmahajan.com/standard-scaler-v-min-max-scaler-machine-learning/",300733.0
116286,502811.0,"You use k-fold when you have less dataset and you spliting the data between train, validation and test is going to result in compromising the model output due to lack of sufficient information to buidl the right model",317514.0
116286,502814.0,K-fold cross validation is primarily used when we have less training data. Since the limited data available is split into different sets and muliple models are trained it resuls in a more generalizable model. As such the chance of overfitting is reduced as well,318334.0
116286,502991.0,"Essentially we are test data hungry There are 2 reasons why K-Fold cross-validation is used. When we do not have a luxury of having a very large amount of data so that we end up with ha fairly good split of train and test data K-Fold will also make sure that every row of data gets tested at one time or other as they would get their turn as ""test data"" at least in one of the total number of folds chosen. No data would escape wihout holding its badge as ""Test Data""",301121.0
116286,502820.0,"Hi Neha, Cross Validation is used to assess the predictive performance of the models and and to judge how they perform outside the sample to a new data set also known as test data The motivation to use cross validation techniques is that when we fit a model, we are fitting it to a training dataset. Without cross validation we only have information on how does our model perform to our in-sample data. Ideally we would like to see how does the model perform when we have a new data in terms of accuracy of its predictions You can refer to he below link for detailed approach on K- Fold cross validation. https://machinelearningmastery.com/k-fold-cross-validation/",308673.0
116286,503293.0,This technique involves randomly dividing the dataset into k groups or folds of approximately equal size. The first fold is kept for testing and the model is trained on k-1 folds. The process is repeated K times and each time different fold or a different group of data points are used for validation. Advantages of K fold or 10-fold cross-validation Computation time is reduced as we repeated the process only 10 times when the value of k is 10. Reduced bias Every data points get to be tested exactly once and is used in training k-1 times The variance of the resulting estimate is reduced as k increases,314183.0
116286,503332.0,"When you have very less dataset and you can't split data in train, validate and test data, you can for Cross Validation. Cross validation will create multiple models within train data set and test data is still unseen to model.",318368.0
118632,511978.0,Great! thank you.,319006.0
118632,519989.0,Thank you,314048.0
118632,520580.0,thank you,306012.0
118412,510962.0,"Hi Abhishek Please refer to the TA approved link in discussion forum below, which will clear your doubt. https://learn.upgrad.com/v/course/208/question/106429",301121.0
116368,503203.0,k-fold CV is basically used to split the available data in number of K groups. Please refer the below link to understand better. https://machinelearningmastery.com/k-fold-cross-validation/,310522.0
116368,503239.0,You can also visit below link which has detailed explanation on k-fold cross validation. https://medium.com/datadriveninvestor/k-fold-and-other-cross-validation-techniques-6c03a2563f1e Hope this helps.,317991.0
116368,503294.0,Please refer the below link: https://medium.com/datadriveninvestor/k-fold-and-other-cross-validation-techniques-6c03a2563f1e,314183.0
116046,501908.0,The catch here is we are not finalizing the model using cross validation. We are trying to find the best value for our hyper parameters so that the model cam be built using that.,310974.0
116046,501989.0,"The key objective is to build optimal model. Number of features is the hyperparameter which we are optimizing in this case. Using k-fold you are splitting the data into train and test sets by sampling the train set. You get different values of R2 score for the same set of train set. If you are randomly sampling the data, you might not get the maximum value always. Hence you go with average value to compare model performance with different feature set of list. You are trying to find for what number of features, your model will be optimal and not overfit. Hope this diagram helps.",317514.0
116046,502184.0,"I would like to add to other answers. My point for max score is - why take average of all models created for each hyperparameter? Why not take the best model instead by choosing the max score and compare the same instead? why take average of all models created for each hyperparameter?- We are not taking average for each hyperparameter. Suppose you specify two values for hyperparameter, C=1 and C=100. Considering, you select 5 fold cross-validation, Total of 10 models will be made. 5 for C=1 and 5 for C=100. Average test accuracy of 5 models for C=1 will be compared with Average test accuraccy of 5 models for C=100. If average accuracy for C=100 is more, you select C=100 as a hyperparameter. When you find the best hyperparameters using cross-validation, you will retrain a new model on the whole train data. And this model will have hyperparameters you selected from cross-validation. Using this model, the one created on whole training data, you make predictions on an unseen test set. Cross-validation is not used to select a model.It is used for hyperparameter tuning and to assess your expectations on test set, when you create a new model on whole data.",301652.0
119458,517376.0,"Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=5 becoming 5 -fold cross-validation. Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model. It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split. The general procedure is as follows: Shuffle the dataset randomly. Split the dataset into k groups For each unique group: Take the group as a hold out or test data set Take the remaining groups as a training data set Fit a model on the training set and evaluate it on the test set Retain the evaluation score and discard the model Summarize the skill of the model using the sample of model evaluation scores Please refe below link for more details - https://machinelearningmastery.com/k-fold-cross-validation/",300691.0
95417,403556.0,you can directly import and then can remove the * symbol in tableau. you will have to remove it otherwise you'll get wrong answer.,302738.0
95417,403571.0,* will be there as a special characters which will result in null while loading the data so before moving to sheet first split the column then you will get numerical value of the column and then you can do all the manipulations . hope this helps,318017.0
95417,403694.0,You can load it as is and deal with the * symbol by splitting over it,311160.0
95417,403784.0,"You can use tableau to clean the data. The data type of run is a string. you can see a few places run shows like 104* and when you change that to number it will be null. First, you have to split the column on * and then change the data type to number. Hope that helps",317845.0
95417,403825.0,Dont remove the * symbol as you are going to need it future questions.,315560.0
95417,404157.0,You can directly import the data set in Tableau. Afterward you ;ve to remove (*) from your data set. Otherwise you will get wrong result.,320689.0
95417,403924.0,"Yes. Reason: Without this the Run datatype is discrete datatype(String) /Dimession. As the question indicates that we new avg,min or max for which we need an continous datatype like numbers or decimal. Use the split option in the tableau or remove the * in using any notepad editors.",318476.0
95471,403821.0,"Yes , you need to change the Data type of runs , by splitting it.",315560.0
95471,403953.0,You can use Transform -&gt;custom split by right clicking on the Runs in dimentions so as to extract only the runs valus which can be taken as measure for further calculation.,311729.0
95471,403692.0,"After loading the Data, you can right click the column and make use of 'custom split' option.",311160.0
95471,403671.0,"Hello Venkata, Please refer below links. https://learn.upgrad.com/v/course/208/question/95417 https://learn.upgrad.com/v/course/208/question/95337 Ankur has suggested below Work around in above links: In Tableau, select Analysis &gt; Create Calculated Field. In the Calculation Editor that opens, do the following: Enter a name for the calculated field. In this example, the field is called, Discount Ratio. Enter a formula. This example uses the following formula: IIF([Sales] !=0, [Discount]/[Sales],0) This formula checks if sales is not equal to zero. If true, it returns the discount ratio (Discount/Sales); if false, it returns zero. sam you have to do for batting average",320195.0
95471,403783.0,"The data type of run is a string. you can see a few places run shows like 104* and when you change that to number it will be null. First, you have to split the column and then change the data type to number. Hope that helps",317845.0
95471,404076.0,"First split the runs so that * is removed, and then change the datatype of that new column from string to int. Hope this will work.",318448.0
95471,404155.0,Use custom split after loading the data,303082.0
95471,404683.0,"Yesp, as all mentioned here, split is the best way to throw away the astericks. But remember that you use this new column in all further calculations.",318007.0
95471,403912.0,"Yes. Reason: Without this the Run datatype is discrete datatype(String) /Dimession. As the question indicates that we new avg,min or max for which we need an continous datatype like numbers or decimal. Use the split option in the tableau or remove the * in using any notepad editors.",318476.0
95489,403823.0,no please check that u have plotted 6s 4s correctly against intervals,317982.0
95489,403827.0,Its a little confusing to read 4s and 6s data but you will undestand it once you plot it right. Try to plot them in a series. You will get the answer.,315560.0
95489,403842.0,I also faced same issue. Do below things :- 1- Split the run column using '*' as separator. 2- use new column to make the bar chart. 3- Use marks pallete to check max 6s and sum of 4s. do not use sum of 6s.,312746.0
95489,403913.0,check for the * in the runs. Remove it. Also check the width of the bin (x-axis) created for histogram. Use sum of 4s and sum of 6s in y-axis.,304319.0
95489,403920.0,No.Please recheck once the bins side and you can use the Label paltes to indicate the 4s and 6s aginst the avg SR,318476.0
95489,403833.0,add the 6S and 4S column to the histogram and compare and check in which run bracket he has hit the maximum number of sixes and then report back the corresponding number of fours in that bracket.,306738.0
95489,404104.0,"No, the values were coming exactly the same, use the new runs split column after removing the * from given runs column.",318448.0
95489,404286.0,"Check the size of the bin, after you have the histogram and set the size you need.",312376.0
95489,404682.0,"Assuming you have bin for 10 interval, add text display on histogram for 6s and 4s. You should be able to find the answer.",318007.0
95489,405375.0,weird the way Tableau auto creates bins. On the graph it show the interval as 10 but it is adding additional interval. This is creating ruckus. Once i figured this out got the answers.,312199.0
95489,405578.0,Control the range of values in your bins correctly... Don't let Tableau fix it.,308637.0
95285,403052.0,pls add the question,300735.0
95285,402962.0,I think you missed to add the question :),306725.0
95285,403145.0,Question please?,311004.0
95285,403675.0,Add the question here .,301557.0
95285,404109.0,Which Question?,318448.0
95515,404685.0,"On a bar cart, determine the ground (ground column) on which max time (total time of min cloumn) was spent. Sort the bars in the descending order of their heights (time spent). Now we need to label the average of strite rate (SR).",318007.0
95515,404034.0,"You can also use the rank condition to highlight the top one , then check avg SR corresponding to it",306738.0
95515,404140.0,All you need is the MIN &amp; SR measures along with the Ground dimension. The sum of MIN measure provides you the time spent in the ground. You need to select the ground with the highest minutes spent and its corresponding avg. Strike rate.,316202.0
95515,404874.0,Use mins for the time spent criteria.,300685.0
95515,404000.0,"The ""Mins"" Measure represents the number of minutes spent for a particular innings and the ""Ground"" Dimension represents the ground on which that innings was played. Using these two parameters you can identify the ground in which Kohli has played for the maximum number of minutes(longest time spent). Hope this helps.",313826.0
95515,408203.0,maximum time means sum of all the time he spend the respective ground for all the matches. Then find which ground he spend maximum time and then average strick rate in that ground.,318461.0
95530,404108.0,First split the runs column and remove the * symbol. Now change the datatype from String to Int for the splitted column. Make sure you create a new column for the splitted value of runs.,318448.0
95530,404100.0,There shouldn't be null values in Runs.. You may be seeing null values since you've simple changed the datatype to 'Whole Number' or something of that sort.. You might want to split the column so that; 'Runs' with all numeric values will be present in your dataset and you can compute using that column.. And the original with '*' can be left as it is..,316349.0
95530,404173.0,There would not be any null values. Please do the split carefully and assign the correct data type. Nulls are there in MINS column.,317689.0
95530,404500.0,"There are no Null values in Runs , just look into Excel first, that would help. However, you need to split the Runs columns and then proceed further.Hope this helps. .",315560.0
95530,404684.0,The nulls would have been entered because of datatype issues. Correct them using Tableau split column and you should be good to go.,318007.0
95530,405163.0,"Runs column has * for ""not out"" as a special symbol. because of it Tableau senses this column as Text and placed in Dimensions panel. To perform analysis one needs to have runs in measures section. for this you can slit the runs coulmn. It will create another column with * separated. Use this to analyse. This new runs column will not be having any null values.",318344.0
95530,405270.0,Split the runs columns with * and then change its data type.,317600.0
95845,406179.0,it means quater 2 ans quater 4 if you drag the year you can get it you might have seen hierarchy video in the tutorial its just like that first drag deep to quater and see which year has increased value from q2 to q4. hope this helps,318017.0
95845,406203.0,In this context it means Q2 and Q4..,305129.0
95845,407187.0,"In this context it means Quarter 2 and Quarter 4 of the year. You can resolve it by adding year into calculations, Teableau has ability to provide drill down view of the date ranges. By this you will be able to see quarter wise result which eventually lead you up to the result.",317412.0
95854,406219.0,Take a look at this post. : https://learn.upgrad.com/v/course/208/question/95489,313517.0
95854,406215.0,"You need to make sure your run Bins are not including higher numbers. ie. your run intervals should be (0, 9) (10 - 19) (20 - 29) In the left pane, right click on ""Runs (bin)"" and select Edit. You can select your bin interval appropriately there.",318458.0
95852,406205.0,i guess you can do it multiple ways one by creating a new formula of runs / total innings and one by adding a new column in data with the average,318017.0
95852,406208.0,Take a look at this post: https://learn.upgrad.com/v/course/208/question/95337,313517.0
95852,406294.0,"It would be done by calculating the runs scored against each team / number of innings irrespective of not outs. So, you need to find the runs scored against each team first.",317689.0
95852,407295.0,"Please chaeck the data type of the Runs column first. You should be then taking the runs by splitting this coulmn in Tableau. The question says Batting average is the (number of runs scored)/(the number of innings) i.e., Number of innings against the respective country. Based on this runs avg can be calculated &amp; used for plotting the simple graph using oppstion. Hope this is useful.",310508.0
95866,406280.0,"In this case, you can change the bin width to 11. It would cover the range that you are asking for.",313517.0
95866,406288.0,"0-10 is actually 11 points. So, bin width would be 11.",317689.0
95866,406351.0,You rather use bin width as 10.01 It will cover (0-10) (11-20)..and so on.,318458.0
95866,407306.0,Batting Performance in Q2 in Tableau Q2 Plot a histogram to see where Virat Kohli has scored the most number of times. Changing the bin size in Tableau for better visualization Histograms. Example. Ref: https://learn.upgrad.com/v/course/208/question/95957 The defaults value in the Size of bins filed can be customized accordingly. Thank you.,310508.0
95882,406358.0,Use calculated field to solve this question as SUM(Runs)/Sum(inns) in rows.,318458.0
95882,406360.0,Please take a look at the following posts on a similar query: https://learn.upgrad.com/v/course/208/question/95337 https://learn.upgrad.com/v/course/208/question/95852 The first step is to see if any transformation of the data has to be done after you have imported the data into tableau. Hope this helps.,313826.0
95882,406397.0,"First, you need to split the runs column to remove *. Omitting this step may result in invalid because we will be having some unknown values. Just take the measure as RUNS( average ). It will automatically take it as run/innings (as no of innings = no of times runs are scored).",318344.0
95882,406905.0,"Please chaeck the data type of the Runs column first. You should be taking the runs by splitting this coulmn in Tableau. The question says Batting average is the (number of runs scored)/(the number of innings) i.e., Number of innings against the respective country. Based on this runs avg can be used for plotting the simple graph. Hope this is useful.",310508.0
95536,404137.0,Can you please elaborate your question. Which question are you referring to? Is it Q2?,316202.0
95536,404143.0,it is in under graded qustion i cant ask directly so i asked in this way..!!,305847.0
95536,404297.0,Use histogram session to get your answer. https://learn.upgrad.com/v/course/208/session/15815/segment/79974,310952.0
95536,404348.0,got it already thanks...!!!,305847.0
95550,404195.0,go to the virat_kohli_ODI.csv (above the table) you'll find and arrow. click on it. go to the text file properties and check if the field seperator is set to 'comma' or not.,302738.0
95550,404237.0,I found below link that might be useful in your problem. Have a look into it. https://stackoverflow.com/questions/36319407/cannot-load-simple-csv-file-into-tableau-public-9-3 Hope this will help.,317991.0
95550,405267.0,"You can load the csv file using the ""connect to a text file"" method or convert the original file to Excel format and then load it into Tableau.",313517.0
95564,404335.0,"I can't directly give away the answer or the approach. Here's a hint, use filter along with bar graph to solve this.",310974.0
95564,404343.0,"thanks i got the answer, i used sum function",308495.0
95582,404512.0,U need to calculat ethe sum(runs) / sum( Innings) . This will give you the batting average. But before that u need to split the score because few scores are with start marks *. Once you split score to say score and score-1 and then apply aggregae function on that to calculate Average.,315679.0
95582,404523.0,"Usually in Cricket, whenever a bastman remains notout in an innings the average is calculated differently. E.g.: Match Number Runs Scored 1 50 2 100* 3 100 4 50 5 150* From the above table the batsman is not out in 2 out of 5 matches. Accordingly, the average of the batsman becomes 450/3 = 150 (As he got out only on three ocassions). However, in our 1st graded question to not make the analysis complicated they have asked us to ignore the not outs and calculate the average for all the innings. So, from the above example the average is 450/5 = 90. Hope this helps!",311160.0
95956,406937.0,"Right-click on the bin and click on ""Edit"". Then you will get to change the size of the bins. Alternately, you can also solve this problem by putting a filter on the derived ""Runs"" field where the range of runs is between 90 and 99. Hope this helps.",313826.0
95956,406952.0,"Apparently, we can create the bins with desired ranges first before using them in the Histogram by selecting create bins option in a particular measure. Using them later in histogram gives us the desired bin ranges. Thankyou for the response",315471.0
95608,404634.0,it is asking which one will give a better visual output,318017.0
95608,404872.0,"Here they have specifically stating that, they want you to produce a chart, where in we can see the spread of the runs from the Virat. They are not asking for quantified details.",300685.0
95608,404690.0,The word 'spread' is the clue here. Which graph gives you an overview of how the values are spread? Not the number of values spread but spread of ranges. Cannot answer more than that and you know why :),318007.0
95636,404848.0,runs without * sign which includes all the values..,316349.0
95636,404880.0,i think you already got the answer! cheers,320606.0
95636,404868.0,You need to split the runs column. This you can do once you load the csv file. There you'll see all file representation. Just right click on runs column and do spilt.,300685.0
95636,405701.0,"You need to take the runs( ofcouser once you have performing the split action to remove *) scored and Pos and plot the graph. Then once you have created the chart with average runs scored and the position numbers, opposition can be visualised using colour feature. Hope this helps.",310508.0
95643,405723.0,Question is to identify the appropriate plots which can clearly visualize the spread of runs scored by Virat against various teams. It should show the entire range of scores and their distribution.,316202.0
95643,404908.0,The question is asking us to choose the most appropriate plot through which we can see the spread of scores that Virat has scored against each of the teams.,313826.0
95643,405878.0,Its asking for the best suitable plot with entire range of score distribution,305655.0
95284,403001.0,1. It should be increasing from q2 to q4. Dont consider q3. 2. It would would average calculated in the question 1 i.e average runs 3. Atleast 1 inning means he should have played atleast 1 game in q2 and q4 for the analysis to be done.,317689.0
95284,403433.0,you need to consider only the quarter mentioned in q 2-4 and there u can plot any chart n clearly see the years where the runs are increasing. it is not necessary that there will be match played in all the 3 quarters.may be he has played only twice .,317982.0
95284,403824.0,Only give attention towards Q2 and Q4 and ignore Q3. The increament from Q2 to Q4 will reflect the right answer.,315560.0
95284,403922.0,Take the sum of runs. And consider q2 to q4. It should be increasing even if q3 data is not there but q2 and q4 should be there.,304319.0
95284,405707.0,"Hi Pulkit, Note that in this question Q2 &amp; Q4 are to be checked, Q3 not to be considered. This essentially means "" that he played at least one match in that period"" which is the condition in question. Data should be indicating upward trend as a sign of score improvement. AVG runs from Q-1 is useful here.You can make use of Tableau workspace for splitting Q's accordingly and colour it for better visualisation. Hope this helps you.",310508.0
95288,404484.0,"Find out the ground where maximum time has spent , i.e. you need minutes, ground and Avg. Runs",315560.0
95288,404474.0,Just sort descending. U will get it right.,310501.0
95288,403467.0,"average sr at the ground where he has spent maximum minutes on field. So, find the ground where he has spent maximum minutes and get avg SR for that ground.",317689.0
95288,403022.0,find out in which ground he spent the maximum minutes and then find the average SR,302738.0
95288,403102.0,it means the you have to find the ground on which Kohli has batted for maximum amount of minutes and find the avg SR of his for that particular ground.,300685.0
95288,403972.0,"Find out the ground where maximum time has spent , i.e. you need minutes, ground and Avg. salary",315560.0
95288,404087.0,"Here the expected result went wrong with my first attempt. The feedback i got for the wrong answer is First, you need to plot the relationship between the mins measure and Ground dimension. Then you need to find the average SR. in the ground where he has spent the maximum time. But where is the Ground dimension here ? My plot after drawing a relationship between Mins and Ground showed Dhaka as maximum time spent . And when included the Avg of SR which i got for Dhaka was 82.7 which i picked as the right answer but it says wrong ? Iam surprised. Is his question or answer is wrong ? what am i missing or misunderstanding here ? Can someone comment ?? Thanks.",310501.0
95288,404148.0,1st find the ground where Kholi spent the max times and then find the average SR.,320689.0
95288,404175.0,Has anyone got this right?? Don't want to know the answer just want to know if I am missing something,311857.0
95288,404176.0,Figured it out,311857.0
95288,408197.0,maximum time means sum of all the time he spend the respective ground for all the matches. Then find which ground he spend maximum time and then average strick rate in that ground.,318461.0
95331,403346.0,"Looking at the histogram, it looks like the score ranges have not been put in bins of appropriate size.",313826.0
95331,403917.0,"check on the bin size. The bin which has been created on the x-axis, you can edit it and change the bin size to 10.",304319.0
95337,403534.0,can we create column for batting average using tableau?,320606.0
95337,403345.0,"Since this is a graded question, can only provide a hint on how to approach this problem. When you import the dataset into Tableau, take a closer look at the datatype of each of the columns. The datatype for some of the columns may not be appropriate which you may need to correct/convert/transform before you can proceed. Hope this helps.",313826.0
95337,403382.0,you need a new data/column for it. average = total runs/total no. of innings those runs to be also included in which he was not out. once you have this data you can plot the bar chart.,311686.0
95337,403435.0,you need to handle the data type of a column to generate batting average. Use the formula given in the question i.e. total runs scored / total matches played irrespective of notouts against each team.,317689.0
95337,403672.0,"Check the data type of Runs Scored and convert it to numbers. Also, remove the not out sign *",306738.0
95337,403794.0,Convert the run as the number and create the calculated field(run/innings). Once you have new measure you can plot the chart.,317845.0
95337,403950.0,"You need to create a calculated field as per the fomula provided. Then you need to use this new field and create the plot acordingly. You can use ""Edit in shelf"" by right clicking on selected field in the column shelf to create this new field. After this it would be quiet simple for you. Hope this answers your doubt.",311729.0
95337,404153.0,you need to calculate of batting average = total runs/total no. of innings It includes not out also.You 've to handle (* ) not out in your calculation.Otherwise you get wrong result. Afterward you plot the bar chart.,320689.0
95337,404293.0,This can be get by adding a field avg batting by using formual given and then plot chart and also Handle *.,310952.0
95337,406181.0,"In case you have not observed, Tableau has Split, Transform &amp; convert functionality, once you right click the column in Data Columns pane. You can use that.",318458.0
95733,405386.0,"Hey Parul, Refer this - https://learn.upgrad.com/v/course/208/question/95284 This question has been discussed in detail. Hope you'll find this useful.",302742.0
95733,405419.0,"Basically, there should be an upward graph in the plot between Q2 and Q3.",310974.0
95733,405536.0,Batting average should increase from Q2 to Q4 and Kohli should have played atleast 1 match in that duration.,317689.0
95733,405705.0,"Hi Parul, Note that in this question Q2 &amp; Q4 are to be checked, Q3 not to be considered. This essentially means "" that he played at least one match in that period"" which is the condition in question. Data should be indicating upward trend as a sign of score improvement. AVG runs from Q-1 is useful here.You can make use of Tableau workspace for splitting Q's accordingly and colour it for better visualisation. Hope this helps you.",310508.0
95374,404105.0,"It's dependent on the bin size, check and change it. I also did the same mistake.",318448.0
95374,403916.0,check for the bin size.,304319.0
95374,403944.0,Even I haven't got the same answer. Not sure if the question is correct or there is some misinterpretation.,311729.0
95374,403497.0,"There is a method to change Bin range , look into that in Tablue videos on Table website. Hope this helps.",315560.0
95374,403498.0,"Don't Sweat, I got it by editing the dimension Runs-Split (bin) size. :D",318335.0
95374,405348.0,After changing the bin-size you can drag &amp; drop relevant headers on the Lable option under the Marks pane. This helps in visualizing the outcome in a better way in Tableau.,310508.0
95374,404184.0,Change the bin size,303082.0
96128,408577.0,"Hi For 0 - 10 , the bin size would be (10 - 0) + 1 = 11 similary, we can find out the bin size of the interval 90-99 as (99 - 90) + 1 = 10 Only logic is include the end points of the interval as well in finding the bin size",302750.0
96128,409047.0,"Hi Sharad, Since Tableau is starting the bin creation at the starting point which is ""0"" it will take places upto 10 which makes the size of 1st bin having 11digits. From there on it will then normalize &amp; set it to 10 places per bin for ease of understanding of the user. I have image below for ref: Hope it helps you.",310508.0
96128,408847.0,Please refer https://www.answerminer.com/blog/binning-guide-ideal-histogram.,318340.0
96131,408583.0,"The question asks to "" approximate percentage "" and not percentile. The type of calculation to be used is "" Percent of Total "" and not ""Percentile"". Hope this helps.",313826.0
100265,430941.0,you have to create bins of 10 and put the avg in columns and runs in rows,318017.0
100265,430988.0,Reference line per bin can help.,318328.0
95463,,nan,
95482,404825.0,"You can use two methods to connect to the banking csv file. Either choose ""connect to a text file"" and then choose the csv file in question, or convert the given csv file into Excel format and then load it into Tableau.",313517.0
96047,407793.0,"In scatter plot and tree map, it is not possible to know the quartiles and outliers. That might be the reason",318329.0
96047,407904.0,"box plot gives you the quantitative analysis and distribution of a particular dimension like median , quartile , spread of values in the range so when we talk about quantitative analysis always use boxplot",318017.0
96047,409054.0,"When it comes to graphically depicting groups of numerical data through their quartiles Box plots are most preferred. Box plots have lines extending upward &amp; downward from the boxes i.e., whiskers indicating variability of the data in the upper and lower quartiles. Since it allows to display variation across a wide spectrum, it is rare that box plots will have no whiskers at all. Agreeing to your point that other graphs could be used, here we are expected to choose the most appropriate graph to covey the results through visualization. Hence Box plot AKA box-and-whisker plot is used. Hope this helps.",310508.0
95171,402174.0,if you see campaign-its also a measure variable. no point using two measures plotted against each other.its not the correct one to use. they have asked us to plot against campaign outcome i.e poutcome. try a simple bar chart. the answer is in the question itself- the average balance for the those where the campaign outcome was successful.,309451.0
95171,402556.0,"Use the column ""Poutcome"" as your dimension. Hope it helps :)",305839.0
95171,402801.0,AVG BAL AS MEASURE AND POUTCOME AS COL IN A BARCHART.,315679.0
95171,402823.0,As the need the outcome = success irrespective of the campaign you dont need to use the campaign measure. Plot a simple graph with poutcome in columns and Average balance in Rows,306725.0
95173,402154.0,"To put it simply, join- used when data is from same data source eg: different worksheets in an excel workbook or different tables in the same database. blend- used when data is from different data source. eg: a worksheet and a table from a database can be combined using blend or different tables from different databases.",309451.0
95173,402736.0,"The following image can help you understand the types of joins: to explain further JOIN is used when we are combining data from the same data source, for example worksheets in an Excel file. It involves row joining in multiple ways like inner, outer, left &amp; right. BLENDING is used when are combining data from multiple data sources giving us dense data in one view, for example a spreadsheets in Excel with a table from a database. Its returns an aggregated result after it completes its query on these multiple data sources. Data blending requires a common field within the data source for effective blending. Both are extremely powerful tools for effective data visualization.",310508.0
95173,402847.0,Refer following link for more detail. Use joins whenever they are supported and can be used i.e same database or same workbook of excel but different worksheets. Use blending when using multiple datasets which cannot be joined.i.e. excel and tables etc. https://onlinehelp.tableau.com/current/pro/desktop/en-us/multiple_connections.htm,317689.0
95173,402187.0,"source- Tableau learning plateform 1) The difference between joining and blending data: Joining your data can only be done when the data comes from the same source, for example from two sheet tabs within a single Excel file. If that same information was stored in separate Excel files you would need to do a data blend in Tableau. A blend is always required if the data is stored in two separate ""data sources"" within Tableau. So even if your data is very closely related and exists in two separate files or databases, you will have to do a data blend if you are combining the data in Tableau. When blending data, the first data source used in your view will dictate how your worksheet view in Tableau is built. The secondary (blended) data source will be able to contribute extra information, but will not be able to change the overall structure of the view. The secondary data source's values can be aggregated and applied to the existing view after you have established a ""relationship"" by assigning a variable that both the primary and secondary data sources have in common. 2) When to use data blending: It is generally preferable to avoid data blending when you can combine the two data sources outside of Tableau. If this is not an option, then you must identify at least one common variable shared by the two data sources you want to blend together. When possible, go for a join rather than a blend. If you need to combine two data sources and for whatever reason cannot manage to join the data outside of Tableau, your only option is a data blend. A simple example is having (a) a data source with three columns including location names and latitude/longitude values, and (b) a data source with location names and detailed information about each location. You could build a map using (a) and then blend in extra supplemental information using (b), where a relationship is built by connecting the data sources based on the location names. 3) When to use joining: You can only use joining when your data comes form the same underlying source (for example, the same Excel file or Access file). 4) When are you unable to blend data from two or more sources? If there are no variables shared between each data source then you will not be able to do a data blend, because there is no information that can be related from one source to the other. However, this does not mean that the column headers (variable names) need to be an exact match. You can edit the relationships manually to point Tableau to the variables that have matching underlying values. For example, if I am blending information together based on countries and source (a) calls it ""Country"" while source (b) calls it ""Locations"", I can edit a relationship manually to blend on these two variables. If the two column headers are an exact match, Tableau may automatically establish the link for you. 4) When are you unable to join data? If the data comes from different underlying files you will not be able to do a join within Tableau. I recommend preparing your data before importing it into Tableau (there are many great tools available, one being Alteryx, that can help with this). In my opinion blending and joining in Tableau should be a last resort for times when you are unable to shape your data into one coherent file for analysis. I hope this helps. Best of luck!",318802.0
95186,402261.0,yes u need average of salary in the month of July. Tableau has an internal calculation option for Percent Difference which can be used in this scenario.,303670.0
95186,402586.0,,318005.0
95186,402830.0,Make a Bar Chart using only Salary and month and do quick calculation on avg. salary.,315560.0
95186,403165.0,Try doing the below and calculate manually to understand the formula:,310974.0
95187,402247.0,"You can see the month of May in the column ""Month"" from the banking CSV. Just take an average of salary for all the months in a pie chart and you will get the information.",303670.0
95187,402589.0,"the question description has changed ,",318005.0
95187,402592.0,,318005.0
95187,402825.0,Consider the montn and Salary columns you can plot a pie chart as below:,306725.0
95987,407490.0,"Analyze the data source, there is a pOutcome column that shows ""success"", ""failure"", ""Unknown"", ""Other"".",311803.0
95987,407272.0,The question means what is the average balance when the campaign outcome is equal to success.,311254.0
95987,407293.0,WHat does successful campaign output mean? Is it that the 'Response' is Yes?,314799.0
95987,407296.0,if I remember correctly there is a pOutcome column. please check it.,319898.0
95264,402800.0,"Blend when different data source, but here they are telling data in three tables (assuming same source). So join will be the better choice.",315679.0
95264,402837.0,Blending is done from different data sources and cross database joins are not supported. In this scenario joining is correct. Refer following for more info. https://onlinehelp.tableau.com/current/pro/desktop/en-us/multiple_connections.htm,317689.0
95264,402784.0,"The answer is correct it should be Join . Joining is where you are able to combine data from the same data source. Blending is where you able to combine data from different data sources, for example, a worksheet in Excel with a table from a database.",301648.0
95281,402934.0,This is calculating difference of avg value from previous month for month of July and calculating percentage of it. (Avg(july) - avg(june))/avg(june) * 100,317689.0
95281,403164.0,Try doing the below and calculate manually to understand the formula:,310974.0
95348,403381.0,avg salary in July and overall avg salary I think. requesting someone to comment/verify.,311686.0
95348,403545.0,thank you,320606.0
95348,403469.0,Refer to attached screenshot,317689.0
95354,403386.0,If you inspect the entries under 'Dimensions' you can find 'Month'. Use this and 'Salary' from 'Measures' and make use of Tableau's 'Show Me' functionality to arrive at your result.,319898.0
95354,403383.0,Column name will be Day. Then you can apply a filter over Day column and get the required result.,301648.0
95354,404445.0,"According to the data dictionary, the month column also denotes the last contacted month. Hence you can use that as a column.",313517.0
95368,403573.0,"Hey Aravind, I guess you're talking about the advantages of Tree-maps over Bar-Charts. If yes, then you need to give an example of a scenario where it would be more suitable for you to use Tree-maps instead of a Bar-Chart. Treemaps can work well if your data falls into this scenario: You want to visualize a part-to-whole relationship amongst a large number of categories. Precise comparisons between categories is not important. The data is hierarchical.",302742.0
95368,403587.0,Thanks Rashmi. Later I got to know after I submit. :(,304026.0
95368,403611.0,Don't worry. It's not a graded question :),302742.0
128741,562097.0,"Jobs are assigned by the client node to the master node , the client node is not a part of the cluster. The master node then distributes the tasks among its slaves.",318730.0
128741,562205.0,client node is not present in the hadoop cluster. where as master and slave nodes are in cluster. Client node submits request/jobs to hadoop cluster where in we have master and slave nodes. then master node will take up the job and assign it to the slave nodes.,300733.0
128741,562236.0,Client node are not part of the Hadoop cluster .Consider it as a thin cilent to connect / submit the job to you Hadoop cluatets. In Hadoop world client node is also reffered as edge nodes .,318476.0
128741,562164.0,"Client Node are the system which are used by user, in this user request diffrent types of jobs or process like user/worker may needs various information like number of passengers travell today or dalay time of each flight so user request this kind of query from their system which we call as client Node. Client node when submit this kind jobs it goes to Master Node( which is again machine) it contains the histroy of all slave node(slave node are all node or system where distributed information or data is stored) .Therefore one master node can have information regarding many slave node and instruct the slave node to provide the information that is requested by user or client node. Once all slave compute each information and then passthe information to Master Node , here master node consolidates the information from each slave node and finally pass the result to client node , which is ultimately used by user.",307843.0
128741,563300.0,"Slave nodes are worker nodes in a Hadoop cluster where -- 1) Physical Data are stored in blocks 2) MapReduce programs are executed In your Hadoop cluster you would typically have plenty of such slave nodes, 1 Primary/Active NameNode and eventually 1 Secondary NameNode and a Standby NameNode. Apart, from the above you would typically have one more node running the ResourceManager. Now, how would you interact with the cluster? For e.g. you need to access a large file (bigfile.txt) stored in the Data (Slave) nodes -- you would need to issue a command for that from a client machine which can be your laptop as well. So you can just get your laptop connected to the same n/w, login to any of the nodes in the hadoop cluster and invoke a command like below -- $ hadoop fs -cat /usr/mydata/bigfile.txt where /usr/mydata is a directory on HDFS where the file was located. You can even have a linux VM loaded in your laptop where you can install the Hadoop distribution and use that as a client.. so the client Node need not necessaruly be a member of the cluster.",301116.0
128272,560180.0,Standby node makes a snapshot of name node which is the primary ( meta data )node available and if there is a failure standby node acts as the name node,320251.0
128272,560168.0,"Secondary node takes snapshot from name node periodically and copies same in standby node. So, Secondary node is becoming replica of name node after periodic interval. In case name node fails, standy node will act as name node with out any outage.",311404.0
128272,560376.0,"Standby node also takes snapshots of the metadata in Name node at regular intervals like the Secondary node. Basis this, the Standby node will take over from the Name node when the Name node fails and there won't be any downtime. In case Standby node also fails, the metadata from Secondary node will be downloaded in the new Name node and the downtime will be atleast 1 hour ( as told in the lectures).",312063.0
128272,561038.0,"Typically, the Active NameNode and the Standby NameNode share a NFS mount in their local filesystems where the metadata exist and shared by the two. In case of failover (auto or manual) they just switch roles",301116.0
128272,559995.0,"Backup Node in hadoop is an extended checkpoint node that performs checkpointing and also supports online streaming of file system edits. The advantage over the checkpoint node is that the namespace (meta-data) present in it’s main memory is always in sync with primary namenode file system namespace, Since it maintains an in-memory, up-to-date copy of file system namespace and accepts a real time online stream of file system edits and applies these edits on its own copy of namespace in its main memory. Thus, at any point of time, it maintains a latest backup of current file system namespace. In Secondary Namenode and Checkpoint Node, checkpoints are created on their local files systems by downloading fsimage and edits log files from active primary namenode and merges these two files and new fsimage copy is saved on their local file systems. But unlike Secondary NameNode or Checkpoint Node, the Backup node does not need to download fsimage and edits files from the active NameNode to create a checkpoint, as it already has an up-to-date state of the namespace in it’s own main memory. So, creating checkpoint in backup node is just saving a copy of file system meta-data (namespace) from main-memory to its local files system. So, obviously checkpoint creation in backup node will always be faster than that of in secondary namenode or checkpoint node. As the Backup node keeps a copy of the namespace in main memory similar to NameNode, its main memory (hardware) specifications should be same as the NameNode. Unlike Checkpoint nodes, there is only one Backup node is allowed to be registered with namenode at any time but multiple checkpoint nodes registration is possible. if a Backup node is in use, then there might not be need for checkpoint nodes and these may not be required to register with namenode.",318476.0
128272,559843.0,"The Backup node provides the same checkpointing functionality as the Checkpoint node(secondary node),and is always synchronized with the active NameNode. Hence it has the metadata of data blocks as in name node and can take over the name node activities immediately in case of name node failure. Please refer below link for more details https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Backup_Node",310467.0
130511,570259.0,"Hi, According to my understanding of the question, you want to know how the data storage is managed in Hadoop. Let's walk through the same: As we know that Hadoop maintains data(metadata) about the data it has stored on its system. To accomplish this there are mainly two files: EditLog FsImage EditLog EditLog is used to persistently record every change that occurs to file system metadata. For example, creating a new file in HDFS causes the NameNode to insert a record into the EditLog indicating this. Similarly, changing the replication factor of a file causes a new record to be inserted into the EditLog. The edit log file is stored in the local host file system of the namenode FsImage The entire mapping of blocks to files and file system properties, is stored in a file called the FsImage. The FsImage is also stored as a file in the NameNode’s local file system. Let's see how the two files are used by the NameNode &amp; how NameNode works with DataNode to achieve operations in Hadoop File System: NameNode The NameNode keeps the entire mapping of blocks in its memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint. DataNode The DataNode stores HDFS data in files in its local file system. The DataNode has no knowledge about HDFS files. It stores each block of HDFS data in a separate file in its local file system. The DataNode does not create all files in the same directory. Instead, it uses a heuristic to determine the optimal number of files per directory and creates subdirectories appropriately. It is not optimal to create all local files in the same directory because the local file system might not be able to efficiently support a huge number of files in a single directory. When a DataNode starts up, it scans through its local file system, generates a list of all HDFS data blocks that correspond to each of these local files and sends this report to the NameNode: this is the Blockreport. For more details, I would suggest to read this: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html Hope this helps!",318355.0
128839,562644.0,There is data block replica in each rack also on the different rack therefore if one data block fails then the nearest data block i.e copy of data node in the same rack can be used and in worst case if the the entire rack fails then the data block copy of different rack can be used. This way the data node is restored in short by creating a replica generraly as per lecture we create 3 replica ofeach data node.,307843.0
128839,562801.0,We have replication factor which tell how many this a duplicate copy of the same data block is maintain in the Hadoop cluster . Example if we have replication factor of 3 we have 2 copy and one original copy of the data . If one of the data node fails the name node will provide the deatils of other data node where the data is replicated and nearest to first node may be same rack or other rack.,318476.0
128839,563348.0,"To avoid this problem of data loss, Hadoop has a concept of a block replica. When the data is being divided into blocks for storage on the HDFS, instead of storing a block of data on just one node, each block is replicated a specific number of times . This way, more than one block replica is then stored on the HDFS on multiple nodes.",314183.0
127985,558507.0,"no, only data nodes do. name node only store metadata",318360.0
127985,558511.0,"Master node a.k.a Name node provide the address of the Data nodes . The first datanode will copy the block to another datanode, which intern copy it to the third datanode. Once it creates the replicas of blocks, it sends back the acknowledgment to Master Node . So Master node does not save the Block of data on it .",311861.0
127985,558550.0,"No master node does not save any blocks of data. It only stores metadata ( i.e. which block of data is stored in which data node, is data node up and running or not etc.). Below is the overview of working of Name(Master) and data node Hope this will help.",317991.0
127985,558822.0,"no, master nodes are name,secondary,standby nodes data nodes are slave",318005.0
128051,559211.0,"There are multiple slave nodes. So, few of them can exist in same rack where as few of them would be in different racks. When you talk about the googles and facebooks of today, the size of the cluster they have can't be fitted in 1 rack and mostly span across multiple datacenters.",317689.0
128051,558932.0,Master and Slave node exist in the same rack but in different nodes. Yes can exist in different rack but that might create latency issuse as every item a client need to get the meta data from name node and then reach out to Slave node to read the block .After getting the meta data information from name node client won't interact with the name node to fetch the data block it's happen directly from the data node,318476.0
128051,561567.0,They may or may not exist on same rack. They can perform on different locations also,301643.0
129100,563879.0,In Hadoop Standby node is always synchronized with active name node which means that standby node is upto date with active name node. On failure of name node standby node is ready to take over which means it is almost uninterrupted.,301121.0
129100,563965.0,"The active NameNode is responsible for updating the EditLogs (metadata information) present in the JournalNodes. The StandbyNode reads the changes made to the EditLogs in the JournalNode and applies it to its own namespace in a constant manner. During failover, the StandbyNode makes sure that it has updated its meta data information from the JournalNodes before becoming the new Active NameNode. This makes the current namespace state synchronized with the state before failover. The IP Addresses of both the NameNodes are available to all the DataNodes and they send their heartbeats and block location information to both the NameNode. This provides a fast failover (less down time) as the StandbyNode has an updated information about the block location in the cluster. https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/",307843.0
129505,565752.0,Rack is a collection of commodity Machine or nodes. Data Centres are physical locations where multiple racks are stored together. Cluster is a collection of data centres. So going by the above definitions we see that rack in a data centre is a part of the cluster,311254.0
128624,563537.0,"The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file. The NameNode responds the successful requests by returning a list of relevant Data nodes servers where the data lives. The NameNode is a Single point failure for the HDFS Cluster. HDFS is not currently a High Availability system. When the NameNode goes down, the file system goes offline. There is an optional Secondary Name node that can be hosted on a separate machine. It only creates checkpoints of the namespace by merging the edits file into the fsimage file and does not provide any real redundancy.",314183.0
128624,562985.0,Yes it doesn't hold any actual data but information about data. Consider it like a index page in books.,311855.0
128624,561994.0,Name node contains meta data information on how to access access the data/bolck stored in different data node .It check the heart beat of the data node and in case of a data node failture redirects the client to the data node contains duplicated data.,318476.0
128624,561594.0,That is correct. Name node does not hold any data but data nodes have all the data.,301121.0
128624,561660.0,Yes Name Node have only the metadata not actual data. actual data hold by data nodes.,310952.0
128623,561593.0,Standby node takes the place of Name node. Secondary node has the snapshot of Name node. After the failure of active name node it starts taking the snapshot of new active name node which was the standby node.,301121.0
128623,561871.0,"After name node failure, stand by node takes place of it immidiately. And if standby also fails then secondary node takes place of it. Please, note that secondary node takes time to take place of standby node and hence user has to face a little delay in services at first. If all of these three fail then entire system fails.",315560.0
128900,562981.0,Please refer to the TA verified answer https://learn.upgrad.com/v/course/208/question/128839,301121.0
128900,563170.0,it creates multiple replica of data nodes. so if a data node failed still we can fetch the data from its replica.,301648.0
128900,563372.0,The blocks will be under replicated the system begins replicating the blocks that were stored on the dead datanode . The NameNode Orchestrates the replication of data blocks from one datanode to another. The replication data transfer happens directly between datanodes and the data never passes through the namenode. Please refer the below link: https://www.edureka.co/community/9633/how-namenode-handles-data-node-failures,314183.0
128900,563350.0,"To resolve issue of data node failure - replication factor comes into picture. As part of this, same data is replicated into different data node (based on replication factor) so that if one fails, data will be picked from another node. To save time/latency to pick data - priority for storing data is in below order:- 1) one in same Node as original data node 2) 2nd in same rack 3) 3rd in another rack",320103.0
128146,,nan,
128648,561910.0,"Hi Bhargav , You can use the command : hdfs dfsadmin -report to check if nodes are active The Hadoop architecture relies on a failover mechanism to elect the Standby Node. The failover can be Manual or Automatic. This is done using Apache Zookeeper. Refer to below link for more details. https://techvidvan.com/tutorials/automatic-failover-in-hadoop/",308673.0
128648,561998.0,Answer is zookeeper. Apache ZooKeeper is an effort to develop and maintain an open-source server which enables highly reliable distributed coordination. https://zookeeper.apache.org/,318476.0
128405,560392.0,"Secondary Namenode's whole purpose is to have a checkpoint in HDFS. Its just a helper node for namenode. That’s why it also known as checkpoint node as it copy or image of namenode of regular interval of time But, It cant replace namenode on namenode's failure.So, Namenode still is Single-Point-of-Failure. To overcome this issue; STANDBY-NAMENODE comes into picture. It does three things: merging fsimage and edits-log files. (Secondary-namenode's work) receive online updates of the file system meta-data, apply them to its memory state and persist them on disks just like the name-node does. Thus at any time the Backup node contains an up-to-date image of the namespace both in memory and on local disk(s). Cluster will switch over to the new name-node (this standby-node) if the active namenode dies. https://stackoverflow.com/questions/44540533/difference-between-secondary-name-node-and-standby-name-node-in-hadoop/44540889",307843.0
128405,560952.0,"Just in case both Active &amp; Standby Namenodes are unavailable, we would need the Secondary NN to build a new Primary NN using the checkpoint data (typically a 'fsimage' file) from the former",301116.0
128405,560880.0,"HI Vinod Here comes the explanation in four parts When a namenode fails, standby node takes charge of name node as it is replica of name node; In fact it can carryout all activities seamlessly as though original name node did not fail at all. At the Same time secondary node would have already taken the snap shot of original name node as it is taking continuously at regular intervals. From now onwards secondary node will start taking the snap shot of presently running active name node (previously standby node) as this will be give the latest status. The most important point is Secondary node is very useful to rebuild a Namenode from scratch using the snapshot. Point #3 would have cleared your doubts. Just to let you know even in real time control systems, this principle is being used widely.",301121.0
128869,562812.0,https://stackoverflow.com/questions/44540533/difference-between-secondary-name-node-and-standby-name-node-in-hadoop,307843.0
128869,564169.0,"There are MAJOR differences between Secondary NameNode and Standby NameNode -- * A secondary NN attempts to maintain a latest image of the filesystem metadata (the fsimage) by fetching the in-memory edits log from the primary NN and applying that on the last fon-disk simage file. It keeps on doing that periodically to serve as a backup. So at any point of time, if the primary NN goes off, we can have the last checkpointed fsimage from secondary NN and rebuild a new primary with that. And that involves downtime * A standby NN, on the other hand, shares the same edits directory (typically a NFS mount point) as the primary NN so both the primary &amp; standby has the same fsimage at any point in time. The data nodes, in case of active-standby configuration, send the block report to both the namenodes simultaneously so that they are always up-to-date and independently available",301116.0
128869,563360.0,"The secondary node takes regular snapshots of the metadata stored on the name node. In the event of a name node failure, you can copy the snapshots to a new name node, but this results in some downtime. There's also a standby node, which takes over as the name node almost immediately after failure and thus, results in almost a zero downtime. Please refer the below links: https://stackoverflow.com/questions/44540533/difference-between-secondary-name-node-and-standby-name-node-in-hadoop http://hadooptutorial.info/backup-node-in-hadoop/",314183.0
128869,562834.0,"Secondary namenode is just a helper for Namenode. It gets the edit logs from the namenode in regular intervals and applies to fsimage. Once it has new fsimage, it copies back to namenode. Namenode will use this fsimage for the next restart, which will reduce the startup time. Secondary Namenode's whole purpose is to have a checkpoint in HDFS. Its just a helper node for namenode. That’s why it also known as checkpoint node. But, It cant replace namenode on namenode's failure. So, Namenode still is Single-Point-of-Failure. To overcome this issue; STANDBY-NAMENODE comes into picture. It does three things: merging fsimage and edits-log files. (Secondary-namenode's work) receive online updates of the file system meta-data, apply them to its memory state and persist them on disks just like the name-node does. Thus at any time the Backup node contains an up-to-date image of the namespace both in memory and on local disk(s). Cluster will switch over to the new name-node (this standby-node) if the active namenode dies. reference: https://stackoverflow.com/questions/44540533/difference-between-secondary-name-node-and-standby-name-node-in-hadoop https://stackoverflow.com/questions/44540533/difference-between-secondary-name-node-and-standby-name-node-in-hadoop/44540889 :",317822.0
128726,562059.0,"Hi Koustav, The Secondary Node only stores the snapshot of the data as done by Name node It acts as a check point. The Standyby Node acts as total replcia of the Name Node and thus is responsible for further processing of the job. Alos, Standby Node and Secondary Node come into picture when the Name Node fails.",308673.0
128726,562227.0,As per question we have only one name node and 5 data node .So we don't have any secondary or stand by node so the answer seems to be correct . But it never happens in real world production senario. we also going to have a stand by node to autoswitch over .,318476.0
128313,603534.0,that is the key,300694.0
128313,560205.0,"Column id is the key ,so the entries which you are counting is denoted by 1 though its a duplicate,",301115.0
128313,560016.0,"Hi Utkarsh, If you see the further explanation in video, it is mentioned that the mapper job was designed to calculate no of times airline name appeared for each column. So, in the example each column had the airline name only once and thus it was marked as 1 for each column which will further go to reducer phase for aggregation.",320103.0
128313,561367.0,"Just think of counting votes manually from ballots received from a booth where there are 5 candidates -- typically someone would pick up each paper and say like -- ""candiate 1 -- 1 vote"", ""candidate 3 -- 1 vote"", ""candidate 1 -- 1 vote"", ""candidate 5 -- 1 vote"" and so on... this is the Map phase. At the end someone would sum up all the 1's for each candidate to decide the winner.. which is Reduce. You can imagine each booth under a constituency as a DataNode in a cluster.",301116.0
128332,560147.0,"A Task is divided into subtasks and these subtasks are known as Jobs. For example, a Task could be: To find and calculate the sum of the values in a particular column Substasks or Jobs would then be to calculate the sum of that particular column in a particular block. So, when combined, these Jobs it will give you your Task . Hope that helps.",317998.0
128332,560160.0,"Job is a set of tasks and task is subsystem of job.Job is what is needed to be done.Task is how it is done. In Hadoop, “MapReduce Job” splits the input dataset into independent chunks which are processed by the “Map Tasks” in a completely parallel manner. Hadoop framework sorts the output of the map, which are then input to the reduce tasks.",320103.0
128332,560187.0,"A job is the one that is given by client to Hadoop for execution; A job is subdivided into many tasks. Application manager takes care of each of entire task throughout its life cycle (that is till execution of task is completed.) To perform each of these tasks you require containers like CPU, RAM etc.",301121.0
128332,564140.0,"MapReduce is the data processing layer of Hadoop. It is the framework for writing applications that process the vast amount of data stored in the HDFS. In Hadoop, Job is divided into multiple small parts known as Task. In Hadoop, “MapReduce Job” splits the input dataset into independent chunks which are processed by the “Map Tasks” in a completely parallel manner. Hadoop framework sorts the output of the map, which are then input to the reduce tasks. Both the input and output of the job is stored in a filesystem. Hadoop framework deals with task scheduling, monitoring, and re-executing the failed tasks.",314183.0
128370,603527.0,it means that the requesting client code is moved to the appropriate node,300694.0
128370,560277.0,"It means Code only in the above page. When client submits some job to Master node, in that scenario code is sent and as per principle of data locality - code is transferred to location of data rather than vice versa. The reason of code sending to data is that - code is in some KB but data is in GB, so sending code to place where data resides is less expensive.",320103.0
128513,561073.0,"The ApplicationMaster is an instance of a framework-specific library that negotiates resources from the ResourceManager and works with the NodeManager to execute and monitor the granted resources (bundled as containers) for a given application. An application can be a process or set of processes, a service, or a description of work. The ApplicationMaster is run in a container like any other application. The ApplicationsManager, part of the ResourceManager, negotiates for the container in which an application’s ApplicationMaster runs when the application is scheduled by the YarnScheduler. While an application is running, the ApplicationMaster manages the following: Application lifecycle Dynamic adjustments to resource consumption Execution flow Faults Providing status and metrics Ref : https://mapr.com/docs/60/MapROverview/c_application_master.html",318368.0
128513,561449.0,Yes you are correct once all the task under the application master is done it's resource or container used is reclaimed for other jobs in the quequ.,318476.0
129061,563892.0,"1) If resources are not available on a node. how is the next node identified by the Scheduler? is it that node which has the data block replica, and the next best alternative for processing efficiently? It is possible that a node is executing several tasks with equal number of Application masters and available resources could be insufficient for this task in that node. In such a case it will attempt on another node with more resources where data replica is present by transferring the code there. 2) Why does the application master need to negotiate with application manager for resources? why it cannot do the same with the node manager, as its the node manager which is launching it? Application manager is at the master node and application master is in the slave node. From the master node it is clearer to find out resource capability of different nodes. and hence Application Manager is in a better position to allocate the appropriate node.",301121.0
128145,564028.0,for me as well - mapreduce video is not working - I Tried on android app with downloading offline video (downloaded successfully - but still did not play). And I also tried playing in on my chrome browser for android tablet - still getting error (There it says error number 3). Cannot proceed. will have to try with win laptop - but shouldn't have to do this. All other videos in Big data worked so far except MapReduce.,300694.0
128145,559743.0,Its not happening for me. Which version of iPad you are using...?what is the software version ...?,311861.0
128646,603531.0,the file is stored in multiple nodes and blocks,300694.0
128646,561984.0,"You can know your data volume based on daily,weekly,yearly calculation example you web site can have a click stream data of 100 GB per day which makes it 365 * 100 GN per year. Earlier you can have 3 month of data for pertict you customer behavior due to cost of storage and process/ computer needed. Now with Hadoop you can do analysis on your on 1 year or two year of data because as we are using commodity hardware and storage the cost is low .As well to can expand the infra by adding more nodes to the existing cluster but in can of your traditional system you have to go for a hole infra update . Hope it help.",318476.0
128646,562899.0,"My responses are in bold. If we know that we have 500TB, is it stored already somewhere? You started with some data and it has groun to 500 TB and still growing How do I know that it is 500TB? If I am capable to storing 500TB somewhere, will I use hadoop just because I fail processing on a normal machine? It is not just storage but we need to access and process them too with fast response as necessary (like google, facebook etc) If it is streamed data, how would i know how much of data will be formed? Yes. We will not know in the beginning and at least we would have paved way for expansion as well as processing by using HDFS. My basic confusion is, all through the module, we have been told, huge data. How did I know that I have huge data if it is not already on one machine? It need not be in one machine and you can always add up various blocks and find out. We can find out size of blocks on different machine. More than anything else we are able to store in a secured way as well as in safe manner by using redundancy and process them at ease with fast response which would not have been possible using conventional single machine.",301121.0
128646,563283.0,The 500 TB data not necessarily from a single machine. It can be from many sources. Lets say if business use RDBMS for its application. Generally RDBMS deals with transactional data which is required for faster processing. But down the line the data grow after few years where historical data before four years or so not required for your current application. So in that case businesses archive the old data so that the existing applications wouldn't burden due to it. So for later processing and analytics they need the large volumes of historical data. Then they will pull the data from these archives. SO for this need we need hadoop like systems to store and process the data.,314244.0
135386,586250.0,"HI Sandesh Here are the ways; Ensure to import: from pyspark.sql.functions import when, col df.withColumn df_kk = df_kk.withColumn(""Birth_Year"",year(""Birth Date"")) df_kk = df_kk.withColumn('mapped', when(col('Orig') == '5','E').otherwise(col('Orig'))) case when kk_view = spark.sql(""select *, case when age_YY between '1' and '18' then 'Minor')",301121.0
135386,586169.0,You can do that with df.withColumn and when-otherwise.,318762.0
139028,600138.0,"In 2nd query you are not joining the table. You need to join it on a common field, like you did in first query. s.student_id=m.student_id",301560.0
92351,388878.0,Please get in touch with your student mentors for any such clarification on the same.,301618.0
92351,389418.0,"Hey Sachita, It depends whether you have answered the question in 1st attempt or 2nd attempt. Please refer below guidelines. The questions in the session will adhere to the following guidelines: First Attempt Marks Second Attempt Marks Question with 2 Attempts 10 5 Question with 1 Attempt 10 0",302742.0
92351,393638.0,I think there were some mistake in marks now it is corrected.Thanks,314621.0
90243,375961.0,Your code might not be handling all the possible cases here.. cross check it to see if it handles 0 factorial.. you may have to use if condition to handle the 0 case and put your reduce code in else loop.. hope this helps..,316349.0
90243,375965.0,Code execution will be successful if there are no syntax errors. It doesnt mean that logic is correct. Please check the code once again for the logic,318084.0
90243,375969.0,'Run code' just execute your code. But when you click 'verify' sample test cases runs on your code and if your code is correct it accepts otherwise reject. Since your code execution is successful there might be the case that you have hardcoded some input. So check two things: 1. Whether you have hardcoded any values then remove that and try to make code generic enough to pass all test cases. 2. If you have not hardcoded then check the logic of your code whether it is developed as per the question requirement. Hope it helps.,317991.0
90243,375971.0,I figured out. There was a # in the code which was giving this error. Its been accepted now. Thank you people.,310508.0
90243,375968.0,"it means your code is syntactically correct but fails to satisfy the test cases. You might not have considered the the factorial of 0, you have to use an if condition to explicitly check that.",309451.0
90243,375976.0,"Run Code : Executes the code against the input provided in the Input tab. Here, only the syntax is checked and no validation of the logic is done. If there are no errors w.r.t syntax , then it says that the code execution is successful. Verify : Executes the code against sample test case(s). Apart from syntax check, the logic is also evaluated against the sample test case and if successful, you get a message saying that the sample test cases passed, else it says failed and display the solution output and expected output for sample test case(s) so that you can correct the logic accordingly. Submit : Executes the code against sample test case(s) and non-sample test cases(s) (if any). Apart from syntax check, the logic is also evaluated against the sample test case(s) as well as the non-sample test case(s) (if any) and if successful, you get a message saying that the sample test cases passed, else it says failed.",313826.0
91168,380884.0,I guess no as the penalty is for non completion of graded and assignments,317982.0
91168,380895.0,"If you have attempted all the graded questions before the deadline, you won't attract any penalty. :)",306040.0
88646,366970.0,"Hi Arpit, Please check if you have completed all the MCQ's listed for the session.",301648.0
88646,366994.0,I did completed all of it. Please help.,310502.0
88646,367050.0,You must watch the video in that session and complete any non graded MCQs or fill in details. Look for a ! mark on these components which tell that they are incomplete.,310974.0
88646,367081.0,"there are only mcq questions on that page. Double check if you have completed all the segments. There should be 3 segments, namely: 1. Peer Interaction 2. Grading &amp; Timelines 3. Different Commmunication Channels Check if you have a tick mark against each of these 3 segments or not. if all are done and have got a ""tickmark"", then you should go for ""report a mistake"" option present at the bottom left of the page, or go to the support section for help.",317998.0
88646,367974.0,Just now I completed.I didn't face any issues,301120.0
88646,368668.0,"Check if there is green tick in each session, if there is play symbol then you might have missed something or you would have not completed it.",320685.0
90467,377363.0,Please try to perform the operations that you did above without using the for loop so that code can be optimised.,318358.0
90467,378756.0,"The code looks pretty good and it satisfies the problem statement well. But when we want to optimise the computational time and perform parallel operations, this might not be efficient when you have large datasets and you want to flip the words in those. So, there are certain libraries in python, which let us perform parallel operations on vectors. You'll learn them in the later lectures, from here,\ https://learn.upgrad.com/v/course/208/session/19861/segment/101047",319721.0
88625,366984.0,"use filter option in excel. leave bangalore (dont count) as u know thats not the ans count the cities given in the options. no need to draw graphs, takes less time",308437.0
88625,366842.0,"yes, we have multiple name of the same city. Bangalore, Bengaluru, Banglore In that case first we have clean the data make one standard name for Bangalore. Once you clean the data it's easy to group. one more way to do the same using regular expression pattern matching using tableu https://www.tableau.com/about/blog/2015/6/become-regular-regular-expressions-39802",317845.0
88625,366881.0,"If you are trying to find this using excel, you can create a pivot table on the entire dataset with city as rows on the count of total records. For count of total records, you can drag the column age(since none of the values of age are blank) to the values section of the pivot and then change 'value field settings' to display count. Once the pivot table is ready, sort in descending order of count to find the 2nd highest record. Hope this helps.",310511.0
88625,366910.0,"Hi Rajarshi, thanks for the answer, the same can be done using tableau when you plot a bar chart and sort, but as Amani mentioned in his comment we would need to clean the data so that multiple names of city can be combined. If we don't do this, we would see that only one of the city has the second highest number of participants but in actual there can be multiple cities but we won't be able to capture them because of the multiple names.",318397.0
88625,366915.0,"Hi Vipul, we have multiple name of the same city.Bangalore, Bengaluru, Banglore..etc In that case first we need to clean the data make one standard name for Bangalore. I am not able to do becuase getting error. So we need to modify excel file as per the error(Invalid table name value) then uplaod into tableau. I was little bit hurry so did not go for changes. without modification please find below solution. You need to count the value for bangalore.",320195.0
88625,367043.0,You can use the sort and filter using cities given in the option to arrvive at the ans. u can use piviot table to arvive the answer if nou dont want to use option given.,306012.0
88625,367085.0,Can be easily done using filters in excel. Select all the bangalore/bangaluru etc cities. Count is available. Then can find the percentage. For second highest learners. Insert a pivot table with city as rows and count of city as values. Sort the count in descending order.,304319.0
88625,367214.0,Hyd/Pune,314936.0
88625,367221.0,"Hi, it can be done in two ways- 1. we can sort the data on cities column alphabeticaly in descending order and cross verify with the options given in the option. 2.Can apply pivot on Rows- put City and also on values put no of cities. images below. then sort the data alphabeticaly.",305129.0
88625,367401.0,"Well you can go ahead with following steps: 1. Format the city names and bring it to single name. For eg: Bangalore has some 3 to 4 different varianys like Bangalore South, Bengaluru, etc. 2. Once the above process is achieved, then fire a pivot and arrange the students list in descending order. Hope it helps.",318455.0
88625,367714.0,"Hyderabad &amp; Pune is the 2nd Highest city Regards, Manoj Kumar. B",320008.0
88625,367720.0,"Hi, You can find by two ways: 1.) You can use bar charts in Tableau. In bar chart, there will be two catagerical data and one dimensional data. 2.) As well as you can pivot table in excel .",314183.0
88625,367883.0,I used R to solve the problem. read the csv file into r and then used summary to answer few questions. also implemented aggregation by certain columns to get the required result set to answer the questions. gave me a good refresher on some of the R concepts,300708.0
88625,367913.0,Hyderabad/Pune = 57,306244.0
88625,369350.0,Since i didn't see any 'None of these' option in the question.. i just filtered the results and counted it manually.. hardly took 20 secs.. :),316349.0
88625,368762.0,This can be easily achieved through Tableau with the help of Group Members option. Draw a bar chart using City and Number of Records fields. Now select all the cities altogether that represent a single city on a bar chart. e.g as I have done by selecting all the possible Bangalore combinations. Hover over selected cities and click on Group Members option OR right click on selected city and click on Group Members. All the possible combinations will come under single category. Similarly you can do for other possible cases. Happy to help.,318741.0
88625,369528.0,No i got answer while practcing Python code - L[2:] -will return from index 2 . :-) Thanks,320008.0
88683,367249.0,"Hi Sweta, The deadlines will be in IST and could vary based on your local time. For e.g. - If you are in London and following BST (British Summer Time) which is UTC +1 then deadline for you in local time would be 7:29:00 PM BST if the deadline in India is 11:59:00 PM IST. Simlar pattern would follow for modules being unlocked as well. You can go through page 6 of this document for more details: https://cdn.upgrad.com/UpGrad/temp/f39088ba-dbb2-453a-891a-188e22144cc5/Student+Assessment+And+Learning+Experience+Manual_DS+C8.pdf",317998.0
88683,367254.0,Unlocking will happen as per IST Standardized for all. You need to schedule accordingly more importantly for graded assignments as there 30% penalty also provisioned as well.,319869.0
88683,367477.0,Yes module unlocked at 12:00pm IST only. in case you are facing difficulty kindly refer FAQ's or ask your student mentor.,305847.0
88683,367921.0,All content works with IST only. So paln accordingly and finish the assignment before deadline reaches.,304397.0
88683,403812.0,Yeah i check it was unlocked ISt 12.00 am ist but my question why is it weekly now earlier it was forthnight?,300687.0
88590,366719.0,What i used was using pivot chart as below: i.e just using city and count of city.,318495.0
88590,366779.0,"Hello, I did a bit of data Cleaning and arranging before going for the question. There are so many cities which is the same, just spelled incorrectly or different name written. Eg : Bengaluru ---&gt;&gt; Had entries of Bangalore, Bengaluru Area,India , Bengaluru-560102, Bengalure So I replaced all these values with Bengaluru. Similarly For Hyderabad. Steps : 1. Inserted a new column 'DifferentCities' -- &gt; Copied all the values from cities column. 2. Did the cleaning and then Go to Data and Removed the duplicates . So we are left with all unique cities with all the cohort. 3. Create a new column - CountofCohort -- Write a simple COUNTIF Formula in 1st cell =COUNTIF(H2:H805,K2). (Check image for reference). This will find the count of the second argument( EachCity ) in the Range provide( List of all cities ).Now click on the right bottom of the cell, so it will fill all the column with the count value of each of the city. 4. Now insert a PIVOT table with these two columns. Select add to the Data Model option. 5. Now you can choose the two fields in the Range option of Pivot Chart field. Insert any column/ bar chart. Sort the values by countofcohort field in descending order. You will have something like below.",315028.0
88590,366966.0,"Just to summarise what has been explained in detail above. As there are multiple different you'll have to clean the data and convert and varieties of Bangalore in one, say ""Bangalore"". Then create a pivot table and populate the total Bangalore entries. Divide that number by total (806)and calculate the required percentage.",308962.0
88590,366856.0,"Since same city has multiple names first we have clean the data make one standard name. Once you clean the data it's easy to group. use pivot table and you can see the count of city and it's easy to find second largest https://support.office.com/en-us/article/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576 one more way to do the same using regular expression pattern matching using tableau, where you don't have to clean the regex will do the work https://www.tableau.com/about/blog/2015/6/become-regular-regular-expressions-39802",317845.0
88590,367560.0,"Select the CITY Column and copy it and then paste it in a new column. Now select this new column, go to data Menu and click remove duplicate and check my data has headers and click ok. Now all the unique cities will be in column. tHEN CREATE A NEW COLUMN SAY NOS AND NETER 1 FOR FIRST CELL AND THEN DRAG TO BOTTOM RECORD. Then In the Next Column, select cell at row no 2. and enter formual SUMIF(RANGE, CRITERA, SUMRANGE) RANGE WILL BE CITY COLUMN, CRITERIA WILL BE THE CELL IN PREVIOUS COLUMN AT SAME ROW I.E. NEW CITY COLUMN'S CELL HAVING UNIQUE CITY AND SUM RANGE IS NOS COLUMN. Now Drag the formual to the bottom of the column, and the sort this column in largest to smallest. You will get Bangalore 162 Bengaluru 65 Hyderabad 56 Pune 56 and so on Make sum of Bangalore and Bengaluru consider it as one city, then on second place is hyderabad and pune.",317811.0
88715,367486.0,firstly you have to remove calculation in your total means if u get total female is 152 so u can directly calculate 152/806 that will be18.89% near about to 19%.,305847.0
88715,367407.0,Thanks for your answer buddy! Make sense!,301124.0
88715,367402.0,"Just because the value of a particular column is missing, you cannot/should not remove it from the total count. This is because, what you are finding is the certainty of percentage of women in the cohort. ie, you need to find that x% of the cohort are Definitely women. could have been more, if more values were given. so, you may consider other missing values as 0 or null but, you cannot remove it from the entire sample space, ie total count, since they are also there in the cohort. (just because a parameter is not known..would not mean/should not be assumed that they are not there) Hope it helps.",317998.0
88715,367408.0,"Missing values too are the part of your dataset, so you need to use 806 rows. Going forward when Machine Learning will be introduced, you'll need to replace missing values with either '0's, means, etc as a part of your data cleaning process.",306248.0
88715,367453.0,"First, you have to clean the data, replace all empty value with NA because if you count on column empty/blank will go missing. Once you replace the empty/blank with NA create the pivot table and calculate the percentage.",317845.0
88715,367501.0,Thanks guys for all your quick response. Appreciate it!,301124.0
88715,367502.0,"I actually agree with Bishnu. If there is missing data, you can either remove those data points or put impute/calculate/estimate it. Given that we had a largish data set even if you removed missing data points, the more accurate calculation is 152/654 ~23%. The assumption here is that the missing data points are random and there is no reason why one gender is more likely to not report",305653.0
88715,367531.0,"Yes, like everyone pointed out. Its very important to carefully analyze the data before doing calculations on it. The Gender column has many blank values as many people preferred not to diclose their Gender,maybe it wasnt mandatory field in the survey form(if the data is caputed from a survey form which usually it is) to capture Gender. As a result, there are many blank fields in Gender which doent mean that they cannot be considered as candidate to calculate total percentage. I appreciate your way of calculating manually by filtering, however there's an easier way to calculate just by putting in an pivot to the entire data set, Drag "" Gender "" in "" Row section "" and "" Years of Experience ""(the only field with all entries and no blanks) in "" Value section "". Then expand on the value and Click on Value field settings and Select "" Show Value As "" tab and select "" % of Column Total "". It will automatically populate the percentage of Male, Female, Blanks in your pivot table. We could have done it the crude way as well, but making use of tools and features in excel itself makes our calculations easier and more informative. I have attached a picture for above explanation. The same can be done using Tableau as well which will provide a graph of the metrics calculated using Excel Pivot.",307176.0
88715,367544.0,"Open the csv file. Click Ctrl+shift+L to get sort and filters option one each column. Then go to gender column, click on filter option ans select only females and click OK. See the count coming at bottom right bar of excel sheet. Count-1 is the real count of women in cohorts =.153-1=152 Then clear the filter on gender column. Then go to year of experience column and select the column. See the count coming at bottom right bar of excel sheet. Count-1 is the real count of total studnets in cohorts=807-1=806 Now 152/806*100=18.85 close to 19% is the answer",317811.0
88715,367666.0,"Missing values should be taken into account while counting the total number of candidates. So as per data provided, percentage of female candidates should be 19%.",301890.0
89303,370285.0,The questions can be easily answered using excel. Need to apply the proper filters and do a bit of simple calculations on that. Also pivot table can be helpful there.,309451.0
89303,370272.0,You can solve using Excel or Tableau or any other tool you know.,317991.0
89303,370337.0,thank you both.,315661.0
89303,370398.0,"use Excel, pivot , find replace and you will be done.",306244.0
88642,366975.0,"Hi Naga, 1. You need to select the cell and then count will appear in the status bar. 2. for the percentage you can use the appropriate formula in excel or can do the math.",301648.0
88642,366977.0,"Hi Naga, Try this out. 1. Create filter (Alt+D+F+F) and Clean the data - Many cities will be named differently by different people (ex - Bangalore, Mumbai etc). Filter each and give one name to each entry. 2. Create a pivot of the worksheet - Select City in Rows and Values. 3. Copy this pivotted data as values and beside the Count of City column, apply percentage formula and drag throughout. 4. Now you have total sum and percentage of each city next to it's entry. Hope this helped :)",308962.0
88642,367678.0,You can use pivot table for this but make sure to check the city names as in the data eg. Bangalore is having many variations in name. We can replace by one name for every city and then proceed with pivot table.,301890.0
88642,367331.0,Though it is pretty easy with Pivot but I rather suggest not to look for specific solution first. Make a pivot and play around using different row column selection and you will know easily how to derive anything like that..pls must try same with Tableau as well. U can even simply use formulas available to get solution.,319869.0
88642,367217.0,"Since same city has multiple names first we have clean the data make one standard name. Once you clean the data it's easy to group. use pivot table and you can see the count of city, Now next to count column create percentage column and write =current count column index/total count column index*100 https://support.office.com/en-us/article/create-a-pivottable-to-analyze-worksheet-data-a9a84538-bfe9-40a9-a8e9-f99134456576 one more way to do the same using regular expression pattern matching using tableau, where you don't have to clean the regex will do the work https://www.tableau.com/about/blog/2015/6/become-regular-regular-expressions-39802",317845.0
88642,367040.0,"Hi Naga, The best way to solve the above question would be by using a pivot table 1.Select insert pivot table and select the data range. 2. select the city name in rows column and sort the data in ascending order. 3. Once the data is arranced in ascending order you can scroll through and search for spelling variants of the same city like Banglore, Bangaluru etc. Once identified you'll have to go back to the base data and standardize such spellings. 4. Once the above step is completed, you have to drag the city name to the values column (by default it would show count as it is a dimension (non numeric variable). You can drag the city name again to values and right click and select value field settings -&gt; under show value as tab -&gt; select show values as % of column total and you'll have your answer.",318021.0
92662,390386.0,Please get in touch with your student mentors for any such requirements.,301618.0
123967,538388.0,CART: When there are only binary splits CHAID When there are more than 2 splits. You can also refer to the link given below where it is mentioned that CHAID is used mainly for analysis and CART is for prediction. http://www.bzst.com/2006/10/classification-trees-cart-vs-chaid.html,301121.0
124006,538650.0,If the logical regression show x1...xk are the important feature to determine the log odd then take only x1....xk as your tranining feature for the decision tree and build the model check if have any improvement compared to the logistics model.,318476.0
124006,539164.0,It can use Gini-Index(if you go Gini-index way) or Entropy/information gain way to determine the priority of split and you will know what features it has considered important first. You can refer to the lectures in the module for further information.,307176.0
122868,534413.0,"The equation for a logistic regression model is given by P=1/1+e−(β0+β1x1+β2x2) The above equation can be rewritten in the log odds form: ln(P/1−P)=β0+β1x1+β2x2 In the above equation, the term P1−P is known as the odds . Here, the odds indicate the chances of a consumer being male (P) as a proportion of chances of the consumer being female (1-P). logistic regression model, you choose a cutoff value c , say 0.5. In a binary classification task(y = 1|0), if P &gt; c, then the predicted value is 1 else 0. You can calculate t using c by substituting c in the following equation: t=ln(c/1−c) Since β0+β1x1+β2x2=t represents a straight line, the conditions β0+β1x1+β2x2&gt;t and β0+β1x1+β2x2&lt;t represent two areas divided by the straight line (corresponding to the two classes). Thus, the straight line β0+β1x1+β2x2=t is the decision boundary of a logistic regression model. please refer the below link for logistic regression : https://stats.idre.ucla.edu/stata/output/logistic-regression-analysis/",314183.0
122868,537337.0,You can get the log odds by taking the exponent of the coefficients. np.exp(clf.coef_) #for example Please refer: https://stackoverflow.com/questions/39626401/how-to-get-odds-ratios-and-other-related-features-with-scikit-learn,302735.0
122909,534726.0,"Hi, Please follow the below links http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/ https://github.com/Rambatino/CHAID",344894.0
123085,535880.0,"Natural Language Processing( NLP) algorithms could be used to detect fake news by these companies. There is need to do ongoing research and advancement of NLP algorithms to detect fake news. The said social media companies will need to develop models that could understand human text , be able to differentiate between real and fake news. These models need to be trained ongoing basis. Then these companies should on real time basis process the texts put in social media and stop the spread of the news. Data science will have a major role to play here in coming years.",306009.0
123085,536872.0,"Fake news detection can be automated, and social media companies should invest in their ability to do so. “Public interest algorithms” can aid in identifying and publicizing fake news posts and therefore be a valuable tool to protect consumers . In this vein, computer scientist William Yang Wang, relying upon PolitiFact.com, created a public database of 12,836 statements labeled for accuracy and developed an algorithm that compared “surface-level linguistic patterns” from false assertions to wording contained in digital news stories. This allowed him to integrate text and analysis, and identify stories that rely on false information. His conclusion is that “when combining meta-data with text, significant improvements can be achieved for fine-grained fake news detection .” In a similar approach, Eugenio Tacchini and colleagues say it is possible to identify hoaxes with a high degree of accuracy. Testing this proposition with a database of 15,500 Facebook posts and over 909,000 users, they find an accuracy rate of over 99 percent and say outside organizations can use their automatic tool to pinpoint sites engaging in fake news.They use this result to advocate the development of automatic hoax detection systems .",300713.0
123085,537035.0,"Hi, I suppose, Fake news curbing will be classification just like email spam-ham. 1) We can use Random Forests(ensembles) with many decision trees, which could be efficient 2) SVMs(radial), since data collected will most likely I think it will be non-linear 3) Since Fake news could be ony subject, we can have subject wise models, 4) Also given fake news curbing should be efficient before it gets viral, the training for model will be a bigger challenge. and say if authenicity of the news shared cannot be recognized by algorithm, an auto labelling should be added along with news to say authenticiy cannot be verified. Thanks, Kiran",306736.0
123085,538271.0,NLP can be used and Logistic Regression can be applied,306008.0
123085,538877.0,"Machine Learning is being extensively used to identify the authenticity of News in social media. Usually the ML process starts with acquiring the News Dataset through Twitter or Facebook APIs. Once the data is acquired it needs to be labeled. Here are some common practices to label the training dataset on whether a News is fake or not: Sources, citations, Publisher’s credibility Content quality (typos, biases, sentiments) Timeliness of post Number of Shares &amp; Likes Once the data is labeled, the supervised learning process starts. Natural Language Toolkit (NLTK), a Python library for natural language processing (NLP), is used to clean up the news data, preprocess the data for feature extraction, before training the machine learning algorithms for classification of the news into true news or fake news. As news is text data, n-gram and vector-based models are used to clean the data for reﬁnements like stop-word removal, tokenization, lemmatization, lower casing, sentence segmentation, and punctuation removal. N-gram is a popular feature identiﬁcation and analysis approach used in Natural language Processing. N-gram is a continuous sequence of items with length n. It could be a sequence of words, syllables or characters. The most used n-gram models in text categorization for fake news detection are word-based. The word-based n-gram generates text features to classify the document and differentiate between fake and honest news articles. Typically, various sets of word-based n-gram frequency proﬁles generated from the training data helps to classify fake and truthful news accurately. Some of the popular text feature selection methods are Term Frequency (TF) and Term Frequency-Inverted Document Frequency (TF-IDF). Term Frequency (TF) is where each word count is converted into the probability of that word existing in the document. Term Frequency-Inverted Document Frequency (TF-IDF) is a weighted metric wherein the number of times a word appears in the document is counteracted by the frequency of the word in the corpus. The standard steps involved in Fake News Detection starts with preprocessing the data set, by removing unnecessary characters and words from the data. N-gram features are extracted, and a features matrix (TF or TF-IDF) is formed representing the documents involved. The next step in the classiﬁcation process is to train the classiﬁer to predict the class of the documents with different machine learning algorithms: Logistic Regression, Stochastic Gradient Descent (SGD), Support Vector Machines (SVM), Linear Support Vector Machines (LSVM), Naïve-Bayes, K-Nearest Neighbour (KNN), Neural Network, Decision Trees (DT) and Random Forest. Reference for Further Reading: 1. http://news.mit.edu/2018/mit-csail-machine-learning-system-detects-fake-news-from-source-1004 2. https://www.cp.eng.chula.ac.th/~piak/paper/2018/Detecting-Fake-News-submit.pdf 3. https://www.summer.harvard.edu/inside-summer/4-tips-spotting-fake-news-story 4. https://www.analyticsvidhya.com/blog/2019/04/datahack-radio-machine-learning-identify-fake-news-mike-tamir/",301644.0
123085,540237.0,"One of the ways is to label data by crowd sourcing. We can take user inputs to label articles as fake /real. Once we have a significant amount of labelled data, we can use machine learning to label future articles and also continuously keep learning. We can create model by cross checking various attributes like publisher, list of followers who like or comment on the article etc.",317996.0
123085,546649.0,Fake news identification is a classification problem where we need to differentiate between the real and fake news via subject line or the manner in which content is formulated. Advanced classification techniques like SVM and random forest will prove to be more efficient when compared to logistic regression as the data will not be strictly linear. Text analytics techniques like NLP will also be best suited. The train dataset will be dynamic as the fake news mongers will come up with innovative ways to formulate the content and subject.,308967.0
122236,532311.0,"Hi, Please follow the below link https://github.com/Rambatino/CHAID",344894.0
123422,536962.0,"Hi CART stands for, Classification and Regression Trees which we have learnt so far in Tree models course CHAID is Chi-square Automatic Interaction Detector ( CHAID ). Chi-square tests were one of the optional modules in Statistics Course, but CHAID is different which we need to learn. Thanks, Kiran",306736.0
123462,537034.0,"If the value of log odds is equal to the threshold, then it is on the decision boundary of a logistic regression model. So we can find this best by "" Sigmoid Curve "".",311117.0
123462,536940.0,If the log off values is &gt;=threshold then It will be depend on how you said about threshold like Let's say threshold = 0.75 and you made a rule that if threshold &lt;= 0.75 then C1 else C2 So it will depend upon how you made the rule,318476.0
133015,580026.0,I am able to view all the sessions under additional resources including C7 live sessions. Suggest you to report error in case you are facing issues with accessing any of the videos.,313826.0
99599,426175.0,"I think, there are no exceptions under which we can accept null hypothesis. As part of hypothesis testing, we are trying to gather data to prove the alternate hypothesis (and thus reject the null hypothesis). If we gather such data then we accept alternate hypothesis. If we fail to gather such data, then we can't accept the alternate hypothesis. However, we have not proved the null hypothesis either and hence we cannot ""accept it"". Hence we fail to reject the null hypothesis .",313826.0
99599,426217.0,"[Borrowing from Nassim Taleb and his seminal book, The Black Swan] Say if you were trying to prove that all swans in the world are white. So your hypotheses are as follows: Ho: All swans = WhiteH1: All swans &lt;&gt; White For Null Hypothesis, unless we have observed the entire population of swans, we cannot, with certainity, say all swans are white. They might be so rare that your sample does not capture them. Does not mean they dont exist. To accept alternate hypothesis i.e. not all swans are white, you need just one black swan amongst all observed swans. One black swan is an irrefutable evidence that our null hypothesis is false and should be rejected. The only exception to this rule is if you observed the entire population. And if you did, you dont need hypothesis testing",305653.0
99599,426256.0,"A proposition that undergoes verification to determine if it should be accepted or rejected in favor of an alternative proposition. Often the null hypothesis is expressed as ""There is no relationship between two quantities."" For example, in market research, the null hypothesis would be ""A ten-percent increase in price will not adversely affect the sale of this product."" Based on survey results, this hypothesis will be either accepted as correct or rejected as incorrect.",318476.0
99599,426576.0,"We can only accept the null hypothesis when we examine the entire population. Since the population size is always large, we go for samples. Hence it is not possible for the null hypothesis to be accepted. If the population size is small, there is no need of hypothesis testing.",304319.0
99599,426414.0,"When you do a hypothesis test, two types of errors are possible: type I and type II. The risks of these two errors are inversely related Type I error When the null hypothesis is true and you reject it, you make a type I error. The probability of making a type I error is α, which is the level of significance you set for your hypothesis test. An α of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis. Type II error When the null hypothesis is false and you fail to reject it, you make a type II error. The probability of making a type II error is β, which depends on the power of the test. The probability of rejecting the null hypothesis when it is false is equal to 1–β. This value is the power of the test. Truth about the population Decision based on sample H 0 is true H 0 is false Fail to reject H 0 Correct Decision (probability = 1 - α) Type II Error - fail to reject H 0 when it is false (probability = β) Reject H 0 Type I Error - rejecting H 0 when it is true (probability = α) Correct Decision (probability = 1 - β)",314183.0
99934,428234.0,"As stated in the course, the most popular once are 1% and 5%. But, there can be others used according to the application. I think in general, the significance level should be small.",310974.0
99934,428321.0,"you should try to take significance level as small as possible coz this will create a less chance of type 1 error you can select .01 , .05 or .1",318017.0
99934,429572.0,"If the significance level is not given, take it s 5% , as suggested .",315560.0
99934,429687.0,"Default is taken always as 5% across all tools unless given, in which scenario we can change it to given value",314431.0
99964,428429.0,"That symbol will be used when the null hypothesis would have a ≤ symbol associated with it. Basically, null and alternate hypothesis have opposite symbols since they occupy non-overlapping regions. Go through the null and alternate hypothesis segment once again, check the Flipkart examples and the 2 quizzes given to get a better idea.",313517.0
99964,429697.0,"Alternate will always be opposite of what Null is selected as. They will never overlap. Signs in Null hypothesis will always be having equal to sign ( =, &lt;=, &gt;= ) with alternate hyothesis having exact opposite of it.",314431.0
99964,431664.0,To avoid the overlapping between Null and alternative hypothesis. You should use the below symbol. The null hypothesis always has the following signs: = OR ≤ OR ≥ The alternate hypothesis always has the following signs: ≠ OR &gt; OR &lt; The symbol '≥' includes the equal and greater than whereas symbol &gt; only includes the greater than values,320687.0
99967,428461.0,"The approximation of a sample's standard deviation with the population's standard deviation is taken because of unavailability of the population data. In real life scenarios, it is difficult to have population data beforehand and hence we need to work with the sample data only. So a general rule of thumb is that you do the above approximation and proceed with the hypothesis test. The main use of the std. the deviation(of either the sample or the population) is in the calculation of the sampling distribution's standard deviation on which the critical values or p-values are calculated. So, depending on the sample size and type of test you're performing, do the approximations accordingly and proceed with the test.",313517.0
99968,428464.0,"The significance level is something we decide beforehand, i.e. it is decided before the hypothesis test is even conducted. In real life cases around 5% is taken as the general significance level for computing the critical region. Only in the case of p-values, you don't have to specify the significance level beforehand, but you generally take 5% as the standard value when you need to compare the p-value with something to know whether you would reject the null hypothesis or not. Also, if you would've gone through the errors segment of Concepts of Hypothesis Testing-II, then you might be knowing that the significance level is also the probability of a Type-I error. Hence, what we generally do in real life scenarios is that we select the threshold specific to our test as to how much error we can permit in our test and then proceed to conduct the hypothesis test",313517.0
99034,421632.0,"After further reading, I figured out the answer one has to check whether the claim statement has an ""equal to"", ""greater than equal to"" or ""less than equal to"" symbol and then on the basis of that decide whether the claim statement or the complement of it would become the null hypothesis.",318344.0
99034,421634.0,"The null hypothesis always has the following signs: =, ≤ , ≥ . The alternate hypothesis always has the following signs: ≠, &gt;, &lt; . Read the Flipkart example just above this question. The example presents two scenario to clarify the above mentioned concept.",301652.0
99034,423894.0,"Null Hypothesis is current State or default state or claim statement. Alternate Hypothesis is any hypothesis other than the null. The null hypothesis always has the following signs: =, ≤ , ≥. The alternate hypothesis always has the following signs: ≠, &gt;, &lt;. The acceptance of alternative hypothesis depends on the rejection of the null hypothesis i.e. until and unless null hypothesis is rejected, an alternative hypothesis cannot be accepted.",310952.0
99034,424825.0,"The null hypothesis always has the following signs: = , ≤ and ≥ The alternate hypothesis always has the following signs: ≠ , &gt; and &lt; Here, the claim contains the ‘more than’ sign, so the null hypothesis is the complement of the original claim. it will be μ ≥ 35 minute for null hypothesis and μ &lt; 35 minutes for alternate hypothesis.",314183.0
99814,427424.0,"Do you mean switching null and alternate hypothesis? If so ,you cannot do that. Null and Alternative hypothesis are defined by specific rules so switch cannot happen Null Hypothesis is a prevailing belief about the population and alternative hypothesis is a claim opposing the null hypothesis.",311254.0
99814,427453.0,"You can go through below link, there similar question has been asked and many answers are given that will give more insights on your doubt. https://www.quora.com/Can-I-switch-around-the-null-and-alternative-hypothesis-in-hypothesis-testing Hope this will help.",317991.0
99820,427432.0,Hint : You need to use sampling distribution's standard deviation. It was explained in the Inferential Statistics module and it has been used in the examples of hypothesis testing.,311254.0
99820,427607.0,"Prakash, if you look at the first point for the Central Limit Theorem it is stated that the Sampling mean distributiuon is equal to the Population mean. And you can approximate the sample mean to population mean if population mean is not known. And hence, you can use the sample mean as your population mean here.. And the value that you get with; Sigma divided by squareroot of sample size is 'Sample Distribution' so, it is safe to use the sample mean as your population mean if not known.. :)",316349.0
99679,427326.0,"Under sampling and estimation section of additional resources of inferential statistics under T distribution this link is provided ,which give an idea why and when we have to used sample standard deviation in place of population standard deviation. http://www.stat.wmich.edu/s216/book/node79.html",318426.0
99821,427420.0,"If sample mean is less than population mean, Zc comes out to be negative.",311254.0
99821,427449.0,"Z-Score A Z-score is a numerical measurement of a value's relationship to the mean in a group of values. If a Z-score is 0, it represents the score as identical to the mean score. Z-scores may also be positive or negative, with a positive value indicating the score is above the mean and a negative score indicating it is below the mean. Positive and negative scores also reveal the number of standard deviations that the score is either above or below the mean. Hope this helps.",317991.0
99821,428247.0,"If you are trying to find Upper critical value, Zc will be positive &amp; If you are trying to find Lower critical value, Zc will be negative.",312093.0
99682,426834.0,Gaussian Critical Values The example below calculates the percent point function for 95% on the standard Gaussian distribution in python. # Gaussian Percent Point Function from scipy.stats import norm # define probability p = 0.95 # retrieve value &lt;= probability value = norm.ppf(p) print(value) # confirm with cdf p = norm.cdf(value) print(p),318476.0
99682,426910.0,"Significance level = alpha = 0.05 Cumulative probability of the critical point = 1 - 0.05 = 0.95 Now, try to find the Zc value at 0.95 from the Z table",316349.0
101064,,nan,
100179,429717.0,"If it is a left tailed test, negative values of Zc are considered. Suppose the confidence level is 95%,Zc is +1.96 and-1.96 Since left tailed test, we will consider -1.96",311254.0
100179,429719.0,"Yes. When you have to calculate the lower critical value you will need the left side of value from graph. Suppose 2% significance value in a left tailed, you will use this directly (0.02) to get a Z score which will be negative. In a two Tailed test, to get a LCV(lower critical value) you divide 2% in two halfs and then find Zc value for 0.01 which will be negative. (Similarily UCV will be Zc of 1-0.01 i.e. +ve)",314431.0
100183,429700.0,"Hypothesis testing is about testing a claim. So while setting Null Hypothesis, generally we give statement/equation which supports Status Quo i.e. directly the claim. However, by design Null Statement needs a 'equal to' (= , &lt;= or &gt;=) comparison. So if a claim is not having 'equality' as a component then we set the Null hypothesis as reverse of the claim to get this 'equality' component. Hope this helps.",311686.0
100183,429699.0,"Null Hypothesis definition as explained in the module is the prevailing belief of the population. Null Hypothesis states that all things remain equal. No phenomena is observed or there is no relationship between what you are comparing; And as given in the module Null hypothesis mathematically defined is : =,&gt;= and &lt;= Example: a) Commute time: The average commute time of employees is at least 40 mins. This represents the prevailing belief. So it is the Null Hypothesis Ho &gt;=40. Alternate hypothesis H1&lt;40 which opposes the null hypothesis b) Average rainfall: The average rainfall in a state is at most 200 This represents the prevailing belief. So it is the Null Hypothesis Ho &lt;=200. Alternate hypothesis H1&gt;200 which opposes the null hypothesis c) Sales: The average sales in a store is equal to 500 This represents the prevailing belief. So it is the Null Hypothesis Ho =500. Alternate hypothesis H1!=500 which opposes the null hypothesis",311254.0
100183,431328.0,"The null and alternative hypotheses are two mutually exclusive statements about a population. Ahypothesis test uses sample data to determine whether to reject the null hypothesis. ... The alternative hypothesis is what you might believe to be true or hope to prove true. Null hypothesis can include =, &gt;=, &lt;= alternative hypotheses can include =!, &gt;,&lt;",314183.0
100182,429703.0,"Hypothesis testing is about testing a claim. So while setting Null Hypothesis, generally we give statement/equation which supports Status Quo i.e. directly the claim. However, by design Null Statement needs a 'equal to' (= , &lt;= or &gt;=) comparison. So if a claim is not having 'equality' as a component then we set the Null hypothesis as reverse of the claim to get this 'equality' component. Hope this helps.",311686.0
100182,429708.0,"The thing here to look at is, when claim has a equal to or not. If claim says less than 400 (&lt;400) it means that the opposite of it becomes greater than or equal to 400 (&gt;=400). Since Null hypothesis will have equal sign and Alternate cannot have that, we set the Null hypothesis to &gt;= 400. Hope it answers your question",314431.0
100182,431176.0,"Null Hypothesis is always an assumption which contains these symbol &lt;=,&gt;= or = where as Alternate Hypothesis is always a challenging or contradict to Null Hypothesis which contains these symbol &lt;,&gt; or not =",311466.0
100196,430942.0,"This is the theoretical approach. Status quo or H0 can be expreseed only as = , &lt;= , &gt;=. The complement of these relaional operators automatically the reverses the status quo.",318340.0
100199,429981.0,Understood the problem and approach.,306736.0
99707,427070.0,"The bottom line is even if we fail to reject the null hypothesis, it does not mean the null hypothesis is true.Hence we do not use the term accepting null hypothesis. This is because a hypothesis test does not determine which hypothesis is true, or even which is most likely: it only assesses whether available evidence exists to reject the null hypothesis Please check the link below: http://blog.minitab.com/blog/understanding-statistics/things-statisticians-say-failure-to-reject-the-null-hypothesis",311254.0
99707,427335.0,In hypothesis testing most we have two conclusion : 1) Reject null hypothesis means we have enough evidence to proof our alternative hypothesis 2) Failure to reject null hypothesis means we don't have sufficient data evidence to proof in favor of alternative hypothesis.,318476.0
99707,427901.0,"In hypothesis testing we are attempting to provide enough evidence for the alternative hypothesis. We are not trying to prove that the null hypothesis is true. The null hypothesis is assumed to be an accurate statement until contrary evidence tells us otherwise. As a result, our test does not give any evidence pertaining to the truth of the null hypothesis. The below link may be helpful https://www.thoughtco.com/fail-to-reject-in-a-hypothesis-test-3126424",317996.0
98707,420321.0,"Ok, I see this : ""But in some instances, if your claim statement has words like “at least”, “at most”, “less than”, or “greater than”, you cannot formulate the null hypothesis just from the claim statement (because it’s not necessary that the claim is always about the status quo ).You can use the following rule to formulate the null and alternate hypotheses: The null hypothesis always has the following signs: = OR ≤ OR ≥ The alternate hypothesis always has the following signs: ≠ OR &gt; OR &lt; "" However in Goodyear problem, this hint does not help distinguish null from alternate hypothesis",309211.0
98707,420362.0,"I got my answer from Upgrad TA Mahima Prasad Khuntia , thanlka Mahima! this cleared my query. So, In the Goodyear example, Goodyear claims that each of its tyres can travel more than 7500 miles on average before they need any replacement. So the problem says More than , however clearly as per the hint, &gt; , &lt; cannot be null hypothesis and hence &gt;= cannot be the choice. It has to be &lt;= Also in the case of Defendent is innocent/ not innocent, the problem statement will clearly state which option would be chosen as null hypothesis",309211.0
98707,424818.0,Please follow this link; https://learn.upgrad.com/v/course/208/session/18006/segment/91601,314183.0
98707,426941.0,"From the videos,I understood to form the claim statement first. After forming the claim statement, we need to check for the operator signs &amp; if there is any '=' sign then we need to conclude that the stament having '=' operator is Null Hypotheses &amp; the other is Alternate hypotheses",312093.0
99259,424816.0,Please follow the link: https://learn.upgrad.com/v/course/208/session/18006/segment/91601,314183.0
99259,423150.0,Check this post : https://learn.upgrad.com/v/course/208/question/99034,313517.0
99259,423895.0,can go here https://learn.upgrad.com/v/course/208/session/18006/segment/91601,310952.0
99259,426622.0,tyres can travel more than 7500 means U&gt;7500. Since it does not have an equal sign it will be the alternate hypothesis. The opposite of this would be null hypothesis which is U&lt;=7500.,304319.0
99715,427040.0,basically null hypothesis is something u are assuming to be true and alternate hypothesis is that you are trying to reject the null hypothesis..both are opposite to each other. so if the sale of AC units is 350 this will be null hypothesis and alternate hypothesis is sale is not equal to 350 these 2 hypothesis should b opposite,317982.0
99715,427067.0,"As mentioned in the module, Null Hypothesis is: a) Prevailing belief about the population. b) Assumes that status quo is true where as Alternative Hypothesis is a claim that opposses null hypothesis and needs to be proved. So for example, if it is given that the avarage AC units sold in a month is 350 this becomes the null hypothesis So any sale value not equal to 350 becomes the alternative hypothesis. Another example is the average commue time of employees is at least 35 mins. In this case the null hypothesis &gt;=35( Since it is given that the commute time is at least 35 mins. So it can be equal or greater than 35 mins) Alternative Hypothesis in this case would be &lt;35 mins",311254.0
99461,425180.0,"Alternative hypothesis is the complement of null hypoithesis. Alternative hypothesis is what the researcher believes to be true and would like to reject the null hypothesis. Here in this case, when the null hypothesis is mean=70, the alternate must be mean not equal to 70.. Alternate hypothesis is intended to be accepted if the null hypothesis is rejected (could lesser or greater). Hope this helps.",305335.0
99461,425681.0,"Null hypotheses states that μ = 70, so that Alternate hypotheses will be complement of the null hypotheses.It would be μ ≠ 70. If we are asumming that the alternate hypotheses states that μ &lt; 70 that Null hypotheses will be μ &gt;= 70 Null hypothesis will be rejected if μ &gt;= 70 or μ &lt;= 70, then Alternate hypotheses will be accepted.",314183.0
99461,425181.0,yes apurva can be lying in this case and also she could underestimate herself so we need to check this scenario as a two tail we are making assumptions that she is lying but there can be a possibility that she could be underestimating her performance. when you have null hypothesis as = then alternative hypothesis is always not equal to so check both the tails . hope you got it 😁,318017.0
99461,425335.0,"Alternate hypottheses is always to disprove the null hypotheses. As the null hypotheses states that μ = 70, th ealternate hypotheses opposite to this would be μ ≠ 70. In case the null Hypotheses would have stated that μ &lt;= 70 then alternate hypothese would be μ greater than 70. In order to reject the null hyptheses we only need the evidence that μ ≠ 70, either we provide evidence for μ less than 70 or μ greater than 70 both would result in rejection of null hypotheses. Hope this helps.",306725.0
99464,425175.0,as the null hypothesis will be = 350 there is both possibilities that selling lacks a certain limit or selling increase by a certain limit so it should be checked in both right and left side. if the case would have stated that selling is &gt;= 350 then we could have said that alternative hypothesis should be &lt; 350. hope you got it 😁,318017.0
99522,425635.0,,318476.0
99522,425696.0,"For 3% significance level, you would have two critical regions on both sides with a total area of 0.03. So, the area of the critical region on the right side would be 0.015, which means that the area till UCV would be 1 - 0.015 = 0.985. So, you need to find the z-value of 0.985. The z-score for 0.9850 in the z-table is 2.17 (2.1 on the horizontal axis and 0.07 on the vertical axis).",314183.0
99522,425777.0,"For 3% significance level, you would have two critical regions on both sides with a total area of 0.03. So, the area of the critical region on the right side would be 0.015, which means that the area till UCV would be 1 - 0.015 = 0.985. So, you need to find the z-value of 0.985. The z-score for 0.9850 in the z-table is 2.17 (2.1 on the horizontal axis and 0.07 on the vertical axis).",314183.0
99522,425632.0,"Consider the alpha=3 % i.e 0.03 for the hypothesis test . As this is for both side tail meaning area under both side of the tail is 0.03/2=.015 1-0.15 =.9850 is the area under the curve for the higher tail. For this the Z value is 2.17 <v:shapetype id=""_x0000_t75"" coordsize=""21600,21600"" o:spt=""75"" o:preferrelative=""t"" path=""m@4@5l@4@11@9@11@9@5xe"" filled=""f"" stroked=""f""> <v:shape id=""Picture_x0020_2"" o:spid=""_x0000_i1025"" type=""#_x0000_t75"" style='width:450.75pt;height:319.5pt;visibility:visible;mso-wrap-style:square'> <v:imagedata src=""file:///C:\Users\Sandeep\AppData\Local\Temp\msohtmlclip1\01\clip_image001.png"" o:title="""">",318476.0
99524,425647.0,"No, the status quo is that Apurva is good at archery. Try to compare it with the justice system example, where every person is assumed to be innocent. This is the status quo and thus null hypothesis. You have to gain substantial evidence to prove the person guilty. Similarly, Apurva had established the fact that she was good at archery, and that was the null hypothesis. You are saying that it has been established that Apurva is lying and we do not believe her claims. We do not believe her claims part is true. But lying part can only be confirmed after hypothesis testing, and that too with a certain level of confidence.",301652.0
99524,426571.0,"The given statement or the statement to be tested is called null hypothesis. The word ""NULL"" is used because we try our best to nullify this hypothesis on the basis of sample collected, and if we are not able to do so, we have no choice but to accept it.This is the NULL Hypothesis. Here in Apurva's case, the Null hypothesis is she is good at archery (this is the statement which we should test). Hope this helps.",305335.0
99524,426308.0,"Adding a few points to Ashish's explanation Null hypothesis represents the current belief in a situation. The null hypothesis (H 0 ) is a hypothesis which the researcher tries to disprove, reject or nullify. The 'null' often refers to the common view of something, while the alternative hypothesis is what the researcher really thinks is the cause of a phenomenon. In this case Apurva claims that she is good at Archery. we do not believe her claim. So we try to prove that her claim is wrong. So H0: Apurva is good at Archery H1: Apurva is not good at Archery(Which we are try to prove)",310467.0
100144,429327.0,significant region is the value of alpha which should be basically kept low for the type 1 error not to occur significantly,318017.0
100144,429441.0,it depends on your business impact.. like the example given in the video on the 'defendant being' innocent with Type I error will be convicted anf hence can enter into death penalty.. and hence we would want to minimize the significance to 0.0001 as said by Professor.. but usually it ranges till 0.05 and even excel takes it a default value..,316349.0
99590,426082.0,The process of finding both would be the same while Z* doesn't talk about the critical points and Zc is for critical sections.,318329.0
99590,426418.0,"A critical value often represents a rejection region cut-off value for a hypothesis test – also called a zc value for a confidence interval. For confidence intervals and two-tailed z-tests, you can use the zTable to determine the critical values (zc). The critical values for a 95% confidence interval. These are the same as the rejection region z-value cut-offs for a two-tailed z test with alpha = .05. When alpha = .05 we are using a 95% confidence interval.",314183.0
99590,426262.0,"For Zc: A critical value often represents a rejection region cut-off value for a hypothesis test – also called a zc value for a confidence interval. For confidence intervals and two-tailed z-tests, you can use the zTable to determine the critical values (zc). Example Find the critical values for a 90% Confidence Interval. NOTICE: A 90% Confidence Interval will have the same critical values (rejection regions) as a two-tailed z test with alpha = .10. The Critical Values for a 90% confidence or alpha = .10 are +/- 1.645. For Z*: Using CLT, you can estimate the population mean from the sample mean and standard deviation. For example, to estimate the mean commute time of 30,000 employees of an office, you took a sample of 100 employees and found their mean commute time. For this sample, the sample mean 𝑋̅ = 36.6 minutes, sample standard deviation S = 10 minutes. Using CLT, you can say that the sampling distribution for mean commute time will have - 1. Mean = μ {unknown} 2. Standard error = 𝜎√𝑛=𝑆√𝑛=10√100 = 1 3. Since n(100) &gt; 30, the sampling distribution is a normal distribution In fact, you can generalise the entire process. Let’s say you have a sample with sample size n, mean 𝑋̅ and standard deviation S. Now, the y% confidence interval (i.e., confidence interval corresponding to y% confidence level) for μ will be given by the range – Confidence Interval = (𝑋̅−𝑍∗𝑆√𝑛,𝑋̅+𝑍∗𝑆√𝑛) Where, Z* is the Z-score associated with a y% confidence level. For example, the 90% confidence interval for the mean commute time will be – μ = (𝑋̅−𝑍∗𝑆√𝑛,𝑋̅+𝑍∗𝑆√𝑛) Here, 𝑋̅ = 36.6 minutes S = 10 minutes n = 100 Z* = 1.65 (Z* corresponding to 90% confidence level) So, the confidence interval is – μ = (34.95 mins, 38.25 mins)",318476.0
99731,427075.0,How have we derrived at the below rule? The null hypothesis always has the following signs: = OR ≤ OR ≥ The alternate hypothesis always has the following signs: ≠ OR &gt; OR &lt; because it seems like in some cases we're considering the claim to be null hypothesis and in other cases the opposite of the claim as null hypothesis.,307176.0
99731,427073.0,"In the module it is mentioned You can use the following rule to formulate the null and alternate hypotheses: The null hypothesis always has the following signs: = OR ≤ OR ≥ The alternate hypothesis always has the following signs: ≠ OR &gt; OR &lt; It is mentioned Flipkart claimed that its total valuation in December 2016 was greater than $14 billion. Here, the claim contains &gt; sign (i.e. the ‘more than’ sign) which according to the above mentioned rule should be alternate hypothesis Null Hypothesis would be &lt;=$14 billion I hope this explains your doubt.",311254.0
99924,428166.0,"Hi Varun, The value of σ/√n isn't the standard deviation of the sample or the population but rather it is the standard deviation of the sampling distribution for the given sample. We conduct our hypothesis test on the sampling distribution only. Thus when the standard deviation of the population isn't given, we take the sample's standard deviation to calculate the sampling distribution's standard deviation.In that case we used S/√n instead which denoted that we can only use the sample's standard deviation in this case.",318822.0
99924,467423.0,0nihvum,318822.0
99924,428179.0,"The formula to calculate the Critical Value(CV) is CV = μ + ( Zc * σ¯x ) μ --&gt; Population Mean Zc --&gt; z-score σ¯x --&gt; Standard deviation of the sampling distribution = σ/​√N where σ --&gt; Population Standard Deviation if available, else Sample Standard Deviation (S) Plugging in the values we get CV = 2.5 + ( 1.88 * ( 0.6 / ​√100) ) = 2.5 + ( 1.88 * 0.06 ) = 2.5 + 0.1128 = 2.61128.",313826.0
99462,425178.0,imagine a scenario that a case is being fought to prove a criminal guilty so there are two possibilities 1 that he is guilty and other being not guilty so this example can be distributed in null and alternative hypothesis we can divide every hypothesis into these two cases only.,318017.0
99462,425589.0,simply stated it divides possibilities into 2 opposite segments - 1st is the hypothesis that the status quo is true and 2nd that the status quo is not true.,308962.0
99462,425794.0,Null and alternate hypothesis conditions and mutually exclusive and exhaustive. means both can not overlap and also there is not third possibility other then these two.,318344.0
99462,425340.0,Null and Alternate Hypotheses are always opposite to one another such that all the possible scenarios should fall under either Null hypotheses or alternate hypotheses. e.g if we say our null hypotheses as hotstar has highest viewership through mobile then our alternate hypothesis would be that hotstar does not have the highest viewership on mobile rather than saying that hotstart has highest vierwship on TV or tablet or any other media steaming device. Null and Alternate hypotheses would be completly opposite to each other and much encompass all possible scenarios. Hope this helps.,306725.0
99462,425142.0,it means that what possible outcomes can be arrived at basis whether null hypothesis is rejected or failed to get rejected. There can only be 2 outcomes in such a situation as mentioned above.,310509.0
99940,428858.0,"I think it is for the test of independence. The example basically says that the researcher wants to confirm if there is a relation between gender and alzheimer's disease, hence wants to identify if the two are dependent or independent.",318397.0
99940,428319.0,yes it is for godness of fit,318017.0
99962,428424.0,I think you meant how we calculate Z-tables and not Zc tables. Here is the relevant post for it. https://learn.upgrad.com/v/course/208/question/99248,313517.0
99962,428441.0,"Gagan - Z-distribution is standardized Normal distribution. where Z = (X- mean)/ (std Deviation) This standardizes normal distribution with mean = 0 and Std Deviation = 1 in z table. So to understand how Z table is calculated, we also need to know how normal distribution is generated. Normal Distribution below formula - It is continuous function with mean (mu) and std Deviation (sigma) So create z distribution ie. Standardized Normal Distribution, 1. Shift mean to 0 from (mu), Z score perform (X- mu) 2. To make std deviation 1, this need to be divided by (sigma) Hence z-score is (X- mu)/sigma. Here X = f(x) which is normal distribution",318458.0
99903,427988.0,Found this link useful https://www.youtube.com/watch?v=Ke9ttUj7AQc,317514.0
99903,428189.0,"The inbuilt Anova function part of the ""Data Analysis"" toolkit can also be used to get these values. Click on ""Data Analysis"". Scroll up/down and select ""Anova : Single Factor"". A new window pops up for providing the various parameters. Select appropriate range of data, output column, alpha. The output is printed in the range selected.",313826.0
99903,428943.0,"If you look at the formula for calculating SSB, there is a square of the difference between the mean of the group and the grand mean. The correct formulae is 11 * (8.045 - 7.6206)^2, this will give you 1.98. Refer to the formula in the middle:",318397.0
99903,429775.0,Thanks for teh clarification,317514.0
100925,433585.0,https://www.dynamicyield.com/glossary/control-group/ please go through the link,318017.0
100925,440195.0,"A/B testing allows individuals, teams, and companies to make careful changes to their user experiences while collecting data on the results. This allows them to construct hypotheses, and to learn better why certain elements of their experiences impact user behavior. As control groups are the only way to guarantee that you’re not doing more harm than good . For an A/B test, you can only test one potential change at a time, because the A will be the control group.",301646.0
99953,429631.0,that part i know.. my question is how do we calculate value 30.91,316349.0
99953,429600.0,The risk to reject null hypothesis is actually calculated p-value. As it is more than alpha ( 5%) we wont be able to reject null hypothesis,317996.0
99953,429641.0,"After calculation in excel, p-value is 0.309 This is nothing but 30.91 %",317996.0
99952,428373.0,The Bloew like have some good example: https://onlinecourses.science.psu.edu/stat500/node/55/,318476.0
99952,428971.0,"2-sample proportion test is based on category. In Peppy panner pizza we considered the insignificant difference between 'Control' and 'Experimental' population as Null Hypothesis Null Hypothesis definition as explained in the module is the prevailing belief of the population. Null Hypothesi states that all things remain equal. No phenomena is observed or there is no relationship between what you are comparing; As the assumption in the peppy paneer case is that, even though there is the launch of new ad of peppy paneer, the response rates in both the groups are same(Control Group - People who already know about Peppy Paneer and the Test Group- People who bought it after see the ad) Alternate Hypothesis- states the opposite of the Null Hypothesis. That there was some change, or observed relationship between what you are comparing. So Response rate is better in the Experimental population. https://towardsdatascience.com/hypothesis-testing-in-real-life-47f42420b1f7",311254.0
100097,428743.0,"For the given sample size = n, degrees of freedom = n-1. So here, for n=25, d.f =n-1 = 24. When the significance level is 5% (that is 0.05), two-tailed, look out in the given t-table..under d.f = 24, significance level = .05(two-tailed), we get the required t-value.",305335.0
100097,429799.0,"Degree of freedom is sample size - number of unkown variable i.e mean of population in most cases therefore taken 1 .so'formula becomes n-1 for degree of freedom. And to find z score use t table to find value for .025(as it's two tail test),n-1.",318426.0
100097,430091.0,"For the given sample size = n, degrees of freedom = n-1. So here, for n=25, d.f =n-1 = 24. When the significance level is 5%.If significance level is not given in the question take it as 5 % which is the standard values for calculating the problems. If your values is greater than(&gt;30) then t values tends to the z values. If your value is less(&lt;30) then its tends to the t values.The table of t values is given in the session. Hope this will help..",308639.0
99806,427313.0,"We would fail to reject the null hypothesis if the p-value doesn't fall in the critical region. As per the p-value method anything after the alpha (upper tailed test) is considered to be the critical region (Though the critical region terminology is specific to Critical Calue method, we can assume it to be in p-value for easy understanding -in this case ~0 to 5%). Since, p-value is almost 31% which is very much closer to the mean population mean it is out of critical region and hence cannot reject the Ho",311160.0
99806,427800.0,No question doesn't says that p value &gt; 5% then accept the Null hypothesis. In fact there is nothing called accept in hypothesis testing. We only can reject or fail to reject hypothesis. I found below link useful that can clarify your doubt. Please go through it: https://statistics.laerd.com/statistical-guides/hypothesis-testing-3.php Hope this will help.,317991.0
100417,431762.0,if you are using the same data then the result should be same for both the test please check if you are passing the correct set of variables.i tried it and i am getting correct output,318017.0
100417,431943.0,"Ideally, both these tests should give the same result. Proportion test is generally used in case of Categorical variables and the Unpaired test is used in cases where numerical values are readily available for analysis. Check once if the categories assigned to the variables are correct i.e if they are consistently followed across the dataset. Results will match :)",312063.0
99699,426878.0,Please ignore my question. I understood. Thanks,307494.0
99699,427374.0,"i did not understand, can yo uplease clarify",308782.0
99699,428362.0,As the p value is grater than 0.05 that means the results will outside the critica region. In case the p value would have been less that 0.05 the result would fall under the critical region and hence we would have rejected the null hypotheses.,306725.0
101125,434965.0,In that case you may treat the third category as a No/0 or drop these records and carry on with the A/B testing. This will yeild more prper results.,318340.0
100189,429793.0,We to conclude the result for population so just counting the proportion in sample doesn't going to fulfill our requirement .We have calculate other measures for that.,318426.0
100189,430993.0,"Conversion score cannot tell us the ultimate result of A/B Testing. We have to check the whether the observed conversion score is statistically significant or not. The higher convsersion score for new feature can be by chance occured. So, in order to confirm the statistical significance of the conversion score we calculate p value and compare it with alpha. That is why hypothesis testing is used, for e.g, we get sample statistics which can be higher than claim but we chedk the statistical significance by performing T test or Z test.",318328.0
100195,431064.0,Check this out https://towardsdatascience.com/the-math-behind-a-b-testing-with-example-code-part-1-of-2-7be752e1d06f,310974.0
100219,430446.0,"The formula to calculate the degrees of freedom for unpaired two sample t test is: - where s² is the pooled sample variance, n 1 and n 2 are the sample sizes. XLSTAT uses this formula to calculate df for unpaired two sample t test. Using this formula you will get df = 56. Source : https://www.statsdirect.co.uk/help/parametric_methods/utt.htm",318730.0
99452,425251.0,Thanks Ashish and Keerthi. By going through the link below wherein one mentions use Z test and and another T test does it imply that we can use any one of it based on the conditions stated above? https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/more-significance-testing-videos/v/z-statistics-vs-t-statistics https://onlinecourses.science.psu.edu/stat414/node/270/ As Ashish mentioned the output values of both the test remains the same when sample size exceeds 30 so is it ok if we can use either of it?,301114.0
99452,425234.0,"T-distribution : A T-distribution is used whenever the standard deviation of the population is unknown The degrees of freedom of a T-distribution is equal to sample size n - 1 For sample size ≥ 30, the T-distribution becomes the same as the normal distribution The output values and results of both t-test and z-test are same for sample size ≥ 30",318344.0
99452,425047.0,"You can use the z test when the population std deviation is not know and sample size is greater than 30, but the sample std deviation is known. Please refer to the below link for more details. https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/more-significance-testing-videos/v/z-statistics-vs-t-statistics",310467.0
99452,427910.0,you could use z test,318791.0
99641,426668.0,"Yes, I also agree with you. As per my understanding also it should be 9% each side. TA might be a better person to answer the question.",318368.0
99641,426510.0,The value that has to be split is only the area that is in RED. The area that shows 18% is not the critical region but in the accepted region.,310974.0
99641,426909.0,"The p-value is calculated using the sampling distribution of the test statistic under the null hypothesis, the sample data, and the type of test being done (lower-tailed test, upper-tailed test, or two-sided test). The p-value for: a lower-tailed test is specified by: p-value = P(TS | ts | H 0 is true) = cdf(ts).In this case, your test statistic is usually negative. an upper-tailed test is specified by: p-value = P(TS |ts | H 0 is true) = 1 - cdf(ts).In this case, Your test statistic is usually positive.) assuming that the distribution of the test statistic under H 0 is symmetric about 0, a two-sided test is specified by: p-value = 2 * P(TS |ts| | H 0 is true) = 2 * (1 - cdf(|ts|)) Where: P = Probability of an event TS = Test statistic ts = observed value of the test statistic calculated from your sample cdf() = Cumulative distribution function of the distribution of the test statistic (TS) under the null hypothesis",314183.0
99514,426695.0,The current free trial is for 14 days only. There are several free online tools that you can use to perform t-test as well. Here's one: https://www.graphpad.com/quickcalcs/ttest1.cfm,313517.0
99514,425593.0,"No, I think in the video also I saw 14 days.",310974.0
99514,425895.0,"The video showed 30 days but even i found it to be 14 days only. So i think we should live with it free for just 14 days, unfortunately.",303670.0
99954,428982.0,Real life Examples: a) Drug test: Control Group -Uses the standard drug and experiment group - Uses new drug b) Website Feature test: a new feature is added to a website Control Group - Uses the standard website and experiment group - Uses the website with the new feature c) Agriculture Control Group- Old manure is used Experiment Group - Usage of a new manure,311254.0
99954,429735.0,"An A/B test is an experiment with two groups to establish which of the two treatments (experiments), products, procedures,etc., is superior. Often one of the two treatments is the standard existing treatment, or no treatment. If a standard (or no) treatment is used, it is called ""control""... A typical hypothesis is that treatment is better than control.. For example, testing two prices to determine which yeilds more net profit, or testing two soil treatments to determine which produces better seed germination, etc. Say the experimet is planting two seeds in two different soils..one is a regular soil.. and the other is treated soil(may be treated with some fertilizers)..If we look at the average height of the plants...the one in regular soil(control group) is different from the treated soil.. A proper A/B test has ""subjects""(this might be a plant seed, a person..that is the one who is exposed to the treatment/experiment) and these subjects must be assigned randomly. And of course, the test-statistic we use to compare the two groups is also important. But why use control group? why not just run the experiment and compare the result to the previous experience in practical..? Interestingly, without control group there is no assurance that any difference is due to the treatment / experiment or is it just by chance.. Hope i made it clear.",305335.0
99954,428370.0,It’s highly used in the pharm industry for drug testing: Where one group of user receives a new drug and other group of user uses the same drug or a dummy drug to compare the treatment. Below link explain more on the same: https://biologydictionary.net/controlled-experiment/,318476.0
99908,428256.0,In this particular case the possibility of having a ' = ' sign in the null statement has to be checked as it is mandatory in the null hypothesis.,300717.0
99908,428183.0,"This is just one of the possible null hypotheses. The null hypothesis could be one of the three listed below, depending on the status quo claimed: 1. μ1 - μ2 = 0 2. μ1 - μ2 &lt;=0 3. μ1 - μ2 &gt;= 0",313826.0
99908,428087.0,"Hi , In this case our assumption / claim is ""mean of the two populations is same"". Null hypothesis in our case (H0: mean1 = mean2 i.e. H0: mean1 - mean2 =0 ) and alternative hypothesis would be(H1: mean1 # mean2). Null hypothesis is based on our assumption and claim , this is a two tailed test.",305652.0
100526,432094.0,"Unpaired samples are samples are the one where two samples are independent of each other, For eg travel time. We may take sample of staff of Marketing department and finance department separately and check the mean travel time of staff of the two department. These are independent samples and hence unpaired. We may want to test if the mean travel time of marketing department is better than that of finance department staff.At significance level of 5%, if the p value comes to be greater than 5% then you donot reject the null hypothesis , if it is less you reject the null hypothesis. Null Hypothesis : Travel time of Marketing dept employees better than Finance, Alternate Hypothesis: Travel Time of Finance department better than Marketing department",310629.0
100526,433936.0,To add on what Ashish has explained so far: Please make sure that the Null Hypothesis should be: Travel time of Marketing department employee is better than or equal to Finance department. As the Null Hypothesis always contain the = (equal to) sign. It may be any of the following: = &lt;= &gt;= Hope this helps!,318355.0
100171,429528.0,for one sample you can use z test or p test for multiple sample excel methods are preferable,318017.0
100171,430213.0,"Hi Vinod, Kindly go through this link below for One sample t-test or z-test in Excel tutorial from XLSAT https://help.xlstat.com/customer/en/portal/articles/2062324-one-sample-t-test-or-z-test-in-excel-tutorial?b_id=9283 This is good with detailed explanation (with pics and video as well). Regards,",314048.0
100176,431144.0,"Hi Vinod, Please check below link: https://www.topbestalternatives.com/xlstat/",301559.0
100173,429589.0,"Lets suppose we have only 2 size of poulation of age (30,36).Same will be applicable to size N. Then Mean = (x1+x2)/N i.e. (30+36)/2 = 33 Variance = The Variance is defined as: The average of the squared differences from the Mean. i.e ((30-30) Power 2 + (36-30) power 2) / N i.e (-3 Power 2) + (3 Power 2)/2 i.e 9+9/2 Variance = 18/2 = 9 Pearson Correlation : The Pearson product-moment correlation coefficient is a measure of the strength of the linear relationshion between two variables. i.e Age vs Height of Kids is strong corelation that when kids is age is increasing height is also increasing this is called positive corelation. The maximum corelation isdenoted by +1 or -1 in exceptional case. There are many links to understand the pearson corelation 3. T Stat is what we have learned in the Videos like the T point following the accecptance of Hypothesis based on the alpha and one or two tail t points.",307843.0
100310,431351.0,the two test sample have two different data source thats why the unequal variance is selected,318017.0
100310,431948.0,"Equal variance option is choosed when you have measured the variance of both the samples and conclude that it is the same for both samples. In all other cases where the variance is not equal/there is less surity on the variance values, the unequal variance option is preferred. In the video, he says that he is assuming unequal variance since he doesn't know the variances of the samples. and hence chose the unequal variance option.",312063.0
99978,428554.0,"It depends on your situation. Generally, 1 denotes that a conversion has happened whereas 0 denotes that no change has occurred. Your null hypothesis and the frequencies would change when you choose the opposite scenario and thus you would have to change those values in the XLSTAT addon accordingly,",313517.0
99978,431949.0,"Since normal parlance is used as 1 for positive and 0 for negative, so same coding is used for t mean proportional test also.",313767.0
100948,433625.0,"Yes, your logic is correct.",313517.0
100029,429147.0,thanks for response,301118.0
100029,428694.0,"In industry processes mostly p-value method is followed because you don't have to state the significance level before-hand. Even though both CRV and p-value would yield the same decision for a particular hypothesis test, the p-value approach is generally more preferred. Also, p-value requires fewer computations and is easy to explain intuitively in layman terms. And the definition of p-value is pretty handy and can be used on non-normal distributions as well to conduct hypothesis test as . Please go through the p-value approach in the additional resources session to gain further understanding",313517.0
99957,428396.0,Standard deviation 110 represents a quantity expressing by how much the members of a group differ from the mean value for the group.,311254.0
99957,428417.0,Standard deviation represented by sigma relates the values of members of a group to its mean.. means the variance between the value of other members to the mean of the group..,305129.0
99957,428467.0,"Bhargav - Standard Deviation talks about how much dispersed/spreaded sample is from mean. When you say average paracetamol content of 900 tablets is 510 mg, it mean if you take sum of weights of all tablets, divide it by 900, then you will get 510 mg. This talks about central tendency. So there tablets less than 510 mg (left side of mean) and there are tablets more than 510 mg. (right side of mean) But mean doesnt talk about how widely they are spread. It can be between [500, 520) or between (410, 610) or (300, 700) For that we need std deviation. When we say std deviation of 110 mg, it tells me that there are tablets which can weigh less than (510 - 110) = 400mg. And there tablets which can weight beyond (510 + 110) = 620 mg.",318458.0
99957,431941.0,in simple terms it is deviation of individual values from mean.,313767.0
99965,428439.0,"That's a good question. Yes, they may have the same value. See, one general rule of thumb that you can use is that the product of alpha and beta remains constant for a particular hypothesis test. So for example, if alpha*beta = 0.196, then you can have a situation where both of them are equal to 0.14(basically do the square root of the product). This is not exactly how alpha and beta values might be related to the hypothesis test. There might be complicated relationships and they could involve some complicated formulas. But since they're inversely related, you can use the approximation I mentioned above to get an idea. Hope this helps.",313517.0
99965,429822.0,Totally depend on problem and hypothesis.If we want our alpha more than .05(as we can tolerate critical error in case experiment doesn't have major side effects overall) and beta less that .20 so that power is greater than .80 (as we don't want to miss any effect of experiment).We can reach to some approximate where both values to be equal based on scenario.,318426.0
99966,428450.0,"It depends on your discretion. Generally, we don't get p-values that close to the significance level. For example, if the p-value comes out to be 4.96% and the significance level is at 5% then it is up to the experimenter to see if this is a statistically significant result or not and then make a decision. Most probably if the test needs to have a better accuracy you would have to conduct the experiment again to see if the test produces a better result or not.",313517.0
99974,428504.0,"No, the claim statement's opposite doesn't become the null hypothesis by default You need to check the symbol associated with the claim statement first, which is mentioned in the feedback above as well. The claim statement has a more than(&gt;) symbol associated with it. Hence it would become the alternate hypothesis in this case. Thus H1 becomes the claim statement. And similarly, H0 is decided as the complement of H1. As Abhishek mentioned in his post above, if Goodyear had made a claim that its tyres run for at least 7500 miles, then the claim statement would be H0 itself since the symbol associated with the claim statement is &gt;=(at least). And H1 would have been its complement.",313517.0
99974,428497.0,"No. It is not true. Even your claim statement can be null hypotheis, if it has =, &gt;= or &lt;= sign. eg. If I claim that all students in my cohort can score atleast 3.5 CGPA. Here null hypothesis is claim itself. ie. H0 : Mu &gt;= 3.5 and H1: Mu &lt; 3.5 In given example if Goodyear claim each of its tyre can travel atleast 7500 miles on average, then H0: Mu&gt;=7500 And H1: Mu &lt; 7500. But Goodyear claimed each of its tyre can travel more than 7500 miles on average, hence to add equality(= sign) in null hypothesis, H0: Mu&lt;=7500 And H1: Mu &gt; 7500.",318458.0
99775,427263.0,it would be checked on the basis of null and alternative hypothesis if your null hypothesis is = then its two tailed if it is &gt;= then it is left tail and it null hypothesis is &lt;= then it is right tailed,318017.0
99775,427284.0,"By alternative hypothesis sign you can know that if it is != it it non directional i.e on both side there is critical region. When it is &lt; , critical region is on left side and when it is &gt; , critical region is on right side.",318426.0
99775,427720.0,You always make use of the alternative hypothesis to decide whether if it is right side distribution or left side distribution. Sign of the alternative distribution will tell you where the critical section will be.,315028.0
99775,427764.0,"Try to remember visually: If alternative Hypotheis is greater then it is on RIght If alternative Hypothesis is smaller, it is on left if not equal - two tailed",312490.0
99564,425859.0,"Sorry, incorrect tag 'Concept of Hypothesis Testing - I ' (I'm unable to edit the tag)",316349.0
99564,425869.0,The explanation provided is correct and I see the calculation is also correct. 1.88 * (0.6/10) = 0.1128 adding 2.5 to 0.1128 2.6128 which is rounded to 2 values.,311160.0
99564,426330.0,As per the caclulation: 2.5 + 1.88(.6/10) = 2.6128,317689.0
99803,427328.0,Below like explain in details about alpha and beta and there relationship and the effect of each other. https://stats.stackexchange.com/questions/59202/stats-relationship-between-alpha-and-beta Hope it helps .,318476.0
99697,426906.0,X-bar == Sample Mean = 370.16 Mu(x-bar) == Sample Distribution Mean == Mu == Population Mean = 370,316349.0
99697,431953.0,if we take average height of whole India population the average will be mu x bar. Though this is difficult to ascertain. though if we take a sample from the India population the average height of sample is x bar. This is easier to calculate.,313767.0
101090,434450.0,Check this post : https://learn.upgrad.com/v/course/208/question/100956,313517.0
101090,434714.0,https://www.khanacademy.org/math/ap-statistics/tests-significance-ap/error-probabilities-power/v/introduction-to-type-i-and-type-ii-errors check above link for better clarity,303673.0
100124,428949.0,"For proving null hypothesis , p-value is of great help. Higher the p-value, higher is the probability of failing to reject a null hypothesis. On the other hand, lower the p-value, higher is the probability of the null hypothesis being rejected. Also,The P-value approach has the advantage in that you just need to compute one value, the P-value, to do the test.Because of just one computation, most statistical software and calculators use the P-value approach for hypothesis testing. For the critical value approach, you need to compute the test statistic and find the critical value corresponding to the given confidence or significance level .",311254.0
100124,431945.0,"In my opinion though there is nothing as such advantage, only this is seeing p value you can ascertain if null hypothesis is to be rejected or accepted. Higher p value goes in favour of failing to reject null hypothesis and lower p value goes in favour of alternate hypothesis.",313767.0
99714,427133.0,Type 1 It's a false -ve. Null hypothesis is true but we are concluding that it is not true. Type 2 It's a false +ve. Null hypothesis is not true but we are concluding that it is true. Hope this helps.,310974.0
99714,427262.0,A Type I error occurs when a true null hypothesis is rejected. A Type II error occurs when a false null hypothesis is not rejected.,320635.0
99714,429001.0,"I have explained it below using one of the example shared earlier in discussion forum. 1. Type 1 error: When a null hypothesis is true. But, you reject the null hypothesis. Its a Type 1 error. 2. Type 2 error: When a null hypothesis is false but you fail to reject null hypothesis. Its a Type 2 error. For example: Suppose there is a village. Null hypothesis is that there is no wolf in the village and alternative hypothesis is that there is a wolf in the village. In the village, a boy pretends there is a wolf and the villagers believe him when there is no wolf in reality. This is a Type 1 error. And, if the boy says there is a wolf in the village and the villagers dont believe him. When, in reality there is a wolf. Now, this is a Type - 2 error. Hope it helps. Below is the link as well. https://learn.upgrad.com/v/course/208/question/99920",311855.0
99719,427007.0,The below link explains Type 1 and Type 2 errors. Also you can find a few examples here. https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/modal/v/introduction-to-type-i-and-type-ii-errors,310467.0
99719,427330.0,The below links explain the difference and relationship between alpha and beta. https://stats.stackexchange.com/questions/59202/stats-relationship-between-alpha-and-beta,318476.0
99719,428489.0,thank you,317618.0
99648,426509.0,Type 1 It's a false -ve. Null hypothesis is true but we are concluding that it is true. Type 2 It's a false +ve. Null hypothesis is not true but we are concluding that it is true.,310974.0
99648,426888.0,"When you do a hypothesis test, two types of errors are possible: type I and type II. The risks of these two errors are inversely related Type I error When the null hypothesis is true and you reject it, you make a type I error. The probability of making a type I error is α, which is the level of significance you set for your hypothesis test. An α of 0.05 indicates that you are willing to accept a 5% chance that you are wrong when you reject the null hypothesis. Type II error When the null hypothesis is false and you fail to reject it, you make a type II error. The probability of making a type II error is β, which depends on the power of the test. The probability of rejecting the null hypothesis when it is false is equal to 1–β. This value is the power of the test. Truth about the population Decision based on sample H 0 is true H 0 is false Fail to reject H 0 Correct Decision (probability = 1 - α) Type II Error - fail to reject H 0 when it is false (probability = β) Reject H 0 Type I Error - rejecting H 0 when it is true (probability = α) Correct Decision (probability = 1 - β)",314183.0
99648,426579.0,"Type I error is an error that takes place when the outcome is a rejection of null hypothesis which is, in fact, true. Type II error occurs when the sample results in the acceptance of null hypothesis, which is actually false. Type I error or otherwise known as false positives, in essence, the positive result is equivalent to the refusal of the null hypothesis. In contrast, Type II error is also known as false negatives, i.e. negative result, leads to the acceptance of the null hypothesis. When the null hypothesis is true but mistakenly rejected, it is type I error. As against this, when the null hypothesis is false but erroneously accepted, it is type II error. Type I error tends to assert something that is not really present, i.e. it is a false hit. On the contrary, type II error fails in identifying something, that is present, i.e. it is a miss. The probability of committing type I error is the sample as the level of significance. Conversely, the likelihood of committing type II error is same as the power of the test. Greek letter ‘α’ indicates type I error. Unlike, type II error which is denoted by Greek letter ‘β’. Find more : https://keydifferences.com/difference-between-type-i-and-type-ii-errors.html",318476.0
99648,427258.0,"I can explain with an example. 1) Drug A was thought to be effective throughout the research ,but was pulled from the market later on when it was found ineffective in practice. This is example of Type 1 error as we assumed Null hypothesis true when it wasn't. 2) There was promising research regarding Drug B but research was abandoned because the test data showed no significant improvement for patients takem during test. This is example of Type 2 error where we assumed null hypothesis false when it can be true.",318426.0
99648,427172.0,A Type I error occurs when a true null hypothesis is rejected. A Type II error occurs when a false null hypothesis is not rejected.,320635.0
100156,431936.0,"simple thing, whenever Ho is taking bi-directional including Ho = 0 and H1&lt;&gt;0 or vice versa it is Two tailed test. in case Ho is &gt;= or &lt;= then it is one tailed test.",313767.0
100156,429413.0,,310974.0
100156,429549.0,"i)When H0 = a is the null hypothesis, the alternate hypothesis would be H1 != a In the above case to test and reject the null hypothesis, we check if the sample mean is not equal to a, i.e, the sample mean can be either greater than a or less than a. So,this is a two tailed test., since the rejection region lies on both the side of distribution. ii)when H0 &gt;= a, then H1 &lt; a In the above case, to test and reject the null hypothesis, we check if the sample mean value is less than a, which falls in left side of the distribution. So this is one tailed test or lower tailed test iii) when H0 &lt;= a, then H1 &gt;a In the above case, to test and reject the null hypothesis, we check if the sample mean value is greather than a, which falls under right side of the distribution. So this is one tailed test or upper tailed test",302750.0
100156,431952.0,"Depends on the alternate hypotheses entirely. If H1 has not equal to sign then it is bi-directional, else uni-directional. If H1 has &lt; sign, then it is left tailed test and in case it has &gt; sign, then it is right tailed test.",312063.0
99173,422594.0,not an answer but a thought.. does such a case follow normal distribution?,311686.0
99173,423178.0,"Could you specify the video in question? Also, hypothesis tests are generally made on claims based on the status quo, and not on facts. You can't go and say like you need to test the hypothesis that 2+2=4 or not.",313517.0
98739,,nan,
99736,432109.0,How do you calculate beta ?,317689.0
99736,427101.0,"Now a statistically smart move doesn't necessarily mean that it'll give the correct result for the population. Because the basic assumption that we have in a hypothesis test is that the sample is a true representative of the population. And when this assumption fails, we get a false result. For example, let's say that you picked a sample of cars where the sample mean didn't fall in the critical region and hence you failed to reject the null hypothesis. However, when all the cars' (all the 10000 cars)mileage was tested it came out to be less than 30. It may so happen that it was because you picked a sample of cars whose mileage was higher than 30 that led you to believe that the null hypothesis is true and the rest of the cars all of them have a mileage less than 30 and it pulled the entire average of the population less than 30. You can never know the characteristics of the rest of the population because you have never tested it. And hence it can behave contrary to the given sample and give us a result different from the one concluded by the hypothesis test. So even though it was a smart move to not reject the null hypothesis based on the sample, it was an overall mistake to not reject the claim since the claim is in fact false. And this is where the concept of errors help us to understand this ambiguity between what the hypothesis test on the sample gives us and what is the actual mean. Basically, when you conduct the hypothesis test, you have 2 choices: to either reject or fail to reject the null hypothesis. A type I error happens when you rejected your null hypothesis on the basis of the hypothesis test done on the sample, but when the entire population was tested, it came out to be true. In the above cars example, if you picked a sample of cars and you rejected the null hypothesis that its avg. mileage was at least 30 kmpl, but later found that the population's avg. was actually more than 30 kmpl, then you have conducted a Type-I error. Similarly, a type II error happens when you fail to reject the null hypothesis when as a matter of fact it was false. This is exactly what happened in the situation explained above. You failed to reject the null hypothesis for a sample of cars that have a higher mileage than 30, but the rest of the population had much lower values than that and hence the entire population's average went less than 30. And so the given null hypothesis came out to be false. The probability of making a Type-I error is denoted by alpha whereas the probability of making a Type-II error is denoted by beta. Now, this is the intuition behind how type-I and type-II errors can occur and what do they exactly mean. The detailed math of the errors is a bit difficult to explain with limited scope, but a basic understanding should be sufficient as to how to interpret the values of alpha and beta for various situations. Please watch the video and segment again if you have understood what is written above and then see if the situations explained by the Professor are clear or not.",313517.0
99736,427107.0,"Also, the given graph is for your exposure only. For your knowledge a basic idea about the types of errors and the risks associated with it as mentioned in the video is sufficient. You can get to know the detailed math behind it from the videos available here : https://www.khanacademy.org/math/ap-statistics/tests-significance-ap/error-probabilities-power/v/introduction-to-type-i-and-type-ii-errors To summarise, we conduct the hypothesis test on a sample to predict the population mean assuming the sample is a true representative of the population. When our assumption fails and our hypothesis test gives us a false result, we conduct an error. For any other doubts, please ask below.",313517.0
99667,426711.0,Multiply with 2 is need as we need to consider both side area under the probality curve for two tailed test.,318476.0
99667,426659.0,"When you do a two-tailed test you are in fact obtaining both the positive and the negative of the statistic. Remember a two-tailed test means that your are testing whether your alternative hypothesis is different from the null, which could mean either greater or less than. The ""greater than"" part gives you the critical region on the positive side of the curve, whereas the ""less than"" part gives you other critical region on the negative side of the curve, so multiply by two.",318368.0
99667,426885.0,"2 tail is generally valid where the null hypothesis is something equal to a number and alternate hypothesis is some thing not equal to the particular number. So the sample mean can be greater than the population mean or less than the population mean.So we need to look at area under both sides of the tail to reach a conclusion.He we need to multiple the pvalue by 2 so as to verify the hypothesis. on both sides of the tail. If hypothesis is to check greater than or less than value , then you neednot multiply by 2.",310629.0
99667,426889.0,"If z=1.95 and if it's a one-tailed hypothesis test, our p-value is 0.0256. But, if it's a two-tailed hypothesis test and z=1.95, we must multiply the p-value of 0.0256 by two. Hence, the correct p-value is 0.0512 for the two-tailed hypothesis test. It is two -tailed. The test is the probability of seeing that value or something more extreme if the null hypothesis is true. 2 is more extreme than 1.95 So you want Pr(Z≥1.95)+Pr(Z≤−1.95)Pr(Z≥1.95)+Pr(Z≤−1.95) which, by the symmetry of the normal distribution is equal to 2×Pr(Z≥1.95)",314183.0
99667,427280.0,As earlier in calculation we have take singal critical value region and then calculated acceptance region for same that was p value for either one of tail that is generally UCV. and to find overall value we multiple P value former with 2 .,318426.0
91369,381946.0,Any meaningful name that tell you the purpose of the UDF.,310511.0
91369,381941.0,yes. but if you see the grading criteria it says that any variable used should have proper name. I think this Statement is also applicable for UDF. so make sure it is a proper name which relates to the objective.,311686.0
92470,389482.0,"No, there is no need to delete 49 rows after calculating Moving Average to generate signal.",310419.0
92470,389496.0,go through these links plz :- https://blog.modeanalytics.com/moving-averages-in-sql/ https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/,318802.0
92470,389525.0,We just need to ignore these rows as signal would not be valid for these.,317689.0
92470,389538.0,"Hi , No need remove , you can just simply ignore them. You can apply the winodws function knowledge to ignore specific number of rows using where clause. Hope it helps.",305652.0
92470,389935.0,Do we need to put a HOLD signal for the first 49 rows ? or shall we keep it as NULL ?,318756.0
92470,389918.0,I put in null values for these rows.,311857.0
92470,389946.0,Ignore means do i need to insert null values or HOLD signal,311952.0
92470,390282.0,"No, you no need to remove the rows .. but as per details shown in a screen shot in assignment either null or blank would make a meaningful result to highlight ""top 19 data of 20MA and top 49 data of 50MA are not used /considered for BUY/SELL/HOLD",318454.0
92470,390519.0,"We don't need to delete the first 49 rows , we can simply populate the NULL value by logic ( when row number is &lt; = 49 , populate NULL )",311861.0
92470,390790.0,These rows don't really matter. So you can populate them with NULL or remove them.,319721.0
90911,379065.0,I think all the stock files have 889 records,311686.0
90911,379442.0,Resolved this,310974.0
90911,387910.0,How did you resolved it? Facing same problem.,307495.0
92488,389621.0,You should use a window function called LAG() to solve this. Give it a try.,310974.0
92488,389649.0,are you passing the input date in correct format? this might be a cause for the issue you are getting. make sure that date is provided as input in correct format.,311686.0
92488,389774.0,Use Lag Window function...,305129.0
92488,390074.0,"use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92488,390661.0,"LAG window function is what one can use, also if you are trying to use user defined function to return the Signal just check how can you pass the values in fuction.",320197.0
92488,393972.0,"thank you all for your suggestions, I actually solved it myself by giving parameter value in the where condition by removing '@' symbol before the parameter. thanks anyway to all",316889.0
92447,389376.0,There are two issues: 1. There are blanks in the data: Solution: Change datatype of Deliverable quantity to varchar. 2. Data type is too small for certain values in Total Turnover. Solution: Change datatype of Total Turnover to Bigint,319357.0
92447,389386.0,Try using script to load file to handle nulls in data and data-type issues. Please refer to other question on how to do load file inpath statement for this.,317689.0
92447,389416.0,Most probably because of null values and data type storage,319869.0
92447,389451.0,"When you use a default import, wizard you don't get any errors, data is imported without any issues. Don't modify anything in the import wizard. Then you get the strurcture of the table. Once we have the strucure, table you use the MYSQL command to load the data from the file. You might get a truncate warning, when you using MySql command, at this point alter the column based on the requirement.",301113.0
92447,390231.0,"You try the wizard. But in the wizard, change the data type for Float or date to String. THen it will be imported. THen create one more table and load the data from this table to that. Alternativey you can also execute update table command to update the data and then alter table command to modify the datatype. Many have faced and solved this issue using different approaches. You are free to select your approach as creating table from the data is not part of assignment :)",301555.0
92447,393345.0,"Hi VinayKumar, This happened for me as well. Most prabably, the reason is that when we import using the wizard it takes all the columns as 'NOT NULL'. This configures the import process as whenever any cell is found blank in the row, that row is not imported leading to a lower number of records in the table. In the excel sheet, if you filter the data, you'll see the blank spaces in some of the cell. Try to import the data only with the required columns according to the assignment and you'll get the 889 records as in the excel sheet. Hope it helps!",318355.0
92457,389446.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
92457,389439.0,"just a hint, use wondow function to get the averages with appropriate number of rows defined in over clause for 20 day and 50 day MA..",316349.0
92457,389424.0,Check the windows frames lecture. It'll help you,318084.0
92457,389499.0,For calculating 20 day MA and 50 day MA follow below link may be it will help you: https://www.google.co.in/amp/s/www.essentialsql.com/sql-puzzle-calculate-moving-averages/amp/ or https://stackoverflow.com/questions/9784027/calculate-moving-averages-in-sql,310419.0
92457,389879.0,"Refer to the windowing lecture,instead of sum use avg and apply.",310629.0
92457,389896.0,"Select columsname1,columnname2 ,Avg(Clomnname1) over (order by column2 ASC ROWS 19 PRECEDING ) AS newCloumnname You could update the required number of columns changing the number before the preceding",318732.0
92457,390187.0,https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/ Follow the steps mentione din this link.,318802.0
92457,390036.0,For calculating 20 day MA and 50 day MA follow below link may be it will help you:https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/ for better understanding about moving average below is the link https://www.youtube.com/watch?v=WlHgUtrGalI&amp;t=29s i think it will going to help you.,306242.0
92456,390541.0,"We need to generate BUY, SELL and HOLD.",312479.0
92456,389441.0,don't confuse with language in question.. we're expected to generate all the three signals based on the conditions defined in 3rd question itself... UDF is just going to display the signal from the table that is already generated in question 3..,316349.0
92456,389422.0,"bajaj2 table must have BUY/SELL/HOLD. In the 4th question, we have to look up on this table and display the result.",318084.0
92456,389447.0,"Please generate BUY, SELL and HOLD all.",317811.0
92456,390343.0,"Generate all the 3 signals for question 3. in question 4, you need to create a function which gets the Date as input and retrives the signal data from the table you had created in question 3.",316202.0
92354,388921.0,it is mentioned that MA20 and MA50 should be stored in bajaj1 table. This needs to be done for al 6 stocks. wy are you updating the values? Simple use create table clause on top of your selct query which identifies the signal,304814.0
92354,388952.0,exactly i was facing the same error.. you may want to save your select clause results of 20Day MA and 50. Day MA into some temporary table and then join it with the bajaj1 and update the values by extracting the results from temporary table.. this has helped me in getting rid of this error.. hope this helps..,316349.0
92354,389199.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date. ANd this values needs to be kept in bajaj1 table and so on for all six stocks.,317811.0
92354,389375.0,Yes storing of the data is mandatory for each of the stock. Clearly mentioned in the problem set.,310629.0
92354,390028.0,i am thinking it will going to help me to get result 1) create temporary table and store the result in temporary table using select. 2) create bajaj1 table and load date and close_Price. 3) using join condition join both tables. 4) then drop temporary table.,306242.0
92354,390127.0,And do not orget to comment your steps/slutions,301555.0
92502,389677.0,"Simple solution is to go to the ini files stored at ""C:\ProgramData\MySQL\MySQL Server&lt;&gt;"" location and set secure-file-priv paramter to null Like secure-file-priv="""" Note:- Program Data folder might be hidden, so unhid it and check commenting secure-file-priv will not work.",304814.0
92502,389619.0,Check this out https://stackoverflow.com/questions/32737478/how-should-i-tackle-secure-file-priv-in-mysql,310974.0
92502,389659.0,"i guess u are getting this error when u are trying to load the data into tables.This secure-file-priv option in MYSQL limits the directories from which files can be loaded. so its better to move all the excel/csv files to the directory specified by 'secure-file-priv' . this can be foind by executing below command in MYSQL command line. SHOW VARIABLES LIKE ""secure_file_priv""; then will get the directory from where the files can be accessed by MYSQL without any restrcitions. so,paste all the csv files to that directory obtained above and try to load data now. It will be successfull",300733.0
92502,389933.0,"You can set secure-file-priv="""" in the ini file under ""C:\ProgramData\MySQL\MySQL Server&lt;&gt;"". Restart the server and it will pick files from any location.",318756.0
92505,389670.0,"It is not cross over as we dont have the MA prior to that. So, you can put this under the assumptions.",317689.0
92505,389643.0,I think we can consider it as cross over. let someone comment or verify.,311686.0
92505,389656.0,it is a starting point and is cross over,301115.0
92505,389661.0,"In my opinion, a crossover happens only when SMA crosses above or below LMA. SMA merely being above or below LMA cannot be a crossover at any stage.",313826.0
92505,389660.0,i assumed it as hold.. since the data doesn't actually cross at this point.. i considered it as 'no daga points till 49th row' and as soon as it started registering they were way apart than can be considered crossing over.. no evidence of crossing over.. !!,316349.0
92505,389766.0,"in my opinion, it has to be considered as Hold as signal can only generate once there is a crossover, but at the very first point 50MA cant be cross over. Plot it via graph for better understanding..",305129.0
92505,389768.0,It should be considered as HOLD since there is no crossover. We are starting to get data for both 20MA and 50MA from that row which we can compare.,318756.0
92505,389674.0,"Yes, this can not be cross over as there is no previous MA. This has to be Hold. Moreover, if you implement the correct logic to identify Buy/Sell/Hold, then you code should automaticaly give the result as Hold.",304814.0
92505,389858.0,"It Should be consider as Hold. For Cross over below condition needs to be true: CASE WHEN ((20DayMAPrevious&lt;50DayMAPrevious) &amp;&amp; (20DayMA &gt; 50DayMA ))THEN 'Buy' WHEN ((20DayMAPrevious&gt;50DayMAPrevious) &amp;&amp; (20DayMA &lt; 50DayMA)) THEN 'Sell' ELSE 'Hold' END as ""Signal""",318476.0
92505,389911.0,The concept clearly says that merely having a higher value in lower moving average than the higher moving average can not be adjudged as a golden cross. I think the row50 should be HOLD,312758.0
92505,390636.0,no high low or low high indication can be predicted with only one sample,318005.0
90176,375782.0,"In order for the signal to be generated, there needs to be an intersection point. Either the longer peried moving average has to cross smaller period moving average or vice versa. If the data set is of daily closing period, then it doesn't happen daily and takes few days to weeks generally. But, if the dataset is of intraday data, then it is possible to happen on daily basis as well.",318329.0
93677,395001.0,"Considering given sample date data is ""dd-MMM-YYY"" format, you will use str_to_date as below - SELECT STR_TO_DATE(`infosys`.`Date`,'%d-%M-%Y') as `Date`.......",318458.0
93677,394739.0,"Correct format is str_to_date(""INPUT"", ""%d-%M-%Y"") as output (""%d-%M-%Y"", this can vary on the format requirement )",315277.0
93677,394783.0,"Even though after I did that, I was getting null as the output. I went through the documentation and tried with manually entering any date as str_to_date(""24-12-2018"", ""%d-%M-%Y"") returned null. so did other combination. Is it version dependent, so did I miss something else.",301649.0
93677,394763.0,"Read str_to_date documentation and u need to work arround this parmeters ""%d-%M-%Y""based on the file date format.",301115.0
93677,394997.0,"Rahul - Check below. select str_to_date(""24-Dec-2018"", ""%d-%M-%Y"") &gt;&gt; Use %M when you have month in MMM format Output: 2018-12-24 select str_to_date(""24-12-2018"", ""%d-%m-%Y"") . &gt;&gt; Use %m when you have month in mm format. Output: 2018-12-24",318458.0
92511,389682.0,"STR_TO_DATE(str,format) here the date format is 1-July-2018 so we have to use the format Str_to_date(date,'%d-%M-%Y')",319056.0
92511,389669.0,Use str_to_date() to change it to appropriate date format. Than change it to date.,317689.0
92511,390078.0,"(cola, colb, colc and so on) SET DateColumName=STR_TO_DATE(Datecolumnname, '%e-%m-%Y')",317811.0
92458,390158.0,"Hi Vinay, Which MySql command are you using?",318429.0
92458,390339.0,Try importing the table via Table Import Wizard available in the schema. This does the job well.,316202.0
92458,389445.0,"First Create the table Then LOAD DATA INFILE 'FullFilePath' IGNORE INTO TABLE TableBame FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\r\n' -- or '\n' IGNORE 1 LINES (@Cola, ) SET YourCOlumn = STR_TO_DATE(@column, '%e-%M-%Y')",317811.0
92458,390906.0,"You can use the latest version you're using or you can use, MySQL 8.0. I'm not sure, why you're facing that error, though. For now, you can import the data by using the UI. Like Sham Kumar mentioned, you can import the table in your schema after you created a schema. To import a Table you can go to Table Import Wizard and browse for the files.",319721.0
92514,389701.0,Thanks Anshul for the information.,307176.0
92514,389697.0,"I dont think so because asignment would be evaluated using MySQL. I asked the same question that whether can I use Oracle, but answer was no. So unfortunately you need to use MySQL",304814.0
92514,390080.0,"No, You should use MYSQL. As you submission files will be running on MYSQL for grading.",317811.0
92417,389272.0,"Try plotting the values. Generate a signla when both the lines cross each other. i.e lets say MA 20 day was 200 today and MA 50 day is 205. If yesterday MA 20 day was 195 and MA 50 day was 210. So, in this case HOLD. But if tomorrow MA 20 day becomes 210 and MA 50 day becomes 206 than that means MA 20 day has crossed the MA 50 day so generate signal as BUY. And if it continues to be higher going forward than it should be hold. Hope its clear now.",317689.0
92417,389253.0,It means the difference has to intersect each other over 2 subsequent rows ie become positive from negative or vice versa. Hope this helps.,310511.0
92417,389274.0,it's not about how much difference actually; but the first point when your 20DayMA goes above 50DayMA and vice versa.. there is no definite difference value which will generate the signal. It should be an arithmetic operator which looks for the current value and compares the history data points and generates the signal.. and later the as long as the condition prevails it stays as Hold.. hope this helps a little.. you may want to visualize this with plotting a graph or manually understanding the data points.. let us know if it is still confusing..,316349.0
92417,389444.0,"At a row on which MA20 crosses MA50, it is required to put the appropriate signal else it is required to put Hold.",301113.0
92417,389453.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line",317811.0
92417,389607.0,"Just a cross over should generate asignal, does snot matter by how much diference or margin.",300717.0
92417,389917.0,"The lower moving average can be more than higher moving average at many points but the golden cross can only occur when the value of lower moving average ""suddenly"" becomes more than the higher moving average ....it means that lower moving average should be smaller than the higher moving average just a day before the day lower moving average becomes more than the higher moving average (the day of golder cross). Refer to the following investopedia link for more information https://www.investopedia.com/terms/m/movingaverage.asp",312758.0
92417,389692.0,,318005.0
92417,390133.0,https://learn.upgrad.com/v/course/208/question/92353,301555.0
92464,389470.0,I think you can make your assumption and mention it. I feel it has to be based either BUY/SELL depending on the condition of MA. I've made this assumption and mentioned it. Maybe TAs can confirm if this assumption is ok from evaluation perspective.,318084.0
92464,389507.0,"Since we don't have MA50 value available for 49th day, so I don't think one can make a good judgement about the trend on this day.",306250.0
92464,389565.0,"Reproducing a part from the problem statement: ""Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal."" I think it has enough hints on the conditions to consider when a BUY/SELL Signal is generated .",313826.0
92464,390077.0,"use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92464,390789.0,"As Ranjay pointed out correctly, we do not have data for the 49th day. So, the 50th day's cannot be generated even if have some MA condition given.",319721.0
92476,389514.0,"Hi , Please check the other threads (https://learn.upgrad.com/v/course/208/question/92085 ) , they have right pointers. Please find the below sample syntax: LOAD DATA INFILE &lt;file_name&gt; INTO TABLE &lt;table_name&gt; FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS( @var1, &lt;column names&gt; ) SET `Date` = STR_TO_DATE(@var1,'%d-%M-%Y');",305652.0
92476,389521.0,"Use STR_TO_DATE(`date`,'%d-%M-%Y'). %m and %M both are different.",317689.0
92476,389544.0,yes i tried the same but its not working still passing only NULL to all rows Any other way to namage ....,318723.0
92476,389558.0,"hi, please read about the str_to_date parameters documentation it's gives you idea what should come in place of %y%m%d,the error is at this point",301115.0
92476,391954.0,"I was also getting the NULL after using STR_TO_DATE() function then able to fix it, First you need convert string to date, then set to standard date format %Y-%m-%d DATE_FORMAT(STR_TO_DATE(date, '%d-%M-%Y'),'%Y-%m-%d')",306731.0
93558,394297.0,"CREATE TABLE stock_mst AS (SELECT bajaj_1.date as stock_date, bajaj_1.Close_Price as bajaj, eicher_1.Close_Price as eicher, hero_1.Close_Price as hero, tcs_1.Close_Price as tcs, tvs_1.Close_Price as tvs FROM bajaj_1 INNER JOIN eicher_1 on bajaj_1.date = eicher_1.date inner join hero_1 on eicher_1.date = hero_1.date inner join infosys_1 on hero_1.date = infosys_1.date inner join tcs_1 on infosys_1.date = tcs_1.date inner join tvs_1 on tcs_1.date = tvs_1.date);",303230.0
93558,394046.0,FROM bajaj_1 INNER JOIN eicher_1 inner join hero_1 inner join infosys_1 inner join tcs_1 inner join tvs_1 ON bajaj_1.date = eicher_1.date = hero_1.date = infosys_1.date = tcs_1.date = tvs_1.date Instead of this the correct way is below:- FROM bajaj_1 INNER JOIN eicher_1 on bajaj_1.date = eicher_1.date inner join hero_1 on eicher_1.date = hero_1.date inner join infosys_1 on hero_1.date = infosys_1.date and so on,317811.0
93558,394303.0,"i am still stuck on the same issue as my query gets stuck while executing. tried the following query also to same effect. Please help as i am unable to proceed CREATE TABLE stock_mst AS (SELECT bajaj_1.date as stock_date, bajaj_1.Close_Price as bajaj, eicher_1.Close_Price as eicher, hero_1.Close_Price as hero, tcs_1.Close_Price as tcs, tvs_1.Close_Price as tvs FROM bajaj_1 INNER JOIN eicher_1 ON bajaj_1.date = eicher_1.date inner join hero_1 on bajaj_1.date = hero_1.date inner join infosys_1 on bajaj_1.date = infosys_1.date inner join tcs_1 on bajaj_1.date = tcs_1.date inner join tvs_1 on bajaj_1.date = tvs_1.date);",303230.0
92431,389314.0,Use / instead of \,318084.0
92431,389322.0,"Please check the links you have posted, CSV files need to be stored in PROGRAM DATA not in PROGRAM FILES if I am not wrong u trying to create folder Uploads in PROGRAM FILES.",301115.0
92431,389452.0,"Run command SHOW VARIABLES LIKE ""secure_file_priv""; You will see a path where you can put the csv file and then it can be imported. Also, while importing use forward slash / instead of bacward slash \ in filepath.",317811.0
92515,389708.0,"question 1.. Bajaj1 table.. you may want to pull the date, close price from bajaj1 and using the same table generate the signals: BUY/SELL/HOLD",316349.0
92515,389765.0,"Hi, its the Question 1 in which we created a table with 20MA and 50MA, from which a new table needs to be created and generate signals based on the 20MA and 50MA..",305129.0
92515,389719.0,"Hi ,Its the table we have asked to create with four columns DATE,CLOSE_PRICE,20_DAY_MA,50_DAY_MA for all the six stocks we have to use that table in question 3",319056.0
92515,389778.0,"Question clearly states that "" Use the table created in Part(1) to generate buy and sell signal. Store this in another table named 'bajaj2'. Perform this operation for all stocks."" Just be careful while reading the question and you will come to know which table is to be used.",317991.0
92186,388339.0,Can anyone guide how to make first 19 rows null You don't need to make them NULL. Ignore them during the signal generation. Any idea how to compare MA 20 and MA40 to add signal? That's the challenge :),310974.0
92186,388421.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
92187,388297.0,"Yeah you are right, but for user interface point of view if you think you can understand that. What if we business logic comes complex, it is just for the practice purpose to get our hands dity with the UDFs with simple examples. But in real life cases it seems quite handy to have UDF to just pass input and get the out and rest complex business logic runinng under the hood. hope it makes sense.",304813.0
92187,388356.0,"Felt the same,or we missing any clue for the last part-- analysis",301115.0
92187,388374.0,UDF is to identify that signal itself. You need to have values in signal column which would be filled by UDF. hope I'm not missing something here,318329.0
92187,388418.0,"UDF Is created so that a end user can use it to find the signal for a date by giving an input date to this function. This will help end user to instead knowing the complex sql query how to find signal for date, a udf can be easily used.",317811.0
92187,388654.0,Creating UDF makes it easy in future to generate the resultant value. Even it is easier to understand in future how the values were created.,301555.0
92187,391677.0,when i am running the udf function message is showing ok but output is not being displayed what can be the problem,314678.0
92978,391558.0,i guess datatype is causing a problem here.. can you check the datatype of date column in your table that you're using in UDF? just do describe.. and cross verify if the same is passed in your UDF as well.. mostly I've seen it as TEXT for those using table import wizard to create tables.. also you may be interested in finding out if the date is in yyyy-dd-mm or yyyy-mm-dd.. (provided you're handling the holidays and others dates which are Not found in Bajaj1 table)..,316349.0
92978,391730.0,"Make sure you have wrapped the date with double or single inverted commas when u pass them through the function. ex - ""2015-01-01"" or '2015-01-01'",312376.0
92978,391745.0,FYI - if its not wrapped in the quotes the system considers it as numeric value. if u give 2015-01-04 it takes it as 2010. When enclosed in quotes it will identify automatically a date vs just text.,312376.0
92767,390670.0,Yes.. how did you import the tables?? Because one value is skipping due to one row value is blank or null..,305129.0
92767,390678.0,Import wizard will import only 888 rows try Create table and then Load data into table,306244.0
92767,390701.0,You need to load it via load data infile script as there are some data qualtiy issues which are not handled by import data wizard. Refer to other question for the entire syntax.,317689.0
92767,390743.0,"No Pratik - we shouldn't be ignoring the missing row since that has a valid close price (31st Aug 2015); all your calculations are on close price and so, yes it does make a difference in the results.. You can achieve this with import table wizard as well.. Set your datatype for ""Deliverable Quantity"" and ""% Deli. Qty to Traded Qty"" as ""Text"" while importing the data using table import wiard.. and see the results; it brings all 889 rows from the csv file.. Please let us know if you still face the issue.",316349.0
92767,392191.0,"Kindly user the required columns mentioned in the question, else you would end up getting less number of rows.",312259.0
92767,392126.0,"Hey, you can load it using the wizard also. But if you load it using the wizard too, you should be getting 889 rows. You can select the columns which are asked to be selected.",319721.0
92771,390665.0,Yes but till 19 as on 20day it will take average of 20 rows.. reason for taking first 19 rows as null because otherwise we can get wrong signals.. same for 50ma first 49 rows as null..,305129.0
92771,390673.0,"Code will fill the average of top rows in 20 days and 50 days, Signals will start from row no 50. while calculation you can ignore top 49 rows rather than trying to deal with NULL. So ignore these rows when Creating Signals.",306244.0
92771,390699.0,It is not required to put nulls prior to 20 days. Only we dont have to generate signals for those days. This can be written as assumption.,317689.0
92771,390907.0,"When we calculate the moving average for 20DAY MAYthe close proce upto 19th day and the 20th days is considered , so effectively we calculate MA for 20th Days. The same applies for 50 Day MA.If you convert any of the file in excel and review it you will understand it better.",310629.0
92771,391062.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
92771,391733.0,Need to NULL till 19th for 20 Days Moving Average and NULL for 49th till 50 days moving average ...,300735.0
93205,392305.0,Do you think it's an optimised way? I don't think so.,319721.0
93252,392481.0,"it's not just yesterday/today, signal has to be generated based on the golden cross.",319770.0
93252,392479.0,only todays and yesterday,318017.0
93252,392480.0,Not the close price. Yesterday’s and Today’s moving average,304814.0
93252,392531.0,it has to be only yesterday's &amp; today's MA,319876.0
93255,392499.0,You can go through below stackoverflow link which explain update multiple tables in same command. https://stackoverflow.com/questions/4361774/mysql-update-multiple-tables-with-one-query Hope this will help.,317991.0
93255,392500.0,try joining the table1 and table2,307710.0
92773,390683.0,"Hi Abhijeet, Please refer these links: https://learn.upgrad.com/v/course/208/question/92353 https://learn.upgrad.com/v/course/208/question/92449 Thanks.",305652.0
92773,390698.0,Refer to following question. I have explained this in detail. https://learn.upgrad.com/v/course/208/question/92449,317689.0
92773,391061.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line",317811.0
93256,392487.0,report should have the findings which you have obtained by doing the assignment it can be both data analyst and technical perspective,318017.0
93256,392493.0,You can find TA's response for your question in below link: https://learn.upgrad.com/v/course/208/question/93156,317991.0
90737,378167.0,"I'm yet to check the assignment to see if there is anything specified. If nothing is specified, then closing averages are usually calculated on the closing price of the Stock.",318084.0
90737,378170.0,Generally MA is calculated on closing price. I'm yet to check the assignment.,318329.0
90737,378187.0,Nothing mentioned. But it should be on Closing Price because that is the 'Price' of the stock on that date.,311686.0
90737,379501.0,Moving Average to be calculated on the Closing Price. WAP Stands for Volume Weighted Average Price. For the assignment however you need to use only the close price.,304281.0
93259,394008.0,Just write your select query inside return statement bracket that will work,315423.0
93259,392508.0,write the select statement inside return,318479.0
93259,392501.0,use returns befor begin,318017.0
93259,392514.0,"You can include SQL statements within stored functions , although you should be very careful about including SQL statements. However, you cannot return a result set from a stored function: trying to create a stored function that contains a SELECT statement without an INTO clause will result in a 1415 error. Declare new variable and pass the value returned by select query using INTO .",307495.0
93259,392510.0,Not allowed to post code. However use into in select clause,304814.0
93259,392509.0,There is no return clause in the UDF. Please look at the example. I can not write the code here because of it being a graded component.,304281.0
93259,392505.0,"you don't have to use begin end here, put the complete select under return",319770.0
93259,392527.0,use INTO a variable clause and then return the variable,307710.0
93259,392582.0,Jut modified your code. this may work. CREATE FUNCTION GEN_SIG(dd Date) returns varchar(4) deterministic return ( SELECT sig from bajaj2 where MarketDate=dd ),312019.0
93259,392636.0,"There are two ways : 1. Just use select statement in return clause as in : return ( select sig from bajaj2 where MarketDate=dd ); 2. This is related to the approach you are taking You have to declare the variable into which you want to save your result and then later return it. To save the output of select query into a variable we are using into keyword here. This code will work for your case: DELIMITER $$ CREATE FUNCTION GEN_SIG(dd Date) returns varchar(4) deterministic BEGIN declare sign varchar(4); SELECT sig into sign from bajaj2 where MarketDate=dd; return sign; END $$ DELIMITER ; select GEN_SIG(""2016-08-01"");",318576.0
93259,393528.0,Declare a variable signal inside the function of type varchar(4). Do a Select sig into signal From &lt;table&gt; Where &lt;field&gt; = &lt;value&gt; Finally return signal just before end of your function,319006.0
91377,382053.0,This error is not related to data type. As SpreadCloseOpen is the last column in the row. you have to check LINES TERMINATED BY parameter in the load command.,320073.0
91377,382054.0,"For Error Code: 1148: you can follow below link:- https://wiki.ispirer.com/sqlways/troubleshooting-guide/mysql/import/command-not-allowed-data-load And for Out of range value for column 'SpreadCloseOpen' at row 1"", 1264 error - you can go through below link: https://stackoverflow.com/questions/14284494/mysql-error-1264-out-of-range-value-for-column Hope this will help.",317991.0
91377,382037.0,"Please check the data type used for the column Spread Close Open. If length of the column is lessthan the value inserting, then increase the value to higher value like int(5) to int(10) or change the data type which suits the column.",311502.0
93262,392502.0,Absolutely,304814.0
93262,392506.0,right,318017.0
93262,392522.0,you are on right track,307710.0
93211,392321.0,That's one of the correct approaches. You can refer to the windows lectures for more clarification on MAs.,319721.0
93264,392534.0,The zip file can be submitted with any name. The system generates a unique id based on the learner.,304281.0
93264,392520.0,you can submit the folder with your name,318017.0
91383,382175.0,"Your signal column should have buy sell and hold values. So, you should have logic to compute hold as well.",318329.0
91383,382196.0,"Signal should comprise of all 3 values buy, sell and hold.",317689.0
91383,382346.0,"If you write the logic for 'Buy' and 'Sell', whatever cases that will be left behind can be assigned the value 'Hold'.",310511.0
91331,381702.0,"Hi, Can try this SELECT REPLACE('Auto_Expo', '_', ' '); replace is a function which replaces any character with anything, like in this case i have replace underscore with space. for reference; https://www.w3schools.com/sql/func_sqlserver_replace.asp Hope it helps :)",305129.0
93210,392315.0,"No, it doesn't. Please refer to the pinned post, https://learn.upgrad.com/v/course/208/question/93156",319721.0
93210,392318.0,"As long as the complete data is loaded into the tables with changing the data at source, It should be fine.let TA also confirm",301115.0
93610,394411.0,"Yes.. Most of them are hold, BUY/SELL are very few for the given datasets.",303673.0
93610,394499.0,"'BUY' or 'SELL' will happen only when `20 Day MA` crosses above/below the `50 Day MA` . Such occurences are very few and for all other cases the signal is 'HOLD'. So, most of the signals will be 'HOLD' with comparatively very few 'BUY' or 'SELL' signals.",313826.0
93610,394492.0,hold and buy only occurs a few time as a hint they dont oocur alot,318017.0
93209,392313.0,"Here, the table you're supposed to be using has only close price and date, correct? So, the null values in other columns, shouldn't really affect you.",319721.0
93209,392328.0,"Each of Stock data CSV file(s) Bajaj, Eicher, Hero, etc. has 1 record with NULL value for below 2 columns, should we keep these rows throughout the assignment steps and calculations as we are using the mainly ""Close Price"" for calculating MA and ""Close Price"" value is available for this record. (1) Deliverable Quantity (2)% Deli. Qty to Traded Qty) This 1 record is missing in all 6 tables when I imported the CSV files.",306731.0
91882,385079.0,Change the format from integer to sting and try to load.,317689.0
91882,385440.0,I would drop these columns as I do not see any value of these in assignment. Or one can very easily unselect the unwanted columns during import. That fixes the import issues.,318007.0
91882,385640.0,"while importing the CSV file with load data infile command , try to set the null values of 'Deliverable Quantity' with 0 (zero)",301115.0
91882,386128.0,Can We ignore the complete null row?,318846.0
91882,387741.0,"I think Yes , this is what I m doing as of now .",311861.0
93267,392546.0,"If the whole commands are executed again in sequence and the output is as desired,then it should be fine",301115.0
93267,392554.0,"I agreed with Samyuktha! Also, it is not required to create temporary tables. The assignment has clearly defined the requirement of creating a table from another table, e.g bajaj1 from bajaj table. So, just curios to know why do you need a temporary table to store the result. Am I missing anything ?",312479.0
93274,392590.0,You should get BUY/SELL/HOLD for all columns except rows where moving average is not applicable. Hope this will help.,307495.0
93274,392586.0,Also pass a command for HOLD for other than the condition given... Signal should be BUY/SELL/HOLD,303673.0
93274,392583.0,It should display BUY/SELL/HOLD... Kindly check the date format,303673.0
93275,392613.0,"If you are importing the data usinf the LOAD DATA.. command, ensure that you specify the below line aswell. LINES TERMINATED BY '\r\n' The above line is for Windows.",313826.0
93275,392588.0,This error happens when you define column name a varchar(10) while creating table but actual column name length is varchar(20). Above is just example. Please check for CSV file for any such mismatch.,307495.0
92223,388658.0,Use case staements. There can be two types of dates and hence it gives error,301555.0
92223,388590.0,Even i have the same issue and getting the same error message,300687.0
92223,388472.0,"on your second code; don't use table word also, replace character e with d",316349.0
91043,380059.0,There were sessions on creating user defined procedures and user defined functions. You can refer to that.,318329.0
91043,380128.0,Can you elaborate on the question a little. This seems very vague.,310511.0
91043,389421.0,"Hi Subhashis, You could refer to this link from our UpGrad sessions: https://learn.upgrad.com/v/course/208/session/19880/segment/101136 For the usage of the variables, you have a better insight from the below link: https://stackoverflow.com/questions/1009954/mysql-variable-vs-variable-whats-the-difference",315797.0
90634,377944.0,"As mentioned in the course, you can deal with this you can use the backquote key (`). It is located at the top left corner of your keyboard. Please let me know if you are facing any other issues.",304281.0
90634,378934.0,"I'm facing the same issue, I've taken the column names same as in the CSV and also used ``. Still, the table won't get created throwing syntax error.",310974.0
90634,380652.0,i created table using ' keyword. it worked for me Ex: Create table 'test table'(id int);,311952.0
90634,394171.0,use back quotes and it works perfectly,311803.0
92371,389108.0,"after IGNORE 1 ROWS; please add below line as well (@date) 'Set Date=str_to_date_(@date, '%e-%m-%y')",317811.0
92371,389158.0,use data type of date as char(30) before importing,318005.0
92986,391596.0,"Hi, Its not exactly regarding the Bajaj stock . Its about the overall analysis process how we have carried out . Let's wait for TA to confirm on the same.",319006.0
92986,391611.0,"Summary should be on the entire assignment. Your results, the method you followed to arrive at the results and the inference you can draw from them.",310511.0
92986,391628.0,"Hi, The assignment is not looking for any specific analysis. It just wants to see our creativity/capability in terms of data interpretation. You can cover different aspects in your analysis using prices/signals of different stocks and thus finding good observations. You can also pick any specific stock and get some insights using its data only. You can also talk about some overall generic points as well as some specific stock together. All that is needed is some insights using the data available with us. Of course the constraint is 250 words. Hope this helps.",311686.0
92986,391626.0,Imagine you are doing this corss over analysis in your work and write a summary out of the results you got in the analysis of 2.5 years data for 6 stocks.,318328.0
92986,391654.0,It totally depends on you how you want to represent the summary but I would say include at least these: 1. methods/approach of analysis 2. Findings 3. Conclusions,317845.0
92986,391750.0,"You need to write the executive summary for the full assignment. Below 250 words and Not an essay or big a report. Maybe you can include the problem statement, your approach, results and your conclusion from it.",312376.0
92986,392574.0,Summary should contain the brief of overall analysis of all the stocks consisitng of approach to get the stepwise results and findings of output.,311117.0
92373,389116.0,CREATE TABLE bajaj2 select * from bajaj1; It will insert the data from baja1 into bajaj2 Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
92373,389157.0,create table table2 as (select * from table1);,318005.0
92373,389167.0,yes. it is possible. you need to use a combination of components like conditional statement like CASE and window function like LAG in your Select query.,311686.0
92373,390128.0,https://learn.upgrad.com/v/course/208/question/92353,301555.0
91447,382469.0,try using case in place of if else,318017.0
91447,382468.0,try giving a default value for the new column and see if it helps.,317998.0
91447,382410.0,I guess Alter and select are two separate sql statements. Please include a semicolon after the Alter statement and execute it. Then execute the Select statement. alter table abc add xyz varchar(20); Hope this helps.,318084.0
91447,382496.0,You need to put a semi-colon at the end of the alter table command like this: alter table abc add xyz varchar(20) ;,313826.0
91447,382544.0,"Add a semicolon and restructure your second query. The syntax below gives you a better picture : alter table abc add xyz varchar(20); select name, age, date, (case when (expression_1) then 'man' when (expression_2) then women' else 'child' end) as xyz from abc;",309211.0
91447,383321.0,"This should be correct command. alter table abc add xyz varchar(20); select name, age, date, (case when (expression_1) then 'man' when (expression_2) then women' else 'child' end) as xyz from abc; If does not work, try restarting your laptop. It might be someissue with Database",301555.0
91447,383516.0,"This becomes very tricky if you forgotten sentence terminator ; in any of previous commands, so it think that the current select is part of your previous command.",304813.0
91449,382452.0,"Closing price of one day need not be equal to opening price of the next day. This is stock trading..... So demand and supply comes into play. A stock might close at Rs100... But due to overnight bad news about the stock, the demand for the stock will go down and buyers may not be available for Rs100 and hence the opening trade could start with Rs80.... Hope this was helpful as well.",318084.0
91449,382446.0,Thanks! Yes that helps. Another question: I was expecting the closing price would be equal to the opening price for the next day. But it doesn't seem to be the case.,318438.0
91449,382440.0,Moving average has to be calculated on ClosingPrice Spread refers to the difference in values. So Spread high-low refers to difference between High and Low value of the stock on that day. Similarly Spread close-open refers to difference in the value between Close and Open. Hope this was helpful.,318084.0
91453,382461.0,"MA refers to Moving Average. It is not present in the file. It has to be computed as part of the assignment. As part of this assignment, we are expected to compute 20 Day Moving Average and 50 Day Moving Average. Basically it means that we have to calculate the average closing price of the stock using the last 20 days and 50 days backwards. For example, to calculate the 20 day moving average of a stock today, we take the last 20 traded days and calculate the simple mean of that. Like-wise for 50 days also.",318084.0
91453,382593.0,"MA is abbreviation for Moving Average, what it means is that the average is moving as per the window selection and it depends on the date as well. It is calculated based on the recent closed prices, For eg. if we have to calculate the MA for 10 days so we will take the sum of last 10 closed prices and divided by 10 will gives us the MA.",305129.0
91453,382755.0,Came across this youtube video on Simple Moving Average https://www.youtube.com/watch?v=x4nO5yoarM0,313826.0
91453,382582.0,"MA is moving avergae. it is termed moving because it keeps changing based on the values. for example, if we are to consider a 2 day moving average of a stock starting from today. then, todays 2 day MA would be averagr of todays value and yesterdays value. now, again tomorrows MA would be avg of tomorrows value and todays value. thus, since the price would have changed tomorrow, your value of avg would also change. it goes on to change like this. hence the term moving. hope that helps.",317998.0
92791,390780.0,`` is used to take into account for reserved keywords for mysql as column name and spaces in the column name. If you have a space in column name put it into backticks. `` Here it is used to create a column name with space in the name close price.,317689.0
92791,391006.0,with back tick SQL can differentiate user named column and reserved keyword..Within backtick you can put whatever name you want for your column headers even with space also ..as you might have noticed that if you are using any name even other than reserved keyword also you can not put spaces . Back tick is very important in this scenario,319869.0
92791,390784.0,"Hi , Back tick (`) identifiers are used to make the string between them as one unique entity. They are called quoted identifiers and they tell the parser to handle the text between them as a literal string. They are useful for when you have a column or table that contains a keyword or space. (Source: stackexchange). Please refer link for more information and examples : https://dba.stackexchange.com/questions/23129/benefits-of-using-backtick-in-mysql-queries Thanks.",305652.0
92353,388885.0,REfer the below link will give you some idea https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/,300687.0
92353,388925.0,"well, if you dont have previous SQL experience, then I agree that this question is little hard, so dont loose your heart. Now regarding the solution, first try to understand the concept i.e. you need the Buy/Sel signla at the point of intersection of MA20 and MA50. Try this out in excel first and then replicate the logic in SQL. Hint : Use Case statements. You also need to compare current row MA's with previous row MA's",304814.0
92353,388962.0,"hope this ll be a little motivation to get close to it.. I've been struggling with this assignment for last 5 days.. I'm a chemical engineer with no coding experience.. and so i spent huge efforts on understanding the logic, took help of google and then literally downloaded a free pdf copy on mysql and went through it.. imagined the if conditions with a function which will compare the previous rows values and will help me in getting the expected result.. sometimes i took manual notes and have plotted the chart numerously.. finally today evening i cracked the logic and got the expected results and i really felt very nice and confident after that.. a small hint since this was never covered in course, see how you can connect the LAG window function with IF condition (in select clause) to get the answer.. Sometimes its all about searching for a right keyword on internet and using it effectively to make it work.. and hence i didn't felt like making a noice.. i hope this helps in retaining a little hopes.. :)",316349.0
92353,389617.0,"I don't think the course is designed to teach everything, it also expects us to be explorers for some parts.",300717.0
92353,389115.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92353,390123.0,"LAG Function -&gt; The LAG() function returns the value of the expression from the row that precedes the current row So, when you use this function, you can compare second record with frst, third record with second and so on. So you need to use LAG function and then followed by CASE function and then you should be able to get the result. Now, i do not want to encourage you to straight go ahead and implement the two code together. First write code forCASE, it is easy. And then practice LAG . Once you undertsood with the code these two fucntions, then try clubbing them. Still an issue, please reach out to us and we will explain again with more clarity possible (Unfortunately annot provide th code :( )",301555.0
92353,390121.0,"Firstly, we try to include everything as much as possible in the course. Sometime, something gets missed and then based on your feedback like this, they are added in the course as well. So do not get dishertened. Now, if the concepts in the course are not clear and not covered, it is fine. Google and we TA's (both together) will help you solve your problems. :) This Assignment can be done using LAG function and using CASE Statements. Case Statement link to help you further- https://www.w3schools.com/sql/func_mysql_case.asp LAG function link to help you further - http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/ First we need to identify the logic to implement it. Ankit has explained that if you can plot or imagine it from Graph perspective. But the main idea is, 1.get the data sorted based on dates and key columns like stock. 2. Then compare second value with first value. In case it is greater or less or same, find the signal like BUY/SELL/HOLD. 3. This has to be done for all CASE function is used to determine or implement IF/ELSE scenario like the decision in step 2 can be done using CASE Statement.",301555.0
91071,380587.0,Refer this for importing csv into mysql : https://stackoverflow.com/questions/8163079/importing-a-csv-to-mysql-with-different-date-format,310974.0
91071,380642.0,You can import csv file data to mysql using workbench. Please refer below link https://dev.mysql.com/doc/workbench/en/wb-admin-export-import-table.html First you need to import all csv files with respective tables with tables names specefied by Upgrad. Then you can use Temp tables if you need for your analysys.,311952.0
91071,380644.0,"Hi Nagaraju, You can do this in 2 ways: 1. Write a create table statement to create a table containing columns from the CSVs. And then run a load from csv statement to insert the data in your table. Eg. LOAD DATA INFILE ""/home/paul/clientdata.csv"" INTO TABLE CSVImport COLUMNS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' ESCAPED BY '""' LINES TERMINATED BY '\n' IGNORE 1 LINES ; 2. Use the table import wizard(right click on the schema on the left navigation window) and follow the steps. Hope this helps.",310511.0
91071,380737.0,"You can import the CSV data to MySql table in two ways 1. Using MySql Workbench https://dev.mysql.com/doc/workbench/en/wb-admin-export-import-table.html http://www.mysqltutorial.org/import-csv-file-mysql-table/ 2. Using SQL statement LOAD DATA LOCAL INFILE 'c:/tmp/discounts.csv' INTO TABLE table name FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS;",317845.0
91071,380753.0,Please go through below link to 'import-csv-file-to-mysql-table' https://stackoverflow.com/questions/3635166/how-to-import-csv-file-to-mysql-table It will help.,317991.0
91071,381079.0,Right click on the Schema. There you would find the option Table Data Import Wizard. Use this option to import. This is skip some rows due to bad data. Hence should load using LOAD DATA INFILE command after creating the tables.,317689.0
91071,388663.0,"There are many option to do that. Primarily two is being used widely. 1. Using LOAD DATA INFILE command (Recommanded). By this command you can bring your local or remote csv file to any table. Benifits of using this is that you can change data while importing at your will and you can decide what to import and what to not. 2. If you are using mysql workbench, import table data wizard. In our case I will recommand first option as to import file.",317412.0
92465,389475.0,"CREATE TABLE new_tbl [AS] SELECT col1,col2,...coln from master_tbl;",307843.0
92465,389481.0,"You can get detailed explanation for ""CREATE TABLE ... SELECT Syntax"" on below link: https://dev.mysql.com/doc/refman/8.0/en/create-table-select.html Hope this will help.",317991.0
92465,389489.0,For creating new table for selecting different column of different table follow below link: https://stackoverflow.com/questions/2529656/how-can-i-select-records-from-two-different-tables-in-one-mysql-query or https://dev.mysql.com/worklog/task/?id=358 or https://dev.mysql.com/doc/refman/5.7/en/multiple-tables.html,310419.0
92987,391650.0,"if you are using Postgres SQL database, you will have a lot of syntax differences. I think it's better to use MySQL because the course is designed with MySql database",317845.0
92987,391631.0,"I believe as long as you are connecting to MySQL server 8,0 and above then it shouldn't matter which clent you use",300694.0
92987,393175.0,You need to use MySQL only/,319721.0
91083,380390.0,"Don't what query you are using. But if you are doing a set operation, do not use ""And"" between the columns. You just need to provide values with comma separation. UPDATE sometetable SET somecolumn1 = 'Some column 1 value', somecolumn2 = 'Some column 2 value' WHERE somecolumn3 = ""Some column 3 value "";",318368.0
91083,380647.0,you are inserting date value with double data type. Please change column data type to date in your table ddl using alter statement.,311952.0
91083,380751.0,"The link below gives few scenarios where one can get ""Truncate double error"". You can go through below links and check where problem is coming. https://stackoverflow.com/questions/3456258/mysql-truncated-incorrect-double-value https://stackoverflow.com/questions/16068993/error-code-1292-truncated-incorrect-double-value-mysql/16069446 Hope this will help.",317991.0
93089,391912.0,The SQL standard imposes a constraint on window functions that they cannot be used in UPDATE or DELETE statements to update rows. Using such functions in a subquery of these statements (to select rows) is permitted. MySQL does not support these window function features: DISTINCT syntax for aggregate window functions. Nested window functions. Dynamic frame endpoints that depend on the value of the current row. For more inof - https://dev.mysql.com/doc/refman/8.0/en/window-function-restrictions.html,312746.0
93089,391982.0,Window functions iterate over rows right. An if condition needs fixed input to evaluate. Think of passing a list of numbers versus just a number in the if(),318007.0
89007,368994.0,"Since it is assignment, I can't really give out the answer but general practice is to eliminate the nulls and you can do that using the mean value of the existing data. Please see if this applies to your case and then go forward. Good luck ☺️",310974.0
89007,378226.0,"Since, it is MA and mean doesn't make sense to replace the NULL values, ignoring those rows for signal generation is a better option and that is what I've in realtime signals generated on technical charts.",318329.0
89007,388171.0,"i think for null ,we hv to not gnerate any signal and keep signal field null ,",318005.0
92790,390783.0,"To answer your questions, First 50 rows don't matter. You can either fill them with null or just delete them. Only working days are considered. So, data on weekends is irrelevant. You need to give an executive summary of findings which will be backed from the facts you obtained from your analysis.",319721.0
93057,391811.0,1.you can only import date and close price but it would be great if you load everything for some self analysis. 2.signal should be calculated based on ma suppose the ma for shorter term is greater then the ma of longer term then we should check wat was the trend on previous day 3. i would say generate a generic code which would work for any stock on every date writing based on bajaj2 will give you answer but is not generic. 4. summary should be based on wat you found in a particular stock.,318017.0
93057,393203.0,"1. Just the required columns can be loaded. 2. The signal is generated only once. It's not an iterative process. 3. UDF is based on the signal table we generated. 4. The summary should be written taking care about the data, the analysis you performed and the results obtained. It should be summarized with analysis and inferences need to be drawn. The results you obtained from the analysis should be stated in the summary and explained in theory, accordingly.",319721.0
92395,389215.0,You are trying to import data from csv file directly as datetime datatype. This will not work as this is not standard format for date to be read into mysql. You need to handle this date conversion. Read it as string and than do a type conversion using str_to_date function.,317689.0
92395,389177.0,"if you're using table import wizard, use date columns datatype as string and then later convert it to date datatype using str_to_date function.. this error could be because SQL stores date only in yyyy-dd-mm format.. and not able to understand the date that you are passing from the csv file..",316349.0
92395,389193.0,"While importing at the end, use (@Columnname) SET YourDateTimeColumn = STR_TO_DATE(@Columnname, '%e-%M-%Y'),",317811.0
91485,382929.0,,317982.0
91485,383263.0,Can you share the metadata of the table please and few records from the table showing the values in the date column?,301555.0
91485,382712.0,Since this is an assignment question I would not ask you to post any kind of sql query. And based on your explanation I found below link that might help you. Please go through this links: https://stackoverflow.com/questions/12423287/date-field-not-sorting-correctly-in-sql https://www.sitepoint.com/community/t/dates-are-not-sorting-correctly-with-date-format/92194 Hope this will help.,317991.0
91485,382733.0,Please revisit the topic 'Advance SQL--&gt;Updating Table--&gt;Date Manipulation'. The initial write throws light on ordering columns containing 'Date' type data. Please find the link for the topic below: https://learn.upgrad.com/v/course/208/session/19878/segment/101126,313826.0
91485,382818.0,"You need to make changes in your date in order for the sorting to work. Sorting works differently in strings, integers and date data types.",317689.0
91485,382839.0,check the type of the date column - that might be the issue.,300694.0
92679,390468.0,MySQL has supported window functions since version 8.0.,319056.0
92679,390476.0,Upgrade to MySQL version 8 for windows 32 bit.,317689.0
92679,390561.0,Please install MySQL 8. The WINDOW functions are introduced in version 8.0. The MySQL installer comes with 32 and 64 bit together for Microsft Windows. I had also faced the similar issue and later upgraded to 8.0.,312479.0
92680,,nan,
91923,385383.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
91923,385467.0,For calculating 20 day MA follow below link may be it will help you: https://www.google.co.in/amp/s/www.essentialsql.com/sql-puzzle-calculate-moving-averages/amp/ or https://stackoverflow.com/questions/9784027/calculate-moving-averages-in-sql,310419.0
91923,387214.0,Hi .. I am facing the same issue can you please let me know how you resolved the issue,301641.0
92682,390438.0,Can you provide the SQL you are issuing?,318438.0
92682,390474.0,Please check MySQL version. Window function is supported for version 8 and above. If it is an older version please update it.,317689.0
92682,390487.0,"Please ensure you are using ROW_NUMBER function Before window you call this like below ROW_NUMBER() OVER (ORDER BY &lt;FIELD&gt; ASC/DESC) as RowSeq,",318454.0
91492,382820.0,"Try to capitalise it by tweaking the settings. however, it doesn't matter much specially because in the submission files we have to show queries from creation of Bajaj1 (and other tables) and subsequent tasks. Creation of schema and upload of data files as tables do not need to be part of the submission file.",311686.0
91492,385448.0,"It does not matter, as MYSQL considers both assigment and Assigment as same.",317811.0
92077,,nan,
91498,382814.0,You can set a default value as Hold or as Null as it is mentioned to ignore the days for which it cannot be calculated.,317689.0
91498,385435.0,"Yes, For the dates, where either 20dayMA is null or 50 day MA is null , It should be kept on Hold.",317811.0
91493,382806.0,I dont think spaces should be an issue while importing the csv. I guess you have to ensure that encoding of the import is UTF-8 and verify the data types prior to importing the data. Hope that helps!,318085.0
91493,382817.0,you can use the backticks`` to load the columns which have space as shown in the video.,317689.0
91493,382832.0,"you have a few options: * refer to the column names with space with backquotes e.g `column name` * rename the columns after the import proce by using ALTER TABLE tableName CHANGE ` oldcolumnname ` ` newcolumnname ` datatype ( length ); * create your own table and column first and then use those columns in the wizard * use sql to import the csv, instead of the wizard, then you can decide how the columns get mapped during the import process",300694.0
91511,382900.0,"Hi, 1. as long as your output is correct i dont think so it matters, plus mysql is not case sensitive so it will run same on both. 2. yes, we need to create seperate tables for each but with the columns name same so yes need to include sql query- only the data is taken from the respective tables. Hope it helps..",305129.0
91001,379790.0,"Hi, For your reference: https://www.quora.com/How-do-I-import-a-csv-file-in-Python",311363.0
91001,380107.0,"It seems, there is truncation for Open Price column, as double.Inc ase you want to check , you can create thiscolumn as String and then convert it to double. Like create a new column open price tempof type doublem, load the data from open price to open price new and then you can find the error. If it is successful, you can rename it. This could be done in same table using alter table commands. Just an indirect way of doing it",301555.0
91001,380618.0,"Hi, you can change the column name in csv file and change the format of the column as per sql standard and then import the data. It will work. Thank You.",308965.0
91001,381078.0,Try to import the tables from MYSQL Table Data import wizard. This option can be found by right clicking the schema. You can specify the data type there or do the type conversion later after loading the data.,317689.0
91001,381144.0,"instead of double, use float(10,2). Worked for me.",318329.0
91001,381673.0,"Sometime,there is a mismatch in understanding the available data and hence you get all thse unnecessary errors. Esp whe you loading it frm file.",301555.0
91001,391192.0,I am unable to convert Date from text format to date format. Did you get how to do it?,317990.0
90810,378450.0,Yes write it and comment it the statement used however it is your choice for and forward ahead for the solution.,307843.0
90810,378452.0,No. It is not needed. However if you are importing using some statements you can let it be. Grading criteria starts from creation of new tables.,311686.0
90835,379611.0,I know CROSS is the terminology over there in the financial domain but it makes it easier for ppl solving this if INTERSECT is used instead.,310974.0
90835,388036.0,,318005.0
90835,379395.0,"I think, signal should be ""buy"" only the first tim e the short term MA beomes greater than long term MA . Thats the time when the MAs cross each other. Similarly signal should be ""sell"" only the first time the long term MA becomes greater than short term MA.",310467.0
90835,380099.0,"Being assignment, cannot definitel give a guideline or answer directly. But you can use your own understandin and methods based on your implementation capability, like someone can write code to analyse each time CROSS or INTERSECT, depending on their coding extent and capabilities. However, try to provide your comments. However, as per the problem, it seems it is talking about CROSS more the INTERSECT",301555.0
90835,386524.0,"Hi All, Can you help to understand more on this CROSS.. Thanks.",307494.0
92685,,nan,
92619,390246.0,Solution from 1 to 5 is must. you can put code of the earlier operations as well if you wish.,311686.0
92619,390250.0,"Hi , Yes , it is better to mention the code from starting i.e. from create schema till create function. Zip it with pdf analysis document. Thanks.",305652.0
92619,390255.0,thanks all for your response.,320103.0
92619,390251.0,"You have to submit the well commented SQL file and the PDF file for the Executive summary. For the SQL file, provide the comments for all the queries that you write, so that they understand what logic is applied in the query.",316202.0
92532,389817.0,"If no data is present for missing days, then how would you calculate the avegage. just take previous 20 days and calculate the average. Note: Share market is not open on weekend, so no data would be present. Keep things simple",304814.0
92532,389844.0,"Do not consider missing date, or impute data for missing dates. Calculate Moving average on the data present.",318554.0
92532,390324.0,You dont need to worry about the missing dates. Use all the rows that's provided in the table and compute the Moving averages accordingly. E:G - Bajaj table will have 889 rows. We need to consider all these while calculating the averages and we can ignore the missing dates. Just consider the previous 20 days and previous 50 days to calculate the moving averages accordingly.,316202.0
92999,391724.0,You can validate all the outputs by yourself using the excel sheets. It is fairly easy.,312376.0
92999,391976.0,This is what i did -,312376.0
92999,391957.0,"I used the below logic for bajaj1 my sell counts and buy counts are diff, could you pls tell where am i going wrong.",306735.0
92999,391679.0,Buy Sell Bajaj 12 11 Eicher 6 7 Hero 9 9 Infosys 9 9 TCS 12 13 TVS 8 8,316349.0
93597,394388.0,"adding a few more..... 4) The installation of SQL became night mare for many students and instead of focusing on the ""data science"" questions, many were solving the installation issues. This should be a better way of handling this problem. 5) Simiarly, there should be a clear instruction on the data load, whether it could be done through IMPORT WIZARD or SQL Query or either is fine.",312758.0
93597,394611.0,"The general info, like the table names and columns names, should be clearly mentioned as a part of the assignment notes. Set the expectations right from the beginning. Very important point mentioned by Ashwini. Installing right version of mySQL workbench and import of data was a nightmare.",312376.0
93597,394797.0,"Hi Anshul and Ashwini, I had similar complaints as like you guys have but I understood that it's me who overlooked the assignment. Now I don't mean you overlooked it too :) However, following are few of my hunches on your concerns. Answers for your question in bold. 1) It was not mention to have exact column and table names. I used underscore in table and column names, but had to redo entire code. One of the TA mentioned to have same column names and table names only couple of days before I remember this has been already told in the assignment description clearly 2) Usage of function is also not very clear. We are asked to identify signal in Q3, then what is the point of creating same UDF in Q4? This added lot of confusion. The UDF acts as a utility to return the Sginal given a date. This is how API works in real scenarios 3) For all other questions, we were asked to create 6 tables one for each stock, but UDF was specific to bajaj. I created a generic function then had to redo to keep it specific for Bajaj. I agree on this could be made generic! Rather than hard coding just to Bajaj. 4) The installation of SQL became night mare for many students and instead of focusing on the ""data science"" questions, many were solving the installation issues. This should be a better way of handling this problem. Agree. Still it's the part of learning 5) Simiarly, there should be a clear instruction on the data load, whether it could be done through IMPORT WIZARD or SQL Query or either is fine. Data loading is upto us, can be done in any possible way. Thanks, Nyamath",318080.0
93597,395595.0,"Hey guys, I understand that you have a few concerns regarding the assignment. First of all, thanks a lot for sharing the feedback. This will help us make the learning experience even better. There are few points I agree with you guys too and they will be taken care of in the future. And Nyamanth has already mentioned a few points I too wanted to address. Let me add my views on the first point, ""There was a rubric which said in the segment on Grading Criteria which said, ""The names of the fields are marked correctly."" This was a little ambiguous, I agree. But from our end, we thought, it'd be conveyed like the names of columns which were given would be used. However, there have been a lot of questions on the discussion forum regarding the same. And that's when I realised I need to generally address this to everyone and hence I put up a post explicitly stating that the exact column names should be used. One more point I need you guys to understand is that there would be a lot of assignment submissions. And as you're aware, these assignments are graded manually to provide a personalised feedback to each of you. So, if you guys make all your assignments in a consistent way, it'd be easier for the grader to evaluate your assignments. So, that to maintain consistency with row and column names was a requirement. """,319721.0
92535,389815.0,"If you referring to stock tables, then data is not availabe for weekends. In your case, there is data for 26 and 27 July, but not for 28 and 29 because 28/29 is a weekend. Regarding price, there are multiple columns containing different prices such as Open Price High Price Low Price Close Price",304814.0
92535,389842.0,As the question mentioned if data is missing (in this case weekend/holiday records are missing beacuse those days sensex is closed) we should not do imputation. And as per the question the analysis is based on Close Price.,318554.0
92535,389995.0,Close price is price for specific day. Use the rank to generate days column and use that for yr calculation. In that way you will not have to handle the missing date.,315679.0
92535,390323.0,"""Close Price"" is the price for the day. You dont need to worry about the missing dates. Use all the rows that's provided in the table and compute the Moving averages accordingly. E:G - Bajaj table will have 889 rows. We nned to consider all these while calculating the averages.",316202.0
91875,385221.0,"hi, I asked this question some days back. as per TA's verification, we need to share some overall analysis about those 6 stocks. there is no fixed format. you can discuss all of them and 1-2 specific as well if there is some special observation you can find. just make sure to substantiate your conclusions/observations with data and facts. you can also talk about the approach you followed in arriving on these conclusions of space permits. it is open ended and to check our capabilities to interpret data.",311686.0
92539,389841.0,I believe it can be any format as long as you are consistent. And you should store the date in same format in the table also to be in synch.,318554.0
92539,390318.0,"Use the same format for Date that you would have used while creating the ""Signal"" table. Because your function will retrieve the date from the table and it throws an error if the data type is not matching.",316202.0
92540,389839.0,Not Sure what caused the issue. it got automaticlly fixed.,300708.0
92540,391087.0,"Here's something that might explain the error, If you get Commands out of sync; you can't run this command now in your client code, you are calling client functions in the wrong order. This can happen, for example, if you are using mysql_use_result() and try to execute a new query before you have called mysql_free_result() . It can also happen if you try to execute two queries that return data without calling mysql_use_result() or mysql_store_result() in between.",319721.0
92683,,nan,
90932,379191.0,it should not come on localhost. try restarting your MySQL workbench.,317689.0
90932,390404.0,"Hi Darshna, 1) don't use table name as master_table try some different name 2) use inner join to join two tables hope it will help you with any issues",306242.0
90932,379266.0,"Hi Darshna, As mentioned by Arihant, this error is not common on localhost, since we have a very fast connection with the database (both being on the same physical device). However, this might happen due to connection timeout if your query is taking very long time to operate upon. You can: Try to optimize your SQL query. Try restarting MySQL (using MySQL workbench) | https://database.guide/how-to-stopstart-mysql-using-mysql-workbench/ Try to check and increase the connection timeout time | https://stackoverflow.com/questions/10563619/error-code-2013-lost-connection-to-mysql-server-during-query",317987.0
92688,390453.0,check the date you're passing in your UDF if it is present in the Bajaj2.. you may want to handle the dates which arw not present in your bajaj2 dataset..,316349.0
92688,390469.0,See you have declared N as date variable and also you are passing the date value in YYYY-MM-DD format Also ensure you are RETURNing value from UDF,318454.0
92688,390472.0,Please check the format of the date in table bajaj2 and UDF. In UDF you would be passing it in date format and in bajaj2 it might be string. Hence causing issue in comparison.,317689.0
90752,378191.0,"You are right. It will not come as empty. Note these lines mentioned in the 'Data Set' page of the assignment : 'Please note that for the days where it is not possible to calculate the required Moving Averages, it is better to ignore these rows rather than trying to deal with NULL by filling it with average value as that would make no practical sense.' I understand we can ignore those rows even if they are not empty.",311686.0
90752,379496.0,The initial days when you CAN NOT calculate the moving averages should be ignored. Please note that the signal has to be generated when the moving average lines CROSS only.,304281.0
90754,378189.0,"While uploading the file, it should automatically consider it as date. But if not happening, you need to change the type of the column in your DB.",311686.0
90110,375239.0,"Use the below code: pd.read_csv(filename, usecols = [0,1,2,3,4,5,6]) filename --&gt; Enter the name of the file usecols --&gt; You can either give the index or if you know the names of the columns that has to be loaded, please specify the column names. This will work.",318084.0
90110,375253.0,The topic selected is related to MySQL.(Assignment-Stock Market Analysis). I was taking wrt MySQL!,317993.0
90110,375274.0,"LOAD DATA LOCAL INFILE 'file.csv' INTO TABLE t1 FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' (@col1,@col2,@col3,@col4) set name=@col4,",318017.0
90110,375385.0,"Use below statement. Provide all required column at the end of the statement. LOAD DATA INFILE '/tmp/db.txt' INTO TABLE test FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' IGNORE 1 LINES (id, mycol1, mycol2);",318368.0
91997,385962.0,"We have to update the signal field with 3 possible values - BUY, SELL and HOLD. When 20 DMA moves over 50 DMA, then it is BUY, after that 20 DMA will continue to be over 50 DMA, but the signal value should be HOLD. Similarly when 20 DMA goes below 50 DMA. Hope this is clear now.",318084.0
91997,386702.0,"Hey, For question number 3; can this be achieved with a single update statement? or we've to use the procedure with if conditions? I updated the column with buy and sell if 20MA&gt;50MA and 20MA&lt;50MA but struggling with first value as Buy and sell and other as hold.. please help with some hint or logic..",316349.0
91997,386799.0,This can be achieved using windowing/frames Lag function...,318084.0
90991,379755.0,"Hi Chnadan, I think it should be the overall analysis on the assignment, explaining the method you followed in arriving at the results and the inference you draw from it. Hope this helps.",310511.0
90991,380103.0,"Its a report. Can be on multiple factors, not necessarily on one point. You can show your understanding with the data points to support it.",301555.0
90991,380129.0,I don't understan the purpose of the master table. How can it be used in the analysis if it doesn't capture 20/50 Day MAs and the signal?,310974.0
90991,380131.0,that is just to compare the different stocks I think,311686.0
90991,393655.0,As earlier in our learning journey of sql in the end of every seesion a 250 words takeaways required ..it is pretty similar to that ..we just need to provide our observations and learning while doing assignment,319869.0
92405,389169.0,"hi, hope the explanation provided in the attached screenshot will be of help.",311686.0
92405,389261.0,"Hi Manasa, If the difference of 20dma and 50dma for (n-1)th row and nth intersect each other, ie become positive from negative or vice versa, it generates a Buy or Sell signal respectively. Hope this helps.",310511.0
92405,389162.0,"20MA need to cross 50MA when compared to previous days,if the previous days also same trend(rising or declining) then it's a HOLD",301115.0
92405,389188.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line",317811.0
97365,416759.0,check the lag function on google,318017.0
97365,416831.0,"thank you, I got it.",311046.0
89305,370319.0,I don't think so. Any version should be fine but it's better to take the latest one.,310974.0
89305,390760.0,you need to install mysql 8 version as windows functios nt work prior to 8 version,304812.0
91560,383235.0,There should be only one record per date. Moving average has to be calculated on the Closing price. Hope this is clear now.,318084.0
91560,383320.0,we need to calculate the moving averages based on the closing price of the stock.,317689.0
93087,391903.0,"Sum(&lt;column name&gt;) function needs column name whose values you want to sum up. for e.g. select ssn, concat(fname, ' ', lname) as emp_name, dno, salary, sum(salary) over (partition by dno order by ssn rows unbounded preceding) as cumulative_total, sum(salary) over (partition by dno order by ssn rows between 1 preceding and 1 following ) as one_above_and_one_below from employee; Please check above example and try to embedd your logic into it.",312746.0
91005,380064.0,"For 20 MA, you would have store average of previous 20 days in a column and similary, average of previous 50 days for 50 MA in another column.",318329.0
91005,380192.0,"To caculate 20 Days and 50 Days MA at the same time, we can use windows function (please refer session 3 of Advanced SQL) . These functions preserve individual characteristics of different rows while displaying the group characteristics simultaneously.",310467.0
92543,389861.0,I tried to make this work for a lot of time and finally went with mysql CLI (Command Line Interface). I recommend you to do the same.,310974.0
92543,390269.0,In workbench 8.0 you should use LOAD DATA INFILE dont use this LOAD DATA LOCAL INFILE..,300726.0
92543,391098.0,"Hey you can do it manually using the UI too. After loading the schema, you can import the data from the Table Data Import Wizard.",319721.0
91935,385629.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date. Then you will get null for first 19 values in MA20DAYS COLUMNS AND NULL FOR FIRST 49 VALUES IN MA50DAYS COLUMN.,317811.0
91935,385487.0,That's totally fine. Proceed with the next steps.,310974.0
91935,385635.0,"Proceed with further steps,Once the basic assignment is finished fine tune the code with CASE statement as CASE statement would have been used in futher steps",301115.0
91935,386426.0,what is CASE statement?,310509.0
92547,389875.0,I hope the issue is with total turnover column data at that row. Try using the datatype as Double.. if it is not working youc can import the data as string and then update the datatype to double. Hope it helps,300727.0
92549,389888.0,i guess using join itself is a optimized query as taught in Query Optimization session.. i cannot recollect of any other easy way then joining them..,316349.0
92549,390071.0,"you can use a few ways - implicit join and also nested queries, and some more advanced ways also 'with', etc but why not to use join if that makes the most sense e.g. select a.column1, b.column1 from a,b where a.id=b.id",300694.0
92549,390294.0,"I don't think of any other efficient way apart from join query itself. For creating the master table, join function works very well.",316202.0
92549,390570.0,"You can use WHERE clause, but JOIN is better. It gives you a very cleaner approch for writing a SQL statement.",312479.0
92551,389898.0,"if Possible can you help with an example syntax on how to frame the query ? Just the body how it looks like ... Hope, i am not asking you the exact query :)",300727.0
92551,389908.0,Refer implementation of LAG here. Can't post the actual implementation as this is a graded question. http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/ In MySQL 8.0 or above LAG function is supported.,317689.0
92551,390044.0,"use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92551,390290.0,"As this is a graded question, providing the query is not possible. Below is the logic you can apply: You need to use both CASE and LAG functions together in your query. You can write your conditions to check if the 20 day MA is above 50 day and your previous row's 20 day MA is below the 5o Day MA. Then signal will be ""BUY"" Write the similar logic for ""SELL"" and rest all will be ""HOLD"" The conditions should be within the CASE and END function. Below is the link for how to use CASE function; https://www.w3schools.com/sql/func_mysql_case.asp",316202.0
92551,390842.0,"I am using update table, inner join , set signal value with where condition. its with same day, how to get privous day data here. please help",312019.0
92551,390889.0,"LAG and lead function is best function to do the comparision within the rows of the same column, You us it in the CASE function to create a condition and update the results",310629.0
92551,390908.0,https://stackoverflow.com/questions/15766102/i-want-to-use-case-statement-to-update-some-records-in-sql-server-2005 This link will help,312019.0
92551,389886.0,you can use LAG in select clause itself with IF conditions.. not sure if mysql has got a bug or something since i ran the same logic with cases; it keeps on generating an error 'field list column doesnt exist.. even when it exist without an error.. and mysql syntax looks without any warning..,316349.0
91600,383518.0,Oh. You can put it as 'perc_deli_qty_to_traded_qty'.,304813.0
91600,383531.0,"Although, technically you can have columns with spaces in names, its usually a better practice to have some kind of connecting delimiters like '_' or '-' instead of spaces while ensuring meaningful column names. As frs as the assignment is concerned, its not necessary to rename the column name.",318085.0
91600,383927.0,It is not true that we cannot change column names - there are many ways of changing the SQL column names but not changing the xlsx file,300694.0
91600,383979.0,"You can alias columns as Select ABC as "" Column is ABC"" from table Note: I have provided the alias column in quotes. This way it is possible. However, in practice we try to have it with Underscore.",301555.0
92550,389877.0,load data infile is the command,301115.0
92550,389909.0,It seems like there is a problem while you are writing LOAD data local infile. Either try using load data infile or refer to following for the fix. https://stackoverflow.com/questions/18437689/error-1148-the-used-command-is-not-allowed-with-this-mysql-version For the load data infile to work you need to make some changes to my.ini file and restart your mysql server. https://stackoverflow.com/questions/32737478/how-should-i-tackle-secure-file-priv-in-mysql,317689.0
92550,390227.0,use LOAD DATA INFILE 'PathofFile' Remove Local Keyword,317811.0
92550,391271.0,It's alright even if you use the UI to load the tables from the Import Wizard. You could get it loaded right?,319721.0
91615,383641.0,I did it. in fact I rounded off up to only 1 digit after decimal based on the example shared for MA. hope it is correct.,311686.0
91615,383983.0,You can roundoff to 2 decimal,301555.0
91945,385607.0,"hi, you need to generate the signal BUY when 20 DAY MA becomes greater than 50 DAY MA. but it will be BUY only for the 1st time it becomes higher. In all subsequent higher 20 DAY MA you need to generate HOLD. Now when for the 1st time in any subsequent day 20 DAY MA becomes lower than 50 DAY MA then it will be SELL. For all subsequent records when 20 DAY MA is lower than 50 DAY MA it will be HOLD. So basically whenever there is a 'transition' of 20 DAY MA to higher or lower comparative to 50 DAY MA there will be BUY or SELL. otherwise it will be HOLD. hope I could explain it properly.",311686.0
91945,385661.0,"For Concept of generating signal, Let say Try plotting a graph having x axis or categorical values are date sorted in ascending order and y axis or measures will be both 20DAYMA and 50DAYMA. You will see that there will be some points where 20DAYMA line will cross 50DAYMA line in graph. Those cross points are the dates to buy or sell. Rest state of line either 20DAYMA line is below or above 50DAYMA line are hold dates.",317811.0
91945,385631.0,Use tables created in part 1 : eg- Bajaj1 to populate the data into Bajaj2 signal generation logic :it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal.,301115.0
92193,388292.0,"Good question! Format - I think for a robust code, you can take date value in any format and conver it in the bajaj2 date format. But as far as the assignment is concerned, we should be good if we only consider bajaj2 date format. Value - I guess if is any other than which is present bajaj2 it should return NULL.",304813.0
92193,388371.0,Input date conversion can be taken care during importing of data in which case the input for user defined function will be just Date format and is common format across all tables,318329.0
92193,388404.0,You can handle the date format while importing data to main tables so that all tables would have same date format.,317689.0
92193,388517.0,UDF needs to be created to search data in bajaj2 table.,317811.0
92193,388623.0,"Function should be created to return results from bajaj2 table for inputs from date column , otherwise it should return null.",305652.0
92100,387557.0,"It should import 889 rows for each csv file per table. Otherwise there is some mistake. Check for DateFormat and column having missing values , Handle these both it will import all 889 rows.",317811.0
92100,387907.0,"Hi Kunwar, You should use command line to load CSV (craete table + load data infile) , if you use import wizard than rows count will be less as wizard will silently ignore the rows if there is any mismatch in settings (File data format , string quote character , file encoding etc). So it's always preferable to use command line to load CSV , incase if you are using wizard. Hope this helps.",305652.0
92100,387242.0,"If the count is different, then it means that there is some mistake in the process of importing the data. You could identify the rows which were not imported and check for what reasons those rows were not imported. For this, you could export your table data to a csv and do lookup with your source data to find the missing rows. Hope this helps.",313826.0
92100,389160.0,"analayze the error shown ,you will find insights to solution,,,",318005.0
91949,385667.0,"Yes, you are right but if you looks looks closely, you will find that we are using closing pricde of 10th day as well for 10DAYMA calculation, but this 10DAYMA will be avaiable to us after closing of 10th day i.e. on 11th DAY.",317811.0
91949,385630.0,"hi, MA in real world is also calculated on Closing price. so you are right in saying that for 10 day MA you will get the value on 11th day only. the primary purpose of MA is to see the trend by removing noises. the Buy and Sell signal generated in corssovers days is one of the strategies to use MA trend. but the signal for a particular date can be generated on the next date only. as said it is used as trend and this next day generation of signal is thus okay. here is a link explaining MA concept in detail : https://www.investopedia.com/articles/active-trading/052014/how-use-moving-average-buy-stocks.asp",311686.0
92765,390643.0,You can generate HOLD signal for the NULL values,310974.0
92765,390746.0,You can skip the NULL values as it doesn't make any sense to show any signal.. but just to avoid the complexity in code; mostly everyone is setting this up to 'HOLD'.. you can take a call of your own here..,316349.0
93055,391810.0,"Hi,When the signal is neither buy nor sell, it is classified as hold. If you already own the stock, keep it and if you don't then don't buy it now. refer problem statement in assignment",319056.0
93055,391816.0,a signal will be hold in the following case : suppose your ma20 is 2000 and ma50 is 1999 then you would think it should be a sell but wait and check the ma20 and ma50 on the previous day if the trend is ma20 &gt; ma50 then you should hold the stock the market is not changing so if the previous day has ma50 &gt; ma20 then you should set it as sell,318017.0
93055,391837.0,I think my intention hasn't been communicated clearly. I have derived the signals properly in the bajaj2 table. What I intended to ask is about the UDF that we are going to write which will take a DATE as input parameter and return the signal for the concerned date for bajaj stock. What will happen if a date is not present in the table ? Suppose a date of year 2014 ?,318756.0
93055,391865.0,"It depends on how did you handled first 19 rows for 20 Day MA and first 49 rows for 50 Day MA, 1. If you have put NULL for first 19 rows for 20 Day MA and first 49 rows for 50 Day MA then UDF created will return NULL for the dates which are not present in bajaj table. 2. If you have ignored first 19 rows for 20 Day MA and first 49 rows for 50 Day MA i.e you generated the values but you did not take these rows while generating signal then in this case UDF created will return the signal generated (mostly Hold) for the dates which are not present in bajaj table. Hope this clarifies your doubt.",317991.0
93055,391938.0,Similar question : https://learn.upgrad.com/v/course/208/question/92112,318756.0
93055,392543.0,I had asked the similar question ( For today's live discussion : Do we need to add any conditional check in the function ?) https://learn.upgrad.com/v/course/208/question/93184 . This might help.,312479.0
92085,387054.0,You can also use import data option of work bench .. in that just guve the path and click on import. you will get the table created with field mentioned in header.,318802.0
92085,386896.0,"at the time of import when using load command. SET DATE COLUMN by Tell the import wizard using STR_TO_DATE what is the format of the date in the column in csv file. i.e. It is like 17-August-2018 means date format is '%e-%M-%Y' Please refer below link, It will solve your problem. https://stackoverflow.com/questions/8163079/importing-a-csv-to-mysql-with-different-date-format",317811.0
92085,386889.0,Pls specify What is the datatype of date column defined for your table where you are importing? Also if you are using windows then use LINES TERMINATED BY '\r\n',307843.0
92085,386945.0,There are two options. First declare the type as text and import second follow the below to set date function while importing as per below link http://www.mysqltutorial.org/import-csv-file-mysql-table/,307843.0
92085,387003.0,Is it resolved?? I'm also faccing the same issue.,311004.0
92085,387490.0,Import as Varchar and then you can convert it accurately. The numberof rcords is 889,301555.0
92085,387735.0,"Best option for new users is to Table Data Import Wizard .om MySQL workbench You can do right click on tables and see below options Create Table Create table Like Search Table Data Table Data Import Wizard Refresh All Use this method ' Table Data Import Wizard' , it is just uploading a file .",311861.0
92085,387804.0,"use data type of date as char(30) in the table before importing. After importing update table with set date=str_to_date(date,'%d-%M-%Y'); after importing you can change date back to date type.",318005.0
92085,388502.0,Please help me to calrify below doubt. 1. May I do all the operating in sql assignment as keeping date in varchar ?. 2. Mai I keep id as primary key in tables for this assignment.?,318497.0
92085,388665.0,"Use below optino while importing table. set nullif(@Column_reference_name,'') by this MySQL put null where it find empty string while importing data.",317412.0
94034,396282.0,you have to convert only permalink and company permalink,318017.0
94034,396260.0,only convert the specific column values instead of entire dataframe.,317811.0
91650,383713.0,Yes its 888 rows are returned.,305129.0
91650,383715.0,But file has 890 including the column names.,300687.0
91650,383775.0,"I also had faced the same issue. I verified the row which was not imported and got to know the reason for its omission. Further, I made some changes to the table structure and imported the records manually using 'LOAD DATA INFILE...' command and could import all the records to the table. Please note that the first row in the csv is a header and should not be imported into the table.",313826.0
91650,383935.0,889 rows should be imported -&gt; hence you might consider another way to load the csv file besides the wizard as the wizard will not be able to load all the rows,300694.0
91650,385335.0,"One row that is not importd is the columns heading row, which is good. The other row not imported is the one with no values in two columns. Hence you will see this issue in wizard import. You may have to consider alternative to import.",318007.0
91958,385677.0,"Looks like the connection is lost in middle make sure your MySQL sever is running. In Navigator, click Administration , under INSTANCE click startup/shutdown and you will see the server status. If the server is not running start the server and try firing your command. Hope it helps !",306735.0
91958,385734.0,"It seems your server is not up. Use below command in cmd to start the server : C:\&gt; ""C:\Program Files\MySQL\MySQL Server 8.0\bin\mysqld"" that &lt;path of mysqld service on your system &gt; For more information visit below link - https://dev.mysql.com/doc/refman/8.0/en/windows-start-command-line.html",312746.0
91958,387528.0,"use services.msc from run in window ,to start mysqlserver from sevices if that was stopped",318005.0
91958,389763.0,"I believe it's not the problem with sql serve, other queries are running fine, the problem is with this particular query. can anyone suggest me the other way around?",312756.0
91958,391272.0,use joins,308495.0
92106,387176.0,You can do that make a note of this assumption when you do the submission. This is what TAs have be recommending for similar queries.,318084.0
92106,387143.0,"not necessary.... one day 20 DMA could be less than 50DMA and the next day it could go up, thereby creating a crossover.",318084.0
92106,387158.0,can be generate signal only after valid 20 ma and 50 ma value comes because 50 day ma will comes on 50 day onwards,318005.0
92106,387234.0,"The signal to BUY or SELL has to be given only when the 20DayMA crosses the 50DayMA. For rest of the days, the signall should be HOLD. This is as per my understanding.",313826.0
92106,387488.0,"i analyzed the data with excel ,and yes both ma avg has not be same on some date for crossover. i understand now crossover nd signals. thanks",318005.0
92106,387456.0,"Understand Concept of Signal by an example of line graph plotted below:- Plot a graph having x-axis/categorical variable as Date sorted in ascending order. Y axis will be for both 20DAYMA and 50DAYMA. Join all the points for 20DAYMA to draw a line. Similarly Join all the points for 50DAYMA to draw a line. Now, you will see these two lines cross each other at some points. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively.",317811.0
92498,389606.0,"as per the instructions, you can ignore the values coming as per your default logic in the rows where ideally it should be null.",311686.0
92498,389611.0,doesn't mattwr actually if you keep it null or ignore it since that is ultimately going to generate a signal as Hold..,316349.0
92498,389773.0,its better to calculate the null or else it can lead to generate the wrong signal.. also after setting the 20MA values you can null the first 20 values at a later part..,305129.0
92498,390073.0,It will be hold for null values in either column 20DAYMA or 50DAYMA,317811.0
92498,390333.0,You can keep the initial 49 rows as null for the signal. As the actual comparison between 20 Day and 50 Day MA will start from 50th row. So computing the signal can start from 50th row onwards.And the initial 49 rows can be retained as NULL.,316202.0
92498,390522.0,"The question does not describe the NULL value for 20 day MA and/or 50 day MA. The question ask to calculate a signal based upon the comparison of both values, it could be either lesser or greater. So, It's better to leave initial 49 rows.",312479.0
92498,390955.0,"Also, please note that ""Please note that for the days where it is not possible to calculate the required Moving Averages, it is better to ignore these rows rather than trying to deal with NULL by filling it with average value as that would make no practical sense."" has been mentioned in the problem statement.",319721.0
92499,389681.0,"Headers are just the alias, for that particular header the value stored is nothing but the close price of that stoc of that date. Join all the tables using date, and get the close price of each stock. Keep the stock name as alias for the close price of respective stock.",304814.0
92499,389610.0,use alias to name the column headers and use join to bring the required data from the tables..,316349.0
92499,389769.0,"Hi, refer to the join concepts which are used to join the tables together to pull the data which so ever is required.. Joins in mysql- https://www.w3schools.com/sql/sql_join.asp",305129.0
92499,389949.0,"You can use the syntax "" Create table tablename AS ((select query)). This will create the table with same column name as mentioned in the select query.",318448.0
92499,389989.0,Use CTAS statement and alias the close price from respective table to label the column header.,315679.0
92499,390331.0,"Use the Create Table As query along with the inner joins to create the master table. Date is unique and common column in all the tables. So, you can use Date as a condition for joining the tables.",316202.0
94032,396261.0,"while importing the csv file, SET Date = STR_TO_DATE(@e_date, '%e-%M-%Y' )",317811.0
94032,396272.0,you need to convert the type of the column from string to date.,301648.0
94032,396284.0,first load using import wizard then update str to date then modify using alter,318017.0
94032,396315.0,Use str_to_date to convert the string to appropriate date format. Than use alter table command to change the datatype.,317689.0
94032,396592.0,"Convert all the existing values on the Date field to Date type first update upgrad_assignment.bajaj set `Date` = STR_TO_DATE(`Date`,""%d-%M-%Y""); Then update the data type",312160.0
94032,397192.0,"turn of the safe mode by setting, SET SQL_SAFE_UPDATES = 0;",319721.0
91676,383859.0,"Since this is a graded question, I'm not giving you the answer straightaway. Avg can be used. When you say the data is getting added randomly, please check that data once and change the no of rows preceding and check what values you get. You should be able to find out where it is going wrong.",318084.0
91676,383919.0,I wrote a query to update the MA in bajaj1.. now facing a new issue 'The target table bajaj1 of the update is not updatable'!! where am i exactly doing wrong?,316349.0
91676,384162.0,"Avg will give one result. It seems you are trying to update columns with avg. Thiscould be an issue if multiple columns are to be updated wth same valus.Cannot analyze any further without the sql knowledge. Hence,you might have to try debugging yourself or connect with me offline",301555.0
91676,384186.0,Thanks All - solved this with a different logic!!,316349.0
91676,385317.0,"Hi, I am getting moving average for all the row for calculating 20MA. How to make first 19 rows as blank. Please guide.",307494.0
91692,385413.0,"Maya, I think you should acknowledge the answer that helped you move forward or is something that will help others. There are folks simply answering something not covering the point of question. This will help knock off the noise.",318007.0
91692,383932.0,depends on your definition of exclude In my opinion - shoud you exclude it from being loaded? I believe you should load it should you exclude it from calculations for MA? I don't think it is relevant for that,300694.0
91692,384110.0,As per the guidelines we need to exclude rows in below scenario,301115.0
91692,385050.0,"Hi , Could you please check this error while loading data from csv , I am facing below issue: Error Code: 1366. Incorrect integer value: '' for column 'Deliverable Quantity' at row 230 There is no data (null) at 230th row for column 'Deliverable Quantity' Any suggestions ?",305652.0
91681,383882.0,"is 6 stocks mean ""bajaj"" , ""tcs"" , 'infosys' , 'tvs motors', 'eicher motors', 'Hero Motocorp' tables ? And Moving Average calculation is done on individual record for each table based on 20 day or 50 days with respect to Date &amp; Closing _price. How is it realted to other stocks ?",301108.0
91681,383867.0,You need to create a new table for each of the 6 stocks that have been provided for analysis. The new table should have the following columns: 1. `Date` : Reocrd Date 2. `Close Price` : Closing Price of the stock on the rocird date. 3. `20 Day MA` : 20 days moving average of the stock on the record date. 4. `50 Day MA` : 50 days moving average of the stock on the record date. Hope this clarifies.,313826.0
92098,387026.0,i guess to complete the assignment it doesn't matter you make it a primary key or not.. anyways date can be used while joining two tables without making it a primary key..,316349.0
92098,387541.0,"Yes, when creating table bajaj2 and others ie.e table havng date, close price and signal, Create Date as PRIMARY KEY as udf function will search faster in the signal table on basis of date.",317811.0
93085,391901.0,"Use Refresh button after executiing the function. Your function will be shown under ""Functions"".",312746.0
93085,391911.0,"If you try to re-run ""create function command"" you will see error coming that function already exist. If the error comes it means function you created exist. Then you can refresh the schema as shown above by Alok. If the error doesn't come then your function will get created. Hope this will help.",317991.0
92099,387507.0,"No checkon this implies, you can choose your way o importing as long as the data does not get affected and the number of records are same. And your queries gives the same and correct result when executed on my machine having the same data, might be imported differently (if both of us have not damaged the datatype and data)",301555.0
91969,385857.0,"The error is telling that you have lesser number of columns defined when compared to the data you are trying to insert. A very general example - you have columns A1, A2, A3 and your data row has Data1, Data2, Data3, Data4 Hope this helps.",317149.0
91969,388606.0,"Hope, now the Null Row isue removal, you are able to execute the query",301555.0
91702,384026.0,this should not happen. i did not face this issue. can you please retry everything after restarting your machine?,311686.0
91702,384050.0,"Also, this happened while creating the master table. I created the master tables by using create table table name as and then the ""Close Price""columns from 6 other tables. The query is getting timed out and disk usage is 100%. Any help would be appreciated.",316202.0
91702,385255.0,It might be something in background might be killing it. Try restarting your laptop and see if the issue reappears.,301555.0
92112,387736.0,the dates given in the csv files do not contain any holidays since the market is closed on those dates. Hence by my understanding the udf should return null when the date of a holiday is passed.,318374.0
92112,387426.0,"Even if as a input date to udf, a date is passed which even does not exists in the csv DATASET, It should return NULL. Same is the case for all the dates whether a holiday or not exists in dataset.",317811.0
92112,387367.0,"My opinion is it should be HOLD. Since you wont be buying or selling on a holiday (i.e. market is closed), it should be HOLD by default. Lets wait for a TA to verify the understanding.",318085.0
92112,387520.0,"but holiday criteria is not mentioned in assignment ,and also when will be holiday sunday ,monday ,saturday etc,or national holiday. I think we have to not think of that criteria. also their is no any column specifying holiday.",318005.0
92112,388014.0,any day which s not in dataset will release null signal,318005.0
92112,388243.0,"Is the ""input date"" has to be take from bajaj2 table? or any date value? And also if we consider any date then input date format conversion has to be taken care? Please clarify.",318846.0
92112,388535.0,"I am of the opinion that the signal from the last working day should be given, so that on the next working day appropriate action can be taken.",313826.0
92112,388616.0,"On a holiday,you do not do any transactions. Means that you get a HOLD signal for a Holiday or weekend.",301555.0
92112,388703.0,I think we only needs to focus on the date we have in the table.But TA can help here.,318476.0
92112,388707.0,"Holiday is usually a non business day. Hence there won't do any business transactions done. Meaning, that you get a HOLD signal for the same.",318080.0
92112,388972.0,"Are we expected to solve Question 4 with the lessons taught in the lectures? I am not able to think of any solution by using ""insert into bajaj2 select"" and then figuring out when 20MA and 50MA cross. I wonder if this is possible just with one insert/select statement. Any clues?",318007.0
92112,392537.0,If the given date is in between the dataset dates and signal is null (which means Holiday or weekend) we have to return HOLD. If the given date is outside the dataset dates we have to return null.,314244.0
91966,385756.0,"You can either calculate the moving average for those days (1 day, 2 day.... so on till 49 days) or leave it blank. Those will not be used for signal calculation further. You can make your assumptions and mention it in the comments section. This is what TAs have been saying for similar questions. Hope this helps.",318084.0
92119,387419.0,"Instead of just into table tcs, use IGNORE into table tcs Instead of ignore 1 rows, use IGNORE1 lines Instead of ENCLOSED BY '""',use OPTIONALLY ENCLOSED BY '""'",317811.0
92119,387537.0,"keep the data type of date in table to char(30) and then import; after importing use update yourtable set date=str_to_date(date,''%d-%M-%Y'); then u can change date back to date type as if u will leave it to str ,then u will not get order by date desc correct.",318005.0
92119,388618.0,"insert the data as character and then change the date. you can write case statement to find the data typ and then convert the dates accordingly. It may be that different records have different date formats and hence case statement will help. I can only guide you unfortunately. But yes, there are manythread on this topic where the issue is resolved. It might help.",301555.0
91971,388608.0,"Do not delete any data. You can use what you want, and ignore what you do not . And yes, the other rows are required for computation",301555.0
91971,385886.0,i think you don't need to delete those rows. as closing price of each date will be needed for calculating MA and subsequent tasks. however will request someone to comment on or verify this understanding of mine.,311686.0
94072,396448.0,"Hi Vishal , Please execute the below before update statement. SET SQL_SAFE_UPDATES = 0; When safety option is set to 1 , it won't allow the update unless you specify key in where clause. Please refer below link for more details,. https://stackoverflow.com/questions/11448068/mysql-error-code-1175-during-update-in-mysql-workbench Thanks.",305652.0
94072,396450.0,thank you :) i didnt knew this .,319846.0
94072,396444.0,"Hi Vishal, Please execute the below before update statement. SET SQL_SAFE_UPDATES = 0;",305652.0
94072,396500.0,SET SQL_SAFE_UPDATES = 0 ; Execute above command and then run your query. it will work.,317811.0
94072,397366.0,"Follow the following steps before executing the UPDATE command: In MySQL Workbench Go to Edit --&gt; Preferences Click ""SQL Editor"" tab and uncheck ""Safe Updates"" check box Query --&gt; Reconnect to Server // logout and then login Now execute your SQL query",314183.0
91973,385943.0,Based on the logic; If row 1 is your header then it has more characters than 15 that you've passed in your float.. :) [Not sure though if this is the reason],316349.0
91973,386009.0,This is happening because Delivery quantity and %Deli field has a space in csv file and while creating the table it has float and so it's failing there.,308965.0
91973,387149.0,I mentioned all the column names along with@date1 and it works for me.,310598.0
91973,387509.0,"you had to remove not null specification while creating table ,otherwise null value cant load. also use char(30) for problematic column you can change data type later also use more size for ex 30,50 etc for char for date use char(30) later use str_to_date(column,'%d-%M-%Y') to update column; then u can change date back to date type",318005.0
91973,389455.0,You need to mention all the columns as variables (similar to @date) and below you need to mention the actual column names from the table.,301113.0
93062,391839.0,You can go through below links to get detailed explanation of best moving average days. https://tradingsim.com/blog/best-moving-average-for-day-trading/ https://www.investopedia.com/articles/active-trading/010116/perfect-moving-averages-day-trading.asp https://www.investopedia.com/ask/answers/122414/what-are-most-common-periods-used-creating-moving-average-ma-lines.asp Hope this will help.,317991.0
92122,387362.0,"Try This: STR_TO_DATE( string , format ) Here is the reference : https://www.w3resource.com/mysql/date-and-time-functions/mysql-str_to_date-function.php",318085.0
92122,387382.0,"str_to_date function should work. i.e. STR_TO_DATE( string , format ) You need to modify the format parameter of the query so it describes how your date is described in the string. Here is the reference for the fomats you can use: https://www.w3schools.com/sql/func_mysql_str_to_date.asp",318085.0
92122,387412.0,"STR_TO_DATE( string , format ) format can be like '%e-%m-%y' where each character meaning is explained in below table. Format Description %a Abbreviated weekday name (Sun to Sat) %b Abbreviated month name (Jan to Dec) %c Numeric month name (0 to 12) %D Day of the month as a numeric value, followed by suffix (1st, 2nd, 3rd, ...) %d Day of the month as a numeric value (01 to 31) %e Day of the month as a numeric value (0 to 31) %f Microseconds (000000 to 999999) %H Hour (00 to 23) %h Hour (00 to 12) %I Hour (00 to 12) %i Minutes (00 to 59) %j Day of the year (001 to 366) %k Hour (0 to 23) %l Hour (1 to 12) %M Month name in full (January to December) %m Month name as a numeric value (00 to 12) %p AM or PM %r Time in 12 hour AM or PM format (hh:mm:ss AM/PM) %S Seconds (00 to 59) %s Seconds (00 to 59) %T Time in 24 hour format (hh:mm:ss) %U Week where Sunday is the first day of the week (00 to 53) %u Week where Monday is the first day of the week (00 to 53) %V Week where Sunday is the first day of the week (01 to 53). Used with %X %v Week where Monday is the first day of the week (01 to 53). Used with %X %W Weekday name in full (Sunday to Saturday) %w Day of the week where Sunday=0 and Saturday=6 %X Year for the week where Sunday is the first day of the week. Used with %V %x Year for the week where Monday is the first day of the week. Used with %V %Y Year as a numeric, 4-digit value %y Year as a numeric, 2-digit value",317811.0
92122,387396.0,"Please refer below link: http://www.mysqltutorial.org/mysql-str_to_date/ the function - STR_TO_DATE(str,fmt) will help you achieve this. fmt - This means format. One such example for it is ""'%d,%m,%Y'""",319006.0
92122,389163.0,"for date type 3-jan-2015 ,,,option can be %d-%m-%Y for date type 3/jan/2015 ---option can be %d/%m/%Y note small m,capital Y above and separator '/' look at this date 3#january#15 date format is %d#%M#%y here M as full january is written otherwise m here small y as shorten year is written otherwise Y",318005.0
93075,391861.0,"Question states that 3. Use the table created in Part(1) to generate buy and sell signal. Store this in another table named 'bajaj2'. Perform this operation for all stocks. Checkpoint that can be infered are: 1. Succesfully generate buy and sell signal using table created in Part(1). 2. Create new table 'bajaj2' and store buy/sell signal into this. It is not specifically mentioned that how many buy and sell signal will be there, so I would suggest to cross verify no. of buy and sell signal by 1. either make line chart to cross check buy/sell signal or 2. manually go through the bajaj2 table and cross check buy/sell signal. Hope this will help.",317991.0
92492,389673.0,Commands are needed since your sql file will be evaluated. btw if you have created the table using wizard. just run below command to get create table commands.. show create table &lt;tablename&gt;,312746.0
92492,389596.0,"Hi, Commands are better as we need to submit the .sql file for evaluation. HoweEven ver if you are using the wizard also , on the background it should be using commands and is it possible to get those commands and save it in the .sql file. But even if you are using wizard , it should be fine as we need to focus on the steps after creating table and loading data that will be the main point of evaluation. Check with the mentor once.",317460.0
92492,390335.0,"The assignment starts from creating the tables from the imported tables. Question 1 is to create a table ""bajaj1"" from original table. So, it doesnt matter if you have wriiten a query for importing the original table or used the wizard to import it. Grading starts from Q1.",316202.0
92492,390953.0,"Hey, it's okay even if you use the wizard to import tables. As Sham mentioned, the grading starts from Q1 which doesn't depend on how you import the tables. But it's always better to use commands to do so.",319721.0
91736,384009.0,csv is sending the date value as string.. you might want to read it as text and then convert it as date datatype.,316349.0
91736,384038.0,Use STR_TO_DATE function to change the format during import,310974.0
91736,384203.0,You are importing data into table as string but your column type is Date. Try to use str_to_date conversion function in SQL. Your problem will get resolved.,317412.0
91736,384105.0,"try reading this link,which will provide answers https://stackoverflow.com/questions/18927249/how-to-load-date-data-in-mysql-when-using-load-data",301115.0
91983,385883.0,we have to create six tables. one for each stock.,311686.0
91983,386331.0,"Yes, we have to create six tables for individual stocks.",307494.0
91983,386806.0,"It depends on how you can manage it. In one table or 6 tables, BUT mention your process in comments. It shuld be fine as long as any specific information is not provided in the question",301555.0
91983,387151.0,Yes question clearly mention that we have to create 6 tables,306011.0
92144,387689.0,Right click on the schema and use import table wizard option.,318084.0
91733,383978.0,"For 20 day moving average, it doesnt matter what the first 19 rows has. It can either be blank or fill it with whatever value. It'll not be used for Signal calculation.",318084.0
91733,384329.0,"For calculating moving average of 20 day and 50 Day refer below link which help you to form logic https://forums.mysql.com/read.php?52,148717 or https://www.statisticshowto.datasciencecentral.com/moving-average/ or https://www.investopedia.com/terms/m/movingaverage.asp",310419.0
91733,384897.0,Please refer https://www.mssqltips.com/sqlservertip/5248/mining-time-series-data-by-calculating-moving-averages-with-tsql-code-in-sql-server/ It will explain you the conecpt of moving averages and its calculation,317811.0
91733,385354.0,"I think the TA should mark answers 'verified' only if they answer the question appropriately right? The question posted is asking about validity of the moving average of the first 20/50 rows. The answers verified only talk about ""What is moving average and how to calculate"" and do not answer the question raised. Premnath is the only one who read the question correctly right? Am I missing something on when a TA likes your answers?",318007.0
91733,387543.0,u can easily calculate moving average in excel using formula . dont forget to use $ for making dynamic formula. you can refer excel video lectures to calculate averages.,318005.0
91750,384101.0,Use LOAD DATA INFILE --&gt;it works for SQL Workbench 8 if using .csv file format,301115.0
91750,384281.0,"Thanks Samyuktha, It helps resolve 1148 error but post that I encoutered another error 1290. ERROR 1290 (HY000): The MySQL server is running with the --secure-file-priv option so it cannot execute To resolve this, I had to comment out ""secure-file-priv"" parameter in my.init file. Now I am good.",304020.0
91750,384901.0,"ERROR 1290 (HY000): The MySQL server is running with the --secure-file-priv option so it cannot execute for this, execute command show variables LIKE ""secure_file_priv"" It will show the location where you can put the csv file and then try importing. It will work. ALso while importing use / instead of \ in the filepath",317811.0
92146,387776.0,,316349.0
92146,387658.0,"Which are not buy or sell, mark them as hold.",317811.0
92146,387686.0,BUY = the day when 20DMA &gt; 50DMA Sell = the day when 20DMA &lt; 50DMA Rest of the days HOLD,318084.0
92146,388642.0,The concept is very well explained by Ankit https://learn.upgrad.com/v/course/208/question/92149,301555.0
92036,386561.0,"code looks good, check if Bajaj already exist? or send the error which you get after running this code which wil help in knowing what is the issue here..",316349.0
92036,386588.0,"i realised my error - i should use short tick (`) to create table headers, i was using apostrophe (')",310509.0
92036,386601.0,"yes, you've to use backtick when there is a space in column name!!",316349.0
92214,388401.0,"There are some nulls also in a few rows. So, best way is not to use table data import wizard but use script and handle nulls appropriately by changing the data-types. Use: LOAD data infile Refer to following for more details https://stackoverflow.com/questions/14127529/mysql-import-data-from-csv-using-load-data-infile",317689.0
92214,388407.0,"Strange! but why the table import wizard is excluding null values?? RATHER i think it is the header row which is excluded, we need to find if the header is considered as a row or not! since the count is only one less 889 and 888.. i think most of them who are not from CS background will usually prefer to go for table import since LOAD INFILE was not part of session.. i don't think table import should be a problem! let's hear from others too.. and TA..",316349.0
92214,388449.0,"TRUE, but anyway importing the csv file through table import wizard with data types as text and double doesn't hurt i guess.. i can see all rows except 1 excluded.. can someone help on which exact row is exluded? till now i was thinking it as Header row and didn't care much!",316349.0
92214,388441.0,"well there are some null values in ""Deliverable Quantity"" and ""% Deli. Qty to Traded Qty"" columns. If you have defined data type of these columns as integer, then such row would be rejected. However, if you change datatype of varchar, then all rows would be uploaded. If you want to keep integer datatype, then you need to handle NULL values. Please refer to below link for that https://stackoverflow.com/questions/2675323/mysql-load-null-values-from-csv-data",304814.0
92214,388838.0,"To check the rowe failed, while improting you can set the show log button and see the log for each row, for the row failedit show exception.",304813.0
92214,389048.0,"It is because, the data import is failing for one of the rows from all the datasets where the date is either '31-August-2017' in some datasets or '9-December-2015' in the remaining datasets becuse the data is missing(null). So, it imports all the rows but one.",303229.0
92214,389037.0,"The record difference is due to NULL values tried to assign into INTEGER columns which throws error and that record got skipped. Due to NULL (blank) values present for columns `Deliverable Quantity` and `% Deli. Qty to Traded Qty` for record with date '31-August-2017' , mysql has skipped the record after showing a warnin while import works. ALternate to this is use LOAD DATA LOCAL infile after SET GLOBAL LOCAL_INFILE=1;",318454.0
92214,389675.0,Header row is not included that is why you were one row less.,319869.0
92210,388656.0,"THis is assignment. We are expected to be tested on all the modules we learnt, and a repeat question is possible. There is definitely some points in UDF creation which is not considered in bajaj2 table.",301555.0
92210,388369.0,"That query is not a straight forward one and all six stocks needs the same signal generation logic. Hence, it makes sense to have a user defined function which avoids duplication",318329.0
92210,388403.0,We need to create just 1 function not for all stocks. Also only 1 input is specified and stock name is supposed to be fixed as bajaj. I think it is just to test that we understood functions or not.,317689.0
92210,388415.0,"Q3 says to create a table for each company/stock which has date, close price and signal. This table holds the entire signal data for this stock for all the dates provided in DatSet. Q4 says to create a function that a end user can use to find the signal for a date by giving an input date to this function. This will help end user to instead knowing the complex sql query hiw to find signal for date, a udf can be easily used.",317811.0
92210,388718.0,"Question 3 - To create a TABLE for each company/stock which has date, close price and signal. The created table holds the entire signal data for this stock for all the dates provided in DatSet. Question 4 - To create a FUCNTION that an user can use to find the signal for a date by giving an input date to this function. Suggestion - Here we can make use of User Defined Functions.",318080.0
91759,384172.0,"That is correct. Rest of the days, it should be HOLD.",318084.0
91759,384462.0,Yes. That is correct.,310511.0
92899,391331.0,I do not think the intent is to look at microscopic scenarios but to find out how we work with data. Did you find some data in the csv files provided that do not have any trades?,318007.0
91593,383642.0,"hi, it can be solved through window/frame concept that was explained in the content. you need to use either sub-query or LAG function with window/frame logic.",311686.0
91593,383761.0,"what does this crossover signify, we need not just take the greater lless than and calculate buy sell?",317982.0
91593,384139.0,"Crossover is the day when 20 Day MA rises or goes below 50 Day MA, that is when we have to mark it as BUY or SELL. After the crossover, 20 Day MA could still be either over or below 50 Day MA. In that case, the signal should be updated as HOLD.",301555.0
91593,385258.0,This being assignment cannot give technical solutions. But you can do it either way as long as any specific instruction isnot provided in problem statement. You can comment your thought and explain why you progresed with this approach.,301555.0
90585,388886.0,"I think its a typo , i have raised this as a mistake too via 'Report a mistake '. Waiting for the update from upgrad on that.",311861.0
90585,378185.0,Can someone please guide?,311686.0
90585,378929.0,"I think it was a mistake, we need to create tables for the given stocks only. No Wipro.",310974.0
92149,387773.0,,316349.0
92149,387680.0,BUY = the day when 20DMA &gt; 50DMA Sell = the day when 20DMA &lt; 50DMA Rest of the days HOLD,318084.0
92149,387908.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line",317811.0
92149,388641.0,Well Explained using Graphs by Ankit.,301555.0
92149,389155.0,rough diagram,318005.0
91760,384611.0,"Hey Guys, Tried your suggestion of reopening the workbench, restart of computer, reestablishing the connection to server, also increasing the connection read time from 60 to 6000 in seconds but it is howing me the same error. What should i do now ?",318451.0
91760,384169.0,Just close the workbench and reopen it and execute the query. I faced a similar situation. Restarted worked.,318084.0
91760,384205.0,"A restart will solve your problem. If it does not work, ping me I will resolve it.",317412.0
91760,384459.0,"Hi Tanay, If the issue isnt resolved by reopening the workbench, you can increase the connection read time out time to keep the instance alive during long running queries. To do this, Go to Edit -&gt; Preferences -&gt; SQL Editor and set to a higher value this parameter: DBMS connection read time out (in seconds). For instance: 86400. Close and reopen MySQL Workbench. Kill your previously query that probably is running and run the query again.",310511.0
91760,384683.0,"You can follow below link for ""lost connection error"" https://stackoverflow.com/questions/10563619/error-code-2013-lost-connection-to-mysql-server-during-query Hope this will help.",317991.0
91760,384786.0,"First, can you tell me, that were you able to connect to your local connection properly ? And if Yes, you were able to connect to your local connection successfully, was your web server running good ? (Using web server I meant to say was your XAMPP, WAMP, etc working properly ?) Try running your web server again, may be that can be the main issue for your SQL not connecting to the server Hope that this helps :)",301655.0
91771,384241.0,Yes.,313826.0
91771,384327.0,"You need to find the signal which u r creating in bajaj2 table, so you need to create function on this table",317982.0
91771,384373.0,"I'm confused now, what are we supposed to do in question 3 then? only create a table with three columns with signal null? and then impute null with buy/sell/hold with UDF in step4?",316349.0
91771,384456.0,Yes. The purpose of the UDF should be to fetch the value of the signal from bajaj2 table based on an input date.,310511.0
91771,391564.0,Note we should work on bajaj1 or bajaj2. bajaj2 less code not more logic. matching date signal to retrieve. If its on bajaj1 coding required as we did for bajaj2 from bajaj1. Please clarify,312019.0
92007,386126.0,UDF needs to be created only for bajaj stock using input as date from date of bajaj2..,316349.0
92007,386224.0,User define function is only created for Bajaj stock not the other stocks.,310419.0
92007,386838.0,You can create function generic for all stocks or Bajaj Stock only. But the question is for Bajaj Stock. You should explain what you doing in comments and it should be good as long as all your other steps are alignedwith problem statement.,301555.0
91780,384657.0,"if it's windows,use'\r\n ' for lines terminated by,also hint( after seeing your code) if you're able to import data successful,check the dates",301115.0
91780,384893.0,"In STR_TO_DATE, you should tell what is the format in excel's data column i.e. '%e-%M-%Y' When you look at error message it will tell you that it is trying to import dates on 11-August-2017 like this format.",317811.0
91780,385161.0,Data Truncation might be because the length in the file and hat in the table is different.,301555.0
91780,384448.0,"Hi Praneeth, This error might be because of length mismatch between the column length and that of the actual data. Try using float instead of double for the mentioned column. Better still you can create the Table import wizard on the left navigation panel (by right clicking on the schema) to create the tables directly from csv files. Hope this helps.",310511.0
91781,384317.0,"Yes, you have to perform analysis for all the stocks.",310419.0
91781,384307.0,yes.. it is mentioned that you perform the same for all other stock as well..,316349.0
91781,384449.0,"Thats correct. Except for the UDF, all other step have to be performed for all the 6 stocks.",310511.0
91781,384734.0,it is clearly mentioned to do for all 6 stocks - obviously it is not possible to create a signals table without having data for 20 and 50 day MA: 3. Use the table created in Part(1) to generate buy and sell signal. Store this in another table named 'bajaj2'. Perform this operation for all stocks.,300694.0
92339,388882.0,Bajaj1 table start with 2015 date because to predict the signal of today's date we have to compare the past date data.,310419.0
92339,388833.0,"2015, your calculations will be on historical data.. Buy/Sell/Hold signals :) Ideally (if I'm not wrong), data analytics is done on historical data to predict the results.. here it is Buy/Sell stocks..",316349.0
92339,388844.0,yes. ideally first 19 rows for MA20 should be null. but as per the instructions if the query gets complicated we can go ahead with the values getting populated as per the logic written for subsequent rows. the table should have the same dates as in the data file.,311686.0
92339,389197.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date ascending,317811.0
92166,388148.0,"Since all are stocks and it's been mentioned that the data is taken from NSE site, We can assume that the dates are going to be same.",318329.0
92166,387978.0,"it depends on how you're bringing data from these six tables.. on the basis of nature of join.. inner will bring only the common data, and accordingly for other joins.. but as far as this case is concerned i guess date is across all the six stocks..",316349.0
92166,388357.0,"well,if dates are different then stock closing price for that column should be zero,we can achieve that using full outer join with date as key, MySQL doesn't support direct full outer join command, need a work around .",301115.0
92166,388510.0,"As Date is same across all tables, any join you put. It will work fine",317811.0
93080,391926.0,Instead of single quotes ' ' try tilde ` ` where ever you have used single quote.,317991.0
94204,396911.0,You can follow below link to solve this error https://stackoverflow.com/questions/32737478/how-should-i-tackle-secure-file-priv-in-mysql Hope this will help,317991.0
94204,396937.0,"You can use the following command to check the directory. Then move the files, which you want to load, to that directory. SHOW VARIABLES LIKE ""secure_file_priv""; t he error would be solved. Hope that helps.",317998.0
94204,396947.0,Use import wizard ..it will work,300735.0
94204,397109.0,Please try to use the table data import wizard.,304281.0
91346,381842.0,Tableau offers 32 bit version as well. You can download from following link. https://www.tableau.com/support/releases/desktop/10.3.1#esdalt You can check for the latest version of the 32 bit version. Also mysql would also be available for 32 bit.,317689.0
91346,394757.0,you might have to use an year old version - 10.4 https://www.tableau.com/support/32-bit-upgrade,312357.0
91237,381360.0,"There was a note saying that if MA calculation is not possible is not possible for certain days, better to avoid generating signal for them. So, you may or may not have empty values for first 19 or 49 rows but avoid signal generation is what I understand.",318329.0
91237,381412.0,This is what I have done yes - first 19 and first 49 I have left empty; and left the signal as hold for first 50 days,300694.0
91237,381529.0,"So, basically the understanding is that it doesnt matter whether the 20 DMA and 50 DMA rows for the first 20, 50 columns are filled or not. Just that it need not be used for Signal generation. From evaluation perspective, It would be better if some TA confirms this.",318084.0
91237,381619.0,"This is a tricky take and i would suggest look at the question more closely. The hint is correctly mentioned, and I can only guide that you all are thinking right. BUT unfortunately cannot give any staement here, as any statement is directly the solution. Hope you understand. But You can make your assumptona and understanding and comment in the code.",301555.0
91345,381844.0,If you are using window function than order of the data in the file doesn't matter.,317689.0
91345,382132.0,"I think Moving average needs to be calculated based on the past data , so as to predict the future trends/Signals. To decide whether I need to buy/sell today I can only tell by looking to the past data. Future data will not be available in real-time. Just that we have dataset for particular period. And in that case past data means ordering data in ascending order.",314565.0
91345,381980.0,"Moving average should always be calculated in the ascending order. The order by clause in over() sorts the result in ascending order. So, no prob. You can always validate the signal based on the prev values.",310974.0
91132,380714.0,You need to create bajaj1 and master table from the existing tables using select query with appropriate logic. ex: create table test select * from existing_table;,311952.0
91132,380722.0,"Hi Babita, The tables bajaj1 and master table has to be created using create table statement with subquery. See the following eg. CREATE Table_name AS SELECT COLUMN_LIST FROM SOURCE_TABLE_NAME The above query will both create the table and insert rows into the table from source table. Hope this helps.",310511.0
91132,381076.0,You need to write CTAS statement i.e. create table as select this is often used to create tables from other tables.,317689.0
91122,380686.0,it means that for the initial several days you will not be able to calculate 20days or 50days moving average because of absence of past data. so ideally we should have NULL against these rows in respective columns. the code to do so will thus get complicated. hence it is suggested that we do not need to handle it explicitly and the normal code written to populate those columns will do even if it fills some values for those initial rows as per the written logic.,311686.0
91130,381606.0,"You can import data into table the way you feel comfortable, This part is not evaluated. Only thing is, you should not get reduced data due to any reason, then your results will vary.",301555.0
91130,380735.0,"Hi Rambabu, We shoild be allowed to use either process. The process of creating the initial stock table isnt mentioned in the steps of the assignement. This is a prior process.",310511.0
91130,380755.0,I think it will be a struggle with data import wizard anyway because it has limited options for dealing with bad data .. so most likely script will be the only/best option,300694.0
91133,381152.0,"You can make use of STR_TO_DATE method to convert a formatted string into Date type of SQL. For Ex of our case: STR_TO_DATE(@DateColumn, '%d-%M-%Y')",318329.0
91133,380750.0,"For me personally, i had trouble with using the table import wizard due to some 'invalid' data in the csv file - and I did not want to edit the CSV file as that is bad practice, in my opinion so I reverted to using the 'load data infile ... ' command which then gives you a lot of power and additional options, including, but not limited to: * importing missing data as nulls * changing the type of the csv file columns to different types when being written to db. so here you can look at changing the string date format to MySQL's date/datetime format on the fly while data is being imported; there are several examples via google search that can show how to do this",300694.0
91133,380728.0,"Hi Rambabu, You can do this in several ways. The following 2 are the most common ones: 1. You can change the datat type of the column in the table export wizard while importing the csv into a new table. 2. Create the table with date column in text format using create statement. Then write an alter statement to update the date column datatype. Then load the date from csv using load statement. See the below eg of load statement: LOAD DATA INFILE ""/home/paul/clientdata.csv"" INTO TABLE CSVImport COLUMNS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' ESCAPED BY '""' LINES TERMINATED BY '\n' IGNORE 1 LINES ; Hope this helps.",310511.0
91133,381612.0,"You Can use any of the above approach, as long as the data is loaded correctly in correct format and there is no data loss.",301555.0
91133,382101.0,"Hi , I am unable to use 'load data infile in MySQL 8.0 , it is giving error that it is not supported by this version. I am using MYSQL 8.0. I can see the INFILE is enabled, but still I am unsable to do. Please help as I need to load data into the tables. mysql&gt; show variables like '%INFILE%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | local_infile | ON | +---------------+-------+ 1 row in set (0.01 sec) mysql&gt; SELECT @@GLOBAL.`local_infile`; +-------------------------+ | @@GLOBAL.`local_infile` | +-------------------------+ | 1 | +-------------------------+ 1 row in set (0.00 sec)",307494.0
92167,388040.0,"Just restart your workbench, It should work. BTW, Its is bug in current workbench version. for more info :- https://bugs.mysql.com/bug.php?id=90875",312746.0
92167,387989.0,I guess restarting the workbench will stop this issue.. previously was facing the same issue..,305129.0
92167,388300.0,"It is with newer versions of workbench, restart might help!",304813.0
92167,388514.0,"Just Restart the MYSQL, it will work fine",317811.0
92167,388644.0,You must have Deselected it in the View pane. Select the options in View Pane and set it from there.,301555.0
92167,389098.0,"This is due to some BUG in workbench Instead of to reastart there is an ALTERNATIVE way. Click the button which is third from Execute/Run button . Its with Zoom + Execute and says ""Execute the EXPLAIN Command "" after clicking above button, the Explaiin window opens where the right side you can see """"RESULT GRID"""" Click that and scroll above ; or resubmit the ""SQL Stmt"".. the grid result will reappear. Hope this quick solution helps all. Incase still its not then restart the workbench after saving changes.",318454.0
92167,389153.0,satish is right,318005.0
92016,386145.0,Yes you can ignore them although the MA functions won't provide any nulls. Please cross check once you implemented it.,310974.0
92016,386278.0,You need to import all rows. This statement talks about the rows for which the MA cannot be calculated i.e. first 19 or first 49 days.,317689.0
91852,384938.0,"There are no guodeline on how to import the csv files, so it could be done either through the wizard or using the 'LOAD DATA ...' command. I have noticed that the wizard is not very efficient in importing data and may ignore some rows due to the kind of data present in those rows. On the other hand, the command gives a lot of features to load data like converting a data of string type to some other type etc. My suggestion would be to use the 'LOAD DATA..' command.",313826.0
92285,388626.0,"If you're using the table import wizard, change your column data type to 'Text'.. this will pull the missing value rows for you..",316349.0
92285,388655.0,Change the format from integer to sting and try to load.,317811.0
92285,388637.0,use load infile command to load file.in to table.,306735.0
92285,388659.0,Both options will work.,301555.0
92285,388662.0,"You can fill the missing value with 0 and import the CSV file. Open the CSV file using notepad++ or visual studio code search for ' ,, ' Now place 0 between ',' and ',' save the file and import again, you don't have to change the datatype another workaround you can create the Table using SQL statement and insert the data using Python.",317845.0
92285,388697.0,Integer dont accept nulls. You need to convert it into string to load.,317689.0
92285,389046.0,"To load that record 230 , you can also use LOAD DATA LOCAL infile after SET GLOBAL LOCAL_INFILE=1; Due to NULL (blank) values present for columns `Deliverable Quantity` and `% Deli. Qty to Traded Qty` for record with date '31-August-2017' , mysql has skipped the record after showing a warnin while import works. alternate is to edit the CSV file. but suggestion to avoid edditing the source file manually as that may tamper the source truth/worth . All data manipulations and data cleansing are advised to perform in DataWarehouse rather than editing it in Source file. Consider , you only handover the SQL Script to PSS or BAU team for perioding run, But they wont know where/what to edit in source file and how to perform that.",318454.0
92285,389327.0,"This is two types 1_If you are using import through Wizard change the data type to text through the procedure of importing 2_If you are importing through Code then use Varchar datatype, so it could take even the empty Cell also (i too faced same issue and resolved by 2nd point)",318732.0
92285,389533.0,"Hi! If you have resolved this error (or any other error as a matter of fact), can you please share the error and the method to resolve it in this post . This way, learners can find solutions to all the errors in a single question, making debugging and error resolution easier. Thanks!",306733.0
92285,389556.0,"Ok , as per my analysis , if you see the dates are in ""1-August-2017"" (DD- MONTH- YEAR) format which is a text type , so when you are converting TEXT to DATE datatype it is throwing error . I think here first we need to format the text in supported DATE datatype format and then the change cloumn to DATE datatype . DATE datatype formats may be like : DD-MM-YYYY , YYYY-MM-DD, DD-MON- YYYY.",306147.0
92285,389823.0,"The Issue got resolved with creating the table with these fields as text , then importing data.",318436.0
92283,388577.0,"Hi, Capitilize M as below set date= Str_to_date(date,'%d-%M-%Y');",305129.0
92283,388684.0,"If the input string is illegal, the STR_TO_DATE() function returns NULL. The STR_TO_DATE() function scans the input string to match the format string. the function - STR_TO_DATE(str,fmt) fmt - This means format. One such example for it is ""'%d,%m,%Y'"" here the date format is 31-July-2018 so we have to use the format Str_to_date(date,'%d-%M-%Y') %Y Four digits year e.g., 2000, 2001,…etc. %y Two digits year e.g., 10,11,12, etc. %D Day of the month with English suffix e.g., 0th, 1st, 2nd, etc. %d Day of the month with leading zero if it is 1 number e.g., 00, 01,02, …31 %M Full month name e.g., January, February,…December %m Month name with leading zero e.g., 00,01,02,…12",319006.0
92283,388699.0,Make it %M to take full month name which is given in the file. %m is used for shorter format for the month.,317689.0
92283,389372.0,Convert the date in text column in file in format YYYY/MM/DD and then use the STR_TO DATE function on the date column,310629.0
92283,389608.0,"If you view the csv file in excel, it will show up in the format set in excel. But when you open the csv file in text format, you will notice what the input feed actually looks like and get the right format to use in the str_to_date function. Alternatively, select a few strings that have been read into your table and see how it comes in.",300717.0
92283,389678.0,capitalise function use %M instead of m only,319869.0
92283,389635.0,"Write something like this UPDATE table SET Date = STR_TO_DATE(Date, '%d-%M-%Y') &lt;WHerecondition if any&gt;",301555.0
92173,388139.0,"Over should be used in some method like RANK or SUM or AVG etc.,",318329.0
92173,388299.0,you might have error or missing statements in earlier sql commands in the sheet.,304813.0
92173,388651.0,OVER is not a direct command. It has to come with RANK . Share previous command or complete command to help you completely.,301555.0
92173,389620.0,Flag my solution and then i can contact you offline and you can share me your code. We cannot shar code here as it is assignment.,301555.0
91590,383427.0,It has to be permanent columns.,318084.0
91590,383435.0,we need to create a table bajaj1 with thoe columns,317982.0
91590,383530.0,You would need to add the MA columns permanently as you might have to refer to them further in the assignment especially when generating the BUY/SELL signal. Hope that helps!,318085.0
91590,389301.0,"How the table look for baja1. Just create table with Date, ClosePrice, 20_Day_MA, 50_Day_MA. What should we do further (This has to be done for all 6 stocks). I did not understand what it is and how to go head further. Please help in this regard.",312019.0
92328,,nan,
92042,388289.0,"Its is the when 20 day MA crosses 50 day MA, each new change is considered a cross over point.",304813.0
92042,386531.0,"Signal = BUY on the day when 20 DMA &gt; 50 DMA. The next few days, 20 DMA will be still be more than 50 DMA but Signal has to be HOLD. Similarly Signal = SELL on the day when 20 DMA &lt; 50 DMA and HOLD for the rest of the days. Hope it is clear now.",318084.0
92042,386529.0,"your 20 days moving average should cross with the 50 days average values.. now this could happen in two cases, one if 20day MA is moving upwards while 50 day downwards and second if 20day MA is moving downwards and 50 day MA upwards.. now the signals should be Buy in first case with the first value after cross and later even when it stays up, it should be Hold.. and same for second case, at the firat value of cross it should generate a signal Sell and later even when it stays down it should be Hold.. hope this helps..",316349.0
92042,387919.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line",317811.0
92042,387927.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line",317811.0
92052,386542.0,MySQL should be the one as there are a few syntaxial differences when compared to Oracle PL/SQL and MySQL being one thats taught. :),312259.0
92052,386646.0,We have to prefer Mysql,314183.0
92052,386859.0,As the assignment is in MySQL you should install MySQL work bench. As per TAs evaluation would be done in MySQL and there is some sytax different between mysql and Oracle.,317689.0
92052,387393.0,There is an installation istructions document. It has details on prerequisites also. Go though it.,303666.0
92052,387692.0,MySql. This is free and also preferred by the IIIT-B,312732.0
92052,388686.0,We have to use MySQL. It is preferred by Professesor and SME,314183.0
92052,389060.0,"We need to use MYSQL. The installation details are available in ""Pre-Launch Preparatory Content"" -&gt; ""Module 3"". You can find the same in the link ""https://cdn.upgrad.com/UpGrad/temp/89ddbb36-9d58-4db6-92b9-d0bebd4d6862/MySQL+Installation+Guide+(Windows).pdf"".",303229.0
92052,388955.0,"Yes, you have to install MySQL from here. You are lucky as the latest version (8.0.13) is out with the bugs we faced fixed in it. We all installed 8.0.12 but please use the latest version. https://dev.mysql.com/downloads/mysql/",318007.0
92052,389039.0,Since tHis is mySQL assignment and we should use MYSQL query standards but not other DBMS. Though we have choices for evaluating the assignment the TA will be referring MYSQL coding/standards.,318454.0
92046,386551.0,You can make your assumptions and mention the same in the comments. This is the general feedback from TAs for questions where there are no clear cut instructions. You can also start calculating the signal from the 50th day.,318084.0
92046,386684.0,"It is better to keep it on ""HOLD"" as there are no numbers available for comparison. It was mention in assignment that whereever there are NULL values keep it as hold insted of making any other assumptions. We can calculate BUY and SELL only basis of comparison where 20 Day MA and 50 Day MA are available.",308439.0
92046,388869.0,No. You will only able to generate the Siganl (Sell/Hold/Buy) on the 51 day beacuse we need one pervious day values to figure out where its a cross over or its the movement in the same direction.,318476.0
91324,,nan,
91325,381642.0,"Hi, I guess which shares needs to buy and sell details to be provided by looking at the final data.",305129.0
91325,381644.0,"There is no constant format to be followed but I guess you can add the bfief of the assignment, the method follow to arrive at the results and the inference from the results.",310511.0
92045,386566.0,Please use SELECT @@GLOBAL.secure_file_priv; to find out where you have permission to save the files. Alternatively you can save the csv files to ProgramData\MySQL\MySQL Server 8.0\Data\ YOURMYSQLPROJECTFILE. I had to do this even after removing the settings in the .ini file.,317149.0
92045,386719.0,"Please use LOCAL keyword, like LOAD DATA LOCAL infile",308439.0
92045,386640.0,"Hi, Run below command to check path of secure file, SHOW VARIABLES LIKE ""secure_file_priv""; you may disable this parameter in my my.ini file of path C:\ProgramData\MySQL\MySQL Server 8.0 or else you may paste your csv files in C:\ProgramData\MySQL\MySQL Server 8.0\Uploads",311004.0
92045,386883.0,Try executing the below code SET SQL_SAFE_UPDATES = 0;,301555.0
92045,387024.0,"Finally solved it with setting secure-priv with the default path ""C:/ProgramData/MySQL/MySQL Server 8.0/Uploads"" creating a table for bajajauto with all fileds as in csv with appropriate data types then using :: LOAD DATA infile 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/BajajAuto.csv' INTO TABLE `bajaj auto` FIELDS TERMINATED BY ',' LINES TERMINATED BY '\r\n' IGNORE 1 LINES;",318005.0
92045,388113.0,"LOAD DATA infile 'C:\ProgramData\MySQL\MySQL Server 8.0\uploads\TCS.csv' INTO TABLE `TCS` FIELDS TERMINATED BY ',' LINES TERMINATED BY '\r\n' IGNORE 1 LINES; Still getting below error. Error Code: 1265. Data truncated for column 'Deliverable Quantity' at row 657",318802.0
92045,388619.0,"Where can we find my.ini file. I tried all above query - to know path of secure file - SHOW VARIABLES LIKE ""secure_file_priv""; and it returns NULL. Tried all above but stll getting error Error Code: 1290. The MySQL server is running with the --secure-file-priv option.",320103.0
92045,388704.0,"Hi Khusbu, you can find the file here C:\ProgramData\MySQL\MySQL Server 5.6 Program data folder is hidden . Check it.",319006.0
92045,389027.0,"It will show you the path :- SHOW VARIABLES LIKE ""secure_file_priv""; also update this query SET SQL_SAFE_UPDATES = 0;",318802.0
92045,389020.0,"I am facing the same trouble. Error Code: 1290. The MySQL server is running with the --secure-file-priv option so it cannot execute this statement. However I am using MAC, so unable to find solution as listed above. How to solve it in MAC?",318335.0
92045,389026.0,put files at below location LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Bajaj Auto.csv',318802.0
92045,389032.0,"create table and put date as varchar(40) after date load data using LOAD DATA infile 'C:/ProgramData/MySQL/MySQL Server 8.0/uploads/Infosys.csv' INTO TABLE `Infosys` FIELDS TERMINATED BY ',' LINES TERMINATED BY '\r\n' IGNORE 1 LINES; and convert date by using - update talename set Date = date(str_to_date(date,'%d-%M-%Y'))",318802.0
92045,389119.0,"Step 1: Disable this parameter in file my.ini file in path ""C:\ProgramData\MySQL\MySQL Server 8.0"" by adding below stmt as last row in that my.ini file. This diables the load path of specific MYSQL directory instead LOCAL disk any location. secure_file_priv="""" Step 2: Use LOAD DATA LOCAL infile while loading data from command prompt",318454.0
92045,389686.0,Well on my MacBook this is the error I am getting. ERROR 1148 (42000): The used command is not allowed with this MySQL version any ideas?,300734.0
93081,391875.0,You can definitely pass the table name as a variable but I doubt you can use it in the select query as usually the variable are used to replace values in the where condition.,310974.0
93084,391896.0,https://learn.upgrad.com/v/course/208/question/92505 https://learn.upgrad.com/v/course/208/question/92578 These Questions are similar to what you are asking. Comment if you still have some doubt.,301652.0
93084,391913.0,You are right. Just ignore the first 49 values using row_number() function.,312746.0
92059,387180.0,i tried to do it but not successful,310509.0
92059,386662.0,Use keyword Local Like: Load Data Local Infile,308439.0
92059,386832.0,In private mode you can only upload file from its default location i.e. c:/ProgramData/MySQL/MySQL Server 8.0/Uploads/,318429.0
92059,387454.0,"Mny have faced this issue and the problem is resolved using the approach mentioned by Ankit. Let us know, what you did and what is the error",301555.0
92059,388072.0,you can use this below statement SET SQL_SAFE_UPDATES = 0; let me know if this works for you...,311861.0
92059,386935.0,"Exceute command SHOW VARIABLES LIKE ""secure_file_priv""; It will show you the path where you can put the csv file and it will load succesfully. Also while mentioning the file path use / instead of \ in the file path.",317811.0
93086,391898.0,"I am not sure but it seems that in master table there are more than one column with mDate being one of them. And this statement is trying to insert new records with only one column in select query. as no default value is set for mDate, in insert query it is looking for values for mDate column as well for all the new records getting entered.",311686.0
93098,391960.0,The requirement is to do it for Bajaj Stock. But I think there is no harm if we create UDFs for the other stocks aswell.,313826.0
93098,391924.0,"Yes questions clearly states that we have to do it for Bajaj stock. If we would have to do it for all, it would have been mentioned clearly as it is mentioned in question 1 and 3.",317991.0
93098,391933.0,UDF only for Bajaj Stock,314048.0
93098,392286.0,We should use bajaj2 or bajaj1 table for UDF creation ?,312019.0
97386,416832.0,"Problem is in below code. Rest of the code looks fine. SELECT `SIGNAL` AS SIGNAL_INFORMATION FROM ASSIGNMENT.BAJAJ2 WHERE DATE = DATE; RETURN(SIGNAL_INFORMATION); Use ""into"" in place of ""as"": SELECT `SIGNAL` INTO SIGNAL_INFORMATION FROM ASSIGNMENT.BAJAJ2 WHERE DATE = DATE; RETURN(SIGNAL_INFORMATION); LEt me know if that worked or not.",317991.0
97386,416833.0,"I replaced AS with INTO. and it worked. the function got created , but when I pass this function in sql statement to retrieve single value of signal on a perticular date, I am not getting output. below is the query I am using select * from ASSIGNMENT.BAJAJ2 where SIGNAL_INFO('2015-01-05')",311046.0
97386,416830.0,"the select query has where clause with date as input parameter. even with out firing query on the table , how the function determined that it is a result set , but not a single value. How can I solve this function to provide single value",311046.0
97386,416821.0,"The error is coming because you are returning a result-set, but function can only return one value. You can go through below link to get help regarding the error you are getting. https://stackoverflow.com/questions/11880306/why-mysql-is-giving-error-not-allowed-to-return-a-result-set-from-a-function Hope this will help.",317991.0
91312,381617.0,you need to print the value of the variable secure-file-priv and then in your infile statement you need to use this format of addressing directories: LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/filename.csv' note the direction of the slashes,300694.0
91312,381626.0,"May be restart your system and try the steps again. I think, it could some kind of issue with the current instance of MySQL Restarting will reset it and will work.",301555.0
91312,381643.0,"Goto following folder where my.ini file lies. I am assuming windows. Please check for your OS specific path. C:\ProgramData\MySQL\MySQL Server 8.0 Open the file my.ini Find secure-file-priv. Set it to secure-file-priv="""" Restart you machine which in case restarts your mysql server.",317689.0
91312,381550.0,"Hi Ranjay, This means, your MySQL server has been started with --secure-file-priv option which basically limits from which directories you can load files using LOAD DATA INFILE . You may use SHOW VARIABLES LIKE ""secure_file_priv""; to see the directory that has been configured. Then move your file to the directory specified by secure-file-priv and then finally run your load statement.",310511.0
91312,381918.0,"try to load via from workbench, after creating the table with correct datatypes right click on the table name below Schema and select Import Wizard and then it is a step by step process. Very Simple! Just try once..",304813.0
93099,391939.0,There might be two cases:- 1- Your function is not returnign anything. 2- you are viewing Action grid. Your output will be shown in Result grid. Just choose the Result grid.,312746.0
93099,391962.0,Sometimes the results grid does not show up eve though the query was executed successfully. Restart the workbench and execute the query again.,313826.0
93099,391968.0,It was an issue with the input data type. I was passing DATE as a data type. But then changed it to the Varchar. Now showing me the Signal value for particular date. But I have not handled the exceptions.,310522.0
93212,392325.0,you need to use the same column names specified in the assignment statement.,319721.0
93213,392338.0,"As part of the assignment table structure ,its not a column, but if it helps to go through till the end, finish the basic structure and check the data correctness,later while optimizing ur code ,that column can be dropped.",301115.0
93213,392336.0,"No, you don't need to. It's not mentioned in the problem statement.",319721.0
93214,392344.0,It's not actually necessary. But do comment why you're using the row number here. Be clear in the comments on why you think the row number helps you with the analysis.,319721.0
93216,392354.0,"No, it;'s not. Graders will load the file from their system so, just import the data in any way which is comfortable for you. Also, you won't be graded on this, so don't worry.",319721.0
94312,397248.0,"hey, set the datatype for importing date column as text/string and then change the date datatype using str_to_date.. sql is not able to read the date from excel as the default date format for SQL is yyyy/mm/dd.. try to import it as text and you shouldn't face the problem.. please let us know if problem persists even then..",316349.0
94312,397229.0,"Convert string to date form using str_to_date('date','%d,%m,%Y')",303673.0
93217,392359.0,"Hey, you need to use the exact column names. In fact there is a concept you're tested on which needs you to use the column names which are given. You might be facing an error, but the task is, you need to solve it.",319721.0
93217,392364.0,There is already a clarification given by TA 1. Creating new tables: Make sure you are creating 6 different tables as mentioned. And the column names should be exactly the same as given(if given) in the problem statement throughout the assignment. Below is the link of the above clarification. https://learn.upgrad.com/v/course/208/question/93156,317991.0
93203,392314.0,Use IF instead of IFF and check .,301648.0
93218,392363.0,Please check your code. where address like '%Houston'; Insetad of this use where address like '%Houston%';,319721.0
93218,392374.0,Please look at my snapshot,320687.0
93218,392394.0,please look at the snapshot,320687.0
92735,390528.0,"Hi please note , the SCRIPT will be rerun by TA and LOGICS also reviewed while grading scores. So suggesting to use LOGIC to do apply NULL rather hardcoding. the MOVING AVERAGE logic will add values to all columns &lt;50 rows. you can use LOGICS like ""IF check logic on row_number to make NULL if &lt;50 rows"" like.",318454.0
92735,390560.0,"yes, it should be NULL. Please see the problem introduction section where signal is generated only for the live data and not for NULL. We will have to compare the values between '50 days MA' and '20 days MA', if any of the value is NULL then we cannot predict the signal whether it should be BUY, SELL or HOLD. This is my understanding.",312479.0
92735,391067.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
92735,390553.0,yes the first 49 rows should have null values in it as the moving average will start when first 50 entries are present,318017.0
92735,391882.0,"Hey, here we need 50 values to calculate MA50 right? So, how can we have MA50 for the first 49? Think about it.",319721.0
93105,391958.0,Yes you would need to have 6 different tables each for one stock.,306735.0
93105,391969.0,"Question clearly states that, it has to be done for all 6 stocks that means you have to create 6 different tables. 1. Create a new table named 'bajaj1' containing the date, close price, 20 Day MA and 50 Day MA. (This has to be done for all 6 stocks)",317991.0
93105,391975.0,"Yes. Create tables like tcs1, tvs1, hero1 etc. You will be needing these populated in another question.",318007.0
93105,391986.0,"Yes, same operation for all stocks.",306250.0
93105,392052.0,"Yes this has to be done for all six, you can try making function to take this to next step.",306244.0
93105,392182.0,Yes since it has been clearly mentioned that create same table for all the six different stocks.,318756.0
93105,392385.0,"We should make 6 stock tables with csv. Next 6 tables for each stock with date,closeprice,mv_20,mv_50. Next 6 tables are signal based with Date, ClosePrice and SignalStatus. Bajaja_Auto Bajaj1 Bajaj2 Same for other five stocks tcs,tvs,hero,infosys,eicher one master table from each stock close pirce col taking",312019.0
93105,393471.0,"Yes , except for the task 4 (udf creation) all the other has to be done for all 6 stocks",314565.0
92737,390545.0,Yes. Since there won't be any tranding done on Holidays :),300727.0
92737,390548.0,that is because you might have holidays and weekends in between when the stock market is not active,318017.0
92737,390550.0,"Yes, this is correct. It is because Stock Market closes in holidays and weekends. So, data is not generated. Please correct me if I am wrong here.",312479.0
93106,391972.0,"Yes you are correct, first cross over is on day 12 given in the example.",317991.0
93106,391974.0,Yep you can still tell how to begin with on Day 10 and detect cross on Day 12,318007.0
93389,393025.0,it has already been asked many times and answered the answer is actually on the submission page of the assignment just the sql and pdf file is needed,300694.0
93389,393155.0,You don't need to upload the csv files. Just submit the MySQL file and the PDF file.,319721.0
93330,392723.0,It will Impact the Grading as per TA .,301648.0
93330,392738.0,"It will be good, if we use same column name mentioned in Assignment. See below discussion. https://learn.upgrad.com/v/course/208/question/92567 See response from Sushmitha Belede. Quote and unquote Hey, guys please understand that there are constraints to verify and grade multiple assignments. So, you are asked to use the same name as provided.",307495.0
93114,392009.0,"hi, please find below the link from the discussion forum with the answer to your question https://learn.upgrad.com/v/course/208/question/92744 Hope this helps",317149.0
93114,392051.0,"use SHOW VARIABLES LIKE ""secure_file_priv""; and load your csv file from the result directaory.",306244.0
93114,392150.0,"Hi Parinita, I recommend to follow the approach (GUI) I have described in my answer here: https://learn.upgrad.com/v/course/208/question/92909 Hope it helps!",318355.0
92938,391320.0,I am calculating using sql,301107.0
92938,391324.0,Thank you for confirmation. I am not sure If i can share my sql query for confirmation.,316215.0
92938,392472.0,you need to do this using mysql query.,317811.0
92938,391328.0,"whole assignment has to done in sql, query needs to be submitted for evaluation.",306244.0
92938,391375.0,This needs to be done in sql as we need to submit only sql files.,317689.0
92938,391448.0,thq. Finally I made it using rank() created sequence table by date.,316215.0
92938,391515.0,Needs to be done in sql,305846.0
92938,391697.0,"Pleae calculate in SQL Query you can use below code. Need to modify .. SELECT Date, ROW_NUMBER() OVER (ORDER BY Date ASC) RowNumber, ClosingPrice, IIF(RowNumber &gt; 19, MA10, NULL) MA20, IIF(RowNumber &gt; 49, MA30, NULL) MA50, CASE WHEN RowNumber &gt; 49 AND MA20 &gt; MA50 THEN 'Over' WHEN RowNumber &gt; 49 AND MA20 &lt; MA50 THEN 'Below' ELSE NULL END as Signal from bajaj1",300735.0
92938,391978.0,"calculated first in excel , converted to graph , and then started doing in sql",318005.0
92938,392178.0,"You need to do it in MySQL only. Please do not perform operations in an excel and import that data. And please understand that this is an assignment to test your SQL skills, so if you use excel, you will not be evaluated. But if it helps you to visualise data using excel you can do it in there. But remember that while importing the dataset to sql, you cannot modify it.",319721.0
92938,392350.0,"You can you window function in sql to calculate 20MA and 50 MA values and it is preffered to used sql only,",318427.0
92938,392298.0,"their is a special designed plugin for ma20 ,ma50 but its better use excel formula for average using and ,or ,if etc to work on ,in this way u can learn more about formulas in excel,,, in the end excel,sql,python,tableaue all will be there to help u but do this only after completing sql assignment. or can do if have time.",318005.0
94360,397484.0,"Hi , Are the column sizes same for the both tables? , if not create tables with similar column size. or use the below command to crate new table and copy the data from old table. create table table-name as select * from old table",305652.0
94360,397491.0,Generally this kind of error comes when there is a mismatch or wrong use of data type. For ex: float() and decimal() are almost similar with little difference but sometimes If we use decimal() whereas it was required float() then this kind of error comes. Try to chane the data-type and see if error is still there or not. Below are the examples of same error: https://stackoverflow.com/questions/14764080/error-1265-data-truncated-for-column-when-trying-to-load-data-from-txt-file https://stackoverflow.com/questions/15169788/getting-a-mysql-error-code-1265-data-truncated-for-column-when-doing-a-load-da?rq=1,317991.0
92454,389487.0,I cross checked there are no null values in close price of any dataset. There is a possibility that you are getting null values because of some logical error or data type mismatching in 'close price' column. Check the data type you are giving while creating table. Hope this will help.,317991.0
92454,389527.0,"Hi Ram, There are no no null values in data set , if you are using import wizard to import the data then some rows will be silently ignored because of the import wizard setttings and other column null values. https://learn.upgrad.com/v/course/208/question/92100/answer/387907 There must be 889 rows after importing CSV file in each table.",305652.0
92500,389609.0,if you're talking about the UDF then mostly based on the discussion forum we've assumed showing a signal as Hold.. because might be if you're owning any stock better to hold it then taking a call on either selling it or buying it..,316349.0
92500,389767.0,"I guess it will be a hold, if we think of it practically as NSE/BSE- we cannot sell or buy the shares on a holiday, leaving us of an option of Holding the stocks,, similarly i guess will be an Hold here as well.",305129.0
92500,390084.0,"Some of the dates we get may be holiday but does not matter, just see whether 20DAYMA cross 50DAYMA or not. Basis on same you should generate signal. Also Data is taken from NSE/BSE site. It wont have holidays data",317811.0
92500,390547.0,"This is all based on our way of doing and assumption If we are using the 3rd question output in our UDF , then we don't need to worry as holidays are not present . If we are building the same logic as used in 3rd question , we can use case below logic CASE WHEN ((20DayMAPrevious&lt;50DayMAPrevious) &amp;&amp; (20DayMA &gt; 50DayMA ))THEN 'Buy' WHEN ((20DayMAPrevious&gt;50DayMAPrevious) &amp;&amp; (20DayMA &lt; 50DayMA)) THEN 'Sell' ELSE 'Hold' END as ""Signal"" in the above logic , for Holidays we don't have any 20MA or 50 MA value so it will be Hold . Hope this help ..",311861.0
92500,391020.0,"We can either buy or sell on weekdays only. We cannot do so on weekends or holidays. So, it will be a hold for those days.",319721.0
93148,392169.0,null and na are different and have different meanings there is alot of debate on this so for the purpose of assignment use null only,318017.0
93148,392158.0,"If you don't find the moving average, it should be HOLD. Don't insert NULL or NA.",312479.0
93148,392137.0,"I dont think it matters, since it will be anyways ignored, keeping it null will be same as providing default column value against writing NA explicitly, either of these should be fine i think. i would stick to null",319770.0
93148,392192.0,"Assuming from your query two scenario is there where we don't have Moving Averages. 1. While calculating 20DMA previous 19Day MA is irrelevant and for 50DMA previous 49Day MA is not required. So for this case you can either fill them with null or just delete them as informed by TA and you can check on below link: https://learn.upgrad.com/v/course/208/question/92790 2. Date which are not given in dataset i.e weekend and holidays. In this case also only working days are considered. So, data on weekends is irrelevant.",317991.0
93148,393392.0,"Those are float data types , so better keep it NULL",315679.0
92588,390060.0,what is stopping you from installing version 8+? It takes about 15 minutes .. so suggestion would be to install MySQL v8+,300694.0
92588,390092.0,Install MySQL v8. The installation guide also suggested to use MySQL 8.,317689.0
93122,392049.0,as per instuction all changes has to be done on SQL. refer link : https://stackoverflow.com/questions/9426993/mysql-change-date-format,306244.0
93122,392121.0,"No, please don't change anything in dataset. Please read the instructions provided @ https://learn.upgrad.com/v/course/208/session/19885/segment/101164 """" You must go through these guidelines- Make sure you have not made any changes to the original dataset provided to you. Your SQL code should work on the dataset given to you as part of the problem statement. You are not allowed to make modifications in the dataset using excel and then use it in your SQL code. Entire data processing must be done in SQL only. During grading we will be running your code on the dataset provided by us, in case your code gives errors with that, then marks will be deducted accordingly. """"",312479.0
93484,393582.0,IIF is a function in MS SQL server. IF is the equivalent function in mySQL.,302738.0
93484,393615.0,Please visit :http://www.mysqltutorial.org/mysql-if-statement/,312479.0
93484,393592.0,"Hello Lovis, Instead of IIF, Please use IF...ELSEIF...ELSE.. You will get result for Signal.",320195.0
93388,393084.0,They have mentioned in a few of the comments that it doesn't matter what you do with those 50 rows. You can ignore these.,318397.0
93388,393028.0,that would be giving away the asnwer to a graded assignment try looking at mysql manual to see how you can limit updating just the first n number of rows to null using sql,300694.0
93388,393051.0,"The logic would be to add the row number column while creating a table and apply the condition for first 49 rows. Once you apply the condition, get rid of the unnecessary columns. Hope this helps!",310522.0
92561,390579.0,"Yes, absolutley correct. Even, I thought the same. We cannot use the SP/UDF. At present, I have added it for all the 6 tables.",312479.0
92562,390281.0,"only with LAG function, i dont think you will be able to generate the signals. You need to use both CASE and LAG functions together in your query. You can write your conditions to check if the 20 day MA is above 50 day and your previous row's 20 day MA is below the 5o Day MA. Then signal will be ""BUY"" Write the similar logic for ""SELL"" and rest all will be ""HOLD"" The conditions should be within the CASE and END function.",316202.0
92562,390042.0,"use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92562,390909.0,"Check the date format in the table with the date format you are using as input, It may happen that one of the date format is in text while other is date format resulting in mismatch",310629.0
92562,391955.0,"I used the below logic i get this for buy and sell . but i guess its wrong , could anyone pls help.",306735.0
93139,392105.0,"try changing the order of the tables you are passing to check. Check your dates in each table, convert dates of all tables string to date.",306244.0
93139,392111.0,"It seems that your join statement is wrong. You have datasets of Bajaj and other compinies where data of 'date' column is same across the datasets. You can JOIN on the basis of Bajaj's date with other tables' (TVS, Infosys etc) date columns.",312479.0
93139,392115.0,Make sure that you are selecting the correct filed in the select query corresponding to each of the tables.,313826.0
93139,392593.0,Use the order as per given table.,308639.0
93140,392130.0,The summary report should be maximun of 250 words and sholud contains deatils on the summary you can dervied for the table you have created.,318476.0
93140,392096.0,"You don't need to make graphs. It should have a maximum of 250 words. In my opinion, a small project report which should have the information on what you have achieved with the help of SQL statements. Suggesting to add comments in SQL file to understand SQL statements by a third person.",312479.0
93140,392215.0,You can go through below link which has similar kind of question and response from TA. https://learn.upgrad.com/v/course/208/question/90991,317991.0
93140,392598.0,In my opinion we should write about how we approach to solving this problem and how we derived the solution and our interpretation of the data .,306011.0
93140,393428.0,You can use graph if you feel the results would be depicted better. However the requirement is 250 words. The objective is to present a conclusion on the assignment based on the data provided,310629.0
92564,389924.0,"No, in question 3 itself you're supposed to generate the signals either BUY oe SELL or Hold. in question 4 you just have to create a UDF which checks for the date in Bajaj2 created in question 3 and shows the signal for that date..",316349.0
92564,390104.0,In the question 4 UDF you can use the table created in question 3 to populate the signal.,317689.0
92564,390556.0,"I think we can do either way based on one's way or assumption . Simple way , use the output generated in question 3 and UDF will check this table and give the signal for a particular day or , we can build the same logic as done in question 3 inside a UDF too.",311861.0
92564,390267.0,First Create tables as per question 3. Then you need to create a function which simply gets the Date as input and retrieve the Signal data from the table you have created in question 3.,316202.0
93450,393385.0,It means sql file with comments and the pdf in a zip.,317689.0
93450,393553.0,yes it seems to be a type - I didn't even notice it; the commenting is referring to the SQL file - same as when we did the python assignment - we need to write good comments in our code so assessor can understand what the student is doing and why,300694.0
93450,393719.0,It Means that Please put comments with every query in SQL file so that it can be eaily understood at time of assessment.,317811.0
93142,392146.0,"Use the window function lag. Ex lag(condition,number of row preceding) over() Also, don’t forget to define the window over function and use the defined name. I think it’s better optimized with window lag function.",312376.0
93142,392127.0,We need to use window function like avg(cloumn_name)over (&lt;you window side&gt;) to calculate the moving average. Using of lag fuction depends on the implementation to get the previous day lags for MA20 and MA50 days.The same can we done using slef join over row number.,318476.0
93145,392134.0,https://learn.upgrad.com/v/course/208/question/90991 https://learn.upgrad.com/v/course/208/question/91875 https://learn.upgrad.com/v/course/208/question/91325 Same question has been answered multiple times. These are some of those answers which have been already verified.,301652.0
93145,392326.0,"You can include anything you observed while carrying out the analysis. Here, what is expected is that you need to summarise your findings from the analysis by stating respective facts obtained from the same. You don't need to use a lot of financial jargon and complicated financial definitions. A basic understanding of the analysis needs to be summarised. Imagine you are at an interview and the interviewer asks, what did you achieve from th assignment? Then the summary should help you answer his question.",319721.0
92565,390537.0,"You can use like - str_to_date(date, '%d-%M-%Y'); It will give you a date in dd-mm-YYYY format.",312479.0
92565,389929.0,"str_to_date function should work. i.e. STR_TO_DATE( string , format ) Here is the reference : https://www.w3schools.com/sql/func_mysql_str_to_date.asp Example: update table set date=str_to_date(date,'%d-%M-%Y');",320195.0
92565,390041.0,"SET Date = STR_TO_DATE(Date, '%e-%m-%y');",317811.0
92565,390066.0,check the date time masks table -&gt; https://www.w3schools.com/sql/func_mysql_str_to_date.asp you are using the wrong format when using %d-%m-%y - possibly all three are wrong,300694.0
92565,390103.0,%m and %M both are different. For full months we use %M. Please refer to date conversion str_to_date() documentation for more details.,317689.0
92565,390569.0,While using str_to_date function pls look in which format you have data in your source file also as it will impact in which format to mention desired query of str_to_date..function sample already mentioned in earlier answers,319869.0
92565,390337.0,"You can use Update tablename set date = str_to_date(date,'%d-%m-%y'); , but this will not change the datatype of the column. If you check by using describe tablename it is still car only. So to change the datatype of column , you can use alter table tablename modify column_name datetime.",318448.0
92565,391073.0,"The date format provided in the csv file matches the format ""%e-%M-%Y"". Use this format string in the str_to_date function.",306246.0
92565,390244.0,"If the input string is illegal, the STR_TO_DATE() function returns NULL. The STR_TO_DATE() function scans the input string to match the format string. the function - STR_TO_DATE(str,fmt) fmt - This means format. One such example for it is ""'%d,%m,%Y'"" %Y Four digits year e.g., 2000, 2001,…etc. %y Two digits year e.g., 10,11,12, etc. %D Day of the month with English suffix e.g., 0th, 1st, 2nd, etc. %d Day of the month with leading zero if it is 1 number e.g., 00, 01,02, …31 %M Full month name e.g., January, February,…December %m Month name with leading zero e.g., 00,01,02,…12",319006.0
93088,391910.0,"If you are referring to the assignment. I would suggest do not use insert statements at all. use CTAS (create table as select). e.g- CREATE TABLE artists_and_works SELECT artist.name, COUNT(work.artist_id) AS number_of_works FROM artist LEFT JOIN work ON artist.id = work.artist_id GROUP BY artist.id; Please refer below for more info - https://dev.mysql.com/doc/refman/8.0/en/create-table-select.html",312746.0
93128,392079.0,Prepare a select query from bajaj table and use create table table on top of this select. It would look something like create table bajaj1 as select from bajaj,304814.0
93128,392116.0,make a query to create table followed by select using window function from original table.,306244.0
93128,392119.0,"CREATE TABLE IF NOT EXISTS Bajaj1 AS SELECT col1, col2 FROM Bajaj You will have to write a SELECT query to fetch the records from Bajaj table as per the question.",312479.0
93144,392143.0,"I think inner join is the better. It forms a table by comparing tables on date and populate the closing price. Optimization is not just the time taken to run the query, it’s also about using the inbuilt functions instead of making the query longer.",312376.0
93144,392202.0,Solving a particular problem can have many approach. Follow that approach which give better overall performance whether it is join or by using where clause.,317991.0
93144,393161.0,"As this is a relatively smaller dataset, the time taken for both is almost the same. But the number of computations done while we use one of these differs. So, here, you're expected to use a function which performs relatively fewer steps to arrive at the results.",319721.0
93144,394591.0,I used inner join. But duplicate rows are being created.Trying to elimniate..But couldnt. Any idea.,308638.0
92567,389967.0,"Hi Ranip, i used the same name they provided like Date and Signal. but these are keyword in sql so to use these name we have to enclosed these name with back tick (`) like `Date` , `Signal`. but lets TA suggest, whether we can change the column names or not.",320073.0
92567,389965.0,I asked this question some time back. it was advised by TA that we should use the given column only. the assignments code will be cross checked with the given column names only as I understand. you can use backticks to handle the column name conflict.,311686.0
92567,390062.0,I Also used different column names (I did not change the excel file but in my script I renamed column names). I really hope we are not expected to use the same column names because those are some messy names,300694.0
92567,390259.0,"Hi Ranip, As mentioned in Data Set page , it is better to use the column names mentioned . They are not standard column names , but you can use back tick (`) as mentioned and for selecting rows you can always use &lt;select `column name` as col1&gt; different column names of choice. Please refer: https://learn.upgrad.com/v/course/208/question/91600 Thanks.",305652.0
92567,390263.0,Please use the same column names as per the data set provided. They will be executing your query using the data sets provided and it might cause an error if you change it. Enclose the column name with the back tick(`) and it works well.,316202.0
92567,390292.0,"Suprised to know the COlumn names should be as mentioned in the Assignment. but as highlighted the columns Date, Signal are should be used within back quote symbol ( `` ) From python assignment experience, the chances are high on tough validation",318454.0
92567,390782.0,"Even with ticks column names tend to break down sometime down the line. specially when you need to get the same column from multiple table in a same query. Even table.column syntax won't work with spaced column names. If we are not changing the main data source then shouldn't it be fine to handle these columns smarter way, maybe with an underscore?. Why to break the thing and then fix it?",312490.0
92567,391077.0,"my mentor said he checked with TA and we have to use the same names as provided - I think that is really wrong .. but oh well .. have to do as we have to do: ""Yes I checked...You need to use the same column name as it will be graded accordingly.""",300694.0
92567,391274.0,"Hey, guys please understand that there are constraints to verify and grade multiple assignments. So, you are asked to use the same name as provided.",319721.0
92592,390052.0,"Please Help Somebody, I have imported data using MySQL Workbench but only 888 rows are imported for Bajaj Auto.csv where as there are 889 rows, I even tried to change the coalition to Latin1, but of no help.",300734.0
92592,390099.0,"could help you if you had a windows machine Mac - sorry, hardly anyone uses those for doing real development perhaps install a virtual machine and install windows? then it will be easier to get help also in the future",300694.0
92592,391283.0,"Hey Rishi Kanth, you can load the tables manually using the UI if the commands aren't working. Once you create a schema, right click on it and open Table Data Import Wizard. You can add the csvs from here.",319721.0
92438,389353.0,"all the data needs to be imported to the tables. if table import wizard is comfortable ,we can acheive importing null values by changing the data type for the columns which has null values,search the forum for the same question answered multiple times.",301115.0
92438,389366.0,While using table import wizard there is no option to change the data type to varchar as mentioned in the article. Has anyone used the Table wizard task to complete this?,300698.0
92438,389390.0,Use load data inpath statement to import rather than importing from import table wizard as import table wizard cannot handle data quality issues.,317689.0
92438,390008.0,Got it.. I changed the data thpe of two columns as text and was able to impart all 889 rows .,300698.0
92571,390007.0,20 day MA would be created by using Windows function and we can not updatea table using Windows..,300698.0
92571,389961.0,Try creating and inserting data into bajaj1 in single query. for reference you can search how to create a table from another table. hope this will get you some idea.,320073.0
92571,389962.0,ideally the column needs to be populated while creating the table itself. the code needed is: create table Bajaj1 select .......... from Bajaj:,311686.0
92571,390040.0,"CREATE TABLE bajaj1 select Date, Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date. from bajaj",317811.0
92571,390100.0,"Use CTAS statements to do such tasks. It stands for Create Table as Select. create table bajaj1 from select a,b,c from bajaj. Now this query select a,b,c from bajaj can be a simple select statement where you can write different window function etc. based on your logic and understanding of the rule.",317689.0
92571,390338.0,You can use Create table bajaj1 AS ((select statement with column names)),318448.0
92571,390744.0,Using window function you can create the required column Refer the following link will give you idea on the same:- https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/,303673.0
93154,392163.0,use to_num function,318017.0
92449,389378.0,"The point when the 20dayMA and 50dayMA cross and move above or below is the point where signal to be generated for buy or sell. post this point if 20dayMA or 50 dayMA remains above or below the point of intersection as above the signal will be hold. if again the two averages cross each other and move above or below that point will be signal to buy or sell , post this point subsequent MA will result in hold till the time they cross each other again",310629.0
92449,389384.0,"Try plotting the values. Generate a signla when both the lines cross each other. i.e lets say MA 20 day was 200 today and MA 50 day is 205. If yesterday MA 20 day was 195 and MA 50 day was 210. So, in this case HOLD. But if tomorrow MA 20 day becomes 210 and MA 50 day becomes 206 than that means MA 20 day has crossed the MA 50 day so generate signal as BUY. And if it continues to be higher going forward than it should be hold. Hope its clear now. So, you need to change the signal as soon as it crosses the line chart. consider the blue line and red line as 20 day MA and 50 day MA. So, only for the days having cross over highlighted points you need to generate signal.",317689.0
92449,390081.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92449,390136.0,https://learn.upgrad.com/v/course/208/question/92353,301555.0
92449,390508.0,You have to put condition like when 20dayMA is more than 50dayMA and previous row value of 20dayMA is less or equals to revious row value of 50dayMA then buy when 20dayMA is less than 50dayMA and previous row value of 20dayMA is more or equals to revious row value of 50dayMA then sell please find the below link for previous day value: http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/,319444.0
92602,390113.0,I don't see any error in this statement. Can you tell the error code you are getting ?,317991.0
92602,390134.0,"Hi Prakash, Your syntax is wrong: Refer following for correct syntax:- SELECT * OVER (ORDER BY id ROWS BETWEEN 'no.' PRECEDING AND CURRENT ROW) FROM mytable link for reference: https://stackoverflow.com/questions/5462205/mysql-select-function-to-sum-current-data",318429.0
92602,390223.0,AVG(`Close Price`) OVER (ORDER BY Date ROWS BETWEEN 19 PRECEDING AND CURRENT ROW),317811.0
92602,390308.0,"The query looks fine. Please check if the previous row is properly ended with "";"" . SQL might have considered this query to be a continuation of previous row if its not closed with ; Because error 1064 comes if the syntax is not correct. In your query, it looks fine.",316202.0
92602,390518.0,AVG(`Close Price`) OVER (ORDER BY Date ASC ROWS 19 PRECEDING),319444.0
92602,390775.0,"1. Make sure previous query ended with "";"" 2. Between columns names specify comma(,) and also average functions 3.specify quotes for '20 day ma' also.",313200.0
92602,390879.0,Which version of SQL are using? Update the version to 8.0 and try again,303673.0
93565,394089.0,"hi, you can use table data import wizard.. and while importing it just set the datatype of deliverable and deliverable quantity % to text in order to get all the 889 rows.",305129.0
93565,394097.0,"Run command SHOW VARIABLES LIKE ""secure_file_priv""; You will see a path where you can put the csv file and then it can be imported. Also, while importing use forward slash / instead of bacward slash \ in filepath.",317811.0
93565,394103.0,You can directly import the CSV file using the import wizard which is easy way... Kindly go through the below link: https://www.youtube.com/watch?v=F_L-8vkDpi0&amp;t=74s,303673.0
93565,394157.0,Just place your CSV files in c: program data/MySQL/MySQL server /uploads then load data,319869.0
93567,394075.0,Yes.. Evaluator will cross check with given database in SQL.. We need to upload only the sql script file,303673.0
93567,394087.0,thanks venkatesh,308495.0
93567,394158.0,"Not an issue, these are not script errors. Since the database or schema will be unavailable until you connect with the database, it will show error. Go ahead with the upload.",315028.0
93567,394152.0,Don't worry they will check it syntactically and indentation etc and your approach .For more clarity visit Grading Criteria,319869.0
93161,392194.0,A problem can have many solutions. Your question forced me to read the artical http://www.mysqltutorial.org/introduction-sql-views.aspx,312479.0
93161,392225.0,"See as a first thought I think, if we use different tables, these can be referred to implement different functions and not change the entire dataset. Like if there are tasks I need to perform on the entire data and there are other tasks which need to be performed only on a part of this data. If I have tables created I can perform specific tasks on them without having to have my entire dataset changed.",319721.0
92895,391147.0,https://stackoverflow.com/questions/32737478/how-should-i-tackle-secure-file-priv-in-mysql,300694.0
92895,391295.0,"Execute below command in MYSQL SHOW VARIABLES LIKE ""secure_file_priv""; then it will show the directory from where the files can be accessed by MYSQL without any restrcitions. so,paste all the csv files to that directory obtained above and try to load data now. It will be successfull",317811.0
93466,393469.0,"hi, pls see the screnshot attached: thus, you need to keep a track of previous data (i.e. previous row) as well while generating signal.",311686.0
93466,393474.0,"Hello Shubham, You need to use Lag function and case..logic will be what you need to do is, COMPARE the comparison of todays ma20,ma50 and yesterdays ma20,ma50. for eg, if yesterdays ma20 was lower than yesterdays ma50 AND todays ma20 is greater than todays ma50, then THAT would mean a BUY signal. and vice versa for sell signal.. hint: read about lag function to get values of previous day.",320195.0
93466,393548.0,this has been asked sooo many times on the discussion forum. please do a search of the forum postings,300694.0
93466,393459.0,"you've to generate the signal for current data with reference to the previous data.. for example; currently your 20MA is greater than 50MA but for the previous date it was opposite 20MA was less than 50MA and in this case, 20MA is crossing over 50MA and hence your signal will be BUY anbd vice versa.. Previous date data is just for reference, your signal will be on current date date..",316349.0
93466,416722.0,WHAT IF 20MA AND 50MA ARE SAME FOR FIRST ROW?,311046.0
93468,393461.0,"Hello Rajat, These rows don't really matter. So you can populate them with NULL or remove them.",320195.0
93468,393506.0,"Ignore , as you cannot compare the MA values yet.",318340.0
93468,393465.0,You can set them as HOLD. The assignment has mentioned that the status should be HOLD for other than BUY and SELL. You can also ignore those rows irrespective of the signal.,310522.0
93468,393644.0,it better to keep them null because it might disrupt the data..,305129.0
92577,390075.0,there are other ways besides the import wizard to import data look into 'load data infile' - which has been mentioned a few times on the discussion forums,300694.0
92577,389979.0,You must have taken the Data type and INT. Try with VarChar and allow NULL and then try. e.g. Column Name Varchar(10) NULL,316036.0
92577,390009.0,"(cola, colb, colc, cold and so on) SET ColumnName = IF(columnname = ' ', NULL, columnname), while importing use above command for the columnname for which your have null values in csv",317811.0
92577,390260.0,"There are NULL values for ""Deliverable Qty"" and ""% Deli. Qty to Traded Qty"". Hence these rows will not be uploaded if you do via wizard. You can either manually write a Load data infile query and modify the data types for these two. or You can modify these 2 columns data types while importing via wizard. You will have all the columns listed while doing the import via wizard. You can change it in that section.",316202.0
92577,390768.0,"Try by changing the data type of ""Deliverable Qty"" and ""% Deli. Qty to Traded Qty"" while importing wizard",303673.0
92577,391059.0,"USE load data infile function and replace the "" ,, "" in the csv file with "" ,\N, "". This way you can retain the data type of the field and not change it to text.",306246.0
93472,393546.0,this has been asked sooo many times on the discussion forum. please do a search of the forum postings,300694.0
93472,393501.0,"format the date column to dd-mm-yyyy in the excel file itself and then import it. Post import you can use DATE_FORMAT(STR_TO_DATE(date,""%d-%m-%Y"" ),""%Y/%m/%d"" ) to pdate the respectice column and then alter it to date datatype.",318340.0
93472,393716.0,"while importing file via file load command after IGNORE 1 ROWS; please add below line as well (@date) 'Set Date=str_to_date_(@date, '%e-%m-%y')",317811.0
93472,393487.0,"Hi Lovish, Please go through below link for better understanding. https://learn.upgrad.com/v/course/208/question/92921 Firstly you can update the `Date` Column values of all the stock tables separately and then modify the column datatype which will make rest of the assignment tasks easier. -- update date column values from string to date format update &lt;table_name&gt; set &lt;column_name&gt; = STR_TO_DATE(&lt;column_name&gt;, '%d-%M-%Y'); -- modify date column datatype to date alter table &lt;table_name&gt; modify &lt;column_name&gt; &lt;date datatype&gt;;",320195.0
94442,397959.0,"I am using insert into statement. But using decimal makes it a string, right?",310472.0
94442,397946.0,"Hi , what is the command you are using ? how about using decimal data type instead of double data type ?",305652.0
93163,392209.0,"Hey Dinesh, can you tell me the process you used to import the data? Also, you can easily import it using the import Wizard instead of coding to import..",319721.0
93163,392188.0,It might help you . - https://stackoverflow.com/questions/17158367/enable-binary-mode-while-restoring-a-database-from-an-sql-dump,312479.0
93163,392200.0,"now this error ERROR 1064 (42000) at line 1: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Date,Open Price,High Price,Low Price,Close Price,WAP,No.of Shares,No. of Trades,' at line 1 Operation failed with exitcode 1",319969.0
93163,392224.0,"here are my steps. 1. run the sql work bench 2. used create a new scehma button ,then created new file name i.e new assignmnet 3. then used import the file , it is at my desktop then unziped that file and used bajaj file 4. used import button .. then file imported with 1064 named error . what is wrong and how to fix it",319969.0
93163,392859.0,You will get a `table wizard import` data option if you right click on the Assignment schema,315889.0
93163,392857.0,"Hello Dinesh, You can refer to the Aditya Chopra's answer in this question for smooth and easy import of .csv files https://learn.upgrad.com/v/course/208/question/92909",315889.0
93474,393492.0,"Hello Krunal, Please go through below link for better understanding. https://learn.upgrad.com/v/course/208/question/92921",320195.0
93474,393503.0,"format the date column to dd-mm-yyyy in the excel file itself and then import it. Post import you can use DATE_FORMAT(STR_TO_DATE(date,""%d-%m-%Y"" ),""%Y/%m/%d"" ) to pdate the respectice column and then alter it to date datatype.",318340.0
93474,393542.0,this has been asked sooo many times on the discussion forum. please do a search of the forum postings,300694.0
93164,392213.0,"Hi Rajeev, Assignment needs to be submitted in a zip file containing SQL file which have all the commands used for analysis and PDF file with analysis written. I think PDF looks more readable than the wordpad , you can simply go to word pad-&gt; file -&gt; choose 'save as adobe pdf' (provided you have adobe installed). Thanks.",305652.0
93164,392207.0,"The format should be pdf. You can write the summary in WordPad. But this needs to be converted into pdf format and be submitted. Please understand that there are constraints while grading. To ensure consistency in all the submissions we need to follow a specific format. You can convert word to pdf like this,",319721.0
92368,389122.0,"The above steps solved the issue, thanks Anuj.",318448.0
92368,389083.0,I have solved the issue - Steps for Mac High Sierra - =========================== 1) goto $HOME (For me it is /Users/anujgarg) 2) vim .my.cnf 3) Copy/past below lines - [mysqld_safe] [mysqld] secure_file_priv=/Users/anujgarg 4) Open System Prefences 5) Open MySQL Configuration Window 6) Click Configuration Tab 7) Give the below file name in Configuration File text box. /Users/anujgarg/.my.cnf 8) Click on Apply 9) Goto Instances Tab 10) Stop and Start MySQL,312479.0
92368,389631.0,The steps solved the issue. Thanks Anuj!,318335.0
92368,390869.0,"Not sure why, but this is still not working for me. I recently updated to macOS Mojave!",312376.0
93165,392199.0,"Hey, convert the datatypes of different columns and try. This is an error which occurs when a datatype has values which it cannot support.",319721.0
93489,393602.0,perform all sql queries in one single file. And summary writeup should be a pdf file. You need to zip these two files only and upload for submission.,309451.0
93489,393601.0,"Yes, you can write queries in multiple files. Please make a zip which includes your all SQL files along with PDF.",312479.0
93489,393714.0,Please write all queries in SINGLE SQL FILE. There should be total two file one will be sql and another will pdf.,317811.0
93489,393608.0,"For more information : https://learn.upgrad.com/v/course/208/session/19885/segment/101164 "" Here are the steps that you must follow during submitting any assignment- Collect all the files (if there are multiple files) and compress them together. Try to upload this compressed file latest by 11:30 PM Download your submission and check that you have included all the required files. Check that none of the files or the zip is corrupt. If it is found to be corrupt during grading, you will NOT be allowed to re-submit. If you are 100% sure that you will not need to make any more changes in the assignment, click “Submit for Grading”, else, just let it be. Unless you remove it, it will be automatically submitted at the upcoming deadline. """"""""",312479.0
93489,393989.0,"Hey, Please write all queries in a single file. A small write up not less that 250 words in PDF format.and Zip these two files together for Submission.",306729.0
93489,394139.0,You can copy all the SQL queries in a single file and then upload with the PDF.,315028.0
93489,394878.0,you can write your quiries in mulitple file then zip in one folder and can upload. there is no problem with that.,300735.0
93489,394993.0,"we could have written queries in multiple files ,,,but their should be some mechanism to tell whch to run first. if same names of files were to to be transferred then keep them as assignment_sql_1 ,2,3...etc,, or could have been contacted student mentor for the same..",318005.0
92610,390209.0,"lag(`20_DAY_MA` -`50_DAY_MA`,1) does not make any sense here. lag(`20_DAY_MA`) -lag(`50_DAY_MA`) &lt;0 use something like this",317811.0
92610,390245.0,"As this is graded, i cannot provide the syntax. But you are in the right path, Use the CASE statement to check: if 20 Day MA is &gt; than 50 Day. If Yes, use the Lag function to check the same condition (20 Day &gt; 50 Day). and if both is true, it will be Hold else it will be BUY. You have to repeat this for sell criteria.",316202.0
92610,390573.0,"I am curios to know why window function is requried here. Any how, it has to compare both the values (20 day ma and 50 day ma) row by row. If you use window fucntion, it won't be overhead ? The same can be achieved using CASE (w/ SELECT) or IF(w/ SELECT).",312479.0
93531,393807.0,"Hi Murali, Logic should be like below using CASE and LAG : Bajaj1 table WHEN `20 Day MA` &gt; `50 Day MA` AND 'Lag_20_MA' &lt; 'Lag_50_MA' THEN 'BUY' WHEN `20 Day MA` &lt; `50 Day MA` AND 'Lag_20_MA' &gt; 'Lag_50_MA' THEN 'SELL' ELSE 'HOLD'",320195.0
93531,393820.0,Add two more columns for Lag function which shows previous day's value adjacent to the current one. Then do compare the values among current one and lag ones. Hope this helps.,310522.0
93531,394054.0,"No, Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
93529,393804.0,`Close Price` column only.,313826.0
93529,393896.0,"Moving average in stock is clalculated on closed price of last intervals only, use Close Price column.",306244.0
93529,394049.0,"Yes, Moving average is calculated on Close Price in window of days",317811.0
92618,390243.0,right from the first to last row.. wherever it doesn't cross or null &gt;&gt; results in HOLD if crossed &gt;&gt; results in either BUY or SELL,316349.0
92618,390254.0,"The actual comparison for the cross over starts from 50th row (20 Day MS Vs 50 Day MA). So, you can either have ""Hold"" or ""NA"" for the initial 49 rows and the actual signal of ""Buy"", ""Sell"" or ""Hold"" starts from 50th row.",316202.0
92618,390342.0,All rows where 20DAYMA does not cross 50DAYMA should be hold,317811.0
92618,391288.0,"Hey, it doesn't matter what you give for the first 50 rows. Buy/sell/hold apply from the 51st row,",319721.0
92920,391330.0,Try create as (select),306244.0
92920,391262.0,Since this is an assignment Its not appropriate to ask you for sharing your query. Generally this kind of error comes when there is a mismatch or wrong use of data type. For ex: float() and decimal() are almost similar with little difference but sometimes If we use decimal() whereas it was required float() then this kind of error comes. Try to chane the data-type and see if error is still there or not. Below are the examples of same error: https://stackoverflow.com/questions/14764080/error-1265-data-truncated-for-column-when-trying-to-load-data-from-txt-file https://stackoverflow.com/questions/15169788/getting-a-mysql-error-code-1265-data-truncated-for-column-when-doing-a-load-da?rq=1 Hope this will help.,317991.0
92920,391237.0,"i am not sure about the error, just make sure you are using the reference of bajaj auto to create it.",305129.0
92920,391247.0,you are making some mistake please share your query,318017.0
92920,391299.0,"I faced similar kind of issue. Instead of updating the table give the window fucntion while creating table create table tablename select Date,`Close Price`, window function as 20DAYMA,.... from reference table; this should help you",303673.0
92627,390305.0,I don't find any issues with the query. The syntax is fine.May be check the previous row if it has any continuation. May be the previous query is not closed with ;.,316202.0
92627,390315.0,Try to Use MA inside single quote. 'MA' or `MA`. Though I dont think that will be an issue. Try to select(highlight) only the query and execute,318554.0
92627,390340.0,"dATE, AVG(`Close Price`) OVER (ORDER BY Date ROWS BETWEEN 19 PRECEDING AND CURRENT ROW )",317811.0
92627,390486.0,Check your version of MySQL. Window function is supported for version 8 and above.,317689.0
92627,390598.0,"Close your SQL workbench, restart the server and try again.there is a bug in SQL workbench 8.0.",306735.0
92627,391289.0,"Hey, PRakash, you need to update your version to 8.0 or above.",319721.0
92621,390237.0,"Hi, are you using wizard to create schema or script ?",319006.0
92621,390249.0,Try stopping the server and restarting it. It should work. Sometimes restarting the server helps.,316202.0
92621,390253.0,"Hi Lalitha, Please check if mysql service is in started state. If you installed work bench 8.0 then you will find service with name ' MySQL80' on below path start -&gt; services.msc -&gt; MySQL80 -&gt; right click and start if not started (restart if already started). Alternatively you can use command prompt to start the service: Open cmd with admin rights , execute the command &lt; net stop MySQL80 &lt; net start MySQL80 Hopo this helps.",305652.0
93530,393813.0,"Hello Sri, Please go through below links: https://learn.upgrad.com/v/course/208/question/92042 https://learn.upgrad.com/v/course/208/question/90176",320195.0
93530,393831.0,This means that you have to compare the current day's close prices as well as one day prior's close prices. This has been answered well in some other questions.,310522.0
93530,394051.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line",317811.0
92905,391248.0,Generally it is Closing Price. And also one of the TA has confirmed that it should be closing price. You can check the below link to verify: https://learn.upgrad.com/v/course/208/question/90737 In this link TA answered the same question asked by other colleague. Hope this will help.,317991.0
92905,391188.0,It's closing price. This question has been asked multiple times previously as well. You can search for them in the Discussions :),301652.0
92815,390919.0,There is a difference in %m and %M. %M is used when we have full month name in string. Try to change it and it shall work.,317689.0
92815,390943.0,"In your query, Use %m and %y in caps. E:G - str_to_date(Date, '%d-%M-%Y'). This should work.",316202.0
92815,391005.0,"In addition to the above suggestions, see that the date string you are trying to convert and the format that you give, are in the same order. You might stumble, if you attempt to convert STR_TO_DATE('2015-05-13', '%d-%m-%Y'). This will fail. In that case you may have to use '%Y-%m-%d'.",302740.0
92815,391010.0,"Try changing %d-%m-%y to '%e-%M-%Y' and chevk if that helps STR_TO_DATE('2015-05-13', '%e-%M-%Y') hope this should work.",317845.0
92815,391056.0,'%d-%m-%y) ' colon is placed wrong in ur query make this as below ' %d-%m-%y '),317811.0
92815,391463.0,"UPDATE `assignment`.`bajaj auto` SET `bajaj auto`.`Date` = str_to_date( `bajaj auto`.`Date`, '%Y-%m-%d' ) try dis it will work.",303115.0
92815,391417.0,"Day of the month as a numeric value (01 to 31) %m is generally used for Month name as a numeric value (00 to 12) %y is used for Year as a numeric, 2-digit value For your case, you might need to used %d %M, %Y %D is when Day of the month as a numeric value, followed by suffix (1st, 2nd, 3rd, ...) %M is generally Month name in full (January to December) %Y - Year as a numeric, 4-digit value Try as below, str_to_date(Date, '%d-%M-%Y') Cheers!",318080.0
92815,391676.0,"UPDATE `assignment`.`bajaj auto` SET `bajaj auto`.`Date` = str_to_date( `bajaj auto`.`Date`, '%d-%b-%y' ) this will work beacuse given format it dd-mon-yy",300735.0
92815,391752.0,"In addition there is another simple option in workbench. Click on your table and select alter table, on the column which you want to modify you can see dropdown and select date datatype as well.",318370.0
92815,391866.0,"Use the same format of Date as the data e,g if its YYYYMMDD, then use %Y-%m-%d if its DDMMMYYYY, then use %d-%M-%Y",308495.0
92815,391966.0,"If you look into the CSV file, the date is provided with complete names of the months like 'January' and complete year format like '2015'. In such cases the format that resembles is %d-%M-%Y. Please be informed that %m and%M are different for different formats and %y and %Y are also different for different. Use the format in which the data is present in csv to convert to the date and you should be able to do this.",311729.0
92815,392581.0,"First covert date as the date data type. If you convert date as data type then only you will get the expected output, if u don't change the data type as date it will gives you all the rows with YYYY-MM-DD which is not looks good.",308639.0
92815,392894.0,https://www.w3schools.com/sql/func_mysql_date_format.asp This link will help you to changes date format according to table used in assignment.,318077.0
92649,390407.0,"Hi , It is better to keep the records in ascending order from 2015 to 2018 for final analysis. but someone can confirm officially. thanks.",305652.0
92649,390483.0,There is no need to store data in sorted format in table. The window functions which you use can cater for the sorting of the data.,317689.0
92649,390421.0,I dont think order should be of any concern here as it is a table and we are not printing any report.,318084.0
92649,390432.0,"I think the Data has to be sorted by date in ascending order for bajaj1, as the moving averages are calculated from past to current data.",303229.0
92649,390534.0,"In my opionion, order is a concern. You can see the example provided in ""Problem Statement"" section where days are increasing from 1 to N. If you use the descending order (2018 to 2015) then intial 49 values of '50 Day MA' would be NULL which is not correct. After all we need to calculate moving average for the history of 4 years.",312479.0
92649,391665.0,"Hey, Harshit, let me repeat what you already understood just so everyone else is clear too. So, we need to do the analysis by taking the past day's value and current value. This will be done in an ascending way. Starting from the past and moving forward from 2015 to 2018. And while storing the obtained results, it's preferred that you do it in the same way, but as long as your results are right, the way you're storing it shouldn't affect grading.",319721.0
92911,391207.0,"No, you're understanding it wrong.. we don't just need to generate the signal for 49 rows.. but infact all the rows across your dataset.. first 49 rows is just going to be 'NULL' since your actually 50 days average will start appearing after 50th row.. so, apply your logic to generate the signals as 'BUY/SELL' for crossing over each other and 'HOLD' otherwise.. Hope it's clear now.. :)",316349.0
92911,391293.0,"When you dont have complete data to buy or sell, you will HOLD",317811.0
92911,392384.0,"For the first 50 Days, you will not be able to generate a signal. First 49 days: You don't have 50 Day MA Day 50: You do not have any previous data to compare to generate the signal. From Day 51, you have all the data to generate signals.",304281.0
95595,404746.0,"I have a pdf that I prepared for the analysis but there is no way I can share here. But, here's what I did, for each stock I assessed the way the buy/sell signals are generated based on the 20/50 MA. There were some inaccurate signals. What I mean by that is, there is a sell signal immediately after a buy signal for all the stocks many times. I concluded saying 20/50 MA may not be accurate to generate the signals. May be should look at a more long term one like 200 day MA.",310974.0
95595,405538.0,"The summary should have inferences derived from the analysis. Though there are many ways the interpretation can be carried out, we need to remember that the results carried out should be a part of the summary. The following can be a few of the many aspects included, a. Total number of buys and sells for each of the stock can be calculated and we can conclude that the stock with the highest of the numbers can be considered most volatile. b. The time period in which most of the stocks could be bought or sold can also be derived. c. There can be a small profit analysis with maximum and minimum profits obtained by calculating the differences of close prices when signals are generated.",319721.0
92909,391203.0,It would be helpful if you could share the complete error. Since this is not the actual error it is just the upper part of the traceback.,304391.0
92909,391204.0,,314313.0
92909,391226.0,There seems to be some problem with your dataset. In my opinion you should take care of the following things: 1. Download the dataset again and try to re-import the data 2. While importing the data please observe the columns that are being imported in the previous step. If there is any column named none then uncheck it. (Although there are no such columns in the given dataset),304391.0
92909,391228.0,"Is this happening for all the data files ? Can you try with other data files ? If it is the same then something should be wrong with the table creation.. i think it is better to drop the table, recreate and try importing the data again",300727.0
92909,391236.0,"This is my excel file. I have dropped the table and tried with other files too, still getiing the same error. Worried how to submit the assignment.",314313.0
92909,391241.0,Can you share the screen shot of your table ? and the query used to create the table ?,300727.0
92909,391581.0,"Are you using CSV or excel file for import? Use the CSV file and also import these two columns ('Deliverable Quantity' and '% Deli. Qty to Traded Qty') as a text for all files, since it contains null values.",312376.0
92909,392218.0,Aditya pl check the no.of rows inserted.,308641.0
92909,392393.0,"You can use Aditya's answer and also (as a general practice) only import the data that you actually need. No point in importing the complete table. And also check if there are any nulls and those columns have been marked as not null. In that case, that rows would not be imported and might hinder the analysis.",304281.0
92909,392135.0,"Hi Koustav, Please follow the screenshots and let me know if you're still facing any challenges: Hope this helps!",318355.0
92918,391230.0,"Hi, Yes you need to sort according to the dates in ascending order, so that in order to calc. the MA we need previous date reference.",305129.0
92918,391380.0,"You dont need to sort the saved data. While applying the window function the sorting can be taken care of. You just need to convert it to date datatype first and than it can be sorted inside window function,",317689.0
92921,391326.0,"Hi update command will not change the datatype of the column, for that we have to use alter command alter table table_name. modify column column_name datatype ;",319056.0
92921,391240.0,"I would suggest go table wise , update one table at a time.. instead going for updating the assignment directly.. set date= Str_to_date(date,'%d-%M-%Y');",305129.0
92921,391296.0,Change the data type to date after executing your statement. Alter table tablename modify columnname datatype. This will convert from varchar to date data type.,311502.0
92921,391378.0,Str_to_date converts it into appropriate date format which can later be converted in date type using alter table.,317689.0
92921,391355.0,"Firstly you can update the `Date` Column values of all the stock tables separately and then modify the column datatype which will make rest of the assignment tasks easier. -- update date column values from string to date format update &lt;table_name&gt; set &lt;column_name&gt; = STR_TO_DATE(&lt;column_name&gt;, '%d-%M-%Y'); -- modify date column datatype to date alter table &lt;table_name&gt; modify &lt;column_name&gt; &lt;date datatype&gt;;",311218.0
92921,392871.0,"update table table_name set date=str_to_date(date,%d-%M-%Y);",308432.0
92921,393521.0,hello rashmi could you help me with following command alter table &lt;table_name&gt; modify &lt;column_name&gt; &lt;date datatype&gt;; alter table bajajauto modify `date` date; is this right?,315650.0
92917,391231.0,Have you defined a column named row?,305129.0
92917,391291.0,use Row_number,317811.0
92917,391636.0,Try removing quotes enclosing the row. This might help.,313691.0
92917,392397.0,"I think you are using the single inverted comma, you should use the back quote (`)",304281.0
93537,393891.0,make sure you are using the correct schema when creating table by double clicking or by query use assignment; you can import using import wizard but make sure you are getting 889 rows.,306244.0
93537,393908.0,There are multiple ways to import a file in mysql. Best way here would be create table with appropriate datatypes and load using load data infile command to ensure that you donot miss any data.,317689.0
93537,394073.0,"Step 1: Create a table having all relevant columns. Step 2: Insert data in table created by importing csv file using below command LOAD DATA infile 'fullfilepath' INTO TABLE Ass_bajaj FIELDS terminated by ',' enclosed by '""' LINES terminated by'\r\n' IGNORE 1 ROWS; refer https://stackoverflow.com/questions/8163079/importing-a-csv-to-mysql-with-different-date-format",317811.0
93537,394209.0,You should only import the date and close price first then try to add 20 and 50 day MA columns since these are to be calculated and can not be imported.,304281.0
93537,396408.0,Please look.can you correct my code.How to calculate values of 20 Day MA and 50 Day MA.,320687.0
92819,390926.0,Please check the http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/ for better understading on LAG Function,318804.0
92819,390928.0,"Not sure, what is the logic that you are applying the LAG function in your query alsong with the window function. You can create a signal using the CASE and END function. Within this loop, use the conditions (20 Day MA &gt; 50 Day) and LAG (20 Day MA &lt; 50 Day). If this condition is met, it will be ""BUY"" signal. Write the similar query for Sell signal accordingly. Rest all will be ""Hold"".",316202.0
92819,390931.0,Lag and window functions are supported for MySQL version 8 and above. Check you MySQL version.,317689.0
92819,391000.0,I suspect this colud be systex error. Please go through below link to read more about lag function and lag() with case https://stackoverflow.com/questions/50985107/sql-lag-in-case-statement http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/ https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_lag,317845.0
92819,391111.0,"For lag function, you can follow this link https://www.techonthenet.com/sql_server/functions/lag.php",314183.0
92819,391281.0,am getting the same error... can someone please guide,303673.0
92819,391294.0,"Got it!!! Instead of updating the table give all the functions(Case and End loop, Lag fucntion) while creating table. Create table tablename select ... case end",303673.0
92819,391519.0,You cannot use WINDOW functions in UPDATE statement. This was mentioned in one of the sessions. Please refer to: http://learn.upgrad.com/v/course/208/session/19879/segment/101133,313691.0
92819,391684.0,"Try this might you need to twick little bit in code. SELECT Date, ROW_NUMBER() OVER (ORDER BY Date ASC) RowNumber, ClosePrice, IF(RowNumber &gt; 19, MA10, NULL) MA20, IF(RowNumber &gt; 49, MA30, NULL) MA50, CASE WHEN RowNumber &gt; 49 AND MA20 &gt; MA50 THEN 'Buy' WHEN RowNumber &gt; 49 AND MA20 &lt; MA50 THEN 'Sell' ELSE 'Hold' END as Signal FROM bajajauto",300735.0
92742,390576.0,"Actually, they have not asked a method to import the sample data into the tables. You can use either way. Many people (including me) faced the issue for importing the sample data using LOAD data INFILE command. It would be requried to add the secure_file_priv parameter in my.cnf file.",312479.0
92742,391030.0,"You dont need to worry about the way the CSV files are imported to mysql. Because the grading starts from Question 1 where we need to create tables for bajaj1, eicher 1 and so on...",316202.0
92744,390591.0,"Please disable secure file priv option. You can do this by updating the my.ini file available at C:\ProgramData\MySQL\MySQL Server8.0\my.ini file secure_file_priv="""". Once it is done, please restart mysql and workbench as well.",311502.0
92744,390614.0,"If it is still not working, try to move the Data sets to the directory specified by ""secure-file-priv"". You can have a look at this link, ""https://stackoverflow.com/questions/32737478/how-should-i-tackle-secure-file-priv-in-mysql""",303229.0
92744,391065.0,"Run command SHOW VARIABLES LIKE ""secure_file_priv""; You will see a path where you can put the csv file and then it can be imported. Also, while importing use forward slash / instead of bacward slash \ in filepath.",317811.0
92744,391158.0,"The Command returns me a null value. SHOW VARIABLES LIKE ""secure_file_priv""; Another thing, I am using Mac OS and can't locate my.cnf or my.ini file. So, I am importing the CSVs through GUI.",310522.0
92756,391959.0,"Importing files can be done any way you want to. Whenever your assignments are graded, the TAs will have these tables loaded.",319721.0
92756,390615.0,"As asked in problem statement you must create new Schema with specifically mentioned name ""Assignmnet"" You can name your tables like bajaj1,eicher1,hero1,tvs1 and so on and for signal create tables with same name like bajaj2 eicher2 respectively",319869.0
92756,390680.0,"We can mark our comments there, Grading will start from Q1 but before this we are writing code for Importing the CSV files in MySQL.",306244.0
92756,390703.0,You can put it in the script. There is no harm for having too much details in the code.,317689.0
92756,390985.0,"You can provide your comment at the start of the script page mentioning that you had imported the CSV using the wizard (If you have taken that approach). Because the grading starts from questions 1, which is from crating bajaj1, eicher1 and so on.",316202.0
92748,390649.0,I never had to load a csv file for the assignment that gives the above data. We only have to load the base stock tables from csv.,310974.0
92748,391132.0,"You shold not load any file for moving average. 1) Load the CSV presented into 6 different tables 2) Using SQL, identify the moving average from tables created in step-1 and load that data in baja1 table ( for Bajaj as Stock). You can use create table bajaj1 as select ......",304814.0
92598,390082.0,"well logically thinking about it - I personally did not create a signal for the 50th day because you can't determine a crossover based on one point .. you need two points so you would need the 50th day and the 51st day to make a recommendation for the 51st day. For recommendation on 50th day, you would need crossover data from day 50 AND day 49 - which we don't have",300694.0
92598,390088.0,"We dont have cross over details for 50th day as for 49th day there is no 50 DAY MA. So, i would suggest start signal generation at 51st day.",317689.0
92598,390144.0,"I would say having no signal is synonymous to the signal 'Hold'. Unless its not a Buy or Sell, I think you are good.",310511.0
92598,390225.0,"Yes, we have to. When there is no buy or sell. It should be hold.",317811.0
92598,390564.0,"To generate a signal we need two points (20 day MA and 50 day MA). I think, we cannot generate a signal if any of the value is NULL. So, it is good if you compare with the valid values ( NOT NULL) to generate a signal.",312479.0
92598,390774.0,"the 50 day MA is not actually a 50-day average till day 50 (too many 50s in the sentence!) For example, on the 30th day, the ""50 day MA"" that is being calculated is actually a 30-day MA. If you were the investor who believed in this 20day/50day MA strategy, you would not want to buy/sell based on the signal generated on the 30th day, right? I believe there can be two ways to go about it- either dont generate signal for first 49 days or generate a ""hold"" signal for first 49 days.",305653.0
92598,390888.0,"Since we donot have the historical MA for 50th row to decide I believe we should have no status to it. We cannot assume that the previous 20 MA would be higher or lower than 50 MA and on 50th day it crossed, would be wrong assumption. .",310629.0
92189,388295.0,"No, it is not. you can use whiichever way you are good with. Evaluation is not on that basis. Rather is is on the query for table creation/syntax and analysis.",304813.0
92189,388338.0,I would prefer laoding data using SQL command LOAD INFILE as the workbench import created some issues post import when I tried.,310974.0
92189,388354.0,"if all the records from CSV file is loaded into table,then any method is fine,but cross check the number of rows as it is important we don't loss any information while importing data.",301115.0
92189,388406.0,You will not be able to load all rows using import wizard tool. There are some records with nulls which are creating issues and count of records is not matching if we use import wizard.,317689.0
92189,389107.0,"Import wizard can helps only certain level and ignores records in scenarious like NULL rec loading into NUMERIC column. to avoid record loss, use LOAD DATA command triggered from SQL COmmand prompt. BUt the Table template should be created to load the data. So one suggestion, you can use the wizard to create a table columns and delete the loaded records ; and then use LOAD DATA.",318454.0
92189,389437.0,I tried in this way (wizard option) but out of 889 only 885 rows imported?? what is the best way to import csv into mysql??,318322.0
92189,389330.0,Better use LOAD INFILE command so that it would be good to analyse the issues while resolving.Anyway both ways can be used,318732.0
92578,390004.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92578,390093.0,Refer to below link for implementation of lag function. http://www.mysqltutorial.org/mysql-window-functions/mysql-lag-function/,317689.0
92578,390005.0,Lag will give you previous day price. So previous day diff and current day diff must cross. Use case statement and check the values. Previous day and current day must go opposite.,315679.0
92578,390257.0,"You can use the below logic using CASE function along with the lag function to generate the signals. Logic can be: if 20 Day MA &gt; 50 Day MA, then check if the previous row's (Using LAG) 20 Day MA &lt; 50 Day MA, if yes then it will be ""BUY"" else ""Hold"". You can apply the same logic for ""Sell"" signal.",316202.0
92578,390892.0,Use the CASE function and use the Lead and Lag function. You can do the comparision between the current 20 DAY MA and current day 50 DAY MA and previous day 20 DAY MA and previousday 50 DAY MA to generate signal. You can use the case the function and update the signal when you create new table for each stock.,310629.0
92578,391279.0,"Ayush, please do not post the code of your assignment, which is a graded component.",319721.0
92753,390611.0,The MA should be calculated with the past data.,303229.0
92753,390674.0,We have to check with the past data only.. or else once you reaches to the last row it will not have any below row so might through an error..,305129.0
92753,390704.0,MA is calculated based on the previous date as you can never have future data.,317689.0
92753,391064.0,"use Lag function and case..logic will be For every current row/date, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
92948,391413.0,"i guess we've to calculate it based on the number of rows than date.. Since the calculations involve only close price averages, it makes sense to have only the rows giving away the intended number of rows average..",316349.0
92948,391424.0,"If you do your calculation on the rows given in the dataset, then holidays and weekends will be taken care on their own.",310511.0
92948,391614.0,"Hi , Data on weekends / holidays is irrelevant. Please check the TA comments on below link: https://learn.upgrad.com/v/course/208/question/92790 Thanks.",305652.0
92951,391423.0,"Hi Ankur, Why dont you create the column with date datatype while creating the table. And then while loading data into this table, use str_to_date function on the input date column. Hope this helps.",310511.0
92951,391456.0,You can use ALTER TABLE command to change that the data type of a column in the table.,313691.0
92955,391426.0,Try this: select * from `bajaj auto`; ie put it within tildas when there is a space in the table name. Hope this helps.,310511.0
92955,391430.0,"it shows syntax error ,select keyword is underlined red",300684.0
92955,391431.0,Please use SELECT * FROM `BAJAJ AUTO`; The symbol next to numeric 1 ` &amp; not ',312259.0
92955,392013.0,Please try with the following code: SELECT * FROM `Assignment.BAJAJ AUTO`;,300715.0
93243,392454.0,You can have null values in the moving average columns. And can specify the signal as hold. That won't be a problem. In fact these rows don't really matter because analysis cannot be done on these.,319721.0
93243,392438.0,"First 49 rows, keep the signal as hold",319770.0
93243,392442.0,https://learn.upgrad.com/v/course/208/question/93229 please refer this,319770.0
92931,391325.0,"Hi please check the column count in your table and that in your csv file both has to be same also cross check your query, importing hero motocrop file into eicher table",319056.0
92931,391329.0,Please check you code you are importing hero csv while inserting it into eicher table.,306244.0
92931,392752.0,"There are data types in SQL which do not take null values. So, you need to convert the tables to the datatype which support null values. Select the datatype as varchar and try it.",319721.0
92755,391956.0,Thanks for sharing! It's great!,319721.0
92469,389479.0,"You need to specify column name explicitly. Below is the syntax for insert command: INSERT INTO table_name ( field1, field2,...fieldN ) VALUES ( value1, value2,...valueN ); You can also go through below link for detailed explanation of same kind of error: https://stackoverflow.com/questions/9798411/mysql-error-1136column-count-does-not-match-value-count-at-row-1 Hope this will help.",317991.0
92469,389536.0,"Hi , Alternatively you can insert the data while creating the table itself with Create table1 + select &lt;column-names&gt; from table2 . please refer link: https://dev.mysql.com/doc/refman/8.0/en/create-table-select.html",305652.0
92469,389567.0,"Insert directly using the select query as follows: insert into emp2 select fname,minit,lname,ssn from employee; Here, emp2 table has only these 4 columns in the same sequence: fname,minit,lname,ssn",313826.0
92469,390065.0,"CREATE TABLE BAJAJ1 SELECT Date`,`Close Price`,`20 Day MA`,`50 Day MA` FROM BAJAJ",317811.0
92822,390993.0,"Hi, For importing from csvs, the best option is to use CREATE statement in tandem with LOAD infile COMMAND. Create a table with columns based on those in csv. Then use LOAD INFILE command to load the data into your table. The benefit of this is with this command, you can some added controls like assigning defaults values, setting a column so that it accepts null values, etc. Hope this helps.",310511.0
92822,390957.0,"i knw two options 1. create a table and use the command like this : LOAD DATA INFILE ""/home/paul/clientdata.csv"" INTO TABLE CSVImport COLUMNS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' ESCAPED BY '""' LINES TERMINATED BY '\n' IGNORE 1 LINES; 2. create a table and follow the below steps ;",318017.0
92822,390996.0,You can import the CSV data into database different ways: 1. using LOAD DATA INFILE 2. using MySQL Workbench You can read more details here https://dev.mysql.com/doc/workbench/en/wb-admin-export-import-table.html http://www.mysqltutorial.org/import-csv-file-mysql-table/,317845.0
92822,390984.0,1. You can create the table and then write a script to import the data. In this case you have manually name the columns and assign proper datatypes to it. You have to use LOAD DATA INFILE statement. http://www.mysqltutorial.org/import-csv-file-mysql-table/ 2. Else you can use the table data import wizard. it will automatically use the column names and assign suitable data type https://www.youtube.com/watch?v=huRbA9n2ctw,313691.0
92825,390987.0,"Hi Rushi, I do not think you can rename the schema directly. But what you can do is that, you can use SQL to generate a script to transfer each table in your source database to the destination database. You must create the destination database before running the script generated from the command. SELECT CONCAT ( 'RENAME TABLE $1.' , table_name , ' TO $2.' , table_name , '; ' ) FROM information_schema . TABLES WHERE table_schema = '$1' ; ($1 and $2 are source and target respectively). Hope this helps.",310511.0
92825,390994.0,"Rename of the schema has been removed for security risk in MySql. however, you can achieve it by dumping and reimporting: . dumping : $ mysqldump -u username -p""password"" -R oldDbName &gt; oldDbName.sql create a new database $ mysqladmin -u username -p""password"" create newDbName Reimporting $ mysql -u username -p""password"" newDbName &lt; oldDbName.sql You can read more detail here https://chartio.com/resources/tutorials/how-to-rename-a-database-in-mysql/ http://moinne.com/blog/ronald/mysql/rename-a-mysql-database-schema-safely",317845.0
92889,391112.0,"Hi ,check the MySQL version You are using MySQL Doesn't have window functions until the most recent release: MySQL 8",319056.0
92889,391151.0,your exact SQL is working perfectly on my MySQL db - so you probably don't have MySQL v8 or above so please install the latest version of MySQL,300694.0
93595,394307.0,remove the begin and end from the udf function .,319770.0
93595,394312.0,delimiter $$ CREATE FUNCTION NameofFunction (inputvariableName DATATYPE) returns DATATYPE deterministic BEGIN return(your sql query retuning signal); END $$ delimiter ;,317811.0
93595,394399.0,"Add a delimiter after END, if you had declared $$ as your delimiter use END$$",310533.0
93595,394403.0,"While making the UDF, you might have defined a custom delimiter as in code by Ankit. delimiter $$ means from now on, the default delimiter is $$ instead of ; Thus, after END, you might need to add the custom delimiter you might have assigned previously. If no custom delimiter was assigned previously, use the default one i.e. ; (semicolon)",317987.0
93168,392196.0,"Hey Abhijeet, Stop Loss Order is a method followed where there are price caps. Like the one mentioned in the example in the link, if the price of ABC declines, after a certain point, it is sold. But here, there is no cap as such. We are comparing MA20 and MA50, and with the relative differences, we are deciding to sell/buy/hold. So, I don't think Stop Loss Order is ideal to use in this case.",319721.0
92898,391169.0,"add Lines Terminated by /r/n instead of just /n, it should go through fine in the load infile statement",319770.0
92900,391269.0,"HI Rohith, 1) to task managet clt+alt+del 2) click task manager 3) in processes end mysqld 4) then go to C:\ProgramData\MySQL\MySQL Server 8.0\Data\assignment check which file occuping more space 5) delete that file. it going to help you",306242.0
92900,391829.0,"Faced same issue. In my case SQL query timed out, since i ignored optimizations. I am assuming, it went into infinite loop which caused my c:drive to become almost full. When i checked, it was my assignment MY SQL which took more than 100GB. As Gopichand B suggested, i had to reinstall MY SQL. Hope it helps.",307495.0
93170,392211.0,Please refer this note by TA: https://learn.upgrad.com/v/course/208/question/93156,313826.0
93170,392212.0,You can use the import wizard.,319721.0
93170,392234.0,In my view any method which import data into mysql without any error and successfully add data into repective tables is fine. You can cross check the number of rows in excel and mysql table after importing data.,317991.0
92678,390456.0,"try to convert your date from string to date for each table bajaj1,eicher1,hero1 etc.... and then join all of them as master table.. may be your date in some table is in different format which is messing up the master table..",316349.0
92678,390481.0,Convert date from string to date format. Use str_to_date to set appropriate format than convert it into date.,317689.0
92678,390491.0,As date is the key field in all the tables please check if all tables contains same format of the date try running join on two tables first and then adding others one by one hope this helps,319056.0
92678,390498.0,"Hi Recreate all the STOCK tables with column `date` with DATE format. in SQL, DATE is internally stored as numeric value when you read/convert to DATE format from sources. SInce few STOCK tables are with VARCHAR/CHAR format to the DATE while do INNER JOIN the results are unable to populate and brings zero.",318454.0
92678,390512.0,"All the dates are in the same format for all tables. I have done inner join and that's how I was able to create the master table. I'm only having issue with displaying the table. I have created the tables using the MySQL Workbench wizard and not commands. So I can only alter the table. Also CSV files have been imported using the wizard. So re-doing the entire procedure again will be tedious. Please suggest another method if possible. For all the tables, while importing I have made the date column as varchar and the other columns as decimal. The following code is used to display the master table which is going wrong somewhere. Please help. select STR_TO_DATE(Market_Date, '%d-%M-%Y') as 'Date', cast('Bajaj' as Decimal(10,2)) as 'Bajaj', cast('TCS' as Decimal(10,2)) as 'TCS', cast('TVS' as Decimal(10,2)) as 'TVS', cast('Infosys' as Decimal(10,2)) as 'Infosys', cast('Eicher' as Decimal(10,2)) as 'Eicher', cast('Hero' as Decimal(10,2)) as 'Hero' from master_table;",310505.0
92678,390566.0,"use below date conversion in all 6 tables UPDATE `assignment`.`bajaj auto` SET `bajaj auto`.`Date` = str_to_date( `bajaj auto`.`Date`, '%d-%M-%Y' ) And then join all 6 tables based on the date by using Alias , you should get correct result. Let me know if you don't get the correct result after this .",311861.0
93171,392254.0,"where is T""able data import wizard "". is it under the title of server",319969.0
93171,392230.0,"You can import the files directly using the table data import wizard. First, you create a new schema. Go to the table section. Right click and click on the Table data import wizard. Follow a list of simple steps and import the data. It is highly preferable to only import data that we need. It will ask which columns to import, select only that you think are important for analysis.",304281.0
93171,392342.0,under the menu server -&gt; Data Import,319770.0
93174,392231.0,"By creating temp table you are doing additional computation to create table, insert into table and then transfer data to main table. I think you can take rid of this process by just creating main table.",317991.0
93174,392226.0,"If you use additional tables, it just makes the process more tedious and takes more computational effort too. Do you think that it's optimised?",319721.0
93175,393616.0,It's your choice to import data any way you want. No extra points are awarded for this step,302735.0
93175,392232.0,"Yes , wizard also can be used as per TA's comments: https://learn.upgrad.com/v/course/208/question/93156 But make sure that no rows are skipped while importing rows from wizard. Wizard silently ignores the rows based on the settings and also if any rows have null values . We need to take care of null valued column data types (example changing to varchar). Thanks.",305652.0
93175,392227.0,In my view any method which import data into mysql without any error and successfully add data into repective tables is fine. You can cross check the number of rows in excel and mysql table after importing data.,317991.0
93175,392238.0,"It is perfectly ok to import data nay which way you want. As you might have seen in the grading guidelines, there are no points reserved for the process of data importing.",304281.0
93175,394182.0,"Yes, we can use wizard but make sure that no rows gets skipped while importing, check below link for more details https://learn.upgrad.com/v/course/208/question/93156",307015.0
93176,392239.0,"Hi , Below are the links with TA's comments: https://learn.upgrad.com/v/course/208/question/90991 https://learn.upgrad.com/v/course/208/question/91875 https://learn.upgrad.com/v/course/208/question/91325 Thanks.",305652.0
93176,392235.0,https://learn.upgrad.com/v/course/208/question/93156 refer to the post above please. And you do not have to use a lot of jargon or use typical financial definitions and methods. BAsic summary of your results and what you performed and the outcome of your results is enough. It should be analysis oriented.,319721.0
93177,392241.0,The first value of the first row will not have an MA20 or an MA50 right? So we can't decide if it's buy or sell. Just let it be default hold.,319721.0
93178,392242.0,Then you need to format date for all the tables and make sure it's consistent for all the tables.,319721.0
93179,392245.0,"Her, it is an assignment's task which has weightage of marks. Please refer to the concepts of updating tables in the lecture module. I cannot answer this directly. It would be unethical.",319721.0
93179,392253.0,"Since it is an assignment I would not give exact solution but I can tell you the approach. Since you have already find MA, you can use that query in select clause. And then you can use inser into &lt;table_name&gt; select &lt;column_name&gt;, MA calculation query from &lt;table_name&gt; Hope this will help.",317991.0
93179,392311.0,"Thanks for the quick resonse, I could figure it out myself using update command.",311061.0
93179,392333.0,"Hello I am stuck in the same place In fact, I tried Vipul suggested but it gives me an error 1136. My SQL query works fine but not the update. Can you give some clue?",301121.0
93179,392555.0,"1. create a select query to finding MA with one column as Date as this is unique key update &lt;table&gt; as a inner join (select &lt;unique_key&gt;, avg(&lt;value_to_ma&gt;) over (rows n preceeding) as &lt;column name&gt; from &lt;table&gt; ) as b on a.&lt;Unique_key&gt; = b.&lt;unique_key&gt; set a.&lt;column name&gt; = b.&lt;column name&gt;; hope this helps. PM me if you need further help",311061.0
93179,546025.0,"Hi Sabin, we are in same group for case study... my contact number is 8368463915. Please connect with me.",317990.0
92958,391438.0,Can you please drop not null constaint and try again . let me know if this works .. The reason it is being dropped as some fields has null value .,311861.0
92958,391453.0,You will probably have to mention the field separators and line separators explicitly in your load infile statement. Hope this helps.,310511.0
92958,391455.0,Please try removing the NOT NULL constraint that you have used while creating the table. This will help populate the table with values.,313691.0
92958,391470.0,"can you try this length `Date` varchar(30), `Open Price` float(7,2), `High Price` float(7,2), `Low Price` float(7,2), `Close Price` float(7,2), `WAP` float(11,4), `No.of Shares` int(8), `No. of Trades` int(7), `Total Turnover (Rs.)` float(13,2), `Deliverable Quantity` char(10), `% Deli. Qty to Traded Qty` char(10), `Spread High-Low` float(10,2), `Spread Close-Open` decimal(9,2)",311861.0
92958,392141.0,"Hi Manjusha, This happens when the data which we get from the file is not of the same type as declared in the columns and sql ends it up with null in most of the cells. One of the possible reasons is that the first row of the csv contains column names which are all of text type. Make sure you tell SQL to ignore this header in the csv file with IGNORE 1 ROWS; while importing. Hope this helps!",318355.0
92969,391526.0,"You can edit the SQL Editor preferences in MySQL Workbench: In the application menu, select Edit &gt; Preferences &gt; SQL Editor . Look for the MySQL Session section and increase the DBMS connection read time out value . Save the settings, quite MySQL Workbench and reopen the connection.",311861.0
92969,391522.0,Which version of MYSQL version you are using .. Can you please restart your workbench ( please save your script ) Let me know if the issue still persist.,311861.0
92969,391572.0,"When joining the tables, if you do not provide any key on which you want to join, then a cartesion product takes palce and results in creation of a huge table which might throw such an error. Ensure that you are providing the key on which you want to join using the "" on "" clause.",313826.0
92969,391533.0,"This happens because the connection between your MySQL client and database server times out. Essentially, it took too long for the query to return data so the connection gets dropped. The MySQL documentation https://dev.mysql.com/doc/refman/5.7/en/error-lost-connection.html suggests increasing the net_read_timeout or connect_timeout values on the server. If this doesn’t solve the problem, you may need to increase your MySQL client’s timeout values. You can go through below link to change timeout. https://stackoverflow.com/questions/10563619/error-code-2013-lost-connection-to-mysql-server-during-query Hope this will help.",317991.0
92969,391637.0,"as mentioned above - ensure timeout setting are correct, ensure you are using the correct key on the join tables (here it is useful to use table aliases and to refer to columns on the join using aliases since all tables will have Date column), you shouldn't need ot but you can introduce an index on the join column to speed up the join",300694.0
92969,391722.0,"Once you restart your workbench/sql server, make sure you use ""table_name1 inner join table_name2 on condition"". Else, as Vinay has mentioned, there will be a timeout since there is no join mentioned.",312376.0
93187,392307.0,"Any method is correct, though it would be slightly easier (i think) to do it using the rank function.",304281.0
92974,391553.0,"A UDF can always return only a single value . Essentially, it means that the return sattement should have only one variable being sent back. In case you are using a select query to return the value, then run the same query directly and ensure that it returns a single value. You could make use of other constructs like group by or order by or limit etc. to return a single value.",313826.0
92959,391444.0,"Join the all tables based on date ( use Alias ) and select closed price field from all these tables, let me know if this help ?",311861.0
92959,391446.0,"To create the master table, you can use join function to create a new table with date as condition for join. refer to this link to get an idea: https://stackoverflow.com/questions/11359089/mysql-6-tables-join-together-in-one-query",313691.0
92959,391451.0,Join tables from result 1 on dates . Select close price from all the tables and give aliases to the close prices as the stock names. Hope this helps.,310511.0
92959,391868.0,Use CTAS (Create Table &lt;Table Name&gt; as (Select ....) and use join,308495.0
92959,392229.0,"Other way is create table and select date from one of the table. alter table with add column and update the table, inner join table1 with Date match.",312019.0
93182,392240.0,You can go through videos in the below link to know how to create UDF. https://learn.upgrad.com/v/course/208/session/19880/segment/101136 https://learn.upgrad.com/v/course/208/session/19880/segment/101137 Hope this will help.,317991.0
93182,392257.0,We have to use the table which contains the signal values for this. Take the input as a day and run the query to return the signal for that day.,304281.0
93183,392269.0,"Even if you do not convert, you should be able to perform all the operations required to be performed, using the numbering of rows.",304281.0
93183,392260.0,if date is declared as varchar then str_to_date can be used to convert to DATA type,301115.0
93184,392280.0,"If the input date is such that it does not lie in the table, you can return a null value but that is not compulsory as MySQL will take care of it by default. And all weekends and holidays would not be in the table.",304281.0
93184,394150.0,"There are no holidays or weekend mentioned as a part of the date range. Hence only the default return of the signal if the date is not present will be null, should be sufficient.",315028.0
93185,392256.0,"read the documentation on str_to_date parameters %y%m%d makes all the difference, you need to provide this parameters based on the source data",301115.0
93185,392270.0,"use this ""%d-%M-%Y"" instead of %Y,%M,%D' all caps",306243.0
93185,392300.0,The date format being used is %D-%M-%Y. You will have to use this to convert the string to date.,304281.0
93220,392372.0,Input has to be the date as mentioned in the Assignment. You can use the table in the return clause however.,304281.0
93221,392405.0,still not working even simple code Update bajaj1 set 20DayMA = select ClosePrice FROM bajaj1; why this giving 1064 error,320648.0
93221,392370.0,"remove AS and try ,it worked for me",301115.0
92980,391613.0,Do a select on the created tables after loading data and see if they follow the logic you applied manually. Thats all you can do. Theres no checkpoint values in this assignment against which you can validate your results.,310511.0
92980,391627.0,A simple way to validate your results is to replicate the analysis in excel spreadsheet. I did it for babaj stock and compared the signal values with values you've got in SQL. I found it matched for my output. Try yours and Good Luck!!,318328.0
92980,391655.0,try viewing the data using the select statement and export the results to CSV. randomly select a few records and cross verify with the original data because we don't have any checkpoints.,317845.0
92980,391981.0,"only excel cn save ur time ,otherwise raw run",318005.0
92396,389191.0,"It should be like: LOAD DATA INFILE 'PathofCSVFile' IGNORE INTO TABLE TableName FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '""' LINES TERMINATED BY '\r\n' -- or '\n' IGNORE 1 LINES and also at end take care of columns having missing values and date column format.",317811.0
92396,389135.0,"Can you try as LINES TERMINATED BY '\r\n' Source : https://dev.mysql.com/doc/refman/8.0/en/load-data.html LOAD DATA INFILE 'data.txt' INTO TABLE tbl_name FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\r\n' IGNORE 1 LINES;",311502.0
92396,389212.0,You need to handle some columns datatype for which this error is occuring. From the error I can see the datatype which you have defined for SpreadCloseOpen is smaller than the values provided in the sheet. Try changing it to higher datatype. Observe few values and decide on the datatype. Hope it is helpful.,317689.0
92396,389246.0,I managed to solve it by changing the datatype to char(50) for spread close open column.,318448.0
92983,392351.0,"Hello, Use simple inner joins with table using alias for every table.",311004.0
92983,391575.0,"When joining the tables, if you do not provide any key on which you want to join, then a cartesion product takes palce and results in creation of a huge table which might throw such an error. Ensure that you are providing the key on which you want to join using the "" on "" clause.",313826.0
92983,391583.0,"This happens because the connection between your MySQL client and database server times out. Essentially, it took too long for the query to return data so the connection gets dropped. The MySQL documentation https://dev.mysql.com/doc/refman/5.7/en/error-lost-connection.html suggests increasing the net_read_timeout or connect_timeout values on the server. If this doesn’t solve the problem, you may need to increase your MySQL client’s timeout values. You can go through below link to change timeout. https://stackoverflow.com/questions/10563619/error-code-2013-lost-connection-to-mysql-server-during-query Hope this will help.",317991.0
92983,391602.0,"Hi All, I am also facing same issue and stuck here since last three days. I have tried all the options which was discussed on discussion forum but no luck. We are able to run select query but when we are creating table master and getting timeout or loss error.",320195.0
92983,391605.0,"Usually it indicates network connectivity trouble and you should check the condition of your network if this error occurs frequently. If the error message includes “during query,” it can also happen when the client is attempting the initial connection to the server. For more details .Kindly go through the below link. https://dev.mysql.com/doc/refman/5.7/en/error-lost-connection.html",319006.0
92983,391624.0,"Oddly or weirdly enough, create table is working fine when created the bajaj1 and the other tables with windowing and all. But when trying to a table with table name as master, it goes into the error mode. To check if it was something in the query, I even tried creating a basic table with only 2 columns from bajaj1. I am still getting the lost connection.",319302.0
92983,391741.0,"Gave it another shot with the suggestions here and across the discussion forum. No avail. I am just this close to letting go of the SQL assignment. WIll give it one last try tonight, if it doesn't work then will drop the SQL assigment and try to recover with the other courses. Based on the scores - Assignments have 30% share in the total, so if i do not submit SQL assigment, i am guessing i will take 15% hit.",319302.0
92983,392271.0,If you are using Windows and MySQL workbench do the following Go to Edit -&gt; Preferences -&gt; SQL Editor set the connection timeouts as below. After that close the work bench and reopen it. It should work.,314244.0
92983,392634.0,"Well, after umpteen attempts finally was able to get through this step of the assignment. It did take uninstalling and installing everything twice for it to finally work. Thank you to everyone, who responded to my query and helped.",319302.0
92983,393422.0,"Yes Mysql has a inherent bug . If the sql is taking time to execute, you need to uninstall and install the same again.Its a long procedure but thats the only way out",310629.0
93339,392762.0,You need to use 'Close Price' value of each stock to create 20 Days MA and 50 Days MA columns. Window function concept will be needed to do this.,311686.0
93339,392775.0,Use window fucntion to calculate Moving average. https://www.essentialsql.com/sql-puzzle-calculate-moving-averages/ Hope this helps.,306735.0
93339,392819.0,The concept of MA can be applied by using Window function. The column that needs to be considered for each stock is the closing price.,319876.0
92893,391130.0,"Loading all the column values as varchar is not a bad approach. Infact it is used more often than not when source system are files. Think that in real world scenario you might get data from multiple sources and each source would prefer to have data in its desired format, and you would agree that we can not modify the code again and again and for each source, so having VRACHAR is a decent option. Loading data into tables is the first step, and almost always you wold need to extract required/meaningful data from these table hence all the correct data type conversions can happen during these steps. Now for this particular asigment, I would personaly advise to load files as is in varchar format, and data type can be converted in subsequent table i.e. bajaj1.",304814.0
92893,392376.0,"As a general practice, always import only the columns that you need. And specify the data type in which you want to import the data. This should help.",304281.0
93396,393083.0,"Yes, it is to be created only for Bajaj stocks.",318397.0
93190,392268.0,"No, that doesn't happen. I could add it. Let me know about the error pelase?",319721.0
93190,392805.0,You don't get the error when you use backquote ` for column. Example: `Signal`,306731.0
93180,392249.0,Please check the syntax you are using for the frame. And make sure that the parentheses have been used.,304281.0
93191,392261.0,Just update the rows of the column with null.,319721.0
93283,392612.0,Use row_number() function to find out which row you are dealing with. This can be compared with 50 and the result can be made null when not interested.,318007.0
93283,392664.0,you can make it Null or generate specifically Hold signal for these rows.. or either logic that comes to your mind since this anyways doesn't make sense.. your actual signal generation matters is from the 50th row where we've a logicalBot genuine values which can compared to get the signals of interest.. hope this helps..,316349.0
93225,392367.0,"You can ignore them, they don't really matter,.",319721.0
93225,393390.0,Keep them but those will be NULL.,315679.0
93192,392327.0,"You can use a create table command using window in the inner query to accomplish the task. As has been mentioned int he course, they can not be sued in the update command.",304281.0
93192,392264.0,MySQL documentation clearl states that: The SQL standard imposes a constraint on window functions that they cannot be used in UPDATE or DELETE statements to update rows. Using such functions in a subquery of these statements (to select rows) is permitted. You can go through below link for detailed explanation. https://dev.mysql.com/doc/refman/8.0/en/window-function-restrictions.html Hope this will help.,317991.0
93192,392275.0,You could use the CREATE TABLE AS SELECT(CTAS) statement.,313691.0
93228,392382.0,"You need to identify this by the change in the MA20 and MA50s. For more details please refer to the lectures on window functions. It's clearly stated there, also there is a description provided in the assignment's problem statement",319721.0
93404,393134.0,"Yeah, you can. Just mention that you're using this for the summary, as comments. And make sure that the steps you are asked to carry out in the assignment are used for the inferences you make in your summary along with the additional inferences you want to make.",319721.0
93284,392609.0,Because always one is greater than the other. We do not have equals fitting in there. We have to get Hold when trend continues as previous day. Where are you looking for trend of previous day in your logic? So you need tweak to read the previous day data and compare with current day data.,318007.0
93284,392630.0,"No, your logic is not correct. what you are doing is simply comparing todays 20day ma with todays 50 day ma. and since, they are never equal, you are no getting any Hold signal. Instead, what you need to do is, COMPARE the comparison of todays ma20,ma50 and yesterdays ma20,ma50. for eg, if yesterdays ma20 was lower than yesterdays ma50 AND todays ma20 is greater than todays ma50, then THAT w ould mean a BUY signal. hint: read about lag function to get values of previous day. hope that helps.",317998.0
93193,392272.0,"Yes. Yeah, I understand that it might seem to you that it doesn't matter, but there are few constraints while grading. Also, here you've been tested on a small concept.",319721.0
93227,392377.0,"Hey, you need to use backticks here. ` ` these.",319721.0
93195,392335.0,You can use a stored procedure or even a function to create the six tables. The points are however reserved for the results and the efficiency of the codes.,304281.0
93195,392408.0,Thanks for the confirmation. I have verified the script for one particular stock and modifying to the stored procedure.,311115.0
93229,392387.0,Put is as the default hold. Doesn't really matter. Anything which is convenient for you.,319721.0
93229,392390.0,"First 50 rows don't matter. You can either fill them with null or just delete them. This is clarified by TA, You can cross check this response on below link: https://learn.upgrad.com/v/course/208/question/92790",317991.0
93229,393432.0,We shouldnot put it as default hold as we donot know t what the previous trend in MA . I suggest you put it as null rather than hold,310629.0
93196,392279.0,"first 50 rows May not be useful for signal generation so,those can be ignored or dropped",301115.0
93196,392278.0,"No, a basic window function will do the job, please check your logic.",319721.0
93231,392402.0,https://stackoverflow.com/questions/14226418/return-true-if-two-columns-in-record-are-equal see if this something you are looking at,319770.0
93231,392425.0,You will have to use case when for that. You can learn more about case when at the link provided below: https://www.w3schools.com/sql/func_mysql_case.asp,304281.0
93231,392482.0,"I got null value in INDICATION column.. What's wrong with the below query? select date, case when B is null and A !=null then A end as `Indication` from dummy;",318013.0
93231,392445.0,"Yes, you can compare multiple columns and have multiple conditions. You need to use case as below Select Case when col1 > col2 Then whatever When col1=col2 then whatever When .... Else ..... End as colname From Table",304814.0
93238,392427.0,"Ideal candidate for primary key must be uniquely identifies each row. Since, it is graded question, I let you decide.",320074.0
93238,392436.0,It is not necessary for a column to be the primary key to be used for the join operation. You can use columns that are not keys for joining.,304281.0
93427,393252.0,rank the rows based on order and push null values for anything below x row number.,319770.0
93427,393301.0,you can use the row_numbee as well..and apply the logic to make nulls accordingly,307710.0
93427,393308.0,No need to put Nulls. Just ignore the values for first 49 rows using row number. Use subquery to get row number on the data set and then filter out first 49 rows in signal calculation.,312746.0
93427,393475.0,"Thanks all. Wouldnt adding row number to the respective table add an additional column for the table and change the required structure provided in the assignment module. Based on Python assignment submission, the structure of the output and variable name are also important for the submission. Just want to be sure on this.",319302.0
93427,393728.0,Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date.,317811.0
93619,394491.0,please change the data type length you might have set it to some decimal make it double,318017.0
93619,395496.0,You can use a round() function too and limit the decimals to 2 digits.,319721.0
93198,392290.0,"declare those feilds as float instead of int, then data can be uploaded with no issues",301115.0
93198,392293.0,Can you try using the Table Import Wizard and consider only the required columns,312259.0
93199,392287.0,"Can we also write about other metrics like %Delivered qty, turnover etc...?",318084.0
93199,392295.0,"You can include anything you observed while carrying out the analysis. Here, what is expected is that you need to summarise your findings from the analysis by stating respective facts obtained from the same. You don't need to use a lot of financial jargon and complicated financial definitions. A basic understanding of the analysis needs to be summarised. Imagine you are at an interview and the interviewer asks, what did you achieve from th assignment? Then the summary should help you answer his question.",319721.0
93233,392410.0,"This is a graded question which carries marks and you'll be evaluated on this. I cannot give you the direct answer. If you have any specific conceptual doubt, please ask.",319721.0
93233,392430.0,"There are 2 ways, 1) you can directly create bajaj1 table using the query you prepared to calculate MA. Something like create table bajaj1 as select query 2) if you want to update bajaj1, then use the query prepared to calculate MA as a psuedu table and join bajaj1 and this select query pseudo table on date (as this is unique). Check syntax on google (update using join)",304814.0
93200,392294.0,"As stated in assignment, merely &lt;&gt; MA cannot be criteria for bulk or sell, as for I know it should be hold",301115.0
93200,392299.0,"50th row, doesn't really matter. We need to compare it with the previous value and there is no value for the 49th row.",319721.0
93230,392396.0,You can create individually. And also some clarification is there by TA regarding creating new tables. 1. Creating new tables: Make sure you are creating 6 different tables as mentioned. And the column names should be exactly the same as given(if given) in the problem statement throughout the assignment. Below is the link for above information: https://learn.upgrad.com/v/course/208/question/93156,317991.0
93230,392392.0,You can do them individually.,319721.0
93350,392830.0,Why ? is it mentioned in the problem statement or grading criteria ? I don't think so.,312479.0
93350,392835.0,"I have seen a thread for rounding off, just wanna make sure we dont lose it. Could you please confirm TA's.",306735.0
93350,393197.0,"No, your marks won't be cut if you don't round off values obtained. But, there might be datatypes which cannot support more than a certain number of digits, take care of those.",319721.0
93136,392099.0,A select query can be written to return a single value. Please see an example below: select fname from employee where ssn='123456789'; which will retun only the 'fname' where the ssn equals '123456789'. Hope this helps.,313826.0
93136,392117.0,"1) Create a sperate function for each company. 2) Accept date as function parameter 3) Execute SELECT query (as mentioned by Vinay) to fetch the SIGNAL depending upon the input date 4) Retun the signal. Finally, test the function as below - Select SignalForBajaj('2015-01-12');",312479.0
93201,392306.0,Use Load function and ignore first line. Make sure your column data type of table can accomadate the respective value from excel.,312746.0
93201,392302.0,Instead of going with system proposed data type for null values columns try using float,301115.0
93201,392324.0,Try to import only the columns that you will need. Importing the complete table as is would be a waste of computational space and power. And import with data types that can be supported.,304281.0
93235,392421.0,Since the deliverable_quantity is not needed for any calculation its upto you whether you want to update the data type or not. I would recommend leave that as it is.,317991.0
93235,392417.0,"I don't think it matters, since you would anyways ignore this column, only date &amp; close_price is what matters, will leave it for TA to confirm.",319770.0
93235,392424.0,"Hi, if you are using data via table import wizard, that in that case datatype for deliverable as text and %age of deliverables also as text. but if you have imported 889 rows successfully than there is no use of changing the datatype for it, as we are not going to use it.",305129.0
93235,392423.0,"Actually, it's better to load only the 2 columns mentioned, the date and the close price in the table we perform analysis on. Additional columns re not required here and only occupy additional space so, that's not the optimal way. So, please update it.",319721.0
93202,392346.0,"Check that there are no two buy or sell signals not differentiated by an opposite signal. Also, check that the pattern is not too haphazard and that it can be explained.",304281.0
93202,392297.0,You can use MS EXCEL to validate the signals generated.,313691.0
93202,392334.0,A simple way.. go to whatsapp forum.. you get checkpoints over there.,312746.0
93306,392660.0,I'm assuming this case is for first 49 rows.. and that could be because you might have not made them as Null.. so ideally it has be anything other than Buy or Sell.. you can make it Null or generate specifically Hold for these rows.. or either logic that comes to your mind since this anyways doesn't make sense..,316349.0
93306,394256.0,"In a case where the previous day and current day MAs are equal, we can not generate a signal buy or sell as the 20 and 50 Day MAs have not cut one another. So we will go with the default hold.",304281.0
93242,392439.0,We Should use Table Nam as per Stock Name .,301648.0
93242,392446.0,"It's not clarified clearly yet, bajaj1 mentioned in the assignment description is not same as the name of the csv file TA can you please confirm "" The names of tables , not the column names, i understand the column names should be same.,",319770.0
93242,392450.0,"No, you cannot use underscores. Spaces are to be used. And the names should be exactly like specified.",319721.0
93104,391979.0,"All you will have to make sure is if your SQL is picking up days in sequence. When we calculate averages, we have to makes sure that close prices of consecutive days are averaged but not of random days. So it is all about sequencing/ordering your rows. If you think you can order your rows by dates properly and not like character comparison , you are good.",318007.0
93104,392078.0,date conversion will help you in making master table.,306244.0
93156,392316.0,"Could you please explain Signal approach.. I have used lag function on 20 Day MA and 50 Day MA with the cases for generating signal, Is it correct approach??",311004.0
93156,392404.0,"i am getting syntax error on Q3 codes, i need a feedback on the error i have done in the codes, cn i share the codes here",308495.0
93156,392406.0,Pl suggest when month's full name ie May and month's short name ie Mar written.. how to import it?,318791.0
93156,392407.0,Is it possible to update two columns with a single update function? Each column have a different condition to update.,312376.0
93156,392184.0,I am getting error 1265 while populating 20DAY MA,319319.0
93156,392233.0,Are the moving average field required to be empty for the top 20 and 50 rows? Also do we need to round the moving averages to any decimal point?,318576.0
93156,392237.0,"Hi, 1. How to resolve a 1364 error? 2. If we get a 1064 error, but can't find anything wrong with the syntax used how can we resolve the problem? 3. Does a join have to happen on primary and foreign keys or can it happen on any key? 4. Can value assignment using SET happen with JOIN? Will SELECT and JOIN only display the result or also assign value in the table?",312608.0
93156,392244.0,could you let us know what should be the final out put ? as per my understanding we need to show the final out put as BUY/SELL/HOLD for particular date which will be an input of UDF when we called the UDF,304693.0
93156,392251.0,1. Can we keep singal as NULL for the 1st 50 rows since we are having values from 50th row only? 2. What the UDF shall return if the date passed is a holiday/weekend which is not present in the original dataset(csv file) ?,318756.0
93156,392263.0,"for question 3, need some sample table for more clarification.",318322.0
93156,392281.0,"3. Does a join have to happen on primary and foreign keys or can it happen on any key? -&gt; in case of RDBMS systems join can happen on any column, if primary key is specified, you will have a optimal performance on the join. 1 &amp; 2, if you can describe more ,",319770.0
93156,392285.0,"For generating signal it is mentioned in the Assignment that ""Merely being above or below is not sufficient to generate a signal."" so what are the criterias we can consider for a signal it is someting to do with the difference in the amounts if yes then how big a difference should ideaally be considered",304695.0
93156,392304.0,"Signal for First 50 Rows should be Hold or Null, please confirm?",300734.0
93156,392309.0,there is no live seesion today ??,319969.0
93156,392320.0,do we have to sort the closeprice by dates to calculate MA,308495.0
93156,392312.0,"while creating bajaj1 table am getting error 1265 data truncated for column MA50 at row 3 ,, likewise 35 warnings ? please guide",308495.0
93156,392332.0,"Susmita, Why are you suggesting us to create 6 tables one for each stock where all the information for 6 stocks can be put into a single table. This way we have to repeat all the tasks 6 times, which I feel is rather odd and inefficient in terms of writing SQLs. Ideally the DB design suggests we put similar elements in the same table. In our case it could be one table identified by an additional column named Stock identifier. Currently we end up creating 6+1+6 table by the time we complete first three parts. Which in real world would be 3 tables. Please advise. -- Rajesh R",300708.0
93156,392358.0,Will SELECT and JOIN only display the result or also assign value in the table?,312608.0
93156,392352.0,I hope we don't need to create any keys as we can use date as a key to join ?,304693.0
93156,392357.0,2. 1064 error This is my code UPDATE master SET a.TVS = b.`Close Price` FROM master as a INNER JOIN tvsmotors as b ON a.mDate = b.Date; I have checked it every which way and cant find the syntactical error. The only highlight I have is the both mDate and Date are primary keys. Unsure if that makes a difference. Is there a different syntax for using set with join as compared to select with join? The exact error I am getting is: Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'FROM master as a INNER JOIN tvsmotors as b ON a.mDate = b.Date' at line 3,312608.0
93156,392361.0,"while calculating 20day moving average, it generating 20 th row onwards (20day moving average) so what about before 19 rows, do i need to make it as null / cummulative average of previous days..from 20th row onwards it maintain 20days average??",318322.0
93156,392368.0,Please explain the UDF cocept.,308964.0
93156,392391.0,"For creating the master table, Join wouldnt be the correct approach as there is a chance of any data getting dropped if it didnt find a matching date key in one of the tables. I would prefer using UNION instead of Join. That way I am sure to not miss any of the dates. --Rajesh R",300708.0
93156,392399.0,"For creating the master table, Join wouldnt be the correct approach as there is a chance of any data getting dropped if it didnt find a matching date key in one of the tables. I would prefer using UNION instead of Join. That way I am sure to not miss any of the dates. select pkColumn, sum(col1), Sum(col2) from (select pkColumn, col1 as Col1, 0 as col2 from table1 Union select pkColumn, 0, col2 from table2) A group by pkColumn; --Rajesh R",300708.0
93156,392412.0,"Hey, everyone, please don't post your questions as comments, ask them separately in the discussion forum, even for the live session. I just posted this so that it'll help you guys clarify few things,",319721.0
93156,392416.0,when we were able to show the signal to BUY/SELL/HOLD. I hope it doesn't matter how many rows we imported?,304693.0
93156,392484.0,please provide some check points in Q3 to verify the output,308495.0
93156,392486.0,"i have not deifned any primary key in the individual tock tables, still in master table when i join over date it has picked up teh close price of relevant stocks in that date ?",308495.0
93156,392488.0,"i am not from stockmarket domain and have no experience of trading, have been able to create tables, but donot know how to analyse the table, can you pelase guide on what basis to analyse the data",308495.0
93156,392497.0,Can lag function be used with create table?,312608.0
93156,392551.0,"Can any one explain what is the meaning of this when generating a signal ""Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal.""",317600.0
93156,394825.0,Testing,302827.0
93156,394822.0,Testing,332701.0
93156,394823.0,Testing,302827.0
93245,392457.0,"No, just leave it as hold.",319721.0
93245,392456.0,It has to be Hold,304814.0
93246,392464.0,Yes,304814.0
93246,392463.0,"No no. You need to compare it only with the exact previous day. You shouldn't do it for the days prior to that. If there is no change, then it's clear that the stock should be kept hold.",319721.0
93247,392462.0,that depends on the code you are writing it its optimised then no one can cut your marks. and you cant write ma20 and ma50 code merged they are different only hope this helps,318017.0
93247,392460.0,You will not lose any marks for not generating 20 and 50 Day MA for the days when it can not be calculated.,304281.0
93247,392461.0,I will let TA answer that but I am sure most of us have similar assumptions,304814.0
93249,392466.0,this error is caused when you have created a column with wrong data type : check the below link if it help . https://stackoverflow.com/questions/19351633/error-code-1052-column-admin-id-in-field-list-is-ambiguous,318017.0
93249,392467.0,"That means there are more than one columns which are names 'Date'. So, SQL doesn't understand which one of these columns you're referring to. Make sure you have only one Date column in your table.",319721.0
93249,392468.0,When you join two tables a/b having date as common column and you write select date in the main select clause.,304814.0
93249,392512.0,when u join multiple tables and 2 or more tables having same columns and retrieve the column without alias will give this error,307710.0
93357,392875.0,"Its clearly mentioned ""Create a new schema named 'Assignment' Import the CSV files in MySQL , naming the tables as the name of the stocks. "" After this, create table bajaj1 with 4 cols. Hope this helps !",306735.0
93357,392848.0,"Frist import all six datasets into respective tables (Bajaj, TCS, Infosys...etc). In the second taks, we need to prepare a query to SELECT the records based on ""20 day MA"" and ""50 day MA"" and CREATE another tables Bajaj1, TCS1, Infosys1... etc. This is applicable for all the six stocks. The new tables will have only 4 columns (Date, Close Proce, 20 Day MA, 50 Day MA).",312479.0
93239,392429.0,https://stackoverflow.com/questions/1992314/what-is-the-difference-between-single-and-double-quotes-in-sql,319770.0
93204,392301.0,No. You can do it directly. Won't be an issue.,319721.0
93367,392918.0,What's your query?,318429.0
93367,393010.0,Yes we can produce signal for all other symbol it will help you to write your summary task better.,318476.0
93365,392914.0,You may need to IF-ELSEIF-ELSE structure rather than IF-ELSE structure. Pls refer below link for more info http://www.mysqltutorial.org/mysql-if-statement/,306735.0
93365,393075.0,It’s working now it’s was a simple syntax error I had comma after “buy “,300687.0
93348,392792.0,"Issue is with your update query, here's the syntax. UPDATE TABLEAME SET COLNAME='COLVAL'; I'm wondering why you need to use WITH clause for updating, can u please explain what are you trying to do ?",306735.0
93096,391937.0,"On a holiday,you do not do any transactions. Means that you get a HOLD signal for a Holiday or weekend. This is the response from one of the TA for similar kind of question. Below is the link: https://learn.upgrad.com/v/course/208/question/92112",317991.0
93096,391943.0,we can hold the stocks for those days,319056.0
94511,398252.0,"hi, you will need 2 components working together: 1. LAG() window function 2. Conditional clauses (if-else or CASE) Hope this helps.",311686.0
94511,398396.0,You can have 2 approaches: 1. LAG and window function as cited by Chandan 2. JOIN and row_number() Both should work. Its all upto you to choose what you prefer.,318762.0
93372,392946.0,https://learn.upgrad.com/v/course/208/question/92186 https://learn.upgrad.com/v/course/208/question/92771 https://learn.upgrad.com/v/course/208/question/93148 https://learn.upgrad.com/v/course/208/question/92186 Same/Similar questions asked previously....,301652.0
93317,392751.0,"I assume you are using import wizard. Import Wizard ignores null values if any exists You need to change your datatype for ""Deliverable Quantity"" and ""% Deli. Qty to Traded Qty"" as text , so that while importing it will take all 889 rows.",307495.0
93322,392700.0,There is no mention about the table to be used in the UDF. The only requirement is that the UDF should provide the signal fir thw input date dor Bajaj Stock. So whichever method is easier could be used to write the UDF.,313826.0
93322,392701.0,You would need to use the Step 3 table result to get ans for step 4. Simple use bajaj2 to get the signal value.,306735.0
93322,392824.0,I think we can use any of the tables bajaj1 or bajaj2 whichever is easier to use since there is no mention of what needs to be used.,319876.0
93250,392470.0,"Hey, please understand that you cannot display your code here. It's a graded component. I have to delete your question.",319721.0
93250,392473.0,Dont use single quote in 20_Day_MA column. Use back tick.,304814.0
93250,392475.0,for date use `` istead of '',318017.0
93250,392476.0,"you are adding , before from, remove it, and it should work fine",319770.0
93379,393087.0,You need to use a return statement inside which you can define whatever you want the output to be.,318397.0
93379,392975.0,Dont assign select query to a variable. Use into in select query to assign output to a variable,304814.0
93379,393004.0,select (column_name ) into (varibale ) is the correct syntax for sql,318476.0
94657,399313.0,What column are you getting the error for? Bigint may help if it an integer column,319357.0
94657,399357.0,"1. Use 'Create Table Data Import' Wizard of mySql workbench. 2. Browse &amp; select csv file, click next 3. use create new table radio button &amp; 'drop table if exist' checkbox. click next 4. Do not change anything in 'configuration import setting' section. it should show encoding, column names &amp; data types. Click next. 5. Click next again to create table and import data,",318458.0
94657,399529.0,"This is related to the data type. Can you share a screenshot of the error message. Try using the import wizard and change the data type for ""Deliverable Quantity"" and ""% Deli. Qty to Traded Qty"" columns to text. This worked for me.",316202.0
94659,399395.0,"once table is created, you can also perform - Select `Date`, `Close Price`, ............... into bajaj2 From ......",318458.0
94659,399345.0,"Create table doesnt work with Select statement. Are you trying to insert rows into bajaj2 table? Then use - insert into bajaj2 select `Date`,`Close Price`, Case WHEN `20 Day MA` &gt; `50 Day.........",318458.0
94659,399307.0,Have you mentioned the value to be returned when the condition is true? Mention that return value if you haven't after 'THEN' and the case statement needs to end with 'END'.,307487.0
94659,399310.0,Thanks will check it out,315831.0
94659,399480.0,Please share snapshot of complete query,317811.0
94659,399810.0,"""AS"" KEY WORD IS MISSING. THIS STYLE OF QUERY IS CALLED CTAS , CREATE TABLE AS . Proper syntax CREATE TABLE &lt;TABLE_NAME&gt; AS SELECT column_1, column_2 FROM &lt;TABLE_NAME_1&gt;",315679.0
95309,403170.0,me too,318017.0
95309,403259.0,Have you gone through the solution video? Date is taken as text and has not been convert to date anywhere.,318329.0
95309,403277.0,The solution is relying on the source order of excel files. Instead i my code I am sorting descending in the window function. The solution video imported the data as text. I also thought the same that file has data sorted. Why do extra job of converting into the import wizard and then sort in windows function. I did it because it made more sense rather than on relying on file.,301643.0
93382,392974.0,"Yes, you can ignore rows using window function",304814.0
93382,393000.0,Yes we can remove this now as it's not required for the second part of the assignment which is signal generation,318476.0
93361,392880.0,This is what I did:,310974.0
93361,392917.0,"Use exactly the same name, while importing file in to mysql else you may lose marks.",306735.0
93361,393013.0,Keep the table name same as mention the in the assignment. The Marks rubic define it clearly.,318476.0
93207,392323.0,The UDF should only work for bajaj stock. so you can use the SELECT statement with only date as the input.,313691.0
93207,392349.0,Only one UDF needs to be created and that is for the bajaj stock. No need to create UDF for other stocks.,313691.0
93207,392343.0,But that way I have to write different UDF for every stock. Doesn't look very optimized. DRY principle you know...,313515.0
93207,392355.0,"It is clearly mentioned that 4. Create a User defined function, that takes the date as input and returns the signal for that particular day (Buy/Sell/Hold) for the Bajaj stock You can cross verify with TA response from below link https://learn.upgrad.com/v/course/208/question/93156 Hope this will help,",317991.0
93207,392356.0,Please note that the UDF has to be created for only a single stock and that you can use the table in which you have generated the signal to return the values.,304281.0
92053,386858.0,For null values change the datatype of the columns having null to string and it will be able to import.,317689.0
92053,386672.0,Below are few methods two load the same: * Use LOAD DATA LOCAL INFILE * Ignore the column containing NULL values while importing through wizard I would suggest to go with option 1 as it will import all rows with all columns.,308439.0
92053,386941.0,"While loading csv file, use below command as well (cola, colb, colc, cold ..........upto last col of table) SET`columnname` = IF(@col = ' ', NULL, @col); It will ensure null values to handled correctly.",317811.0
93384,392985.0,"this has been answered many times to me, it doesn't make sense to generate signal on row 50 as we do not know the trend for the 49th row; so indeed 51st row is where it makes the most sense to start generating signals",300694.0
93384,392996.0,Yes we can start generating the signal from the 51 rows. Or else the any other signal which are not satisfied the condition can be marked as HOLD,318476.0
93384,393029.0,Start generating the signal at 51st row as the crossover function should have 50 rows to aggregate over.,317689.0
92556,389900.0,"Buy or Sell signal should be generated only at the time the cross over happens to minimize the loss. If you keep buying till the stock price falls down, how would you minimize loss? And if you keep selling till the stock price comes up, how would you minimize loss? Stop and think about the stock prices in numbers and you will understand why this is the guidance. Apart from that, that's what the assignment wants us to do :)",310974.0
92556,389907.0,In practical scenario these averages are not that simple. People do take exponential moving averages and other variables in account while doing the actual transaction for buying or selling. And if the price is falling and your signal remains BUY than it would ultimately be loss situation for you.,317689.0
92556,391384.0,Ankit - thanks for sharing.. makes sense to me.,308633.0
92556,390043.0,"Understand Concept of Signal by an example of line graph plotted below:- 1. Plot a graph having x-axis/categorical variable as Date sorted in ascending order. 2. Y axis will be for both 20DAYMA and 50DAYMA. 3. Join all the points for 20DAYMA to draw a line. 4. Similarly Join all the points for 50DAYMA to draw a line. 5. Now, you will see these two lines cross each other at some points. 6. Let say if at first cross point, 20DAYMA goes above 50DAYMA line, the date at this point is signal to buy. Then at next cross point 20DAYMA will surely go below 50DAYMA line and is a signal to sell and it will continue for all cross points so on in the order buy point, sell point, buy point, sell point and so on alternatively. Note: Only Cross or cut points of both lines are either buy or sell points (dates). Rest are hold where either 20DAYMA line is below or above 50DAYMA line use Lag function and case..logic will be For every current row, Case when MA20 is above than MA50 and in row previous than current row MA20 is not above than MA50, then signal to buy when MA20 is less than MA50 and in row previous than current row MA20 is not less than MA50, then signal to sell else signal to hold",317811.0
93412,393152.0,"here, the statements do not say that you'll have to remove these rows. A moving average for 50 values cannot be calculated for less than 50 values. And similarly for 20 values. So, it means that, in the MA50 column, the first 49 can be left null and in the MA20 column, the first 19 values can be left null.",319721.0
93412,393150.0,You can ignore those rows while performing analysis or generating Signals this means. exclude them in signals of buy sell or hold,306244.0
93412,393151.0,"the instruction means that while creating Bajaj1 table, you can ignore the values getting populated in 20 Day MA and 50 Day MA columns in initial rows as per the logic written by you. ideally they should be null but it's okay if we do not handle it to be null in our query.",311686.0
93409,393200.0,"We can also use STR_TO_DATE(&lt;string_value_for_date&gt;,&lt;formate it is in&gt;) Example: STR_TO_DATE(trade_date, '%d-%M-%Y');",318476.0
93409,393148.0,There is a drop down which you can use to convert the field to datetime type. This comes as a configuration setting.,319721.0
93398,393144.0,"as per the discussions till now, we should use null for initial 50 rows instead of dropping them.",311686.0
93414,393164.0,"If you're importing it into an empty table created and using this table as the bajaj1 table etc, you need to mention the queries used. If you're explicitly creating bajaj1 again, you don't have to.",319721.0
93186,392250.0,What should be the value of the 50th row? buy/sell/hold,318084.0
93186,392255.0,"Here, we cannot conclude anything because we don't have averages for these. So, let it be the default hold. Ideally, you should ignore these rows.",319721.0
93186,392258.0,You can fill these rows will null because you don't have data for comparison and cannot generate a signal. I have filled the first fifty rows of the signal with NULL.,313691.0
93186,394012.0,You can put HOLD as signal in such cases,315423.0
93402,393143.0,"Check your code with an output, manually. You'll get to know if it's right. If you know that for a particular date, the stock is buy/sell/hold, you put in that date and check the results displayed accordingly.",319721.0
93402,394045.0,"Hi, I have used the below approach, worked for me.Please replace with your input param,datatype and query DELIMITER $$ CREATE FUNCTION test(input param) returns datetype deterministic BEGIN return(query); END $$ DELIMITER ; select test(give any date that is present in bajaj2)",302750.0
93416,393168.0,01-JAN-2015. As you want to have results from the past dates.,319721.0
93416,393188.0,We can not calculate the MA by lookingto the future date in this case 31-JUL-2018.This will cause look ahead bias. https://www.investopedia.com/terms/l/lookaheadbias.asp,318476.0
93416,393354.0,"The assignment is on stock price forecasting, which is done based on legacy data to predict the future date price trend.",301644.0
93439,393332.0,you have to include it. Put 19 and 49 in condition.,318802.0
93439,393334.0,n-day moving average is calculated including the nth day data. Current row + preceding 19 rows in case of 20day MA.,301644.0
93439,393389.0,You can start generating signals starting from the 51st row and avoid previous rows.,317689.0
93439,393483.0,"Hello Shubham, These rows don't really matter. So you can populate them with NULL or remove them.",320195.0
93439,393561.0,this has been asked sooo many times on the discussion forum. please do a search of the forum postings,300694.0
93439,393724.0,"You can start generating signal from the first row, as even if 20 day ma or 50 day ma is null and does not cross each other, u have to HOLD.",317811.0
93419,393198.0,The logic is correct.Just check the window function date order and the MA calculation once more.,318476.0
93419,393208.0,"The logic is correct, just check the sorting while taking lag",317811.0
93419,393309.0,it's correct... check the lag 20 n lag 50 are calculated on 20 day ma n 50 day ma n not on close price,307710.0
93419,393340.0,For the first 20day MA 20th row data is needed in the calculation. Check definition of moving average in the assignment introduction page. Same for 50th row in 50day MA.,301644.0
93419,393331.0,For the Calculation of signals ? Do I need to avoid non 20th and 50th row ?,315423.0
93436,393336.0,yes.. use below path- 'C:/ProgramData/MySQL/MySQL Server 8.0/uploads/Eicher Motors.csv' and run below query also. SET SQL_SAFE_UPDATES = 0;,318802.0
93436,393361.0,"No, there is no guideline to use standard Path. You can infact also upload the file using Workbench. Dont worry about how TA's would load data into table. You would be good",304814.0
93436,393386.0,"TA's please note , good to mention that specific path so that SQL scripts from studatent can become generic.",315679.0
93436,393562.0,"there is no generic path stipulated - please load as per convenient to you, including the data import wizard which allows you to pick a file using file manager",300694.0
93448,393383.0,"It must be a commented SQL file rather than commented ZIP file. No, you should comment the whole SQL file.",313691.0
93448,393388.0,"I don't we should comment the sql file as it would be difficult for the TAs to differentiate between actual code and comments. So, it should rather be sql file with comments.",317689.0
93448,393438.0,I agree with arihata...it should be a SQL file with comments,307710.0
93448,393559.0,you can add comments to the SQL file using /* */ or -- this will help the assessor yes the mistake was typo - you can't comment a zip file,300694.0
93448,393722.0,It means that write comments as well for every query that we write in sql file so that it can be easilu understood at time of assessment.,317811.0
93544,393933.0,You can do that but that would be tedious. It's better to change the format during the loading.,310974.0
93490,393629.0,"Hello lovish, Average is something fixed for all date windows but moving average is average which changes according to window time period. So basically no need to consider first 19 rows for 20DAYMA and 49 rows for 50DAYMA. These rows don't really matter. So you can populate them with NULL or remove them.",320195.0
93490,393711.0,It says that 20 DAY MA should be calculated starting 20th row when table sorted by data ascending and 20 DAY MA should ebe calculated starting 50th row when table sorted by data ascending,317811.0
93481,393571.0,"Hello Murali, You can use update table where SET Column name = NULL where Rownumber&lt;20.",320195.0
93546,393903.0,You need to create columns with some calculation for moving average.,317689.0
93546,393987.0,You need to create the moving averages column using windows function in SQL,303673.0
93546,394106.0,"hi Naren, you've to use the window function to get those averages calculated for you.. add this results as a new column in bajaj1.. take help of Windows function session and that should help you out here..",316349.0
93546,394169.0,You need to calculate moving averages using window function . you can try same in excel by data analysis function before doing in SQL for more clarity.,319869.0
93542,393893.0,Because you having multiple fields called date. use alias and try again.,306244.0
93542,393905.0,use alias name for the tables if you are joining on same named column in each table. eg: employee.id = project.id,317689.0
93542,394064.0,"SELECT bajaj_1.date as date, # Select Date only once from any of the table like in this query. bajaj_1.Close_Price as bajaj, eicher_1.Close_Price as eicher, hero_1.Close_Price as hero, tcs_1.Close_Price as tcs, tvs_1.Close_Price as tvs FROM bajaj_1 (any join) eicher_1 on bajaj_1.date = eicher_1.date inner join hero_1 on eicher_1.date and so on",317811.0
93573,394142.0,"You must be getting averages of remaining top rows from where the condition applies, you should ignore these rows in later stages when doing analysis or generating signals.",306244.0
93573,394144.0,it does not matter ..but moving average must be available where it is required u can ignore first 19 or 49 rows,319869.0
93573,394154.0,"You can write a condition using the row_number, and track the row number and generate the average for only those rows else keep it null. i.e for the 1st 19 row value for 20 Day MA should be null.",315028.0
93573,394275.0,"Below approach will solve your problem and you will get null for first 19 rows in case of 20dayMA. for 20DayMA, Use CASE for Calculate moving average where row number is greater than 19 by using row number window function on the entire table sorted by date. for 50DayMA, Use CASE for Calculate moving average where row number is greater than 49 by using row number window function on the entire table sorted by date.",317811.0
93550,393983.0,"Hello Naren, Please click refresh option available which will automatically display the tables.",303673.0
93550,393970.0,"Hello Naren, Please right click and refresh table.",320195.0
93550,393992.0,"I have clicked on refresh option both at Schema level as well as table level , but i am not able to see them. if I try creating same table, i am getting an error saying the table is already created. so, this shows that tables are there, but I am not able to find them. ANy other way to get these tables displayed.",311046.0
93550,394020.0,I have restarted workbench and I can see tables now. Thank you,311046.0
93550,394153.0,I faced the Same issue. I restarted the MySQL again and it worked fine for me.,318557.0
93550,394163.0,Might be you have not refreshed,319869.0
93550,394189.0,Please try and refresh the schema or use a show tables command and see the complete list of tables.,304281.0
93463,393467.0,"Hello Nithya, The date format being used is %D-%M-%Y. You will have to use this to convert the string to date. update table table_name set date=str_to_date(date,%d-%M-%Y);",320195.0
93463,393460.0,you've to maintain the order and also the characters case (either upper or lower) use something like ; '%d-%M-%Y',316349.0
93463,393509.0,"format the date column to dd-mm-yyyy in the excel file itself and then import it. Post import you can use DATE_FORMAT(STR_TO_DATE(date,""%d-%m-%Y"" ),""%Y/%m/%d"" ) to pdate the respectice column and then alter it to date datatype.",318340.0
93483,393590.0,"What is the method you used to import tables? I think you used the import data wizard. When you do that, check the boxes on the columns which are asked to import.",319721.0
93483,393609.0,It worked.Is it required to change the data type of date field from text to date time later?,318493.0
93483,393597.0,Date should be text data type pls check,300687.0
92770,390668.0,For 1st and 3rd Question we need to do for all stocks as they have clearly mentioned for all stocks.. But Q4 we need to do only for Bajaj2..,305129.0
92770,390675.0,As per instruction (This has to be done for all 6 stocks) for Question 1 &amp; 3.,306244.0
92770,390723.0,"Question 1and 3 clearly states that you have to perform operations for all the 6 stocks 1. Create a new table named 'bajaj1' containing the date, close price, 20 Day MA and 50 Day MA. (This has to be done for all 6 stocks) 3. Use the table created in Part(1) to generate buy and sell signal. Store this in another table named 'bajaj2'. Perform this operation for all stocks. So you need to submit SQL for all the 6 tables created in 1 and 3. Question 2 clearly states that you have to make one table 2. Create a master table containing the date and close price of all the six stocks. (Column header for the price is the name of the stock) So you only have to submit SQL for just one table. Hope this will help.",317991.0
93581,394183.0,yes we need to calculate for six tables. And master table also.,313200.0
93581,394214.0,you must have written all your codes in sql workbench file and also the pdf which summarizes the results obtained.. juat zip these two and submit it..,316349.0
93581,394271.0,what should be the extension of the file name to be saved eg: filename.extensionname,312892.0
93581,396130.0,filename.sql,308638.0
93589,394501.0,"Yes. For some of the stocks the signals seem to be in accordance with the overall trend of the closing price movement, but for some stocks it seems to be counter productive.",313826.0
93589,394261.0,"Yes, there are only some consecutive buy and sell points in most of the stocks whereas for one of the stocks all consecutive buy and sell points are making profits",317811.0
93589,394296.0,Why do you think that it doesn't seems to be right? I verified them to be correct using excel.,318329.0
93560,394032.0,the point you mentioned is in fact a good observation. I didn't notice that :) there are certain other generic observations as well that you can talk about.. you can compare different stocks for some high level analysis or pick up one stock and go deeper into it.. I am sure you will come up with some interesting insights..,311686.0
93560,394113.0,Google and go to some articles https://www.mssqltips.com/sqlservertip/4467/using-sql-server-data-analysis-for-stock-trading-strategies/ https://www.mssqltips.com/sqlservertip/5405/an-overview-of-data-science-for-stock-price-analysis-with-sql-part-1/ which will tell you what you can look into the results obtained.,317811.0
93560,394161.0,Yes .you can check How to analyse stock buying decision using moving average you will get information,319869.0
93560,394180.0,The expectation is a narrative that you can spin with the help of the data. We are aware that you may not have expertise in the subject but you can draw interesting observations like the one you mentioned. There are no set guidelines other than that the narrative should be backed by data.,304281.0
93588,394212.0,"some column likes Date will be the matters. set date = str_to_date(Date, '%e-%M-%Y');",313200.0
93588,394233.0,"No issue at all, TA has confirmed that you can use import wizard to import the data into your tables. Just make sure that you don't lose any rows as well as you have your data type for date field as date.",315028.0
93588,394320.0,what is filename for submission eg filename.extension,312892.0
93588,394298.0,"Hello Atulyan, Not at all. It doesn't matter., TA confirmation. https://learn.upgrad.com/v/course/208/question/91130",320195.0
93588,394224.0,i guess not.. I've completed the assignment with the same import wizard and haven't faced any issues in generating the signals or completing the assignment.. the only worries is to convert the date to ease the sorting.. for that you can use str_to_date funtion..,316349.0
93600,394353.0,"Spelling of ""preceeding""....it should be PRECEDING AVG(Close_Price) over (Order by Date asc rows between 19 preceding and 0 following)",312758.0
93600,394378.0,Try averaging on the preceding (n-1) rows. Also try with ascending order,310533.0
93600,394358.0,"Hello Naren, It should be PRECEDING. SELECT AVG(ClosingPrice) OVER (ORDER BY Date ASC ROWS between x PRECEDING and current row)",320195.0
93586,394206.0,"1.Use data type float (10,2) for `Close Price` column. 2.There are null values at rows and also data is too long. LOAD DATA infile 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Assignment/xxxxxx.csv' INTO TABLE `table name` FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '' LINES TERMINATED BY '\r\n' IGNORE 1 LINES;",313200.0
93586,394267.0,"SET SQL_SAFE_UPDATES = 0 ; Execute above command. Then, chage the column datatype of table to float(10,2)",317811.0
93591,394287.0,In your single script you need to provide end to end code which support all the operation required to perform the assignment. If you have created the table using the wizard then you should provide the create table structure.,307843.0
93591,394293.0,"Here is the answer, It doesn't matter., TA confirmation. https://learn.upgrad.com/v/course/208/question/91130",319770.0
93591,394292.0,"Hello Maya, Do not worry about Import data. Please upload your ZIP of SQL files which contain all four questions &amp; PDF file.",320195.0
95295,403066.0,I have the exact marking on my sheet and ain't clear!,311160.0
95295,403257.0,"I have gone through the solution video and my understanding about difference was correct. But, the thing that is not clear to me is, how does the logic with difference is simpler compared to the one with the case logic? In this case, 3 updates are being done to the table after its creation while I've created the Signal column while creating the table itself which is more concise and optimzied. I even remember many questions on the discussion forum with CASE logic which were TA verified. If it was that important that we need to use the difference logic, this should have been made it clear before itself? Now, who can confirm that which is more simpler way or which is more optimized way? Can some TA please confirm.",318329.0
95295,403272.0,"I have just run both (Queries from the solution file and create table with case logic of mine) and below are the results of execution time 0.374 sec (adding signal column to bajajx, creating bajaj2 from bajajx, updating signal column with buy signal and updating signal column with sell signal) 0.141 sec (creating the table including signal column using case logic) Which is simpler and better?",318329.0
95295,404133.0,"Hi Nagaraju, Please apply for revaluation with your exact concern and we will make sure we try our best to address your issue. Thanks",301619.0
112639,486001.0,Check this https://stackoverflow.com/questions/51182025/python-very-large-correlation-heatmap-for-many-variables/51182474,310974.0
112639,486004.0,"The standard code that works well for me is : plt.figure(figsize = (16,10)) # Size of the figure sns.heatmap(df.corr(),annot = True) plt.show() Make sure that you are calling the plt.figure() before sns.heatmap().",313826.0
112639,486012.0,"thanks I got it usage of large numbers helped me. plt.figure(figsize=(46,30))",304692.0
112639,486507.0,"If you click in the space that shows the heatmap, it opens up in a larger view. I have tried that in Jupyter notebook",308435.0
112618,485950.0,"If you look at the various values of the column, ""seconds"" seems to be unit of measurement. It may be different. TAs answer please.",313691.0
112618,485980.0,"Values range from 0 to 2272, though 0 seconds desn't make sense by looking at other values it looks like seconds.",318328.0
112618,485982.0,"If you see the values in ""Total Time Spent on Website"", it is varying from 0 through 2272. It should be in ""Minutes"". Still TA may verify.",311117.0
112618,486097.0,"I think whatsoever the unit may be, it is common for all. As such it makes no difference in analysis. After all you have to eventually scale this with any of the scaler and unit is of no significance as long as it is same for all.",311729.0
112618,486385.0,Units are not important here..just go ahead with assignment,308437.0
112618,486644.0,"Dont worry about the Units after scaling, as Scaling bring every feature with same sacle/units.",315560.0
112625,485978.0,"Create for both and check the collinarity. If they are highly collinear, drop one of them.",310974.0
112625,486339.0,"Do we needs to do corr only for these dummies creation for last activity and last notable activity. Because when we do each one 17+17, 34 features comes out. If add to original data set still more We should do corelation between only these and drop or check for full df corr ?",312019.0
112625,486049.0,Yes you can create for both and check for collinarity using corr() function.,306010.0
112775,486513.0,"This is the target. Our model will predict the conversion rate. If it meets the target, that’s good. If not, then we need to provide recommendations on how it can meet the target. Hope this helps.",308435.0
112775,486636.0,The current conversion rate is 30% we need to identify the key drivers and predict at least 80% of the data through our final result. It's a value which will help you to finalize the model.,318451.0
112775,487004.0,"Hi, Once you have created the bucket of Hot Leads as per your model. The accuracy metric on your predicted values on Test Data should be more than 80%. Hope this Helps.",318605.0
112785,486536.0,columns with large null values can be outrightly dropped. however there is no fixed value for null value percentage. it can vary case to case and also column to column.,311686.0
112785,486523.0,you can drop the columns having more than 3000 missing values. avoid dropping rows. you can visit the following link for TA varified answer: https://learn.upgrad.com/v/course/208/question/112697,302738.0
112785,486631.0,Actually it is difficult to impute values in columns with missing data and hence it would be better to drop them.,313767.0
112785,486548.0,"Columns having more than 25% null values can either be dropped or imputed. That will depend on the analysis of the columns. If it looks to have no significance in the analysis, it can be dropped.",304319.0
112785,486582.0,"Check the relevance of the field while dropping it, Larger null values indicated that data is not complete but relevance of the fields also need to be evaluated",310629.0
112785,486711.0,"before going for straitght away drop, It is better to look for conversion ratio in the null values and if it is high..then we can mark it as 'others' and go ahead for dummies",300723.0
112785,487246.0,"before dropping just check the data diversity in the columns as well. Unique values in the data may refer to something that can be useful in the model. If it looks important, impute the nulls with some default value. else, drop them",305839.0
112785,488067.0,yes we can delete,308964.0
112802,486570.0,"Hey Ruchita, I think the link mentioned below can help you in clarifying your doubt better! https://discuss.analyticsvidhya.com/t/what-is-the-difference-between-predict-and-predict-proba/67376/2 Hope this helps you out!",301655.0
112802,486577.0,Got the difference. predict() is used on the GLM statsmodel whereas predict_proba() is used on the model after LogisticRegression without the stasmodel.,304319.0
112802,486650.0,Predict will give either 0 or 1 as output Predict_proba will give the only probability of 1.,320636.0
112802,486610.0,"Not really, .predict() and .predict_proba() are different functions available under sklearn. Predict - gives the class of the outcome (0 or 1 for binomial problems) predict_proba() - gives the probability of outcome. Given this probability and your chosen threshold, the outcome could be 0 or 1 https://stats.stackexchange.com/questions/329857/what-is-the-difference-between-decision-function-predict-proba-and-predict-fun sklearn and statsmodels are 2 different packages/framework for machine learning.",318438.0
112820,486629.0,"Yes, we can delete such variables. Please check below link on a similar query https://learn.upgrad.com/v/course/208/question/112676",313826.0
112820,486640.0,yes you can delete such columns. no variation means no information so keeping them would not benifit you. rather deleting them would reduce the dimentionality.,302738.0
112820,486670.0,"Yes. We have 5 columns (i.e. Magazene, Recieve more updates, Update me on supply chain, Get update on DM, Agree to pay amount) in our data set, where it contains ""NO"", which don't have any significane to keep. Hence, it could be dropped. TA has verified this also.",311117.0
112820,486748.0,Yes these columns can be removed as they dont explain any variance for data. So will not impact predicted variable.,317156.0
112820,486815.0,"In my opinion both Newspaper and NewspaperArticle should be dropped because if we look at these columns all the Values are 'No' , that means every Lead/customer selected only 'No' i.e. same option which doesn't explain much variance , also if we look it from a different point of view like if everybody selected the same option how can it be distinct and helpful in determing our goal of identifying who can be a potential Lead and who can not be. Likewise there are many columns like this which have only one value. I dont think these can be helpful in deteming our goal.",315560.0
112820,487016.0,Variables which have only one value can be dropped as they bear no significance in prediction. We have followed this approach in previous assignments for Multiple linear regression as well.,317689.0
112821,486639.0,"You can use the folowing code to concatenate: df1= pd.concat([df2,df3] , axis=1) for the reason behind the error you can refer this link: https://stackoverflow.com/questions/39534676/typeerror-first-argument-must-be-an-iterable-of-pandas-objects-you-passed-an-o Hope that helps.",317998.0
112821,486637.0,i would suggest you not to use PCA. instead go for RFE. please refer the following for more clearity: https://learn.upgrad.com/v/course/208/question/112613,302738.0
112821,486672.0,Make sure you are using a list or a data frame in Pd.concat and on the correct axis. Refer the official documentation. For Pd.concat.,318340.0
112830,486689.0,You can visit below link where TA's has mentioned to drop column having more than 3000 NaN values https://learn.upgrad.com/v/course/208/question/112697 Hope this helps.,317991.0
112830,486686.0,"You can drop the columns having more than 3000 NaN values. TA has confirmed this. Rest all, try business logic to impute or if the values are small, drop the rows. You ought to have at least 70% of the original data.",302738.0
112830,487043.0,Drop all the columns with more than 3000 NaN values(which includes 'select' values also in some columns). Also you can drop the columns with same value in all the rows.,318448.0
112747,486555.0,"Yes, PCA should be used for dimensionality reduction and removing multi-collinearity. But before that all the categories need to converted to numeric by one-hot encoding (using dummy method).",304319.0
112747,486411.0,"PCA is used for capturing variance and eliminating multicollinearity, resulting in dimensionality reduction. For PCA numeric variables are needed. If most of the variable are categorical then it is not a good idea to use PCA. After data cleaning, 70% or more of the original rows should be there. Row elimination may not be preferred as you will lose data for all 37 variables by row elimination.",301644.0
112747,486428.0,"You can visit below link for the similar kind of question. You can go throughTA's verified answers. https://learn.upgrad.com/v/course/208/question/112519 Hope this will help,",317991.0
112843,486752.0,I can see how they might be different - newspaper article might be an ediorial piece while Newspaper might be an advert - just my opinion. If for example you check the values and it shows they are [nearly] the same then one will auotmatically get dropped during the subsequent steps.,300694.0
112843,486835.0,check if the data seems interesting enough then keep it else drop it,318017.0
112843,486813.0,"In my opinion both Newspaper and NewspaperArticle should be dropped because if we look at these columns all the Values are 'No' , that means every Lead/customer selected only 'No' i.e. same option which doesn't explain much variance , also if we look it from a different point of view like if everybody selected the same option how can it be distinct and helpful in determing our goal of identifying who can be a potential Lead and who can not be. Likewise there are many columns like this which have only one value. I dont think these can be helpful in deteming our goal.",315560.0
112843,486761.0,"If we have confusion for similarity of two columns, its better to keep both and check the collinarity. If they are highly collinear, then drop one of them. You can refer this TA verified discussion: https://learn.upgrad.com/v/course/208/question/112625",311117.0
114450,493469.0,"No Lovish, you can check the data spread using a pairplot and see that there is a significance of this variable.You may not want to discard this variable altogether",305839.0
114450,493746.0,"Hey Lovish, So, first you should check the null percentages of the columns. If it is more than 3000 (approx 30%) then you should drop them since they would not have sufficient information to help in analysis. Second, if the percentage of null values is not that high (more than 30%) and the columns seem to be important from business perspective, then you could impute the missing values using statistical methods such as Mean, Median or Mode. value_counts() function might be helpful in the second approach. Hope that helps.",317998.0
114450,494586.0,"30% and above null or na values exist, then drop the column. If less than 30% null or na values, then impute the missing values.",312953.0
112857,486884.0,"First, convert all the ""Select"" values to ""Nan"" as per the problem statement. Then you can delet all the columns which has more blanks (based on the cutoff %age you decide). Should not impute ""Unknown"" for ""Select"" values.",316202.0
112857,486834.0,try to think as a user i think select is the value when a user is trying to find the specialization and he ends up not having his specialization and nulls are value where he or she has not even selected the dropbox,318017.0
112857,486799.0,"hi Rohit, 'select' value should be treated/converted to Null. this has been also mentioned in the problem Statement page. in fact there are multiple columns where this 'select' value can be seen.",311686.0
112859,486833.0,we can drop the column as imputing the values in this column is not possible as high number of values have null and select,318017.0
112859,486858.0,"in my opinion, you should go step by step. first convert all ""select"" in the dataset to null. you can use np.replace() function for the same. then check and drop columns with high null values. because there might be other columns as well that show high null percentage. thus a systematic approach might be better than checking individual columns and dropping them. hope that helps.",317998.0
112859,486880.0,"You need to convert all the ""Select"" va;ues to Nan and then decide which one to delete.Once all the ""Select"" values are treated, based on the %age cutoff you decided, you can delete them.",316202.0
112859,486822.0,"yes. with select value as null, the %age of null values will go to about 74. it's better to drop this column.",311686.0
114452,493788.0,As per my suggestion you simply drop that column as it has more than 35% null values. or else you may Impute it as 'Other' country also make sure that 'Select' value should be consider as Null,311004.0
114452,493466.0,"Hi, Country and Occupation are significant variables and hence should not be dropped. For such categorical variables, the impute strategy should be to use a default value that will not give undue advantage to the existing values. 'Unknown'/'Other' are good options in my opinion. Regards, Soumik",305839.0
114452,493743.0,"Hey Lovish, So, first you should check the null percentages of the columns. If it is more than 3000 (approx 30%) then you should drop them since they would not have sufficient information to help in analysis. Second, if the percentage of null values is not that high (more than 30%) and the columns seem to be important from business perspective, then you could impute the missing values using statistical methods such as Mean, Median or Mode. value_counts() function might be helpful in the second approach. Hope that helps.",317998.0
112881,486894.0,"i think, the problem might be before the test train split. check the number of 0s or 1s in that column before the split. if it has very high percentage of only ONE type value, then that column should be dropped since it would not be very helpful in analysis and prediction. hope that helps.",317998.0
112881,486926.0,"Hello Sudheer, In such a case it is always a good practice that you can go back to EDA, to do a little bit of further analysis and do the test using ""count"" how many are 0 and how many are 1 which will eventually will give you a clue to drop that column or not. In fact, this should be the case as and when you find some anomaly or any weird output after certain codes. EDA is always an eye opener in such situations! OR You just pass into RFE, it will remove those variables. If still, you are getting such kind of variables, then pass it into model, the model will show as insignificant variables.",301121.0
112889,486908.0,"Between 'unknown' and NULLs, Country has ~27% and City has ~40% nulls. I cannot figure out a way to impute them. Best possible solution is to drop them.",318438.0
112889,486916.0,"hello Subhransu, I would suggest: Drop the columns manually based on the following information and not on what it appears to be. 1. Columns which have too many null values (more than certain percentage) These include values 'Select' in some cases in addtion to Null values 2. Columns have same values throughout (Like No or Yes) having almost 0 variation. 3. If two columns are exactly identically mapped. (if one column is x and another columns y and the pattern is same throughout) then one any can be dropped. If none of the above is true, let RFE take care of it and we have to do using normal procedure of probability for significance and VIF for multi-collinearity",301121.0
112889,486903.0,"Hey Subhransu, Yes, again it depends on your understanding of the domain and what your business logic is! Our group dropped it because of high null values and not being able to impute categorical variables. Hope this helps!",301655.0
112889,487105.0,YES DROP THEM,315679.0
112897,486933.0,"In my opinion, It would depend upon your perspective of the business and its domain. If the data in a column does not seem to be useful for analysis / prediction and has high percentage of Null values, it might be more prudent to drop them.",317998.0
112897,486952.0,city column looks ambiguous as if u select country outside india then also city column has city in india only so try to remove it all together.,318017.0
112897,487743.0,I think you can drop the country column because it has no value to keep in analysis. As you can see in the dataset most of the people are from india as compare to the foreign country.we can calculate the percentage of country(India) who opted this course and we can neglect the foreign country.Country column has a categorical variable.So simply we could drop it. Hope this help!,308639.0
112898,486951.0,you can drop the columns if the data in it is null for more then 30 percent,318017.0
112898,486929.0,"Yes we can drop it. As per TAs verified answers, any column having more than 3000 null values can be and should be dropped, since it would not be of much help in our analysis. You can refer this link for TA's verified answer: https://learn.upgrad.com/v/course/208/question/112697 hope that helps.",317998.0
112898,486958.0,"Problem statement says Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value. But here is a catch, statement says as good as null but it doesn't tell it is Null with sureity. So I think we should not make all Select values to Null beacuse if we do so than in many column number of Null values will increase and eventually we will have to remove that column. One solution is try to imputer some logically values from the column in place of 'Select', and if it is not possible then only consider it as null. Rest TA can give more information about it. Hope this will help",317991.0
112898,487040.0,"This is my approach Imputing Nulls with value 'Select', create dummy variables and drop the _select column. This way you won't lose information.",310974.0
112898,487106.0,Better drop it,315679.0
112898,487976.0,Drop and City column . There is no impact on the modelling as Number of NaN are more,313526.0
112900,486930.0,"Yes we can drop it. As per TAs verified answers, any column having more than 3000 null values can be and should be dropped, since it would not be of much help in our analysis. You can refer this link for TA's verified answer: https://learn.upgrad.com/v/course/208/question/112697 hope that helps.",317998.0
112900,486950.0,you should not convert every select to null check each column and based on that column apply null to it.,318017.0
112900,486959.0,"Problem statement says Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value. But here is a catch, statement says as good as null but it doesn't tell it is Null with sureity. So I think we should not make all Select values to Null beacuse if we do so than in many column number of Null values will increase and eventually we will have to remove that column. One solution is try to imputer some logically values from the column in place of 'Select', and if it is not possible then only consider it as null. Rest TA can give more information about it. Hope this will help",317991.0
112900,486960.0,select may have some meaning wrt to each column.. analysis of that may help you to decide to drop it or keep it.,317982.0
112900,487038.0,"This is my approach Imputing Nulls with value 'Select', create dummy variables and drop the _select column. This way you won't lose information.",310974.0
112926,486998.0,yes you are perfectly right.,302738.0
112926,487008.0,Yes 'Converted' is target variable. For Lead Score we need to create new column. Rest TA can confirm on 'Lead Score' column Hope this will help.,317991.0
112924,486991.0,"If two columns have duplicate values for most of the rows, then it is better to Drop one of them. Since the information is already captured by the other column. This can be checked before creating the dummy variables and the column can be dropped there itself. You can also refer to this TA verified answer: https://learn.upgrad.com/v/course/208/question/112582 Hope that helps.",317998.0
112924,487041.0,you can drop any one of them or let them be as it is as RFE will eventually drop any one of them anyways due to high correlation.,302738.0
112924,487104.0,"If we have confusion for similarity of two columns, its better to keep both and check the collinarity. If they are highly collinear, then drop one of them. Can be referred TA verified discussion: https://learn.upgrad.com/v/course/208/question/112625 https://learn.upgrad.com/v/course/208/question/112843/answer/486761",311117.0
112947,487302.0,"I think cutoff probability is 80%, TAs can confirm.",308437.0
112947,487304.0,"The lead conversion rate would be the ratio of number of leads converted to total number of leads while the accuracy considers the count of correctness. So, I believe we need to choose the cutoff to get the best model and then compute the lead conversion rate.",318329.0
112947,487374.0,"Hi, 80% Lead conversion rate means your model P recision will be 80%.",344894.0
112923,486993.0,I think it should be done before test-train split. The finding of train data will be applied on test data. So if we perform scaling after split then situation will arise that train dataset will be scaled one but test data will not be scaled. And the finding on train dataset will be on scaled dataset. While when we apply these finding to test dataset then it might cause problem because of unscaled test data.,317991.0
112923,487000.0,"Hi, You will scale only those variables which are not created as Dummy variable or which does not have modifie Binary Values. You can scale the variables after doing Train/Test Split but you need to be careful to Scale on the Train Data as well as the Test Data before you create your model. Hope this helps.",318605.0
112923,487037.0,I think it has to be done before splitting the data so that both train and test frames would be of same scale.,318329.0
112923,487092.0,"You should perform standardisation only on training data. i.e after test- train split. Test data is something which is used to test your model. So, if you do standerdization on whole data before split then you will not know the model validation / model performance on unknown or unseen data.",318328.0
112923,487352.0,Scaling should be done after train test split and also it should be done only on continuous variables. Dummy variables should not be included as already they have combination of 0 and 1,317156.0
112957,487126.0,It could be because you are not using inplace=True. and hence the updated values might not have been saved. check if you have done that or not. hope that helps.,317998.0
112957,487132.0,"Also, do note that you should not replace it with ""NaN"". because ""NaN"" would become a string and not be counted as in null calculation. instead, you should use np.nan like this: df.replace(""Select"", np.nan, inplace=True) hope that helps.",317998.0
112957,487128.0,Why do you have to replace it with null? Just count the number of select + null and decide if you have to drop the column. You can use df.replace() function. Below links should help. https://stackoverflow.com/questions/17097236/how-to-replace-values-with-none-in-pandas-data-frame-in-python https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html,312376.0
112958,487216.0,Just copying the exact text from the word document. Question 1: Which are the top three variables in your model which contribute most towards the probability of a lead getting converted? Question 2: What are the top 3 categorical/dummy variables in the model which should be focused the most on in order to increase the probability of lead conversion? Both look similar to me as well. TAs could you please confirm,310481.0
112958,487371.0,"Hi, Question 1: Which are the top three variables in your model which contribute most towards the probability of a lead getting converted? Ans. Based on the positive and high beta value you have to write three variables which can be numeric or categorical in your original data. Question 2: What are the top 3 categorical/dummy variables in the model which should be focused the most on in order to increase the probability of lead conversion? Ans. In your final model just select the categorical variable (I mean in original data variable was categorical but now that variable in dummy format ) and based on the positive and high beta value you have to write three variables",344894.0
112965,487157.0,"i guess by the question they mean after the cleaning part whatever we are left with, they want 100% of that. i think one way to achieve this could be keeping the probability cutoff value low. it will mark some cold leads as hot leads but will mark all the hot leads accurately.",302738.0
112965,487225.0,"I think you need to use the Logistic regression model evaulation techniques like ROC curve, Specificity and Sensitivity concepts which were taught in logistic regression model.",310481.0
112965,487637.0,"This is regarding the hyperparameter of the model which is the threshhold (cut off) probability. On increasing it, we increase the sensitivity while on decreasing it, we increase the specificity. So depending on the scenario, we have to increase or decrease it.",304319.0
112967,487203.0,"Hi, a general question. If this is only Logistic regression case study why this was given after PCA module instead of logistic regression model. I understand that PCA may not work well with the categorical variables. In the telecom churn case study Rahim has used PCA with logistic regression for model stability also potential loss of information due to dropping lot of variables etc.. please provide your thougts.",310481.0
112967,487159.0,"No. as we have almost all the variables as categorical one's, you should go for RFE. you need not perform PCA.",302738.0
112967,487169.0,"For categorical data, we should go with RFE. Detailed verified discussion, can be refered: https://learn.upgrad.com/v/course/208/question/112546 https://learn.upgrad.com/v/course/208/question/112613",311117.0
112967,487315.0,"RFE or PCA are different ways to achieve the same thing, reducing the number of features. As far as the argument/discussion going on, on whether RFE is to be used on Categorical Data and PCA on continuous, please realize 'none' of the machine learning models (there are some in NLP space, but not in what we have learnt so far) work on categorical data. Every categorical data has to be converted to numerical equivalent either through LabelEncoding or One Hot Encoding or any other means. Review the Telecom Churn with PCA notebook. The same results/accuracy has been achieved by using RFE and PCA. With PCA, the number of steps has reduced significantly, with the added task of interpreting the components. So it is not true that you cannot use PCA in this case. If this argument is incorrect I would expect the TA's to provide a commentary.",318438.0
112967,487322.0,You can use PCA and RFE.... nothing wrong in doing that.... it is just that you are proving that your model is upto the mark to the client.,318446.0
112967,487634.0,"Both PCA and RFE should work well. But infering the important features after PCA becomes difficult. So in this case study, as we need to suggest top 3 imp features, pca should not be used.",304319.0
112994,487349.0,"I would suggest: Drop the columns manually based on the following information 1. Columns which have too many null values (more than certain percentage) These include values 'Select' in some cases in addtion to Null values 2. Columns have same values throughout (Like No or Yes) having almost 0 variation. 3. There would be cases where you can impute If none of the above is true, let RFE take care of it and we have to do using normal procedure of probability for significance and VIF for multi-collinearity",301121.0
112994,487341.0,"Dropping variables/features should be done after analysing them. Not sure what you mean by ""plotting sns plot"". What values are you trying to plot?",318438.0
112994,487413.0,"When you get a dataset, start with the basics( what we have learnt in our previous assignments) 1) You do Exploratory Data Analysis i.e check for nulls,remove or impute nulls ,drop columns that has too many null values,check for outliers etc 2) Once the dataset is cleaned proceed with logistic regression. Follow the steps given in the logistic regression module",311254.0
112994,487348.0,It may be due to there are large no. of columns. First do the EDA and then on that basis drop columns which are not required. Then try to plot heat map/pair plot,318429.0
112994,487724.0,"Hi Umesh, I think you are talking about plot as the heatmap of the correlation matrix. With large number of variables you will get a big heatmap which on double clicking you should be able to expand. Thereafter, you need to closely look into the heatmap. The large correlation values whether positive correlation or negative correlation will be there in a different colur in the heatmap. You have to atentively look into these values and when you find a large correlation e.g. above 0.7 (just hypothetical figure), you can drop one of the two coulmns who are contributing to this correlation. Just to make the work somewhat easy, I can suggest you to choose one of the half of the diagonal seperated by values 1. The values in upper half diagonal are copied to lower one in the way that the correlation between column A and column B is same as correlation between column B and column A. This will reduce half of your effort.",311729.0
112991,487303.0,Its the ratio of the leads converted to total number of leads.,318329.0
112991,487295.0,It is a benchmark for your models.,310974.0
112991,487411.0,TA has answered it here - &gt; https://learn.upgrad.com/v/course/208/question/112947/answer/487374 and it is precision metric.,318329.0
112991,487630.0,"Lead conversion rate for the model would be from the total predicted converts, how many have actually converted. This is also called the positive predicted value or precision.",304319.0
112991,487739.0,thanks everyone for nice explanations,318005.0
112991,487399.0,In Literal Terms Lead Conversion Rate is the number of Leads which are actaully converted from the total lead that you have. In Machine language terms it would signify the accuracy rate of the model that you are building where the task is to have a rate of more than 80%. Hope this helps.,318605.0
113012,,nan,
113018,487372.0,"Yes, you are right. We ned to drop the features with p value &gt; 0.05.",316202.0
113018,487385.0,"The model needs to be as lean as possible, not compromising the various metrics. Variables with high VIF (&gt;5) and high p-value ( &gt;0.05) can be dropped one at a time and the metrics can be re-assessed to ensure no significant drop in metrics.",313826.0
113019,487421.0,ROC curve seems fine to me. as far as accuracy is concerned i got better accuracy but i guess 73% is also ok. we need precision to be 80%. check that. you can refer the following for TA varified answer: https://learn.upgrad.com/v/course/208/question/112947,302738.0
112945,487111.0,you may look at the pca component plot to find the variables contributing to your finding.,310217.0
112945,487134.0,"Hello RG, I am not sure why you have to use PCA in the first place for this case study. But to answer your question which features actually contributed to PCA, there is a way to find the loading factors on all PCs by columns you used for PCA. But it gives you The following set of codes will give you loading on PC1 and PC2 by original variables as an example If you have executed pca on normalized_df colnames = list(normalized_df.columns) pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames}) pcs_df",301121.0
112945,487231.0,I hope the following link helps. Especially look at the answer by behzad.nouri in the below link. https://stackoverflow.com/questions/22984335/recovering-features-names-of-explained-variance-ratio-in-pca-with-sklearn,310481.0
112546,485680.0,"If by dimensionality reduction you mean PCA then .. It depends. There are ways you could use PCA for dimensionality reduction, but personally I will just be using RFE to minimise the number of features",300694.0
112546,485674.0,"There are a lot of features in the dataset given. Also, most of them are categorical variables. We need to use RFE to reduce the variables.",308435.0
112546,486070.0,"PCA can be used here by assigning numeric variables to the categorical variables however PCA is best at capturing maximum vaiance. Since we will be attributing steady values to the categorical variables, PCA will not perform very well. The best option would be to drop columns with high null values, drop some manually based on your assessment of their relevance and then create dummy variables and use RFE",316416.0
112546,487987.0,"For Diemension reduction use follow steps : Data cleanp : # dropping columns having more than 25% NULL values # # dropping columns having single value as NO #### Converting some binary variables (Yes/No) to 0/1 ### Drop the ID columns City, Country #### Remove Outliers",313526.0
113034,487419.0,if your model is working fine both the curves will show approximately the same cut-off. so i think check both and arrive at an optimal value.,302738.0
113034,487513.0,You can use either of them based on how well the cutoff is shown clearly in the chart.,310974.0
112838,486708.0,try to impute data with some value rather the. deleting it and create dummy variables on it,318017.0
112838,486718.0,One approach can be: Try to bring the categories down say from 10 to 4 or 5. This way you can reduce the number of dummy variable to be created later.,317991.0
112838,486724.0,"what value to iimpute it with, thats like completely a grey area. What to keep in mind?",301649.0
112838,486756.0,you can go with bucketing - so say if you have 20 different categories you can think about 'binning' it down to 5 - for example. An example might be countries - where if you have 100 different countries you can think about breaking it into regions,300694.0
112838,486848.0,"Check how many nulls are there. If it is more than 3000. remove the column if not then try to impute it with a work like unknown , or if there is very less Nan like 5% then you can drop the rows",311952.0
113072,487686.0,"Following is what is written regarding the submission: Please submit a zip file containing the following three files: Python commented file : Should include detailed comments and should not contain unnecessary pieces of code. Word File: Answer all the questions asked by the company in the word document provided. Presentation: Make a presentation to present your analysis (you should include both technical and business aspects). The presentation should be concise, clear, and to the point. Submit the presentation after converting it into PDF format. Point 2 clearly indicates that the approach and answers to Q3 and Q4 should be written in the word document only. Also since it is a subjective and verbose question, it would not make sense to include it in the python notebook. Conrete answer in the word document would make more sense. Hope that helps.",317998.0
113073,487625.0,You can go for both the approach but scalling and split will reduce your work as you don't required to scale test and train separately.,306010.0
113073,487556.0,yes. both the approaches will do. you can either go for scaling before splitting or splitting before scaling. but dont forget to scale the test set too if you are splitting and then scaling.,302738.0
113073,487682.0,"As per TAs verified answer, You should first split the data into Train and Test and then go for standardisation or scaling on the Train data . refer to this TA verified answer for more insight: https://learn.upgrad.com/v/course/208/question/112923 hope that helps.",317998.0
113073,487731.0,"If you are doing the scaling before spliting the data then you are actually creating the relation between test and train. e.g. max and min values will differ in both the scenario. Test and train should actually be completely seperated and as such you should do the scaling on the train data only. this train data scaler than should be applied to as to conclude to must extant your model based on training data is adapting to the test data. In real sceario also suppose if max value for one variable is coming as 100, tomorrow the max value may change to 110. Your model also needs to be tested on such flexability. Therefore, it is highly recommended to perform scaling on train data and make the test data use the same scaling to get the correct model.",311729.0
113259,488947.0,If the NULLS are low then. If the NULLS are pretty high then probably not a good idea.,304813.0
113259,488492.0,A fresh approach which I've never seen in any articles I've gone through. Interesting. I can't say yes or no but good thinking :),310974.0
113259,488556.0,"It is quite unconventional even though interesting. If such a huge portion (70%) is filled with values in existing data ratios, it is a big assumption. Data analysis may go haywire resulting in incorrect in prediction and may mislead in effect.",301121.0
113259,488624.0,"Approach is unique. But generally what we do is remove the column if the missing values are very high. And in this case it is too high to impute. Now coming to your approach suppose we go by your approach. In that case you are generating hypothetical values. Now if we do prediction on these values then that prediction will also be hypothetical. This we generally don't do in Data Science. In Data Science we work on past available data for prediction, but not on hypothetically generated data.",317991.0
113100,487672.0,"I dont think the ROC curve intercepting on the Y-axis gives any indication of the model being overfit. One indication of overfit could be if the AUC (Area under the Curve, which is 0.96 in the pic you have attaced) is very large for the Training Set, while it is comparatively very Low for the Test set. That could be an indication of the model being overfit. In an ideal model the inflection point of the ROC curve would be very close 1, and AUC would also be very close to 1. (ie, the Y-intercept of the curve would tend towards 1). Thus, a part of the curve would lyinh on the Y-axis (intercept approx 0.78 in your pic) does not really indicate that it is overfitting. You may also refer to the following posts for more insight on ROC and overfitting: https://stats.stackexchange.com/questions/71946/overfitting-a-logistic-regression-model/71950 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2698108/ Hope that helps.",317998.0
113129,487836.0,"good question, same one I had and was causing me confusion. Glad I am not the only one - upvoted!",300694.0
113129,487938.0,It is leads that we predicted as converted/total number of leads. total number of leads=leads converted+leads not converted.,318084.0
113129,487942.0,"This is my interpretation Assume X education has 10 leads identified. In absence of any model, all 10 are predicted as Hot Leads and are nurtured equally. 3 of the 10 predicted get converted to paying customers. You Lead conversion is : Predicted Yes is actual Yes / total number of predicted Yes = 3/10 = 30% Now say for next month you build a model based on past data. Say next month you have 10 leads identified. Using some cutoff you predict 5 of them as Hot Leads. You nurture only those 5 leads and say 4 of them get converted. You Lead conversion is : Predicted Yes is actual Yes / total number of predicted Yes = 4/5 = 80% Now you need to find the optimal cutoff that would give us conversion rate of 80% Note that there might be cases where from the remaining 5 not nurtured leads, some get converted. Precision in this case study = leads who got actually converted/leads that we predict as converted",317514.0
113106,487675.0,"Yes. If some columns have only one unique value then it should be dropped. Since it would not be of any help to us in our analysis. Furthermore, if any column has 2 unique values, But majority of those values (say 99.xx%) are of ONE type only then also that column should be dropped. Since it can be considered to be having a singe unique value only, which is not useful in analysis. Hope that helps.",317998.0
113115,487750.0,"Hello Jaykumar Good that you have completed the first step and landed on to 31 columns. Look for those columns where there is hardly any change, It is almost the same value throughout which means it does not have any bearing on the 'Converted"" . You can drop all those columns Null values in Some categorical columns can be imputed with categorical value which has appeared maximum number of times. See What you can do with Country column looking at the corressponding city column. Null values in Some numerical columns can be imputed with mean EDA plays fairly significant role in this case-study in getting the unwanted columns removed. I hope it is clear.",301121.0
113115,487719.0,"If you mean to say that you are having 5 columns with around 15% null values, then you can check the data dictionary and as per your perspective of business understanding/ or problem statement decide whether the column is useful or not. If you think the column is not useful in terms of business understanding it would be better to drop them. If a column is useful then you could do either of the following: There are two ways to treat these small percentage of Null values. 1. Delete the rows of the dataset that contain null values for the said column. This would mean loss of information, but since the null percentage is small, the loss would also be small and thus acceptable. 2. You can impute the missing/null values using statistical units such as mean, median or mode of that column. Hope that helps.",317998.0
113114,487713.0,"There are two ways to treat these small percentage of Null values. 1. Delete the rows of the dataset that contain null values for the said column. This would mean loss of information, but since the null percentage is quite small, the loss would also be very small and thus acceptable. 2. You can impute the missing/null values using statistical units such as mean, median or mode of that column. In either case, you should treat them and then only continue. Hope that helps.",317998.0
113114,488009.0,We can remove the rews having 1 % missing value. This would not have much impact on the analysis,305650.0
113116,487716.0,"Yes you can create dummy variables for all of them. The columns would increase, but using Feature selection method such as RFE or PCA you can reduce the number of columns. Alternatively, you can also club some the unique values of a column into another unique value. for examply, value x,y,z could all be clubbed under ""others"". Hope that helps.",317998.0
113116,487758.0,"Hi Arjun, 15, 20 and 36 are not many especially compared a data set of more than 9000 rows. They are fairly reasonable. Treat them as categorical variables. I would suggest you to get how many times each value appeared in a categorical variable using value_counts() This may give you ideas to impute them later with appropriate values based on the number of times it has appeared throughout the dataset for that column. In RFE, any you would enter a number between 12 to 15 anyway which would reduce the number of columns considerably.",301121.0
113116,487971.0,My approach : data cleanup : Drop columns for Max NULL values Drop columns based on Only Single values (No cases) Convert No/YES to 1/0 Do clean up Then go for dummy varibales . Then . number of dummies will be on only 4 varibales Lead_**(2 variables) and Last_**(2 variables),313526.0
113116,487972.0,Data cleanup then Data Prepaation then modellling,313526.0
113119,487736.0,"Hi Prashant, You can go through the following TA verified discussions to get a better idea of the approach for the case study: https://learn.upgrad.com/v/course/208/question/112439 https://learn.upgrad.com/v/course/208/question/111457 https://learn.upgrad.com/v/course/208/question/112433 Hope that helps.",317998.0
113119,487761.0,"Hello Prashant Just few Major points here to remember by which you have to start You have to use Logistics Regression, but before that, You will have to drop so many columns as many will not contribute to model As usual RFE, probability and VIF test, predict ROC curve Accuracy, sensitiviy etc As Harshit mentioned, there are lot of discussion in discussion forums",301121.0
113134,487842.0,Drop them. With 40% nulls you can't impute and dropping rows doesn't make sense.,310974.0
113134,487846.0,"Hey Anmol, Actually, I've done the other way round I have imputed the values of columns Asymmetrique index and profile scores with their respective mean values and after that I did all the dummy variables creation and all. I got an accuracy of 79% from the model building process and a pretty much good looking ROC curve.",301655.0
113134,487874.0,"The approach we are following is that if it has more than approx 30% of useless data (blanks, 'Select') AND we can't impute meaningful values then we drop the column",300694.0
113154,487928.0,"As per TAs verified answers, you should drop all columns having more than 3000 null values (approx 30% null values). since columns with high null values will not have enough information to help in our analysis and jmputing them would introduce a bias. hence it is better to drop them. hope that helps.",317998.0
113169,488010.0,"As TA has suggested, precision is the 80% cutoff we are looking for Personally, I was giving greater priority to sensitivity (over accuracy) but changed my cut-off to give priority to precision (and hence to specificity). Our model's sensitivity fell to just over 62% in the test data at that point. But accuracy is circa 80% in both cases.",300694.0
113169,488080.0,Precision talks about the reproducability whereas sensitivity tells you how small variation in probability can affect the conversion rate in our case. It is preferable to have 0.8 with fairly less sensitiviy for the model to be stable. TA can clarify.,301121.0
113169,488154.0,"It depends on the business case that you are looking to address. In the current case study, we are given target lead conversion rate of 80% or precision of predicted lead conversion as 80% i.e leads that are Predicted Yes is actual Yes / total number of predicted Yes",317514.0
113169,488959.0,"Focus on Precision for the case study, these parameter changes as per business needs. May be the case study requirement can change if the CEO would have said, the recall(count of predicted true which are actual treu from total number of actual true, i.e. RECALL) shoud be 80 % :)",304813.0
113175,488262.0,We need to check the combination of p-value and VIF for final variable selection,305650.0
113175,488019.0,You should use a combination of both. High p-Value definetely signifies that the feature does not contribute much to rejecting the null hypothesis.,318438.0
113175,488124.0,"Every time you build a model, you need to check for p value. if it is greater than 0.05 you need to drop those variables. So even if ViF&lt;4, but p values are high, variables should be dropped. Model should be a combination of variables having low p values and ViF&lt;5",311254.0
113175,488022.0,I believe in the churn case all p values were below 0.05 hence they went ahead with VIF you should follow same steps as before: (1) ensure no p values are greater than 0.05 - if there are keep dropping one by one until condition is satisfied (2) THEN ensure no VIF values are &gt; 5; if no values &gt; 5 then you can stop; BUT if there VIF values &gt; 5 then drop only one feature (ideally the one with greatest VIF but there might be business reasons to choose something else); then go back to step 1;,300694.0
113175,489607.0,"And also find out whether you have achieve 80% precision with the identified list of variables (15, 20, 25)",310518.0
113190,488102.0,"If you think the continuous variables in your dataset have nearly equal or close range and/or variance then you dont need to scale. Otherwise, if the data of one column has a far greater or smaller range as compared to other column then you should scale it so that all columns come to have similar range. Quote from the link below: ""The idea is to rescale an original variable to have equal range and/or variance."" You can refer to this link for more insight on when and why to scale: https://www.listendata.com/2017/04/how-to-standardize-variable-in-regression.html Hope that helps.",317998.0
113190,488144.0,If the selected variables are in wide range than you need to scale for the model to converge fast,317514.0
113190,488120.0,"Check if your dataset has continuous variables or not. if yes, feature scaling is needed.",311254.0
113190,488295.0,check the variance between continuous data columns using df.describe and see if all of them are around the same value - if so then you might not need to do any scaling,300694.0
112439,485639.0,How to find which columns are important? Is there any particular way mentioned anywhere or just have to do it logically?,318448.0
112439,485275.0,How important are these columns? Aswer to this question should help you decide what to do.,312376.0
112439,485392.0,Most missing values occur in the index and score assigned to each customer based on their activity and profile and tags which do not seem unnecessary,311864.0
112439,485161.0,"Generally, the approach is like 1. First check the variables having high null values are significant i.e. are required for analysis or not. 2. If not then you can simply drop that column but if it is required then check wether we can impute some values or not. 3. If we can't impute logical values then only step left is drop the columns. 4. columns which have a high number of missing values. Clearly, these columns are not useful. Since there are 9000 datapoints in our dataframe, eliminate the columns having greater missing values So need to check from above steps that which one is better in this case study. Rest TA's can give more insight,",317991.0
112439,485686.0,"Just as we have approached the same problems in all our other assignments.. Impute or drop, depending an various factors",300694.0
112439,485771.0,Prospect ID and Lead Numer are unique. We can drop Prospect ID ? If missing more 40% like this i think drop them.,312019.0
112439,485921.0,It that column isn't very helpful in our analysis. Its better that we drop it.,311855.0
112439,486194.0,If the column has only single value or high null values then consider dropping as this column will fail to bring any insight during EDA.,301644.0
113201,488205.0,Thanks for the response. Another easier way would be df.isin(['Select']).sum(axis=0),312096.0
113201,488145.0,"you can use the value_counts() function of pandas to get the count of each unique value of a particular column. you can loop through the list of colmns in your dataframe and use the value_counts() function to get those values for all columns. Additionally, you can use an if condition to restrict the findings to only "" select "" value. hope that helps.",317998.0
113201,488208.0,Use df.isin(['Select']).sum(axis=0). It would give you out the result which you are looking for.,311160.0
113210,488311.0,"The model that we create computes probability for each ""Customer""(each records in dataset) getting converted to ""Lead"" . This probability is the lead score for each lead. Higher the score, higher the probability of the lead to get converted. And viceversa.. The below link explains the same with an example : determine whether a transaction is fraudulent or valid. https://blog.clearbrain.com/posts/how-to-predict-yesno-outcomes-using-logistic-regression You also need to predict on test data, Based on test data you will able to evaluate your model",310467.0
112613,485964.0,you can but it will be not appropriate. refer below link for clarity. https://stackoverflow.com/questions/40795141/pca-for-categorical-features,318377.0
112613,485917.0,yes you can. please refer the links below for TA approved answer: https://learn.upgrad.com/v/course/208/question/112546 https://learn.upgrad.com/v/course/208/question/112519,302738.0
112613,485928.0,You can not. If there are categorical variables PCA shall not be applied. PCA shall be applied on numerical varibales after standardization. You have to use RFE (Recursive Feature Elimination) to eliminate features which are correlated. It shall be done using significance factor (probability) and then VIF.(Variance Inflation Factor) which will finally leave you with fairly independent variables for you to proceed further.,301121.0
112613,486182.0,PCA performs well on continuous data. Here we have more of categorical data. Using RFE would be a better technique,316147.0
112613,486846.0,Performing Logistic Regression to find the variables which impacts for calculation of Lead_Score is sufficient. I dont see any need to apply PCA,308636.0
113216,488254.0,i guess starting anything between 20-30 variables would be sufficient enough to get a decent model.,302738.0
113216,488258.0,ok thanks Naseem,305650.0
113216,488261.0,You can even go for 15 variables for RFE and you would be able to get a fairly very good model.,301121.0
113216,488950.0,RFE will always give top vars whatever you pass it. If you want to cover the most part of it pass higher number and step by step remove by mnaual observations.,304813.0
113216,488291.0,"Try a few different figures like 10,15,20 and see how it impacts your model - not just for training data but also for test data. Then choose a number accordingly. I remember hearing in one of the videos that it doesn't make sense to go beyond 20.",300694.0
113216,488672.0,"Start with 25 variables, in the final model, you will get some dummy variables as well. if you look at actual original variables you end-up using 10-12 vars.",300735.0
113216,489657.0,You can start with higher no. of variables and then reduce the number. Make sure you are able to achieve your end goal of precision 80%,310518.0
112919,487036.0,"This is my approach Imputing Nulls with value 'Select', create dummy variables and drop the _select column. This way you won't lose information.",310974.0
112919,487049.0,Even I think it's better to replace select with null values and then we can proceed dropping the columns if the NULL % is too high.,318804.0
112919,486982.0,"Quoting you, ""The reason for 'Select' value in column could be that customer has not selected any value or forgot to select values."" correct?? In EITHER CASE we DO NOT have the value. The DATA is missing. ie. it is EQUIVALENT TO a NULL value. Thus, we can safely replace them with NULL and drop columns having high percentage of nulls. Quoting you again, ""By this way we are losing information unnecessarily."" But, you already DONT HAVE information for that column. If a column has 70% null values (null+ select combined), then you as it is dont have much information from that column. It be best to drop them, in my opinion. Rest TAs can better advise. Hope that helps.",317998.0
112520,485961.0,"separate data in two sets, numeric and categorical. now do outlier treatment and dummy creation on respective dataset and then concactenate both on index.",318377.0
112520,485533.0,"For Data Preparation, along with treating NAs, we need to treat 'Select' also. And then Data Normalisation also to be done. The further steps can be : test-train split -&gt; PCA -&gt;Predict-&gt;analysing the model and test train split-&gt;checking for mutocollinearity-&gt;removing correlated columns -&gt; RFE -&gt; manual selection -&gt;predict -&gt;analysis of model",304319.0
112520,485681.0,"These are some of the steps, but you also need to treat all other 'bad data' - whether that is to drop the data or impute is up to your individual understanding. Other steps are correct, although I did standardization also better dropping outliers",300694.0
113217,488290.0,yes absolutely - as per the Accuracy/Sensitivity/Specificity curve - the higher we go above beyond around 0.55 the lower will be the accuracy. So you need to find a good middle ground after understanding what each metric represents for our case study and which ones are the most important.,300694.0
113219,488274.0,Please check the link. TA has explained here the difference between the questions https://learn.upgrad.com/v/course/208/question/112958,311254.0
113219,488286.0,I raised this qn already - please check my thread on this https://learn.upgrad.com/v/course/208/question/112958 I have asked for further clarification after TA answered my qn as it still was not clear,300694.0
113219,488304.0,"As far as I understood this, in Q1 they want the top 3 variables which are contributing the most towards the probability from the model. And in Q2 they want the top 3 categories that should be focused on. For example if I get occupation_student,Country_India and lead_source_google as the top 3 variables in my model the answer for Q2 should be occupation, country and lead source. This is what I think. It would be better if TA could confirm this.",302738.0
113221,488268.0,Precision has to be more than 80%.,311254.0
113221,488306.0,we need precision to be 80%. you can refer the following for TA varified answer: https://learn.upgrad.com/v/course/208/question/112947,302738.0
113221,488368.0,"precision is what is referring to is what we have been told - we are waiting for further clarification since morning. I don't believe Accuracy is the right metric nor has it been mentioned to be the right metric anywhere I have read so far. btw, I don't remember it saying that it has to be MORE than 80% anywhere .. I think it is suggesting AROUND 80%",300694.0
113221,494704.0,"The way to understand precison vs sensitivity is like this- Precision talks about how much of your predicted 1's have actually turned out to be 1s in reality. Sesitivity talks about how many of the actual 1's were predicted correctly as 1s by the model. In our case study, the aim was to come up with nurturing of leads so that 80% would turn out be paying customers. This means asking us to build a model where if we predict 100 customers would pay, atleast 80 of them should end up paying. This therefore is a measure of precision.",318079.0
113221,488392.0,"In the Q&amp;A session, he mentioned that depends on wahtever the metric u choose , accuracy or precision, for test data it should be 80% as mentioned. But most of the times he said the precision on test data.",300733.0
113223,488277.0,It is mentioned any column having more than 30 percent nulls can be dropped. Rest all you can impute.,311254.0
113223,488287.0,"Hello Sumit, If a column has many blanks &gt;30% you can remove but you can impute Country witht the followiing information. When check the city column and country column, you would find that most of the blank columns in Country has City column filled with Indian city or places in India Hence you can impute with 'India' on all those blanks in country column I suggest not to worry about so many dummy variables as many would get eliminated by RFE anyway.",301121.0
113231,488346.0,Try RFE with a lower number - say somewhere between 15 and 25.,318762.0
113231,488363.0,wow 69? what is the point of RFE then? Try somwhere between 10 and 15,300694.0
113231,488518.0,that is very large number of columns and RFE will not be able to crunch it for you. Reduce the size to about 20 columns to work with.,312199.0
113231,489613.0,Try with lowest no. of variables and try to achieve p &amp; VIF in the range and figure out precision of 80%. Then reiterate with higher no. of variables and then try to achive p/VIF/precision. You can stop when you see if it doesn't change much further. Those will be your optimal no. of variables for Regression with RFE.,310518.0
113234,488334.0,No that would defeat the whole purpose of understanding who is getting converted. You need both the categories of data to learn the pattern and predict.,310974.0
113234,488362.0,if you remove the unconverted leads then what will the model learn on? Everything will point to converted leads and there will be no variance,300694.0
113237,488350.0,This primarily deals with the cutoff values for sensitivity/specificity.,318762.0
113237,488410.0,"You generally maintain a balance of sensitivity and specificity in your model so that you dont have too many false positives or false negatives. In Q3 and Q4, some conditions have been changed. In Q3, you can afford to make high number of calls. So false negatives must be minimized(this relates to sensitivity). In Q4, you have to minimize number of calls. So false positives must be minimized(this relates to specificity). You need to therefore tweak the cutoff values to manipulate the sensitivity/specificity accordingly.",318762.0
113123,487781.0,we need precision to be 80%. you can refer the following for TA varified answer: https://learn.upgrad.com/v/course/208/question/112947,302738.0
113123,487791.0,"""target lead conversion rate to be around 80% TA's - Can you please give some example to understand this statement better ?",317991.0
113123,487831.0,"Hi Vipul, I think, we need to select optimal cut off point of the model in such a way conversion rate(i.e. percentage of leads that we predict as converted in the model) =80%. Whereas, precision =leads that we predict as converted / leads who got actually converted. I raised this question in the below link, https://learn.upgrad.com/v/course/208/session/24277/segment/124409",310467.0
113123,487935.0,"This is my interpretation Assume X education has 10 leads identified. In absence of any model, all 10 are predicted as Hot Leads and are nurtured equally. 3 of the 10 predicted get converted to paying customers. You Lead conversion is : Predicted Yes is actual Yes / total number of predicted Yes = 3/10 = 30% Now say for next month you build a model based on past data. Say next month you have 10 leads identified. Using some cutoff you predict 5 of them as Hot Leads. You nurture only those 5 leads and say 4 of them get converted. You Lead conversion is : Predicted Yes is actual Yes / total number of predicted Yes = 4/5 = 80% Now you need to find the optimal cutoff that would give you conversion rate of 80% Note that there might be cases where from the remaining 5 not nurtured leads, some get converted.",317514.0
113123,488946.0,"It is as below: You made 100 predictions as 'Yes' and out if that 100, 80 are actual 'Yes' and rest 20 are 'False Yes'. That is, you said them 'Yes' but in reality those were 'No'. Thanks!",304813.0
113123,489114.0,A balance between sensitivity and specificity will help here to get more conversion. High sensitivity will increase the right prediction of leads which can get converted so effort won't be wasted and High Specificity will ensure the leads which have low conversion are predicted correctly so these leads will not be touched anyway. senstivity= TP/(TP+FN) Specificity=TN/(TN+FP),300735.0
113249,488422.0,It can happen if the test data set has values beyond the range of train data set.,318762.0
113249,488431.0,I think there might be something that you are doing wrong because By general mathematics rules probability lie between 0-1 it cannot be less than 0 or exceed 1.,302738.0
113262,488488.0,You just need to assign a lead score to each of the leads and that is calculated as 100*Convert_Probability,310974.0
113262,488568.0,While doing this you must have all rows from train data as well as predicted data while applying the lead score as mentioned by Ram. 100*Convert_Probability,301121.0
113267,488513.0,refer below : https://stackoverflow.com/questions/53041669/error-perfectseparationerror-perfect-separation-detected-results-not-availab,318732.0
113267,488653.0,"This happens when all of the values in one of the predictor categories (or a combination of predictors) are associated with only one of the binary outcome values.i.e. either 0 or 1. When this happens a solution cannot be found for the predictor coefficient. For more details, please refer: https://stackoverflow.com/questions/34668868/unable-to-run-logistic-regression-due-to-perfect-separation-error",311117.0
113267,489204.0,Check the variables of x and y after applying test and train split.,305335.0
112525,485760.0,"We should drop the columns having very high percentage of NULL values, since dropping rows has to be done carefully or else it will lead to unneccessary loss of data.",318756.0
112525,485638.0,"I think you should drop the rows, if you drop the columns may be you will loss the data, which will effect on building the building model.",308639.0
112525,485963.0,before thinking of droping think if you can impute nulls with some values.,318377.0
112525,485684.0,It obviously makes sense to drop the columns .. For it is the columns that have a high proportion of missing data. Why would you drop the rows .. You would lose a lot of rich data if you drop the rows,300694.0
112525,485701.0,"TA verified Process for handling the columns and rows are mentioned here, you can refer the same: https://learn.upgrad.com/v/course/208/question/112439",311117.0
112525,485568.0,Drop the columns instead.Dropping rows would lead to unnecessary loss of data. It would make you lose half of the data.,305655.0
112525,485765.0,"If there are 45% null values in a column, it is better to drop the column instead of dropping the rows.",318448.0
112525,486298.0,"yes, please go droping with columns instede of rows",306242.0
112525,487073.0,"Drop Columns if null values more then 3000, as per TA..",300726.0
113275,488577.0,Thank you,300716.0
113275,488560.0,You have to get predicted probaility multiplied by 100 to get the lead score. You can do for all train data as well as predicted data.,301121.0
113293,488644.0,"Hey Sandesh, You can try &amp; increase columns to see if the accuracy score improves. Please note we need precision to be 80%. You can refer the following for TA verified answer: https://learn.upgrad.com/v/course/208/question/112947 Hope this helps.",302742.0
113293,488667.0,"Increasing number of variables for RFE will not help. By increasing you may increase the multicollinarity, It is advised to try by varying probability cut off to achieve required 'Precision' I hope this helps",301121.0
113293,488766.0,"Instead of providing RFE to limit of 15 columns, you can check for higher value. Check by fine tunning (dropping variables) you are able to get p values &lt;0.05 and VIF&lt;5 and still have higher accuracy. You have to decide the approach you want to take (a) accuracy, sensitivity and specificity and target for sensitivity closer to 0.8 without too much compromise on accuracy and specificity OR (b) accuracy, precision and recall and target for precision closer to 0.8 without too much compromise on accuracy and recall",317514.0
113293,488743.0,"Hi Sandesh, you can see from the Accuracy curve how changing the probability cut-off will change the accuracy. And yes ofcourse adding more columns can increase the accuracy - but it will most likely be a marginal increase (an additional 0.001-0.01 if you already have say 0.78 accuracy) - depending ofcourse on how many columns you chose for RFE initially (you said 15, so then i believe increasing to 20 will bring a very very small negligent accuracy improvement)). And yes multicollinarity could increase (but VFE will take care of that). It is not that hard to increase to 20 columns .. just takes an extra 2 minutes to check the impact. More importantly, have a look at the forums .. we are not aiming for 80% accuracy but 80% precision as already explained many many times in the forum.",300694.0
113293,489592.0,If you fine tune your p value (&lt;0.05) and VIF &lt; 5 with a fair no. of variables (don't consider too many variable. 20-25 variables sufficient) so that you wont over fit the model.,310518.0
113302,488752.0,Make sure your correlation matrix has only numerical columns and remove any categorical/non-numeric columns,318438.0
113302,488756.0,May be you have too many columns and displaying the heatmap in allocated space is getting challenged. You can try to display just the values(.corr) instead of plotting heatmap. Extract the high correlation values.,317514.0
113302,488757.0,"Check the data for the columns showing white lines in heatmap. Typically, it happens when there is very minimal data like one or two rows.",308435.0
113302,488762.0,"It may be because of ""NaN"" values (i.e. no Variation). And since "" NaN"" is not a number, it's plotted as white. Detailed discussion may be referred here: https://stackoverflow.com/questions/19639575/r-why-heatmap-shows-white-color-for-rows-with-equal-number",311117.0
113304,488991.0,How did you choose cutoff probability?,318329.0
113304,488740.0,"This implies that your probablity cutoff is selected correctly. This can be seen seen from graph where all 3 (accuracy, sensitivity and specificity) cutoff each other.",307495.0
113304,489233.0,Pick you cut-off based on the business objective (precision of near to 80%). Intersection of different metrics does not always yield the right result,318438.0
113308,488739.0,there are already atleast 2 posts dealing with that in the forum,300694.0
113308,488794.0,Other post - https://learn.upgrad.com/v/course/208/question/112958,307495.0
113321,488825.0,"You can replace NaN as ""Unknown"". While creating dummy variable, you can drop this level, since it does not provide any meaningful information to business.",310467.0
113321,489291.0,You can impute it with Others. RFE will itself take care of it,304319.0
113313,488787.0,This approach works only if you are trying only to understand which are the significant variables effecting the conversion but not trying to predict which ones are more likely to convert in the future.,310974.0
113313,489306.0,"You have to take into account both types of data. Where lead has converted and lead has not converted. There might be a value say occupation - working professional. Many of them having this value has converted. But there are some which have not converted. If we do not include them, then our LR model would predict 100% working professional would get converted which is not the case. The converts might depend on other factors too.",304319.0
113313,489214.0,"If you are working on the data set which has all leads conevrted and ignoring the other set where leads were not converting . Then you are doing / building the model on a specfic data . One should build a model with generalize data set , so that if data set chnage your model result will not change much.",311861.0
112527,485682.0,"Personally, if 50% missing data and no way to impute data with high precision then I would prefer to drop. Even 30% .. I would analyse the data and usually the outcome will be to drop",300694.0
112527,485958.0,if we drop the all nulls then dataset will become very samll. so scramble through columns and category in each column and find if you can impute some value for nulls.,318377.0
112527,485598.0,"30%- 50% of the data missing in any column is hard to impute as it might lead to exaggeration or overestimation.Imputing should be an educated guess.I would advice you to analyse the descriptive statistic of the numeric data,including the dummys and then make decisions about imputing.In my opinion 50% data missing is too high of a share to be imputed and might lead to exaggerated outcomes.",305655.0
112527,485635.0,You can go through this webinar video for more clarilty on approaches to hande missing values. https://thecraftofstatisticalanalysis.com/video-approaches-missing-data/,318479.0
112527,485696.0,"TA verified Process for handling the columns and rows are mentioned here, you can refer the same: https://learn.upgrad.com/v/course/208/question/112439",311117.0
113326,489053.0,Yes. You can use churn example for your reference.,319319.0
113326,489012.0,"I am not sure if this is how we should do it but what I did was, I merged the test and train data back at the end and then assigned the lead score.",302738.0
113326,488977.0,Yes.,304813.0
113326,488941.0,What other code do you need to write? predicted probabilities * 100 is the lead score right?,318329.0
113326,488914.0,you can write the code for that in python file and can mention it,318017.0
113326,489653.0,"I have the same question earlier. It is similar to giving a weightage to each variables which we have found thru RFE, Regression with p&lt; 0.05 &amp; VIF &lt; 5. We can calculate overall score of a Lead with adding weightage of each variables. But that will be same as predication factor identified with optimal cutoff selected (precision = 80%). Hope this helps!",310518.0
113326,489704.0,"Yes, we have to write code for lead score.",314048.0
113355,489176.0,"Iterate until the P-scores have falled within range, and there aren't any obvious multicollinearity (using VIF)",318438.0
113355,489058.0,There is nothing like optimal number. I would suggest somewhete between 15 -20 would give you a fairly good model.,301121.0
113355,489229.0,It's not a hit and trial technique required. Around 15 numbers of feature give a good model.,311117.0
113354,489061.0,Minimum Optimal probabilty cut off will be provided by ROC curve. Later you can tweak it upwards to arrive at at a required precision percentage.,301121.0
113354,489153.0,"The cut-off point of ""accuracy, sensitivity and specificity"" gives the optimal point of ""Cut-off probability"". Further, we can escalate it for the required precision percentage.",311117.0
113354,489219.0,Any of the above metric should be considered based on the business objective. The instersection of any of these is not necessarily the ideal point. My business need may be to give higher importance to Precision than recall/sensitivity. In such case you will have a higher cut-off.,318438.0
113359,489244.0,Somewhere around 15 gives a good model. You can refer the discussion here: https://learn.upgrad.com/v/course/208/question/113355,311117.0
113359,489175.0,"For any modeling exercise, it's always better to reduce the number of variables. More advanced algorithms will penalize use of more variables",318438.0
113359,489256.0,"I think you must start with RFE=25. Try various models, dropping cols on the basis of Vifs and P value&gt;0.5. This will give a optimal model.",304319.0
113359,489054.0,"While using RFE, I would suggest to go with variables 10,15 ,20 and go ahead with process of elimination on probablity as well as VIF. More than 20 will make it very laborious and it will not make considerable improvement later in the model.",301121.0
113359,489601.0,If you can achieve 80% precision and within limits of p&lt;0.05 &amp; VIF&lt;5 with 25 variables. you can consider the model is good enough.,310518.0
113311,488754.0,"As per the given problem statement in assignment, select is as good as null value. It’s better to replace it with NaN. Select signifies that the user did not enter any data for those columns. Hope this clarifies.",308435.0
113311,488771.0,"Yah I understand, but on dropping the columns which have select and null values.... are we significantly lossing the information of those who are potential leads. Almost 40% of 'select' are 'converted'. We have some columns in which select and null values together give the significant information on potential leads. I have tried imputing them but my doubt is if impute am I deliberately fitting the data?",318013.0
113311,488803.0,"You have to drop Lead Profile as you don't get any clue. If you impute with Select, you tend to agree that having no clue is the way to get converted which will mislead the model.",301121.0
113311,489619.0,"Imputing of variables depends on the datatype &amp; profiling results. For Example, Lead Source don't have Select. So, you can choose new category like 'unknown'. For Total Visits, Used median of Total Visits for imputing. For Country, use India as 95% population having data has India as Country. It all depends on the datatype &amp; profiling results.",310518.0
112519,485699.0,I think for this assignment first we need to reduce independent variables using RFE and then use manual tunning,317156.0
112519,485685.0,PCA is not the best approach for this assignment ..,300694.0
112519,485547.0,https://stackoverflow.com/questions/40795141/pca-for-categorical-features This answer discourages using PCA for categorical variables.,301652.0
112519,485562.0,"We can apply the PCA algorithm on categorical variable once we transform this variable in proper format. for example transform this variable by applying one-hot-encoding before applying PCA. For more, this link can be followed: https://discuss.analyticsvidhya.com/t/dimension-reduction-technique-for-categorical-variable/1713 https://www.researchgate.net/publication/286927869_Dimensionality_Reduction_of_Categorical_Data_Comparison_of_HCA_and_CATPCA_Approaches",311117.0
112519,485571.0,"PCA is mainly designed for continuous data. For categoircal data, there are libraries available in Python to perform categorical PCA (CATPCA)",316147.0
112519,485628.0,"Short Answer: PCA is intended to be used for categorical columns only. Long Answer: While you can use PCA on binary data (e.g. one-hot encoded data) that does not mean it is a good thing, or it will work very well. PCA is desinged for continuous variables. It tries to minimize variance (=squared deviations). The concept of squared deviations breaks down when you have binary variables. So yes, you can use PCA. And yes, you get an output. It even is a least-squared output - it's not as if PCA would segfault on such data. It works, but it is just much less meaningful than you'd want it to be; and supposedly less meaningful than e.g. frequent pattern mining.",317987.0
112519,485670.0,"As per the TA, RFE is to be used for feature selection/extraction",308435.0
112519,485900.0,"I do'nt think PCA can be used for categorical data. Also, the values in categorical data has more than two levels which will not give a reliable result with PCA.",301643.0
112519,486633.0,"Try PCA it works and with PCs able to explain more than 85% of variance. RFE is taking a lot of time for solving it. As far as categorical variables are concerned , suggestion would be to bin them and convert those to dummy",313767.0
112519,486474.0,We should use RFE for categorical data. PCA works better with numerical data.,317689.0
113402,489267.0,you can check the following link for TA verified answer: https://learn.upgrad.com/v/course/208/question/112958,302738.0
113474,489719.0,thanks all ..the issue was related wit theh conversion of categorical variable.,310504.0
113474,489575.0,Prospective ID and Lead Number are not required for Regression. You need to remove them in EDA. Please try and check regression.,310518.0
113474,489477.0,"Did you drop Prospective ID and Lead number? If not, Please do try after that.",301121.0
113474,489421.0,Just check your data frame doesn't have string in it. only numerical data needs to be passed into regression model building.,311466.0
113495,489579.0,"There should not be much difference between the values of the various metrics for the train and test datasets. They can be a little off , but the difference is very high in your case which indicate at sub-optimal model.",313826.0
113495,489665.0,The difference between accuracy of train and test data in this case is quite high. Generally it should not come like this. The reason might be that the model you created is not stable one and might contain insignificant variables. Ideally it should have been close or relatively closer to each other.,317991.0
113495,489980.0,"Yes , such difference for train and test indicates an issue or the model is not stable enough.",306009.0
113495,489669.0,it is very unlikely to have such a big difference in accuracy of train vs test set. surely there must be some issue. have a detailed relook. we also faced this and then observed we missed scaling the test set. such silly mistakes can happen. also there is no 'allowed difference' as such. it's all subjective and you can proceed ahead with whatever model you could build.,311686.0
113502,489624.0,It is not necessary to use PCA at all. It is basically logistics regression. You use stats model and RFE.,301121.0
113515,489773.0,There are atleast 35 main variables and you should be able to find definitely more than three. Hope you are not looking at p-value and looking at coefficients of these variables,317514.0
113515,489820.0,Please find below TA verified answer which will clarify your doubts on this. https://learn.upgrad.com/v/course/208/question/112958,301121.0
112433,485133.0,"Hi, It will depend on your understanding of the case study. Hint. - You need to check Precision, Recall, AUC and then based on the optimal value of probability, you need to decide the probability threshold for 0 and 1.",344894.0
112433,485706.0,You have to decide benchmark probability based on which -0 and 1 and derived. As per lecture generally it should not 0.5.,317156.0
112697,486229.0,"Hey Ravindra, In my perspective there are some other columns which carry pretty much more importance than this tags columns, some of them can be Asymmetrique Index Score, etc So, I would suggest to checkout and see if you can drop that column or you can impute the values in those.",301655.0
112697,486316.0,"Hi Ravindra, Tags is explained in data dictionary, as: ""Tags assigned to customers indicating the current status of the lead"". Yes, if you see the current status in CSV file, it looks like it would be an important factor for conversion of a lead. So we will have to take care of this column.",311117.0
112697,486367.0,"Hi, Drop the columns having greater than 3000 missing values as they are of no use to us.",344894.0
112697,486584.0,"30% of this column has null, If you go through the contents of the fields some of the contents may not be indicative of anything (as per below). Its better to drop the same Interested in other courses', 'Ringing', 'Will revert after reading the email', nan, 'Lost to EINS', 'In confusion whether part time or DLP', 'Busy', 'switched off', 'in touch with EINS', 'Already a student', 'Diploma holder (Not Eligible)', 'Graduation in progress', 'Closed by Horizzon', 'number not provided', 'opp hangup', 'Not doing further education', 'invalid number', 'wrong number given', 'Interested in full time MBA', 'Still Thinking', 'Lost to Others', 'Shall take in the next coming month', 'Lateral student', 'Interested in Next batch', 'Recognition issue (DEC approval)', 'Want to take admission but has financial problems', 'University not recognized'",310629.0
112697,488204.0,The Reply Helped me a lot Thanks,300734.0
112837,486717.0,"Use fillna() function ( https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html ) leads['Lead Source'].fillna('Click2call',inplace=True)",313826.0
112837,486705.0,You can use isnull() instead of isnan(). You can also visit below link for detailed explanation for the same type of error. https://stackoverflow.com/questions/36000993/numpy-isnan-fails-on-an-array-of-floats-from-pandas-dataframe-apply Hope this will help,317991.0
112837,486757.0,just use fillna() - it will impute whatever value you want into the nan cells,300694.0
112932,487029.0,i m not sure if this is the right approach or not but i replaced the null vaules in the column 'What is your current occupation' by 'Other'. as it seems an important column and dropping it would not be ideal.,302738.0
112932,487030.0,"Well, impute is what I would do since the percentage of null columns isn't huge.",310974.0
112932,487031.0,"As per TAs verified answer, any column with more than 3000 null values should be dropped since it would not be helpful in our analysis. hope that helps.",317998.0
112932,487044.0,Refer to this: https://learn.upgrad.com/v/course/208/question/112889,318448.0
112981,487266.0,"Yes, both need to be calculated. Lead score = 100*(probability of converted)",308435.0
112981,487255.0,"Yes, both of them have to done. first the prediction wheather the lead will be converted or not. second, a leadscore on that lead based on the probability value. hope that helps.",317998.0
112981,487974.0,Both should be calulcated . But also . you need to consider the VIF values where it will define which attribute has lowest VIF which is the way of communcations should be,313526.0
113121,487775.0,"Outlier treatment is required as otherwiese RFE would falter. In fact, I could find 2 of the three numerical columns requiring outlier treatment. If you refer to the RFM code, you will able to see how outliers are found and eliminated. In fact, it is a good idea to do boxplot of that variable before and after outlier treatment which will give you a visualization of elimination of outliers. I hope this clarifies",301121.0
113121,487877.0,Yes outlier treatment is necessary to give a better model,319876.0
113121,487876.0,yes it has beem mentioned several times in the forum that outlier treatment is required,300694.0
113121,487824.0,"You can either impute the outliers or cap it . In this case study, I think it would be better if we cap it. Please find below the link which explains the capping method. https://stackoverflow.com/questions/42207920/how-to-take-floor-and-capping-for-removing-outliers",310467.0
113215,488252.0,It depends on the conversion probability you are getting. Highest probability should be assigned with the score 100 and so on till the score 0. So basically you’ll assign the lead score to top 101 conversion probabilities.,302738.0
113215,488253.0,Lead score is as follows: Lead Score = 100*(probability of Converted) This measure is provided by TA in below link: https://learn.upgrad.com/v/course/208/question/111457 Hope this will help.,317991.0
113215,488292.0,just multiply the probability converted by 100,300694.0
113292,488669.0,Hello Vikas It is alright. When you see would have seen e raised to very high value . 10^4. If you observe. in the same row you would find the probability close to high value of 0.99. You need to drop those columns and proceed. Hope this helps.,301121.0
113292,488746.0,"did you perform the step first where you check the correlation between variables? several variables are highly correlated so you can do a correlation matrix and/or heat map and drop some variables before creating the model - then you will not get such high coef. Alternatively, I suppose you can run the model with highly correlated variables and just drop them iteratively.",300694.0
113292,488885.0,"if p value is almost zero then your model is good, u can also check the VIF for any high values. before building the model make sure you check for correlation And perform standardisation.",306735.0
113394,489231.0,You need to do for Train/ Test Data . Also refer below existing post on this topics https://learn.upgrad.com/v/course/208/question/113275,311861.0
113224,488332.0,"You should not choose the cutoff to obtain the target precision rather you should chose to keep those variables in the model that will eventually lead to a cutoff suggested by the plot that gives you 80% precision. Besides, 78% is good and is close to 80%. The rubric says the precision should be around 80% not a minimum of 80%.",310974.0
113224,488366.0,the intersection of the three will not always be the best place to choose your cut-off. Say accuracy is the most important aspect then you would choose a value accordingly and not necessarily at the intersection. So know your business requirements and hence which metrics are the most important to you ..,300694.0
113335,488939.0,"In that example, first an arbitrary value of 0.5 was chosen to explain about the metrics. Then the cutoff was chosen based on sensitivity, accuracy and specificity. Then the cutoff was chosen based on precision, recall. Which method to choose depends on business context and goal to be achieved. And, the cutoff has to be same for both train and test, otherwise the metrics will vary and you would not know if the model performed better/worse or is it because of the cutoff.",318329.0
113335,488912.0,firstly cut off is set to .5 later on based on the findings and roc curve we can change the cut off and once changed for test and train the cut off should be same.,318017.0
113343,488972.0,"It shoud perform well in test test. If it is performing at 100 % in train data and it falls very badly in test set then it very likely that it is overfitted. There are other ways to handle this in very complicated scenarios, like hold set this is again very much like tes set just to check on evaluation you hiding some amount of data aprt from test data. There also exists K-fold validation where you keep on shuffling the test data from the maim set and keep on checking the performancr in test set. Thank you",304813.0
113343,489220.0,"Check the accuracy , AUC and PR score on train and test data. If they are similar, it's fine. If there is a considerable difference, it means there is overfitting",318438.0
113164,487968.0,"Actually it does not matter. Whatevet unit it could be, you use it as a numberical variable. RFE will take care of it.",301121.0
113164,488014.0,does it matter? as we are not comparing it to anything else and furthermore we are standardising it .. I don't think matters,300694.0
113164,489610.0,It doesn't matter unless until we have normalization on those variables.,310518.0
113164,488063.0,"It does not matter the units used. Whatsoever the unit may be, it is common for all. Please refer the TA verified discussion here: https://learn.upgrad.com/v/course/208/question/112618",311117.0
113509,489737.0,You have to change the cut-off probability - increase/decrease and rerun to check the precision. You will find the right path to final result.,301121.0
113509,489778.0,"You can change the cut-off to alter the precision. Take care that you are manging the balance between precision, accuracy and recall. You will have to compromise a bit to achieve the desired target level.",317514.0
112778,486564.0,"If we are not sure about how these values can affect, we can leave it to PCA/RFE to take care. We must remove only those columns for which we are sure that they will not have any significance in the analysis or having too many null values.",304319.0
112778,486499.0,"Such cases can be handled using RFE. Lead Source is an important variable from business perspective. If we create dummy variables for this, then RFE can eliminate excess ones based on calculations. Please also follow the attached link https://learn.upgrad.com/v/course/208/question/112582 , you can try this solution as well.",308435.0
112778,486569.0,"If you look at the TA approved response to the question (https://learn.upgrad.com/v/course/208/question/112582) it indicates that if the remaining attributes caters to only 5% of the total data set then better we categorize into one variable rather than keeping multiple values contributing less percentages. I also agree Lead Source is important variable, but do we need to realy carry 15 variables contributing less than 5%? We have lot of such variables and the question was limited to ust this variiable.",317514.0
113318,488911.0,"Hi Nagaraju, For 80% precision, you need to do experiments and inc/dec cutoff so that precision came out to be 80%. In my case I increased the cutoff range.",318429.0
113318,489070.0,ROC curve will provide you with optimal probablity and you may need to tweak updwards or downwards to reach required prcision percentage.,301121.0
112544,485955.0,"Prospect ID and Lead number should be droped, it is just an ID so drop",318377.0
112544,485672.0,"There is unique Lead Number for each Prospect ID. You can identify Prospect Id hence customer based on Lead Number, hence would delete Propect ID for analysis.",317514.0
112544,485679.0,"This is a predictive exercise, not clustering, so you can drop prospect id ..",300694.0
112544,485768.0,"I think it is a unique Guid, so it can be dropped.",318448.0
112544,485785.0,"Since its a unique id for identification of the leads, so it can be dropped.",313691.0
112544,485845.0,I suppose its a unique ID hence can be dropped..,304697.0
112544,486111.0,You may check for existance of duplicate values for Prospect ID and Lead Number and drop these columns after that.,318078.0
112544,486659.0,"Both are unique and seeing the data it is 1:1 mapping. Lead and Prospect, both means same -&gt; potential customer. You may drop both the columns for analysis. Any one of that column, will be useful incase we want to know, for this prospect/lead id what is the predicted value and what is the actual value. But for logistic regression process those columns would not be required.",318554.0
112544,486127.0,I think Prospect ID and Lead Numbers Should be dropped becasue it has no use of analysis.Simply you can drop them.,308639.0
112544,486175.0,Each prospect id represents a customer. It can be dropped while performing predictive analysis,316147.0
112544,486340.0,"Prospect Id can be dropped. Finally we may need Lead Number , to give Lead Score for each Lead Number.",312019.0
112544,486383.0,Yes! You can drop Prospect ID as we are performing predictive analysis,311004.0
114309,492512.0,"Such a deep level cleaning could be difficult. Please check the corresponding 'city' data, wherever the country cells are blank. If it gives a clue you can impute blank cells of the country variable and drop the City. If country is USA/ Canada and city is Mumbai than remove the rows. It is a good to use the ""value_counts"" to get the counts of different values in a categorical variables, which will give you a fair idea for the purpose of imputing.",301121.0
113171,487999.0,"There are 2 ways by which you can treat columns having few NaN values: 1. Either drop all rows that have null for that particular column. this would lead to a loss of information, but since the percentage of null values is small, the loss would be small and thus acceptable. 2. you can impute the missing values with using a statistical metric such as mean median or mode. hope that helps.",317998.0
113171,488005.0,"This has been discussed already in the forum - you have the two options avail to you: * impute values if you feel you can impute meaningful values * drop the column if you feel there are too many null values (&gt;30%) You can also drop rows but you would, in my opinion, lose a lot of useful info in the other columns that do have data in the rows you are dropping",300694.0
113315,488805.0,"Hey Aditya, Maybe the link mentioned below can help you out to some extent: https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas/41453817",301655.0
113315,488786.0,"I checked the documentation, there seems to be no option like that https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html.",310974.0
113315,488868.0,"Take correlation value in and parse it row by row check if it's greater than .80 if yes, take take the combo in to accountability else ignore. I'm not sure wheater we have any inbuilt feature for this in python.",306735.0
113315,493333.0,"You could do something like these: corr = df.corr() sns.heatmap(corr[ (corr &gt;= 0.8) | (corr &lt;= -0.8) ], annot=True)",304815.0
112576,485801.0,"The same has been replied here, and verified by TA. https://learn.upgrad.com/v/course/208/question/112525 Hope it will help. If you have any further query, can be discussed.",311117.0
112576,485770.0,we can impute or drop the columns based on business logic. you can refer the following links for TA varified answers: https://learn.upgrad.com/v/course/208/question/112439 https://learn.upgrad.com/v/course/208/question/112525,302738.0
112576,485783.0,"When there is a large number of null values in a column(in this case more than 45 %), its ideal to drop the columns even though its useful for analysis. Also refer to this TA verified answer: https://learn.upgrad.com/v/course/208/question/112439",313691.0
112582,485811.0,you should try to keep as much data possible with the data explaning most of your findings,318017.0
112582,485824.0,"1. If you are trying to derive new metrics like categorising country as india and outside india, provide a reason as to how it will support the business problem and then you can go forward categorsing the counrty column. 2. Last activity and Last notable activity are two columns containing different data. Last activity is related to the customer whereas last notable activity is related to the student. So drop only those columns not relevant to solve the business problem.",313691.0
112582,485841.0,"'Last Activity' v/s 'Last Notable Activity' - Data Dictionary says last activity performed by the customer and last notable activity performed by Student. Isn't the customer, a student and vice versa? Even if you look at the data for these 2 columns, it does not help much. e.g. Last Activity - 'SMS Sent' Last Notable Activity - 'SMS Sent', 'Modified' Need more clarification on this.",318438.0
112582,485908.0,"1.Can we categorise country as india and outside india? Yes, you can if the 'Outside India' caters to only 5% of the total data set then better we categorize into one variable rather than keeping multiple values contributing less percentages",312093.0
112582,486009.0,"Collinearity check will be fruitful option. If they are highly collinear, then drop one of them, else keep both.",311117.0
112582,486179.0,Last Activity &amp; Last notable activity - both columns have same data -it seems to be duplicate. Though can check multicolinearity before dropping one,316147.0
112582,486255.0,"Last Activity vs last notable activity ____ student vs customer, not all customers are converted as students, there may be few who may turn as students in near future like who is waiting to finish graduation or someone waiting to join next batch of the course, let it be , let the model decide which column required during RFE",301115.0
111457,480840.0,"Hi, A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted. So basically you need to predict the probability of Converted. Lead Score = 100*(probability of Converted)",344894.0
111457,484981.0,"After Dummy variables creation and cleanup of data we have left with 227 columns, which is very difficult to run the GLM models to find out corelation matrix to drop highly corelated variables.However this very difficult to plot corelation matrix plot for this huge number of variables. This limits us from identifying highly corelated variables and eliminate them using RFE. The other only option left is to perform PCA to find the number of top variables whcih can be used to create a logestic regression models. Please advise can we perform PCA before logstic regression? or is there any other alternative approach to reduce the high number of features ?",316215.0
112626,486087.0,"Hey Ram, I think this column can act as a crucial player in deciding the lead score for each of the leads. So, I would suggest not to drop it and keep it.",301655.0
112626,486088.0,In web analytics page views per visit is the ratio of total pages viewed by total visits. For example if a user visits only the home page of a website 3 times then page views per visit will be calculated as 1/3 = 0.33. If the user views 3 page on a single visit then page views per visit will be 3.0 . Hope this clarifies...,301644.0
112626,486310.0,"Hi, Clearly the levels Lead Profile have a lot of rows which have the Nan , no use to the analysis so it's best that we drop them.",344894.0
112626,486551.0,"Yes. I agree with Anuj. This column can be an important feature so don't drop it. Infact, scaling would be required.",304319.0
112626,486749.0,"yes i agree with above views as this indicates user's intent for site. More the value, more user wants to know about this site",317156.0
113310,488753.0,"This happens when all of the values in one of the predictor categories (or a combination of predictors) are associated with only one of the binary outcome values.i.e. either 0 or 1. Can refer the same discussion: https://learn.upgrad.com/v/course/208/question/113267 https://learn.upgrad.com/v/course/208/question/113263 For more details, please refer: https://stackoverflow.com/questions/34668868/unable-to-run-logistic-regression-due-to-perfect-separation-error It may be helpful for you.",311117.0
112665,486099.0,I think this should not be dropped because still there are enough values other than which which can provide significant evidence in successful conversion of lead.,311729.0
112665,486125.0,I think dropping is the better option rather than performing data manipulation like imputing the values.,313691.0
112665,486202.0,Lead Profile is a very important column for analysis. Try finding other way of dealing Nulls and Select values.,310974.0
112665,486312.0,"Hi, Clearly the levels Lead Profile have a lot of rows which have the Nan , no use to the analysis so it's best that we drop them.",344894.0
112665,486380.0,I think we can consider them as 'other leads'. Correct me if i am wrong,318352.0
112663,486085.0,"Hey Lokesh, Yes, it can be logical at times. But at the end it depends on how your model acts and there should be no multicollinearity in it.",301655.0
112663,486095.0,"Hi Lokesh, I would recommend to impute the value as ""Unknown"" rather than ""Others"" because in a real scenario logically there can be values among the other options available.However, irrespective of whatever name you give, a dummy variable will be created. There is as such no need to drop this variable as it will automatically not be picked up by the RFE as significant. It won't appear in your model. By adding this value, you are just keeping provisioning space for the rows to be considered for the model. The model preparation is somewhat in your own hands and the charces are quiet sure that you will be able to drop this column from the model so created. Tiil the time the situation doesn't arrive, you can proceed with your approach confidently.",311729.0
112675,486116.0,"As per my understanding , select means user has not selected any options under that variable, so it can be taken as blank",318732.0
112675,486120.0,"A hint is given in the problem statement regarding the 'select' which can be treated as a null value. Select is present when the user has not provided any information for that variable. As given in the problem statement: "" Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value (think why?) "". Refer to this link: https://learn.upgrad.com/v/course/208/session/24277/segment/124409",313691.0
112675,486634.0,"Select is basically a null value, which is not fed by respondent. So this need to be treated like a null value only.",313767.0
112678,486119.0,"I too have thoughts , as select is drop-down having its value but user hasn't selected anything . so it can taken has null logically",318732.0
112678,486123.0,"A hint is given in the problem statement regarding the value 'select' which can be treated as a null value. Select is present when the user has not provided any information for that variable. As given in the problem statement: "" Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value (think why?) "". Refer to this link: https://learn.upgrad.com/v/course/208/session/24277/segment/124409",313691.0
112678,486198.0,"This is my approach Convert NaNs to select, create dummy variables and drop the select dummy variable.",310974.0
112676,486115.0,"Columns with a single values like 'Magazine' , 'Receive More Updates About Our Courses' which contains only 'No' can be dropped. Also columns with very low variability can also be dropped. But the threshold that decides the variability of a column is subjective.",313691.0
112676,486118.0,"few variables values which has only single value can be dropped as it didn't add any value for analysis . And can't drop variables only because of which has less variables, we should consider it's significance too",318732.0
112676,486322.0,"Yes. We have 5 columns (i.e. Magazene, Recieve more updates, Update me on supply chain, Get update on DM, Agree to pay amount) in our data set, where it contains ""NO"", which don't have any significane to keep. Hence, it could be dropped.",311117.0
112743,486565.0,Proceed with both of them. RFE/PCA will take care of them.,304319.0
112743,486537.0,both are different.. modified means that the last activity which he did is now changed.. it is kind of value like we have select in our cases..,317982.0
112743,486632.0,https://learn.upgrad.com/v/course/208/question/112582,318451.0
112743,486970.0,with data set and dummy variables of 'Last Activity' and 'Last Notable Activity'. too many variables. To be removed some variables here before RFE or PCA ?,312019.0
112930,487019.0,"When you have dummy variables, you better use Min-Max scaling as it converts all the variables irrespective of One-hot encoded varibales in between the range 0 to 1. When you have just 0's and 1's as your variable values, it would retain its values.",311160.0
112930,487028.0,"You can apply scaling only to those variables which are not dummy, because dummy variables will already have value of 0 or 1. I would suggest you to visit exmple of Telecom+Churn+Case+Study. Hope this will help.",317991.0
112930,487035.0,"i dont think we need any scaling for dummy variables. although, you can use standard scaling or min-max scaling for 'Total Time Spent on Website' and 'TotalVisits'.",302738.0
112930,487119.0,"there is no need to apply scaling to dummy variables as the values are only 0 and 1. you apply standardisation to other continuous variables to get them to be in a similar range to 0and 1 if you try and standardise them, then they will change. but the point is that you shouldn't.",300694.0
112930,487354.0,Only continuous variables should be scaled not dummy variables.In case of telecom churn example only continuous variables are scaled,317156.0
113294,488666.0,"You have to get these information after RFE Q1: You will have to select top 3 variables (from the original set) which plays the best role for Conversion, Q2. You will have to provide top 3 Dummy/Categorical variable which plays the best role for conversion.",301121.0
113294,488759.0,Q2: Top 3 Subcatgegories or Dummy variables Q1: Group the categories based of positive coefficient and find the top 3 categories/continous variables .,317514.0
113294,488680.0,"As far as I understood this, in Q1 they want the top 3 variables which are contributing the most towards the probability from the model. And in Q2 they want the top 3 dummy variables that should be focused on. For example if I get occupation_student,Country_India and lead_source_google as the top 3 dummy variables in my model the answer for Q1 should be occupation, country and lead source. you can also check the following link for TA varified answer: https://learn.upgrad.com/v/course/208/question/112958",302738.0
113554,490008.0,"This is an interesting way of imputing data, rather than just imputing it with mean, median or mode. It would be difficult to expain the algorithm here, but you can refer to the following link for a detailed insight on how it used (with an example) : https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637 I also read it and found it to be very interesting. Hope that helps and thank you for putting up the question. Learnt a new method of imputing :)",317998.0
112944,487056.0,it is not necessary for you to drop variables after heat map. you can directly proceed to perform RFE. it will take care of the multicollinearity.,302738.0
112944,487057.0,"We can use Heatmap to check if any variables can be visually identified for high correlation and drop them accordingly. in the Telecom Churn case study given in the module the professor did use this. however, directly RFE could also be used since RFE would not select the correlated variable as it as. hope that helps.",317998.0
112944,487120.0,"rather than a heatmap, you can output and sort the correlations as a list with three columms. much easier to read and even action automatically via code",300694.0
113158,487954.0,Try scaling after resetting the index. df.reset_index(drop=True),310467.0
113158,487957.0,"""The error comes up when you are trying to assign a list of numpy array of different length to a data frame"" "" Work around Solution (use with caution): convert the list/array to a pandas Series, and then when you do assignment, missing index in the Series will be filled with NaN "" Reference for above quotes: https://stackoverflow.com/questions/42382263/valueerror-length-of-values-does-not-match-length-of-index-pandas-dataframe-u hope that helps.",317998.0
112537,485700.0,"TA verified Process for handling the columns and rows are mentioned here, you can refer the same: https://learn.upgrad.com/v/course/208/question/112525 https://learn.upgrad.com/v/course/208/question/112439",311117.0
112537,485683.0,"Shouldn't we, as students, and based on our learnings know how to approach the analysis? I don't think it's fair to other students if TA is expected to just spoonfeed student the answers and/or ENTIRE approach. We have been given more than enough education in the course to know the approach.",300694.0
112537,485633.0,you can refer these urls https://learn.upgrad.com/v/course/208/question/112439 https://learn.upgrad.com/v/course/208/question/111457 https://learn.upgrad.com/v/course/208/question/112433,318479.0
114252,492347.0,"Hi Lovish, Some correction ! Specialization How did you hear about X Education What is your current occupation What matters most to you in choosing a course Tags Lead Quality Lead Profile City Asymmetrique Activity Index Asymmetrique Profile Inde The above columns can be dropped but country can be imputed. I counted 'Select' as null values.",301121.0
114252,492319.0,"I would suggest you refer to the link TA approved. https://learn.upgrad.com/v/course/208/question/112994/answer/487349/comment/115812 To be slighly more specific, you can impute the columns like How did you hear about X Education, What is your current occupation, Specialization, What matters most to you in choosing a course, as they may have some important information, You can impute country too, if you do slight comparison with what is mentioined in column 'City' wherever 'country' is blank. You will find imputing quite interesting espeically when you identify the logic behind that.",301121.0
114302,492509.0,"Hello Lovish If any variablecontains the same data for 99.5%, of rows, then they can be dropped since they will have not bearing with dependendent variable.",301121.0
114302,492549.0,they can ne dropped as we know we should consider variable where variance is more.,301648.0
114302,493195.0,If you are only considering the above variables then it is advisable to drop these variables as they are of no significant use.,308439.0
113225,488342.0,"From the error, It appears that your ""leads"" is not a Dataframe and instead is a float type. and you are trying to access a subscript ""Converted"" of this float object. You can refer this link for more insight on the error: https://stackoverflow.com/questions/19991591/typeerror-float-object-is-not-subscriptable My suggestion would be check the datatype of ""leads"" and see if it is a dataframe or not. Hope that helps.",317998.0
121784,529229.0,i faced the same.. you need to download the graphviz zip file and after unzipping put it in your C drive Program files.. follow the steps that are mentioned in the upgrad page below the video.. that helps..,316349.0
121784,529261.0,https://stackoverflow.com/questions/33433274/anaconda-graphviz-cant-import-after-installation This solution worked for me...it suggests installing python-graphviz....,318479.0
120700,526185.0,"Yes Ram, You are correct. If you increase the number of features, it will become computational high. Thanks",344894.0
122781,534759.0,"Hi, # plotting tree with max_depth=3 dot_data = StringIO() # Define the String output file export_graphviz(dt_default, out_file=dot_data, feature_names=features, filled=True,rounded=True) # this function will save tree into dot_data # dt_default is your tree model graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) # plot the graph graph.write_pdf(""dt_heartdisease.pdf"") # save the pdf",344894.0
122781,536159.0,"Hi Manish, Make sure you have your installations of the packages done.If the required softwares are in place,just donwload the python code which is there and run it.It will automatically generate a pdf which you can use for your questions. Regards Somnath",314617.0
122781,546024.0,"Hi Manish, we are in same group for case study... my contact number is 8368463915. Please connect with me.",317990.0
122786,536552.0,"Have you had any success yet? I had the same problem as you but got it fixed and these are the steps I followed: Download the package from the link they provided: unzipped it onto my desktop at the graphviz level (so inside this folder is another folder called release and inside that is everything else). I moved this folder into the C:\Program Files (x86). Then I added C:\Program Files (x86)\graphviz-2.38\release\bin to the User Variables section (click EDIT after highlighting ""Path"" and then NEW to add the path they have provided) and then add C:\Program Files (x86)\graphviz-2.38\release\bin\dot.exe to the System Variables section (click EDIT after highlighting ""Path"" and then NEW to add the path they have provided). This is where I got stuck and confused so just did both: I opened up Anaconda Navigator, installed pydotplus and graphviz. Then I went to Jupyter Notebook, clicked NEW on the top-right and clicked TERMINAL as shown in the image below: Once there, all I did was type: pip install graphviz (press enter button) pip install pydotplus (press enter button) And now I can run it all.",315022.0
122786,534158.0,upgrade your pip,318017.0
122786,534194.0,Checkout below link for similar query: https://learn.upgrad.com/v/course/208/question/121248,313826.0
122394,532461.0,"In original notebook ""import os os.environ[""PATH""] += os.pathsep + 'C:/Program Files (x86)/graphviz-2.38/release/bin/' "" is commented. To successful run the code for plotting tree with max_depth=3, you need to remove comment from the above code as shown in figure below. Hope this will help.",317991.0
122394,533036.0,"Hi, Please run following commend in your jupyter notebook !conda install python-graphviz Please comment here if you are still getting error",344894.0
122394,532874.0,Coded as per instructed. Still getting the same error. Please Help,314313.0
122394,536594.0,did this error resolve for you?,317981.0
126103,549604.0,"It is NOT necessary to build a Random Forest Model as well. We basically need to build two models. 1. with PCA and something else to predict 2. without PCA -&gt; logistic regression or one of the tree models to find most influential features You can refer to the following TA verified answers for further insights https://learn.upgrad.com/v/course/208/question/125841 https://learn.upgrad.com/v/course/208/question/126003 So, Yes you can make predictions through LR and feature identification through DT. Hope that helps.",317998.0
115929,501427.0,"Install these packages by following the below steps: 1. After launching the Jypyter Notebooks, open a terminal by clicking on the ""New"" dropdown and selecting ""Terminal"" as shown below: 2. Execute below commands: ! pip install graphviz ! pip install pydotplus 3. Open a new notebook and you should be able to import these packages.",313826.0
115929,501491.0,"Thanks Vinay, Yes I was able to install pydotplus and graphviz now and packages are available now.",310463.0
115929,524726.0,"Hi Magesh/Vinay, I tried installing pydotplus and graphviz using Terminal as you mentioned, but the terminal just shows closed and I am not able to type anything there. Can you please help me out?",318397.0
115929,527571.0,"Just open windows command line , use follwing commands if you downloaded graphviz package as suggested pip install graphviz Install pydotplus - pip install pydotplus",311227.0
115929,528973.0,"You do not have Python explicitly installed on your machine? Used only Anaconda installer for Jupyter notebook? If the Terminal option on notebook does not work, look for a tool ( Anaconda Prompt ) installed on your machine. Just type first few characters (anac) in your run command. pip commands should work there without any issues.",318007.0
115929,529205.0,"another way to install using Anaconda navigator, search for packages you would like to install, select the package and then install.",310463.0
120509,522770.0,Reload the page. You will see the tree.,304319.0
120509,523815.0,"Hi, Please check, Now it is working fine.",344894.0
122154,531013.0,You can use the conda install for both graphviz and pydotplus. But make sure you use the zip extract as mention in the instruction and set your OS path correctly.,318476.0
122154,534051.0,Its not necessary that you have to use pip if you are working in conda environment then install using conda command,320685.0
122154,536115.0,"Hi , Just try to Navigate to Anaconda promt by typing in Start if you are in Windows and then try running your conda command in that terminal.It will install. Regards Somnath",314617.0
121966,530048.0,"Try restarting the kernel and then importing pydotplus. You can also check if the installation has happened by running the below command on Anaconda Prompt, which lists all the modules installed: pip list",313826.0
121966,530540.0,"Hi, Run the commend in Jupyter Notebook ! pip install pydotplus",344894.0
121966,534053.0,try to install from jupyter notebook directly go to the jupyter notebook terminal and install from there,320685.0
121248,526610.0,I am also getting same error and I followed all steps given in forum but no solution. Require a help on from TA.,301113.0
121248,526615.0,you can refer to following link for TA verified answer for similar type of query: https://learn.upgrad.com/v/course/208/question/115929,302738.0
121248,526681.0,You can install using Anaconda navigator Search for graphviz and select the package Click on apply,317845.0
121248,526696.0,"After installing the graphviz and pydotplus packages on python, the following steps needs to be done ( applicable for Windows only): 1. Download and installl Graphviz software from the below link https://graphviz.gitlab.io/_pages/Download/Download_windows.html 2. Find out the path where the software is installed. Usually it will be : C:\Program Files (x86)\Graphviz2.38 3. In the jupyter notebook, change the path accordingly # If you're on windows: # Specifing path for dot file. import os os.environ[""PATH""] += os.pathsep + ' C:\\Program Files (x86)\\Graphviz2.38\\bin\\'",313826.0
121248,526872.0,"Setup continues for a while but when when everything stops, all the boxes are unchecked again and none of the packages show installed",319951.0
121248,527859.0,Did you set up the path for system and user variables?,301643.0
121248,529266.0,Thanks a lot.. that extra step has helped me too... Appreciate,304694.0
121248,528188.0,"Hello Team, I also faced the same issue. So here is what I did to resolve the issue. Before proceeding further please note that I am using Anaconda with Python Notebooks. So if you are using any other, this will not work. 1. Download the graphviz-2.38.zip package. 2. Unzip and copy the graphviz-2.38 folder to C:\Program Files(x86) or wherever you want to. 3. Go to the Anaconda directory. For e.g d:\Anaconda3\Scripts 4. conda install -c conda-forge pydotplus 5. conda install -c conda-forge graphviz Still the code will not work and you face the module error with respect to graphviz 6. So do this extra step here conda install python-graphviz This will do the trick and the python code will run. In the python notebook, there is a code os.environ[""PATH""] += os.pathsep + 'C:/Program Files (x86)/graphviz-2.38/release/bin/' Put the exact path where you have copied your graphviz-2.38 folder. This should work. Thanks and Regards Vijay K",310482.0
119674,519823.0,Please run the python code. You will get above tree,344894.0
121275,527037.0,"Click on Start and type anaconda, you'll se the results with Anaconda prompt as one of the options",307176.0
121275,527277.0,Adding paths to environment variables.,307176.0
120440,522016.0,Please checkout another related discussion to see if that helps https://learn.upgrad.com/v/course/208/question/115929/answer/501427,313826.0
121481,528062.0,"For Ques 3 IF 'Thal'&lt;4.5, tests works for left side of the branch . So it has two cases 1. Pain.type &lt; 3.5 &amp; ST_depression&lt;2.52. Pain.type &lt; 3.5 &amp; Flouroscopy.coloured &lt; 0.5",311004.0
121481,528063.0,"For Ques 3 IF 'Thal'&lt;4.5, tests works for left side of the branch . So it has two cases 1. Pain.type &lt; 3.5 &amp; ST_depression&lt;2.52. Pain.type &lt; 3.5 &amp; Flouroscopy.coloured &lt; 0.5",311004.0
121481,528066.0,"For Ques 3 IF 'Thal'&lt;4.5, tests works for left side of the branch . So it has two cases 1. Pain.type &lt; 3.5 &amp; ST_depression&lt;2.5 2. Pain.type &lt; 3.5 &amp; Flouroscopy.coloured &lt; 0.5 Ques 5: IF ‘Thal’ &gt; 4.5, THEN tests works for right side of the branch . So it has below two cases 1. Flouroscopy.coloured &lt; 0.5 &amp; Exercise.angina &gt; 0.5 2. Flouroscopy.coloured &gt; 0.5 &amp; BP &gt; 109",311004.0
121481,527690.0,"The decision tree for the heart disease dataset is given below. Question 3: If the decision tree algorithm predicts that a person doesn’t have heart disease and ‘Thal’ &lt; 4.5, it has to perform the tests highlighted in yellow in the above decision tree. Since 'Thal'&lt;4.5, tests on left side of the branch should be performed. It could be either 1 of these. 1. Pain.type &lt; 3.5 and ST_depression&lt;2.5 2. Pain.type &lt; 3.5 and Flouroscopy.coloured &lt; 0.5 Question 5: If the algorithm predicts that a person does not have heart disease, and it is known that she has ‘Thal’ &gt; 4.5, then either one of the following conditions needs to be true. 1. Flouroscopy.coloured &lt; 0.5 &amp; Exercise.angina &gt; 0.5 2. Flouroscopy.coloured &gt; 0.5 &amp; BP &gt; 109",310467.0
121515,527783.0,"We can check the homogeneity of the dataset after splitting the datset on different attributes of the dataset. Then choose the attribute that results in maximum homogeneity. Some metrics to check the homogeneity are for classification : Gini index, entropy, information gain for regression : R squared.",310467.0
121536,527889.0,"They can be used in pretty much all the classification problems in the real world scenarios. For eg; credit scoring, crime risk, medical diagnosis, failure prediction. They can also be used in Churn Analysis, Sentiment Analysers",315028.0
121536,528029.0,"wheather a person will default the loan, wheather a person will buy a product. wheather one team will win world cup etc",318017.0
121536,528250.0,"Hi, Decision Trees are a statistical/machine learning technique for classification and regression. They are capable of discovering complex interactions between variables and making accurate predictions on new data. Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal. Types of decision Trees include: ID3 (Iterative Dichotomiser 3) C4.5 (successor of ID3) CART (Classification And Regression Tree) CHAID (CHi-squared Automatic Interaction Detector). ... MARS: extends decision trees to handle numerical data better. Conditional Inference Trees. Some techniques, often called ensemble methods, construct more than one decision tree: Bagging decision trees, an early ensemble method, builds multiple decision trees by repeatedly re-sampling training data with replacement, and voting the trees for a consensus prediction. A Random Forest classifier uses a number of decision trees in order to improve the classification rate. Boosted Trees can be used for regression-type and classification-type problems. Rotation forest - in which every decision tree is trained by first applying principal component analysis (PCA) on a random subset of the input features. In Model Selection module after this module you will find the CART and CHAID Trees explained: https://learn.upgrad.com/v/course/208/module/10558",318476.0
121536,528252.0,Predicting Price of House. The price of House might follow linear model for metro and nonmetro area. You would need two different model here. Predicting weight of a person. This will follow linear model within different age band.,317514.0
121536,531551.0,"We’ve probably used a decision tree before to make a decision in your own life. Take for example the decision about what activity you should do this weekend. It might depend on whether or not you feel like going out with your friends or spending the weekend alone; in both cases, your decision also depends on the weather. If it’s sunny and your friends are available, you may want to play soccer. If it ends up raining you’ll go to a movie. And if your friends don’t show up at all, This is a clear example of a real-life decision tree . We’ve built a tree to model a set of sequential, hierarchical decisions that ultimately lead to some final result. Notice that we’ve also chosen our decisions to be quite “high-level” in order to keep the tree small. For example, what if we set up many possible options for the weather such as 25 degrees sunny, 25 degrees raining, 26 degrees sunny, 26 degrees raining, 27 degrees sunny…. etc, our tree would be huge! The exact temperature really isn’t too relevant, we just want to know whether it’s OK to be outside or not. The concept is the same for decision trees in Machine Learning. We want to build a tree with a set of hierarchical decisions which eventually give us a final result, i.e our classification or regression prediction. The decisions will be selected such that the tree is as small as possible while aiming for high classification / regression accuracy. Please refer below link: https://towardsdatascience.com/a-guide-to-decision-trees-for-machine-learning-and-data-science-fe2607241956",314183.0
120981,525374.0,"An attribute can be used in more than one node of a decision tree. Each node represent the test condition of an attribute. In the example given below, ""flouroscopy_coloured"" is an attribute of a dataset. The decision tree given below has done a test on this attribute in two nodes.",310467.0
120981,527102.0,"Any attribute can be present in more than 1 nodes. For example if you take age. So, first node you can take age &lt; 50. In the next level based on above node you can decide on age&lt;30 in left node and age &lt; 70 in right node.",317689.0
120981,528059.0,"The . Attribute can present on the node, Left side of the node and right side of the node.",313526.0
123241,536163.0,"Hi, Please follow the below link https://anaconda.org/conda-forge/pydotplus",344894.0
121642,528429.0,Did you try giving the proper file path in the write pdf command? Alternatively have you updated the PATH variable for user and system settings? That may help,308435.0
121642,528821.0,Used the zip version of graphviz and append the OS path to the release/bin of your graphviz folder. Hope this will work,318476.0
121642,529074.0,"graphviz is not installed on your machine, if you have already installed then environment variables are not set. please do these 2 activies",301113.0
123248,535964.0,did you add graphviz to environment variable?,307176.0
123248,536174.0,"Hi, Please follow the below link to install packages https://learn.upgrad.com/v/course/208/session/31161/segment/164636 https://anaconda.org/conda-forge/pydotplus",344894.0
121471,527613.0,"Hi, See the % of 0 class at the node of Flouroscopy.coloured , left side has high percentage compare to right side. That's we will not check BP, and we give more priority to Exercise.angina. Thanks",344894.0
121471,533848.0,"If you look st the bottom of the pdf, value defines how many people are tagged to each class. For example, value [89,7], shows there are 89 people who don't have heart diseases, whereas 7 who do. In the question, Exercise Angina is pretty easy as you see leaf with value [20,9] clearly indicates person doesn't have heart dieases, so to reach that leaf Exercise Angina has to be less than 0.5. If you look at the second yellow leaf, you cannot make a decision with value [2,1]. The blue ones show a clear tilt towards Heart Disease. In that case, the only way to reach the other earlier mentioned [20,9] leaf is through Flouroscopy Cloured less than 0.5",304022.0
123415,536742.0,Have you set the os.path for the graphviz package. In the jupyter notebook it's commented.Set it correctly to your path of the graphviz lib after extracting the zip mentioned in the instructions. This error will be fixed .,318476.0
121362,527094.0,Change the path mentioned in the notebook specfic to your installation directory of graphviz. If you are installing it with anaconda than it will be installed as part of anaconda packages and will lie under following path by default: C:\Users\ YOURUSERNAME \AppData\Local\Continuum\anaconda3\Library\bin\graphviz,317689.0
121362,527525.0,Do - pip install graphviz,317514.0
121362,527091.0,"Check if the following steps are followed. Make sure graviz file is unzipped and placed at Program Files (x86) Environment Variable: Add C:\Program Files (x86)\graphviz-2.38\release\bin to User path Make sure that the following steps are executed correctly. Steps for Windows Users : Download graphviz from here (ZIP file) Un-zip the file and copy-paste it to C:\Program Files (x86)\ Make sure your file is unzipped and placed at Program Files (x86) Environment Variable: Add C:\Program Files (x86)\graphviz-2.38\release\bin to User path Environment Variable: Add C:\Program Files (x86)\graphviz-2.38\release\bin\dot.exe to System Path Install python graphviz package- pip install graphviz Install pydotplus - pip install pydotplus Steps for Mac Users : To install the graphviz software on Mac, you can use homebrew: Install homebrew from here: https://docs.brew.sh/Installation Run this in the terminal brew install graphviz Install pydotplus, pip install pydotplus Install the python graphviz module, pip install graphviz",310467.0
121608,528255.0,"I managed to get it working. I do not have any python explicitly installed. It is all the anaconda and jupyter taking care of python stuff. The pip commands did not work for me as I have perl pip and not python pip. The pip commands worked for me when the commands were executed on 'Anaconda Prompt', that is found installed as part of tha anaconda installation. Thank you!",318007.0
121608,528253.0,Try conda install graphviz. It worked for me.,318476.0
138453,597925.0,"I believe it would indicate overfitting. Since when you have a very complex and overfitted Tree model, the leaf nodes would be very homogenous. so, when we see a significantly large change in homogeneity, it would be indicating overfitting. Just my point of view, I could be wrong as well. So, lets wait for TAs verification. Hope that helps.",317998.0
138453,599391.0,"As per me, it would indicate that the selected feature set on the basis of which the split is made is highly related to the target variable. For instance, if feature A is more than a threshold results into a certain class of target variable then such a split in the tree would give significantly large homogeneity in the very next split. Now, if we have a sufficiently high number of samples that we are splitting, it might not indicate overfitting but for really tiny sample size, it might also indicate that we are overfitting.",317987.0
122292,532253.0,"We perform check only on 'Validation set' to protect our test data from exposing. If we go via conventional split of test and train and perform check on test data, its higly likely that test data memorises the pattern and may lead to overfit. Thats why total dataset is split in test, train and vaildation set. So, in order to separate the test data all together and get final performance of model on test data, performance check is done on validation set. After this you test the final model on test data. Hope this will help.",317991.0
122705,533579.0,"I found the answer at : https://stackoverflow.com/questions/46480457/difference-between-min-samples-split-and-min-samples-leaf-in-sklearn-decisiontre and its a worth read. To get a grasp of this piece of documentation I think you should make the distinction between a leaf (also called external node ) and an internal node . An internal node will have further splits (also called children ), while a leaf is by definition a node without any children (without any further splits). min_samples_split specifies the minimum number of samples required to split an internal node , while min_samples_leaf specifies the minimum number of samples required to be at a leaf node.",318770.0
123026,534784.0,"Suppose there are 1000 datapoints at a particular node of the tree and when we try to split this node further into leaves, suppose we get only 2 datapoints in one leaf node and 998 in another leaf node. However, we want to create a new leaf node only if it contains atleast a minimum number of datapoints say 10 datapoints. In that case, we may choose not to split the original data node with 1000 datapoints and thus make it a leaf node. This is controlled by the hyperparameter min_samples_leaf=10 .",313826.0
122141,531066.0,Below is tutorial link of how to handel categorical data and when to use label encoder https://www.datacamp.com/community/tutorials/categorical-data label encoding allows you to convert each value in a column to a number. Numerical labels are always between 0 and n_categories-1. But it has as disadvantage that the numerical values can be misinterpreted by the algorithm. Should the carrier US (encoded to 8) be given 8x more weight than the carrier AS (encoded to 1) Check the link for more deatils.,318476.0
121986,530042.0,Checkout the below links on similar discussions https://learn.upgrad.com/v/course/208/question/115929 https://learn.upgrad.com/v/course/208/question/121248,313826.0
121986,530232.0,"How I did - google ""anaconda ModuleNotFoundError: No module named 'pydotplus' "" and had opened the result : https://anaconda.org/conda-forge/pydotplus And followed the instruction to install package at Anaconda websiite. Same for GraphViz.",312479.0
122410,533724.0,No solutions are not given,306010.0
122410,532570.0,"Let me answer the question assuming I understood it correctly Question you are referring to is ""Coding Practice questions""; Note that hint to the answer is already in the code but commented out. If you open the practice workbook on jupyter and the ""Coding Practice questions"" window side by side, you can easily solve the practice questions. 1) If your solution is incorrect, the code throws ""Error"" while you run the code (click on ""Run Code"" to run your code) 2) If the solution is correct, the code run is successful, you can see the plots and the result expected by coding console and your result both match hence the run is successful 3) Once (2) is successful, click on ""Submit"", the code is ""Accepted"" if the code passes the results as expected I was able to run the code to success and the code was Accepted by the console. Hope this answers your question.",309211.0
122410,546889.0,You can see the sample solution provide by clicking the sample solution tab.,304281.0
119784,519653.0,Value is nothing but the count of labels in each subgroup generated by that particular decision Node.,317984.0
119784,519497.0,"Value is the element count of the different labels. For e.g. in the heart disease problem, the first node is thal &lt;= 4.5 gini = 0.494 samples = 270 value = [150, 120] Here 150 is the count of elements for which thal &lt;=4.5 and for remaining 120, thal &gt; 4.5",318438.0
121967,529959.0,Both are different astype('category') typecasts a pandas object to category Label encoder assigns ordinal levels to categorical data,317514.0
121730,529028.0,Found out the solution. Notebook is running.,310505.0
119882,519716.0,"This question is unclear, but if you play around with the data a little bit, you will find that a1=2,12 means a1 should be between 2 and 12. so one branch should be evaluated as 2&lt; a1 &lt;12 and another as 2&lt;=a1, 12&gt;=a1",318438.0
119882,519871.0,"by seeing the Graph I was able to comprehend and find the solution but the statement a1 = 2, 12 got me confused",317984.0
119882,520881.0,"Hi, a1=2,12 means a1 should be equal to either 2 or 12",344894.0
119882,528289.0,"a1=2,12 means a1 is either 2 or 12.",304319.0
119882,532778.0,this should be mentioned with more clarity in the question.. it is confusing!,318791.0
121304,,nan,
122188,531366.0,Can you please attach the error .,301648.0
122188,531580.0,check that your pip is updated and try again installation of the required library's,318017.0
122188,532246.0,"Hi, try with conda install",344894.0
121438,,nan,
121451,528584.0,"Hi, Try 'max_depth' :range(1,20) it may work",306242.0
121451,527587.0,"Hi, There are some test cases which are not able to execute in a time range, So you need to optimize your code",344894.0
121451,529573.0,"Yes, when I decreased the max_depth, the code worked",301643.0
121451,529797.0,The reason is your choice of range for hyperparameter max_depth. Reduce the same to get the compilation done on time.,306726.0
121451,536941.0,"Hi , For the test cases to pass we have to reduce this:-(for an example) # parameters to build the model on: specify a range of max_depth parameters = {'max_depth': range(1,20)} Regards Somnath",314617.0
123123,535326.0,"Pruning ensures that the model does not overfit on the training data. Also, the complexity of the model reduces and becomes more generalized. As a result, it is expected to perform better on the testing data and hence the accuracy is also expected to increase.",313826.0
121520,528261.0,"Can you check if the below steps are done or not : # convert target variable income to categorical df['income'] = df['income'].astype('category') # Putting feature variable to X X = df.drop('income',axis=1) X = X.as_matrix() # Putting response variable to y y = np.array(df['income'])",318476.0
123418,536908.0,"Hi Jyotishri, The new observations in the leaf will be same as before as correctly suggested by @Sambit Mohanty analysis. Regards Somnath",314617.0
123418,536829.0,"The observations in the leaf will be the same as before. Say, you have 4 observations in the node and you split it such that the left partition has 1 observation and the right partition has 3 observations. Now, if you chop these partitions off, the original node which is now a leaf, will still have 4 observations.",318476.0
122118,530781.0,"Normalization is generally useful, when you are solving a system of equations, least squares, etc, where you can have serious issues due to rounding errors. In decision tree, you are just comparing stuff and branching down the tree, so normalization would not help. In other words, in other modeling techniques you need to bring every variable on same scale as we are comparing each of the variable with all the other variables but in Decision Trees comparison is done between two likewise variables at every step or node and not woth the whole data set. Thats why normalization is not required in decision trees.",315560.0
122118,531489.0,"Scaling just multiply vars with some constant. In decision tree as per nature of algorithm, in a var it’s compare with its own value and it creates rule so multiplication of constant does not make any difference. It is just an additional step..",300735.0
122118,532656.0,"Normalization should have no impact on the performance of a decision tree. It is generally useful, when you are solving a system of equations, least squares, etc, where you can have serious issues due to rounding errors. In decision tree, you are just comparing stuff and branching down the tree, so normalization would not help. But if do perform normalisation, the result will be same. Decision trees, and regression trees does not require inputs (features) to be normalized, since it is invariant to monotonic transformations of the features.",314183.0
122510,532869.0,The Qusetion is asked for the left node: Splitting You cannot further split the left node. Why? For Right node your analysis is correct,318476.0
122510,533910.0,"Hi, You also need to check Gini index of the partitions. If you check for a1&lt;6 it will come less.",344894.0
126526,551338.0,"type='prob' will give you the predicted probabilities for the classes whereas method='class' will give you the class name as the prediction output. For example, in a binary classification of two classes (say A and B), the first case (type='proba') will give output prediction as 65% class A and 35% class B. ie. this is the prediction by the model that it thinks the given input has a probability of 65% of belonging to class A. While, if you use method = 'class', the model will give prediction as the class name directly. ie. it will predict that the given input is of Class B. (or A). Here probability wont be in the output. You can refer to the following answer for more insight: https://stats.stackexchange.com/questions/90937/how-do-i-know-which-class-my-random-forest-model-is-predicting Hope that helps.",317998.0
123666,537752.0,"Hi Siddhanth, Please try to seperate the columns into - Year, month, day, Hour, Minute and Second.",320103.0
123666,553914.0,"Create derived columns from it. You can convert that string to a datetime format. You can extract required information based on problem statement, i.e. Year, Month, Day etc.",318368.0
120715,524379.0,"I dont think he specifically said or meant 'Mean'. Rather he said/meant 'Average'. And as we know, all the 3 summary statistics - Mean, Median and Mode are measures of averageness or central tendency. That is statistically, mean is not the only average measure.",310511.0
120715,524475.0,"Aggregation in random forest is considered to the mean(not mode) in case of a regression problem, not a classification one. I guess you missed the regression part. :)",310511.0
120715,530917.0,"The question is specific for regression models where the target variable is not a label but a number. So, the output would be a mean of all the trees in Random forest. However, if it was a classification problem where the target variable is a label, the output will be the mode for sure.",306726.0
120715,536482.0,"For regression model, if each prediction gives a decimal value say 10.87, 10.86, 12.87, etc. There wont be any value which will have maximum occurence. This makes ""Mode"" not so useful. Mean has its own problem of outliers, but this can be reduced significantly if you have large sample size. Median can be used if outliers are a big concern. Please check the below link for detailed discussion on this. https://stats.stackexchange.com/a/306416",320074.0
122714,533782.0,"Even I got a plot which is different from what has been derived in the video. This could be due to the parameter random_state=None , due to which the decision trees generated are different from those as explained in the video and hence the instance that you have run has generated such a result. Suggest to re-run the gridsearchcv for n_estimators again to see if the plots are any different.",313826.0
122714,533840.0,TA please clarify this qus as I have also the same concern,305650.0
122714,534845.0,The random state is None hence the trees get generated in a different way everytime.,300706.0
122264,531578.0,yes the dataset is not only a part of a single tree in random forest that is basically the idea behind a random forrest you take a large number of different trees with same data set using a different chunk of data for predicting.,318017.0
122264,531862.0,"If the product of number of trees and dataset excluded in the training set is equal to or greater than one, than you have atleast a dataset excluded once from a tree in ensemble. If you have higher number of trees than the probability of it not getting covered is higher as long as the dataset excluded is higher. Say you have 100 observations and you include 80 random observations in training dataset and exclude 20 random observations. If you have 10 trees, then each observation will be present in 8 trees and missed out in 2 trees",317514.0
122264,534879.0,So effectively the data is partitioned such that each data point is left out of atleast one training set.,311857.0
122740,533951.0,"parameters = {'n_estimators': range(100, 1500, 400)} is similar to writing parameters = {'n_estimators': [100,500,900,1300])} . The GridSearchCV is run for each of the values passed as hyperparameter n_estimators to the Random FOrest model (estimator object). This is true for multiple hyper-parameters being passed to the model as-well.",313826.0
122740,534305.0,"Yes. It is creating a dictionary object with key = n_estimators having values which is output of range function. range(100,1500,400) which is equal to values = [100,500,900,1300]. So, n_estimator will take 4 values.",317689.0
122740,534358.0,If you're looking for an answer on how to chose what range is applicable as parameters I'd suggest you to take a call using your own gut feeling. You can chose any range which you feel should be good enough to analyse the parameters on.,307176.0
123983,538595.0,Yes it is not needed as we are using Bagging Ensemble method and OOB. Not sure why it was used in the sample notebook.,310974.0
123983,539154.0,"You're right. Test-train split is not necessarily needed in Random Forest as Random forest make use of conditional split in-itself. But I'd still suggest you to make test-train split to get a better understanding of the accuracy on the test data. I know its additional work, but I guess its worth it to make sure you're getting a consistent accuracy.",307176.0
123983,553913.0,"Yes, its not required. Random forest will do conditional split for test accuracy.",318368.0
122289,531904.0,"Answer for question 2 : You can use 'oob_score' to check the OOB error from sklearn.ensemble import RandomForestClassifier rfc=RandomForestClassifier(oob_score = True) rfc.fit(x_train,y_train) rfc.oob_score_",310467.0
122289,531909.0,Not sure about the specific reason for the train test split in the credit card example. It would be better if TA can answer this. In general it is not necessary to split a data set into training and test when building a random forest model since each tree built uses a random sample (with replacement) of the training dataset. However it is a good practice to split for the following cases. 1. You’re using out-of-bag (OOB) error estimates to optimize a hyperparameter. 2. You’re comparing a random forest model to other techniques 3. It’s simply good practice Please find more info regarding this in the link below https://www.dataminingapps.com/2018/02/is-it-really-necessary-to-split-a-data-set-into-training-and-validation-when-building-a-random-forest-model-since-each-tree-built-uses-a-random-sample-with-replacem/ .,310467.0
124858,544499.0,"The bootstrap method samples the given training tuples uniformly with replacement. That is, each time a tuple is selected, it is equally likely to be selected again and re-added to the training set. For instance, imagine a machine that randomly selects tuples for our training set. In sampling with replacement, the machine is allowed to select the same tuple more than once.",420665.0
122261,531579.0,"yes you can use different models like svm , logistics regression , decision tree etc in a single random forest where the individual model when taken as a part of ensemble perform well then the individual model but this may not be always possible sometimes a individual model can also perform better then ensemble",318017.0
122261,531837.0,"Agree with Deval. Add further to that, Individual model might perform better on trained data, but random forest shall perform equal or better on unseen or new data compared to individual model.",317514.0
122261,532678.0,Thanks...i guess i also wanted to find out how to create such models in python?,310509.0
123567,537639.0,"rerun your code once, you will definitely get output and also wait for few minutes as some of the code took 10 minutes despite having i7 and 16gb ram.",318429.0
123286,535963.0,"You are right. It uses sort of cross-validation test while training and running the model. But, I'd suggest you to keep aside test set to get results of the model performance.",307176.0
123552,537162.0,"Since you are increasing the number of trees you are making the model more generalize, as even if few trees overfit, the rest will lower it down.",318756.0
123552,537217.0,It's make the model better not complex as it's average out the output from each tree to the final result . The may tree work on there data set and feature set resulting few tree might overfit and few might not. But yes the time complexity increased in random forest compared to the normal decision tree.,318476.0
123570,537161.0,No i guess everytime u dont need to check the tuning of individual parameters. You can directly specify a range of values for different parameters and do gridsearchcv to get the optimal value of each paramater.,318756.0
123570,553919.0,Doing invidual will save some time for you as for next set of parameter you will have fixed values. but both are same and output will not differ.,318368.0
123577,537478.0,Yes ofcourse as it is not negligible,310974.0
123577,538398.0,It is considered. The expected number of levels in a tree is given by log(n) where n is the number of observations being considered to build the tree. That is the reason why log(n) is a part of calculation for time taken in a forest.,301121.0
123577,553915.0,"Yes, algo will count all the time as time required for split is significant and can't be ignored.",318368.0
123473,537087.0,"Hi Jay, OOB is calculated on the set of points that has not been part of training set. For eg. if there are 100 points and you choose 70 for creating the trees in training set, OOB will be calculated on the remaining 30. It works similar to the logic of train test split , where the model has to perform efficiently on the unseen data ie. test data",308673.0
123473,537484.0,There must be a data structure included as part of the algorithm implementation which tracks which observation is part of which tree's training.,310974.0
123473,537980.0,The algorithm does that internally. Trying to find observation which was not used in a particular tree doesnt seem very useful here.,307176.0
123473,540821.0,OOB error on trained data serves the purpose of predicting accuracy. Algorithm handles it internally to evaluate which observations were left out in the sampling process during the model building. It is similar to CV which provides an unbiased estimate error,316147.0
123571,537481.0,Most of it is with intuition and trial and error. There is no way to determine to best values.,310974.0
127468,556464.0,"Just to add to Arihant's point, the random forest doesn't check all the variables and further split, it uses the random variables and hence it wouldn't be biased.",311160.0
127468,556826.0,"Decision trees tend to overfit as they grow deep. After every split there will be fewer and fewer samples for the next split to work with. Decision tree at each node, it will make the decision among a subset of all the features(columns), so when it reaches a final decision, it is a complicated and long decision chain. Only if a data point satisfies all the rules along this chain, the final decision can be made. This kind of specific rules on training dataset make it very specific for the training set, on the other hand, cannot generalize well for new data points that it has never seen. Especially when your dataset has many features(high dimension), it tends to overfit more. Random forest avoids the overfitting problem of decision trees by adding more trees instead of building one big tree. Averaging the outputs of the trees in the forest means that it does not matter as much if the individual trees are overfitting.",329936.0
127468,556409.0,"Overfitting occurs in decision trees as sometimes trees are sometimes modeled to perfectly fit all the samples in the training dataset. Therefore it ends up with branches with strict rules of sparse data. So, accuracy takes a hit when we are predicting samples which are not part of training sets. In random forests an ensemble of many decision trees is taken. These decision trees individually may be overfitting but when taken collectively to create a model reduces overfitting by averaging it out, as all the trees chosen are created on different samples of dataset. Please refer to https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991",317689.0
138148,595881.0,"In CHAID trees, Chi-Square test determines the best next split at each step. Please refer below link for more clarity: https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/",313826.0
122885,534681.0,"You can go through the below link, where Entropy has been explained with Example. https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8 This may be helpful for your understanding.",311117.0
122666,533414.0,"When the partition is done based on 'Age', 700 out of the 1000 datapoints have age less than 50 which means p(age &lt;50) = 700/1000 = 0.7. Similarly, 300 out of the 1000 datapoints have age greater than or equal to 50 which means p(age &gt;=50) = 300/1000 = 0.3. Hope this helps.",313826.0
121768,528903.0,When you are calculating gini index of a single node you wont use those fractions. But when you want to calculate Total GINI index for all possible partitions based on a attribute then you need WEIGHTED gini index for all nodes. Thats when you mutiply by fractions,317996.0
121773,529485.0,"Hi, There are no such rules. It depends on your type of data &amp; problem and how you want to solve .",344894.0
121773,531494.0,It’s depend on objective. There is no rule to predefined threshold value. It’s better to control hyper parameter by truncating and pruning of tree..,300735.0
121773,531988.0,Value of threshold depends upon data and how you want to calculate.,314183.0
121412,527631.0,"Decsion tree is differnt algorithm than Linear and Logistic regression, It is used for modeling for higly non lenear data points where as regression is used whn there is linear relationship between target and indepdent variable.",311227.0
121412,527265.0,"Most important point to remember is Decision trees algorithm is basically a classification technique with the leaves being the class label. Now, in extension to the ""leaves being just a class label"", it can also be used with Regression algorithms(linear or logistic) to produce ""leaves as regression model"". It totally depends upon the type of problem in hand. Decision trees helps classify data into several classes on which the regression model can be run in combination to make predictions. Like professor explained, there are situations where the output variable varies linearly on a dependant variable and has different slopes at different points. Its there where Decision tress classifies the different points(class labels) on which regression algorithm can be applied differently on different classes of data labels. Inshort, Decision trees classifies the data into different labels for different Regression models to be run.",307176.0
121412,527622.0,"Hi, DT is a different algorithm. It works based on rules. Please follow the below link https://datascience.stackexchange.com/questions/9159/when-to-choose-linear-regression-or-decision-tree-or-random-forest-regression",344894.0
121412,534594.0,"Decision tree is mostly used in classification problems. - can be seen as a way to summarize the structure found in data. Also models presented by Decision tree are easily understood by humans. The rules can be easily understood and can provide intuitions which are not always obvious by a simple inspection of data. Due to its obvious construction the decision trees have started to be used at a fairly large scale together with other models like logistic regression, linear regression by a ot of practitioners",306009.0
121412,532016.0,"Linear regression and Linear classifier. Despite an apparent simplicity, they are very useful on a huge amount of features where better algorithms suffer from overfitting. Logistic regression is the simplest non-linear classifier with a linear combination of parameters and nonlinear function (sigmoid) for binary classification. Logistic Regression (ML algorithm) may be used not only in the “Modeling” part but also in “Data Understanding” and “Data Preparation”. Decision trees is often similar to people’s decision process and is easy to interpret. But they are most often used in compositions such as Random forest or Gradient boosting. K-means is more primal, but a very easy to understand algorithm, that can be perfect as a baseline in a variety of problems. PCA is a great choice to reduce dimensionality of your feature space with minimum loss of information. Neural Networks are a new era of machine learning algorithms and can be applied for many tasks, but their training needs huge computational complexity.",314183.0
123411,537485.0,"It is highly improbable that the values of Gini index of various features be same. But in that case, it happens, it is a good question :)",310974.0
123411,538440.0,"Hi, In that case, it will choose randomly any one variable.",344894.0
121926,529711.0,"The data is split so that child nodes are homogenous. A data set is completely homogenous if it contains only a single class label. In real-world data sets, one will almost never get completely homogenous data. So the algorithm tries to find value which split the nodes such that the resulting nodes are as homogenous as possible So if the node is salary, the algorithm will try and split it with a rule (value) such that all the data points that pass the rule have one label (or are as homogenous as possible)",317514.0
121926,530535.0,"Hi, Please follow the below link http://www.statsoft.com/Textbook/Classification-and-Regression-Trees https://stats.stackexchange.com/questions/220350/regression-trees-how-are-splits-decided",344894.0
138152,595710.0,TA please provide a proper solution to this question with all possible comments (On Paper),311466.0
138152,596964.0,"Hi Arpit, Please share the distribution of labels when A=0 or if you want with another example then please follow the below link https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134",344894.0
138151,596992.0,"Hi Arpit, E = -P1*log2(P1) - P2(log2(P2)) = 0.2864 where P2 = 1-P1 So 0.2864 = -P1*log2(P1) - (1-P1)(log2(1-P1)) For formula of log(1-p1) https://mathhelpboards.com/pre-calculus-21/power-series-solution-log-1-x-3065.html",344894.0
138151,598891.0,( E = - P1*log2(P1) - P2(log2(P2)) = - 0.2864 ) or ( E = - P1*log2(P1) - P2(log2(P2)) = 0.2864 ) where E = - 0.2864 This should be the equation right ? Entropy can be negative and its between 0 to 1 ?,311466.0
138151,600302.0,Here I have done the numerical part step by step which I was expecting by you but I can't find your solution correct please check it and if you have copied from somewhere please let me know the source ! Thanks Correct me if I'm wrong !,311466.0
138151,606976.0,The log properties are given below in the image: So log 2 ( Pi ) = log( Pi ) / log ( 2 ) This should be the logic correct?,311466.0
110743,476790.0,"The concept of singular value decomposition or SVD is essentially for Dimension Reduction. So here, we choose k &lt; n , i.e. k principal components, for achieving dimensionality reduction. In our lecture session, it is clarified also: https://learn.upgrad.com/v/course/208/session/25597/segment/131652. Why k is rational number and not integer?: It is a framework for unifying themes of latent variables. It may be ordered categorical (ordinal), unordered categorical (nominal), counts, or combinations of these variable types. For more details, please refer : https://www.statmodel.com/features.shtml Hope it will help to understand.",311117.0
110743,476934.0,"it is an integer. look closely. they are options 1,2 and 3 and the values are given as 539, 345 and 565. and as we are taught k&lt;=n, so we'll choose 539 and 345 as the answers.",302738.0
110993,478127.0,"What are the shortcomings of PCA as a Dimensionality Reduction Technique: Though it has been explained and alternative (t-SNE/ICA) techniques have been suggested in "" PCA in Python "", but for more clarity, you can refer this link: https://stats.stackexchange.com/questions/276154/what-are-shortcomings-of-pca-as-a-dimensionality-reduction-technique-compared-to",311117.0
110993,477808.0,"refer this https://learn.upgrad.com/v/course/208/session/25598/segment/131662 you'll find the shortcomings of PCA mentioned here. we just have to explain those. these are the shortcomings: PCA is limited to linearity, though we can use non-linear techniques such as t-SNE as well. PCA needs the components to be perpendicular, though in some cases, that may not be the best solution. The alternative technique is to use Independent Components Analysis PCA assumes that columns with low variance are not useful, which might not be true in prediction setups (especially classification problem with class imbalance)",302738.0
110774,476888.0,"In Python you have to use the following command u, s, vt = np.linalg.svd(data, full_matrices=True) data refers to the name of the data frame. Please ensure that it shall have only standardized numerical columns. This clue is given in practical questions.",301121.0
110774,476895.0,You can go through this specific example. This will help: http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm,311117.0
110775,476896.0,You can go through this specific example. May be helpful: http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm,311117.0
110775,476898.0,"If one can get a good percentage of variance explained with certain number of principal components (k), it would be good enough. I suggest you see the scree plot, when you will variance of 85% visible with 3 components. This scree plot test gives you a good visual result of what you are looking for.",301121.0
110772,477676.0,"yes, drop the name and then execute you will get the results",317600.0
110772,476854.0,"If you can show the error code and snapshot, this will be more useful in finding the error you are getting",318429.0
110772,476864.0,"If you are facing this issue: TypeError Traceback (most recent call last) &lt;ipython-input-20-68b66ab40115&gt; in &lt;module&gt;() ----&gt; 1 U, s, VT = np.linalg.svd(MFR, full_matrices=True) ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\linalg\linalg.py in svd(a, full_matrices, compute_uv) 1442 1443 signature = 'D-&gt;DdD' if isComplexType(t) else 'd-&gt;ddd' -&gt; 1444 u, s, vh = gufunc(a, signature=signature, extobj=extobj) 1445 u = u.astype(result_t, copy=False) 1446 s = s.astype(_realType(result_t), copy=False) TypeError: No loop matching the specified signature and casting was found for ufunc svd_n_f Please check, dtype = 'float' You can see more clarification here: https://stackoverflow.com/questions/47838306/getting-no-loop-matching-the-specified-signature-and-casting-error",311117.0
110772,476902.0,"Hello Theerendra, I guess what error you might be getting. Please drop the name first before trying the command for svd df = df.drop(['Name'], axis=1) u, s, vt = np.linalg.svd(df, full_matrices=True)",301121.0
110363,475498.0,"Substitute x = 3 and y = 4 you end up having this equation (3*0+4*1, 3*-1+4*0) =(4,-3)",318451.0
110363,475405.0,"(3*0+4*1, 3*-1+4*0) =(4,-3)",318451.0
110363,475466.0,"why is 4*1? If you look at the attached screenshot from the 3Blue 1 Brown example, the calculation formula shown is different. Also, if you visually change the basis, the new co-ordinates come as (-4,3)..",310509.0
110363,475534.0,"After 90-degree anticlockwise rotation, negative x-axis became y-axis and positive y-axis became x-axis.",311117.0
110363,475435.0,"Question saying that "" we rotate the basis vector '90 degrees anticlockwise', the new position of i becomes (0,1) and that of the new j becomes (-1,0) "". Hence, Y-axis becomes the new X-axis, so the previous Y coordinate becomes the new X coordinate. Negative X-axis is the new Y-axis, so the negative of X-axis becomes the new Y-axis. Therefore, the new coordinates of P are (4,-3).",311117.0
110363,476213.0,"Hi Chetan, As Brijesh mentioned, After 90-degree anticlockwise rotation, negative x-axis became y-axis and positive y-axis became x-axis. And the point remains in the same place. Draw this on as piece of paper. Now, to get the units, rotate the entire paper clockwise 90-degree. Now both axis and the point too will rotate. If now you determine the co-ordinates, it will be 4 to right and 3 down or -3. If you draw on paper, it will automatically become clear.",334535.0
109953,474101.0,"Hi Brijesh, If you navigate to the partical question that have provied the method name call to get U,S,Vt: u, s, vt = np.linalg.svd(X, full_matrices=True) Make sure the X is array without named columns.",318476.0
109953,474530.0,"Brijesh, There is a hint of how the decomposition is done though it is vague and incomplete. The initial k is same as n that is all the 5 columns are considered. Then looking at the ratings first 3 books are rated high and other 2 books rated low by some users. similarly some users rated the exact opposite. So it helped identify two genre or themes - romance and philosophy. Still no idea how or waht the third theme is. Probably accounts for the combination where users prefer some romance and some philosophy. Now that the k value is 3...other calculations follow. How exactly the weights and such are calculated is not given.",311857.0
109953,474531.0,"If you want to see the actual calculation I would suggest the following steps: Step 1 - Matrix Multiplication Step 2 - How Eigenvalue and Eigenvector is calculated Step 3 - SVD &gt; How different components of SVD i.e. USVt are calculated After these steps, SVD would make sense A birdseye view on SVD will be as follows: m = rows n = columns k = number of components in the data/ the components you choose m * n (Original Data) U (m * k) (row*themes/components) - relationship from row to the themes/components / Observation in the new columns S (k*k) (which theme/components are the strongest) VT (k*n) - how the new themes/components get formed using the earlier columns/ Different themes or features in the new space",318451.0
110720,476652.0,"We are currently planning one on coming Wednesday. If you've any further queries do let us know and we can include it in that session. For the time being, you can go through this link to understand further:http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm.",313517.0
110838,477152.0,Please refer to the link below in discussion forum which is quite good. https://learn.upgrad.com/v/course/208/question/110817,301121.0
110838,477322.0,"Hey Vimalan, I think you can refer these links: https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/ https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60 Hope this helps!",301655.0
110722,476655.0,You can take a look here to understand further: http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm,313517.0
110722,476670.0,Also please tell me what is the best approach to find the value of k?,318448.0
110722,476674.0,"You can use a scree plot to measure the variances measured by the principal components. Choose the value of k, which is the same as the number of principal components on the basis of a specific cut off as per your choice.",313517.0
110722,476873.0,Check this out. https://www.youtube.com/watch?v=P5mlg91as1c I hope this helps.,308962.0
110726,476686.0,"Kindly use below code: U, s, VT = np.linalg.svd(foodRating_5, full_matrices=True) and then make a diagnol matrix using np.diag(s[:3]), you will get same weight.",318448.0
110817,477132.0,Thanks for sharing Deval.,334535.0
110817,477135.0,"Deval, Thanks for sharing.",301121.0
110646,476378.0,"Here in the question, it is just asking the shape/sizes of U and Vt, i.e. dimensions of U and Vt, which is "" m x m "" and "" n x n "" respectively. You can see have understanding with the answer also. U is a matrix containing all eigenvectors of AAT. So, if A is an m x n matrix, the dimension of U will be m X m . Similarly, V is a matrix containing all eigenvectors of ATA. Therefore, its dimension would be n x n .",311117.0
110646,476392.0,"Hello RG If you use the following command, this is what you will get. (full_matrices = False) #Implement SVD u, s, vt = np.linalg.svd(MFR, full_matrices=False) print (""Shape of U :"",u.shape) print (""Shape of S :"",s.shape) print (""Shape of VT :"",vt.shape) Shape of U : (12, 9) Shape of S : (9,) Shape of VT : (9, 9) I am not sure if it is correct to use ""False"". TA can clarify.",301121.0
110834,,nan,
110770,476840.0,Got the issue.. Thanks,304812.0
110301,475103.0,"Matrix V, referred from the previous Question no. 4: where the matrix V, taken from A = U∑VT (SVD).",311117.0
110301,475293.0,"the matrix V represents the relation between themes (PCs) and the original variables, i.e. each of the k columns of V contains a principal component vector",318476.0
110843,477141.0,I have removed the Name column and tried using dtype also..still the same error,303228.0
110843,477381.0,"If you drop the "" name "" first before trying the command. df = df.drop(['Name'], axis=1) u, s, vt = np.linalg.svd(df, full_matrices=True)",311117.0
110843,477319.0,"Hey Bharati, I also encountered the same error. What I did was I removed the name column from it and ran the same code again and it worked. For further reference, you can refer the link given below: https://stackoverflow.com/questions/47838306/getting-no-loop-matching-the-specified-signature-and-casting-error Hope this helps!",301655.0
110843,477190.0,Got the solution..please ignore.,303228.0
112996,487344.0,Sorry I missed to answer about Binary standardization. It is actually not required to standarize binary data before PCA as the range of variable is obviously 1.,304814.0
112996,487321.0,"This is a very hot question on our discussion forum, and everyone has presented different views and all point of view are correct in one or other way. What we have seen till now is that PCA and RFE both can be applied to Binary values as Rahim explained this in Telecom-Churn-PCA module in which he first preformed RFE and then PCA, and the result of both the approaches matched. So, in my opinion, at this point of time, we can use either PCA or RFE for binary variables.",304814.0
112996,487328.0,"The question was more on should the binary data be standardized before PCA analysis. Since this is binary data, Standardizing this probably would not make a lot of sense, but would like to hear from the experts",318438.0
112996,487668.0,The binary features are categorical in nature and standardizing these features won't help in any manner. Standardization or normalization should be done on Continuous variables only. For choosing features you can go for RFE as TAs confirmed in other threads. But you can also give a try with PCA similar to telecom churn problem.,310481.0
110700,477621.0,"I guess here we are multiplying by the inverse of the matrix of the new basis vectors = [[0,1],[-1,0]]*[3,4]",300694.0
110700,476635.0,"Hi Pulikt, Could you please see the following matrix multiplication?",301121.0
110700,476643.0,"Y-axis becomes the new X-axis, so the previous Y coordinate becomes the new X coordinate. Negative X-axis is the new Y-axis, so the negative of X-axis becomes the new Y-axis. Therefore, the new coordinates of P are (4,-3).",318476.0
110700,476638.0,"The same has been discussed and I have replied, You can find here: https://learn.upgrad.com/v/course/208/question/110363 Question saying that "" we rotate the basis vector '90 degrees anticlockwise', the new position of i becomes (0,1) and that of the new j becomes (-1,0) "". Hence, Y-axis becomes the new X-axis, so the previous Y coordinate becomes the new X coordinate. Negative X-axis is the new Y-axis, so the negative of X-axis becomes the new Y-axis. Therefore, the new coordinates of P are (4,-3). After 90-degree anticlockwise rotation, negative x-axis became y-axis and positive y-axis became x-axis.",311117.0
110700,478096.0,"Team, Using your left hand, make a L out of your index finger and thumb, with thumb as base or x-axis, and index as y-axis. The back side of your hand is facing your. The point 4, 3 will be in between the L. Now rotate anticlockwise, with the point not moving, but only index finger and thumb rotating (anti clockwise). The thumb/Xaxis becomes y-axis. Not get the hand back to original position, i.e., rotate clockwise. But this time the designated point also moves. So the thumb is back to original position as base of L and as x-axis. And index finger is y-axis. Only this time the point has moved to quadrant between x-axis and negative y axis. So it will have co-ordinates as (4, -3) This is for simplicity stake. In mathematical terms, note answer by Muthu above",334535.0
112716,486376.0,"see, each PC, is made of all the variables. each PC is linearly dependent on all the variables with varied coefficients(weightage). when you select a subset of PC, you are selecting a subset of PCs, but all those are still dependent on the initial 30 variables. hope this helps!",305839.0
112716,486559.0,"when u see the components_ attributes of the pca object obtained after the fit(), it containe the loadings of various features. Convert it into a df. Add the feature column. The values which are high, the corresponding feature is significant. Infact first 2 PCs hold around 50% of the information. Since you are considering dimensionality reduction, 14 PCs contain information of 30 features. From the loadings you can find out the no. of features significant.",304319.0
112716,486726.0,"Consider a principal components as the combination of all other features with different weightage associated to them. As we know, our model takes more tiime to learn if we have more number of columns. Another point is, if we have more number of columns, there are high chances of multicolliearity. So, these PCA components takes care of above 2 problems. Now, once we have the result of the model ready, we can map the feature with highest weightage See the loadings.",301649.0
112716,486661.0,"Treat the principal components as a new entity explaining variance/information for all the features in their respective magnitudes. It may carry 90%information of one feature and lesser for other features. Refer the telecom churn case study , where principal components are plotted to explain the feature variables. In order to check which row each cell of the principal components corresponds to , you need to merge the data frames of the original dataset and principal component dataset.",318340.0
112716,486309.0,"When the 30 original variables are reduced to 14 principal components (PCs), it does not mean that only 14 important original variables are considered. The objective of a PC is to capture the maximum variance of the linear combination of all the original variables. Each PC vector will contain loadings with respect to all the original variables. The magnitude of the loading will provide the information on the relative strength amongst the original variables in the principal components. Refer to the following links for better understanding: https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/principal-components/interpret-the-results/key-results/ https://stats.stackexchange.com/questions/343905/most-important-variable-in-a-principal-component",313691.0
112716,486759.0,"PCA does not discard variables - it tries to squash all the original variables into a new space. The first PC is the strongest, followed by the remaining PCs. Ys there are clues in the co-efficients when you print out the PCs via the components_ vector/matrix vs features vector; but as we saw in the PCA assignment each PC is affected by several features, not just one (based on the value between -1 and 1) - so I don't think you can so easily use PCs to find which cluster is most aligned by the 'Converted' vector - it is a worth a try maybe. So let's say we find, via PCA, that Country and City are the most important features -&gt; how does that help? how do we know that those two features explain say 80-90% of the variance in 'converted'. That is what we are trying to do right? Trying to find features that are most strongly correlated to 'Converted'. There are ofcourse ways to achieve this via PCA .. But .. there are much better ways to solve the assignment and get full marks ;-) (joking)",300694.0
110953,477717.0,"When we do SVD, it gives us three matrices U, S and V. Themes is what we interpret from U matrix. Number of columns of U gives us the number of Themes.",318329.0
110953,477911.0,"Hi Rishi, Same has been discussed here: https://learn.upgrad.com/v/course/208/question/110774 You can go through this specific example. This may be helpful: http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm",311117.0
110648,476339.0,"I would suggest you use the following command to represent as diagonal matrix. np.diag(s[:3]) Maximum comes out correctly. array([[12.72792206, 0. , 0. ], [ 0. , 10.57703788, 0. ], [ 0. , 0. , 8.84826058]]) You will get this in the diagonal matrix format,",301121.0
110648,476390.0,"s returned by np.linalg.svd() is a 1D array containing the weights of the PCs. It can be converted ino a diagnol matrix by using np.diag(s[:3]). Since we are interested only in the first 3 PCs, hence s[:3]. The max weight is the first element of the diagnol.",304319.0
109891,474099.0,"Hey Jetendra, Not exactly this will clear your doubt, but it may help you to some extent: https://valiancesolutions.com/variable-reduction-an-art-as-well-as-science/ https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial And, another thing, if you want to find out whether or not the variables are correlated with each other, all you can do is plot a simple heatmap which potrays the correlation of different variables. Hope this helps!",301655.0
110141,474926.0,Below link will help to understand which approach are available.The idea behind is to capture the maximum variability in the data. “An Approach to Choosing the Number of Components in a Principal Component Analysis (PCA)” by Arthur Gonsales https://link.medium.com/cSqWc9uWgU,318476.0
110141,475110.0,Ranjana when we derive pc1 it captures maximum variance in the data and is defined in terms of all the orignial p dimensions. Next we find the variance that is orthogonal to pc1. Suppose you realize the residual variance is 0 or close to 0 then you may decide to stop at pc1. It means we started with say p=2 or 3 then we end up with only 1. So dimension is reduced. But none of the original variables or dimensions are ignored as pc1 is linear combination of the original features. Hope this helps.,311857.0
110689,476516.0,This might help https://medium.com/data-science-group-iitr/singular-value-decomposition-elucidated-e97005fb82fa,310974.0
110689,476529.0,Singular Value Decomposition (SVD) tutorial: It is explained here with calculation with mathematical explanation: http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm This may help you.,311117.0
110426,475569.0,"Yes in some sense you are right but the principles used to find one are different. In finding best line we stop at finding one line but here, we try to find different lines (to put it in a crude way) which are perpendicular in nature such that their determinant is zero hence uncorrelated.",310974.0
110426,498303.0,"Chetan, The idea, according to my understanding, is that we need to capture as much variance in data as possible in all possible directions. For Ex: in case of 2D grid, if we select the best fit line , acc. to your question, we might explain the variance in only one possible direction. This is where PCA comes into play, we select components such that both of them are independent to each other. Here independent means that the dot product between two or more components should be zero. Hope this helps",302735.0
110024,474409.0,"We have only information in the question that "" the distance of the point from the origin is 5 "". No information about which coordinate it is. it may lie in any of the coordinate. So it is true that "" there are four possible coordinates for this point "", i.e. (5,0),(-5,0),(0,5),(0,-5). And the question is asking, which is not true. Hence, the third option.",311117.0
110024,475193.0,"The co-ordinates can also be (3,4), (3,-4), (-3, -4), (-3, 4) and similar combinations with 4 as x and 3 as y. All these points will also have distance as 5 units from the origin (using Pythogoras theorem). So there are multiple ways to represent a point with distance of 5 units from the origin in the two-dimensional co-ordinate system. Hence the option stating that there is only one way for such a representation will be false.",310505.0
110438,475675.0,You can use np.diag(s[:3]) to print first 3 diagonal components of the s.,320073.0
109864,473675.0,I beleive ... from sklearn.decomposition import PCA,312479.0
109864,473682.0,"Just found it out. No specific library important is required. I removed the ""Name"" column and it worked fine. Thanks for your reply Anuj.",312942.0
109864,476950.0,Numpy and Scikit Learn has been used for examples.,308440.0
109900,473877.0,"You can take the vectors to form a matrix and check its determinant (a11*a22- a12*a21). If the determinant is non zero, then the vectors are linearly independent . Otherwise, they are linearly dependent . For example, please go through this maths link: https://math.stackexchange.com/questions/412563/determine-if-vectors-are-linearly-independent",311117.0
109900,474097.0,"Hey Bindu, May be this video below can help you out: https://www.youtube.com/watch?v=vJclTPm3ofM Hope this helps you to understand better!",301655.0
109900,475358.0,"you can take the vectors to form a matrix and check its determinant. If the determinant is non zero, then the vectors are linearly independent. Otherwise, they are linearly dependent.",318322.0
110637,476649.0,Yes we definitely need a session on PCA for better understanding.,318448.0
110637,476735.0,Yes it will be definitely helpful,308644.0
110637,476701.0,We are having one on coming Wednesday. Please add your doubts here so that they can be discussed in that session.,313517.0
110637,476433.0,Thanks Vipul for initiating the request. I second that.,302742.0
110637,476845.0,Yes we definitely need this,305650.0
111982,482674.0,"Hi Rakesh, Please see the below thread, it might give some clarification: https://learn.upgrad.com/v/course/208/question/111169",305652.0
111982,482713.0,"Do we need to get the absolute Values in this analysis. I think values given are sufficient for analysis. However, TA may verify this.",311117.0
117354,507139.0,Try this https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2?source=rss----7f60cf5620c9---4 https://www.ics.uci.edu/~djp3/classes/2007_04_02_CS221/Lecture20/paperMarlin.pdf https://www.stat.washington.edu/people/pdhoff/public/cmutalk.pdf,310974.0
109849,473872.0,May I please know the solution for this?,303670.0
109849,473629.0,got it.,300718.0
109849,475320.0,"I dropped the column Name, but still getting the error. Am i missing something? --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-25-72a67319b6cd&gt; in &lt;module&gt;() 1 from sklearn.decomposition import PCA 2 ----&gt; 3 u, s, vt = np.linalg.svd(data, full_matrices=True) ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\linalg\linalg.py in svd(a, full_matrices, compute_uv) 1442 1443 signature = 'D-&gt;DdD' if isComplexType(t) else 'd-&gt;ddd' -&gt; 1444 u, s, vh = gufunc(a, signature=signature, extobj=extobj) 1445 u = u.astype(result_t, copy=False) 1446 s = s.astype(_realType(result_t), copy=False) TypeError: No loop matching the specified signature and casting was found for ufunc svd_n_f",316036.0
109849,476107.0,"u, s, vt = np.linalg.svd(data, full_matrices=True) data should be in the form or array Try this way df1 = df.drop('Name', 1) X = np.array(df1) then use this X in svd first argument.",312019.0
110441,475616.0,"As per the question, point P = lies in the basis space, and the basis matrix is given as A matrix. Hence, Multiplying Matrix A with vector P (⅗ *5 + -⅘ *0, ⅘ *5 +⅗ *0)=(3,4) transforms the vector in the conventional reference frame as the matrix itself has been represented with respect to conventional reference frame.",311117.0
110441,498277.0,"Chetak, if you look at this video https://www.youtube.com/watch?v=P2LTAUO1TdA The concept is more like a frame of reference. Point A is in another frame of reference with different set of basis vectors whereas In our, conventional system [1,0,0,1], we know point A as point P. Therefore, to convert point A from one frame to reference to ours. We need to multiply Matrix A with P i.e. (⅗ *5 + -⅘ *0, ⅘ *5 +⅗ *0)=(3,4) The resulting point (3,4) is point A in our frame of reference. Hope this helps.",302735.0
111013,477853.0,"You need to have only numeric values as part of the data that is being passed to the np.linalg.svd() function. Please check the datatypes present int he "" data "" dataframe. I reckon that a column by name "" Name "" is present in the dataframs which is causing this error. You need to drop this column and then proceed.",313826.0
111013,477921.0,"Please drop the name first before trying the command for svd df = df.drop(['Name'], axis=1) u, s, vt = np.linalg.svd(df, full_matrices=True)",301121.0
141903,612840.0,enumerate: This method adds counter to an iterable and returns it (the enumerate object). Annotate: https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.annotate.html,311466.0
141903,613326.0,"enumerate() provides a enumerate object with the first item being the index of the iterable element and the second item being the iterable itself. For example if the value contained in pcs_df.Feature is ['PC1', 'PC2'], then for i,txt in enumerate(pcs_df.Feature): print(i,txt) 0,'PC1' 1,'PC2' plt.annotate() is used to write a certain text at a particular x,y co-ordinate .",313826.0
110736,476730.0,"One way of deciding the classification could be ""Multi-class"" and ""Mulit-label"", and could be solved. Steps for solving with this classification is mentioned here in the link (pls refer point 4 and further): https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/ and for Probability Estimation for Multi-Class Classification based on Label Ranking, can be read from here: https://pdfs.semanticscholar.org/3fde/0a33bbd1821372d67a6c6051b01367435461.pdf Hope this will help.",311117.0
110661,476391.0,check the correlation among the various numerical value you have in your dataframe.plotting the correlation plot will help to get the values .check which two varibles are highly correlated.,318476.0
110288,475088.0,"""SVD"" is generally used for data compression(like image processing and compression) in various fields, through the matrix factorization method. It is a better way than the "" Fourier Transform "" and "" Wavelet Transform "" methodology. The method for matrix factorization is given and explained there as, A= U S Vt. The details and intuition behind this technique can be seen in the attached link: https://medium.com/data-science-group-iitr/singular-value-decomposition-elucidated-e97005fb82fa The difference from Fourier and Wavelet transform can be seen here: https://slideplayer.com/slide/12520377/",311117.0
110288,475165.0,"In the later example, we were explained about Indian Food, Chinese Food, Japanese Food. Each of it is considered as themes. Likewise, it can be applied to the other patterns that SVD would pick for different Datasets",311160.0
110288,475938.0,Themes can be referred to as interpretation of the dataset. Its like grouping of the various menu items into cuisines in the same example given by Harsha above.,308435.0
112027,482932.0,"Hello, PCA ouput will be linear combination of original features / variables. After performing PCA with number of components , feed the ouput to clustering (no need to analyse the PC's back to original columns). Once clustering is performed and cluster ids are formed , then you need to map the ouput to original dataset. Once it is done then you can analyse the resultant dataset for patterns.",305652.0
112027,483047.0,"So you have decided the number of principal components on basis of cumulative explained variance, using the cutoff as 80 or 85%. All the columns (principal components that were selected by you, 3 or 4 or whatever) need to be considered for clustering algorithms like K-Means or Hierarchical clustering.",301652.0
110491,475748.0,"Yes, even I experienced the same.",301655.0
110491,475789.0,"Yes, the sync should have been done. It is affecting to understand the concept properly.",317991.0
110491,475899.0,"Correct, even i have experience the same",316036.0
110491,475933.0,"Yes, I too experienced it. Wondered whether it was happening with my internet connection. But seemingly, its happening for all. I hope they fix this soon",308435.0
110491,476677.0,TA please raise a ticket to fix this,301115.0
110491,476815.0,"Hi, for the time being, please go through the Jupyter notebook. We've arranged a live session on Wednesday where we would go through a walkthrough of the code.",313517.0
110491,476887.0,"Yes the video and slides are not in sync, This is really bad on quality.",318814.0
110491,476932.0,Same issue. Not able to understand the code.,310210.0
110829,477104.0,"Random_state is used as seed for pseudorandom number generator in scikit-learn to duplicate the behavior when such randomness is involved in algorithms. When a fixed random_state, it will produce exact same results in different runs of the program. So its easier to debug and identify problems, if any. Without setting the random_state , different seeds will be used each time that algorithm is run and you will get different results. It may happen that you may get very high scores first time and can never be able to achieve that again. Now in machine learning, we want to replicate our steps exactly same as performed before, to analyse the results. Hence random_state is fixed to some integer. . Please access the link given below https://stackoverflow.com/questions/43321394/what-is-random-state-parameter-in-scikit-learn-tsne",301121.0
110835,477161.0,"Normalization rescales the values into a range of [0,1]. This might be useful in some cases where all parameters need to have the same positive scale. However, the outliers from the data set are lost. Standardization rescales data to have a mean (μ ) of 0 and standard deviation (σ ) of 1 (unit variance) You have to use standardization method of Scaling and centering",301121.0
110835,477390.0,"Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1. Normalization transforms your data into a range between 0 and 1. MinMax Scaling can create problems when there are large outliers in your data. Suppose your max value is very high. Then rest of the data will get squeezed into a small interval near Zero after MinMax transform. But MinMax gives you a guaranteed bounded data after transform which means all data will be within 0 and 1. Also, you don't get any negative values in MinMax scaling. Some algorithms needs such bounded data and only positive values to perform better. For more details with example and graphs, pls refer link : https://stackoverflow.com/questions/32108179/linear-regression-normalization-vs-standardization",311117.0
110621,476244.0,"Yes. Y ou can use PCA or Factor analysis by using SPSS and SAS also. There are so many options of rotation like Varimax, Promax, Quatrimax etc. and in both the software it is easy to use. In SPSS it's very easy. If in values there is a big difference then log transformation you can use otherwise no need. you can make data normal but there is not any compulsion. For a detailed understanding, please follow the link: https://www.researchgate.net/post/How_do_I_use_principal_component_analysis_PCA_for_mixed_data A pdf extract is also attached: chp%3A10.1007%2F0-387-22751-2_10.pdf . it will give more clarity.",311117.0
110876,477378.0,Could you mention the specific Question?,311117.0
110876,477890.0,"The answer should be (1,0) since the width remains same and only height changes which affects the height as well as diagonal.",301121.0
110876,477846.0,Which of the three vectors will not change its magnitude after this transformation of space?,308434.0
110876,477903.0,"Yes. (1,0) will be the correct answer. As in the Question ""Space is vertically scaled, which transforms the square into a rectangle with double the height of that of the square, though the width remains the same"". Hence, (0,1) as well as (1,1) will be scaled.",311117.0
110926,477561.0,So that when you use the .max function.. It doesn't capture the diagonal entry (value 1)..,316349.0
111219,478960.0,The code seems to be correct. Try re-running from the start.,313826.0
111219,479859.0,"Hi Pranav, Check randomized spelling. ""z"" should be used; not ""s"".",334535.0
111000,477843.0,"Had encountered the same and after googling found that adding extra two lines helped in generating the 3d plot (sorry do not have the link on which I found the information). Please find below the complete code: %matplotlib notebook %matplotlib notebook %pylab from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(figsize=(8,8)) ax = Axes3D(fig) # ax = plt.axes(projection='3d') ax.scatter(df_train_pca[:,2], df_train_pca[:,0], df_train_pca[:,1], c=y_train.map({0:'green',1:'red'}))",313826.0
111000,478492.0,There are a few ways to solve this -&gt; 1. put %matplotlib notebook twice - like Vinay has mentioned above - this did not work for me 2. remove any matplotlib inline statements - this worked for me 3. try putting %matplotlib notebook right in the start of your code before any package imports even In all cases make sure you restart the kernel,300694.0
110542,475870.0,"Nope, I got the right answer.",310974.0
110542,475934.0,Yes I too got the right answer.,308435.0
110542,475951.0,I went through the code again and identified the mistake.,313826.0
110542,478021.0,I get the the right answer,305650.0
111041,479106.0,"Hi Harshal, Overall the snippet looks okay. A few inputs: 1. Check whether optimal hyperparameters are correctly chosen using cross-validation. 2. Check whether Pipelining of PCA and the classifier is done correctly, 3. Check whether optimal 'n_components' are chosen.",334535.0
111051,478099.0,How does Centering make a difference in PCA (for SVD and Eigen Decomposition): It is explained with an example in stats.stckexchange.com (in below link): https://stats.stackexchange.com/questions/189822/how-does-centering-make-a-difference-in-pca-for-svd-and-eigen-decomposition. Maybe helpful.,311117.0
111051,478194.0,it means you're standardizing your data so that mean is centered with standard deviation of 1.. (x-xmean)/sigma this brings your complete dataset on the same scale which helps in giving an equal weightage to all the attributes..,316349.0
111051,478490.0,"we have centred data many times in the course, with PCA being the latest one (we also did it in Linear and logistic regression). Basically you scale the data such that it has a mean of 0 and Standard Deviation of 1. By doing this all your data/vectors are in a similar scale while still maintaining the variance betweem each row. We do this because we don't want the units of the features to have an influence on how much influence the feature has on the model. So say you are plotting Price of a house vs bedroom. Price could be in the 100,000,000 while bedroom will usually be between 0 and 5 - so it would not be a fair - if you drew a scatter plot you would see that for each additional bedroom maybe the price jumps up 100,000 -&gt; this is not good.",300694.0
111051,478312.0,"PCA is based on seeing the high variance. If we do not standardize and center them, PCA will misunderstand the variance due to units (like km and cm) and will give out absurd results. We should standardize the variables before applying PCA because it will give more emphasis to those variables having higher variances than to those variables with very low variances while identifying the right principle component. Let’s say your data set has variables with different unit like one in KM and another one in CM (centimeter) but both have same change in value so here for variable in KM will reflect minor change where as another one will have higher change. In this case, if we do not standardize the variable PCA will give higher preference to centimeter variable. If variables of data-set have same units of measurement and values may lie in the range of 70-130 for one variable but for other one in between 2-8 for all the records. Here PCA will give more weight to the first variable. It is a good idea to standardize and cener them.",301121.0
111082,478282.0,"Random_state is used as seed for pseudorandom number generator in scikit-learn to duplicate the behavior when such randomness is involved in algorithms. When a fixed random_state, it will produce exact same results in different runs of the program. So its easier to debug and identify problems, if any. Without setting the random_state , different seeds will be used each time that algorithm is run and you will get different results. It may happen that you may get very high scores first time and can never be able to achieve that again. Now in machine learning, we want to replicate our steps exactly same as performed before, to analyse the results. Hence random_state is fixed to some integer. . Please access the link given below https://stackoverflow.com/questions/43321394/what-is-random-state-parameter-in-scikit-learn-tsne",301121.0
111082,478529.0,"The same has been discussed here : https://learn.upgrad.com/v/course/208/question/111126 Producing again, maybe helpful: 'random_state': means that everytime you run it without specifying random_state , you will get a different result, this is expected behavior. If you put 'random_state=some_numbe r' , then you can guarantee that the output of Run 1 will be equal to the output of Run 2 , i.e. your split will be always the same. It doesn't matter what the actual random_state number is. The important thing is that everytime, you will always get the same output the first time you make the split. This is useful if you want reproducible results. For more understanding, please refer: https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn",311117.0
111082,478486.0,"when we ask a random number to be generated - it is exactly that RANDOM. But sometimes we might not want random to be so random because we need some solid basis for comparing between the models that are built on top of randomness so we use random_state to have some certainty about the randomness. So as an analogy ... say you have 100 marbles and are asked to pick 20 randomly - so each time you will probably get 20 different marbles more or less. Alternatively, imagine you put 20 random marbles in a bucket (bucket 1); then another 20 random marbles (out of remaining 80) into another bucket - so like that 5 buckets. Then as part of your experiment you can say I want bucket number ... 3, or 4; the marbles are random in each bucket but you can have some basis for comparing [different models] if you pick the same bucket each time.",300694.0
111082,478283.0,"refer this for TA varified answer for the same question: https://learn.upgrad.com/v/course/208/question/110829 I m pasting the answer for your convenience: Random_state is used as seed for pseudorandom number generator in scikit-learn to duplicate the behavior when such randomness is involved in algorithms. When a fixed random_state, it will produce exact same results in different runs of the program. So its easier to debug and identify problems, if any. Without setting the random_state , different seeds will be used each time that algorithm is run and you will get different results. It may happen that you may get very high scores first time and can never be able to achieve that again. Now in machine learning, we want to replicate our steps exactly same as performed before, to analyse the results. Hence random_state is fixed to some integer. . Please access the link given below https://stackoverflow.com/questions/43321394/what-is-random-state-parameter-in-scikit-learn-tsne",302738.0
110697,476545.0,"If linear regression assumes independent predictors (an assumption often violated in practice), why is PCA not a standard step in the procedure? https://www.quora.com/If-linear-regression-assumes-independent-predictors-an-assumption-often-violated-in-practice-why-is-PCA-not-a-standard-step-in-the-procedure",318451.0
110697,476559.0,"PCA does completely different thing to logistic regression. PCA is not a substitute for logistic regression. You could actually even use them together. PCA can be used to remove dimensions that have strong correlations before PCA transformation. PCA has actually a problem that it only looks into the dimensions, not into categories and for that reason there is linear discriminant analysis (LDA). For more depth understanding, please follow this link: https://stats.stackexchange.com/questions/244677/how-to-decide-between-pca-and-logistic-regression https://www.researchgate.net/post/Can_we_do_PCA_before_logistic_regression",311117.0
113244,488435.0,Use the following code df1 = df[‘column_name’] df1 = df1.to_frame(),302738.0
113244,489198.0,you can use new_df = original_df[['column name]],318756.0
110558,475968.0,"The kaggle webpage mentions - ' The test data set, (test.csv), is the same as the training set, except that it does not contain the ""label"" column. Your submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. ' The labels in the train set are for us to find accuracy and verify for ourselves if our predictions are correct. Then we apply it on test set to make predictions. These predictions are to submitted in the said format to be evaluated by kaggle.",311857.0
111908,482068.0,"The PCA perfoms certain internal steps based on which you get the principal components. Principal components are made out of original features and all these principal components are expected to beindependent of each other or in other words have very low iner-dependency. In effect PCA, PCA is like black box where you feed in the features and get the principal components but there is no need to get the feauture name by which these principal components are made of. You can work with these principal componenets for creating clusters. I hope I could I express what you required.",301121.0
111908,482334.0,"Thank you for your explaination on PCA. Without knowing the features list, how do you drop the features and move on with clustering. My understanding is we have to do PCA. We will get the important features. Drop those features and perform K-mean and Herachi clustering. From clustering ma the countries based on features mapping, conclude the development need for a country. Can you please clarify why we are not bothere about features names?",303666.0
111108,478439.0,Try this https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/,310974.0
111108,478469.0,"I suggest you try this site too. A Beginner's Guide to Eigenvectors, Eigenvalues, PCA, Covariance and Entropy https://skymind.ai/wiki/eigenvector",301121.0
111108,478483.0,"As I read somewhere (https://www.youtube.com/watch?v=PFDu9oVAE-g), and totally agree. Eigenvectors and Eignvalues are very very simple concept. The problem is that you need to have studied the pre-req - matrices, linear transformations, spans, basis, etc. Once you have a good grasp of these topics then Eigenvectors and Eigenvalues are simple otherwise there will always be something foundational missing and that will trip you up.",300694.0
111108,479707.0,Thank you,311803.0
112070,482995.0,"Hi Venkat, Yes , we need to remove the 'country' variable while performing PCA. Only numerical variables need to be feed to PCA. Please verify this approach: https://learn.upgrad.com/v/course/208/question/111624",305652.0
110619,558742.0,"Hi, PF the assignment link here. https://learn.upgrad.com/v/course/208/module/10555",301618.0
111419,479994.0,Yeah the whole idea is to work with PCA variables instead of the RFE ones right.,310974.0
111745,481349.0,You may get the good understanding in today's Webinar. I would suggest to go through once the recorded session.,311117.0
111745,481329.0,"Since Principal Components is a projection of all original values, you can do clustering based on PCs geberated by PCA,",301121.0
111745,481505.0,We need to use only the Principal Components for Clustering purpose. Take a look at the suggested approach provided by TA : https://learn.upgrad.com/v/course/208/question/111624/answer/481465,313826.0
111617,481668.0,"Hi Neha, You need good amount of RAM, around 12gb+ to run. Check whether you can run on alternative laptop. Also check out below thread. https://github.com/tensorflow/tensorflow/issues/6019",334535.0
112093,,nan,
141905,612805.0,The correlation matrix is made on columns by default so you have to transpose according to the desired output you want. if the data on which you want to create a correlation is in rows then you need to transpose.,311466.0
140144,605705.0,Clustering can be done with any number of features.Each cluster can have different means / centroids for the features involved.,318476.0
140144,605947.0,"Clustering can be performed with multiple attributes. Actually it can be done using same concepts as you lerned as a part of clustering module in predictive analysis. You may use any algorithms, like k-means, to form clusters.",301560.0
140289,606016.0,Yes it's an important criteria but if a user don't have any credit card or loan then banks looks for salary statement and income tax filing for the person.,318476.0
140289,608260.0,"Cibil score can be 0 if - You do not have a credit history or you do not have enough of a credit history to be scored, i.e. you are new to the credit system You do not have any credit activity in the last couple of years You have all add-on credit cards and have no credit exposure In such cases, banks may still issue loan on the basis of last six month bank statements, your employer history, IT returns but it would be at a slightly higher interest rate depending on the case",316147.0
141058,,nan,
141082,609477.0,"Options are correct. You need to take 10% for affluant customers and calculate cost and revenue for them. You should get the answer. As it is graded question, I would not be able to help more.",301560.0
141358,610815.0,They might be correct values or could be data discrepencies. We are not having information on how exactly these values are calculated. You may take necessary assumption and use these information to build your model.,301560.0
141174,610619.0,"Exactly, the materials are a mess..Many places there is no sync with videos and the actual results.",318756.0
141174,610414.0,"Hi TA, Can you please confirm this? It is confusing &amp; also did not understand why was the section placed before even building a model? Video says 'Random Forest is not a good model' &amp; notes says 'Random forest is a better model'",312093.0
141174,610796.0,Issues mentioned with the videos have been raised with concerned team. It should be rectified soon. You may still report it using the link provided at the bottom of the page wherever you find any issues.,301560.0
140532,608242.0,Please report it using the link at the bottom of the page.,301560.0
140714,608940.0,"Here overlap means customers who are contacted in previous campaigns as well. Here customers are contacted in more than 1 campaigns. If you see the first column for first 2 rows, there are &gt; 1500 customers who were contacted in previous campaigns.",301560.0
140806,608316.0,"Hey Shefali, Refer this article to understand the use of cumulative gain &amp; lift curve for measuring the performance of a Marketing Campaign. https://select-statistics.co.uk/blog/cumulative-gains-and-lift-curves-measuring-the-performance-of-a-marketing-campaign/ Hope this helps.",302742.0
140776,608403.0,"Python scikit-learn provides a Pipeline utility to help automate machine learning workflows. Pipelines work by allowing for a linear sequence of data transforms to be chained together culminating in a modeling process that can be evaluated. The goal is to ensure that all of the steps in the pipeline are constrained to the data available for the evaluation, such as the training dataset or each fold of the cross validation procedure. Source: https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/ You can also visit below link for more insight on Pipeline. https://www.quora.com/What-is-a-pipeline-and-baseline-in-machine-learning-algorithms https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976 Hope this will help.",317991.0
141727,613465.0,This is a mistake which is raised with concerned team to correct. Please consider logistic as best model.,301560.0
140151,608711.0,"Yes. It is not clear how they map the macroeconomic data to individual-level analysis and how do these attributes impact the call campaigns and prospect conversion, Please clarify.",311115.0
140151,607398.0,"I am guessing these are macro economic indicators on the day the call was made. Hence it is changing across customers. As the month of call is changing, macro economic indicators are changing",317996.0
140151,605971.0,This data is maintained by credit agencies like CIBIL. Banks generally receives credit history data from such agencies who maintains individuals' credit information based on past credit history.,301560.0
140151,608251.0,Macroeconomic data is the aggregated data relating either to sectors of the economy or to the economy as whole. Over here it seems to be mapped to an individual on the basis of when the prospect was approached in the campaign thereby to relate the prospect's response with various economic parameters during that timespan,316147.0
141665,612310.0,"Hi Rex, Below link provides good explanation on Imbalance class. https://elitedatascience.com/imbalanced-classes",308673.0
141665,612403.0,"- imbalance class are categorical variables, mostly target variables in classification models, which predominantly have only one category value. For example, in fraud detection dataset, you may have &lt;1% of records with fraud detected. - Such imbalance classes make it difficult to get derised accuracy , sensitivity and specificity for your model. You may get very high accuracy as all cases might be predicted as one category only, in above case, as non-fraud. - You can use different sampling techniques to make it balanced class.",301560.0
141667,612408.0,params_grid has the parameters used to tune random forest model. You would have learnt about these parameters in the sessions for random forest. You can start with the one provided in the lecture video and then try to change some of the values to learn how it affect your model performance.,301560.0
141651,612398.0,There is a mistake in the content which is raised to the concerned team to get it corrected.,301560.0
141348,610623.0,"The C parameter in logistic regression is not cutoff. It is an inverse regularization parameter, smaller values of C specify stronger regularization. Refer the scikit-learn documentation for the same : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html",318756.0
141378,610788.0,"Generally if there are duplicates in dataset, it is recommended to remove duplicate records. So, yes you can remove duplicate records if present. Hope this will help.",317991.0
141136,610690.0,"Assuming the cost of call in $1 per minute, you can calculate the cost as Cost = Duration/60 +1 Hope this helps",306725.0
141136,611539.0,You can assume call rate per second as well.,314621.0
141136,611038.0,"TA, please verify. Can we take any assumptions regarding the cost of the call as it is not given in the problem statement?",304319.0
141397,610692.0,You can copy the EDA from existing notebook provided,306725.0
141397,610783.0,As mentioned in the checkpoint Perform data preparation (no marks are awarded for this step) You can use the code provided in the lectures to complete all the data preparation steps We can even use code for data preparation. And assignment actually starts from checkpoint 2. Rest TA can verify.,317991.0
141160,610849.0,"As mentioned, we need to create it ""Note: Before starting the assignment, you may find it helpful to create a unique ID for each prospect. """,318448.0
141160,610099.0,I think we need to change the index of the dataframe or make a new column to number every prospect.,312731.0
141160,611099.0,Add a new column for prospect ID which would be an incremental column that would be unique per record. This ID will be helpful for creating one of the dataframe mentioned in checkpoints,316147.0
141390,610755.0,I guess we have to submit pdf file alongiwth the jupyter notebook. So a single zip file containing both of them.,318756.0
141390,610852.0,"If we need to submit PDF, then what needs to be mentioned in that ? TA please confirm whether we need only notebook or pdf or both?",318448.0
141390,611480.0,"Its mentioned at top in the Checkpoint "" Also, you only have to submit a Jupyter Notebook for this assignment, and so, you have to report some of the metrics as comments in the Jupyter Notebook."" A So we need to submit Jupyter notebook and pdf file for task 2.",310179.0
141390,610777.0,As per the rubric we have to perform two task. Task 1 - will be evaluated based on the provided Jupyter Notebook. Task 2 - will be evaluated based on the pdf file submitted. So we have to submit zip file containing Jupyter Notebook and PDF file.,317991.0
141340,610697.0,"qcut divides the data set by percentile calculation. if you choose to divide your data set in 10 bins, qcut will separate the data into bins according to [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] percentiles. You can validate the numbers by doing a describe on the column and checking the values for various percentiles. Hope this helps.",306725.0
141440,610940.0,"The ""duration"" column should not be considered only during the process of building the model and later consider it again when creating the final dataframe. Usually the business would provide details on the cost of a standard call (say of duration 60 seconds) based on which the cost of the call can be derived. However, in absence of such details, certain assumptions can be made regarding the cost of making a standard call (like 60 seconds call would cost say $1) and based on this assumption calculate the cost of the call.",313826.0
141339,610699.0,i believe this should be on complete data set rather than being on test or train.,306725.0
140722,608958.0,"I hope you understood about sorting in the order of decreasing probability. For finding top X%, it is given in problem statement that you need to find top 80% responders. Please read that part of problem statement thoroughly to understand the requirement.",301560.0
140838,,nan,
140604,608245.0,You can decide to drop any variable which you may think is not useful for analysis. Please make sure not to drop any variable which might be useful in predicting target variable.,301560.0
140884,610103.0,"Since specifying important predictors is not the goal, we can also go ahead with PCA, I think.",304319.0
140884,608997.0,"During the span of this course so far, we learnt various method to select variables. Some are Forward Selection : The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set. Backward Elimination : The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set. Combination of forward selection and backward elimination : The stepwise forward selection and backward elimination methods can be combined so that, at each step, the procedure selects the best attribute and removes the worst from among the remaining attributes. Recursive Feature Elimination : Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. You can use any of these methods to select variables. You can also visit below link: https://hub.packtpub.com/4-ways-implement-feature-selection-python-machine-learning/ Hope this will help.",317991.0
140884,609077.0,"As you are building logistic regression model, you need to follow same steps you did in logistic regression exercise. The way you select/reject variables one by one till you get model with all significant variables.",301560.0
141273,610283.0,Yes thanks.,304319.0
141273,610223.0,"We will have to determine optimum cut-off point. In problem statement below line is mentioned. ""Find the optimal probability cut-off and report the relevant evaluation metrics""",317996.0
141279,610268.0,Yes you can. You can choose whichever method suits you. There’s no hard and fast rule regarding that.,302738.0
141279,610809.0,It would be better to go with any variable selection method as it is part of evaluation rubric.,301560.0
141279,610306.0,"Yes you can. You can use whichever method suits you. There is no hard and fast rule regarding that. moreover, you can check following link for further clearity: https://learn.upgrad.com/v/course/208/question/140884",302738.0
141816,613554.0,"Employment Variation Rate: Rate at which employment varies for various employees which may include cases like taking another temporary responsibilites other than usual work, moving to higher position or moving to another work in same company. Consumer Price Index: changes in the price level of market basket of consumer goods and services purchased by households. https://en.wikipedia.org/wiki/Consumer_price_index Consumer Confidence Index: an index by The Conference Board that measures how optimistic or pessimistic consumers are with respect to the economy in the near future. (Mostly related to US market) Euribor 3 Month Rate: Euro Interbank Offered Rate, rate at which Euro interbank term deposits within the Euro zone.",301560.0
141502,611240.0,"The ""duration"" attribute should not be considered only during the process of building the logistic regression model. Not sure about the ""sought"" function that you have mentioned. Can you elaborate more as to what you mean by that?",313826.0
141502,611462.0,Duration should be dropped before building the model. Ideally you should take a backup of dataset before dropping as you would need this for calculation of cost. Sorting will be done when you predict probability of response using your model.,301560.0
141503,611246.0,"The cost of call would help in determining the cost of acquisition of X% of customers. This would further help in determining the Gain &amp; LIft when compared to the cost of acquisition when no model is being applied, thus helping in understanding the reduction in acquisition cost and improvement in response rate.",313826.0
141503,612225.0,cost should be duration*no of contacts. You need to find cost of each customer which would help to find total cost as well as the cost needed to contact only top X% customers. This will help you to find cost savings using your model.,301560.0
141825,613568.0,"Employment Variation Rate: Rate of employment variation for employees which may include moving to higher position, taking up another role temporarily or permanently etc. Consumer Price Index: measures changes in the price level of market basket of consumer goods and services purchased by households. https://en.wikipedia.org/wiki/Consumer_price_index Consumer Confidence Index: Degree of confidence consumer is showing in current and future economy which can be seen in the trend of consumer spending. Euribor 3 Month Rate: Euro Interbank Offered Rate, rate at which Euro interbank term deposits within the Euro zone. Number of employees: no of employees in organization",301560.0
141507,611249.0,We have to use the same bank_marketing.csv dataset that was used during the Acquisition Analytics Lab.,313826.0
141514,611627.0,"But in the lecture and sample jupyter notebook provided, the lift/gain is calculated on the test dataset only. Does it matter how much dataset we are using? As we are dealing with percentages.",304319.0
141514,611274.0,"As per the checklpoints, the new dataframe with attributes ""prospect ID, actual response, predicted response, predicted probability of response, duration of the call in seconds and cost of the call"" should be created for each prospect , which would mean that we need to consider the whole dataset. Request TAs to verify if the understanding is correct.",313826.0
141526,612230.0,"here you are supposed to find top 80% responders are in which decile of your lift/gain chart. For example, if 80% responders are covered in 6th decile, then your X will be 6 and X% will be 60%, that is, you can contact 60% customers to get 80% repsponders.",301560.0
141527,611773.0,"We have to do RFE and not PCA. As in one of the rubrics, it has specified the final selection of variables is important. Also sorting of dataframe can be done once the deciles are created.",304319.0
141527,612250.0,it would be better to go with RFE as final variables in the model is the part of evaluation rubric. Soring in the decreasing order of probability can be done only after you have the probabilities. You can get it after you predict for your dataset.,301560.0
141642,612396.0,"You can create a new df with all required columns using a normal syntax to create a df. As this is for assignment, I would not be able to help you with how to create or how to get any of the required columns.",301560.0
141853,613334.0,Ensure that you are concatinating the dataframe with dummy variable with the original datafarame and dropping the categorical variable(in this case loan).,313826.0
141571,611601.0,"I believe you encountered an error when trying to install imblearn. If that is the case, then try installing again and it should go through. Also, restart the jupyter notebook / python session before importing it again. Hope this helps.",313826.0
141571,611526.0,You can use confusion matrix to find sensitivity and specificity. Once you create confusion matrix you can use Sensitivity = True Positive / (True Positive + False Negative) Specificity = True Negative / ( True Negative + False Positive) Hope this will help,317991.0
141882,612680.0,"I am also getting the same not sure what caused this. Anyway, you can go ahead with the LR model without PCA.",301648.0
141882,612765.0,Pls check if you have used the newly created prospect id as one of the independent variables. this should not be included .,317575.0
141882,613694.0,"As this is for assignment, I would not be able to check your code. You may check the input you are giving to PCA and verify if it is correct. Just a suggestion that you may want to go with variable selection method instead of PCA as variables in final model is one of the checks in evaluation rubric.",301560.0
141884,613697.0,You can use duration column to calculate cost. It is dropped only for model building.,301560.0
141884,612853.0,Yes. Clarification required on STEP6: Consider cost = 1*number of contacts made in the current campaign; determine the cost incurred for acquiring 80% of customers using the predictive model. The cost calculation is assumed in step3 and again it is contradicted in step 7. TA. please clarify.,311115.0
141884,613329.0,"Please refer belwo link on a similar query on how to calculate cost of call https://learn.upgrad.com/v/course/208/question/141440 In step 3, we are calculating the cost of call for each of the prospects. In step 6, we are calculating the cost of acquisition of 80% of total responders.",313826.0
141660,612359.0,Thank you,308432.0
141660,612241.0,Here we are suppose to find top 80% responders for the campaign .You need to plot the lift/gain chart on the decile dataframe (obtained by sorting predicted probabilities in descending order and doing qcut to get deciles) and check out in which decile 80% gain is achieved. That decile will be your X,316147.0
141890,612704.0,TA has recommended to go with RFE in below link as final variables in the model are important. https://learn.upgrad.com/v/course/208/question/141527 So you can use RFE for selecting variables.,317991.0
141590,611651.0,"by making a plot of accuracy, sensitivity and specificity",318429.0
141529,611654.0,yes,318429.0
141536,611502.0,You can use a conda install. conda install -c glemaitre imbalanced-learn,301648.0
141536,612012.0,Thank you,308432.0
141536,611489.0,Use !pip install imblearn,318802.0
141536,611384.0,I think imblearn package is not installed. You need to install imblearn package before using it. There are multiple ways to install it. Please go through below links for same. https://stackoverflow.com/questions/40008015/problems-importing-imblearn-python-package-on-ipython-notebook https://stackoverflow.com/questions/50376990/modulenotfounderror-no-module-named-imblearn?rq=1 https://stackoverflow.com/questions/47606873/jupyter-no-module-named-imblearn-after-installation Hope this will help,317991.0
141604,612050.0,You can use the index as the prospect ID like this: df['Prospect ID'] = df.index,318448.0
141604,611700.0,"There is no need to extract the prospect id. Instead, we may create a new column Prospect ID which contains a unique value for each of the rows.",313826.0
141915,613345.0,Check whether the number of columns is same in X_train anx X_test.,313826.0
141915,613166.0,Please check the number of rows and columns in X_test and the data you have used to fit your pipeline. There is a problem with dimensions as it looks,301648.0
141835,612549.0,Yes we need to perform standardization on both X-Train and X- Test. In lecture also standardization was performed on both X-Train and X- Test.,317991.0
141835,613332.0,"Standardization has to be performed on bith tarin and test datasets. However, please ensure that when standardizing the test dataset, the mean and standard deviation of the train dataset are used.",313826.0
141887,613348.0,"We can use the same code as analytics lab up until data preparation step, after which we can work on rwst of the checkpoints.",313826.0
141887,612675.0,We just need to add the Prospect ID column and then we can start from Model Building Steps.,301648.0
141912,613344.0,Check whether the number of columns is same in X_train anx X_test.,313826.0
93753,395173.0,"for excel files add \r at the starting of your location from where you're reading your file.. and for csv it is straightforward, just make sure that the encode argument is matching to the one taught in the session for utf-8..",316349.0
93753,395170.0,Please paste the error here.,317689.0
93753,395680.0,Please help me with more detailed question. I am not able to get your question.,329512.0
93731,395000.0,You need to figure this out as this is a graded component in the assignment. Can't help on this.,329512.0
93731,395008.0,Go through the dataframes you can figure it out easily..,303673.0
93731,395584.0,"I renamed the rounds to column company_permalink to permalink .then used pd.merge with inner option, on permalink it makes table bigger. But data is empty. What will be wrong here ?",312019.0
93756,395169.0,You can use set concept to find the uniqueness.,317689.0
93756,395112.0,Get a unique list(or a set) of companies from the rounds2 dataframe and check if each of the entries is present in a unique list(or a set) of companies from the companies dataframe .,313826.0
93756,395166.0,"Refer the set theory session, that will help you.. 'Set' always brings the unique values. you might want to convert the series to set and then take difference..",316349.0
93756,395222.0,"Once case/special characters issue is resolved , use the Pandas Series unique() on the column of rounds2 df NOT IN Pandas Series unique() on the column of companies df",301115.0
93756,395572.0,"companies_permalink_set . Make set from dataframe column permalink of companies df rounds2_company_permalink_set. Make set from dataframe column of company_permalink of rounds2 df then we should do set difference of rounds2 over comapies set. then it prints the exits in rounds2 , not in companies. Please repond is this correct ? Any case sensitive will be a problem ? i see same names capital in txt and lower case in csv . what will happen this case ?",312019.0
93756,395641.0,I used SET and did minus but before that you need to use lower or upper on paralink and company_paralink and then do the minus.,315679.0
93756,395681.0,"Good answer below. You may also use this: rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :]",329512.0
93756,395948.0,Convert the dataframe into set and then use difference,301643.0
93756,396002.0,"Would it be correct to say that if the same company is listed as ABC and abc, it will be counted as 2 rather than 1 unique? If so, do we then treat the column to fix the CASE for all?",319302.0
93756,396157.0,I get empty after doing set difference. something wrong. When i do intersection it gives values. difference its empty. please clarify,312019.0
93628,394556.0,"I found the following stakoverflow link very useful in the case of handling encoding/ decoding issues in input files... https://stackoverflow.com/questions/9973815/python-to-show-special-characters Please check this out. Most likely this will help you resolve your issue. Of course, deleting/ replacing special characters in not the solution.",306250.0
93628,394628.0,Replacing the charecter can be handled without encoding as well. There is a clue in the link mentioned how to handle such cases.,317689.0
93628,394656.0,"Yes you can replcae the special char of different coding to a generic character as NA or ""Ignore"" so that you explicitly know it when doing the further analysis, for more understanding you can always refer https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe",315277.0
93628,395011.0,"Please don't replace the special characters, you need to figure out a way to solve this by using correct encoding. As this is a graded component I will not be able to help more, Hint: removing-special-characters-in-a-pandas-dataframe.",329512.0
93629,394664.0,"I have seen in a post where even the IIITB mentor was suggesting to do data cleansing on the master dataframe and not the individual tables, so as per my knowledge we need to do the data cleansing after merging.",308967.0
93629,394565.0,if Component dataframes are not cleansed then merge is not successful atleast for the join feild and data which comes from second table into first table(we may be missing some data too).,301115.0
93629,394575.0,Issues here is that the join columns (permalink &amp; company_permalink) are absolutely fine once encoding issues and case is taken care. There are no data quality issues otherwise,306250.0
93629,394627.0,"It would always depend on the size of the datasets and the information present in it. If the columns for which the data cleansing is required are not that important in the final analysis than it can be done in the beginning, so that the processing is faster and less storage is used and less shuffling of data. But if the information is important as it might be required at later stages than you need to do it appropriately at later stages.",317689.0
93629,394801.0,"If the columns on which you form the merge require data cleansing, then its best to do cleansing before merge. But if you are doing a left/right merge, its necessary that any null values that arise because of the merge are handled as well. Hope this helps.",310511.0
93629,395608.0,"As per my understanding, First merge the two dataftame then cleaning should be done.",314183.0
93630,,nan,
93636,394635.0,Contact your mentor. He will provide their email ids to you.,312376.0
91393,382173.0,"You can communicate with your group members either through whatsapp or messages or the best possible means of communication you prefer. The case study would need to be submitted as a group. So every member should be involved as I believe the grading would be equal to all the group members and one case study will be submitted per group for evaluation. Details in the mail - For the upcoming Investment Case Study which is due on November 4th, you would need to work in groups of four students. The idea of group case study is to collaborate and network with your peers. It does not matter if your group members are in the same city or across the world. Make use of Discussion Forum and WhatsApp to know your peers. Each group shouldn't include more than 4 members.",317460.0
91393,382840.0,"you can use any means of communications you wish - whatsapp, skype, email, google hangouts, etc etc",300694.0
91393,383070.0,"Use any possible means of communication. You can meet them also, if you guys are in the same city.",329512.0
91393,383253.0,"Ok, Thanks for the confirmation and deatils:-)",320008.0
93634,394663.0,I beleive it means the number of unique entries in the master frame file,315277.0
93634,395459.0,I think one needs to drop the duplicate value in master_frame data frame and give the count of rows.,318344.0
93634,394670.0,Number of rows in your merged dataframe 'master_frame'..,316349.0
93634,394672.0,Here number of observations means number of rows ( or number of records ) present in the master_frame .,313826.0
93634,394791.0,Count of records in your dataframe,310511.0
93634,394867.0,No of rows in masterframe,317811.0
93634,395577.0,Number of rows are present in Master_dataframe.,314183.0
93634,396141.0,"I did merge with inner option, on permalink. It gave me empty table . any clue please. make both column names as permalink after merging columns names shows. but no values are printing. please help",312019.0
92461,389526.0,You can write to mysql tables from dataframe and than plot the charts.,317689.0
92461,389764.0,You can export the Pandas Data Frame and then use it in Tableau,329512.0
92461,391179.0,Since Tableau can accept csv input as well...export csv output (from pandas) might be another option,306250.0
93764,395161.0,anyone can submit..:),316349.0
93764,395187.0,I think it has to be submitted by group leader. It doesn't make sense for 4 members to submit the same assignment.,318329.0
93631,394649.0,"I beleive it depends on the group members, IIIT-B would not be assigning the task, In our group we all are first going thrugh the whole case study for business context and then we will divide the tasks of on based on submission outputs required and in last it would be great if everyone can discuss it part with others",315277.0
93685,394762.0,https://learn.upgrad.com/v/course/208/question/91084 this would give u answer as they are saying as challenge.,301115.0
93685,394870.0,"Yes, in the dataframe imported from csv file, '0' is to be replaced by 'na' e.g. A0lytics will become Analytics. This will correct other entries as well",317811.0
93687,394805.0,The checkpoint description says top 9 countries which have received the highest total funding. So I think you should consider investment amount. Hope this helps.,310511.0
93687,394785.0,"as per the instructions, I think it's only the investment amount that we should consider.",311686.0
93687,395468.0,"How do we select ""English"" speaking countries from the dataframe since there is no language column in it?",300706.0
93687,395248.0,You have to select the top 9 countries based on the total funding amount and the funding type which have been identified as appropriate in Check point2. The last question in checkpoint 2 is the funding type appropriate for Spark.,310629.0
93687,396593.0,We needs to find just top 9 countries based on amount . means same country may repeat head(9) or unique coountries. just 9 first rows ? of top ?,312019.0
93692,394802.0,"If Average investment amount of one out of the given funding types fall between 5 to 15 million USD, then that ft is the most suited one. Hope this helps.",310511.0
93692,394814.0,"out of four funding types (venture, angel, seed, and private equity) Based on the average investment amount calculated above, which investment type do you think is the most suitable for Spark Funds?",301115.0
93692,395847.0,"I dont think this is correct understanding because question states that ""Considering that Spark Funds wants to invest between 5 to 15 million USD per investment round , which investment type is the most suitable for it?"" If you take the average of 4 investment types, then it does not show how much is invested per investment round.",304814.0
93742,395057.0,"Use inner join and convert the permalink to lower case, also you can change the name of column to company_permalink as in rounds2 for better understanding.",303673.0
93742,395060.0,Inner join need to be done,303673.0
93742,395175.0,i guess we should do a left join since the question is asking us to find thw companies that are present in one dataframe and not in other dataframe.. now make sure that you keep which datafrane in what order to get the results.. though inner join also works here since all other resulta arw matching except few..,316349.0
93742,395374.0,"You can use left join, idially we should ne using inner join and not loosing any rows in the rounds2 data frame. As the permalink and company_permalink coulms must have identical values. Using inner join you would be able to validate that your data sets are correct and you must not loose any data rows in inner join. Hope this helps.",306725.0
93742,395677.0,"Use Inner join, You need only those companies that have information in Companies files and have participated in rounds.",329512.0
93696,394817.0,"You could use following to get the output values of float columns as per your preference (change the 'f' value for different precision) pd.options.display.float_format = '{:,.1f}'.format",306250.0
93696,394842.0,you can convert it to millions,301115.0
93696,394857.0,"You will have to set the format for display the float you can set by using pd.set_option('float_format', '{:f}'.format)",317845.0
93696,395183.0,you can just use str method to display as string just for displaying purpose,318329.0
92425,389693.0,Data Analysis is the criticial part of any aplication development project. Infact majority of the project time goes into gathering the requirement and undestanding the data. Once you understand the data/its flaws/relationships etc then you can come up with various techniques to overcome the gaps. One important term is data quality engine.,304814.0
92425,389633.0,"If you recall what was told to us in the CRISP-DM module, "" Data understanding "" and ""Data preparation"" are 2 important stages in the whole data mining process before we begin ""Data modeling"". So, we achieve this partly by inspection (as inspecting the whole data manually is not feasible), checking programatically for nulls, special characters etc. and partly by uncovering through the warnings and errors we get while trying to process it.",300717.0
93708,394868.0,It is clear that it is inclusive of 5 and 15 million.,317811.0
93708,394873.0,"There are multiple wordings around the range like ""Considering that Spark Funds wants to invest between 5 to 15 million USD per investment round, which investment type is the most suitable for it?"" which can be gramatically be understood as either inclusive or exclusive ""Also, the range of funding preferred by Spark Funds is 5 to 15 million USD "" - which is again ambigious. "" Note: In the following table, all the observations refer to investments of the type FT within 5-15 M USD range. "" - here within would meane inclusive. Hence the ambiguity and would want the TA to clarify",313826.0
93708,394927.0,Inclusive of 5 and 15 million,329512.0
93708,395240.0,You can assume 5-15 MM and mention the assumption in the presentation.,310629.0
93743,395174.0,If you calculate after dropping the nulls the average would be same as nulls are not considered in the average. But if you replace it with 0 than the average would change.,317689.0
93743,395063.0,I've calculated with Null and without Null values.. the average funding amount seems to be same!!,303673.0
93743,395678.0,"Drop them , this means those companies have not raised any funds in any round.",329512.0
93709,394883.0,"In my opinion, since in the previous step(Checkpoint 5) we have done analysis wrt Top 3 English Speaking Countries with investment range in 5 to 15M, the tableau plots would also be for the same three english speaking countries. We could wait for the TA to clarify the same.",313826.0
93709,396472.0,"As all the checkpoint are in sequential order and dependednt on the earlier checkpoint, i would say, its a correct assumption.",306725.0
93710,394881.0,Please refer the below related questions: https://learn.upgrad.com/v/course/208/question/93118 https://learn.upgrad.com/v/course/208/question/93265 An inner join during merging should take care of this.,313826.0
93752,395084.0,"There is a pdf file that can be downloaded from the below link. The link for downloading the file is on the last point under the heading ""Business and Data Understanding"" https://learn.upgrad.com/v/course/208/session/19904/segment/101226 You can use this file and do manual inspection to determine English-Speaking Countries.",313826.0
93752,395171.0,Use the pdf provided and based on that determine the country codes of the english speaking countries.,317689.0
93752,395181.0,"We can do below steps 1. Download the PDF which is given by UGRAD. 2.Convert the PDF into WORD ( online ) 3. Once the data is there in WORD , convert into excel . 4. With the help of below site , do the vlookup and create country code &amp; 3 digit code excel http://www.dnb.com/content/dam/english/dnb-solutions/sales-and-marketing/iso_3digit_alpha_country_codes.xls 5.Use this excel and upload as DF",311861.0
93752,396912.0,There is a PDF to referthe english speking countries.Please refer the PDF,311227.0
93752,397897.0,Manual inspection of pdf is good enough. No need to programming to parse pdf.,318458.0
93726,394974.0,"in mapping file, various sectors have been categorised into 8 main sectors. now in your master frame, you must have got the column 'primary sector' for each record by splitting category column. this primary sector column value might be one of those Various sectors of mapping file which have been categorised as one of those 8 main sectors. you will have to get main sector for each record on the basis of primary sector value. hope it is helpful.",311686.0
93726,395299.0,"It’s pretty simple. Based on the primary sector ( after splitting using Pipe) , you need to identify the main sector using mapping file. Note: each primary sector (688 i guess) is allocated to a main sector (8 excluding blank)",304814.0
93727,395463.0,"data cleaning is required , it is mentioned in rubrics as well.",305652.0
93727,394971.0,"from the various discussions on this forum on this matter, it seems that the figures reported without data cleaning will be incorrect. we should do the cleaning first.",311686.0
93727,395009.0,Analysis after data cleaning will give better results.,303673.0
93727,395251.0,where it is mentioned that Null rows need to be dropped or replaced?,304814.0
93727,395627.0,Data cleaning is required before we start the table 1.1,306729.0
93763,395165.0,Use the method given in the stackoverflow page. That page has explained it elaborately.,317689.0
93763,395163.0,follow the stackoverflow link that is provided in the same page of session.. to verify the results just compare the rows with the index in your master dataframe and the snapshot provided in the same session page.. all the special characters are removed or not!,316349.0
93763,395190.0,"If you want to find out what type of encoding is used do the following: 1. Compare the two dataframes, find out what values don't match, that is what values of companies permalink are not in the rounds2 permalink. 2. You will notice that there are 4 Chinese companies and one Portuguese company. They would commonly use utf-8, if you check the link with all the encoding types on the checkpoints page. 3. Use the stack exchange link to re-encode This is the organic way of dealing with the problem, one that you can use for other data sets in real life situations. Also make sure everything is in the same case(upper or lower).",319357.0
93612,394498.0,"Checkpoint 4 is the 'Sector Analysis' where we find the primary_sector and the main_sector . So, we still need to use the master_frame for coming up with these new columns.",313826.0
93612,394853.0,As per the above statement we wouldbe intrested in sector details for only those countries with suited funding type. So new columns should be added to filter data frame or merged data frame(master_frame) TA please confirm,301115.0
93612,395023.0,"For checkpoint3: Filter your data for the investment type you have chosen, then compare the total investment amounts across countries and choose the top3 English speaking countries. Then onward do the analysis for only top 3 countries.",329512.0
93612,395245.0,1. You find the top 9 countries based on the investment in dollar for the funding type that is apropriate as identified in check point 2. From top 9 companies identify the top 3 English speaking companies 2. For check point 4- Top 3 countries needs to be analysed for sectorial investment for the funding type identified suitable for Spark,310629.0
93606,394395.0,Use inner join and check if you have converted the permalink to either upper or lower case.,329512.0
93606,394431.0,"Use inner join and convert the permalink to lower case, also you can change the name of column to company_permalink as in rounds2 for better understanding.",303673.0
93606,394553.0,Along with inner join also check for special char fix in the permalink. Stackoverflow link is also present for the same. Make sure to convert the columns to either lower or upper case so that we don't drop any row due to case sensitive data.,318476.0
93606,394574.0,"While joining on string type columns, we must take care of following 2 points - 1. both columns must be compulsarily casted as string (df.col1 = df.col1.astype(str)) 2. both columns have either all lower case (or all upper case) characters Without fulfilling these conditions you will get nulls after join",306250.0
93271,392749.0,This can't be done. No need to verify the answer. Please delete your question. Only approach can be discussed not the answers.,329512.0
92787,390781.0,I think they want the row count.,317689.0
92787,391177.0,Its the row count,318476.0
92787,390769.0,basically .. rows,300694.0
92787,391060.0,Total no of records/rows after joining the two dataframes.,317811.0
92787,391131.0,Thanks for clarification. even I had this doubt that what exactly we need to observe :-),304814.0
92787,395460.0,thanks for clarification. I have the same doubt. Since company permalink is unique/not null. we wil have same count of rounds2 dataframe.,310518.0
93768,395231.0,you need to use pd.read_csv command only. Pls. see the link pasted below : https://stackoverflow.com/questions/21546739/load-data-from-txt-with-pandas,311686.0
93768,395238.0,just like you load csv. Make sure to use correct seperator and enoding,304814.0
93768,395256.0,"You can use read_csv command. separator is '\t' as it is tab delimited. And encoding is 'latin-1' import pandas as pd df = pd.read_csv('filename.txt', sep='\t', encoding = ""latin-1"")",318458.0
93768,395288.0,It is same as reading a csv file using read_csv and specify the separator and encoding in the same. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html Refer to documentation for more details.,317689.0
93768,396019.0,"use read csv command with seperator as ""\t"".",314678.0
93778,395262.0,Then if we use inner join is that fine in our case?,318579.0
93778,395267.0,Check points part 1 says Merge the two data frames so that all variables (columns) in the companies frame are added to the rounds2 data frame. I think you should consider other join,317845.0
93778,395257.0,yes you're correct.. only the common data in both the dataframes will be captured.. all other unique data for a dataframe will be ignored in inner join..,316349.0
93778,395282.0,"You need to first format the data (handle encoding issue) appropriately and than decide on the join, so that you dont miss any records which might be required.",317689.0
93778,395304.0,This has been discussed earlier and TA has replied on the same Kindly go through the link: https://learn.upgrad.com/v/course/208/question/93725,303673.0
93778,395659.0,"UNIQUE records in both the data frame are same , so won't be any issue if you make them unique before merge.",315679.0
93778,395802.0,You can use either merge or ljoin which will solve the purpose.,317600.0
93784,395470.0,I think we should drop the NULL value rows in any column,318344.0
93784,395292.0,"Each approach would lead to different result, so it would be better if such assumptions are laid down in the assignment.",304814.0
93784,395297.0,"We've to be very cautious while imputing the null values with any aggregate function. I cannot imagine of having replaced the 'Raised Amount USD' with either Mean/Sum/Category.. But, yes if the number of rows is huge it shouldn't be harmful to drop them. Let's hear from others too..",316349.0
93784,395376.0,"For 'Raised Amount USD' i dont think we should be replacing values with any aggrgate as this column is the base for all of our calculations, also as its a reletively small number of rows i would recommend to remove those. That being said, for other column like Category_list, i would recommend substituting a blank value rather than removing the data rows. Hope this helps.",306725.0
93784,395566.0,It's better to remove those rows.,318427.0
93784,395602.0,It is better to remove null value rows instead of putting 'more occuring values'.,314183.0
93784,395645.0,I don't see enough null values in the column needed for this assignment. So how about not dropping any row and going ahead with all values,300689.0
93784,395688.0,"For 'Raised Amount USD', please drop the nulls. For others decide on what is taught to you. Thi is a graded component, can't help more.",329512.0
93784,396205.0,We should not replace with other valued as that is real time data and business case study . if we do it it impacts the results and investment decision making,318732.0
91084,380381.0,"Mapping file has Analytics as a category, Check for data quality issue. It's a challenge.",329512.0
92990,391630.0,see what happens if you load it with a different encoding .. I Can't mention which one because this is a graded assignment,300694.0
92990,391642.0,This is perhaps because permalink column isnt a string object and you are applying a string function on it. Try loading the dataframe first. Then convert the datatype of permalink column to string object. Then apply the encode/decode functions. Hope this helps.,310511.0
92990,391689.0,"thanks i followed the above process and loaded the table. After that i applied the encoding and decoding and applied the command: print(len(rounds2.loc[~rounds2['company_permalink'].isin(companies['company_permalink']), :]))...i should be getting 0, but i am getting 114949. is this correct?",310509.0
92990,391754.0,use iso-8859-1 encoding,317811.0
92990,391947.0,"For encoding issue, you need to google and find the correct encoding, As this is a part of the graded component, I will not be able to tell you the right answer, but the hint for this is just google: ""non-English characters in pandas encoding issue"" The solution is available in StackOverflow, once you google it. Also run this code and see: rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] If you can see some non-english words in permalink then encoding is not solved.",329512.0
92990,392971.0,I'm using iso-8859-1 encoding but still getting this. any help ?,306735.0
93827,395334.0,It must be top9 countries without considering language and the 3 english speaking countries out of that.,318436.0
93828,395333.0,"Hi Venkatesh, You can replace the NULL values with 0 (zero's). That should help you.",314048.0
93828,395363.0,NULL is fine as they will not affect the averages but 0 will affect the average,301115.0
93828,395565.0,"Null values needs to be deleted, as cleaned data will give accurate results",317811.0
93828,396686.0,NaN values from the raised_amount_usd must be dropped to get accurate results.,311227.0
93832,395339.0,"If you feel that the encoding issues present in ' companies.name ' would affect the analysis that you are doing, then you need to handle them appropriately. Removing them is not a solution.",313826.0
93832,395451.0,Refer to the StackOverflow link provided in the checkpoint 1 of the case study for removing special characters.,307175.0
93832,395545.0,"after importing csv or txt using iso-8859-1. Then Next Step is df[columname'] = df['columname'].str.encode('utf-8').str.decode('ascii', 'ignore')",317811.0
93832,395696.0,"You can only solve it, if you have use correct encoding. I can help your with the hint: Google removing-special-characters-in-a-pandas-dataframe",329512.0
93834,395361.0,Blanks is not a sector,301115.0
93834,395383.0,"Yes, we need to include Others as a category. We need to exclude 'Blank' as a category.",306725.0
93835,395348.0,what is the error you are getting for loading the data,318804.0
93835,395382.0,i got it thank u,315633.0
94249,397006.0,Blank is not a category. So you need to use inner join to get rid of them or may drop them.,329512.0
94249,397007.0,"Nice question! This would be my take on this, the first step in data analysis as mentioned is Business Understanding.. here as soon as you do a info() check on your dataframe you'll be capturing the nulls in it.. and this would be the right point of time to be discussing with stakeholders and business team to decide on dropping it off or considering some false value in place.. and yes obviously as you mentioned in your question, we should make the consequences of dropping such rows clear to clients.. for a data analyst blank subject doesn't make sense but it could for business based on their requirement.. or it could be a scrap values as well.. so better the decision is precise when discussed and finalised through a business call than assuming it.. i hope this makes sense.. thanks for asking this question! :)",316349.0
93838,395360.0,"is special characters taken care in both the dfs? rowcount should not change in both the DFs we are manipulating the data, no dropping . Unique count may be change if those texts are manipulated correctly and are as same as one of the above texts eg: Year another record with Ye@r row count :2 unique 2 after encoding rowcount 2 unique 1",301115.0
93838,395564.0,"after importing csv or txt using iso-8859-1. Then Next Step is df[columname'] = df['columname'].str.encode('utf-8').str.decode('ascii', 'ignore')",317811.0
93838,395573.0,"The link provided in the Checkpoint - Part 1 for ""Removing Special Characters in a Pandas DataFrame - StackOverflow"" is enough to handle this issue. Follow the steps in the link properly and it will be good to go. Reposting the link again for your reference: https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe Hope this will help",317991.0
96473,411019.0,"Yes, that was required. Think of a condition where you have two tables with 10M rows, now decide to merge the two tables based on a primary column city. Now by checking some rows, you saw that the column city have the city name in lowercase, but this condition was concluded based on few random rows, what if some rows have the city name in upper case or mixed, then what will happen? the merge will not happen correctly right? So to be on safer side, always convert the case to either upper or lower when dealing with string columns as a primary key. This is very important.",329512.0
93840,395521.0,"use merge, 2 data frames at a time and join on the common column name. you will easily find the syntax of merge in Google.",300748.0
93840,395370.0,Checkout the below link on stackoverflow which answers a similar question: https://stackoverflow.com/questions/23668427/pandas-three-way-joining-multiple-dataframes-on-columns,313826.0
93840,395417.0,you can use the merge code to join all the dataframes with a common column defined..,316349.0
93843,395406.0,r you applying additional encoding on the column or on the whole dataframe??,301115.0
93843,395544.0,"after importing csv or txt using iso-8859-1. Then Next Step is df[columname'] = df['columname'].str.encode('utf-8').str.decode('ascii', 'ignore')",317811.0
91499,382836.0,as mentioned - it has to be one submission - I imagine by the facilitator of the group; zip containing ppt and python file,300694.0
93848,395405.0,check for the other link provided for additional encoding on that column,301115.0
93848,395413.0,"hey Maya, refer the stackoverflow link provided for encoding and decoding.. that will help you out in getting rid of the special characters..",316349.0
93848,395543.0,"after importing csv or txt using iso-8859-1. Then Next Step is df[columname'] = df['columname'].str.encode('utf-8').str.decode('ascii', 'ignore')",317811.0
93851,395415.0,refer the stackoverflow link provided in the same module.. that will you out in getting rid of all the special characters from your dataframe column..,316349.0
93851,395499.0,"The link provided in the Checkpoint - Part 1 for ""Removing Special Characters in a Pandas DataFrame - StackOverflow"" is enough to handle this issue. Adding the link here again: https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe",317991.0
93851,395561.0,"after importing csv or txt using iso-8859-1. Then Next Step is df[columname'] = df['columname'].str.encode('utf-8').str.decode('ascii', 'ignore')",317811.0
93851,395563.0,"after importing csv or txt using iso-8859-1. Then Next Step is df[columname'] = df['columname'].str.encode('utf-8').str.decode('ascii', 'ignore')",317811.0
93851,395597.0,"Just go for link provided to understand or use df[columname'] = df['columname'].str.encode('utf-8').str.decode('ascii', 'ignore')",319869.0
93863,395516.0,The file is encoded. you need to pass appropriate encoding type when you read the csv. Try with some other encoding method if UTF -8 is not working .,300748.0
93863,395708.0,Refer to this:https://learn.upgrad.com/v/course/208/question/93851,329512.0
93863,395505.0,You need to pass encofing to solve this error. Go through the below link:- https://learn.upgrad.com/v/course/208/session/19864/segment/101067,303673.0
93865,395530.0,"Since, you're in Pandas use 'pd.isnull'; it can accept NumPy arrays of object datatype.. Another way is to set the another argument as ' errors='coerce'..",316349.0
93865,395539.0,df = df[~df['columname'].isnull()] Isnull will help here.,317811.0
93860,395498.0,If it is for calculating the average then there will be no difference in value with or without the NULL,303673.0
93860,395483.0,It depends on your assumption. You can either decide to drop them or impute them to some default value.,317689.0
93860,395540.0,Drop the null values based on the situation step by step. Dont drop null values in all columns in one go.,317811.0
93860,395706.0,"For raised amount column you can drop the nulls, for other please think and do.",329512.0
93004,391747.0,Add two new columns one will be primary sector and other will be MainSector having mapped value of primary sector in mapping.csv,317811.0
93004,391683.0,"The former is what is needed I guess. Because with the latter approach, analysis for checkpoint 5 will be very hazarduos, if possible.",310511.0
93004,392469.0,"1. Extracting the main category from category list, using a split on pipe operator. 2. Use this merge the mapping file with the master file. 3. You can merge it by first transforming your mapping file to long data frame by using pd.melt",329512.0
93012,391744.0,"while merging, you need to mention the names of column on which both dataframes will be joined lefton='columnnameoflefttable' righton='columnnameofrighttable' as both dataframes have different column name for the join column",317811.0
93012,393873.0,this is the screenshot,310509.0
93728,395021.0,"Please check/search the forum - most, if not all, of these have already been answered",300694.0
93728,395551.0,"Hi Sambit, Here are the answers. 1. Once you have choose the suitable investment type, then filter the data based on that investment type after that use that data to get the top 9 countries based on the raised amount and then choose top 3 English speaking countries from the top9. 2. I hope this is clear from my above answer. 3. Yes you may remove un-used columns, but please mention your assumptions in yiyo notebook. 4. Drop the rows in the data cleaning part after you have merged the two files. Rows can be dropped where the raised amount is zero, other cleaning you need to figure out. 5. When you will merge the master file with mapping file, use inner join, the rows will be selected only if they are present in both the files. 6. Tableau plots can be plotted using master file. Any place you think it is required. 8. Fraction is part of total. Only 3 sectors",329512.0
93877,395603.0,"Hi Swathi, Yes, we need to clean and manipulate data(if required) before starting analysis. We might need to analyse the problem description to decide which columns need to be dropped and which columns need missing value treatement. Please find the below references on data cleaning task w.r.to checkpoint-1. 1. https://learn.upgrad.com/v/course/208/question/93727 2. https://learn.upgrad.com/v/course/208/question/93580 3. https://learn.upgrad.com/v/course/208/question/93366 Thanks.",305652.0
93877,395593.0,Check the info for data frames and check null values as they will stop you from computing mathmetical operations like mean etc.Check for duplicacy or unique rows as asked,319869.0
93877,395745.0,"Hi, Yes any data which needs to be analysed needs to be cleansed in such a way that we get the desired accurate results. Or else wrong corrupt form of data can manipulate your results.",305129.0
93878,395743.0,"Hi, we have to unpivot the mapping data in such a form so that we can map it up with the master frame and create a new column as main sector.. In short map it out that which primary sector falls under which main sector.. All needs to be achieved using python..",305129.0
93878,395599.0,"Hi , As per my understanding , we need to map each primary sectors to 8 main sectors based on the mapping file (mapping.csv). We might need to prepare the mapping csv file before start mapping. Thanks.",305652.0
93878,395703.0,you've to use the mapping file data to have the main sectors added to your master dataframe.. match the primary sectors from your master dataframe with the company sector from mapping file.. and then add a column main sector in master dataframe having all the column headers from mapping file next to the primary sector..,316349.0
93878,395904.0,Merge the mapping the file with the master file on primary sector. The matrix in mapping file will get mapped to the master file. You can use the matrix directly to find the count of sector whihc has highest investment or you can use function to consolidate the matrix in one single field by creating a function.,310629.0
93880,395619.0,"Hi Srinivas , We need to drop the rows in data cleaning part , but we need to figure out the strategy and it should be backed up by the assupmptions we made. Please refer the below link - 4th question: https://learn.upgrad.com/v/course/208/question/93728 TA's comments: ""4. Drop the rows in the data cleaning part after you have merged the two files. Rows can be dropped where the raised amount is zero, other cleaning you need to figure out "" Thanks.",305652.0
93880,395885.0,"NaN dont have any impact on average value. So, you can drop the NaN from raised amount USD. For other columns you need to see if you want to drop or impute as this will be an assumption on your side for doing the analysis.",317689.0
93880,395695.0,"yes Srinivasan, we can drop such column and rows since it has no significance in your data analysis and can rather cause problems in your new datasets henceforth created.. also one of the live lecture ut was mentioned that dealing with null values has always been a trouble since we don't know whether it is NaN (not a number) or null or blank.. there is a huge confusion there.. better to drop such rows.. it'll surely impact your analysis but at the same time the minimum amount of data was never known so, let the size of data be little less but quality of data is important to take the calls and use for analysis.. hope this shounds sensible though cannot be completely be agreed upon.. this is always a topic of debate.. :)",316349.0
90070,,nan,
93885,395629.0,I believe it can be done at the respective checkpoints.,317998.0
93885,395630.0,it can be done as directed no major cleaning required.other than null values,319869.0
93885,395686.0,"Ideally whenever you merge two different datasets and it generates the null values, we should clean it up since that merged data set will be used further for analysis.. check if your newly created dataframe has any null values and analyse the quality of data through .info() and then proceed ahead..",316349.0
93885,395715.0,"You can perfrom Data Cleaning, once you have got the merged file from companies and round2. Drop the nulls for raised amount. For other figure out what to do.",329512.0
93885,395883.0,You can drop the nulls for the raised amount. For other columns you can decide if you want to impute them or drop them.,317689.0
90068,375264.0,Hi Abhishek. I would be interested in doing that!,318499.0
90068,375879.0,"Please get in touch with your student mentor for any such query, they will help you better.",329512.0
90068,386141.0,"Hi Abhishek/Achal, is the position still open for forming group, I would like to join, please let me know.",316889.0
93889,395738.0,"Hi, Basically they are asking to find the most funding round type which is most invested and the amount lies in between 5 to 15 million..",305129.0
93889,395650.0,"For the fund type and the country that we have determined in the earlier questions, we have to find the right sectors.",318084.0
93889,395649.0,"No. the question asks for investment PER sector. amongst those, the heaviest. (ie maximum) hope that helps.",317998.0
93889,396132.0,Select the suitable investment type by using raised amount and 5-15M constraint and then filter your data for that investment type. Then use this data for country analysis and find the top3 english speaking countries. After that for sector analysis for the top3 countires the secotrs will follow the 5-15M constraint.,329512.0
93893,395675.0,"This can only be solved if you are using a correct encoding method, google: removing-special-characters-in-a-pandas-dataframe This may help you to find the answer. The answer is in StackOverflow.",329512.0
93893,395809.0,use the same encoding method mentioned in the link provided and replace it with your df name n column name,317600.0
93893,395876.0,The stackoverflow link explains how this needs to be handled. Please take a look there.,317689.0
93760,395242.0,"from which column you want to remove the special characters? For company_permalink, you need to use correct encoding, and dont need to remove special characters.",304814.0
93760,395164.0,follow the stackoverflow link that is provided in the same page of session.. that is more easy to understand and implement also.. Also to verify your results just compare the rows with the index in your master dataframe and the snapshot provided in the same session page.. all the special characters are removed or not!,316349.0
93760,395121.0,"use df['E'] = df['B'].str.replace('\W', '') for detalis - refer below https://stackoverflow.com/questions/33257344/how-to-remove-special-characers-from-a-column-of-dataframe-using-module-re",306244.0
93760,395217.0,"As mentioned in one of the questions answered by TA,please follow the stackoverflow link given and cross check if the special characters issue is resolved by below code rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] should return 0",301115.0
93760,395167.0,"If you are talking about the special charecters appearing in permalink, than these are not sepcial charecters. These are encoding issues. Some charecters are not decoded properly with the given encoding scheme hence they look like special charecters.",317689.0
93760,395489.0,"# Using encoding = ""ISO-8859-1"" https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python",318427.0
93760,395562.0,Below link will helps correctly https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe,312019.0
93760,395596.0,Ues below link https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe,314183.0
93892,395674.0,"Try this: df['category_list'].apply(lambda x: x.split(""|"")[0])",329512.0
93892,395676.0,"Hi Vipul, You need to use ""expand = True"" parameter. ( Error will be resolved , but output is not displaying as desired with lambda function after adding expand parameter ) Try to save the split output to a temp dataframe then from temp dataframe you can retrieve the required positinal category. Thank you.",305652.0
93892,395879.0,"You dont need to use lambda function here. Just use str.split and pass expand= True and n=1. n: Numbers of max separations to make in a single string, default is -1 which means all. expand: Boolean value, returns a data frame with different value in different columns if True. Else it returns a series with list of strings So, you want to split the string based on first occurence of | operator and not later. So, use n=1 and you want to split therefore use expand=True. This is similar as the splitting which we did in the python assignment. Refer documentation of this for more info.",317689.0
93871,395711.0,Use this: https://learn.upgrad.com/v/course/208/question/93399/answer/393593,329512.0
93871,395537.0,"yes, redundant data will be created",300694.0
93871,395606.0,"Hi Harshit, Yes , redundant data will be present as we are mapping aggregate data to normal data. Agreed , ideally new dataframe should be created to address these questions instead new column .",305652.0
93891,396918.0,"Sort the values on investment amount , take the countries top3 (unique codes) , check the refernce PDF for english speaking countries",311227.0
93891,395668.0,"Hi , Manual inspection is required , top 9 countries (using county code / city) need to manually verified in English countries list pdf provided. Thanks.",305652.0
93891,395740.0,"Hi, In order to find out the top 3 countries- we have to analysis using Python in which we have to find the country received the maximum investment across all sectors. Out of that we have to find out the top 3..",305129.0
93891,395880.0,You need to inspect the country code and based on the country code identify the country name from standard list of country codes available on internet. Verify if these country codes are in english speaking pdf provided.,317689.0
93896,395698.0,"we can drop such column and rows since it has no significance in your data analysis and can rather cause problems in your new datasets henceforth created.. also one of the live lecture ut was mentioned that dealing with null values has always been a trouble since we don't know whether it is NaN (not a number) or null or blank.. there is a huge confusion there.. better to drop such rows.. it'll surely impact your analysis but at the same time the minimum amount of data was never known so, let the size of data be little less but quality of data is important to take the calls and use for analysis.. hope this shounds sensible though cannot be completely be agreed upon.. if you're able to impute the null values with some logical data then that's good at the same time.. as you said there is no thumb rule.. if you find it logical enough to impute the nulls.. i guess you should ahead with your assumption.. :)",316349.0
93896,395718.0,Go with your assumptions for data cleaning but not forget to mention it in Python notebook. Also remove nulls for raised amout column.,329512.0
93867,395513.0,"it doesn't do any harm , also shows in the merged frame if we have any records with category list as blanks",301115.0
93867,395710.0,Use inner join to merge mapping and master dataframe.,329512.0
93867,395709.0,"Blank is not a sector, other is a sector.",329512.0
93900,395758.0,check the percentage of null value. if it's high you can drop.,317845.0
93900,395757.0,It has been earlier discussed on the Null values Kindly go throught the link:- https://learn.upgrad.com/v/course/208/question/93896,303673.0
93901,395756.0,Instructions/Checkpoints are clearly given in the assignment.. Kindly go throught the link:- https://learn.upgrad.com/v/course/208/session/19904/segment/101324,303673.0
93901,395776.0,data cleaning and filling the table 1.1 Understand the data in the excel file.,313691.0
93901,395872.0,You need to handle the encoding issue based on the stackoverflow link given in order to read the data properly.,317689.0
93899,395784.0,"once we know the fund type based on the average, I would filter that specific df with range 5-15, so the first level filtering happens @average , then filter out the elements with that specific bucket and pass information to next stage",319770.0
93899,395814.0,you understood it corrctly we have to do avg and later on check the range :),318017.0
93899,395779.0,I don't understand your question but here is the process what is needed to be performed. 1. Choose the correct investment type by taking the average of the raised amount column and then manually apply the 5-15M constraint. 2. Then filter your data for the investment type you have chosen. 3. Then find the top 9 countries using the data you have just filtered out. 4. From the top 9 choose top 3 English speaking countries. 5. Then proceed with your analysis.,329512.0
93899,396242.0,"Even my understanding is same, we have to take average for checkpoint 2 and see that the average funding is in range of 5 to 15 M $. and for checkpoint 5 we need check for actual value of investment between 5-15M",306011.0
93899,396965.0,can you please tell me where we are asked to pick companies in the 5-15 million range - this would be news to me.thx.,300694.0
93907,395798.0,You can use str.lower() function to compare all the values in lowercase . hope that helps,317998.0
93907,395805.0,Merge doesn't handle your uppercase condition. Below is the link of official documentation on merge. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html You need to take care of uppercase / lowercase condition before merge using str.upper() or str.lower() whichever you prefer. Hope this will help.,317991.0
93907,395807.0,"The column on which the join is taking place, A same string with upper case and lower case represent same value. So better if all the string in those columns in same case before working on it.",318436.0
93907,395813.0,convert the column in upper or lower case to maintin the uniformity wheneever you merge it checks for the value on which it is merged it doesnt do any manipulations itself,318017.0
93907,396116.0,Convert the column values to lower case in both the data frame before merge . https://stackoverflow.com/questions/22245171/how-to-lowercase-a-python-dataframe-string-column-if-it-has-missing-values,317845.0
93908,395797.0,"Those will be removed automatically when you merge the two frames. so, you need not worry about those in your analysis. hope that helps",317998.0
93908,395812.0,you have to remove them as they wont be removed if you merge you will not merge on the country hence the value if not present will be present in your data and will effect you in the later stage of assignment.,318017.0
93908,395857.0,"i guess for category list we dont need to remove the rows as there is specific main sector as ""blank"" in the mapping sheet for null values, we can deal on removing it or not at later stage as per the requirement. while for country it is better to remove them as we cannot impute them with any other country name also can mislead the data in later stage..",305129.0
93912,395823.0,"Hello Vikas, if a column, which has null/garbage values is not used in the calculations and is not effecting the data analysis, then I would suggest not to consider the column in the data cleanup.",312758.0
93912,395839.0,"Hi Vikas, Based on the problem description we need to decide if column is useful in anywhere in the analysis otherwise it can be dropped from the dataframe. Please use the earlier Python data cleanup strategy that's been teached to us. We can follow the same approach here as well. Thanks.",305652.0
93912,395856.0,"Hi, I would suggest first identify which columns are required based on the checkpoints provided to us.. after that the columns not required to us simply drop them.. and for the remaining column drop the rows or impute the data as per the cleaning strategy taught to us during cleansing the data session.",305129.0
93912,395871.0,You need to see if this column is being used in analysis or not. Based on that you can decide if you want to drop this data or impute it with some value.,317689.0
93912,396003.0,It is ideal practice to remove NULL values from column if the percentage is relativley higher side as compared to other column but make sure that NULL % is not very high. Because presence of NULL makes the analysis erroneous. But if NULL is present in that column which is very important for the analysis then you can follow one of two steps: 1. Either impute the NULL values with mean / median or 2. Drop data if it is possible. In this case it is very high and you can't afford losing 72% data. So think wisely whether you need to drop or impute or to keep as it is. Rest TA can confirm. Hope this will help,317991.0
93912,396113.0,"if you are not considering the column for analysis then you can go ahead and drop the column instead of doing clean. if you are going to use the column for analysis, you will have to do the data wrangling.",317845.0
93912,396136.0,"Funding round code I think will not be used in the analysis, so let it be as it is. Also in general you will be required to fill it, for that you have various answer below.",329512.0
93912,396585.0,This column has no impact on data. You can drop this column otherwise you can leave as it is.,314183.0
93920,395914.0,"Hi Pulkit, Please find the below explanation: 1. For the point -3 in 5.2 Sector analysis: ( 3. Top sector (based on count of investments)) We need to find the top sector based on the count of (nuver investement occurances ) investments. We just need find how many times investments happened in each main sector and retrieve the top sector . 2. For the point -9 in 5.2 Sector analysis: (9. For the top sector count-wise (point 3), which company received the highest investment?) We need to find the top company which recieved highest investment (maximum) with in the top main sector , which we retrieved from above point (point -3 in table). Thanks.",305652.0
93920,395954.0,"You need to find the company name which received the highest funding in the top main sector which you would have calculated for different countries which received highest amount of funding type. So, Step by step: 1. Find the highest amount received for Funding Type per country. 2. For top 3 countries find the sectors which have highest funding. 3. Find the company in specific sector which have the highest funding.",317689.0
93929,395944.0,Your code should handle all the values even if it's zero except the NULL or NaN which is unique.. Or i may be wrong in understanding your question :),316349.0
93929,395951.0,I didnt understand your question. Are you trying to convert the amounts in million and facing this issue ?,317689.0
93929,395957.0,"Hi Abhijeet , 1. Regarding zeros in raised amount , you can safely ignore them. TA's comments from thread (https://learn.upgrad.com/v/course/208/question/93728) ""4. Drop the rows in the data cleaning part after you have merged the two files. Rows can be dropped where the raised amount is zero, other cleaning you need to figure out."" 2. Regarding the other digits , if you use convertion to million then it doen't matter if the amount is four digits or so. This converstion make sure your values are readable and same format. Thanks.",305652.0
93915,395870.0,"Yes, for joining purpose you need to do this. You need to create a new column which would be populated with the column name of the sector having value =1 as main sector.",317689.0
93915,395877.0,,316349.0
93915,396106.0,"if you see the mapping.csv file category_list is mapped with one main sector where value =1 for instance 3D is belong to Manufacturing where value=1<table style=""border-collapse: collapse;width:70pt"" width=""93"" cellspacing=""0"" cellpadding=""0"" border=""0""> You have to select the main sector for the category_list and merge with master data frame",317845.0
93915,396139.0,"You can use pd.melt to merge the two files. Check it, It will make your life easy.",329512.0
93936,395971.0,"We should not consider status of companies, while doing the analysis, but that does not mean you filter out those records.",302740.0
93936,395998.0,"Hi , status of companies is not part of any problem description. you can decide on columns which are not part of any analysis if they can be dropped from the data frame . Thanks.",305652.0
93919,395922.0,I guess dual axis chart makes more sense when we have two measures and one dimension. Not sure if we should be using it in checkpoint 6.. since it involves two dimesnion and one measure.. we might have to explore other options.. :),316349.0
93919,395956.0,To create a dual axis bar chart you need to have two measures. Right click on one of the measure and select option of dual axis and set the axis appropriately.,317689.0
93939,395988.0,"I guess you're correct, that limit was only for Movies Assignment.. but here, you can try to check the sum of null values in your dataframe and try to logically get rid of those nulls..",316349.0
93939,395989.0,It is ideal practice to remove NULL values if the percentage is on higher side because presence of NULL makes the analysis erroneous. But if NULL is present in that column which is very important for the analysis then you can follow one of two steps: 1. Either impute the NULL values with mean / median or 2. Remove NULL if it is possible. So in short it depends on requirement. For the investment case group project think wisely whether you need to drop or impute. Rest TA can confirm. Hope this will help,317991.0
93941,395987.0,Just try to find the sum of null values in your dataframe.. using something like '.isnull().sum() ',316349.0
93941,395995.0,"Hi , it's completely depend on the assumptions / strategy you follow . You can refer the earlier Python assignment and then you can decide your approach on data cleaning. I guess , if we follow the standard data cleaning process by not over cleaning the data then despite of the strategy we should get similar output like top sector , top English speaking countries. Thanks.",305652.0
93941,396148.0,"We can't help you with the answer validation and it is recommended not to share answer here, you can use your understanding to clean the data with a proper logic and comment your logic in the notebook. We will check if the logic is correct, your answer will be correct. There is not hard answer, it all depends on the data cleaning.",329512.0
93982,396004.0,"Yes. assuming your country 1,2,3 refer to top 3 english speaking countries.",317689.0
93982,396025.0,"This is correct. As per the problem statement, all the filters conditions are combined by 'AND' operand.",310511.0
93982,396674.0,PLUS The total number (or count) of investments for each main sector in a separate column The total amount invested in each main sector in a separate column,315679.0
93985,396016.0,"You can use loc and iloc for both rows and columns selection syntax : data.loc[&lt;row selection&gt;, &lt;column selection&gt;] Example: # Select rows with index values 'Andrade' and 'Veness', with all columns between 'city' and 'email' data.loc[['Andrade', 'Veness'], 'city':'email'] # Select same rows, with just 'first_name', 'address' and 'city' columns data.loc['Andrade':'Veness', ['first_name', 'address', 'city']] Reference: https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/",318454.0
93985,396022.0,"df.loc[:,['column_name']] - will give you all rows from the column 'column_name', in short just the column. If you want to convert a collection of data to a series, you can call it in within the function pd.Series(). Simply writing the name of data_frame will print the dataframe. However, if you want to convert a collection of data to a dataframe, then you need to call it within the function pd.DataFrame()",310511.0
93985,396075.0,You can explore about function idxmax.,320103.0
93985,396087.0,iloc and loc can be used to get the column wise data as well. We use them extensively to get the row data as we usually want to analyse rows based on certain conditions. Refer to following: https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/,317689.0
93985,396187.0,pandas.DataFrame.idxmax pandas.DataFrame.transform,317811.0
93985,396213.0,All the below answer will work.,329512.0
91569,383489.0,"When you will perform the join, you will get results for only those sectors present in the master file, rest will be null, you can then leave them. But I don't think you will get any such cases. Also please check for any data quality issue such as the spelling of a primary sector may be wrong or different.",329512.0
91569,383592.0,"thanks for the inputs Maddula. I got some null cases. and about the spelling issues, that is what I meant when I mentioned that for some values there are similar (but not same) values in Mapping file. for example in case of 'Games' we have 'Game', 'Videosl Games' , 'Gaming' etc. is it expected that we go through each such individual case and rectify it?",311686.0
93988,396127.0,It's totally hit and trial. you can read more about identifying the character encoding. http://kunststube.net/encoding/,317845.0
93988,396037.0,Its a trial and error method. But refer to Language field to bring the options down. https://docs.python.org/2/library/codecs.html#standard-encodings,318436.0
93988,396186.0,"First encode using ISO-8859-1 or unicode_escape Then follow the steps in the link provided in case study to handle special characters as given below: https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe df . YourCol . str . encode ( 'utf-8' ). str . decode ( 'ascii' , 'ignore' )",317811.0
93991,396126.0,It's totally hit and trial. you can read more about identifying the character encoding. http://kunststube.net/encoding/,317845.0
93991,396086.0,It is based on languages which are included in the given dataset. Usually it depends on the dataset and we can identify based on language of data in the dataset. But here we have to hit and try based on data and see which all languages are used.,317689.0
93991,396185.0,"Its hit and trial. First encode using ISO-8859-1 or unicode_escape Then follow the steps in the link provided in case study to handle special characters as given below: https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe df . YourCol . str . encode ( 'utf-8' ). str . decode ( 'ascii' , 'ignore' )",317811.0
93999,396085.0,Choosing correct encoder is a matter of hit and trial. But refer to the links given in the assignment. Those links would suffice to choose correct encoder and how to use them.,317689.0
93999,396101.0,https://learn.upgrad.com/v/course/208/question/93500 refer the above link.its been answered in discussion forum,300687.0
93999,396179.0,"First encode using ISO-8859-1 or unicode_escape Then follow the steps in the link provided in case study to handle special characters as given below: https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe df . YourCol . str . encode ( 'utf-8' ). str . decode ( 'ascii' , 'ignore' )",317811.0
93999,398324.0,"The best encoding to use is ""ISO-8859-1"" or ""latin-1"", sep = ""\t"" You can get it by running the command: `df.YourCol.str.encode('utf-8').str.decode('ascii', 'ignore')`",307486.0
94009,396119.0,"When you will import both the files, you will find that there are some non-english characters in the imported file, which is the indication that the files is not loaded using the correct encoder. You need to then check various encodings to resolve this issue. Gooogle: removing-special-characters-in-a-pandas-dataframe. The answer is in StackOverflow.",329512.0
94009,396155.0,"Aditya, you can recognise if open the file notepad try to save as with another name you can see the encode type in the down. Rather worrying in chaging it in file. Do the encoding and decoding python. First when you are importing companies encode to like this...in the end : sep=""\t"", encoding = ""ISO-8859-1"") read rounds2 as is with no encoding. After importing files into datafarmes do the encoing and decoing in both data frames using below: ------------------------------------------------------ df=df.applymap(lambda s: s.encode('utf-8').decode('ascii', 'ignore') if type(s) == str else s) print(df) -----------------------------------",316215.0
94009,396203.0,"Hi Aditya, You can follow the below steps to know the encoding of a file. # First install chardet from cmd pip install chardet # Then use the below code import chardet import python as pd def find_encoding(fname): r_file = open(fname, 'rb').read() result = chardet.detect(r_file) charenc = result['encoding'] return charenc my_encoding = find_encoding('myfile.csv') df = pd.read_csv('myfile.csv', encoding=my_encoding) Hope this helps.",310511.0
94010,396221.0,Below answers will help to solve your query.,329512.0
94010,396129.0,Mapping file is available in the downloads section of the assignment.,318084.0
94010,396160.0,"Hi , It's just another CSV file , which is available in downloads. You need to import the csv file to dataframe with usual method and then merge the mapping frame with master frame. Thanks.",305652.0
94010,396174.0,"However there is a trick here. After downloading and creating a data frame, it needs to be pivoted/transposed so that it can be joined wih master frame properly to get the Main sector.",304814.0
94010,396199.0,First you will need to get the csv data into a dataframe.Then apply a logic to get the column headers based on row values. Hope this helps.,310511.0
94018,396173.0,"pd.merge(dataframe1, dataframe2, left_on=columnofdataframe1, right_on=columnofdataframe2, how ='inner')",317811.0
94018,396168.0,"Hi , You need to use the below syntax to merge the dataframe , unique columns in both dataframes are with different names(permalink and company_permalink) . pd.merge(&lt;data_frame1&gt;,&lt;data_frame2&gt;, left_on=&lt;df1_column&gt;,right_on=&lt;df2_column&gt;,how ='inner') so we should use 'right_on' and 'left_on' parameters to specify the column names explicitly instead of 'on' which is used when column names are same across the dataframe. Ref link : https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html Thanks.",305652.0
94018,396182.0,"permalink is thr in only companies for round2 you have company permalink so you have to join like : left_on=columnofdataframe1, right_on=columnofdataframe2,",318017.0
94018,396195.0,Usually handling of dataframes of this size doesnt throw out of memory error. What is the ram size of your system where you are running Jupiter?,310511.0
94018,396225.0,Try to restart your machine and clear your cache memeory. This should not have happened fro such as small dataset.,329512.0
94018,396246.0,We tried using similar approach only. But this size of dataset is very small to throw out of memory error. Try freeing up some memory in your machine either manually or by restarting.,317689.0
94018,396587.0,"Dont use left, right command , just rename the column name and merge the same name columns. Its easy.However there is no harm in learning new methods. Left, right approach would teach you something good/new.",315560.0
94018,396720.0,"If you get an unexpected MemoryError and you think you should have plenty of RAM available, it might be because you are using a 32-bit python installation. The easy solution, if you have a 64-bit operating system, is to switch to a 64-bit installation of python. The issue is that 32-bit python only has access to ~4GB of RAM. This can shrink even further if your operating system is 32-bit, because of the operating system overhead. You can learn more about why 32-bit operating systems are limited to ~4GB of RAM here: https://superuser.com/questions/372881/is-there-a-technical-reason-why-32-bit-windows-is-limited-to-4gb-of-ram",329512.0
93914,396138.0,"No, No, Please keep it simple, You need not to take rows into consideration. Just calculate the average and find the best one.",329512.0
94015,396158.0,"we have to apply what we have been taught. in assignment we have checked and cleaned the data. you have to observe, analyse and decided on what data should be kept and what to be dropped",310585.0
94015,396175.0,"Yes, we need to do data cleaning otherwise it may impact the analysis badly. We need to drop missing values from fund_raising_amount,category_list,country_code &amp; sector step by step at the points wherever required instead of dropping missing values from all columns in one go.",317811.0
94015,396159.0,"Hi , Yes , we need handle missing values (nulls) for data analysis. What ever the strategy you follow (either removing null values , dropping columns or imputing row values with mean,mode) should be backed up by assemptions you made . You should document the same. TA advised on one of the thread to clean the data in each check point i.e. if the check point is related to Funding type then clean the funding type related missing values and for the country analysis check point , clean the country related values. Clean the data if you think it is effecting the analysis. Please find the below link , I find it helpful. https://learn.upgrad.com/v/course/208/question/93728 As TA quoted in it (""4. Drop the rows in the data cleaning part after you have merged the two files. Rows can be dropped where the raised amount is zero, other cleaning you need to figure out "") , choose the strategy which should backed up by your assemptions. Thanks.",305652.0
94023,396211.0,You can take help from the pdf link given in the objective page of the assignment to identify top 3 english speaking countries.,310511.0
94023,396244.0,Use the pdf file provided. PDF file has the country names. For all these countries get the codes from following link. Based on that identify the country codes which are english spealing. Refer to alpha-3 codes from the following page. https://en.wikipedia.org/wiki/ISO_3166-1,317689.0
94023,396236.0,we have to first sort the countries in desc order based on the investment amount received. this will be done for the funding type we have taken before this checkpoint. After that we have to refer the pdf that out of those top 9 which are the top 3 english speaking country.,305129.0
94024,396210.0,You can take help from the pdf link given in the objective page of the assignment to identify top 3 english speaking countries.,310511.0
94024,396214.0,"Once funding type is decided, create top9 DF based on country analysis, then refer the PDF shared in the problem statement page , manually check the country codes against the English speaking countries pdf and decide the top3 English speaking countries",301115.0
94024,396235.0,we have to first sort the countries in desc order based on the investment amount received. this will be done for the funding type we have taken before this checkpoint. After that we have to refer the pdf that out of those top 9 which are the top 3 english speaking country.,305129.0
93265,392912.0,"Great, thanks!",300694.0
93265,392744.0,"1. When you will use inner join to merge master and mapping file, the issue will be solved, no need to consider them as other. Use only those present in both the files. 2. Ignore those companies. 3.Ignore them.",329512.0
94038,396292.0,if you have changed the company permalink to lowe or upper case then you should be fine,318017.0
94038,396291.0,"I believe that is part of the assignment and answers are not supposed to be shared directly in the discussion forum. Kindly request you to discuss it within your group and if there are any specific doubts on how to approach the problem, please post that.",318084.0
94038,396290.0,regarding how many unique companies - can't answer that because that is a graded qn rounds2 does have duplicates (using *permalink* column) - and that is fine - it should be like that I believe yes *permalink* would be a good approach to match records between two dataframes,300694.0
94037,396278.0,Yes.,329512.0
94037,396281.0,yes,318017.0
94037,396319.0,Yes it is one of the value funding_round_type have avg. amount invested in range 5-15 million.,317689.0
94026,396215.0,replace 0 with na in the mapping df,301115.0
94026,396243.0,Yes this needs to be handled. You need to write a function to convert 0 to na.,317689.0
94026,396973.0,But if we replace 0 with na it may affect the names which really have 0 in it. for example Enterprise 2.0,318579.0
94030,396259.0,"for companies, permalink and rounds2 companies_permalink column will be used to calculate the unique values and null does not gets counted in unique values.",317811.0
94042,396297.0,yes that you can do while filtering data first find the best ft then you can use it ahead,318017.0
94042,396318.0,"This data is static. So, in order to proceed with the analysis you can go ahead with static or hardcoding. The parameterized approach is need when your dataset is changing and you need to have a certain functionality desired for any given input. For example if you want to calculate squares, than it should be parameterized. But in this analysis we have to work on a particular dataset only just like the one we did in movies analysis.",317689.0
94042,396312.0,"Yes, once you find FT then you can use same for further analysis. Similarly, once you find top three countries, use same for further checkpoints.",317811.0
94045,396316.0,It is one of the funding_round_type for which average amount invested lies between $ 5-15 million range.,317689.0
94044,396310.0,pd.DataFrame(dataframe.groupby('columnnameforgrouping').['columnanameforaggrgate'].aggregatefunctions).sort_values(by='columnnameforsorting'),317811.0
94044,396332.0,"df.groupby('col1').sum().sort_values(['some column'], ascending = False)",319770.0
94048,396329.0,do you mean expert l export?,318329.0
94048,396330.0,what exactly you want to do ?,319770.0
94048,396338.0,If you want export you can use to_csv command to export a dataframe to a CSV file. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html,313691.0
94048,396360.0,"yes, i want to export master_frame to excel",308495.0
94048,396374.0,If you want to export in csv you can follow below link https://stackoverflow.com/questions/16923281/pandas-writing-dataframe-to-csv-file And if you want to export to excel then you can follow below link: https://xlsxwriter.readthedocs.io/example_pandas_simple.html https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_excel.html Hope this will help,317991.0
94054,396372.0,"1. When we do import import both files using standard community of python that supports like encoding = ""cp437"" or other standard pyton , you can find details in Stackflow and which are standard encoding available. Python has inbuilt feature when you provide the standard encoding menthof ut works globlabbly accepted",307843.0
94054,396373.0,The encoding issue will be explained to you once the solution will be published. There will be a video to explain this and how to get it corrected in any other dataset.,329512.0
94064,396441.0,You need to identify the investment type which is suitable for funding in the range of 5 to 15 million. One way is to take avg of investment amount by funding round type and based on that you can filter the suitable funding type.,317689.0
94064,396494.0,Find average investment amount by funding type and filter average investments which lies between 5 and 15 million.,317811.0
94064,396737.0,"Yes, this is required to be done for sector analysis. Filter the data with 5-15M constraint.",329512.0
94049,396328.0,https://learn.upgrad.com/v/course/208/question/93936,319770.0
94049,396359.0,"I saw that reply, but didnot understand the meaning of answers",308495.0
94049,396375.0,"No, No need to drop the companies based on the status.",329512.0
94049,396371.0,It is good practice to remove NULL values from column if the percentage is relativley higher side as compared to other column but make sure that NULL % is not very high. Because presence of NULL makes the analysis erroneous. But if NULL is present in that column which is very important for the analysis then you can follow one of two steps: 1. Either impute the NULL values with mean / median or 2. Drop data if it is possible. So think wisely whether you need to drop or impute or to keep as it is. Hope this will help,317991.0
94050,396336.0,"Choose a column which has unique values and present in both dataframes. If the column values don't match, so you end up getting column name without data",313691.0
94050,396327.0,"the other way is rename the company_permalink column so that the column name of both the dataframe matches pd.merge(df1, df2, how = 'inner')",319770.0
94050,396368.0,You can follow below link for detiled explanation and examples on merge https://www.tutorialspoint.com/python_pandas/python_pandas_merging_joining.htm Hope this will help.,317991.0
94050,396366.0,"Check if the unique column is converted to either upper or lower case and then try merging it. Also, make sure not nulls are there. Use inner join. pd.merge(companies, rounds, how=""inner"", left_on=""permalink"", right_on=""company_permalink"")",329512.0
94050,396797.0,"Merging companies and rounds2 with above process, m getting all the values in rounds2 as NaN. while in imported file it is showing all the values. Help pls.",311117.0
94050,396658.0,go for 'Left' join my friend. If you go for 'Inner' it wont show you data. if you go 'Outer' you will get all the data of both dataframes which is of no use. so prefer 'Left' and keep rounds2 file for left,318009.0
94052,396365.0,It is ideal practice to remove NULL values from column if the percentage is relativley higher side as compared to other column but make sure that NULL % is not very high. Because presence of NULL makes the analysis erroneous. But if NULL is present in that column which is very important for the analysis then you can follow one of two steps: 1. Either impute the NULL values with mean / median or 2. Drop data if it is possible. So think wisely whether you need to drop or impute or to keep as it is. Hope this will help,317991.0
94052,396369.0,"No, No need to clean category list column, but yes you need to extract the main sector using the column category_list. The category_list column contains values such as 'Biotechnology|Health Care' - in this, 'Biotechnology' is the 'main category' of the company, which you need to use.",329512.0
93931,395976.0,getting below error Yatin AttributeError: 'str' object has no attribute 'str',311404.0
93931,395977.0,"Hi Arihant, I tried below new2[""primary_sector""]=new2[""category_list""].str.split(""|"",n=1,expand=True) and getting error ValueError: Wrong number of items passed 2, placement implies 1 DO I have any issue with encoding?",311404.0
93931,395974.0,You might also want try companies ['category_list2'] = companies ['category_list'].astype(str).apply(lambda x.convert(x)),302740.0
93931,395993.0,This issue is coming up probably because you have null values in that column. Can you try running your code after removing null values?,318397.0
93931,395949.0,Check my answer at the following thread: https://learn.upgrad.com/v/course/208/question/93892,317689.0
93931,396142.0,Check this: https://learn.upgrad.com/v/course/208/question/93892,329512.0
93931,396468.0,it is giving error because you are not removing NaN values,318077.0
94058,396391.0,"Only one submission is needed. if you look closely, you can see names of all the team members at the first page of the case study.",312758.0
94058,396446.0,"As per the Student Mentors, the submission is to be done by group facilitator only.",317689.0
94058,396479.0,"The submission needs to be done by only one memebr, preferably the group facilitator.",317811.0
94068,396427.0,use to_lower or to_upper in both the frames.,315679.0
94068,396419.0,"Hi Maya, Once you imported both Companies and rounds2 files into dataframes using encoding ISO-8859-1'. You need to do another round encoding to UTF-8 and decode it for both companies and rouds2 dataframes. companies=companies.applymap( lambda s: s.encode('utf-8').decode(' ascii', 'ignore') if type(s) == str else s) print(companies) rounds2=rounds2.applymap( lambda s: s.encode('utf-8').decode(' ascii', 'ignore') if type(s) == str else s) print(rounds2) Notincompanies=rounds2[~ rounds2.company_permalink. isin(companies.permalink)] print(Notincompanies) After this you need to merge both data frames to get master_frame.",316215.0
94068,396436.0,You need to handle the encoding issue properly as given in the link. Post that while joining you need to convert both key columns to upper or lower in order for them to be in sync.,317689.0
94068,396465.0,Thanks all I have done the above steps it’and rechecked still same issue Output should be zero but getting one,300687.0
94068,396499.0,"First import the file using ISO-8859-1 or unicode_escape or latin1 encoding. And Then use the steps in the link below to handle special characters as below: df . YourCol . str . encode ( 'utf-8' ). str . decode ( 'ascii' , 'ignore' ) https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe",317811.0
94068,396862.0,Its working thanks to all,300687.0
94051,396324.0,the measure can be count of rows,319770.0
94051,396333.0,"In this context, the number of observations mean number of rows.",313691.0
94051,396367.0,Number of rows in the merged dataframe.,329512.0
94051,396869.0,Number of observation means number of rows in Master_dataframe,314183.0
94065,396405.0,This should not affect the final result of the analysis. so you can make your assumptions and proceed,318084.0
94065,396414.0,I agree with Premnath and reason is that there is already a row with category as “null” or blank in the mapping.csv,312758.0
94065,396440.0,"There are some NaNs in the mapping file. So, you need to ignore such rows. Please select the join type appropriately to handle such issues.",317689.0
94065,396495.0,You should drop null values from mapping dataframe as well. And also use a join which has matching for both dataframes.,317811.0
94060,396392.0,Yes. data cleaning is expected on the columns/attributes which are vital in the data analysis.,312758.0
94060,396395.0,As per the understaning actual analysis starts with checkpoint 2 where data cleaning plays important role. Checkpoint 1 is just an overview of data . It is not required to clean the data at check point 1 on the basis of constraint 1 and 2. If you do cleaning you might get different answer for table 1.1. But yes before doing analysis i.e checkpoint 2 you should clean the data.,317991.0
94060,396401.0,data cleansing should be done on the columns which will be used for analysis dont clean everything coz it will delete all the rows which might be imp for analysis,318017.0
94060,396445.0,Handle data cleansing only for the columns which are relevant in the analysis. If we try to handle columns which are not used than it might lead to dropping of additional data which might be important for analysis.,317689.0
94060,396482.0,"Yes, data cleaning is required for correct data analysis. Drop or impute the missing values step by step on the checkpoints from the columns having missing values that can impact your analysis rather doing for all columns and in one go.",317811.0
94060,396728.0,"Data Cleaning is required after you have merged the file. Drop the nulls for raised amount USD, for rest columns use your logic and check what is required to be done.",329512.0
94053,396453.0,"It is working on main sector. In main sector the top 3 is corresponding to Others, Social Finannce, Cleantech. But you are also using country code in the analysis as well. So, filtering is done on main sector than that is shown for the countries chosen.",317689.0
94053,396787.0,Use index() function and create a calculated field. Use this calculated field to find top 3 sectors with each country. Step by step process is provided in the below link. https://kb.tableau.com/articles/howto/finding-the-top-n-within-a-category,310467.0
94062,396397.0,"You can use below code. If this return 0 it means encoding issues is handled perfectly. rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] Source:- https://learn.upgrad.com/v/course/208/question/92605 Hope this will help.",317991.0
94062,396442.0,"To check this you need to see if all the company_permalink in rounds is present in permalink of companies. rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] This should return 0.",317689.0
94062,396483.0,"Whatever difference you get in the two dataframes, you can verify your result by manually searching in the txt and csv file whether it says correct, Is really this is difference?",317811.0
91334,390542.0,I still stuck at this,317993.0
91334,381734.0,"This is actually a challange, I would suggest you think about it and try it out. If you fail, then I am here to help you. Some Hints that may help: 1. Check for the data quality issue in mapping.csv file. 2. Treat it. 3. pd.melt can help you.",329512.0
91334,395189.0,Nice HInt. Thanks Maddula. It worked :-),304814.0
94066,396413.0,"You first have to make mapping.csv in a normalized and readable form and to do that please refer to “melt” function of Panda. Once you have the mapping data normalized, then join with the master frame on main category.",312758.0
94066,396415.0,"use melt function df=pd.melt(df, id_vars=['column_to_keep'], var_name='variable_name_label', var_value='variable_value_label') once this is done merge with master frame.",306244.0
94066,396477.0,First use idxmax function to modify mapping dataframe to have category list and its corresponsing mapped sector i.e. sector whose value is 1 for the category list. Then merge the two dataframes .,317811.0
94617,398963.0,Yes include both,315679.0
94617,398902.0,Yes. 5 to 15 Million USD inclusive. Please refer similar query verified by TA. https://learn.upgrad.com/v/course/208/discussions#26021,313826.0
94617,398997.0,"Yes, it includes both $5 &amp; $15 M",314048.0
94617,399046.0,"yes 5 to 15 means including 5 and 15, i think",312731.0
94617,399200.0,yes it does,300735.0
94124,,nan,
93725,394979.0,"Please use an inner join, we want to analyse only those rows for which the sector is present in both the files.",329512.0
93725,395025.0,"I also agree with you - I was surprised that we are doing an inner join in this way and losing data (and infact I spent a long time filling companies with no categories and mapping them to 'Blank' category; and mapping 45 company categories that were not represented in the mapping csv to 'Others' category) but ok if that is the guidance (to lose all these rows) from academia then, eventhough I don't agree/like it .. I will do as advised",300694.0
93725,395381.0,"The mapping file has category names with 0 (zero) in its name, there are total 53 such category names. Due to this the resultant data frame after joining with mapping data is not giving correct results Following are some of the examples: A0lytics Big Data A0lytics Business A0lytics Can0bis Career Ma0gement Chi0 Internet Cloud Ma0gement Contact Ma0gement Digital Rights Ma0gement Digital Sig0ge Document Ma0gement Educatio0l Games Energy Ma0gement Enterprise 2.0 Event Ma0gement Fi0nce Fi0nce Technology Fi0ncial Exchanges Fi0ncial Services",306725.0
93725,395486.0,"Hello rajarshi, how many primary sectors are there in master frame which don't have a match? Can u tell me to cross check...thanks",308437.0
93725,395657.0,Sector is important checkpoint and we can't make inference without the Sector field there is only option to use join which supports equijoin on the sector column and this is achieved using the inner join. Therefore we should use inner join on sector it also removes the unwnted rows that does not need analysis.,307843.0
94129,396540.0,TOP 3 ENGLISH SPEAKING COUNTRIES ONLY.,317811.0
94129,396534.0,It should be the top 3 English speaking countries as that is the focus of investment for Spark funds,318084.0
94129,397112.0,"But the top3 english speaking countries figured out based on the PDF provided, how do achieve it in Tableau?",310481.0
94132,396751.0,"Use inner join to merge, the blanks will be ignored also you may drop them.",329512.0
94132,396549.0,"blanks is not a sector but after joining with master frame ,it just shows companies with blank categories,it doesn't affect the analysis",301115.0
94132,396554.0,Don't modify your original mapping file.. Once it is mapped to your newly created dataframe; you can play around as needed..,316349.0
94131,396749.0,"Yes, correct.",329512.0
94131,396550.0,"Yes, you're correct..",316349.0
94131,396538.0,Yes. on track,318084.0
94131,396541.0,"YES, TOTAL NUMBERS OF INVESTMENTS IN EACH COUNTRY AND TOTAL AMOUNT INVESTED IN EACH COUNTY",317811.0
94131,396948.0,"but does tat mean that for a particular row, whichever is the main sector, corresponding to that we need to display the totao count of investments? For example, all the rows which have Health as the main sector will hav the same number in the count of investments column?",312096.0
94133,396552.0,"You need to pass the encoding and separator since it is a text formated file; something similar to; pd.read_csv('filename.txt', sep=""\t"", encoding = ""ISO-8859-1"")",316349.0
94133,396578.0,Please pass the separator appropriately. For companies file it is \t,317689.0
93500,394387.0,"Same problem, m also facing..",311117.0
93500,393674.0,You try with ISO-8859-1 encoding format,304694.0
93500,393721.0,latin-1 encoding is also working.,315679.0
93500,393757.0,"Use encoding = ""latin-1"", sep = ""\t"" in read_csv function.",312479.0
93500,394394.0,"also facing the same problem, how to resolve it?",310952.0
93500,394426.0,"ISO-8859-1 and ""latin-1"", sep = ""\t"" both working.",311117.0
93500,394658.0,"Try using encoding=ISO8859-1 with delimeter='\t' for text file and encoding='ISO8859-1' with delimiter="","" for csv file",301641.0
93500,394696.0,the special characters are not going away even after doing encoding with ISO-8859 and then using the stackoverflow solution as mentioned in the case study. This is impact the further analysis..how to proceed?,310509.0
93500,394789.0,"To remove the encoding from the dataframe uusing pandas..... df.Permalink.str.encode('utf-8').str.decode('ascii', 'ignore') Here instead of permalink we can use other columns which has encoding issue. To find the ending of csv or any text file: Open up your file using regular old vanilla Notepad that comes with Windows. It will show you the encoding of the file when you click "" Save As... "".",301649.0
93500,395194.0,"Try encoding = ""ISO-8859-1"" is working with sep=""\t""",306729.0
93500,395259.0,"import pandas as pd df = pd.read_csv('filename.txt',sep='\t', encoding = ""latin-1"")",318458.0
93500,395491.0,https://stackoverflow.com/questions/18171739/unicodedecodeerror-when-reading-csv-file-in-pandas-with-python Here use encoding for .txt file For .csv file even i am getting error,318427.0
93500,395773.0,"df= pd.read_csv(r'filename.csv', encoding = 'latin1') This will solve the problem",317600.0
93500,393709.0,"Try ISO-8859-1 or unicode_escape as encoding whie reading text file. Then folow the steps at: https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe df . YourCol . str . encode ( 'utf-8' ). str . decode ( 'ascii' , 'ignore' )",317811.0
94135,396580.0,"It is for top three countries as D1, D2 and D3 and the selected funding type for all D1, D2, D3.. basically you need to filter your daframe in such a manner that it has top company, selected FT and raised funds bbetween the 5M to 15M $..",316349.0
94135,396560.0,yes its mentioned,318017.0
94139,396574.0,It has been earlier discussed on the Null values Kindly go throught the link:- https://learn.upgrad.com/v/course/208/question/93896,303673.0
94139,396573.0,"Hi Praneeth, You can drop the un-used columns if you think it is not needed for analysis . But please make sure that your assumptions are marked in you notebook. Please refer TA's comments for below: https://learn.upgrad.com/v/course/208/question/93728 Thanks.",305652.0
94139,396576.0,"Start your data cleanup with the most important columns. For the inference you want to make, see what are the important columns to be considered. Prioritize the columns and start cleaning up and you'll get the answer to this question.",318084.0
94139,396572.0,You can visit below link where TA gave the answer. https://learn.upgrad.com/v/course/208/question/93912,317991.0
94558,398520.0,primary sector is extracted from category list and main sector will be formed by joining the mapping table so check if the join is correct or not the join should be inner join and if that is correct then some of the thing you are missing,318017.0
94558,398540.0,"main sector should not be Null whereas primary category can be. Since Null is a category in mapping file, so main sector of Null category would be ""Blank"". As suggested by other, use inner join to avoid the records for which primary category is not defined in mappin file.",304814.0
94558,398530.0,"You need to use inner join. Once you use inner join, you will not get null values in the main sector column as the merge will take values which are present in both the files. This has been answered by TA before. https://learn.upgrad.com/v/course/208/question/93725",318397.0
94119,396504.0,around 60-70%.But you need to drop only those missing values that can impact data analysis rather than dropping missin values for all columns.,317811.0
94119,396509.0,drop only the values that you be needing in the analysis else u will end up deleting rows which will impact the analysis,318017.0
94119,396746.0,"Please be very logical while taking any step as it may impact your answers, So clean only those columns that are used for analysis and about 85% is a good sign but it depends on use cases.",329512.0
94151,396611.0,It means enoding issue has not been resolved properly. Follow the steps properly given in below link: https://stackoverflow.com/questions/45871731/removing-special-characters-in-a-pandas-dataframe Hope this will help.,317991.0
94151,396632.0,"First import the file using ISO-8859-1 or unicode_escape or latin1 encoding. And Then use the steps in the link below to handle special characters as below: df . YourCol . str . encode ( 'utf-8' ). str . decode ( 'ascii' , 'ignore' )",317811.0
94151,397201.0,"please note that it does not convert inplace, so you have to for that column and assign",318002.0
94151,397238.0,"Agreed with Rakesh, You have to assign it back to the column. Something like this - df.YourCol = df.YourCol.str.encode('utf-8').str.decode('ascii', 'ignore')",306247.0
94151,398050.0,"pd.read_csv('filename', sep='\t', encoding='iso-8859-1') should work",319006.0
94151,398254.0,Hi I am stuggling to import rounds2 data. Can anyone help me witha that. Everytime I am getting different errors,315856.0
94151,398509.0,"r2 = pd.read_csv('r2.csv' , encoding = 'iso-8859-1' , delimiter = "","") r2.c_p.str.encode('utf-8').str.decode('ascii' , 'ignore')",310529.0
94154,396620.0,"maping=maping['category_list'].replace(to_replace='0',value='na') i had used this but its not being replaced",315633.0
94154,396619.0,use replace function,318084.0
94154,396621.0,Check what you are using is O or Zero.,318084.0
94154,396629.0,it is zero but still not being replaced,315633.0
94154,396666.0,"Since this is a graded question, cant specify the exact code. But, one basic method that you could use is the replace function of pandas series. For more info on how to use the replace function properly, you can go through the follwoing link: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.replace.html Hope that helps.",317998.0
94154,396755.0,"Use lambda and replace, your code will work.",329512.0
94149,396601.0,"it is not required to import PDF to python and match the countries, you can do this manually.",317811.0
94149,396617.0,"Thanks for the Answers. Hi TA, However, I have managed to use PDFMINER to extract the same and able to use it successfully . Can I proceed with the same? Please confirm.",311115.0
94149,396599.0,You can find TA response in below link for similiar lind of question https://learn.upgrad.com/v/course/208/question/93582 Hope this will help.,317991.0
94157,396713.0,Refer the same way that is used for separating genres in Python Assignment,317811.0
94157,396660.0,"Since this is a graded question, cant tell you the exact code. But, the approach to do this could be to take input as a string and split the string with ""|"" (pipe) being the delimeter and output the first subtsring after the splitting. Hope that helps.",317998.0
94157,396644.0,"there are muplitple ways; But, you can use the way that you might have used it in Python Movies Assignment since you'll be more familiar with that.. Anyways I've used the below; df1 = df['ColumnName'].str.split('|', expand = True) df['ColumnName'] = df1[0]",316349.0
94156,396640.0,"I'm not sure why you want to export the data in excel.. If this is related to Case study; you can directly import the .txt file in Python Jupyter Notebook with the code as; pd.read_csv('FileName.txt', sep=""\t"", encoding = ""ISO-8859-1"")",316349.0
94156,396659.0,"From your requirement I can assume you want to import companies.txt data into excel. If that is the case you can follow below step: 1. First create new empty excel file. 2. Then select ""Data"" option from menu bar 3. At top left side you will find get external data --&gt; choose From text. It will ask for file location. 4. Locate and select txt file from which you want to import data. 5. Then some popup will come, as per your requirement select or keep as it is. There you go your txt data is imported to excel. Hope this will help.",317991.0
94156,396664.0,"Since this is a graded question, cant specify the exact code. But, if you want to import the .txt file data into your jupyter notebook for analysis purpose, You can use the pd.read_csv() function directly. You can refer to this link for more details on using the pd.read_csv function. https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html Hope that helps.",317998.0
94156,396756.0,But why you want to import txt to excel? It's import or export?,329512.0
94156,397562.0,Thank you all for the solutions,314629.0
94156,396989.0,I hope this is not a part of case study. Still there are multiple ways to import Go to FILe OPen and select the text file use delimiter. Easy way out is drag the text file into an open excel.,306244.0
94485,398158.0,you have to make two new columns have total count and sum sector wise like for all the main sector others it should have count of the investments and sum of investment for the others sector,318017.0
94485,398157.0,"Any method is ok, you need to just fill the excel file with the correct numbers, no matter how to do it. Creating DFs is just a method of achieving the goal and this ca change, Use your logic and arrive at the answer.",329512.0
94172,397268.0,Expected result is three plots so it is one for each of the questions in checkpoint 6. It's not that complicated. try it.,311857.0
94172,396723.0,You may use different plots for this task.,329512.0
94174,396730.0,"Do this task manually, Check for country name and code in google and then map this manually.",329512.0
94174,396729.0,New column with country by getting value from external source is NOT needed. We can use country code along with the pdf provided to us to get the answers. This is what i had come across as a TA's response on this forum itself. hope that helps.,317998.0
94174,396732.0,"No , inserting a new column with country name is not required. Manual inspection of engilsh speaking countries from the top-9 countries will do the task. Thanks.",305652.0
94173,396724.0,"Count it before clean up and after merging the files, do a proper data cleaning steps.",329512.0
94178,396798.0,"No Mayur, i guess it's sum.. since we're summing up the investments across the sectors.. See the D1, D2, D3 which is calculated in the same way.. Also, in Tableau we're counting the sectors and summing the investments..",316349.0
94178,396789.0,You can go through below link where TA has clearly explained what needs to be done. https://learn.upgrad.com/v/course/208/question/93899,317991.0
94178,396823.0,"Yes, investment type whose average investment amount is between 5 and 15 million.",317811.0
93980,396074.0,"When it comes to calculating total investments/total count of investments, I believe it should include all the investments made irrespective of whether the sector is blank or not. TAs can confirm if this is ok.",318084.0
93980,396149.0,It is ok to get balnk as a sector but other and null are different. Also please check if you have used inner join for merging mapping and master file.,329512.0
93980,396716.0,"Blank is not a sector, other is a sector. Also use inner join to get rid of the blanks.",329512.0
94180,396813.0,"Yes exactly, it doesn't sound logical to impute the CountryCode and Region.. Better to drop them..",316349.0
94180,396822.0,Drop missing values in country code or impute them with NA or unknown. Region does not matter as you will not be using this column fot data analysis.,317811.0
94180,396838.0,But then in further analysis country code is needed to determine english speaking countries. This is the reason I’m quite confused as to what to do,307176.0
94180,396856.0,"In my opinion, we have to drop such records which has a null value for the country code. As we can't impute any value for such records.",320073.0
94180,396898.0,Let it be as it is. It's not going to harm your analysis.,329512.0
94183,396843.0,"Hi, You can check the NaN values using below syntax: np.isnan(df['column_name') for column of NaN values . But if you need to check a single value then you should use the below: import math as mt print(mt.isnan(float(value))) (provided value is an integer) Source: stack overflow Thanks.",305652.0
94184,396836.0,"Hi, Yeah, for some of the columns it has color code embinded which is not changing even with format painter. I think it has no significance as it is not mentioned in the problem description / results expected. Thanks for logging the question , let's wait for TA's confimation.",305652.0
94184,396900.0,"No significance, Please ignore it.",329512.0
94185,396875.0,not working,320689.0
94185,396870.0,"If the above code is not working you can try below syntax: pd.merge(right,left,how='inner', left_on= 'left_column' , right_on='right_column') Do let me know if it works or not.",317991.0
94185,396832.0,"Hi , Could you please eloborate your question ? If it is regarding the merge operation , you should use inner join not the outer join. master_frame=round2.merge(companies, left_on='company_permalink', right_on='permalink', how='inner') Thanks.",305652.0
94185,396839.0,Hi did not get records by using 'inner',320689.0
94185,396903.0,"Please check if both the columns are in the same case either upper or lower, remove nulls and duplicate from company permalink column from both data frame and then try the inner join.",329512.0
94185,396967.0,First convert the perma_link and company_permalink to lower case and then do inner join. It will work.,302741.0
94185,397167.0,Hope you already did clean up in master frame removing Nan where required. Then try create filter based on the investment type Speak interested in. On that filter you can work using group by country code. This should work.,306009.0
94185,397373.0,"As column names are different for both data frames, you have to specify both while merging. result = pd.merge(left, right, how='inner', left_on='left_col', right_on='right_col') And take care while converting both columns to same case.",306247.0
94185,397354.0,Check permalink and company_permalink data should make it same case upper or lower. and clean up of special characters.,312019.0
94187,396868.0,It can be one of two thing : 1. Either funding information is not available or 2. The company does not get any fund hence kept empty. So in both the cases empyt values is of no use for us to consider it for analysis. But if we impute 0 in place of empty value this will effect our analysis. Hope this will help.,317991.0
94187,396849.0,"Missing data. Hence, can be dropped.",318084.0
94195,396878.0,"As the next question says, Identify the top three English Spaking countires from top9 tables, I can interpret entire masterframe data sud be there, rows which contains those 9 countries.",300721.0
94195,396882.0,"The question states ""For the chosen investment type, make a data frame named top9 with the top nine countries "" It is quite evident from the question itself that a dataframe with top9 countries needs to be created. hope that helps.",317998.0
94199,396906.0,The one you have filtered for specific funding type and the top 3 countries,329512.0
94218,396955.0,"Hi , No need to import PDF file , once you have top 9 countries just manually refer the countries in PDF file and decide English speaking top 3 countries . Thanks.",305652.0
94218,396964.0,"It is not required to import PDF to python and match the countries, you can do this manually.",317811.0
94218,396962.0,You can follow TA's comment in the link below for your answer https://learn.upgrad.com/v/course/208/question/93582 Hope this will help,317991.0
91115,380643.0,Better to report the. error using report error on the assignment section since everyone one can have different options but TA will provide the one genic guidance which will help others alsi,307843.0
91115,380972.0,"This is not any type of error but a challenge for you to solve. This is a data quality issue, which you generally encounter working with real data. Please solve this issue and proceed with your analysis.",329512.0
91115,392006.0,Did you report the mistake?,317990.0
92163,387991.0,"This is expected. This will happen if the encoding issue is not properly handled. Don't worry, just use the numbers as you are getting from the Python solution. It will be considered.",329512.0
92163,395894.0,Maddula Naveen - Do we need handle encoding issue to answer Checkpoint 1 Table 1.1 questions?,318458.0
92168,388006.0,If the raised amount is zero that what the need to consider the company in our analysis? I think you got it what I mean.,329512.0
92168,388831.0,Normally you need to do imputation of the columns with the null values. Imputation can be done differently based on the variable/columns. Imputation is done because the Alogrithms in the Machine Learning models gives different or incorrect results when null values are present.,310482.0
92168,391364.0,Clean the value for those columns as this value will don't add any value to the assignment analysis,318476.0
92168,395734.0,"If there are null values in the column and that column is important for your analysis, then removing the rows or column will affect the analysis. That is why you need to impute the values in that column wither by mean or median. Normally my practise is to use the median to balance the effect of of the outliers and it depends case to case basis.",310482.0
92168,395637.0,Two approach one to default it . It is amount so must be defaulted with 0. But defaulting zero may chnage the AVG vaue on this column. 2nd remove the rows where raised_amount_usd is NULL. Removing the records will not impact AVG hence 2nd approach looks better and accurate.,315679.0
94233,396977.0,"Since this is a graded question, cant divulge much. All I can say is read the question properly, it asks for the company that received HIGHEST investment. Hope that helps.",317998.0
94233,397128.0,"Check you analysis then, Harshit please post the page link where you asked this question, it might help.",329512.0
94235,396976.0,Yes aggregate function ignore Null values while performing calculation. You can check for sum() in official documentation for this https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html Hope this clarifies,317991.0
94235,396978.0,"Yes , Null values are ignored when using aggregate functions on pandas series because the default value of SKIPNA parameter ie, skip na, is TRUE. If you set SKIPNA=False, then the null values will NOT be ignored by the aggregate function. Hope that helps.",317998.0
93933,395953.0,"Hi Prateek , We just need to attach the plot screenshot in PPT , but TA confirmed on other thread that tableau workbook can also be attached to Zip file. Thanks.",305652.0
93933,395992.0,You can see TA comments regarding tableau plot submission in below link https://learn.upgrad.com/v/course/208/question/92866 Hope this clasrifies,317991.0
91264,381535.0,"Hi Nitesh, They will be provided to you shortly. Please get in touch with your student mentors for exact details.",301618.0
91264,381732.0,Product key ... have been sent to registered mail ... Kindly check it,317993.0
91264,382478.0,Yes Received the Product key yesterday.please check,300687.0
94237,396979.0,"It appears that something is blocking the access through that port. (9004). it could be the firewall, try disabling the firewall check if it works.",317998.0
93090,391916.0,"1. You may remove unused columns if you feel they are not important for analysis. 2. You may remove, rows which you feel are required to be removed. 3. Yes, detailed data cleaning is required before you start your analysis. Additional Hints: --Check for data quality issue. --Properly impute missing values, if necessary. This can affect the results. -- Check the data before and after a merge.",329512.0
91288,381489.0,I just downloaded it - it worked fine perhaps try right-clicking and click on 'save link as' or 'save as' (depending on what browser you use),300694.0
91288,381518.0,The file can be saved as txt formate. Copy paste it and then paste it in your notepad and save it as txt file. Then you may use it.,329512.0
91288,382738.0,"I am trying to import this file companies and investment, even though companies_db should be something like the bank file that we had with R Programming. Nevertheless, when I import, I get an error: 1054, Any help!",315797.0
91288,390054.0,"In Firefox 1. Right click on the Comapines link, you get a option ""Save Link as..."" option, select this option 2. Window will be opened to save the file as .txt, Save file as ""Companies.txt"" 3. Some time you may not be able to open this file using panda library, you might get file format errors. 4. To solve this open the file and Save it as ""Companies.txt"" with UTF-8 format. With this approach you can load data into data frame",301113.0
96538,411402.0,"Hi Suja, while merging/ comparising columns in both the files should be in lower_Case or upper_Case as the data may be in mix format and can drop few lines due to case sensitivity. So to keep things uniform and avoid any dropping of rows, it is a healthy practice to bring column data in same Case(upper/lower)",315277.0
96538,412673.0,"Hi Suja, Request you to apply for reval with your exact concern and we will sure try to counter your issue with appropriate feedback.",301619.0
94264,397034.0,"Since the count shows ""where most other investors are investing"" and the average shows by what amount they are investing ?",308495.0
94264,397042.0,Only Average investment amount by funding Type where avarege investment amount is between 5 and 15 million needs to be considered for choosing the suitable funding type.,317811.0
94264,397051.0,"If you go through the constraint given in the case study and follow that then I am sure you can figure it out what needs to be done. Considering the constraints of Spark Funds, you have to decide one funding type which is most suitable for them. Calculate the average investment amount for each of the four funding types (venture, angel, seed, and private equity) and report the answers in Table 2.1 Based on the average investment amount calculated above, which investment type do you think is the most suitable for Spark Funds? Hope this will help.",317991.0
94264,397043.0,For checkpoint 2 please get the average dollar amount for the funding type mentioned.,314197.0
94246,397102.0,Found this video helpful: https://www.youtube.com/watch?v=oY62o-tBHF4,317514.0
94246,397002.0,"Please go through the pd.melt documentation. Since it is a graded componenet can't help you much, but your steps are right and you are in a right path.",329512.0
94246,397039.0,I found this very helpful https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.melt.html,314197.0
94246,397236.0,"Once you have done the melt, just remove the rows where the value is not true or zero",312376.0
94246,397417.0,Melt function would try to transpose the data. In maping excel we have 680 odd rows i.e. categories and 9 columns i.e. main sectors. So after the melt function you would have rows*cols rows (cartesian product ) and 3 columns ( Category/Sector/True-False). Now you simple need to flter the records where True-False is 1 giving you the result of all categories with main sector.,304814.0
94273,397085.0,df = df[~df.country== 'CHN'],317811.0
94273,397081.0,You can just ignore this in your analysis. If you want ot drop it than you can use df = df[df.country!= 'CHN'],317689.0
94278,,nan,
94272,397084.0,You need to manually search the country name for country code and then check whether the name exists in the English speaking countries pdf provided in the downloads segment of assignment.,317811.0
94272,397072.0,https://learn.upgrad.com/v/course/208/question/93752 https://learn.upgrad.com/v/course/208/question/94023 this should help,301115.0
94258,397018.0,"You can use ""transform"" operation in conjunction with ""group by"" to do this. Please refer the below link. http://pbpython.com/pandas_transform.html",310467.0
94258,397019.0,It would have repetitive data in the two columns you would create an another dataframe with count and sum and join with final frame you have,314197.0
94294,397160.0,Data cleaning activity has to be done such that it affects only those places which is having incorrect data. Inadvertently changing other data would probably be considered as introducing errors.,313826.0
94294,397159.0,"the TAs response to this was, It is a challenge and you need to figure out a way to handle those. hope that helps.",317998.0
94294,397191.0,But I think Enterprise 2.0 will be ignored when merging using an inner join. If not then check for another option. Since it's a graded component can't help much.,329512.0
94259,397022.0,"Try this: mapping['category_list'].apply(lambda x: x.replace('0', 'na'))",329512.0
94297,397155.0,"From your previous checkpoints you would have derived the suitable Funding Type which they are generically referring as ""FT"" and the top 3 English Speaking Countries which they are referring as ""Country1"",""Country2"" and ""Country3"". Hope this clarifies.",313826.0
94297,397177.0,"You, you need to find these then FT will become you found funding type, and countries will be your top three countries.",317811.0
94297,397157.0,"It means, uptill this point your analysis would have revealed to you a specific funding type suitable for Sparks Funds, lets call that funding type as FT. Also, your analysis would have revealed 3 top english speaking countries, we can call those as country1, country2, country3. and then proceed for the next part of analysis using based on these information. hope that helps.",317998.0
94275,404247.0,Used tablue will more easy,311227.0
94275,404248.0,"Using Tablue will be more easy to do the plotting, it is bar chart only",311227.0
94275,397078.0,This can be done in pandas only. However you can choose to do it in SQL or tableau as well. But you should take it ahead in pandas as other checkpoints can be taken forward as well.,317689.0
94275,397088.0,Using Pyhton Code Only.,317811.0
94275,397100.0,It is mentioned to use dataframe : Identify the top three English-speaking countries in the data frame top9.,312758.0
94275,397106.0,i guess data analysis part is completely doable in python and that's what is expected also..,316349.0
94275,397123.0,"Use Python for analysis, for plots you may use Tableau but that is optional.",329512.0
94303,397164.0,"Since it is an assignment question can't answer directly. But you can go through following description: For the chosen investment type, make a data frame named top9 with the top nine countries (based on the total investment amount each country has received) Read it carefully and decide which columns to keep in top9 dataframe. Hope this will help.",317991.0
94303,397168.0,"It can consist as many or as less columns as you wish, as there are no guidelines on what all columns it should consist. Just ensure that it captures the essense of why the dataframe is being created: namely top9 countries based on the total investment amount each country has received.",313826.0
94303,397176.0,It should contain all columns of dataframe for the top 9 countries,317811.0
94303,397996.0,Confused with the same. I think we need to have all the data from the master frame for only the top 9 countries.,308440.0
94303,398389.0,"when it was mentioned to keep columns keep columns ,when not keep all the data which are required to analyze in the frame..",318005.0
94307,397281.0,"You can use df['col'] = df['col'].str.replace('oldvalue', 'newvalue')",314244.0
94307,397187.0,"str.replace('oldvalue', 'newvalue')",317811.0
94260,397037.0,"After your group by command enter following' .sort_values(['col1,'col2','col3'],ascending=False)' See if you need ascending true or false per your need",314197.0
94260,397040.0,"dataFrame.groupby('columnname').sort_values(['column1', 'column2', 'column2'])",317811.0
94277,397104.0,learn about the best plot for two dimensions and one measure parameters.. Tree Map does show the results in a good way but then yeah there are lot many companies and sectors amd hence it is becoming tough to visualize it.. but still i couldn't find a right plot then that till now..,316349.0
94277,397126.0,"What is your issue?, if you are not able to plot it in one plot, use sub plots. Use groupby data and then plot it.",329512.0
94287,397121.0,You can follow TA's suggestion from below link: https://learn.upgrad.com/v/course/208/question/93265,317991.0
94287,397120.0,You need to clean category list in master dataframe and also in mapping dataframe before joining by inner,317811.0
94287,397134.0,"You may find some of the primary sectors for whom main sectors are not identified per the mapping. Since we are focusing on 8 main sectors, I think we are okay to remove those Nan rows. Unless, you want to further apply statistical analysis and impute those missing main sectors with a value. It all depends on what assumptions you take. Secondly, see that the mapping has clean data and if it also needs data clean up.",302740.0
94309,397217.0,"Hi , How about using below syntax to create new column. Df['newcolumn'] = #series Thanks.",305652.0
94309,397240.0,"please check assign and transform function which would make job easy,both columns creation and assignment can be done in a single Line",301115.0
94309,397221.0,"df_c = df.loc[df.COLUMNNAME== ANYVALUE, :].copy() Pandas isn't 100% sure if you want to assign values to just your df_c slice, or have it propagate all the way back up to the original df . To avoid this when you first assign df_c make sure you tell pandas that it is its own data frame (and not a slice) by using. Please refer https://github.com/pandas-dev/pandas/issues/17476",317811.0
94311,397228.0,Use transform fucntion to add the required columns Kindly go throgh below link it will be helpful:- http://pbpython.com/pandas_transform.html,303673.0
94316,397250.0,Submit it but it was not recommend to use any other files. Please mention in your notebook for the same as a comment.,329512.0
94316,397245.0,this was no where mentioned.. better to ask this to your mentor.. :),316349.0
94320,397278.0,You can manually verify the results with the pdf given and put that name.,314244.0
94320,398091.0,You can manually find the top 3 English countries and then mention in brief about your process in the notebook. This will be fine. About NA's drop them.,329512.0
94320,397298.0,"Yes, you may get all 'NA' grouped by as country and sum of amount of investment for NA can be greater than any other country.",317811.0
94320,397277.0,yes that will affect.. I guess null country has got second highest number of entries.. better to drop it..,316349.0
94332,397318.0,https://learn.upgrad.com/v/course/208/question/94195 https://learn.upgrad.com/v/course/208/question/94303 https://learn.upgrad.com/v/course/208/question/93885 there is a lot of questions asked again for check point 3,301115.0
94332,397319.0,You can go through the below link https://learn.upgrad.com/v/course/208/session/19904/segment/101229 In this link it is briefly explained what need to be done.,317991.0
94332,397399.0,"1. First, You need to find the total investment amount by each country using the investment amount where funding round type is FT(figured out in checkpoint2). 2. Then, you need to sort by total investment amount in decreasing order and take top 9 records only. 3. Then, you need to find top three countries by total investment amount whose official language is english which can be checked manually in englisg speaking countries pdf provided in assignment.",317811.0
94332,398391.0,"and i m again in doubt that no constraint of 5 to 15 will be applied here, it will be applied later with D1,etc i think their should be a obvious reason for that",318005.0
94322,397280.0,yes . you are correct,318084.0
94322,398095.0,"Yes, your correct and so your analysis is.",329512.0
94322,397343.0,"A quote from the Problem statement: ""Also, you know the top three English speaking countries and the most suitable funding type for Spark Funds. Let’s call the three countries 'Country 1', 'Country 2' and 'Country 3' and the funding type 'FT'. "" From this it is quite Evident that the Countries you found in checkpoint 3 , country analysis, are the country1, country2 and country3 . Hope that helps.",317998.0
94337,397340.0,"Since this is a graded question, cant say much. But, what you can do is take input as a string and use the str.split() function, and specifying the | pipe characeter as a delimeter to split and return the first sliced string as output. Hope that helps.",317998.0
94337,397341.0,You can follow below link where TA provided approach to do this: https://learn.upgrad.com/v/course/208/question/93892 Hope this will help,317991.0
94337,397396.0,"df['category_list'].apply(lambda x: x.split(""|"")[0])",317811.0
94335,397335.0,i don't think that would be required unless that is coming on your top 3 list of sectors for each three countries and the selected FT. I hope this sounds logical!,316349.0
94335,397337.0,"Yes , as per TA's response use ""inner join"". You can go through this response for more clarification: https://learn.upgrad.com/v/course/208/question/93980 Hope that helps.",317998.0
94335,397397.0,"Yes, INNER JOIN should be used to merge the master frame and mapping dataframe",317811.0
94335,397355.0,"Thanx. Another observation is one of the primary sector values is 'biotechnology and semiconductor'. Since this is not present in mapping dataframe, the main sector will be null. But biotechnology and semiconductor are present in the mapping dataframe as separate values.",312376.0
94339,397356.0,export to CSV or Excel commands are available in pandas,301115.0
94339,397413.0,"Just like you import csv, similar way you can export csv",304814.0
94339,397461.0,yes found the way...thanks,301644.0
94339,397815.0,this will help you https://stackoverflow.com/questions/45465105/python-pandas-data-frame-to-csv?noredirect=1&amp;lq=1,305804.0
94348,398343.0,"No, better to export complete master data frame and all the plots should be created from this dataset only. For the last plot-3 just create a calculated field where investment_usd is in between 5M to 15 M and create a filter on this field equal to ""True"".",300735.0
94348,397409.0,you can plot any data as per your observations that you want to show in tableau,317811.0
94348,397445.0,Its better to export the clean dataframe,318329.0
94348,397531.0,There is no mention of which dataframe to use and whether any filtering needs to be applied on the dataframe. So it is up to your discretion as to which dataframe to use to be able to plot these visualizations.,313826.0
94348,398186.0,"The plots will follow all the conditions that were imposed like investment type, 5-15 and ENGLISH SPEAKING.",329512.0
94346,397429.0,"This is the requirement mentioned in the Checkpoint 2 Spark Funds wants to choose one of these four investment types for each potential investment they will make. Considering the constraints of Spark Funds, you have to decide one funding type which is most suitable for them. 1. Calculate the average investment amount for each of the four funding types (venture, angel, seed, and private equity) and report the answers in Table 2.1 So it should be funding round type only. Hope this will help.",317991.0
94346,397395.0,Average needs to be calculated only by funding round type.,317811.0
94349,397421.0,What is the issue here? Keping category_list column in main dataframe does not harm. You just have to join master_frame and mapping frame using primary_catgory from master and categry from mapping.,304814.0
94349,397444.0,"When you use right_on and left_on, you get two columns with same series. To avoid this, you can rename the column of mapping dataframe to primary_sector and use just on='column' to have a single column after merge",318329.0
94349,397527.0,"You could do either of the two methods: Join the two dataframes as is using left_on=... , right+on=...and then drop the redundant column or Rename a column in one of the dataframe and join uisnf on=""&lt;colname&gt;"".",313826.0
94349,397490.0,"yes, you are doing correct way. You need to manually drop the columns which are not required or duplicate in merged dataframe.",317811.0
94347,397394.0,"In ppt, you need to paste snapshots of the graphs plotted in Tableau",317811.0
94347,397424.0,You can follow below link in which TA has clarify what is needed. https://learn.upgrad.com/v/course/208/question/92866,317991.0
94347,398101.0,"Submit it in a PPT, also they are optional.",329512.0
94352,397437.0,"You have to use D1, D2, D3",318329.0
94352,397450.0,"i guess you need to use the master frame since D1,D2 and D3 is already sorted on each top numbered country.. whereas the question is asking you to highlight top 3 in one chart itself.. and same does apply for sectors.. so yes, i think you should use master frame..",316349.0
94352,397467.0,"You need to use the 3 new dataframes i.e D1,D2,D3 for the 3 countries",311254.0
94352,398325.0,"this has to be done using D1, D2 ,D3. you can contatenate them and then use the dataframe for the sector plot.",304319.0
93111,392002.0,Got it,317990.0
93111,392710.0,You can use pd.melt,329512.0
94351,397435.0,"It is clearly mentiond in the below link that ""After this, you will need to work only with the master frame ."" For your reference here is the link https://learn.upgrad.com/v/course/208/session/19904/segment/101324 The above statement is mentioned after the table 1.1 Hope this will clarify your doubt.",317991.0
94351,397440.0,You have to merge the mapping dataframe with the master dataframe,318329.0
94351,397464.0,"Yes, Merge the mapping frame with the master frame. It is mentioned in the case study that we have to work on the master frame.",311254.0
94351,397524.0,"By the time you reach Checkpoint 4, you should already be working on a merged dataframe master_frame. In checkpoint 4, you need to merge master_frame dataframe with mapping dataframe.",313826.0
94350,397436.0,"It is clearly mentiond in the below link that ""After this, you will need to work only with the master frame ."" For your reference here is the link https://learn.upgrad.com/v/course/208/session/19904/segment/101324 The above statement is mentioned after the table 1.1 Hope this will clarify your doubt.",317991.0
94350,397441.0,You have to merge the mapping dataframe with the master dataframe,318329.0
94350,397525.0,"By the time you reach Checkpoint 4, you should already be working on a merged dataframe master_frame. In checkpoint 4, you need to merge master_frame dataframe with mapping dataframe.",313826.0
93108,391984.0,You may want to try out different encoding options. It will work ...,306250.0
93108,391991.0,It is important to know how and where are you giving your encoding format. pandas read method hasoption to specify encoding type. Please use that.,318554.0
93108,392709.0,"Trying different encodings will not solve the issue completely, though you will be able to load the file. For encoding issue, you need to google and find the correct encoding, As this is a part of the graded component, I will not be able to tell you the right answer, but the hint for this is just google: ""non-English characters in pandas encoding issue"" The solution is available in StackOverflow, once you google it. Also run this code and see: rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] If you can see some non-english words in permalink then encoding is not solved.",329512.0
93334,392724.0,"Yes, you are correct.",329512.0
93334,393014.0,Yes It's correct.,318476.0
93334,398122.0,Correct!,316891.0
94357,397482.0,figured it out. columns were case sensitive,300685.0
94357,397493.0,"change values in left_on= df1col , right_on= df2col to either str.lower() or str.upper() and then join.",317811.0
94356,397486.0,"This is not any type of error but a challenge for you to solve as mentioned by TA in below link. https://learn.upgrad.com/v/course/208/question/91115 For ""Enterprise 2.0"" you need to take care of it. Rest TA can clarify. Hope this will help.",317991.0
94356,397522.0,You need to handle the discrepancies only at places where it is appropriate and not everywhere.,313826.0
93117,392046.0,I calculated both of them and added the corresponding values in a new column against that main sector. I know it is ambiguous in which they asked but that's the only logical way I could do.,310974.0
93117,392714.0,"You can just sector, count of investments and sum of total amount for a country in a Data Frame. For this you can simply use groupby.",329512.0
93118,392042.0,You don't need to handle those as they are in the scope of the analysis.,310974.0
93118,392465.0,Even I have the same query - should they mapped to Others? I would love for TA to answer this. I have a few more questions like this. Will raise my own thread,300694.0
93118,392716.0,"Use inner join while merging, This will ignore all the absent categories. Also do check first if there is any data quality issue.",329512.0
94367,398165.0,"The category_list column contains values such as 'Biotechnology|Health Care' - in this, 'Biotechnology' is the 'main category' of the company, which you need to use.",329512.0
94367,397512.0,"Answer itself lies in your question. Application Platforms|Real Time|Social Network Media You can figure out what is separating Application Platforms, Real Time and Social Network Media from each other. It is "" | "" Hope this will help.",317991.0
94367,397521.0,"My guess is that you are getting confused between the category_list column present in the companies dataset with the category_list column present in mapping dataset. Only the companies.category_list has multiple categories seperated by the ""|"" pipe symbol. No such seperator exists in the mapping.category_list column. Hope this helps.",313826.0
94368,397530.0,There is no mention of which dataframe to use and whether any filtering needs to be applied on the dataframe. So it is up to your discretion as to which dataframe to use to be able to plot these visualizations.,313826.0
94368,398072.0,The dataFrame can be the one after merged with mapping or the Master_frame created at initial Stage.,318557.0
94373,398816.0,"Ignore ""Blank""",312953.0
94373,397556.0,I dont think we have any Nulls or Blanks in our master_frame.category_list and that makes it 8.,314197.0
94373,397591.0,"""Blanks"" is not a main sector.",313826.0
94373,397635.0,When you do filter on blank in mapping file you will find that there is only one row and that too balnk which doesnot have any category_list value. Secondly it is clarified in below link by TA that blank is not a sector. https://learn.upgrad.com/v/course/208/question/93867,317991.0
94369,397517.0,"Just go through this explanation given in case study For some companies, the category list is a list of multiple sub-sectors separated by a pipe (vertical bar |) . For example, one of the companies’ category_list is Application Platforms|Real Time|Social Network Media. You discuss with the CEO and come up with the business rule that the first string before the vertical bar will be considered the primary sector . In the example above, ‘Application Platforms’ will be considered the primary sector. Extract the primary sector of each category list from the category_list column",317991.0
94369,397520.0,"My guess is that you are getting confused between the category_list column present in the companies dataset with the category_list column present in mapping dataset. Only the companies.category_list has multiple categories seperated by the ""|"" pipe symbol. No such seperator exists in the mapping.category_list column. Hope this helps.",313826.0
94369,397545.0,"""category_list"" column in the merged master_frame has different sectors separated by pipe (|) symbol e.g. check for column with ""company_permalink"" as ""/organization/-qounter"". As mentioned by Vinay above, either you are confused by the mapping file with same column name or you have just looked at a subset of data where there is only one category and hence no pipe(|) symbol.",308440.0
94369,397523.0,"Aditya, you must check category list column in companies.csv where you have multiple categories present. Mapping.csv is a one to one mapping of category list with 8 main sectors.",319876.0
94383,397600.0,You can export the dataframes to csv files using to_csv() function and later use the csvs to plot.,313826.0
94383,397614.0,"hi Brijesh, you need to generate some new xls files to be brought in Tableau for plotting. the questions asked for plotting will help you in identifying which data to be generated as XLS file.",311686.0
94383,397612.0,Thanks Vinay for quick response. Are we going to use .xls file by importing in tableau where we have filledin the outputs or we going to use the csv file where we have run queries? pls suggest.,311117.0
94383,397660.0,"if CSV file generated,u can import in tableau using other files option, no need for XLS format.",301115.0
94383,398382.0,While generating plots in tableau don't forget to highlight the top9/top3 categories as asked in question.,313767.0
94383,398809.0,"Use this to write dataframe to csv... no need of excel files etc etc df.to_csv(file_name, sep='\t', encoding='utf-8')",312953.0
94372,397534.0,you have to make two new columns with total count and total amount per main sector wise,318017.0
94372,397536.0,You can follow below link having TA comments on similar kind of question https://learn.upgrad.com/v/course/208/question/93399/answer/393593 Hope this will help.,317991.0
94385,397609.0,[3. Top Sector (based on count of Investment)] -- You need to specify the name of the Top Sector based on the count of investments in that sector. [6. No of investments in the top sector(refer to point 3)] -- You need to specify the number of investments in the Top Sector as mentioned in point 3. Hope this clarifies.,313826.0
94385,397611.0,"in point 3, it is asking which is the top sector based on count of investment. you have to mention that sector as answer. let us say ABC. in point 6, it is asking no. of investments made in that sector. a number needs to provided here which is the count of investments made in top sector i.e. ABC hope it helps.",311686.0
94385,397615.0,"Hi , Top Sector (based on count of Investment) -- here you need to find 'the top main sector name ', based on the number of investments (occurances) made in main sectors. No of investments in the top sector(refer to point 3) -- here you need to mention the number of investments made in top main sector i.e. from the above point. Thanks.",305652.0
94385,397636.0,Thanks all for clarification ...was focusing too much on numbers,317514.0
94001,396104.0,"if multiple companies share top count after all the filters, we can propose the Same to Sparks CEO",301115.0
94001,396102.0,"There is important facts given Raised Amount,English Speaking and ease of Business where alrady most investment has done. Based on the three we can identity the top 3 compnies to invest.",307843.0
94001,396178.0,This analysis is incorrect as the you forgot to sum raised fund amount by companies as one company may exists multiple times in master dataframe.,317811.0
94001,396206.0,Top company refers to one which got highest total investment after grouping by company name.,310511.0
94387,397637.0,"A plot showing the fraction of total investments (globally) in venture, seed, and private equity, and the average amount of investment in each funding type. This chart should make it clear that a certain funding type (FT) is best suited for Spark Funds. A plot showing the top 9 countries against the total amount of investments of funding type FT. This should make the top 3 countries (Country 1, Country 2, and Country 3) very clear. A plot showing the number of investments in the top 3 sectors of the top 3 countries on one chart (for the chosen investment type FT).",317811.0
94387,397620.0,There is a sample PPT attached which can be used for reference. The only guideline is that the PPT should not exceed 10 slides and rest is up to you to decided how and what you want to present. You can download the sample PPT from below link. It is at the bottom of the page https://learn.upgrad.com/v/course/208/session/19904/segment/101227,313826.0
94396,397937.0,Unique values can be found before data cleaning. You can perform Data cleaning after merging the required dataframes.,311254.0
94396,397699.0,"Yes, do it after merging",318329.0
94396,397694.0,You can perform data cleaning after merging the two dataframes. Else you may loose some important data,303673.0
94396,397702.0,"Hi , Considering unique values -- it is part of data preparation and consider unique values before cleaning the data from dataframes companies and round2 . It is advised that data cleaning task should start after merging dataframes into master dataframe.",305652.0
94380,397593.0,"Try using single forward slash ""/"" in the path like pd.read_csv{""C:/IIITB...",313826.0
94401,397734.0,"for plotting , we can use master frame after main sector derived, also we can use D1,D2,D3 for some plots",301115.0
94423,397809.0,"convert to millions,it's a format issue",301115.0
94423,397822.0,"Hello Pratik, User round function &amp; Convert into millions to get value in correct format.",320195.0
94423,397831.0,You can follow the approach whilch you followed during movie assignment.,317991.0
94423,397930.0,you can rather convert it into integer. eg. series_top9.apply(lambda x: int(x)),318458.0
94423,397982.0,"You can also use below command , which will correct the display issue pd.options.display.float_format = ""{:.2f}"".format",311861.0
94430,397880.0,"investment excel file , a ppt of presentation and jupyter file with the code in a zipped folder",318017.0
94430,397857.0,"Hi , You need to submit Python sollution file, Excel file(downloadable ) , PPT(downloadable). These three are mandatory and evoluated. These need to be zipped in a file for submission. Thanks.",305652.0
94430,397858.0,"In a zipped file include the following - python file where the coding is done, tableau plot files and the ppt.",311254.0
94430,397932.0,"Check Final submission page - https://learn.upgrad.com/v/course/208/session/19904/segment/101231 The zip file should contain the main Python file, one presentation doc (in PDF format) and one Excel file. Submit the zip file.",318458.0
94430,397980.0,Do we need to submit Tableau Plot points as final submission ? Or use this in PPT.,311861.0
94430,398282.0,"Please submit ppt in pdf form, else it may not get uploaded due to size issues. Ppt to encompass tableau plots.",313767.0
94430,398448.0,"Jupyter file, Investment excel file, and ppt in pdf format, all together in a zipped folder. Hope this helps.",316133.0
94430,399197.0,"Code, all the checkpoints in one excel and pdf form of final PPT so total 3 files",300735.0
94413,397818.0,"did u try importing to tableau and saw the data types,also check the data types of the dataframe again before exporting",301115.0
94413,398136.0,Export to excel and then check maybe csv doesn't work.,329512.0
94416,397759.0,"As the error suggests, the object on which you are trying to run to_csv() function is a ""tuple"" object. You can check the type of object by using the type() function.",313826.0
94416,397985.0,"If your objective is to convert Data frame into CSV , below command can help you df.to_csv('CSV_File_name.csv') # This will convert data frame into CSV with file name as CSV_File_name",311861.0
94420,397808.0,"No, i guess funding round code was no where used.. you can group it on the same column though..",316349.0
94420,397933.0,"No, you do not need funding_round_code for that question.",318458.0
94421,397859.0,diagramatic representation of flow chart seems to be the best option.,311254.0
94421,397881.0,a flow chart of the approach you followed to solve the problem would be best i guess :),318017.0
94421,398019.0,Flowchart containing the description of checkpoints sequentially is what you can give.,310511.0
94421,398821.0,please refer to the answer given by TA - https://learn.upgrad.com/v/course/208/question/94494,318458.0
94421,399199.0,yes we need to make one flow chart,300735.0
93564,394093.0,"i guess yes, we're supposed to clean up the data before starting up the analysis.. i read it on one of the post that even TA was suggesting to do the same..",316349.0
93564,394107.0,"Yes, Data cleaning by dropping missing values from country and raised amount column will make analysis more accurate. Also, use the pdf which is provided in Objectives section in Assignment. http://www.emmir.org/fileadmin/user_upload/admission/Countries_where_English_is_an_official_language.pdf which contains the list of countries whose language is english.",317811.0
93564,394218.0,"For language, you are provided with the list of English speaking countries on this page: https://learn.upgrad.com/v/course/208/session/19904/segment/101226",329512.0
93564,394588.0,For calculating the average funding per funding type - which is checkpoint No 2 you need not drop the country code as the country code has no bearing on the checkpoint 2. However in the check point 3 you may drop the coutry code with NaN as you need to find the top 9 countries based on investment.,310629.0
93564,395053.0,Shall we drop the raised amount column or replace value with 0?,320103.0
92605,390145.0,"If data is loaded into the dataframe and you are able to see it, then it should be ok.",318084.0
92605,390220.0,Take data in to excel and see there wheter you are getting uniques no same as in excel and df code,317811.0
92605,390712.0,"When you join 2 data frames, data should be there in the merged dataframe.",301113.0
92605,390488.0,"When there are no non-English weird characters in the loaded data frame. Check using this: rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :]",329512.0
92812,391053.0,"No. Primary key is not a must. It depends on business requirements. If you need to uniquely identify a record in your table, its best to declare the column/columns that does it as primary key. Thus primary keys can be both built on 1 column(single) or multiple columns(composite). Usually when theres a business need of updating a table, its best to declare a primary key on the table. Based on the primary key, you can decide whether to insert the record or update an existing row of the table(if primary key of new record matches to that of an existing record). Hope this explains.",310511.0
92812,391212.0,"For this, you need to identify the primary key and then use it to merge the files.",329512.0
92812,392133.0,"Primary keys also help you in better performance against not using them at all, there is lot of optimization that happens to maintains indexes on primary keys, you will be able to notice such difference with large tables, query by adding primary key once, and then removing it, you will see the difference in amount of time taken considerably",319770.0
94440,397978.0,"You can refer below link , if your question is related to Tableau https://onlinehelp.tableau.com/current/pro/desktop/en-us/buildexamples_boxplot.htm https://community.tableau.com/thread/242465 let me know if this helps .",311861.0
94443,397919.0,Do we need to apply amount between 5 to 15 mn in this case?,302741.0
94443,397951.0,"for country analysis, we can consider all ranges of amount invested",301115.0
94443,397917.0,Case Study Description from below link is as follows: https://learn.upgrad.com/v/course/208/session/19904/segment/101229 Spark Funds wants to see the top nine countries which have received the highest total funding (across ALL sectors for the chosen investment type) You can guess which operation is required here. Hope this helps.,317991.0
94443,398424.0,total sum,310504.0
92691,390473.0,"Yes, they are same but you need to convert them to either upper or lower case to get the correct count.",329512.0
92691,390501.0,only Upper and lower case there... apply mysql's UPPER function and use for sql operations,318454.0
92691,391339.0,Hi.. the data is present in both upper case and the lower case.. firstly we need to either convert it to lower case or upper case.. and after that we have to identify the unique companies.. Hint- all this can be done in python itself..,305129.0
94449,397947.0,mapping dataframe with only value 1 indicates the main sector for the category list,301115.0
94449,398006.0,You have to convert the columns names of values containing 1 to a series and assign them. There's an inbuilt method to do that which you can explore,318329.0
94449,398017.0,The logic is to select column headers where row values corresponding to value '1'.,310511.0
94449,398038.0,Mapping with values 1,315679.0
94449,398064.0,first create that data frame and pick only with values 1,319869.0
94449,398067.0,"An inner join merge of both mapping and Master_frame does this automatically. Since some primary_sector come under 1 and some other 0, its better to consider whole DataFrame.",318557.0
94449,398281.0,pd.melt will change the layout of your dataframe and then you can easily select values which are 1,313767.0
93533,393817.0,"No need to treat it, as it is not used anywhere.",329512.0
94452,398176.0,Only top English speaking countries out of the top 9 countries having larger investments in 'venture' funding round type,319006.0
94452,398099.0,Please match your results with the english speaking countries list given in the begining of the group project. Then consider only TOP 3 of those countries with English as their official language..,307176.0
94452,398065.0,It is expected as Top 3 English Speaking countries only.,318557.0
94452,397975.0,"top 3 english speaking countries, i believe.",317998.0
94452,398003.0,"By 6.2 do you mean the plots? In that case, it's mentioned as top 9 countries",318329.0
94452,398013.0,6.2 - Top 9 countries. 6.3 - Top 3 english speaking countries,310511.0
94452,398037.0,Yes English speaking countries. The objective of case study is to invest in English speaking countries.,315679.0
94452,398955.0,top 3 english speaking countries,319302.0
94452,399096.0,Need to consider the top 3 English speaking country but showing China and telling the reason for dropping it will be more helpful.,300735.0
92866,391078.0,"on submission page it says: https://learn.upgrad.com/v/course/208/session/19904/segment/101231 The zip file should contain the main Python file, one presentation doc (in PDF format) and one Excel file. Submit the zip file below.",300694.0
94460,398167.0,"I had the column labels in the dataframe. But was not getting exported out. I was able to do by using the following syntax. df.to_csv('top3mainsector.csv',header=['values'])",317514.0
94460,398049.0,"First thing to check would be in your file. If the file contains header or not, else you may have to import properly. You can use encoding, comma separator and index to get the proper data.",317600.0
94460,398108.0,"First, add the column labels in the pandas dataframe and then export it. You can also do it manually.",329512.0
94470,398073.0,dataframe can be exported to excel or CSV file from python notebook which can be imported to tableau,301115.0
94470,398075.0,Export the correct (which has all the required) columns to csv and import that to the Tableau for graph generation.,320073.0
92946,391411.0,Right Click -&gt; Select-&gt; Save Link as -&gt; Then you can save the Companies.txt file.,301652.0
92946,391415.0,"or when it's opened in the new tab, just right click and save it in your laptop.. :)",316349.0
92946,395029.0,It is a text file. So you have to right click and save the file.,318421.0
94483,398153.0,Plots and your approach and conclusion. Please state all the business results and no need to explain the code and all.,329512.0
94471,398077.0,"I guess you're talking about category list in companies dataframe to category in mapping file, primary sector holds category from companies DF, hope this gave you about what is required for joining DFs",301115.0
94471,398079.0,You have to merge on the category_list which is in mapping df and primary_sector which you created after splitting the original category_list.,320073.0
94462,398082.0,"what ever analysis done in python needs to be represented in plots using tableau,so the answer should be the same in both python and plots",301115.0
94462,398069.0,There is option to filter the values in the Tableau. Check filter option in Tableau.,320073.0
94462,398107.0,"If you are facing issue using Tableau, use Python and complete your analysis, but Tableau has an option to do so.",329512.0
94462,399006.0,You can filter the values using filter option on the column,317600.0
94481,398140.0,"Yes, please consider it for analysis. You will get to know about it later.",329512.0
94473,398104.0,"6 will use the same results from 5, you need to plot as you go along with the analysis.",329512.0
94477,398114.0,You need to take a snapshot of it and paste it in PPT.,329512.0
94493,398189.0,there are multiple ways pivot table and transform are one of thems,318017.0
94493,398190.0,You can use pd.melt and then use group by.,329512.0
94487,398179.0,"Top 9 you will find by using the highest raised amount. And this is not a subset, once you will find the top 3 English speaking countries, you need to filter the data based on 3 countries that is D1, D2 and D3.",329512.0
94497,398200.0,"Use groupby on Sector as below: df.groupby(['sector']).raised_amount_usd.agg(['count', 'sum'])",329512.0
94474,398102.0,Other is a sector and is needed to be considered.,329512.0
94474,398103.0,"Checkpoint quoted as below: Checkpoint 4: Sector Analysis 1 This is the third goal of analysis — sector analysis. When we say sector analysis, we refer to one of the eight main sectors (named main_sector) listed in the mapping file ( note that ‘Other’ is one of the eight main sectors ). As this is among the 8 main sector so whatever assumption you make do it wisely. Rest TA can clarify.",317991.0
94496,398198.0,you can use tableau with the python data,318017.0
94496,398197.0,"No, add them to your PPT. You may take a snapshot of it and then may paste it to PPT.",329512.0
94478,398123.0,"Cleaning should be done before the merge, there is a data quality issue in the file which you need to identify, rest you need to figure out. Not much needed.",329512.0
94484,398159.0,the final version after merging the mapping data should be used for tableau analysis,318017.0
94484,398150.0,Version-2.,329512.0
94500,398207.0,"No, it can't happen. Please check your analysis.",329512.0
94500,398208.0,yess it has been mentioned on the site also,318017.0
94501,398211.0,"For choosing top 3 English speaking countries you need not consider 5-15M band but after that, for sector analysis, you need to filter the data for 5-15M and then do your analysis.",329512.0
94479,398127.0,df.groupby('id').head(10) https://stackoverflow.com/questions/20069009/pandas-get-topmost-n-records-within-each-group,329512.0
94517,398257.0,The three data frames should contain: All the columns of the master_frame along with the primary sector and the main sector The total number (or count) of investments for each main sector in a separate column The total amount invested in each main sector in a separate column My question is about the second bullet point. A collum is to be introduced right?,318007.0
94517,398253.0,"No need to add this just report this in the excel files provided, also make sure you have the code present in the notebook for this step.",329512.0
94489,398180.0,"Yes, other is the main sector.",329512.0
94489,398183.0,When you will plot other will be same as any other sector.,329512.0
94489,399768.0,"yes, it has important categories such as e-comm and telecommunications",300735.0
94475,398105.0,"No need to worry, just use replace, those will be ignored while merging using an inner join.",329512.0
94495,398196.0,yes you can follow the link : https://medium.com/escaletechblog/writing-custom-aggregation-functions-with-pandas-96f5268a8596,318017.0
94495,398195.0,You may use PIVOT. Any approach is fine to get the answer.,329512.0
94510,398236.0,"No this can't happen, check if you have filtered your data for 5-15M before sector analysis?",329512.0
94502,398212.0,"Use this: df['category_list'].apply(lambda x: x.split(""|"")[0])",329512.0
94491,398187.0,"Use the logic and decide, for raised amount drop all the nulls. Fro other you need to decide.",329512.0
94520,398260.0,I dont think deletion of any column is required. You can retain all the columns in addition to that we have to add count and investments also to the D1/D2/D3 dataframe.,318084.0
94520,398262.0,Only the columns you think are important for analysis. You may have columns that if left out after cleaning.,329512.0
94520,398709.0,"You can have all the columns, but those 3 additional columns are must.",318448.0
94520,398818.0,"You may include all the columns from master_frame dataframe.. but it's must to include primary_sector and main_sector columns too in D1, D2 and D3 dataframes.",312953.0
98911,420809.0,you can use; df = df.loc[df['ColumnName']==1],316349.0
98911,420827.0,dfnew = df{df[column]=1] this will create a df where only 1 value will be there,318017.0
94503,398216.0,"Not needed, you can calculate it and can report it on the excel file, just keep in mind you need to have a code for finding this in your notebook also mention it in your notebook that you used this method.",329512.0
94521,398266.0,Yes. You are correct. You have to group by company,318084.0
94521,398381.0,You need to choose the best company based on the number of investments and not the sum of investment.,329512.0
94521,399022.0,We need to consider max amount of funding or max count of fudning?,317982.0
94524,398280.0,Do you mean any dataframe you created in jupyter notebook or the whole notebook ? If it is specific dataframe then below is the sytax: df.to_csv(r'Path where you want to store the exported CSV file\File Name.csv'),317991.0
94524,398376.0,Please follow this for examples: https://xlsxwriter.readthedocs.io/example_pandas_simple.html,329512.0
94523,398277.0,"No. TA has already clarified that the ""Other"" sector should not be ignored and it should be considered as one of the main sectors.",318762.0
94523,398279.0,See https://learn.upgrad.com/v/course/208/question/94481 for TA confirmation.,318762.0
94523,398339.0,"No, others sector cannot be ignored. It has some very important categories such as e-comm, telecommunications which cannot be ignored while making an investment decision.",300735.0
94523,398728.0,"""Blank"" needs to be ignored from mapping.csv and others needs to be included.",318448.0
94523,398812.0,"Only ignore ""Blank"". ""Other"" is important in analysis.",312953.0
94526,398297.0,number of investment the question is not framed correctly,318017.0
94526,398310.0,"It's the number of Investment round found in Rounds2.csv file. I am sure, by now you might have already merged company , rounds and mapping data. That should help you finding the count of investment for the correspoding sector.",302740.0
94526,398322.0,"Serial # 3 is ""Top Sector name (no. of investment-wise)"". Number of investments in top sector (3) implies Number of investments in the to sector that is identified in Serial #3.",318762.0
94522,398272.0,What error you are getting ?,317991.0
94522,398378.0,Use this link and check verified answers: https://learn.upgrad.com/v/course/208/question/94151,329512.0
94516,398250.0,You need to use the count of investments for finding the companies and not the sum or max.,329512.0
94504,398218.0,"The description states that: Create three separate data frames D1, D2 and D3 for each of the three countries containing t he observations of funding type FT falling within the 5-15 million USD range . That should answer your question.",317991.0
94504,398217.0,Yes. Right,329512.0
94499,398204.0,yesss,318017.0
94499,398205.0,"Yes, the one you have found in funding type analysis.",329512.0
94494,398193.0,"Flowchart means all the steps and approaches you did for the analysis, consider making a cup of tea, what all steps you will follow, that all can be framed in a flow chart. Also, the PPT should contain the plot explained properly and the business results and conclusion, No need to add code and it's explanation.",329512.0
94494,398191.0,the way you have funneled the data foe analysis can be used,318017.0
94494,398192.0,I think Flowchart - would contain the steps you did to reach the checkpoints in graphical form PPT - would contain the analysis done by you through your code. Rest TA can give more input,317991.0
94527,398298.0,the analysis should be the synopsis of the technique which you have followed,318017.0
94527,398385.0,"PPT should contain the results that you think you need to give to Spark Funds. Think like you are going to present this to Spark Funds the other day, what all will you include? It should be very business logical and should not contain any code. The flow chart is required for us and that is where you will include your workflow and approach.",329512.0
94505,398222.0,"Not clear with your question, but if there is an issue with the sector name then use pd.melt after you have merged.",329512.0
94482,398151.0,You can read TA comments for your question from below link: https://learn.upgrad.com/v/course/208/question/93582 Hope this will help.,317991.0
94482,398149.0,"No need to import PDF to python, you may do it manually, If you have already done it, you may go with it but please mention the requirements of packages to be installed at the starting of your notebook.",329512.0
94134,396556.0,"if you observe a specific Patten, use replace function to change the values. do not change anything in the file. all processing has to be done in Python only",318084.0
94134,396577.0,There was question earlier on which TA confired that this needs to be done. I don't remeber the link for that. But this needs to be handled in as part of data preparation and cleansing. (replace 0 with na),317689.0
94134,396563.0,"Hi Sahil, Yes , we need to clean (replace) the data in mapping dataframe first before merging it with master dataframe . You need to replace 0 with 'na' , please refer the below link: https://learn.upgrad.com/v/course/208/question/94026 Thanks.",305652.0
94134,396559.0,Hi premnath! It sound goods as i also noticed a certain pattern. So I request the TA to please reconfirm this for the ease of ereryone!,318427.0
94134,396562.0,There is no wrong spellings. It is data quality issue. You need to resolve this. You can check TA comments in below link https://learn.upgrad.com/v/course/208/question/91115 https://learn.upgrad.com/v/course/208/question/91084 Hope this will help,317991.0
94134,396741.0,"Do not change spelling in excel, rather change in dataframe after importing.",317811.0
94134,396786.0,Hi Ankit I have changed in the data frame now.. But i guess there are some case's that need to be handled while changing the spellings.. is it so?,318427.0
94134,396974.0,But if we replace 0 with na it may affect the names which really have 0 in it. for example Enterprise 2.0,318579.0
94506,398227.0,Thank you.. That was too quick....,303670.0
94506,398224.0,"Yes, allowed but keep this within 10 slides.",329512.0
94532,398335.0,"Hi , You need to use single encoding , please find the below link for more information. https://learn.upgrad.com/v/course/208/question/93500thanks.",305652.0
94532,399000.0,"there different ways of encoding like iso-8859-1, utf-8 sometimes you may have to use ""r"" at the beginning of the path.",317600.0
94532,398334.0,same encoding that you used during import of companies dataset,303673.0
94532,398349.0,rounds 2 is a csv and companies is a csv you have to upload it in different way but the encoding should be same i.e. utf-8 to ascii,318017.0
94498,398202.0,null data is not used for analysis so it should be removed,318017.0
94498,398201.0,Funding round code is not needed to be considered for the analysis.,329512.0
94488,398175.0,"Yes, you need to go through all the data cleaning process.",329512.0
94508,398225.0,"No this can't happen, please check your analysis.",329512.0
92781,390771.0,"as the pdf is a supplemental file, I suppose it is up to each individual/team how they use that info - whether they use a PDFreader to import the file, or the manually use the contents of the file somehow country codes are readily available online - so information can be gotten that way to bridge the gap between companies.txt and the pdf file when to apply filtering? well that is up to each individual/team's interpretation and understanding of the case study",300694.0
92781,391200.0,"In companies files, the Country code is provided, which is to be used. You can check the country code using this: https://countrycode.org/ You will use the country filter, once you start answering the questions asked in the investment.xlsx file, which means when you start your analysis.",329512.0
92932,391338.0,"Hi, On the right side of the country code there is also city column.. which you can google it to know the country name..",305129.0
92932,391381.0,"Agree to what Ranjay said.. but anyways as far as this case study is concerned we're supposed to capture only the top 9 results out of which again only top 3 to be mentioned in the table.. mostly country name is guessable from the code for this top 9 if you compare with the country names pdf.. that is the only shortcut i cam remember of.. or else yes we can right away get the help of google on country codes.. but either way said, it is not the right way.. dataset should have had country names in it..",316349.0
92932,391359.0,"That is correct, ideally they should have provided a dataset for ISO-3166. I downloaded the same from ...https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv",306250.0
92970,391544.0,I tried with 'r' but it is not working out,316399.0
92970,391528.0,Instead of the code you wrote rounds2 = pd.read_csv('C:\Users\Kja technical1\Desktop\rounds2.csv') Try forward slash like this rounds2 = pd.read_csv('C:/Users/Kja technical1/Desktop/rounds2.csv') Hope this will help.,317991.0
92970,391578.0,"You need to mention the path using forward slashes ""/"" instead of the back-slashes. Also, the file has been encoded using a different delimiter and encoding method. Hence, you will need to explicitly specify the seperator and encoding. Please check out the below link under pandas tutorial for more details. https://learn.upgrad.com/v/course/208/session/19864/segment/101067",313826.0
92970,391661.0,"the rounds2.csv file has been encoded with different encoding then utf -8 so you will have to specify the encoding while reading the CSV. pd.read_csv(""test2.csv"", encoding='cp932') Try calling read_csv with encoding='latin1', encoding='iso-8859-1' or encoding='cp1252' (these are some of the various encodings found) You can read more in details here https://jrogel.com/python-3-pandas-encoding-issues/",317845.0
92970,391757.0,"use sep=',', encoding='ISO-8859-1'",317811.0
92970,395309.0,I am using encoding='ISO-8859-1' but still there are a few permalink column data (bottom most of the table) which is not decoded. Have anyone faced similar issue?,314162.0
92970,391936.0,"Your first answer is this: rounds2 = pd.read_csv('C:/Users/Kja technical1/Desktop/rounds2.csv') Now for encoding issue, you need to google and find the correct encoding, As this is a part of the graded component, I will not be able to tell you the right answer, but the hint for this is just google: ""non-English characters in pandas encoding issue"" The solution is available in StackOverflow, once you google it.",329512.0
94528,398315.0,The task requires you to validate that a specific column is unique and then infer that it can be used as the unique key for each company.,318762.0
94528,398306.0,"Hi , You can verify all the columns in the data frame and check for any 'duplication'. Try to find column which has all unique entries with out duplication. thanks.",305652.0
94537,398373.0,use the filter tab in the tableau and set range 5 to 15 million,318017.0
94537,398367.0,Resolved. There's a filter option at data source section.,318329.0
93599,394331.0,"This means the encoding is not solved yet. Use this and check if you can see some non-English characters: rounds.loc[~rounds['company_permalink'].isin(companies['permalink']), :] This is a challenge and I will not be able to help you with this. Hint: Google 'Special characters in Pandas data frame', the answer is in Stack Overflow.",329512.0
94539,398543.0,"Yes, as suggested by others, divide by 1 million i.e. 1000000 and set float format like pd.options.display.float_format = '{:.2f}'.format",304814.0
94539,398390.0,You can convert it into million USD by dividing it by 1000000. For that you can follow same approach of movie data assignment. Hope this will help.,317991.0
94539,398399.0,"You can have 2 approaches: 1. Convert it into millions by diviting it by 1000000 2. Setting the float display format to {:,.2f} in a separate command earlier. This will result in displaying full numbers with 2 decimal places",318762.0
94539,398710.0,Make a new column and divide the raised amount column values by 1000000 so that you can have values in dollars and million dollars.,318448.0
94539,399401.0,you can also use map or apply function on lambda x: int(x),318458.0
94539,399042.0,convert it into million USD by dividing it by 1000000,306738.0
94515,398247.0,"I got your question and the step where you are facing it, there are two ways, either write e if else or use pd.melt. Apart from this hint, I can't help you more.",329512.0
92607,390303.0,True but that's what the assignments wants us to do. We need to focus more on achieving the results than the strategy itself.,310974.0
92607,390495.0,"Check for both mean and median, since it's mentioned that we have to compute a representative value of the funding amount for each type of investment. We can either choose the mean or the median.",329512.0
92607,391341.0,Yes true out of the 4 investment type only one is there which is with range of 5-15 million usd..,305129.0
92607,392092.0,mean of the investment type and absolute value of the investment are quite different.,318585.0
92607,392091.0,I don't think this assumation is required at all.,318585.0
93617,394890.0,"For this checkpoint, master frame should be used.",301643.0
93617,394496.0,"Before checkpoint 2, we have been asked to merge the dataframes and come up with a master_frame and it has been instructed ""After this, you will need to work only with the master frame ."" . So, you need to work with the master_frame only.",313826.0
93617,394483.0,Using master frame you have to calculate average investment of funding types.,310419.0
93617,394551.0,As per instructions use the master frame.,318476.0
93617,394660.0,"Using the master frame created by - ""Merge the two data frames so that all variables (columns) in the companies frame are added to the rounds2 data frame. Name the merged frame master_frame""",315277.0
93617,396922.0,Using master data frame using specific funding type,311227.0
93208,392435.0,"This is a simple plot, Try a box plot for venture, seed, private equity and angel and this will do the job.",329512.0
93208,392471.0,Pie chart?,300694.0
93208,395605.0,"So, what is the final call guys? I'm superbly confused which PLOT will show the fraction of investment in three venture types and also the average amt of investment across all or these three??",316349.0
93208,396377.0,"So, the total here really means total of those 4 types or of all the types?",300697.0
94514,398243.0,"Yes, they are within the range.",329512.0
94563,398555.0,I am not sure it is possible or not. but why you would want that. if group by is on a single column then your first sum() will give what you want I understand.,311686.0
94563,398551.0,"if you are doing sum() using group by, then it would give you the sum of all grouped by entities. Something like this A 10 B 20 C 30 If you want to take the sum on this grouped by data, then at the first place you dont need to group by. You can simply use sum() and result would be like 60",304814.0
94563,398827.0,"After doing the aggration on group by, use pd.DataFrame( df.groupby('columnname').sum() ).sort_values(columnname) pd.DataFrame will give a name to the aggregate column, that you can use further.",317811.0
94563,399213.0,"Yes, there is a way. Create a dataframe out of the results from the first group by results and then again use group by on the new dataframe created. Simple.",329512.0
94563,399400.0,Try above and let me know if it works for you.,318458.0
94563,399399.0,"grp_by= df.groupby(['column1']) grp_by['numeric_column'].sum() # this will give you first level of sum of numeric_column for grouping on column_1 Now you further want to do sum on that sum. It just means you are summing up entire numeric_column, This can be achieved as sum(grp_by['numeric_column'].sum()) or more simple way, just use - sum(df['numeric_column'])",318458.0
94583,398646.0,Probably because it was part of the preparatory course.,310511.0
94583,398687.0,"May be to cross verify the results by visualization. Plotting in tableau was easy for me compared to the time involed in python coding, which helped in validation.",318329.0
94583,398810.0,"By Plotting graphs using Tableau, you will be able to verfiy your results that you got via python code. Also, graphs are easy to interpret the observations.",317811.0
94583,399062.0,Knowledge which is needed for plotting in tableau is already been given in preparatory session . These plots will be used in ppt.,306738.0
94583,399222.0,"This is optional and not mandatory, Please complete the task in Python.",329512.0
94574,398638.0,"The powerpoint is supposed to be for CEO of the spark Fundings. Abstract should explain purpose/objectives, method, assumptions &amp; conclusion.",318458.0
94574,398571.0,"Yes. You can write about the case in general. You can mention what is the context, background and what is the objective.",311686.0
94574,398654.0,Abstract is summarization of the case study -- the brief and the intention. However it is mentioned that the attached ppt is only a sample format and theres no hard and fast rule for us to follow it. You can create your own format. Only restriction is it needs to be within 10 slides.,310511.0
94586,398636.0,"No. Any one member from the group (not necessarily the group facilitator) has to sub,it on behalf of the group as confirmed by the SMs.",310511.0
94586,398806.0,Only One Member (Group Facilitator) needs to submite the Group Cae Study Assignment on behalf od entire group.,317811.0
94586,399583.0,only one member from the group has to submit,318019.0
93137,392100.0,"That's right. Of the fields available, only 'funding_round_type' has details of various funding like private_equity, seed, angel, venture etc.",313826.0
94576,398817.0,"Plot only for Venture, PrivateEquity and Seed.",317811.0
94576,398652.0,"The data has to be shown for only funding types -- private equity, seed and venture. The data source can be filtered in tableau to select the required dataset for the view.",310511.0
94587,398643.0,Are you getting error from the statement? Try this. d1 = master_venture.loc[(master_venture.raised_amount_usd &gt;= 5) &amp; (master_venture.raised_amount_usd &lt;= 15)],310511.0
94587,398685.0,"Remove the extra braces and try. Other than that, seems fine to me.",318329.0
94587,398708.0,"Hi Vasanth, Are you facing any error or some logical issue? One thing I'm able to notice over here is that you've written &gt;=5 and &lt;=15. Did you convert the raised_amount_usd to millions i.e divided by 1000000. Moreover it has to be &lt; and &gt; and not &lt;= or &gt;= Hope this helps!",318355.0
94587,399223.0,df[(df['raised_amount_usd'] &gt;= 5000000) &amp; (df['raised_amount_usd'] &lt;= 15000000)],329512.0
94589,398641.0,No. Total count for each main sector has to be populated in each row for all the corresponding main sector values.,310511.0
94589,398683.0,"For first and second points, it has to be across the sectors.",318329.0
94589,398823.0,"No, you need to calculate the total count of investments in the country across all sectors.",317811.0
94589,399270.0,"No, you need have total count of each main_Sector for all the 3 countries",300685.0
94593,398661.0,Tableau has the power to read data from various data sources. Please go through the following topic from prep course on tableau to understand this: https://learn.upgrad.com/v/course/208/session/15813/segment/79964 The data you want to analize and visualize in Tableau has to be present in one of the sources recognized by tableau and then has to be imported in tableau.,310511.0
94593,398678.0,extract the data that u have merged using mapping and master frame and the questions cab be done there easily as its just sum and avg,318017.0
94593,398680.0,You can save the master_frame you got obtained after merging companies and rounds2 and carry out your analysis on this dataset.,318329.0
94593,398699.0,"You can achieve this in two ways: 1. Do all the cleaning of data and merge the master frame with mappings.csv. Once you have final data set export that in csv and use that as source in tableu. 2. Read all the files using tableu and do all the merge, cleaning using tableu. Tableu can read data from txt file, csv, database and other different sources. I would say if you are expert in tableu go with 2nd option. Hope this helps.",317845.0
94593,399058.0,"you can do it in python and export it as csv file by using df.to_csv(path,sep='\t') than use it in tableau",306738.0
94612,399267.0,"make sure, most of the data analysis is done through master_frame and only few parts in D1, D2 and D3. You can export the dataframe to csv by df.to_csv(""output.csv"")",300685.0
94612,398844.0,That will not work. Please use the master data frame file. you can create all the plot from this file only. you need not import any other file.,300735.0
94612,398867.0,"You can export the dataframe to a csv file. The general syntax is df.to_csv(""my_df.csv"") This will use the df dataframe and export it to "" my_df.csv "" file and save it in current working directory. You con export as many csvs as you require for generating your plots.",313826.0
94612,398984.0,"Yes, you can export 3 different csv's to tableau to plot 3 different plots.",317600.0
94612,399370.0,"master_frame is your final data. Export it to csv using to_csv function, Then use that csv as DB in tableau and then create charts. Hope this explains!",318458.0
94612,399635.0,"There are two ways of doing this tableau plots: 1. By exporting the final mater_data frame to the csv file, and that csv file can be loaded into the tableau and start making the plots as many people here talking about. 2. The plots can be made with the importing of all three files, categories.txt, rounds2.csv and ampping.csv separately into the tableau and do cleaning and joining dataframe operation right here as done in python script.",318585.0
94615,399261.0,"you can do as df['NewCol'] = df.groupby(['col']).count() and same for amount, use sum instead of count",300685.0
94615,398879.0,"You could use the agg() function to get the count and sum in one go as shown in example below df.groupby(['col']).amount.agg(['count', 'sum'])",313826.0
94604,399268.0,United kingdom and GBR are different.,300685.0
94604,398744.0,"Technically no. But, with respect to the case study, i believe yes they are same. hope that helps.",317998.0
94604,398768.0,"You can follow below link to get difference between GBR and United Kingdom. http://mentalfloss.com/article/85686/whats-difference-between-great-britain-and-uk And you can visit below link for country code to country mapping https://countrycode.org/ So, we can conclude that technically both are not same, but for over case study it is same. Hope this clarifies your doubt.",317991.0
94604,398805.0,"Yes, GBR is ISO-3 Country Code for United Kingdom",317811.0
94604,398957.0,Yes it is,319302.0
94604,399057.0,yes as united kigdom counttries is not given in data separately . you can consider them same,306738.0
94604,399198.0,"No please mention it as Great Britain, there are some territorial differences between United Kingdom, England and Great Britain",300735.0
94604,399567.0,No technically GBR and UK are different but that does not matter for the assignment in hand,318019.0
94597,398692.0,You need not consider it. There's no harm if you include though.,318329.0
94597,398992.0,"You need to consider only venture, seed, and private equity according to the checkpoint.",317600.0
94607,398820.0,"In CheckPoint 5, you need to first filter data for three countries, then by venture funding, then by records having raised amount between 5million and 15 million. Then you are good to find the company with higest investment in the top sector in the particular country.",317811.0
94624,398981.0,There are couple of ways you can try 1. Check if your file is corrupted 2. Change your .xls to .xlsx Check this link would be helpful https://kb.tableau.com/articles/issue/error-occurred-while-loading-the-data-source-connecting-to-xls-files,317600.0
94625,399259.0,Its inclusive of oth $5M and $15M,300685.0
94625,398925.0,The range is 5 to 15 Million USD inclusive. Please refer similar query verified by TA. https://learn.upgrad.com/v/course/208/discussions#26021,313826.0
94625,399193.0,It will include $5M &amp; $15M,314048.0
94609,398847.0,"yes, use count only here. do not use distinct. The same amount of funding can be done by multiple investors",300735.0
94609,398819.0,"Yes, each row should display its corresponding main sector's total count of investment in "" count of investments "" column.",317811.0
94609,398959.0,Yes it will repeat,315679.0
94609,399207.0,"yes, it will repeat and its perfectly fine.",300735.0
94626,398973.0,You can try using dual axis,317600.0
94626,399050.0,The default chart you get with one dimension and one measure is bar chart and that does the job.,318329.0
94626,399191.0,As per Checkpoint 6: Plots ... 1. Use Tableau 2. Import Master file &amp; ... 3. You will be able to see the plotting is done against the funding type &amp; ..... You will be able to answer it by your self.. Sorry.. if I cannot answer it completely ( you know why) My apologies. But if you run into questions let us know. Thanks.,314048.0
94626,399316.0,You may use Tableau or Python for the plots.,329512.0
94550,398487.0,You can convert it into million USD by dividing it by 1000000. For that you can follow same approach of movie data assignment. Hope this will help.,317991.0
94550,398454.0,column 'raised_amount_usd' can be converted to millions and round it to few decimal places,301115.0
94550,398554.0,'raised_amount_usd' column needs to be converted to millions and rounded to few decimal places,311254.0
94550,398542.0,Use this pd.options.display.float_format = '{:.2f}'.format,304814.0
94628,398943.0,got it,310504.0
94628,398944.0,I would suggest you to keep only the Top3 sectors as this has been asked specifically in the question.,313826.0
94628,399257.0,Its better to mention only top 3 sectors as its a presentation to the Sparks funds and too much values will make it cluttered in presentation,300685.0
94634,399044.0,"Not required, just the plots in the PPT file. Along with the Python notebook and the to-be-filled Excel file.",312731.0
94634,399010.0,"No. Tableau files are not to be submitted. Only a zip file containing the following documents needs to be submitted: the main Python file, one presentation doc (in PDF format) and one Excel file.",313826.0
94634,399053.0,Just take screenshots of plots and paste in presentation,306738.0
94634,399181.0,"Completed Python file, Power Point presentation in PDF format and Excel sheet with values filled are supposed to zipped and uploaded (per submission guidelines). No need to submit Tableau file.",314048.0
94634,399242.0,"Not in the ask, so not needed",313228.0
94634,399274.0,"Please view the following Submission page, which says The zip file should contain the main Python file, one presentation doc (in PDF format) and one Excel file. https://learn.upgrad.com/v/course/208/session/19904/segment/101231 So, it's not required. Thanks!",318080.0
94634,399365.0,"Tableau files are not required. Only ipnb, xlsx, &amp; pdf files in zipped together shouold be uploaded. Tableau submission is optional as per TA. It means it will be not be considered for evaluation.",318458.0
94634,399719.0,"Tableau plots can be exported to image ,,and they can be pasted in ppt ppt can be converted to pdf then",318005.0
94581,398617.0,"Hi , Might have missed to add that one , but its better to add four funds ( venture, angel , seed, private equity) which are mentioned in checkpoint -2. TA's can confirm further.",305652.0
94581,398649.0,Angel isnt required. We should go by the question description. Ultimate aim is to potray how one venture type is better than the others in question.,310511.0
94581,398814.0,"Yes, you can.",317811.0
93366,392902.0,"1. Yes data cleaning is required before answering any question. 2. No, For checkpoint 2 consider full data. 3. Your approach is correct.",329512.0
93366,393011.0,I think we should follow the steps and output mention in the check points sequence.If the check data need data clean we should do it for that check points.,318476.0
94643,399085.0,"It is clearly mentioned, Only ONE HAS TO upload.",317998.0
94643,399145.0,One one person preferably GROUP FACILITATOR needs to submit the assignment on behalf of entire group,317811.0
94643,399189.0,"Any one member can submit the file. As soon as its submitted, the status would change to submission complete for all 4 members.",309451.0
94643,399195.0,one person can upload no worries :),300735.0
94643,399241.0,if one uploads others can see it as well. so no worries. But make sure others in the team download and verify the integrity of the file.,313228.0
94643,399301.0,It is fine if only one person from a group uploads the file on behalf of the group.,314818.0
94643,399360.0,"Just one person, preferabally group facilitator will upload file. Other people can verify files, once he uploads. Any one can click submit button, Else it will be automatically submitted at 11:59:01 pm.",318458.0
93399,393207.0,The is no specific way to implement this .But one easy way will we having a dataframe with the main-sector and total_count and total_amount value and merge this dataframe with the original dataframe on the main_sector colum,318476.0
93399,393139.0,yes. it doesn't make sense to do so. but as per the instructions we need to do that only I think. you will get same values in all the rows with same sector. requesting someone to comment or verify.,311686.0
93399,393593.0,"You can ignore adding these columns, rather create D1, D2, D3 with sector, count of investment and sum of the invested amount. You will then be able to answer all the questions. The evaluation is not done for D1, D2, D3. So make sure you answer all the question in the investment.xlsx file and have code in the notebook. No matter how to do it.",329512.0
93399,395443.0,"Can this added as a pinned discussion . I think everyone would have this confusion, as it does not make sense to add the columns in D1,D2 and D3 data frames.",306725.0
93399,396381.0,the total number and total amount cant be added in same D1/D2/D3 dataframes as it will have row count mismatch. Instead use D1/D2/D3 to solve total number and amount queries,316323.0
93399,396676.0,If we add count based on sector and total fund and then display all the records then these two agrreage column will repeat.,315679.0
93571,394134.0,"This is just starting and to built a strong understanding everyone is doing all tasks, but when you all will verify your answers as well as as the way other approached in writing queries you will learn better way of solving problems, which is the purpose of group case study.",317811.0
93571,394149.0,only group leader can submit the case study . for Rest team member leader should delegate task or whatever way everybody can contribute to get case study done,319869.0
93571,394194.0,"Hi Alok, You need to discuss this with your team, If you are a team lead, you need to lead the team and take care of this, It's good that everyone in the team is trying to solve the problem in their way, but you all need to discuss the issues and problems you are facing. You may set up daily targets and let every one complete it, then daily evening you may set up a hangout session to discuss the target and the best solution to it. For this only we kept it as a Group case study. Learning increases in group and we want the same from you guys.",329512.0
93571,397096.0,"In my honest opinion, it isn't too lengthy to be a group study. Rather it should've been individual assignment so that everyone could get a better hands on. Plus, it's really a great learning if you do everything.",318381.0
93387,393052.0,"completely agreed,TA please confirm one more time please",301115.0
93387,393210.0,"I don't think any where in the check point 2 its mention to clear the non english speaking.Check point 2 is focus on idenfity the fund type. Check point 3,4,5 needs english speaking country only along with the Fund type dervied from the check point 2.",318476.0
93387,393580.0,"You need to filter the data as you go along the checkpoints, when you arrive at checkpoint-3, you need to consider the english speaking countries. For checkpoint-2, you need to choose the correct funding type using complete data and then on the results you apply the 5-15M cutoff to choose the appropriate funding type, once this is done, you filter the data only for that funding type and then use that data to choose the top3 English speaking countries.",329512.0
93471,393504.0,It is 50 day ma not 30.,318340.0
93471,393518.0,"No, they are not equal.. please check the formula if you're copy pasting it.. and also tey to run it again.. i too faced the same problem but then I realized that the formula was copy pasted but i forgot to make one small change. Also make sure that you're calculating 50 day MA amd not 30 day MA.. could be a typo error .. :)",316349.0
93582,394503.0,"I think in this case, coming to a decision based on manual inspection is sufficient.",313826.0
93582,394208.0,i guess you can go ahead with Country code qnd then verify your top top3 countries if included in the english speaking list of or not.. and yes you can guess the country name easily through country codes or you can take help of google in knowing what country it corresponds to..,316349.0
93582,394207.0,"Yes, you may use any source to complete this task. Also, it is not required to import PDF to python and match the countries, you may do this manually.",329512.0
93582,394216.0,Yes getting from Google is known. But are we allowed to use external data source for assignment is my question. I seriously don't think u can generate country names with country codes or whatever data given...,308437.0
93582,394555.0,Creating a static array for the English speaking country can also help so that you can fillter cloumns based in the value present in the static array,318476.0
93582,394744.0,"You can create a text file with the countries data, import it and then use that for your analysis",301641.0
93580,394204.0,"Yes, A very detailed Data cleaning is required. Data cleaning is a default step which you need to do even if not asked. Think for what to do, if the rows or column nulls are needed to be dropped or needed to be imputed. Apply your learnings from the course.",329512.0
93604,394385.0,"Yes, you need to convert them to either upper or lower case and then merge it.",329512.0
93621,394495.0,"In checkpoint 5, they have asked to create 3 new dataframes corresponding to top3 english speaking countries and the funding type most suitable for Spark Funds. So, I think we should keep the master_frame as is with all the funding types.",313826.0
93621,394557.0,"Since this is a very detailed analysis case, I found that dataframes were references at different points for different analysis. A good option in such a scenario is to create copies to leave the master frame untouched.",306250.0
93621,395030.0,"Don't replace it just filter it, You can also create a copy of it then can remove all other investment types.",329512.0
98590,419595.0,Convert them to float format using pd.options.display.float_format,303673.0
98590,419602.0,"Hi Naren, This is happening as the amount is very high for normal integer sum. For better readability and ease of operations, you can convert the amount to ""million USD"", ie divide the sum by 1,000,000. This will present the figures in smaller numbers which will help you. Hope this approach helps. Regards, Soumik",305839.0
98590,419714.0,you can change the format of the number like : df['Traded Value'] = df['Traded Value'].apply(lambda x: '{:.2f}'.format(x)),318017.0
98590,419778.0,"Use this to set the formatting for floats. pd.set_option('display.float_format', '{:.2f}'.format)",317689.0
98590,419945.0,"change the dtype , how to change check the below link https://stackoverflow.com/questions/15891038/change-data-type-of-columns-in-pandas",305838.0
97285,416339.0,you can check for the na as 0 should be replaced by na do take care of lower and upper case in na,318017.0
97285,416341.0,Please refer the below link which discusses a similar query https://learn.upgrad.com/v/course/208/question/96456,313826.0
98829,420810.0,"Naren, here mostly the questions are interlinked and hence you might want to refer to the previous answers too.. You might have choosen some ""Investment Type"" whcih was most suitable for 'Spark Funds' based on the conditions specified i.e. 5-15lacksdollar something ( really don't remember the range).. Now, you're drilling down your results more to see for the choosen Investment type what are the top 9 countries.. Once choosen the suitable Investment Type, you can filter the dataframe to contain only the selected Investment type using; df = df.loc[df['ColumnName']=='ChoosenInvestmentType'] I hope this helps you in stepping further.. :)",316349.0
98829,420746.0,"Before calculating top9 countries you would have found a investment type in ""Checkpoint 2: Funding Type Analysis"" . In that you are supposed to do Calculate the average investment amount for each of the four funding types (venture, angel, seed, and private equity) and report the answers in Table 2.1 Based on the average investment amount calculated above, which investment type do you think is the most suitable for Spark Funds? And for the investment type called as ""Chosen investment type"" selected in the step 2 above you have to perform Country analysis and find top9 countries. Hope this clarifies.",317991.0
99443,425947.0,You have not mentioned the error that you are getting. Do put a screenshot or the error message that you are getting so that we could try to address the issue.,313826.0
99443,425287.0,Read about .loc and inplace in python,305839.0
98599,419680.0,"Try rewriting it by converting it into a proper DataFrame - by_cntry_type = master_frame.groupby(['funding_round_type','country_code']) master_df2 = pd.DataFrame( by_cntry_type.raised_amount_usd.sum().astype('int64') )",318762.0
98599,419713.0,"you can use code like : data.groupby(""Country"").agg( {""column1"": {""foo"": sum()}, ""column2"": {""mean"": np.mean, ""std"": np.std}})",318017.0
98599,419777.0,"You can use renaming the columns. df.rename(columns={ 'mean': 'Mean'}, inplace=True)",317689.0
98599,419943.0,hope the link will help https://stackoverflow.com/questions/19078325/naming-returned-columns-in-pandas-aggregate-function,305838.0
106770,461884.0,"not working for me, getting the same error..",300735.0
106770,461260.0,Drop one of the high vif variables and see the output p value and the vif for the rest of the variables and continue the process until they are in control,311160.0
106770,462314.0,The VIf values continue to be high even after obtaining factos that have all 0 p-values. (Much higher than the threshold values of &lt;5). any insight in to this?,317575.0
106234,458643.0,Use pd.get_dummies as explained in the course content.,310974.0
106234,458837.0,You can further categorise categorical variable to reduce categories.,318344.0
116351,503121.0,I suggest you access the TA approved link given below where ti is discussed in detail with examples. https://learn.upgrad.com/v/course/208/question/116053,301121.0
141342,610487.0,Please filter out your dataset based on condition for N. Else the dimension of matrix would increase and it will give memory error.,317689.0
141342,611195.0,It would be useful if you run Anaconda Navigator as an admin and then launch Jupyter Notebook.,306726.0
141342,610679.0,"hi, it happened with me as well twice. restarting the kernel helped.",311686.0
141069,,nan,
141368,610676.0,"hi, we need to visualise the number of reviews different beers have got and then select a number N. After selecting N, we will use keep records of only those beers that have got atleast N reviews. You can decide your own N. there is no right or wrong value.",311686.0
141368,612110.0,"there isn't any such finding - you just need to choose, in an educated way, a minimum N cut-off - there is no right or wrong answer per se - it should be justified, your choice of why you went with a particular N",300694.0
141368,612349.0,"As per problem statement, ""this may not have one correct answer, but you should not choose beers that have an extremely low number of ratings."". My take on this is , N can be decided based on two aspects: 1) Total number of ratings that every beer has received (which should not be less significant) 2) Total number of ratings provided by users should not be less significant (those who provided single review)",309211.0
141144,609599.0,"as above comment, remova those duplicate records and proceed",312019.0
141144,610210.0,Error comes because of duplicate records in the attributes that you are passing as 'index' remova those duplicate records and proceed.,310952.0
141144,609556.0,"Hi, This error comes when there are duplicate records in the attributes that you are passing as 'index' and 'columns' in df.pivot. Suppose you have 3 columns in dataset. Column A, B and C. If in df.pivot you are mentioning index=A and columns=B then there should be no two records in your dataset with same combination of values in attributes A and B. Hope this helps.",311686.0
141144,611395.0,remove duplicates and keep last or first before using pivot,318005.0
141144,612352.0,Please use Pandas Pivot table function https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html . Pivot table handles duplicates efficiently.,309211.0
141475,611389.0,review profile name is an alphanumeric unique user id,318005.0
141475,611826.0,"No issue. Please don't remove them. It can happen if the person decided not to provide his/her name, the corresponding date or DOB can be used. Please ignore this and continue.",428646.0
141475,612107.0,that is because you are looking at it in excel which will try and conver to date if you look in notepad++ then it is a bit more meaningful but as has been said .. it is just a handle/name - it doesnt matter what it is as long as it is not null I suppose,300694.0
141475,611150.0,I guess it won't matter much just a name!,311466.0
141475,612325.0,You only need to remove the users which dont have any username. Apart from that anything can be taken as username.,317689.0
141302,610491.0,Logic for taking out N?,315757.0
141302,610418.0,"Yes this is right ""Does above values are equal to "" number of ratings given to each beer "" and "" number of ratings given by each user "" respectively !!"". But yeah, the terminology of average is confusing.",310974.0
141302,612106.0,I don't think they are the same! Average number of ratings given to beer would be the average number of reviews per beer (perhaps by counting all the reviews and dividing it by the number of beers - I did mine a bit differently by taking the average of average)),300694.0
141434,611857.0,Please refer to the verified answer: https://learn.upgrad.com/v/course/208/question/141538,428646.0
141434,611078.0,Correlation Matrix and Heat Map,310974.0
141434,610873.0,For Ques 3 and 4 : The average number of ratings given to the beers - group the resultant data of beer items and along with it take a mean of beers reviews. The average number of ratings given by the users - group the resultant data of reviewers names and along with it take a mean of users reviews. Hope this helps!,311466.0
141434,610970.0,Determine how similar the first 10 users are to each other and visualise - Find the correlation between first 10 users. You can use user_correlation matrix. Compute and visualise the similarity between the first 10 beers. - Find the correlation between first 10 beers. You can use item correlation matrix.,310467.0
141436,610861.0,yes,311466.0
141436,610998.0,"This is because the user has rated very less movies (1 or 2) . Hence we can remove such users like we removed the beers which have less than ""N"" reviews. Even if we include those users, model that we built will not be able to make any useful predictions for such users. It would be better if TA can confirm this.",310467.0
141172,610040.0,you can use pivot_table(). It can handle duplicates in data.,311004.0
141172,609803.0,Try removing item and user common duplicates and keep the 1st entry of it ... Then it must run.,311466.0
141172,609771.0,Use pivot_table not pivot,314092.0
141172,610891.0,pivot_table() should be used instead of pivot for this,306726.0
141172,609744.0,,319759.0
141172,609780.0,try after removal of beer_beerid and review_profilename. use option drop_duplicates(subset=,312019.0
140445,610982.0,"Just like we learnt in EDA visualization is to give an idea of relation between attributes, to help find patterns if any etc.",311857.0
140445,606936.0,"Yes, we require different visualization (Histogram, Box plot, Bar plot, etc) to show the following : 1. The distribution of ratings or reviews 2. The frequency of ratings or reviews 3. Find mean and median This is a rough idea of what we can do and further is upto what you can do more according to the problem statement...",311466.0
140507,609951.0,It is mentioned in the assignment that you should not choose beers that have an extremely low number of ratings.,311004.0
140507,608418.0,There are 50% of the dataset having no of ratings less than 2. Should we consider only half of the dataset?,315028.0
140507,607160.0,It is mentioned in the assignment that you should not choose beers that have an extremely low number of ratings. Hence we are doing the Data Preparation to find the N ratings. So the data exploration should be done on the dataset obtained during data preparation.,311254.0
140507,607151.0,Only data that we got from Data Preparation (only those beers that have atleast N number of reviews).,311686.0
140507,608914.0,"As per problem statement, there is no one fixed answer. "" Figure out an appropriate value of N using EDA ; this may not have one correct answer, but you should not choose beers that have an extremely low number of ratings.""",307495.0
141454,,nan,
141463,611149.0,Check how you have created both the dataframes. Column count for dummy test and dummy train should be same. Only Row count would differ.,317689.0
141463,611250.0,I am confused with the above two contradictory answers. TA please help.,303228.0
141463,611854.0,Refer to this: https://learn.upgrad.com/v/course/208/question/141525,428646.0
141463,612104.0,why would it be the same? they are derived from different dataframes,300694.0
141463,612159.0,"results on pivot on test and train will be different ,it depends on unique no of users and beers in both of these",318005.0
141463,611068.0,dummy_train and dummy_test count should not be same. Dummy_train is derived from train dataset. Dummy_test from test dataset. Dummy_train count should match with train data. Dummy_test with test data.,310467.0
141463,611267.0,"dummy_train is copied and derived from train data set that is 70% of the total data and dummy_test is copied and derived from test data set which is 30% of the total data it can vary person to person as how you select the test_size and train _size while splitting data. Therefore, it won't be the same",311466.0
140376,606961.0,"Yes what you have specified we need to find that only According to me, we need to find this all : check the distribution for avg rating for beer, avg rating for beer according to the users, avg no of the rating given to beer &amp; given to beer according to the user.",311466.0
140376,608670.0,As per my understanding : The average beer ratings - groupby beerid with mean of review_overall. The average user ratings - groupby review_profilename with mean of review_overall The average number of ratings given to the beers - count of beer_beerid and take a mean of it The average number of ratings given by the users : count of review_profilename and mean of it,319759.0
140376,610086.0,below is my understanding: Average beer rating = mean of rating group by beer. Average user rating = mean of rating group by user. The average number of ratings given to the beers - mean of (count of ratings grouped by beer) The average number of ratings given by the users : mean of (count of ratings grouped by users),316036.0
140658,608730.0,"No, you do not have to convert it to dummy, You need to create ""real rating matrix"" from the data.",329936.0
140711,610037.0,"Hi,\ You can use pivot_table(). It can handle duplicates in data.",311004.0
140711,608140.0,Check for duplicates in the dataset. Remove that and try the code again,311254.0
140711,608327.0,Index in pivot means that index and column combination should be unique. Get rid of those duplicates. and then use pivot. It will work. Otherwise you can use pivot_table(). It can handle duplicates.,311254.0
140711,609129.0,"For anyone who is still interested in the difference between pivot and pivot_table , there are mainly two differences: pivot_table is a generalization of pivot that can handle duplicate values for one pivoted index/column pair. Specifically, you can give pivot_table a list of aggregation functions using keyword argument aggfunc . The default aggfunc of pivot_table is numpy.mean pivot_table also supports using multiple columns for the index and column of the pivoted table. A hierarchical index will be automatically generated for you. Source : https://stackoverflow.com/questions/30960338/pandas-difference-between-pivot-and-pivot-table-why-is-only-pivot-table-workin",311466.0
140824,609378.0,Thankyou got it,312019.0
140824,608402.0,Index in pivot means that index and column combination should be unique. Get rid of those duplicates. and then use pivot. It will work. Otherwise you can use pivot_table(). It can handle duplicates.,311254.0
140824,609118.0,"Hi , She is trying to say check for the duplicates where both have common users and items. for e.g. : same user give multiple movie ratings to the same movie.",311466.0
141335,610411.0,"Why do you have to convert review_profilename to integer? Whatever was done in the sample recommendation system example given in the course content is what is needed. Just follow that. Don't get confused with the terminology, whatever was shown in the content is what is needed to be done.",310974.0
140937,,nan,
140946,609090.0,Average beer rating = mean of rating group by beer. Average user rating = mean of rating group by user.,329936.0
140946,610128.0,TAs please clarify the on following. The average beer ratings The average user ratings The average number of ratings given to the beers The average number of ratings given by the users. Based on above explanation given by Abhigyan point 1 and 3 are same similarly point 2 and 4.,317689.0
140946,610561.0,"avg beer rating depend on value of ratings avg number of ratings depends on no of reviews (or ratings) with higher rating value as 5 as same as lower value 2 ,, if there are 4 users and if beerid 1 gets review 3 times as 5,5,4 ,then avg rating for beer1 is (5+5+4)/3 if there are 4 users and if beerid 2 gets review 4 times as 1,2,4,3 ,then avg rating for beer2 is (1+2+4+3)/4 for final avg rating we can calculate avg of all avg ratings of beer #The question is really ambigious as it asked The average beer ratings and plural in ratings is saying that we have to find avg for every beer , but it can be assumed also that we have to find avg of averages and it is better to calculate both value but for no of ratings bear1 has 3 ,bear 2 has 4 and if only 2 bears then avg no of ratings = (3+4)/2 i have understood only this",318005.0
140959,609265.0,Even i am facing the same issue. TA please help,311254.0
140959,611158.0,"This issue is resolved if you run Anaconda Navigator as an administrator and then, launch Jupyter Notebook.",306726.0
140959,610349.0,Thanks Rahul.,320689.0
140959,609369.0,The matrix that is created will be of large size and calculation of pairwise distances and even matrix operations are going to consume memory. You can bring down the no of data points. I choose reviews only if the total count is greater than 10. I worked with 8GB RAM.,315028.0
140959,609130.0,"For anyone who is still interested in the difference between pivot and pivot_table , there are mainly two differences: pivot_table is a generalization of pivot that can handle duplicate values for one pivoted index/column pair. Specifically, you can give pivot_table a list of aggregation functions using keyword argument aggfunc . The default aggfunc of pivot_table is numpy.mean pivot_table also supports using multiple columns for the index and column of the pivoted table. A hierarchical index will be automatically generated for you. Source : https://stackoverflow.com/questions/30960338/pandas-difference-between-pivot-and-pivot-table-why-is-only-pivot-table-workin",311466.0
141257,,nan,
141272,610420.0,It is what was shown in the example in the course content. Just go with the same approach.,310974.0
141322,610413.0,Yes you should handle missing values else they will come in the way of building the correlation matrix later.,310974.0
141322,611022.0,I think we can create anew category for the missing reviewer name or drop those if very small,318554.0
141322,612108.0,some thing has to be done for missing values,300694.0
141312,610416.0,"Strange, I have only 2.9L records remaining after this ""Choose only those beers that have at least N number of reviews"" and deduplication. However, it shouldn't take that much time. Check if you are using the original one by any chance.",310974.0
141491,611196.0,I guess we should fill nan values with 0 TA please conform it.,311466.0
141491,611258.0,I guess we should fill nan values with 0 Try either X.fillna(0) or np.nan_to_num(X) TA please conform it.,311466.0
141491,612157.0,results are different in rmse with fillna(0) try without using fillna(0) on kaggle,318005.0
141491,611843.0,Upgrade to latest SK-learn and try again,317822.0
141491,611253.0,Try either X.fillna(0) or np.nan_to_num(X),311466.0
141811,613033.0,They are totally different questions -&gt; check the forum - it has been asked many times already,300694.0
141811,612567.0,You can go through the some discussions around your qestions. https://learn.upgrad.com/v/course/208/question/140376 https://learn.upgrad.com/v/course/208/question/140946 https://learn.upgrad.com/v/course/208/question/141263,307495.0
141505,611841.0,Upgrade to latest sk-learn and try again,317822.0
141505,613112.0,run conda update scikit-learn in anaconda prompt or run code in kaggle,318005.0
141505,611256.0,I guess we should fill nan values with 0 Try either X.fillna(0) or np.nan_to_num(X) TA please conform it.,311466.0
141505,613097.0,X= X.fillna(0) worked for me,300725.0
141505,612163.0,upgrade scikit-learn by conda update scikit-learn or use kaggle,318005.0
141505,612164.0,latest version of scikit-learn is 0.21,318005.0
141505,612165.0,check ur version by - conda list,318005.0
141505,612718.0,"You can use X= X.fillna(0) or np.nan_to_num(X) Hope, it will help you.",320689.0
141518,611387.0,"avg bear ratings mean avg of ratings of beers,, avg no of ratings means avg of review counts of beers",318005.0
141518,611314.0,Average beer rating is mean of ratings of beer Average number of ratings given to the beers is to count the number of ratings of beer and average it.,320074.0
141525,611394.0,"no of columns shud be same,,no of rows can ne different",318005.0
141525,611554.0,I am also getting different number of columns for Dummy_Train (19826) and Dummy_Test (14261).,310522.0
141525,611560.0,Dummy_train and Dummy_test need not have same number of columns. Please refer the following link where I have explained this using an example. https://learn.upgrad.com/v/course/208/question/141449 Please note that TA has not verified this.,310467.0
141525,611853.0,"Remember when you split the data, split it before the transformation, In that case, the number of columns would be the same.",428646.0
141572,611594.0,Use Google Colab,318329.0
141572,613093.0,i restarted the system to free up the memory and then tried again.,300725.0
141572,611670.0,restarting the kernel might also help. it worked for me.,311686.0
141572,612771.0,I am facing the same memory issue.. Any ideas to remove it ?,308644.0
141574,611862.0,Please refer to the verified answer: https://learn.upgrad.com/v/course/208/question/141538,428646.0
141574,611662.0,yes. the first 10 rows of your similarity matrix. so basically the train data only.,311686.0
141941,613046.0,You can use .loc or .iloc and then sort the values and take top 5 head.,311466.0
141941,613031.0,for those particular users mentioned in the question you would find the highest prediction numbers based on the models you have gotten,300694.0
141941,613243.0,you would find the highest prediction numbers based on your models,310952.0
141933,613690.0,"The issue could be in the Data Cleaning and Data preparation stage. Do check if all data quality issues have been resolved. Also, do check the input parameters given while creating the pivot table. For example, beer_features = train.pivot( index='review_profilename', columns='beer_beerid', values='review_overall' ) In the above example if the wrong column name is given for the values parameter (as underlined above) , it would create an issue. So, you should check that as well. Hope that helps.",317998.0
141935,613154.0,Python file,300694.0
141935,613302.0,python file,305650.0
141935,613989.0,"You need to submit your code in "".ipynb"" format. A python file.",329936.0
141935,612985.0,It is Python only. TA confirmed it.,307495.0
141935,613082.0,"On the Submission page, it is given: Important Note : All your code has to be submitted in one main ipynb file.",311117.0
141935,613247.0,"Python file On the Submission page, it is given: Important Note : All your code has to be submitted in one main ipynb file.",310952.0
141649,612098.0,Try to take fewer data in the dataframe then try to plot them. Slice and Dice that is breaking a body of information down into smaller parts so you can infer something out of it.,311466.0
141649,612344.0,"Basically, you will need to group by data (grouping by aggregates the data). Once done, try plotting the graph . , say a histogram or distplot on the grouped by data(or the aggregated data) to get a meaningful visualization. As outlined by you earlier, when data is very large, ""data as such"" is less meaningful unless aggregated to what you have been looking for, For example, if we have customer purchase data on N items , if we try plotting graph on the transaction data, it does not yeild useful information; however if we group by data on the N items (and group by count), you have information on N items and how many purchases went in per item (derived as count). hope this information helps",309211.0
141649,612736.0,You need to plot grouped and aggregated data and infer the pattern from it. Based on the number of beers/ number of users it is not feasible to do analysis per item. Instead we need to get aggregated results and plot their distributions using histograms or box plots.,317689.0
141785,612466.0,"While predicting the ratings based on corr coeff, few rating have got more than 5 and less than 1 too",301115.0
141785,613091.0,i understand like it is done to bring back the similarity weighted predicted ratings back to the scale of 1 to 5.Then only we can compare with the existing prediction done by user,300725.0
141666,612323.0,requesting TA's to COnfirm on this Otherwise I am gonna calculate for both and put the visualization,317984.0
141666,612112.0,"I don't think you need to plot user id based calculation - you just need to calc these avg from the perspective of beers and perspective of users (I dont think there is any min calc besides choosing an appropriate N) so say from perspective of beers -&gt; you would find the average rating given to beers and you would find the average NUMBER of rating for beers e.g. if there are only two beers, so if beer 1 has 2 ratings and beer 2 has 4 ratings then the AVERAGE number of ratings is 3 and if the ratings are 2,4 and 2,4,4,2 respectively then the avg beer rating would be 3; similar calc would need to be done for reviewers also, this qn has been asked already more or less: https://learn.upgrad.com/v/course/208/question/141601",300694.0
141666,612320.0,"Hi Nitesh. Thanks for the answer. Even I was thinking the same initially that aggregate calculations should be done for whole dataset but Only after Going through the above question, I had this confusion as one of the answer s in the above question is saying that we have to calculate aggregates for each Beer/User",317984.0
141587,611715.0,Yes,311466.0
141587,611834.0,can u please provide ur code snipet . there is also a unusuall nan column,317822.0
141587,611742.0,"Can't say specifically, why don't you check it my .describe() function and see the min and max",311466.0
141587,612327.0,You are only checking head values. Please inspect the dataframe in more details using describe etc.,317689.0
141587,611865.0,"Check of the pivot is created correctly. beer_features = train.pivot_table( index='review_profilename', columns='beer_beerid', values='review_overall' ) movie_features.head()",428646.0
141601,611829.0,It is also intrresteing to see if there is a difference after and before decide N and remove low rateing count beers,317822.0
141601,611659.0,"hi, in the first, you have to visualise the avg ratings each beer has got. in the second, you have to visualise the avg. ratings each user has given. hope this helps.",311686.0
141706,612716.0,There is no deployment required. You just have to identify which model is performing better and should be used for prediction And to predict to users you need to on both the models as given in the question. It can be done as it is shown in the explanation notebook for this module.,317689.0
141263,610087.0,Below is my understanding: Average beer rating = mean of rating group by beer. Average user rating = mean of rating group by user. The average number of ratings given to the beers - mean of (count of rating grouped by beer) The average number of ratings given by the users : mean of (count of rating grouped by users),316036.0
141538,611838.0,well u can do it as follows: select a high value of N e.g. :45 then once u get the similarity matrix consider the top 10 find the correlation between the 10 rows and plot neat map with the values displayed. this works for both user and item.,317822.0
141538,611665.0,"hi, hope you have already removed a big chunk of records from original dataset. this will happen when we keep records of only those beers which have got atleast N reviews. if not, then pls do so and try again. if the problem persists you can also remove records of those users who have given less than X (let us say 30) reviews. this is not mentioned in the question, but is helpful in handling the dataset.",311686.0
141359,610678.0,"hi, it needs to be done on the data for which we create our similarity matrix. so yes, on train data.",311686.0
141829,612577.0,Got the solution. Please ignore this post,303228.0
141829,612781.0,I am facing the same issue.. How did u resolve it,308644.0
141829,612563.0,I assume you are referring to cosine similarity command. You need to select beers which has at lease N number of reviews to redice the datasize. Or else you can use google colab or Kaggle.,307495.0
141829,613159.0,"try not to create too many new dataframes, etc -&gt; and re-use dataframes, etc as Python does not release memory automatically -&gt; check your task manager to see if your memory usage is very high This was happening ot me (with 16gb of RAM) but after consolidating and minimising new dataframes and other containers the problem was fixed",300694.0
141791,612527.0,See this link : https://learn.upgrad.com/v/course/208/question/141785,311466.0
141681,613085.0,i also did the same by taking the first 10 users and beers and plot in heatmap,300725.0
141681,612291.0,"Yes, you can say it a part of exploration’s as this will show the how first 10 user to user and item to item are related to each other between. Basically, it will show the relation between them. You can do it by select first 10 rows from your similarity matrix for both. Then find the correlation between user-user and item-item based and plot visual for it.",311466.0
141681,612337.0,"As per what I understood, first 10 beers signifies the first 10 rows from the final prediction dataset (in this case Item- similarity/ second model). Once you pick the first 10 values, sort descending and choose 10 best values for each of the 10 beers. Once that's done, you get a 10 X 10 matrix , run a correlation and plot the heatmap. You can keep annotation on within heatmap so that you can see % similarity",309211.0
141681,612400.0,"The 10 best values is just an indication to get a square matrix. As number of columns is large, it is pointless to list down all the columns, the heatmap is virtually unreadable if you intend to plot all the values",309211.0
141681,612730.0,"I think we have to take first 10 beers from the item corelation matrix which gives us beer X beer rating for all the beers chosen. So, ideally you need to extract the 10 X 10 matrix from this matrix and get their beerids and plot corealtion among them in form of a heatmap. I dont think we are asked to plot top 10 (as identifying top 10 corelated beers from the matrix itself would be quite cumbersome).",317689.0
141681,613036.0,I would take the first 10 beers from the iterm correlation matrix and see what their correlation is between each other and plot that in a heatmap of some sort,300694.0
141877,613687.0,Prediction is done by doing a dot product of the user correlation (for user based model) and the ratings made by user (using the train dataset) . Test Data is used for the Evaluation purpose . The predictions made (predictions matrix got above) are evaluated on this test data set to evaluate how good or bad the performance of the model is and which model should be deployed . Hope that helps.,317998.0
141877,613807.0,"Hi Harshit, I would have had no issues if the predictions made on the train dataset were used for evaluation. But, this is not what was done. Please go through the movie recommendation system example's Evaluation section. The model is trained on the test dataset and evaluated. This is what my problem is with the methodology.",319357.0
141877,614187.0,"Hi Akbar, Really Sorry, I hadnt noticed this. You have a very valid question. Lets wait for the TAs to respond. @TA Please clarify this. If the Test set is being used for generating the predictions and evaluation is done using these predictions, then what is the even need of having the Train dataset?? what is the need of generating prediction ratings using train dataset? Please clarify. thank you.",317998.0
