id,body,owner_id
117508,can we proceed with Lasso and say columns which have higher magnitude of the coefficient are more important?,317984
119445,nan,312376
117938,We need to convert the MSSubclass column to dummy since it is actually mapped to categorical variables. Can anyone confirm ?,318756
118156,nan,300721
117836,Electrical field has one NA value. Can we straightaway drop this row? LotFrontage has certain NA values. According to its definition it says its Linear feet of street connected to property. So can replace it with 0 (meaning no feet assigned) MasVnrTpe and MasVnrArea have NA values . They are both dependent. Can we delete these rows too? How to deal with GarageYrBlt?,301114
118417,"In the car pricing example, i noticed that the scaling was done before train and test split, which is contrary to what was taught earlier. Any reason why it was done in this manner?",310509
117880,"Say train accuracy is 90% and test accuracy is 70% which is overfitting. As we adjust hyperparameter lambda, we get say test accuracy down to 80% and train accuracy of 80%; is that the conclusion of finetuning the model ? in simple words, my question is ""should r2_score of test and train match before deriving result coefficients ? """,309211
117518,I get different values of Alpha in Ridge and Lasso .. is it fine?Or did something went wrong somewhere,319759
117605,The year column GarageYrBlt (Float type) (The year of garage built) contains few NA values. Any suggestions on how to impute the column values?,311115
117950,"CODE: lm = Lasso(alpha=0.001) lm.fit(X_train, y_train) # predict y_train_pred = lm.predict(X_train) print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred)) y_test_pred = lm.predict(X_test) print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred)) ERROR: NameError Traceback (most recent call last) &lt;ipython-input-373-eecc84d10559&gt; in &lt;module&gt;() 6 # predict 7 y_train_pred = lm.predict(X_train) ----&gt; 8 print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred)) 9 y_test_pred = lm.predict(X_test) 10 print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred)) NameError: name &#39;metrics&#39; is not defined",316036
118441,nan,303228
117468,"Please help in overcoming this error : &quot;# scaling the features from sklearn.preprocessing import scale # storing column names in cols, since column names are (annoyingly) lost after # scaling (the df is converted to a numpy array) cols = X.columns X = pd.DataFrame(scale(X)) X.columns = cols X.columns&quot; I get the error : ValueError: could not convert string to float: &#39;Normal&#39; I try to find which columns are float variables in X - X_float = X.select_dtypes(include=[&#39;float64&#39;]) X_float I get - the serial number column - 0,1,2,3..as the output. None of the other columns are of float type. And there is no entry of &quot;Normal&quot; in serial number.",319759
117267,What re these L1 and L2 regularisations in regressions that are asked? Are they referring to Ridge and LAsso methods?,308437
118448,For the models the R2 score for train is coming close to 0.9 and for test it is close to 0.7. How could this be happening? How to resolve this ?,318479
117276,nan,318455
118433,Will it be fine to remove rows with 'NA' in following coloumns BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2 Similarly in Garage type columns.,318772
118086,"From the coeffiecients obtained after Lasso Regression, how do we map them with the features and find out the final significant variables?",318804
118367,"After performing Lasso, the no. of features with non zero coeff are too many. moreover they suffer from multicollinearity. How to remove it?",304319
118100,nan,308635
118650,"In the example of car price, scaling of features is down first and then test train split is done....but we don't want test data to see the scaling done...hence we need to perform split first and then scaling right... Pls correct my understanding",308437
118102,"In the Adavance Regression assignment categorical variables like Neighborhood, Exterior1st have more than 10 unique values. For example Neighborhood has 25 unique values. How to handle these values, shall I just go ahead and create dummies get like 25 dummies for Neighborhood only. If I do like this including categorical variables I would get more 200 predictors columns. Is there any better way to handle this type of scenario?",301118
118103,"Columns like Yrsold, GarageYrBlt etc., have year as values. Should these variables be treated as categorical or numeric variables ? Also, columns like FirePlaces, GarageCars etc., have distinct numeric values. Should these be treated as categorical or numeric variables ?",313691
117317,"EDA, RFE, Ridge using RFE and Cross Validation, Lasso using Cross Validation.",310419
117322,I see that you cannot drop any columns (totally there are 80 excluding target variable). Hence we need to do RFE to bring down no of features and then build model right.,308437
117745,what the best way to select linear model I mean constant should be less to co efficient of the dependent variable to be less ?,312019
118074,I understand that LassoCV/RidgeCV are models that train on cross-valiation sets and are helpful in hyperparameter tuning. How does this differ from GridSearchCV? Is Lasso+GridSearhCV equivalent to LassoCV? Or should GridSearchCV be used with LassoCV as an estimator as well? What could be the pros/cons of each?,318438
118044,"As per question we need to build a regression model using regularization. Does this mean we need to create linear regression model? If yes, RFE is returning different columns each time the code is run. Any suggestion how tvo tackle this.",320103
117336,nan,314678
117760,"Do we evaluate on R2 Score, RMSE?",304319
118134,nan,303228
118082,nan,318005
117774,nan,314678
117359,nan,318455
118161,The graph does show some alpha value for feature selection. How can we be sure that what we have selected is correct. Should we need to find R2 score for different alpha value selected and the come to a conclusion?,301114
118492,"When I tried to remove outliers for a column by column, I got only 490 records against 1400 records... Can I continue with 490 records only or should I ignore outliers and build the model?",317073
118172,nan,300988
117277,"For many of columns value is NA, which i understood is not a &quot;NULL&quot; value and means a specific value for that column. e.g. Fence: Fence quality GdPrv Good Privacy MnPrv Minimum Privacy GdWo Good Wood MnWw Minimum Wood/Wire NA No Fence So in this case I believe we need to treat this as a value and not &quot;NULL. Please suggest.",320103
117816,"I replaced NAN with 0 in LotFrontage column, not sure if its the correct thing to do. can you please advice?",316036
118785,Cv_results = pd.dataframe (model_cv.cv_results_) Cv_results= cv_results[cv_results[param_alpha]<=200] What is happening in the above code Can anyone explain in detail?? Why,308437
117695,"if outliers pertaining to all the numerical features are handled, then more than 50% records are getting removed. will it be correct to do so?",311686
118214,nan,310472
118236,nan,317575
118011,"After performing ridge and lasso regression for the assignment, I am getting these two graphs for R-squared scoring type: For Ridge Regression: For Lasso Regression: I just wanted to ask if these graphs are okay to go ahead with and also I got ridge and lasso train and test scores as follows Ridge regression train score: 0.8887 Ridge regression test score: 0.8685 Lasso regression train score: 0.9029 Lasso regression test score: 0.8576 Are these scores optimal enough ?",301655
118243,nan,312093
118332,"I understand the difference between MAE and MSE/RMSE, but in what cases should we use MAE over MSE and vice-versa.",318438
118299,nan,303228
118462,"Does Advanced regression asisgnment is similar to linear regression except just finding lamda value for ridge and lasso Does assignment checks for generalized regression, I am not able to find as part of statement.",309212
118249,nan,318335
118080,Soppose I have 10 independent variables out of which 8-9 are showing linear relationship with dependent variable. 1-2 showing complex ex quadratic or sin or cosine relationship. Then should we consider polynomial or simple linear regression for the model ?,315679
118258,"How do you handle categorical variable cases like in the current assignment of Condition 1 and Condition 2 or Exterior 1 st and Exterior 2 nd . Generally, such categorical variables will have large subcategories duplicated in both options. If one dummies both these variables, will lead to large set of variables. Will the current model be able to identify the better predictor of Price? As the option will be split between two columns. Any way to handle such categorical variables?",317514
118273,I am seeing difference of 5-10% between train and test R-sqaured (test being lower) after finding the optimal alpha value for lasso and ridge. Is it expected from current dataset or is there something else that I can do.,318329
118570,If we need to perform scaling on dependent variable also while creating models ? Is it necessary to perform..what implications it has ?,305650
118277,nan,304319
117882,"Problem statement states that ""build a regression model using regularization"" So do we have to build model using both Ridge and Lasso regression ? If yes then - We know that Lasso Regression do feature selection, so we don't face problem while doing this. - But in Ridge regression there is no feature selection takes place, so do we have to perform RFE after Ridge regression ?",317991
117899,Is there a thumb rule on how to select the best lambda (or hyperparameter) values ? OR can any random value be chosen and then we have to fine-tune the same?,309211
118644,Generally we do scaling for only numeric features right? But in advanced regression we r doing scaling for all categorical and numeric features this is what I observed in the coding Correct me if am wrong... Why is ds so?,308437
118292,nan,318335
118175,"Advanced Regression assignment: Do we need to remove outliers? If yes, after creating dummies?",303666
117849,"In lasso and ridge, We check the linearity of the coefficients/weights/ model paramenters with the output variable. We do not check the linearity of output variable with the features/fields/columns. Hence multicollinearity/VIF check are not required for lasso and ridge Please confirm if this is correct.",310467
118302,nan,314197
117853,nan,304319
117854,RMSE for both ridge and lasso is coming to the tune of 10000. Is it ok? how can I reduce it? What should be the values of AIC and BIC?,304319
120162,nan,318481
117489,how to find the column names which have alues Yes or No. Because too many variables to see manually,312019
118820,"for ridge and lasso regression we select range of alpha and then chose optimal value of alpha for which we get coefficients. 1. how to select range of alpha? what is the guideline? 2. how to chose optimal value of alpha? if i keep increasing value of alpha for lasso, the no of coefficients with zero keep increasing, so when to stop?",308437
117868,nan,300721
117185,Regarding Advaced Regression Assignment Finding suitable features for the price of the house Its Multiple Linear Regression Then try with Regularization methods Ridge and Lasso Is its correct ? Or directly after data prepartion we can go into ridge and lasso ?,312019
117495,can we use other tlibrary rather than h2o?,320689
118154,"Hi, What is the impact of skewed data (both dependent and independent) on the overall model accuracy/stability? If you look at the pairplot of the features, lot of them are skewed. Shouldn't they be addressed? 1) What are the common transformations? 2) If we transform the training dataset, should the same tranformation be applied to the test dataset? Or should be ensure that the test dataset is actually skewed before applying the transformation?",318438
118271,"In the advanced regression notebook given for practice (Car Price Prediction) during the lecture videos, the dependent variable &#39;Price&#39; hasn&#39;t been scaled. Is there any particular reason for this? Do we follow the same for the assignment? I&#39;m facing the issue of all the coefficients of the lasso regression model being clamped down to 0 when the dependent variable &#39;Sale Price&#39; is scaled, i.e., no variable turns out to be significant. But there is no such issue without scaling the dependent variable. What must be followed? Please guide.",310505
118291,nan,317460
118333,"In earlier regression model we used statsmodel api, where the metrics like r2, adjusted_R2, AIC, BIC are calculated by package itself. Now do we need to calculate adjusted_R2, AIC, BIC metrics for Lasso and Ridge regression ? TA's can you please confirm.",317991
118361,"As you have determined the optimal value of lambda for ridge and lasso regression during the assignment, which one would you choose to apply and why? What the statement ""which one would you choose to apply and why?"" means?",320103
117878,"I went through lecture videos again and stuck at this. Scikitlearn statsmodeler uses OLS (ordinary least squares) for linear regression ; not sure how to arrive at AIC, BIC and adjusted R2 for Lasso or ridge regression",309211
117881,Do we need to convert datetime data to ordinal data ?,313826
117512,Lasso and Ridge regression shrink the magnitude of the coefficient. While Scaling also effects the magnitude of the coefficient of Features. does it increase or Reduce the coefficients?,317984
118028,"Instead of doing outlier treatment individually for one column, is there any way to implement outlier treatment for all columns at once, as in currrent assignment, no. of outlier treatment are many(are around 20)",318429
117891,,318723
117892,Multicolinearity validation using VIF on lasso model shows high VIF for some variables. Shouldnt lasso take care of this automatically ?,316211
118219,"I have multiple Queries? 1. How to tacke below years &amp; month columns YearBuilt, YearRemodAdd MoSold YrSold 2. Can we remove MoSold (month sold) column and what is the role of this column. I am thinking of removing it.",306243
118360,"Question-5: As you have determined the optimal value of lambda for ridge and lasso regression during the assignment, which one would you choose to apply and why? What the statement &quot;which one would you choose to apply and why?&quot; means?",320103
117895,"Shape of the dataset is 1460/81. While performing EDA I dropped 5 columns having more than 30 % missing values, after that when I checked on rows missing value percentage I found it to be 1% of the rows, I am not willing to remove these rows as I am having just 1460 rows to design model. Shall I proceed with imputation for those rows?",300721
117896,I still don't have correct response to AIC/BIC derivation from https://learn.upgrad.com/v/course/208/question/117878 as I want to understand how much my model is penalized. I do understand Lasso/Ridge are just used to generalize by applying hyperparameter. What I want to know is which python package can I use to derive AIC/BIC and adjusted R square values?,309211
118643,I am trying to run lasso algo. It is taking too much time to execute and not getting output. System hang What could be d reason pls help,308437
117900,Is there a thumb rule on how to select the best lambda (or hyperparameter) values ? OR can any random value be chosen and then we have to fine-tune the same?,309211
117918,How is the optimum value of K decided in k-fold cross validation ?,309211
119240,nan,308495
117603,"If i use ridge or lasso, we wont do any rfe to elimiate features. How to select best features when we apply ridge or lasso. when i use list(zip(cols, model_parameters)) , it gives all features which we made with training set.",312019
118031,"I want to find out the adjusted rquare value after doing lasso regression on test dataset. Since k denotes the number of predictor variables, will it be equal to the number of features(columns) in xtest ? or will it be equal to the number of non-zero columns obtained after after doing lasso regression.",318756
118131,"For the assignment, is outlier removal required ? Because performing Lasso without outlier removal and also with outlier removal has significant impact on performance measures on test data. TA's please confirm",317991
118410,List at least 4 differences in detail between L1 and L2 regularization in regression.,320606
117888,"How to determine whether to go for StandardScaler, Minmaxscaler or which function to use to scale values?",309211
118473,i got around 670 features after dummy variable encoding. am i going in the right direction?,308437
118293,"There are around 18% null values in LotFrontage column. How to handle this ? I know we can perform any one of the following 1. Remove rows having null values 2. Impute values with mean, median or mode 3. Drop column I did first one and found that Train and test scores are quite close. - but concern is the dataset is small so should we drop 18% rows.? When I tried 2nd method of imputing test score drops by 5%. 18% null values is not that much to drop a column. So now which method to pick. ? TA's please help",317991
117731,"We would need to convert the year columns into numeric values like age. In case of GarageYrBlt, since there are datapoints with no garage and the col GarageYrBlt contains 'NA', how do we find out the GarageAge for such datapoints? Should we replace them with -1? Will the scaling get affected then?",304319
118354,I am getting an error &#39; Must pass DataFrame with boolean values only&#39; when doing scaling...tried both MinMax and Standardscaler but error is the same. How to resolve this?,310509
119355,nan,308495
90603,How to access the Live session for Python DS assignment question answer session?,301644
90026,nan,318082
92342,"I have a 32Bit OS and there is no Server Versio 8.0 available for 32bit. On Version 5.7, there is no Window Function available. What should I do. Please do not suggest to install 64bit OS.",318009
93232,"I am not able to understand the method of writing syntax for updating table commands and windowing functions. Till now while writing code we didn't put a comma at the end of each line and just used a semicolon at the end, but in cases like for date manipulation and other updating table commands and window functions we need to put a comma at the end of every line. so when should we insert comma at the end and when not to. and what is the significance of this?? for eg: select ssn, concat(fname, ' ', lname) as emp_name, dno, salary, sum(salary) over () as total_salary, sum(salary) over (partition by dno) as dep_salary from employee order by dno;",317558
92390,"select ssn, concat(fname , &#39; &#39; ,lname) as emp_name , dno , pay , sum(pay) over (partition by dno order by ssn rows unbounded preceeding ) as cummulative_total , sum(pay) over (partition by dno order by ssn rows between 1 preceeding and 1 following) as one_above_and_one_below from employee; The select is showing an error : is not valid at this position for this server version , expecting : &#39;(&#39;, WITH How do I solve this issue??",306729
91846,"Hi All, I am trying below query to get the row number. select e.ssn, e.salary,ROW_NUMBER() OVER ( order by e.salary ) as &#39;row_number&#39; from employee as e; I get an error as &quot;Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;( order by e.salary ) as &#39;row_number&#39; from employee as e&#39; at line 1 0.00017 sec&quot; Any suggetion to reolve this?",311741
91470,"#select Dno,fname,salary,rank() over e as ranking,dense_rank() over e as denserank, #salary,sum(salary) over w as cumilativesalary,avg(salary) over w as avgsalarydeptwise from employee window w as (partition by dno order by dno) window e as (partition by dno order by salary); Howeverthis is throwing error ?",303674
92080,"Hi All, I am running some query wherein i am trying to insert a result of window function into another table. I but i am getting an error as ""Error Code: 1264. Out of range value for column 'tmp_field_1' at row 1"" INSERT INTO table_1 (Date, Close_Price, 20_Day_MA, 50_Day_MA) SELECT Trade_Date,Close_Price,TRUNCATE(avg(Close_Price) over(ORDER BY Trade_Date ROWS 19 PRECEDING),2) as 20_DMA, TRUNCATE(avg(Close_Price) over(ORDER BY Trade_Date ROWS 49 PRECEDING),2) as 50_DMA from assignment.table_2; Any suggestions ?",311741
91541,nan,319006
89830,"Didn&#39;t understand query mentioned below very properly. Can someone help? select ssn, concat(fname, &#39; &#39;, lname) as emp_name, dno, salary, first_value(salary) over (partition by dno order by ssn rows unbounded preceding) as first_val, nth_value(salary,2) over (partition by dno order by ssn rows unbounded preceding) as second_val from employee; Also, unable to imagine any use cases of the same. Pls. suggest some.",311686
91567,"Is it possible to pass a table as an input to a Stored Procedure? If so, how do we access the column names of that passed table inside the stored procedure. Any pointers to this is highly appreciated. If you have come across any useful links, please share.",318084
94059,nan,315436
91693,nan,310009
92111,I wanted to pratice window function but I don&#39;t have dataset like Company table or employee table. Has professor shared with us any data set related to SQL ?,315423
91246,"select ssn, concat(fname, &#39; &#39;, lname) as emp_name, dno, salary , sum(salary) over () as total_salary, sum(salary) over (partition by dno) as dep_salary from company_db.employee order by dno; Throwing the error as07:43:39 select ssn, concat(fname, &#39; &#39;, lname) as emp_name, dno, salary , sum(salary) over () as total_salary, sum(salary) over (partition by dno) as dep_salary from company_db.employee order by dno Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;() as total_salary, sum(salary) over (partition by dno) as dep_salary from com&#39; at line 3 0.000 sec",318846
91879,"For ex : Roll no ,total marks for student table given Roll total marks 1 50 ------------- &gt; calculated as abs(50-60)+abs(50-70)==30 2 60 -------------&gt; calculated as absabs(60-50)+abs(60-70)==20 3 70 -------------&gt; calculated as absabs(70-50)+abs(70-60)==30 and we have to calculate sum of (absolute difference of total marks with respect to other students)for every student . Note: how to exclude current row from calculation in window function",318005
92777,"I am using Ubuntu 16.04 -64 bit . I recently updated MYSQL version 8 from version 5 as I was unable to use window functions. My workbench is still sql version 6.3.6(version 8 not being unavailable for my OS).Hence , while using workbench I am facing the error :Incompatible/nonstandard server version or connection protocol deleted.",318324
91978,nan,301890
91986,nan,301890
91248,If anyone installed mysql8.0..please provide the right link to install the 8.0version. With the current version onlt the Windows_function&#39;s are working. Can we upgrade the existing version 6.3 to 8.0 ?...If anyone istalled please provide the right link.,318846
91881,&quot;DECLARE @hno int;&quot; gives an syntax error - where did i go wrong?,319759
92340,"While executing the Window Functions, getting the below error. SELECT hno, avg(salary) over (order by salary) from employee; Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'SELECT hno, avg(salary) over (order by salary) from employee' at line 2 0.000 sec Please advise. Unable to proceed with the assignments",318804
92067,"select ssn, concat(fname, ' ', lname) as emp_name, dno, salary , sum(salary) over () as total_salary, sum(salary) over (partition by dno) as dep_salary from employee order by dno; Throwing the error as07:43:39 select ssn, concat(fname, ' ', lname) as emp_name, dno, salary , sum(salary) over () as total_salary, sum(salary) over (partition by dno) as dep_salary from employee order by dno Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '() as total_salary, sum(salary) over (partition by dno) as dep_salary from com' at line 3 0.000 sec Even though i have latest version of mysql i.e 8.0",310611
92594,I am unable to run queries using Windows function in MYSQL.,318791
92615,nan,310385
93286,nan,314629
93197,"I have MySQL workbench 6.0. I am getting error message while running the query. Error message ""Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version""",303227
93358,Link for image: https://drive.google.com/file/d/1QarPXdj5L2jgeNUgCP641xuLbedclweO/view?usp=sharing,320687
94708,Here's the link to understand everything about SQL Window Functions and its types. https://www.sqlshack.com/use-window-functions-sql-server/,312953
91690,"In the below query which is used in the lecture to create UDF -Project_Pay_calc, what is the purpose of end if ? begin declare project_pay_per_hour float(8,2); if (pno &gt; 0 and pno &lt;=5 ) then set project_pay_per_hour = 1000; elseif (pno &gt; 5 and pno &lt;= 10) then set project_pay_per_hour = 2000; else set project_pay_per_hour = 3000; end if ; return (project_pay_per_hour * num_of_hours); end",312507
91899,DECLARE @hno INTEGER; Throws syntax error. How to correct it,319759
90061,how to concat and reurn/print varchar in stored procedure in MySQL,317993
91428,"In the given example in this section , When creating a custom function . Wher we are adding &quot;return char(50) deterministic&quot; right after create function statement . In the given function we are passing a string of size char(20) whereas in the next statement we are declaring char(50) with deterministic statement . So, what is the reason behind declaring char(50) in the next statement ?",301108
91450,nan,318429
91507,what is the issue in this code ? suppose if i remove the delimiter after end the storedproceure get saved but then when i call stored procedure it throws an error - Unknown column snn in where clause,303674
91526,Please advise where am i going wrong,300687
93005,"For UDF, I am getting the same result 889 times? What could be the possible mistake ?",308673
91549,"Hello , Can somebody please help me , why there are two returns in below code and why there is a diffirence in input and output datatype ? create function hello (s char(20)) returns char(50) deterministic return concat(&#39;Hello, &#39;, s, &#39;!&#39;); Thanks in Advance ! Deva",306147
91565,"What does &quot; a;&quot; at the end of the query means ? in below code ( select essn, project_pay_calc(pno, hours) as per_project_pay from works_on where essn = n ) a;",306147
92105,"When tried to re-run the user defined function, the below error is displayed. Can anyone please help why this error is coming and how to avoid it.",314730
91667,"why is @ sign required when calling the stored procedure, when it has already been defined as ouput field?",310509
91666,Why is output not received when calling employee details stored procedure ( it required another select command to be run ) whereas ouput is received when employee salary stored procedure is called? what&#39;s the difference between the 2 commands?,310509
91975,What is the purpose of using this keyword while creating UDF&#39;s?,316147
91745,nan,315757
91747,nan,315757
92226,nan,308432
91317,"Hello Can anyone tell me what is the use of Deterministic, why we have to use, what happens if we don&#39;t use it. Thank you Best Regards. Girish",301113
91822,"Hi, If we have cretaed a UDF and server is closed and started, can we use the UDF creted or do we need to recreate it.? Thanks",317410
92065,"Why do we need to add char(20), does it mean the name should not exceed 20 characters? and also why char(50) (i understood that the output should be greater than char(20) but why not char(40))? I didn&#39;t get the concept of using char and varchar. Please help me in this aspect.",308640
93103,Why should we again give select after calling the procedure? Is it because within the definition we just said select INTO and not select as?,318079
92609,nan,320688
92570,nan,316323
92780,"select concat(""kumar"",""avishek""); gives Kumar avishek but select sum(7,8); gives error",300684
92975,"select sum(per_project_pay) into all_project_pay from ( select essn, project_pay_calc(pno, hours) as per_project_pay from works_on where essn = n ) a; end $$ delimiter ;",318334
91648,"Are dynamic queries not supported in MySQL? I tried to write a function which creates a query based on the inputs and then prepares,executes. But getting error saying that dynamic sql not allowed. I wrote a procedure which accepts the same inputs , generates the query and executes. However,cannot call the procedure in a select query. So, I wrote a wrapper function to call the stored procedure and then tried to call the wrapper function in select query. Failed with the same error again.",313826
93338,nan,306008
93340,nan,320687
93407,"In Q4 of assignment, the UDF is returning NULL value. There is no syntax error on UDF or call query; Where is the assigning or logic going wrong? Cannot share code as it is graded question, so any suggestions to debug will help.",301644
93354,"what should be the code for problem Write a SQL query to convert the hello function (used in the previous section) to a stored procedure. The hello function for reference : create function hello (s char(20)) returns char(50) deterministic return concat(&#39;Hello, &#39;, s, &#39;!&#39;);",317558
91974,Can someone explain the difference between these two? All I know is user defined as prefixed by @ and local arent. What is the scope of these two variables?,316147
93512,"select concat(&#39;Hello, &#39;-----------------; end $$ writing delimiter at the end is throwing error. Is there a problem or unrequired statement I am using.",318372
92329,nan,320689
90766,nan,300690
91086,Can anybody explain this in detail. How it actually works at the backend?,305845
94000,"what if we write the below given query select fname, ssn, essn, dependent_name, relationship from ( select fname, ssn, dno from employee ) e inner join ( select essn, dependent_name, relationship from dependent ) d on e.ssn = d.essn and e.dno=4 and d.relationship= &#39;Spouse&#39;; as below. select fname, ssn, essn, dependent_name, relationship from employee e inner join dependent d on e.ssn = d.essn and e.dno=4 and d.relationship= &#39;Spouse&#39;; It returns the same output and i think its optimized. any suggestion on this?",322683
90767,nan,300690
91892,"-- check for uniqueness select ssn, count(distinct dno) as dis_dno from employee group by 1 having count(distinct dno) &gt;1;",318335
92554,"In Join optimization, I am not clear how combing conditions on where clause ogether into inner join&#39;s &quot;on&quot; will optimize. By removing the inner where clause, won&#39;t the inner queries return too many rows, instead of fewer rows? Does this have anything to do with order of execution? select fname, ssn, essn, dependent_name, relationship from ( select fname, ssn, dno from employee ) e inner join ( select essn, dependent_name, relationship from dependent ) d on e.ssn = d.essn and e.dno=4 and d.relationship= &#39;Spouse&#39;;",320074
92620,"In the optimised query in the lecture select fname, ssn, essn, dependent_name, relationship from ( select fname, ssn, dno from employee ) e inner join ( select essn, dependent_name, relationship from dependent ) d on e.ssn = d.essn and e.dno=4 and d.relationship= &#39;Spouse&#39;; What the difference If I use - where e.dno=4 and d.relationship= &#39;Spouse&#39; as in on e.ssn = d.essn where e.dno=4 and d.relationship= &#39;Spouse&#39;; Is there any performance benifit in the former?",318576
92435,"Intro to SQL instructed us to create a database with label ""companydb"". The SQL file required to download in Adding or Deleting Columns has the first lines as use companydb_pp; SET SQL_SAFE_UPDATES = 0; Is it OK to edit the line from use companydb_pp; to the database use companydb; in the file?",315022
91418,ALTER TABLE `DS_SEP_18`.`employee` ADD COLUMN `des` VARCHAR(45) NULL DEFAULT 'manager' AFTER `dno`; error: Error Code: 3664. Failed to set SDI 'DS_SEP_18.dependent' in tablespace 'ds_sep_18/dependent'.,312491
92355,"Hello Friends, In this section, there are no files provided for installation of the Company DB in MySQL database for learning and testing purposes? Kindly advise how are you all running the SQL Queries? Apologies if I have overlooked any steps of downloading the SQL Files to install the schema. Thanks Suddhasatwa",310217
92358,"Considering the companydb database, how do I assign a foreign key for a column of the employee table if the reference of the key is present in another table ?",313691
91025,what does this mean? here we say add fk_super_ssn as foreign key y r we mentioning super_ssn in ()?? also what does references employee (ssn) imply?,308437
91576,,318335
92075,"How to get the second last word in a column ? Ex: in address column of emplyee table ,address is stored as &quot; 638 Fondren, Houston, TX &quot;. But I want to get &quot;Houston&quot;. I dont want to do string split twce,",317073
91495,"In the video example - the line of code used to add ( &#39;add des&#39; ) a column did not specify for a column to be added, whereas the code to drop had &#39;drop column&#39; specified. Why do we have to specify while dropping the column and not while adding?",319302
91314,"In the question, when I run the first set of alter table command i.e., Alter table works_on Drop primary key; I get the following error: ""Error Code: 1553. Cannot drop index 'PRIMARY': needed in a foreign key constraint"". And the primary key constraints(essn and pno) are intact. None of the options mentioned in the mcq have the correct answer. However, looks like the answer is expecting that primary key as well as foreign key constraints would be dropped. Looks like there is a mistake in the question? AFAIK, you cannot drop the primary key before dropping the foreign keys referencing it. Am I missing something??",313826
91525,nan,304398
91536,"like if i want ot convert 18-OCTOBER-2018(TEXT) TO 2018-10-18 (DATE) I HAVE TRIES STR_TODATE ,CAST CONVET AND GETTING NULL",317982
93016,nan,310503
91535,"is it possible to implicitly assign foreign keys while using join functions, in case a foreign key is not explicity set for a table? will SQL assume that the join function &quot;intends&quot; to assign a foreign key in that way?",310509
91575,Tried dropping primary and foreign keys from works_on table but Error no 150 appeared saying Foreign Key constraint incorrectly formed.. pls help,319869
92858,"for Creating Table from an existing table i am using syntax create table emp as select fname, minit, lname, ssn, bdate from employee; but the output is not getting executed. what should be correct code for this??",317558
92860,can anyone explain me below mentioned syntax as i am not really clear about it,317558
92401,#adding a foreign key alter table employee add constraint fk_super_ssn foreign key ( super_ssn ) references employee( ssn );,305655
91697,nan,306996
91956,"In the above example, I did not mention the ';' in the first query statement and I am able to execute it successfully. Now I added another query statement which also executed succesfully even though the below error is shown. 'alter' is not valid at this position, expecting: EOF, ';' Does this mean that ';' is not mandatory to execute a query statement. Also, I would like to know how the statement with EOF error is executed successfully.",314730
91335,I&#39;m unable to load the Company_DB in the MySQL database. can someone please share the steps to do this? I can see the queries in the pane however I dont see table showing anything,306011
92123,With the example provided in the session #adding a foreign key alter table employee add constraint fk_super_ssn foreign key (super_ssn) references employee(ssn); employee table has 2 columns employee.ssn &amp; employee.super_ssn. Is it possible to alter the column employee.ssn or employee.super_ssn without having an impact of foreign key constraint ?,310518
92131,"MySQL is not importing the CSV files for stock exhange assignment as i am trying to do it manually using the import feature. it imports 0 rows if i mentioned the data type for &#39;Date&#39; column as &#39;Datetime&#39;. if i consider the data type as text only then it imports the rows, but the purpose is defeated as the data value are taken as text and later i am unable to change it to date using alter command. Anyone else facing this issue? ned to understand what are the guidelines for importing the CSV files into the schema as i dont think it mentioned in the assignment details.",316036
91786,nan,308432
91240,nan,308782
91123,"Select * from employee Order by extract(year from bdate) desc; This gives the employee details with the youngest last , not first right?",300717
91019,nan,308437
91855,nan,303673
91321,,305335
92178,nan,311857
91254,"create table recipes( recipe_id int not null, recipe_name varchar(30) not null, nothing int not null, primary_key(recipe_name), unique(recipe_id) ); In aove query there is an error - Error code 1064-- Error in SQL syntax. Can you please check and suggest?",300698
92051,SET SQL_SAFE_UPDATES = 0;,308634
91242,,308782
91895,"update employee set des = &#39;dummy1&#39; where ssn = 123456789; 16:17:39 update employee set des = &#39;dummy1&#39; where ssn = 123456789 Error Code: 1175. You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -&gt; SQL Editor and reconnect. 0.000 sec",318474
91568,"This was sample query in the content Can anyone explain &quot; ssn as `select`&quot; part create table e3 as select name as `Full Name`, bdate as `Birth Date`, ssn as `Select` from e2 ;",319869
92743,nan,318079
92516,,310504
93162,"When I execute drop primary key for the recipes table in the lecture, and further execute the describe command, I am getting recipe_id column automatically selected as the primary key. Is this expected?",310533
92670,When i tried to alter the employee table i&#39;m getting the error code 3664. Most of the google solutions revolve around foreign key and suggested restarting or dropping the schema. I tried both and no solution. Anyone have idea about it?,318370
93006,for date manipulation in employee table as explained i have written code as below: select bdate extract(year from bdate) as year extract(month from bdate) as month extract(day from bdate) as day from employee; but this is not getting executed and showing error. can anyone suggest what needs to be done in this?,317558
91853,nan,303673
91431,"alter table tb_1 add `dno` varchar(50); update tb_1 set `dno`=substring_index(`department`,( ),1) where department like `%technical%`;",300690
93401,after creating new column in employee table how to insert house no. from address into that new column?? can anyone please help me with the code,317558
93570,nan,303085
102148,alter table employee drop foreign_key super_ssn; 10:05:04 alter table employee drop foreign_key super_ssn Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;super_ssn&#39; at line 2 0.000 sec,319759
102142,"create table e2 as select concat(fname, minit, lname) as &#39;Name&#39; , ssn, bdate from emp; This gives a syntax error when executed in MySQL. Please shsre the updated synrax",319759
102145,alter table employee drop primary key; alter table employee add primary key (fname); throws error : 09:57:57 alter table employee drop primary key Error Code: 1553. Cannot drop index &#39;PRIMARY&#39;: needed in a foreign key constraint 0.000 sec,319759
123479,nan,300732
92448,nan,304815
90160,1. Are transactional databases and data warehouses powered by the same data management technologies? 2. As data scientists / ML engg. will we work with transactional db&#39;s or data warehouses? 3. How is an organization&#39;s data engineering team related to transactional db&#39;s and data warehouses?,306733
91382,nan,319006
92452,1) For Dates when the 20 Day MA crosses above the 50 Day MA the signal needs to be updated as BUY 2) For Dates when the 20 Day MA crosses below the 50 Day MA the &#39;Signal&#39; needs to be updated as SELL 3) For rest all Dates the &#39;Signal&#39; Value be kept as HOLD is my above undersanding correct - please confirm?,316036
91436,nan,311117
91467,SQL was working during prep time. Now I could not load the company db for practice. So went for uninstall followed by reinstallation. The intallation guide steps are being followed. However the install is stopping just before apply configuration option as shown in the guide. I think it is failing to connect to server as per eror message also assword is not getting accepted. Workbench is installed but most of the features are inactive due to incomplete installation. Please guide and help.,301644
91501,"I&#39;ve seen in sessions Prof. uses DB diagram to understand the relation between data, also it is easiler to read too. So Can we have such tool ?",315423
91517,nan,318579
92533,"I answered star schema e commerce company related both question for dimensional table and list of attribute in more than 100 words but stil it is showing edit your answer . what should I do now , based upon my undestanding and video lecture I attended but still edit answer is showing .",319969
92534,is tehre any easy and quick method available for pivoting long row and cloumn based csv file. any link or any method pl. help me .,319969
90756,nan,308437
91504,"As mentioned, the aggregate fact tables are numeric rollups of atomic fact tables fields, how do we normally create or populate Aggregated fact tables? For e.g., When a new row is added to an atomic fact table, do the aggregated fact tables are automatically recalculated, ( for e.g., in relational database notion, any stored procedures or triggers used to recalculate to aggregate the fields) or do they calculated manually and populated using queries.",311115
90346,Facts were defined as Numerical data attributes. But the Fact table example taken in the schema lesson doesnt have all the columns as numerical. Can someone please explain this?,310511
91594,nan,318421
91638,nan,306996
91625,"Looking at the schema diagram, how do we identify the fact tables? Is there any notation used for fact tables or do we have to go through the table columns and check if it is a fact? Specifically, the Lead table in the upGrad schema has some numeric values but those are present in the conversion table. Customer name and email id are present which could be classified as customer dimension. How to know that this is fact table ?",301654
90508,nan,310419
91217,"I understand that the facts cannot be modified unless there is a necessity, but can we update the dimension and fact table once created or do we need to create additional dimensional tables with the attributes I might need in future?",318397
91847,"In the differences between OLAP and OLTP, prof. RC mentioned that front end technology like service side application - API will not be used in data warehouse. I am wondering why technology cannot be used in the data warehousing. As in the ETL process, there is a possibility to load data from data warehouse database to a service side application like API. Also, why BI tools cannot be used as front-end technology in a transactional database.",314730
91126,nan,319866
91858,There are other schemas like Snowflake Schema and Fast constellation Schema which are widely used and put into practise effectively. Is there any particular reason other than advantages of Star schema with respect to other schema?,301118
91249,Really did not understand why it was a wrong question. Could someone shed some light on this?,308962
91361,"Data persistence: A data warehouse stores current as well as historical data, which may not be the case with locally stored files in MS Excel, etc.",318770
91046,Do Dimension table is auto generated in DataWare Housing for analystics,301108
92156,Operation failed: There was an error while applying the SQL script to the database. Executing: ALTER TABLE `DS_SEP_18`.`employee` ADD COLUMN `name` VARCHAR(50) NULL; ERROR 3664: Failed to set SDI 'DS_SEP_18.dependent' in tablespace 'ds_sep_18/dependent'. SQL Statement: ALTER TABLE `DS_SEP_18`.`employee` ADD COLUMN `name` VARCHAR(50) NULL i'm not able to alter table without dropping and re-creating the table.,312491
93288,,315831
93237,nan,311046
91508,nan,304398
91425,,304696
89461,nan,311254
92477,alter table employee add age int; Error Code: 3664. Failed to set SDI &#39;Company_GQ.dependent&#39; in tablespace &#39;company_gq/dependent&#39;. 0.108 sec,312376
89520,"Is there any deviation from the companydb used in the introductory course, my answers dont seem to be matching even the same query was being used. Will it be possible for anyone to share the dataset? Data Link: https://learn.upgrad.com/v/course/208/session/15786/segment/79808",318381
92195,"The previous question for average distance calculation for employees is working correctly. however the last question in grade IV - the average distance by department say (department no =5) and What will be the department average distance for Franklin T Wong? - is not working correctly. Anybody facing the same issue.? Thanks,",311115
91494,"I executed following SQLs update employee set age = datediff( &#39;2018-09-30&#39;,bdate)/365 and select sum(age) from employee returns 483. However the answer provided is different than what I got. and I lost my first attempt. I updated the age column based on 30-sep-2018. TA&#39;s could you please help me with this.",300708
89627,Quiz 1-question 4- date function- two options are same and birth date of all employee has same month and same date.,310419
91531,,300719
93689,Determine the average distance between the house of CEO (super_ssn is null) and the employees. I'm not able to understand this questions.is the CEO's ssn=888665555.,320687
92538,nan,305847
91546,"Consider the distance between the two houses as the difference in the house numbers, so the distance between house number 2 and 38 is 36 units. the sum value of house which i am getting is not matching with any given value. i understand i need to get the sum of the house numbers of the address or am i missing something not expecting any direct answers.please clarify the question",300687
91929,nan,318335
93045,What is the meaning of answer in units,312892
91007,nan,300690
91577,List the employee whose birthday falls on January 1 before the employee whose birthday falls on January 2 what does it mean Jan 2 employees should be displayed before Jan 1 or Jan 1 employee records should be displayed before Jan 2.. ? ? Why this is being put in a misleading way. ?,306735
91614,nan,300690
91963,"When i am running sub_string index fuction, one of the house numbers which is 21 is returning as 291. Why is that happening?",310509
91701,"""Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal."" the condition will either be : `20 Day MA` &gt; `50 Day MA` OR `20 Day MA` &lt; `50 Day MA` What does ""Merely being above or below is not sufficient to generate a signal."" mean? It is very confusing.",305845
91700,"I am getting the below error when creating a new column hno and assigning the value to it 0 51 19:15:45 update employee set hno = substring_index(address, &#39; &#39;, 1) Error Code: 1175. You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -&gt; SQL Editor and reconnect How to solve this? 0.000 sec",310509
92154,This is the error I am getting. Does anyone know how to manage this? Is it mandatory to use a WHERE to update a table? How can you update a table without the WHERE clause when specifying the where can be tedious for multiple updates of a column?,316416
91751,"I have taken care of finding the absolute difference and also not including the CEO's hno. I am still getting an output that's not in the options. P.S: I have seen a similar question in the discussion forum, but the answers still couldn't solve my problem.",312507
91994,,312746
91673,"Something i came across while attempting the graded question III - After creating a new column when i tried to run the query to update the column i received the below error: Error Code: 1175. You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -&gt; SQL Editor and reconnect. I followed the instructions and changed the setting - and then was able to run the query. For anyone facing similar concern, can refer to the below link for more details: https://stackoverflow.com/questions/11448068/mysql-error-code-1175-during-update-in-mysql-workbench",319302
92009,,300690
91348,"based on the question, i have created a function to find the average distance of an employee with the rest of the employee. However, the average answer i am getting is not matching with the options provided. I calculated the same in an excel file as well, and the answer i get from the functions is matching. Not sure if i am missing something. Any guidance would be appreciated.",316202
91228,nan,308495
91235,nan,308495
94291,eg 30-July-2018 to dd/mm/yy,320606
93329,"If i create a User Defined Function. Can i use it in a window function as a group function ? E.g. in the lecture the group function SUM() is used in the below query: SELECT ssn, dno, SUM(salary) OVER() as total_salary from employee; Let&#39;s say i create a UDF named TAX(salary), and i try to use it as below: SELECT ssn, dno, TAX(salary) OVER() as total_salary from employee; I get error &quot;over is not valid at this position&quot;.",308440
92518,Kindly help am unable to figure out the error,303673
93453,DB browser prompts for a pass key used to encrypt the file that is the dataset for the questions after it.,306726
93423,Can anyone help in solving avg distance 3rd question?,318350
92413,"Hi, Do we have any in-built function similar to sum() for difference . This is w.r.to the question 3 in Graded question-ii.",317410
92574,Am unable to figure out what need to be done in Grade III self join question,303673
92412,Does anyone know when this error occurs? I am running the below script. select avg(abs( hno -(select hno from employee where super_ssn IS NULL ))),310522
92906,how to resolve syntax error with DELIMITER; end $$ DELIMITER; Funciton is compiled but DELIMTER is considered along with next statement instead of ending the function and error is thrown for the next query,311868
92908,nan,315423
92750,My answer does not match with any one of 4,301121
92809,"CODE: update employee set hno = substring_index(address, &#39; &#39;, 1) Error Code: 1175. You are using safe update mode and you tried to update a table without a WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -&gt; SQL Editor and reconnect. I toggled the option in preferences but I am still getting the same error",312608
93189,nan,315560
93368,what is wrong with this code ?,311864
93415,can anyone please explain me code given below and its output select avg(abs(hno-(select hno from employee where ssn = &#39;123456789&#39; ))) from employee where ssn != &#39;123456789&#39;;,317558
93413,"can anyone please explain me code given below alter table employee add hno int; update employee set hno = substring_index(address, &#39; &#39;, 1); select name,hno, rank() over (order by hno) as &#39;rank&#39; from employee; is there any easier method f writting this code?",317558
93434,"alter table employee add age int; update employee set age = datediff(&#39;2018-09-30&#39;, employee.bdate) / 365.5 ; select sum(age) from employee ;",302735
93577,,320603
83382,,310585
82117,nan,308437
82152,nan,307843
86347,,301890
83410,,310585
82535,,311160
83381,nan,310585
86041,I do not have an option of two months free trial as I am not a new user so is it fine if I use MS Excel 2010 for Data Analysis?,315661
87471,nan,317995
85740,nan,314313
85742,,314313
86201,nan,313767
86300,I am not able to find any link from where excel software can be downloaded for mac? Can anyone guide me through this?,318576
87782,"With reference to: Introduction to Data Management &gt; Module 4 &gt; Session 1 &gt; Ungraded Assignment Question: How do we do this better, instead of manually highlighting the top 10% for each region? ------------------------------------------------------- Section 4: Report Making - II Conditional Formatting - I: Within every Region, highlight the top 10% orders by sales in light green fill and dark green border",317987
88142,After downloading the hospital-charges none of the icons such as sort work even after highlighting/selecting them any idea on how to fix this,312892
95872,Important Links for MS- Excel learning: Could be handy for certain concepts. You can check out following important links.. 1. Link For Learning Excel Features www.contextures.com 2. Link For Learning Excel Advanced Formulas www.excelexperts.com 3. Link For Learning in YouTube ExcelIsFun Thank you,310508
82270,nan,308437
87109,nan,318576
80126,"Ok Guys, So here is a problem. I am going through the Excel session in Module 5, In the ""Cell Referencing and Text Functions"" Anand has described about averaging all the 1 responses. But when I try to use the same formula all I am getting is 1 as a result. Here is the formula I am using, can somebody point out what is the problem here. =AVERAGEIF($X$2:$X$5290,Y2,$W$2:$W$5290) I have tried using AVERAGEIFS as well.",300734
76876,"In logical formulae , we r using if condition and or conditions apart from this can&#39;t we use more comands to calulate the eligible person whether if he is more than age 60 or less than age 20",301111
87234,,318344
83571,Well attending the quiz in excel module I choose the option &quot;a&quot; then a pop up came out and says the answer is incorrect then I choose the option &quot;d&quot; it shows correct and what it shows for answer 1st is Correct you missed it Refer the screenshot,312357
88278,nan,310385
83172,"I have applied a formula to a cell. Then double clicked on the Autofill square at the bottom right of cell. What i see is some 84 cells were applied the formula, remaining were not. How do we do it for the complete column in one short?",301114
85619,"Is there is any better way to insert and delete rows and columns in excel using shortcut keys other the below option. I am using Office 365 and I believe there could be other easier shortcut key options than below. To insert row: Select a cell, Shift + Space and then Ctrl ++ To insert column: Select a cell, Ctrl + Space and then Ctrl ++ To delete row: Alt + Q and select the option to delete a row or column in the drop down (Or) Ctrl - and select the approprite option in the right click menu to delete a row ot a column",314730
86130,nan,318344
80603,"Do we also need to know how to write Macro in excel, to work in data analytics?",306011
82175,"Question Using the raw data, find the top 3 profitable Product Sub-Categories in each region Sheet superstore_sales My Solution Below is what I did using column concatenate and averaging the product base margin on top of that and then using custom sort for each region data to find the top 3 sub-categories: Is there an automated way of doing custom sort on top of the aggregate with a formula so that it reports the top 3 sub-categories in the top for each region?",310974
82637,nan,308437
82723,nan,307843
83469,1..i have two queries..firstly when i changed the profit values from count to sum the values turn as zero .pls advise 2.USing pivot we get the solution but as it is asked to do using raw data can we get the solution using raw data .can anyone advise thanks,300687
86283,nan,305842
120683,@TA - Please confirm if we can use kaggle for the MNIST assignment coding and keep it as a private kernel till assignment scores are out. As we have been encouraged to take part in the kaggle competition my understanding is that we can publish the same kernel on kaggle at a later date?? Please confirm.,301644
119216,What is the expectations for runtime duration of SVC assignment code. Is there any runtime threshold that we need to take care when running models on this MNIST dataset as they are taking a lot of time.,306248
120782,nan,314197
120681,nan,301114
119669,nan,305847
120668,I am coming across many discussions where we are being told to split the 20% of the training data into train and test. But will this not clash with what is asked of us in the question? That we have to use test.csv to test the data? Can someone please help me with this query?,310472
120650,"Here, My understanding is as follows, 1. Extract 20% data from given data 2. train test split on this data having 20% for training &amp; 80% for testing Am I right? Or there&#39;s another way, please write in the comment section.",315423
120930,I'm confused how to submit our csv file for MNIST kaggle problem. Can someone guide the steps so that we can follow?,318429
120519,"We can add n_jobs = -1 to utilise all CPU cores to speed up the GridSearchCV modelling step. If I know what kind of evironments are used then I might set it n_jobs=3 for quad core leaving one so the system does not gets hanged or n_jobs = 7 for quad core with hyperthreading. Any information on this would be helpful. Although I have used only 5000 samples, 4 Cs and 4 gammas &amp; CV=5 as advised so my execution time is not very high. This makes 4*4*5 = 80 model combinations which really takes time. Also, Whether TAs will modify the n_jobs parameter to run it faster. I am expecting execution time is not graded but still it will be better for TAs if execution takes 30 mins or 3 mins. So I thought I would just ask here.",313515
119739,"The GridSearch CV is taking lot of time even if I use just 20% of the sample data be it any Linear, poly or rbf ? Can you please suggest if I am missing something to speed up this computation ?",310501
121000,,308495
120001,Can we upload the files to google drive and then read the files?,311254
120004,"When i execute grid search, its taking lot of time. Shows kerner busy symbol in jupyter, how to stop this. Is there any way to stop ?",312019
119218,What is the expectations for runtime duration of SVC assignment code. Is there any runtime threshold that we need to take care when running models on this MNIST dataset as they are taking a lot of time.,306248
120125,"I have divided train and test into 20 -80%, on the train set, I have again divided to 70-30 sub_train data. When performing GridSearch, kernel is running for more than an hour.",318780
118497,"&quot;Since this is a non-graded assignment, you can evaluate your solution yourself using the rubrics provided on the next page.&quot; What exactly does it mean?",310511
119768,nan,300687
120435,nan,315560
120732,nan,315560
119336,"I am finding the data to be clean. However, there are 15% marks allocated to cleaning. Is it just the routine checks for data sanity that is required in this case?",312731
121050,how do we import only 20% of data from the train.csv ? is there any code for that?,320606
119809,How to load MNIST data in Python?,317073
119197,"hi, as mentioned in other questions on this topic, executing GridSearchCV for this problem is very heavy. Some friends (including TA) have suggested to use Colab With GPU to do the coding. But even in that, it is taking too much time. is it okay if we create and run multiple models ourselves with different hyperparemteres values and check accuracy of each to decide the best model? Yes, some code blocks will get repeated multple times but if we don't go for too many values for each hyperparameter(maybe 3 each), it'd be much faster then what we are doing with GridSearchCV. Requesting some TA to respond asap.",311686
120449,"train.csv has 42k, as per the problem we can do 10%, 4.2k obervations test.csv has 28k. so we should take 2.8k observations ? we should scale and do prediction ? But do we have actual prediction data for this test.csv ? to see the test accuracy",312019
120780,Can anyone tell me how can we use test.csv file to predict accuracy?,318448
120062,nan,301114
119836,"If we have an image of our own in Jpeg, png or bmp format, how can we build the numerical data? I mean hown can we assign values to all the columns in the csv file",301113
120249,"Once we do gridsearch and hypertuning, we get optimal values of gamma and C We get corresponding best accuracy score. After this we build final model where we try to predict the value of y In this step, we have to use test.csv Is my understanding correct?",308437
120461,,315560
120258,"Does this assignment really require us to use GPU or colab? TA hasnt mentioned to use this anywhere. I tried with k=5, c from 1 to 100, gamma = 3 values 20 mins so far and still running !!",308437
120259,nan,300687
119860,"We take the 20 % of data ( 42000 rows) and then this 8400 rows we take as our data and then do the further spilt for train and test data ( 70:30) ratio , is my understanding correct ?",318814
120581,Does it mean that we have to use original file (42000 rows) and split it into train and test data into 20% and 80%? Or does it mean that we have to use only 20% of data as the complete dataset and then use 70% of the &quot;selected dataset&quot; further for training and 30% of &quot;selected datset&quot; for testing? Can someone clarify?,312942
120244,"As in the instructions, i used 20% of input dataset i.e., around 8400 samples and proceeded with train and test split. User linear type of kernel and got 91% accuracy. Does this mean the separator (classifier) in this case is actually linear? With 91% accuracy i guess the model will be quite simple",308437
119870,"After reviewing the Dataset, was unable to figure out any data anomalies to be cleaned. no Null values, column names looked ok, all numeric values, so was unable to figure out any data cleanning needs for this asignment. Any further guidance on this would be appreciated.",316036
120270,No module named &#39;mahotas&#39; error while running the Jupyter NB &gt; pip install mahotas lead to following error &gt; Failed building wheel for mahotas on Windows system.. Please suggest,310508
120657,"Regarding back ground data running and percentage completion information during GridSearchCV execution fit. Is anyway possible to get this running info kind and to know how much % it completed. If its running 60 fits, how many are completed information .etc.,",312019
121113,"My laptop is so old that it is not performing good for grid search. The code didnt even executed even after 18hours with 20% train data and 3 each parameters for c and lambda (yup...i did that, with 100% cpu utilization...thankfully the motherboard is not fried). When I am trying on kaggle connection always breaks off with cloud and on google colab also waited for 90 minutes but the data set is not uploading. Totally frustrated now. what to do ?",318479
119658,"Test dataset does not have a response variable &quot;column name = label&quot;, so how do we test our SVM model?",310463
121125,"It is not related to assignment. Someone asked me in the office (not interview) today, I was not confident to give the reason of ""why"". Can you please help me get the answer ? In theory, they look equally good, but in reality, we need to pick the one from the two. How can we find the correct hyperplane to meet the requirement ? Q: See the graph below , which hyperplane is better .. and why ?",312479
118977,The problem statement says &quot;please use train.csv to train the model and test.csv to evaluate the results. &quot; How can we evaluate using test.csv? It doesnt contain the &#39;label&#39; column.,310511
119885,nan,318319
119886,I am not able to derive the correct confusion matrix. Its is not showing the 2X2 matrix although the data is scaled. The below error is popping up: ValueError: Target is multiclass but average=&#39;binary&#39;. Please choose another average setting.,314313
120909,"Since the data is pixel data, any hints for performing EDA? Because imputation will yeild a different result as well as dropping.",314084
120910,"As per understanding heatmap using correlation matrix tells about relationship between independent (feature) variables. But in the assignment there are 756 features, so is the heatmap or correlation matrix really required as a part of EDA in the assignment. ? Also the data provided is of image expressed in pixel value so does it make sense to plot heatmap for this.",317991
119525,"It is mentioned in the problem statement that ""Since the training dataset is quite large (42,000 labelled images), it would take a lot of time for training an SVM on the full MNIST data, so you can sub-sample the data for training (10-20% of the data should be enough to achieve decent accuracy). Also, running a GridSearchCV() may take hours if you use a large value of k (fold-CV) such as 10 and a wide range of hyperparameters; k = 5 should be sufficient."" Train data size = 42k As per the above statement, out of 42k data we can take any data of size 4.2k to 8.4k to train the model. Whole dataset need not be considered to train the data. Please confirm if the above statement is correct.",310467
118686,"Is there any performance benchmarks on this dataset? I have been trying to run on 20% data (~8400) with 3 alpha, gamma and 3 folds (using GridSearchCV), and the program never completes. How can this be addressed?",318438
120913,"I used 10% of data from train.csv for training purpose and on doing a grid search found optimal C and gamma for which accuracy was showing up as 93% in plots. However, there is no label column in the test.csv. How to find the accuracy of the y_pred on test data since there is no label present which can help identify the actual correct value?",315471
120184,nan,314678
120594,nan,313526
120535,nan,318358
120922,,315560
120539,nan,318429
120925,nan,315560
120263,"So if I understand it correctly, the train.csv should be used as a dataset and train_test_split, we have to split 20% a,d 80%. Or prepare a seperate CSV using only 20% of train.csv file and then use that as dataset and do the train_test_split?",307710
120522,Should we be comparing K Fold mean train score with the final (X_test) test score ? Or should it be between K Fold mean test score and final (X_test) test score ?,310467
119987,when i do correlation values table display. many shows NaaN Bit confused here,312019
120764,Anybody getting this error ?ValueError: Target is multiclass but average='binary'. Please choose another average setting.,314197
120940,nan,312502
118629,Usually till now what we have done is break up the given data set into train and test data. But here the requirement says &quot;please use train.csv to train the model and test.csv to evaluate the results.&quot; It also says &quot;sub-sample the data for training (10-20% of the data should be enough to achieve decent accuracy&quot;. Does it mean we use 10-20% data from train.csv to train the model and then use test.csv for testing? Just wanted to confirm the understanding since its a different approach this time.,310511
118805,"Executing GridSearchCV() with k cross validation is becoming computationally very intense. As per the suggestion in problem statement, following were my inputs: 1. k = 5 2. gamma = [1e-2, 1e-3] 3. &#39;C&#39; = [10, 100, 1000] 4. size of training set = 20% of original 5. Also did PCA for dimensionality reduction from 784 features to 283 PCs Still the code is running for ever. Is there a way out? Any help is appreciated.",310511
119956,"In the instructions, it is mentioned that, due to the large size of the training dataset, we can sub-sample the data for training with 10-20% of the dataset. Is there a specific method to be used to attain the sub-sample?",319302
119656,The submission box mentions that there is a PDF file required to be submitted. But what exactly should the PDF file contain? There's no mention of it in the Evaluation Rubric. Please clarify what exactly has to be submitted.,310505
120949,nan,314818
120954,"At present, i am getting 94% of accuracy on 20% of the data. There are many samples are skewed. Do we need to de-skew the sample and perform SVM on it ? I am sure, it will improve the accuracy, but do we need to do that ?",312479
120956,nan,308495
120338,"Need a confirmation from TA : Code submission page has this "" Please make sure that you are not changing any of the file names that have been provided for download. The code that you are submitting should run at our end on the same files without any modification of the code."" Now on the first page, it is written that one should go with subset 20% of data for training, Obviouly the 20% sample file that I select will not original file naming and would differ, and on top of it, for getting better accuracy, I will have picked up even distribution across all digits. So, 1) Do I include the 20% training sample with whatever name of my choice (say train_20pct.csv) along with the code? Note that 20% sample with evenly distributed rows that I select to train will be different from the file that TA might select for evaluation, so accuracy that I comment out in Jupyter workbook may hold incorrect because training file would be different. hence the question 2) There is a mention of ""PDF"", do we also have to get a ppt ready for this assignment?",309211
120139,"Code: # Evaluate the model using confusion matrix from sklearn import metrics metrics.confusion_matrix(y_true=y_test, y_pred=y_pred) Output: array([[787, 0, 3, 1, 1, 1, 7, 3, 3, 0], [ 0, 927, 3, 1, 1, 1, 1, 2, 0, 1], [ 2, 2, 781, 8, 8, 1, 3, 13, 5, 1], [ 0, 3, 9, 851, 0, 7, 0, 13, 11, 4], [ 0, 3, 6, 0, 767, 1, 1, 3, 1, 20], [ 1, 1, 2, 23, 0, 704, 5, 2, 10, 3], [ 6, 0, 5, 1, 3, 8, 785, 7, 1, 0], [ 1, 2, 3, 3, 4, 1, 0, 877, 0, 12], [ 1, 5, 5, 6, 1, 5, 2, 5, 742, 1], [ 5, 2, 7, 10, 9, 2, 0, 15, 4, 836]], dtype=int64)",308639
120928,Also do we need to use test.csv in this assignment?,318429
120959,"""K-Fold"" cross validation takes more time , even if i have added the parameter n_jobs=-1 in grid search .please suggest how to make it fast and also how to monitor the progress.",307018
119626,nan,305335
120600,"Fitting 5 folds for each of 12 candidates, totalling 60 fits Can some one explain what is the actual meaning input as kfolds=5 Gridsearch with two variables with 3 values of each variables. nc2 problems ??",312019
119665,"As per my understanding EDA or data clean up would be carried out with the full Train data. Before training the data for SVM, only 10-20% of data should be selected to avoid high computational risk. Is my understanding is correct?",307494
122032,nan,314818
120809,Do we need to plot any graphs/charts before performing model building on the dataset ?,301655
120430,"I have one data with one col second data frame with n col Rows are same When i do df_test2 = d1.join(df_test1) All second df values will become NaaN for df_test2 What could be the reason. When i use concat also same issue, here rows getting increased Bit confused. some one help me , what could be the issue of NaaN",312019
120262,"I am not understanding one thing. In the SVM module for letter recognition there were 2000 rows and there 5 folds and a wide range was used for C and gamma too. There it was a 26 class problem with 60 fits in the Gridsearch. Execution time was 10 mins !! Here in this assignment am using 8400 rows (after training split it comes to 5000 rows approx) 10 class problem with 27 fits (k=3,C=3,gamma=3) Why the execution time so high??? TA can you pls address this query - Thanks",308437
120278,"Do we need to perform image processing (deskewing the image, re-center of image etc) as explained by Mr. Ankit Dixit in one of the Live Session into this handwritten digit recognition assignment. ? TA's can please verify.",317991
121051,how do we import only 20% of data from the train.csv ? is there any code for that?,320606
119824,"Given test data doesn't contain label, so how we can calculate accuracy on test data?",317073
120050,"In other words, should we treat the label as object or an integer number. The reason I am asking this is because I am not sure if the predicted value is expected to give a numeric result.",311729
119869,I am using the following commands to import csv files to colab from my local. from google.colab import files uploaded = files.upload() Its taking a long time to import. Do we have any other ways to import?,310467
120247,"I used following approach. digit=digit.loc[0:8399,:] --this is to extract only 20% of input dataset values X_scaled = scale(X)--- have not scaled target variable X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.3, random_state = 101) -----here i have set test size=0.3, train size=0.7 This is OK? Because from here actual model building starts and which is a very critical step",308437
120921,Is it allowed to choose a different value of gamma to build a final model ?,312259
120693,It is mentioned in Problem statement that we need to evaluate result using test.csv file. Please help!!,311004
120689,"As execution is taking too much time, is there any provision so that we can see how much progress made till now?",315423
120926,nan,303085
120995,I was surprised to read that we need to produce a heatmap (two questions in the last 14 hrs). TA mentioned that we need to produce a heatmap as SVM is very sensitive to correlated features. So I want to understand again - we are supposed to create a heatmap that shows approx 800 x 800 cell matrix. To show correlation between approx 800 features? (and not to draw predicted vs actual digits) I have done that and I can't make head or tails from such a heatmap .. so that is why am surprised and wanted to triple confirm. Nothing of this is ofcourse explicitly asked in the problem statement so I suppose it comes under 'EDA' step.,300694
121041,"On submission page it is written ""You have to upload a zip file containing one Jupyter notebook."" Since we have to upload a single jupyter notebook file, do we still need to zip that and upload? TA's can help.",317991
100197,Where can i find the Solutions for Assingments and Case Studies from upgrad. To cross check with our Assginment and Case Study answers with actual answers.,312019
90919,nan,306996
90922,nan,306996
90920,"Convert the unit of the budget and gross columns from $ to million $ . Can someone explain what is been asked over here, I&#39;m not able to understand &quot; $ to million $ &quot; with the numbers given in the file, a bit confused.",306010
90926,nan,314678
92440,"Hi, On step3: Use the table created in Part(1) to generate buy and sell signal. Store this in another table named 'bajaj2'. Do we need to use "" Hold "" signal also, when both lower or higher MA are equal.",318319
90159,nan,306250
90390,nan,319444
90167,nan,300733
90929,nan,318788
90168,nan,300718
90935,a=b.groupby('c').somevalue.mean() a c (NO name a=here ? how to assign?) xxxxxx value1 eeeeeee value 2 zzzzzzzz value 3,318340
90171,nan,300733
90943,"Did any face issue in 2.2 in droping column, i can see in movies dataframe the column which i am trying to delete but when ran the command to drop then it gave error KeyError: ""labels ['color'] not contained in axis"" i did used axis=1, which should be right.",306005
90329,"I imported a data set into a data frame. Now, I want to delete some rows based on a condition in a column. I am dropping the rows using drop method. e.g dataframe1 = dataframe1..dropna(axis=0, how='any', subset=('column1','column2')) As you can see, I am providing column names. Is there a way to filter the column names based on a condition ie columns which have more than 90% null values and then delete the rows.",301643
90324,nan,300718
90165,nan,308495
90877,There seems the actor name is incorrect in the task.,304319
90646,nan,317070
90638,nan,318804
90142,"df1 is dataframe of length 11, df2 is another dataframe of length 21 and df3 is the third dataframe of length 17.",310419
90403,"Did something like this movies.dropna(axis=0, how=&quot;all&quot;, subset=[&#39;&#39;,&#39;&#39;], inplace=True) row count reduced to 3891 from 5043 from here on when I am searching for isnull on rows I see only 7 rows. Is it same for all or I am doing anything worng?",301118
90854,"In Python DS asignment, 3.4 , I could sort by imdb_score, but having difficulty adding the rank column with df.rank() function after sorting movie DF for the 250 top movies. The ranking is not occurring from 1 to 250 and column is adding at the rightmost end. How to position column Rank as first and rank values as 1 to 250 for the top 250 movies?",301644
90056,nan,300733
90663,nan,318455
90621,Difference between these four concat join merge append,317269
90655,nan,316891
90656,nan,317070
90120,"With regards to assignment submission, Is it possible to add print statements or other supporting comments in addition to the code instructions. Would this have any bearing on the evaluation?",318085
90218,Are we suppose to index as per IMDB and assign it to new DataFrame ?,306011
90670,nan,301111
90625,nan,318005
90626,nan,310467
90581,nan,318002
90639,"because, if that is the case then we will have to remove any other addtional print statment (that we may have put for our own checking purposes) in various place? so, Should we remove those additional statements? or, we can leave them as it is?",317998
90741,Calculated Profit= gross - budget Sorted desc and picked top 10 movies but still didn't found duplicates in movies,304693
90224,I want to whether if I have to submit the movies dataset along with Python Notebook by compressing in a folder or should I submit only python notebook without compressing.,311727
91020,nan,315856
91024,"Can I split a column in DF by using something like : df['col1','col2'] = df[['colx']].apply(lambda x: str.split('|') Will it add 2 new columns in DF with splitting the data from colx ?",319876
90717,nan,314756
90628,nan,305804
90629,nan,318846
90630,nan,318398
90062,"Subtask 1.2: Inspect the dataframe Inspect the dataframe&#39;s columns, shapes, variable types etc.",318429
90636,nan,318398
90616,,315560
90653,nan,318455
91041,nan,310472
90588,Which of the two should be preferred for adding 2 data frames,304319
90442,nan,300726
90447,nan,300687
91039,This is to do with the assignment on movies,301121
90221,I am confused about whether to include the change working directory command in the Jupyter notebook since the path to the dataset will be different for different computers.,311727
89903,"Hi, How to Identify a perticular film is ""foreign language films"" or not. I mean what is the criteria to identification?",311741
90659,nan,316041
90557,"I collect all the files, can I compress them together in a rar file and upload?",310419
90593,nan,317982
90594,Visualization of n-dimensional numpy array is not clear and how it becomes more beneficial in large dataset. Can you put some light on it. A real world example will help.,315028
90595,Is it important to use the exact way expected to solve any problem in the assigment. I see there are many ways that leads to the solution.Apart from the no of lines of code do we need to check for any performance parameters as well? Does it decide the score?,315028
90560,nan,307708
90666,nan,318455
90607,nan,318005
90611,"using .loc for Top_Foreign_Lang_Film am trying to find ['movie_title'] == 'Veer-Zaara' row ,but it returns an empty data set, but when I inspect the df,it has that particular row. Same syntax has worked for different dfs in the assignment,confused why?",301115
90612,nan,309211
91074,"# Write your code for dropping the colu_mns here. It is advised to keep inspecting the dataframe after each set of operations movies = movies.drop(columns=[&#39;color&#39;,&#39;director_facebook_likes&#39;,&#39;actor_1_facebook_likes&#39;,&#39;actor_1_facebook_likes&#39;,&#39;actor_2_facebook_likes&#39;,&#39;actor_3_facebook_likes&#39;,&#39;actor_2_name&#39;,&#39;cast_total_facebook_likes&#39;,&#39;actor_3_name&#39;,&#39;duration&#39;,&#39;facenumber_in_poster&#39;,&#39;content_rating&#39;,&#39;country&#39;,&#39;movie_imdb_link&#39;,&#39;aspect_ratio&#39;,&#39;plot_keywords&#39;],axis=1) print(movies.shape) round(100 * (movies.isnull().sum(axis = 0)/len(movies.index)),2)",303115
90615,"In some case we use isnull() and another we use isnan(). But how can we decide when to use which function. Also, when we use isnan() why we use syntax as np.isnan(). Could you please explain these 2 in details.",320103
90722,Should we be dropiing columns one by one or all together?,300748
89938,Do we drop both budget and gross? or only budget?,314547
90723,nan,306996
90721,nan,306996
90724,nan,306996
90725,nan,311004
90726,nan,306996
90727,nan,306996
90822,nan,310624
89934,I am little uncertian on the ipynb file submission for the assignment. Is this solutions are validated manullay or through some automated way? The reason i am asking is just to understand wether i need to put any print statement to print the results in each subsection so that automated evaluation framework pics the result from printed results! Thanks!,311741
90956,nan,307018
90633,nan,320073
90598,"For certain rows, values in all the columns are the same, except for one insignificant column. Should such rows be considered as duplicates or not?",313826
89022,nan,300690
90250,,308639
90608,Sub Task 3.5 - While changing the unit of budget &amp; gross columns from $ to million $ . do we need to overwrite the existing budget &amp; gross columns (for millions) or create altogether new columns ?. Please confirm.,311115
89611,"Hi everyone, I've a doubt regarding graded assignment questions for Python: Data Science, they've provided us a IPYNB file which works only in Jupyter Notebook, but I am not flexible working with it and if I need some other kind of resource for the same assignment, then how can I get it ?",301655
90103,nan,312518
90816,nan,309452
90966,do.we have to submit or answers in script file provided in assignment page or need to make new file for solutions,307496
90830,I understand that we need to drop the duplicate values of the profit column.Pls advise,300687
90973,nan,300729
90976,nan,300729
90627,"If i use this syntax df[df['fruit'].isnull()]='mango' , it's changing the data type of the remaining columns i.e,columns which are float is changing to object type... while if use this synatx df.loc[pd.isnull(df['fruit']),'fruit'] = 'mango' , datatype of other columns are not changing in the data frame. What's happening here, Please explain?",318013
90622,"hi ,when i use .loc to set the value for a particular row .i get warning like using .loc[row_index,col_indexer] = value instead , even i use this then also i get warning but the values get set means i get the result what i want but still it give me warning",303674
90845,Subtask,309452
90847,"Hi Can any one provide a hint solving 2.3, droping rows using columns with high Null percentage. I am very much confused.",317410
90960,"struggling for the last 4 hours on this topic. beja fry. don't need code, any ideas are welcome",312199
90748,please explain or give me some hint not able to understand the question,300687
90963,"TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'' Any idea about this error. Even I googled the same but couldnt get much info there",312093
90962,nan,307495
90751,nan,306737
90753,"so do i need to calculate the percentage left over rows with the total number 5042 Length: 3887, dtype: int64 if i divide 5042/3887*100 i get 77percent .As i going in the right path.please advise",300687
89640,"In the assignment for subtask 2.4, I am not getting any rows after completing subtask 2.3. Since the previous subtask already removed all the rows having more than 5 NaN values. len(movies[movies.isnull().sum(axis=1) &gt; 5].index) is fetching me 0. Can I write a comment that no such rows present ? Or am I doing something wrong ?",318756
90632,"In subtask 3.5, for both the questions, it is just mentioned the dataframe. So here we are supposed to use movies dataframe or recently created imdb dataframe?",317984
90647,"In the 3rd question of Task 3.7, it is asked to perform a groupby operation. But, groupby operations dont do anything UNTIL an aggregation is performed. So, for that question, We DO NOT have to print any output, right? or, something has to be printed for that question? please clarify.",317998
89841,nan,307494
90776,nan,300690
90654,"Hi, i am a beginner to programming ,found it difficult to learn comprehension,even after watching video thrice i didnt understand. can u give me a proper explanation or link where i can get it. a newbie to python. thanks.",305847
92490,,320251
90794,"xy[z['b'] = xyz['b'].apply(lambda x:round((x/1000000),1)) it shows all values as zero when i try to round the values xy[z['b'] = xyz['b'].apply(lambda x:x/1000000)-it works fine",300687
90792,"After sub task 2.3, did anyone have null values ""&gt;5"" for sub task 2.4",312199
90798,nan,300687
90357,"If 5 movies have the rating of 8.0, how should the rank be assigned to all those 5 movies.",306725
90775,nan,300690
90805,I haven't been able to start executing from where i left. I am coming across an error. It seems I need to execute again from start. Please correct me if I am wrong or someone faced similar issue and how you resolved it?,305843
89881,"What to it meant by &quot;Convert the unit of the budget and gross columns from $ to million $ .&quot;? Suppose i have a number 237000000, what should be the output after conversion? Is it 237?",311741
89411,nan,306038
90035,nan,318723
89418,nan,318780
90030,,308639
90289,nan,300726
89885,I am getting type error while comparing with 5% of Null Values. Any hints? Don't provide the solution as the question falls under assignment.,314547
90421,"I have been able to successfully remove duplicate rows from movies dataframe, from 3891 rows to 3856 rows using movies.drop_duplicates. Yet the top10 data frame has the same rows. row number 17 and 794 even seems to be duplicated, doesn't get removed.",313676
90652,"Hi, I didn't understand this concept clearly. need help in detail explanation and clarity with examples. thanks",305847
90231,nan,308495
90419,nan,300727
90495,"cols = [1069, 1068, 1067] df = df.drop(df.columns[cols], axis=1)",300687
91987,"Hi All, Is there any undo or rollback kind of function for dataframes of Pandas? I have dropped some rows by mistake now I need to get them back! Or may be a function which will undo the last operation on a dataframe.",316255
90565,"currently in my submission, I have a minimal amount of print(df) statements. So for example when question says: Subtask 3.7: Find the critic-favorite and audience-favorite actors Create three new dataframes namely, Meryl_Streep , Leo_Caprio , and Brad_Pitt which contain the movies in which the actors: &#39;Meryl Streep&#39;, &#39;Leonardo DiCaprio&#39;, and &#39;Brad Pitt&#39; are the lead actors. Use only the actor_1_name column for extraction. Also, make sure that you use the names &#39;Meryl Streep&#39;, &#39;Leonardo DiCaprio&#39;, and &#39;Brad Pitt&#39; for the said extraction. Append the rows of all these dataframes and store them in a new dataframe named Combined . Group the combined dataframe using the actor_1_name column. Find the mean of the num_critic_for_reviews and num_user_for_review and identify the actors which have the highest mean. I just do Meryl_Streep= Brad_Pitt = .. Combined= .. ... num_critic_for_reviews = Combined_grp...mean() num_user_for_review =Combined_grp...mean() Do I leave it like that or do I need to create print statements all over my ipynb file like: print(Brad_Pitt) print(Combined) print( num_critic_for_reviews ) etc",300694
91993,nan,320603
90591,"Apart from that they belong to different libraries, how are the 2 different in terms of their utility? Pls explain with the help of an example.",304319
90600,"can we get cheat sheet of numpy and pandas, because so many methods are there, for remember all we need one cheat sheet on which all methods are there with short explanation. It will be helpful for us until we are used to in the sence off real world.",318319
90599,On what scenarios we should use cancat over appends. Since in many cases we can use append in case of concat.Can you give certain example where we will prefer using concat over append and vice-versa.,315028
90609,"1) Do we write a generic code to remove rows with null values in columns with higher null values or do we drop specific rows based on inspection of percentage output. 2) After doing sub task 2.3, I don't have any unnecessary rows (rows with more than 5 nan values), do i need to still write the code for 2.4 (I get 77% rows simply after 2.3)",319866
90620,"In subtask 3.4 we are asked to create a rank column from 1 to 250. So here, we have to use rank() function- this way movie with same score will get same rank Or we need to just create a rank column in sorted dataframe and assign value to it from 1 to 250?",317984
90650,nan,317982
90649,Do we need to add Markdown cell with approach explanation while solving assignment problems,319869
90645,nan,314547
90651,nan,316041
90580,,315560
90658,Can't we create another DataFrame with arange function to create 1to 250 and then concat with Existing DataFrame with axis=1 I tried but getting multiple NAN value and rows increased from 250 to 475 . please help,319869
90610,AttributeError : 'NoneType' object has no attribute 'loc',318429
90617,nan,318005
90662,nan,316349
90605,"The Task says to Check the number of Retaied Rows. However, the description says to calclulate the percentage of the remaning ROWS as well. Do we need to calculate the Percentage or is it just enough to populate the number rows retained ? If we need to know the percentage how do we do.. on what condition can we perform the calclaution ? &quot;Check the number and percentage of the rows retained after completing all the tasks above&quot;",300727
90669,nan,316041
90641,"Hello, My question is regarding gradation. A problem can have multiple solution and the logic could vary from person to person. While grading the assignments do you check for an optimal solution, or any solution which is syntactically correct and not hardcoded will hold good as long as it gives result,",318002
90644,How to sort based on one column and then storing the dataframe as a new dataframe?,301644
92008,Anyone aware about this error??,315277
90637,Do we need to find column names with highest percentage and input it to the not null function or hard coding of column names which have highest percentage will suffice ?,305652
92000,nan,320603
90631,In pythom assignment # Write code for repeating subtask 2 here What does that mean by the above comment under the 'Under Subtask 3.4: Find IMDb Top 250' section I did not understand what needs to be coded in ths window? Kindly help me,314092
90596,"1. Why am I getting <span style=""font-size:11.0pt;mso-bidi-font-size:9.0pt;line-height:115%;font-family: &quot;Helvetica&quot;,sans-serif;mso-fareast-font-family:Calibri;mso-fareast-theme-font: minor-latin;color:black;mso-ansi-language:EN-US;mso-fareast-language:EN-US; mso-bidi-language:AR-SA""> at the end of every movie name? Did I do something wrong in <span style=""font-size:10.0pt;mso-bidi-font-size:12.0pt; line-height:115%;font-family:&quot;Microsoft JhengHei&quot;,sans-serif;mso-bidi-font-family: &quot;Times New Roman&quot;;color:black;mso-ansi-language:EN-US;mso-fareast-language: EN-US;mso-bidi-language:AR-SA"">read_csv() <span style=""font-size:10.0pt; mso-bidi-font-size:12.0pt;line-height:115%;font-family:&quot;Times New Roman&quot;,serif; mso-fareast-font-family:Calibri;mso-fareast-theme-font:minor-latin;color:black; mso-ansi-language:EN-US;mso-fareast-language:EN-US;mso-bidi-language:AR-SA""> operation? 2.<span style=""font-size:11.0pt;line-height:115%;mso-fareast-font-family:&quot;Times New Roman&quot;; mso-bidi-font-family:&quot;Times New Roman&quot;""> In the code given below, only the result of 2 nd line gets printed out. In the code given below, the result of both lines will get printed, as expected. Why did the method stated above did not work? 3.Have we written this to suppress <span style=""font-size:10.0pt;mso-bidi-font-size:11.5pt; line-height:115%;font-family:&quot;Microsoft JhengHei&quot;,sans-serif;mso-bidi-font-family: &quot;Times New Roman&quot;;color:#242729;background:white;mso-ansi-language:EN-US; mso-fareast-language:EN-US;mso-bidi-language:AR-SA"">DeprecationWarning ? What are some other warnings that might have popped up?",301652
90640,How do we create a data frame of limited rows from an existing dataframe? Does it need to have all the columns or only the column based on which we are creating the dataframe?,311868
91018,top10director = pd.DataFrame(movies.groupby('director_name')) Error message : DataFrame constructor not properly called! What could be the issue ?,319876
90545,"Getting file not found error inspite of using , encoding = ""ISO-8859-1"" while reading the csv file for assignment. What is wrong with following code? movies = pd.read_csv(""desktop/MovieData.csv"", encoding = ""ISO-8859-1"")",301644
90648,nan,316041
90643,"In Subtask 3.3, It was mentioned as ""Drop the duplicate values from the dataframe and repeat `Subtask 3.2`"". But 3.2 task has, we were asked to perfrm the below. So which task should be repeated Create a new column called profit which contains the difference of the two columns: gross and budget . Sort the dataframe using the profit column as reference. Extract the top ten profiting movies in descending order and store them in a new dataframe - top10",318804
89670,nan,318433
91014,nan,307495
90322,"Subtask 3.6: Find popular genres You might have noticed the genres column in the dataframe with all the genres of the movies seperated by a pipe (|). Out of all the movie genres, the first two are most significant for any film. Extract the first two genres from the genres column and store them in two new columns: genre_1 and genre_2. Some of the movies might have only one genre. In such cases, extract the single genre into both the columns, i.e. for such movies the genre_2 will be the same as genre_1. Group the dataframe using genre_1 as the primary column and genre_2 as the secondary column. Find out the 5 most popular combo of genres by finding the mean of the gross values using the gross column and store them in a new dataframe named PopGenre.",307843
91236,"Python Data Science Assignment ...We can upload ZIP also right? Including CSV, Excel Sheets and Jupyter Notes all.",320008
90618,nan,307493
90894,",what is meant by grouping by primary and secondary column?",316416
90949,nan,315831
90904,"To verify solution with checkpoint 3, I couldn't find movie_title = 'Veer-Zaara' in my dataframe. To investigate issue I just try to extract 'Veer-Zaara' movie as shown in screenshot but I could not find it. Is anyone facing simmilar issue of movie_title = 'Veer-Zaara' not appearing in Top_Foreign_Lang_Film ?",317991
90905,What if ranking of IMDB top was done according to two columns? Lets say the IMDB rating and the no of votes? Would one lose marks if the output did not correspond to the actual IMDB rankings? Should I just let the duplicate rankings be or assign the rankings according to the index instead?,319357
90951,nan,306735
90927,"I am using below code to to extract num_voters&gt;25000 and sorting imdb_score by descending order. movies.loc[(movies.num_voted_users &gt; 25000) &amp; movies.sort_values(by='imdb_score', ascending = False), :] but it is failing? Please correct me.",305843
91259,I Download the &lsquo;Movie Assignment.ipynb&rsquo; file . Where do I type in the code and how to submit it,312892
90664,How to extract from this file IMDb_Top_250' can anyone suggest??,301111
90671,"When i am trying to Assign 'English' value to required rows for language column, I am getting below error message:- File ""&lt;ipython-input-103-4fe0d3bd7613&gt;"", line 3 movies.loc(Code)='English' ^ SyntaxError: can't assign to function call",320103
90675,Please explain in brief about Task 2.3 in Python for Data Science Assignment. Exactly which rows needed to be deleted.If possible with some example.,308636
92474,"in 3 part of assignment, generate ""Buy"" or ""Sell"" signal we need to use Hold after Buy or Sell, Signal = BUY on the day when 20 DMA &gt; 50 DMA. Till next few days, 20 DMA will be still be more than 50 DMA, Signal has to be HOLD. Similarly Signal = SELL on the day when 20 DMA &lt; 50 DMA and HOLD for the rest of the days untill 50 DMA &gt; 20 DMA. Could you please confirm ASAP.",318319
91088,"On selecting the top 250 based on IMDB Score, I noticed that a few records with IMDB score of 7.9 got included in 250, while a few with 7.9 score did not. Shouldn't we again sort all 7.9 records on another value (lets say user votes on the movie) to have the more relevant ones from 7.9 picked in the 250?",317144
92522,"3.6. Find popular genres All the tasks were performed correctly. Partial marks have been provided as you didn't impute the null values in 'genre_2'. because of this they reduced 15 marks. But in my dataframe doesn't contain any null values on ""genre_2"" following is the output for it. What should i have to do.",318461
92945,"in 3.3 where we were suppose to drop duplicates i did this top10.drop_duplicates() movies.drop_duplicates() movies.sort_values(by='profit_in_millions',ascending=False, inplace=True) top10 = movies.nlargest(10, 'profit_in_millions') top10 but they are saying the code to drop duplicate is not correct. what is wrong? in 3.4 and 3.6 they say partially correct. when i checked with the sample solution it is word by word same just that i made the Rank column my index and they haven't. does that make my dataframe wrong? isn't thatdata cleaning? please reply TA. i want to ask for a re-evaluation if my code is correct.",302738
134042,nan,314629
133169,"Once, the execution of task is finished and all the results are back with the driver program, how driver program is able to directly contact the developer machine which initially was sending it&#39;s request through YARN resouce manager?",303085
133312,nan,320687
132441,"it is mentioned in the lecture that Spark uses in memory processing and thus is faster than Map Reduce. But if Spark is run on local machines, how can in memory be fast enough to process such high volumes of data? also, how can local machines manage such high volume of data?",310509
132558,I want to know the actual difference between Batch and stream processing. and how the stream works?,315423
138142,nan,311466
134184,nan,311466
133074,"https://learn.upgrad.com/v/course/208/session/37076/segment/200878 Under this section where SparkSession is called and then later merged with rdd, whats the signification in doing so ?",312259
134229,nan,318429
133076,Please help on this error on the first step itself!,316349
133045,My lab screenshot. data science with python not available instructors screenshot,318479
132963,nan,302877
136381,nan,320687
134860,"I check the path in Hive and then added same location in python df=spark.read.format(&quot;csv&quot;).option(&quot;header&quot;,&quot;true&quot;).load(&quot;/common_folder/heart.csv&quot; The currently active SparkContext was created at: (No active SparkContext.) at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100) at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1486) at org.apache.spark.sql.execution.datasources.text.TextFileFormat.buildReader(TextFileFormat.scala:106) at org.apache.spark.sql.execution.datasources.FileFormat$class.buildReaderWithPartitionValues(FileFormat.scala:129) at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:165) at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:312) at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:310) at org.apache.spark.sql.execution.FileSourceScanExec.inputRDDs(DataSourceScanExec.scala:330) at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:121) at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131) at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127) at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339) at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3384) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2545) at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365) at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364) at org.apache.spark.sql.Dataset.head(Dataset.scala:2545) at org.apache.spark.sql.Dataset.take(Dataset.scala:2759) at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:232) at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:68) at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180) at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180) at scala.Option.orElse(Option.scala:289) at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373) at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748)",310179
132035,"I am not able to understand what exactly does the DAG scheduler and Task scheduler do? Also,within the DAG scheduler,what exactly is the purpose of Shuffle map task and Result task? Let me know if there are some resources to intuitively understand these concepts.",318397
132131,nan,300718
133119,df.describe(['review_length']).show() +-------+-------------+ |summary|review_length| +-------+-------------+ | count| 551682| | mean| 0.0| | stddev| 0.0| | min| 0| | max| 0| +-------+-------------+ In [43]:,319759
133120,"Ihave noted that different names are using in the appName section when creating a Spark session such as &quot;PySpark dataframe and sql&quot; or &#39;Spark ML&quot;. Are these specific names to be used everytime or can we use any name of our choice? if specific, how to know which one to use when?",310509
132782,"review_length= spark.sql(&#39;SELECT helpful, overall, reviewText, reviewTime, summary, asin, LENGTH(reviewText) AS reviewLength FROM review&#39;) AnalysisException: &#39;Table or view not found: review; line 1 pos 104&#39; How do I resolve this?",319759
132789,"python code provided in the start of session, how to open it in notebook hub.?",318802
132796,"from pyspark.sql import SparkSession df.createOrReplaceTempView(&quot;reviews&quot;) review_length= spark.sql(&#39;SELECT helpful, overall, reviewText, reviewTime, summary, asin, LENGTH(reviewText) AS reviewLength FROM review&#39;) getting &quot;AttributeError: &#39;function&#39; object has no attribute &#39;sql&#39; &quot; for the above code. How do i resolve this",319759
132769,"I was trying to filter my dataframe using arrival and departure time using the following code: df2= df.filter((df.ArrDelay&gt;=0 &amp; df.DepDelay==0)) When I execute it, I get an error Py4JError: An error occurred while calling o80.and. Trace: py4j.Py4JException: Method and([class java.lang.Integer]) does not exist at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318) at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326) at py4j.Gateway.invoke(Gateway.java:274) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748) If I limit my code to a single condition (i.e. omit using the AND operator), the code works fine. Any idea on what is going on here and how to resolve it?",305653
133024,I got the following Spark connection isue while trying to initialize the spark session object: &quot;Exception: Java gateway process exited before sending its port numbe &quot; This issue is there in all the notebooks.,310511
132823,"review_length= spark.sql(&#39;SELECT helpful, overall, reviewText, reviewTime, summary, asin, LENGTH(reviewText) AS reviewLength FROM review&#39;) How to add this column to the original dataframe df ?",319759
132827,nan,308673
132854,In first question they asked for airport having highest avg arrival delay and we do query taking into considerations destination airport and arrival delay. In second question they asked for airport having highest avg departure delay and we should do query taking into considerations origin airport and departure delay. but in second questiin they asked to merely replace arrival delay with departure delay.,318005
132863,"After logging inCorestack platform I&#39;m tryin to access the Jupyter module.WHen I&#39;m trying to run the scripts provided inthe course ,the script stucks at the statement &quot;from pyspark.sql import SparkSession&quot;.AM I doing anything wrong or is there any other to run these scripts.",318386
132885,"Above code is runnning for almost 15-20 min, everytime aborting it manually. not able to set connection.",318802
132285,It has spark dataframe and pandas dataframe we should work on pandas dataframe or spark df Bit confused here. Again new api functions learning. pandas to spark df conversion I mean opposite of df_pandas=df.toPandas() ?,312019
132658,"df = spark.read.csv(&quot;/common_folder/Advertising.csv&quot;,header=True,sep=&quot;,&quot;); after creating df wanted to apply linear regresssion. How to make features and label data from from here could some one help and what is best apis and methods to fllow all steps like what we did with pandas and ml algorithms here with pySpark df bit confused to do ml models and any other operations.",312019
133101,"Not able to create Spark session since last more than 5 hrs. Tried closing all the Spark session, closing the running notebooks, tried logout and logging in again, still no luck. Tried restarting the kernel also, but the below command is keep in executing.",318448
133294,&quot;Remember that the file has the first row as the column name; don&#39;t forget to mention that in your code.&quot; How to account for this while reading the file?,312096
133299,"Hello, I am not able to establish the Pyspark kernel and getting the below error. I have already tried to restart and log out/log in but it is not resolving the problem. Please help.",310509
132811,"Hi all, Need some help debugging this: Thanks in advance!",318355
133493,,305335
132418,--------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) &lt;ipython-input-2-673bc5964142&gt; in &lt;module&gt;() ----&gt; 1 df = spark.read.load(&quot;/common_folder/airlines/data_2004-08.csv&quot;),319759
132383,from pyspark.sql import SparkSession spark = SparkSession.builder.appName(&quot;SparkML&quot;).getorCreate()--------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-3-9c892f5877f3&gt; in &lt;module&gt;() 1 from pyspark.sql import SparkSession ----&gt; 2 spark = SparkSession.builder.appName(&quot;SparkML&quot;).getorCreate() AttributeError: &#39;Builder&#39; object has no attribute &#39;getorCreate&#39;,302877
132421,"--------------------------------------------------------------------------- not able to rename the sparkdataframe column. in this airlines data first column is empty. trying to rename it TypeError Traceback (most recent call last) &lt;ipython-input-51-b9e9e83936e9&gt; in &lt;module&gt;() 1 df.withColumnRenamed( ----&gt; 2 df.columns(0), 3 &quot;SlNo&quot; 4 ); TypeError: &#39;list&#39; object is not callable",312019
134584,How can I see results after groupby function without any aggregated functions?,317073
132960,"While creating the spark session I am getting an execption Exception: Java gateway process exited before sending its port number from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(""PySpark DataFrame and Sql"") \ .getOrCreate() please help. Thanks",308965
132434,"from pyspark.sql.functions import min, max df.agg(min(&quot;age&quot;), max(&quot;age&quot;)).show() returns - Py4JJavaError Traceback (most recent call last) /usr/local/spark2.4/python/pyspark/sql/utils.py in deco(*a, **kw) 62 try: ---&gt; 63 return f(*a, **kw) 64 except py4j.protocol.Py4JJavaError as e:",319759
132973,"In Practice Analysis of airlines data, it is asked to group data with destination then sort. How to sort the data after grouping based on Destination column?",307495
132879,,314547
132984,nan,308434
132497,how to create data frame in spark from another data frame of spark for some columns in this dataframe.,312019
132675,"Have renamed the columns and tries to fit the model using linear regression from pyspark.ml.regression import LinearRegression lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8) print(lr.explainParams()) lrModel = lr.fit(df) It gives following error : IllegalArgumentException: &#39;Field &quot;features&quot; does not exist.\nAvailable fields: TV, Radio, Newspaper, Sales&#39;",319759
133689,"Presenter shows take as an example of action. It filters input set and picks first n elements, resulting in RDD. Then why is filter not an action? Is transformation, not a mutation of data, not just filtering?",308637
132633,"Now we need to bucket these age groups to four different buckets. The range will be defined by [29, 40, 50, 60, 70, 80]. You need to use the following function: How do we split the age in to 4 buckets with 6 range values - [29, 40, 50, 60, 70, 80] ? What will be the values of age in different ranges ?",319759
132992,The question reads &quot;check the fourth row where reviewLength = 29087&quot;. The review length in my fourth row (arranged in descending order) is not 29087. My value for the maximum review length (question1) is correct. Any idea what could be wrong?,305653
132640,"After fitting Linear Regression to the Advertisement data what is the intercept? What values we need to find here - is it MSE , RMSE,R-square, MAE or explained variance ?",319759
133393,nan,310533
132577,nan,307494
133304,I have run the linear regression model. How can I find out the intercept? Any Stackoverflow links will be helpful. Thankyou,312096
100939,"Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of  and  are controlled at 0.15 -Is it would be different sample size,standard deviation ?-Can explain the properly?",320689
100947,"Please provide any example, that would be great for understanding.",306243
100949,It simply states that the probability of drugs producing satisfactory result =4/5 and probability of drugs not producing satisfactory result =1/5 during the quality assurance of previous batch of products.,320689
100953,"b) You know that two types of errors can occur during hypothesis testing  namely Type-I and Type-II errors  whose probabilities are denoted by  and  respectively. For the current hypothesis test conditions (sample size, mean, and standard deviation), the value of  and  come out to 0.05 and 0.45 respectively. Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of  and  are controlled at 0.15 each. Explain under what conditions would either method be more preferred than the other.",301655
100957,"Now, once the batch has passed all the quality tests and is ready to be launched in the market, the marketing team needs to plan an effective online ad campaign for its existing subscribers. Two taglines were proposed for the campaign, and the team is currently divided on which option to use. Explain why and how A/B testing can be used to decide which option is more effective. Give a stepwise procedure for the test that needs to be conducted.",317600
101029,nan,300721
101034,We can submit the Statistics assignment in one single pdf?,307843
100132,"You know that two types of errors can occur during hypothesis testing  namely Type-I and Type-II errors  whose probabilities are denoted by  and  respectively. For the current hypothesis test conditions (sample size, mean, and standard deviation), the value of  and  come out to 0.05 and 0.45 respectively. Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of  and  are controlled at 0.15 each. Explain under what conditions would either method be more preferred than the other.",310419
99559,nan,311117
100425,What does the list out the three conditions that this distribution follows means ? please some one clarify ?,318814
100344,"Do we need to explain all the steps (which is performed in Excel or other tools) for the test , for example Two - mean or Two- sample test ? Need to understand what they mean by s tepwise procedure for the test that needs to be conducted.",300697
100956,"First the question: You know that two types of errors can occur during hypothesis testing  namely Type-I and Type-II errors  whose probabilities are denoted by  and  respectively. For the current hypothesis test conditions (sample size, mean, and standard deviation), the value of  and  come out to 0.05 and 0.45 respectively. Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of  and  are controlled at 0.15 each. Explain under what conditions would either method be more preferred than the other. Since many people still have a doubt regarding this question, here is a concise explanation of what you need to do. You are doing the hypothesis test using 2 sets of alpha and beta. In one case you have alpha =0.05 and beta=0.45. In the other case, you have alpha = 0.15 and beta=0.15 Your task is to explain a situation where either method would be more preferred than the other. So basically explain a situation where conducting the hypothesis test using alpha=0.05 and beta=0.45 is better than conducting the same test using alpha =0.15 and beta=0.15. Similarly, explain a situation where conducting the hypothesis test using alpha=0.15 and beta=0.15 would be better than conducting the same test using alpha=0.05 and beta=0.45 You don't have to bother about the sample conditions(sample size, standard deviation,etc.). Just use the values of the errors given to explain those situations.",313517
101113,nan,318579
101131,"Let&#39;s say you are a doctor and you approach the scene of a car accident. One victim is lying motionless on the road and you must assess whether the victim is dead or alive, and the victim will be treated accordingly. Based on this information which error (type I or type II) results in the costliest mistake? Let&#39;s try to figure it out. Null Hypothesis - The victim&#39;s status equals a living person. Alternative Hypothesis - The victim&#39;s status is not equivalent to a living person (i.e., victim is dead). Type I error - You reject the null hypothesis when the null hypothesis is actually true. Type II error - You fail to reject the null hypothesis when the alternative hypothesis is true. Cost of Type I error - You erroneously presume that the victim is dead, and do not give him the life-saving medical treatment. Cost of Type II error - You erroneously send a dead person to the hospital in an ambulance. We know that type I and type II errors are inversely related to each other, i.e., if we increase the probability of type I error type II error will decrease and vice-versa. Now, let&#39;s check the cost of errors for both the scenarios: 1) Type I error increases &amp; Type II error decreases If type I error increases, it means that there is more possibility that if an accident occurs you will presume that the victim is dead and do not provide the victim life-saving medical treatment. This is a very costly mistake/error because it is costing someone&rsquo;s life. At the same time in this scenario you want to decrease Type II error which means you want to be very sure that you are not taking a dead person to the hospital. 2) Type II error increases &amp; Type I error decreases If type II error increases, it means there is high possibility that you will take a dead person to the hospital. At the same time, you are decreasing the type I error which means you want to very sure before declaring a person to be dead and not giving him/her the necessary life-saving medical treatment.",318730
101137,nan,315423
101147,the drug only produces a satisfactory result when it's effect time is at most 200. Now Claim statement said that drug produces a satisfactory result (i.e. &lt;=200) &amp; pass QA test (boolean). Which One to use as claim statement for hypothesis.?,315423
100194,I am not able to understand the question list out the three conditions that this distribution follows. Please clarify.,312019
100218,"Can anyone help me to understand the significance of this sentence? If it has any significance to the problem, please help me to understand this sentence",311741
100262,nan,302744
100788,nan,318579
100871,nan,318435
99627,"b) You know that two types of errors can occur during hypothesis testing &mdash; namely Type-I and Type-II errors &mdash; whose probabilities are denoted by &alpha; and &beta; respectively. For the current hypothesis test conditions (sample size, mean, and standard deviation), the value of &alpha; and &beta; come out to 0.05 and 0.45 respectively. Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of &alpha; and &beta; are controlled at 0.15 each. Explain under what conditions would either method be more preferred than the other.",320689
99628,"Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of &alpha; and &beta; are controlled at 0.15 each. Explain under what conditions would either method be more preferred than the other.",320689
99584,"Hello All, can you give me some hint to understand the question better?",307494
100536,"So, here we need to explain using which condition is better 1st condition : alpha = 0.05 and beta = 0.45 (OR) 2nd condition : alpha = beta = 0.15 Only one condition should be high-lightened correct? or do we need to explain the other condition also?",312756
100271,"""Three conditions that this distribution follows?"" I didn't understand about which three conditions is asked to point out in this question. What conditions are they asking us to point out. Can anyone help me on this?",312756
100273,Is the word limit including the blank sheet of paper which we use for drawing figures or writing equations?,312756
100604,"Not able to understand Question 1,. Could some one help.",301641
100410,nan,308638
100341,"What we need to discuss in this? we use central limit theorem and we can solve the problem in the way they have been told us. like sample mean +/- Z* Standard Dev/ square root of sample size. State all the properties of ther equired method means, what we need to explain in that, that too in 100 words?, 50 will be completed for writing answer only.",317073
100575,In this case we just need to eplain about what A/B testing is and why is it used and stepwise procedure which should be performed while conducting A/B testing right? Is this would be general explanation about A/B testing or do we need to explain in this case centric (need to explain as per the given data),312756
100488,What is menat by either method? Are they refering to methods of testing or values of preferable &alpha; ? Please suggest.,311868
100591,"I am bit conufused above sentence. It means we needs to find range with both the sides .0,03/2. 0.015 one side and other side. I mean z value for 1-0.015 ?. Please clarify",312019
100618,"Explain under what conditions would either method be more preferred than the other. In the drug case, we can only prefer one method as the other would be quite unethical. Cant think of any scenario where higher alpha value can be accepted. Should we justify using any other case?",304319
100594,Are we using p value methods anywhere of this statistics assignment. Only Critical vluae method is enough ? Please clarify in general.,312019
100757,What does it mean by - the values of &alpha; and &beta; are controlled at 0.15 each.. is it the values of &alpha; and &beta; are now 0.15 each or the values are increased/decreased by 0.15 each.?,305129
100633,nan,306996
100795,I have doubt . Probability at most 4 means which is correct P(0)+P(1)+P(2)+P(3)+P(4) . or P(1)+P(2)+P(3)+P(4) Please help here.,312019
141282,"Getting the below error for MinMaxScaler ValueError: Input contains NaN, infinity or a value too large for dtype('float64'). How to fix this ?",314092
91362,Current as well as historical data can be stored in locally stored files in MS Excel easily if we dont look towards other factors!! Ref: Intro to Data Management --&gt; Module 6 --&gt; Session 1,318770
88607,nan,305654
87455,"Request /Suggestion: If answer is wrong after Submit , if provide us correct answer then that will help us to where we wrong and improve our self .",320008
85717,Hi I am completely in to Manual Testing and bit knowledge in technical skills -coding and data base ...could you/guide share any in this group where i can start before going to join/start in Main course. Any sites/books/videos to learn/idea on OOPS/SQL etc. Pls help. Thanks,320008
85741,There was a info mentioning that graded question would be made available after the cohort .. Has the graded question been displayed in anyone&#39;s dashboard for tableau.?,301114
95957,Q2 Plot a histogram to see where Virat Kohli has scored the most number of times. Changing the bin size in Tableau for better visualization Histograms. Example. The defaults value in the Size of bins filed can be customized accordingly. Thank you.,310508
95600,nan,300712
95980,nan,317412
88354,nan,318723
80289,nan,300748
84691,"Hello guys, for histogram binning example, i am droping only one measure to rows and after that clicking on show me. but i am not able to click on histogram sample, its not highlighted... please advise how to create a bin out of the salary??",305129
88499,nan,308967
86351,it is showing me just one screen.is everyone facing same issue?,318015
85326,nan,307494
87537,"Hi, I have an issue in accessing the Tableau worksheet provided in the course. It shows an error stating that the worksheet file is created using the latest version of Tableau. Check with the Tableau software to upgrade to newer version. But i have used the link provided in the course to download the Tableau accordingly. Can anyone help me whit this.",316202
84096,can anyone explain with the graph the above question ?.thanks in advance,300687
84095,my graph is attached herewith.as per my graph it shd be 91-100k can anyone help 1.how to manually change the bin width because in the feedback it says try to change manually the bin width 2.if possible pls share the graph,300687
87477,Box Plot - Median Report the job category closest to the median value of average balance for both no and yes types of response respectively.,318360
87570,,318344
87687,nan,320689
87688,nan,320689
88235,nan,318448
88242,,300733
95587,I am getting strange distribution when trying to create box plots using Avg balance and Respone rate. How to solve this?,310509
96147,"I find module 3, session 3 content insufficient to understand. Having expert session just to cover this topic in upcoming week, before we start more advanced topic would be helpful. Thanks! Even there are other people with same problem.. https://learn.upgrad.com/v/course/208/question/96127",318458
87268,"Is there a way to delete a column from imported data source in Tableau. For example, in the below screenshot, I did a split on Order ID column and so would like to remove this column completely as now I have 3 new columns ""Location"", ""Year"" and ""Order ID New"". For the third column, I want to use the same column name as the original one.",314730
87120,nan,318077
88594,"Getting below error [2204:2244][2018-09-30T15:11:47]e000: Error 0x8007010a: Process returned error: 0x10a [2204:2244][2018-09-30T15:11:47]e000: Error 0x8007010a: Failed to execute EXE package. [1BBC:2530][2018-09-30T15:11:47]e000: Error 0x8007010a: Failed to configure per-machine EXE package. [1BBC:2530][2018-09-30T15:11:47]i319: Applied execute package: VC2013Redist, result: 0x8007010a, restart: None [1BBC:2530][2018-09-30T15:11:47]e000: Error 0x8007010a: Failed to execute EXE package.",314159
84696,nan,314197
96355,Some online resource of Tableau: Free online training: http://www. tableausoftware.com/learn/ training Quick start guides: http://www. tableausoftware.com/support/ manuals/quickstart Visual examples: http://www. tableausoftware.com/learn/ gallery Knowledge Base: http://www. tableausoftware.com/support/ knowledge-base,310508
88484,where can I find a Tableu download for a 32 bit OS. the downloaded one is 64bit,318009
81490,"I have downloaded Tableau Desktop. However i dont see installation happening after opening the .dmg file. When I click on the mount, it gives error &quot;Tableau Desktop&quot; can&#39;t be opened because the original item can&#39;t be found . Unable to proceed with Installation. Can somebody help me with this. Warm Regards -- Rajesh R",300708
77909,Tableau prep course availability,300698
84927,I dont have any other version of Tableau installed on my machine still I'm getting below error,306011
88689,nan,313515
82446,I have installed tableau Desktop having 32 bit tableau version(8.1) with Windows 7 as a OS. But this tableau version is very different that the 64 bit latest versions . Is it possible to do all the assignments and practice with the 32 bit version ? I am concern since I see most of the tableau screens/buttons or functionalties are not supported in the 32 bit 8.1 version.Plz Suggest,312623
88586,Did any 1 recieve or needed tableau activation code for todays assignment .,300684
87752,nan,306996
95421,Please help in tableau to achieve below question creation of calculation for percentage difference. example. i have created chart with avg sal and month. now i want to do creation of calculation for prencetage difference. pls help,312019
91068,"My Tableau 15 days trial license is expired, when and how to get the license?",306731
86060,"I downloaded the 14 days trial free version of Tableau from the link given in the lecture (TableauDesktop-64bit-2018-2-0). While running this, it is showing failed running. Could anybody help me in this.. is there any problem of version or bit issue??",311117
86243,,306243
86977,I am not getting why the option 3 is correct. (When there is a common variable but the data belongs to different files belonging to different data sources.) Please explain.,317992
86917,I have a 32-bit system with windows 7.,312376
87388,"Cube have multidimensional view and link data from different tables of our main DB. what is the difference we have.. Can we do same kind of Cube analysis, Comaparison in Tbable also. comaparison of data from differnet sources.",318802
87489,The add connection and add data source tabs are shown below,318576
87563,I want to remove the complete row if any of the column has 'null' value. How can I achieve it in Tableau ?,312479
87502,"Dimension and Measures Given the data set below select the correct option: ID Age Gender Occupation Salary Date of Joining 141056 32 1 Public Sector 35000$ 21/03/2003 000879 49 0 Private Sector 69000$ 19/08/1993 243607 28 1 Public Sector 40000$ 15/07/2007 Answer : Dimension, Measure, Dimension, Dimension, Measure, Dimension",312479
87700,For the below data: How would you get a list of permanent employee numbers only? a. Custom split on '_' and custom split on '-' b. Split on '_' and custom split on '-' c. Custom split on '_' and split on '-' d. Split on '_' and split on '-' The answer is option c. I want to know how.,315471
87441,on Windows 8 64 bit,300727
87732,,306996
87760,nan,306996
86969,"So, I'm running the Ubuntu 18.04 LTS on my machine. Unfortunately, there is no version of Tableau Desktop for Ubuntu(or any desktop Linux OS). Is there any other way to work around this? I am aware of using Windows layers. However, I'm worried about how using a layer might affect the use of a license key. If anyone has an ideas, I'm all ears. Thank you!",306733
89352,nan,315661
88308,nan,315383
94164,How to analyze and work on the Drill Down set as Tableau reader is not providing the dimensions and measures provided at the left.,318372
95527,"What is relevance of factor -2,2 in standard deviation with samples in box plot? Why is it chnaged to -2,2 in video from -1,1?",318458
96426,MySQL Workbench &amp; Tableau have slowed down system performance significantly.,310508
95617,"While working on Tableau MCQ, I am finding it difficult to identify which columns to select after reading the question. Just looking at raw data from file, I doubt anyone can understand it, unless one is already familiar with the domain of the data. eg. Let's say Visualisation with Tableau MCQ with banking-marketing.csv What is the average balance for the those where the campaign outcome was successful? Which column the campaign outcome is? Is it capaign column? Is it pOutcome column? is it Response column? How would I know? Can you please provide data dictionary along with data set?",318458
94614,nan,311859
95279,I am confused how come there will be one answer of percentage diff between SALARY and AVG Salary in the month of july.,315679
95283,nan,300687
95289,"For those of us with Linux machines, there's no official Tableau desktop available. There are ways to use a windows application on Linux: Running a Virtual Machine or using a Windows application layer such as Wine. But, these methods often end up reducing the performance of the system. Also in course 3, there are only 2 modules which require Tableau. Considering these, is installing Tableau desktop really necessary? Or will a service like Tableau Online?",306733
95729,nan,320251
95743,"An error occurred while communicating with MySQL. Unable to connect to the server. Check that the server is running and that you have access privileges to the requested database. [MySQL][ODBC 5.3(w) Driver]Authentication plugin 'caching_sha2_password' cannot be loaded: dlopen(/usr/local/mysql/lib/plugin/caching_sha2_password.so, 2): image not found Unable to connect to the server ""192.168.10.30"". Check that the server is running and that you have access privileges to the requested database.",312491
95760,what is the difference between split and custom split in Tableau?,318350
98674,nan,311046
88560,"I was still unclear about dimension and measures in the tableau. I found this link helful. https://www.oreilly.com/library/view/tableau-data-visualization/9781849689786/ch01s09.html If someone needs it, please go through it.",301649
87766,"For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question, from BANK MARKETTING I got the answer as 8000 but I'm not able to distinguish the response as ""Yes"" and ""No"" in two different lines. After putting ""response"" in color, I got the something in the picture. Getting as 1 &amp; 0. I need to change it to YES &amp; NO, please help me out.",306010
88336,,318723
88428,"Hi All, I have gone through some answer for this question. I am using MAC. For most of the people I can see that the &quot; Response &quot; field is appearing under the &quot;Dimension&quot; category. The &quot;Response&quot; field is appearing under the &quot;measures&quot; category for me. I have tried to download file mutiple time but same issue is happening. what could be the issue here and where i am going wrong ?",320195
88438,nan,318335
88477,nan,316323
93748,nan,310008
88527,,308639
87501,,314313
82562,nan,304812
87558,nan,314313
83563,ue,300687
83596,nan,300687
86443,"For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. According to me, the answer is 4000 but the answer given is 8000. Any help will be appreciated.",314361
80782,"1. For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. Data Source: bank-marketing.csv (https://cdn.upgrad.com/UpGrad/temp/0f0e2afb-2b0b-4db9-8be4-0e2e59f836eb/bank-marketing.csv) 2. What exactly is the use of Attribute and Dimension option (from drop-down menu of a pill )? 3. What happens when we select a Measure pill as a Dimension (See picture: Salary selected as a Dimension , although it's a Measure )",301652
88816,I came across this informative piece and thought of sharing over the forum. This will be helpful in choosing appropriate data representation in various scenarios. .,300713
82801,nan,300690
87467,"I have an issue in using the worksheet provided for practice. E:g - Once i downloaded the Scatter plot worksheet, i am not able to open or use the file. I get a error message that the file version doesn&#39;t match with my Tableau version. Can anyone please help me on this.",316202
80903,"Unable to solve Scatter Plot question in Tableau, and also a few other queries. 1. For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. Data Source: bank-marketing.csv (https://cdn.upgrad.com/UpGrad/temp/0f0e2afb-2b0b-4db9-8be4-0e2e59f836eb/bank-marketing.csv) 2. What exactly is the use of Attribute and Dimension option (from drop-down menu of a pill )? 3. What happens when we select a Measure pill as a Dimension? (See Picture: Salary selected as a Dimension , although it's a Measure )",301652
86522,"P.S. 5 modules are assigned to me; Welcome to the pre-Launch pgm, Intro to Python, Data Analysis using SQL, Data Analysis in Excel &amp; Programming in R",312259
88881,"Use Bank Marketing dataset to answer the following qustions: For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question. 4000, 8000, 16000, 55000 I am unable to figure it out.",302739
89104,I tried this question and tried to use scatter plot for getting the answer. Which eventually i could not get. I saw solutions of other where Tableau uses Gantt plot and not scatter plot. Can this be solved by scatter plot?,318440
87699,nan,305847
87608,In which category of jobs do the people take maximum average duration to respond yes to the campaign. from the Bank Marketing dataset I need help on how to find the answer on tableau.,306010
87719,"Ho to solve following question in Statistics and Exploratory Data Analytics&gt;Module 1&gt;Session 2&gt;Visualising and Analysing Data in Tableau section: For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question.",301641
87764,While trying the scatter plot I get the following erro PFA the error and please let me know how to resolve it,312892
89344,nan,308442
88239,https://www.youtube.com/watch?v=tqS0gF_Ofpk&amp;t=0s&amp;list=PLRNrM8SIqgQY-LdF9T085Lmm96K9MWRNE&amp;index=22,318344
88314,Did anyone had faced any issues with the downloaded file (Bar Chart Worksheet) The file was downloaded but was not able to open it. Any suggestions. Thanks and Regards Darshan,311032
88296,nan,319759
95514,"In the video on how to create scatter plots in Tableau, prof. RC during 2:47 says that aggregated visualisation of profit against sales gives an insight of how profit varies as sales increase and goes on to comment that for certain categories it need not imply increase in sales means increase in profits. I sense a flaw in this discussion because we haven&#39;t yet actually looked at unaggregated values of sales against unaggregated values of profits to comment on how sales and profit fare against each other. The aggregated visualisation just tells us for each category what is the sum of sales and what is the sum of profits and it shoud end there.",318079
88787,"The question I resolved by creating a new copy of salary field and putting this in dimension instead of measure. Then I plotted a bar chart between the copy of salary (which was now a dimension) and the average balance. As required, I made the response display in colors and finally got the answer. However, the assignment was related to Scatterplot and didn&#39;t understand as both aggregate values for Salary and balance go hand in hand. It means that either I need to take average for both or keeping both non-aggregated. The button, aggregate measures suggest that only. Can you explain how to plot a scatterplot between salary and average balance?",311729
95801,An error occurred while communicating with Microsoft Excel Reader. Bad Connection: Tableau could not connect to the data source. Illegal file type for Excel protocol: &quot;D:\New Volume\My work\self\Tableau\bank-marketing.csv&quot;,316926
95542,nan,308495
95916,Hi Can anyone share screen shot of scatterplot question?,314629
94486,nan,303666
95631,nan,304695
96429,cannot load file gives unknown error any suggestion,317988
95977,"For what salary, is the average balance of response = yes lower than the average balance of response = no? Plot a scatterplot to answer the question.",310179
95261,"When i upload the bargraphs (1) file and open it in tableau , I get an error, Could not find the referenced file '/Users/PGDDA/Documents/My Tableau Repository/Datasources/10.1/en_US-US/Sample - Superstore.xls'. Replace it with another file? When i try to update the join i get Unable to complete action Unable to change the join clause information for ""superstore_sales"". Unable to update relation.",312491
95678,nan,314621
95754,nan,308638
95759,Y axis is not clear in seaborn distplot.,307843
96733,How can I Automate dashboards and send to respective business stakeholders in Tableau,300688
102759,"tl;dr: Matplotlib is better than Seaborn when it comes to customizing capacity and the ability to produce beautiful visualizations. When I started out with DataViz in Python, I thought that the libraries offered in Python couldn't compare to those offered by R. With ggplot2, R beat a lot of the data science languages easily. And, with easy integration of high-quality plots provided by the package into documents (.docx and pdfs) through RMarkdown, it even competed with LaTex. But, this view recently changed when I came across seaborn in Python. Seaborn (which I'll refer to as sns now onwards), can produce beautiful plots, very quickly. But, sns is built on matplotlib (which I'll refer to as mpl). So, mpl must be able to produce the same thing too. But, it takes more lines of code to produce a scatter plot facetted by a binary categorical variable using mpl than it does using sns. So, sns is easier. Or that's what I thought. I recently worked on a project (which you can find: here https://github.com/krishnamurthypranesh/studious-spork) where DataViz was the most important component. There, I found that using mpl allowed me to modify my plots to my liking while sns limited that capacity. And, when it did offer customizations, it was through mpl.pyplot methods. So, here's what I think: Seaborn is a great library that can be used to rapid-prototype EDA. But, when it comes to producing plots that capture one's attention, matplotlib is the go-to library. What do you think? I'd be glad to hear your opinions.",306733
97326,"I added all my details &amp; filled Mini Profile earlier. But when I want to edit my details now, couldnt see my content at all &amp; it shows everything blank. Any one facing similar issue. I have used same login id&#39;s now &amp; earlier as well",312093
95705,"Hi Team, When i am trying to update my profile i am getting error at updating my linked-in profile link details. I traied multiple opetions but didn't worked",315455
97385,"In mini profile, are we suppose to mention this PGDDS course in PG section since we are pursuing this.",316147
94973,nan,302741
94993,Did anyone filled up profile book? What type of template to we need to chose on typeforms?,318458
96197,nan,318481
98926,nan,306726
95771,nan,300712
102467,"On the page to confirm the coach, when I upload the resume, a loading symbol is showing but nothing's happening. Is it down?",318329
127346,nan,314818
88605,"I am able to get the answer by sorting the values by count to get the maximum cities on top. Wanted to check how we can get one value insead of multiple city names. Below is the SQL i am using: select b.name, count(1) from world.city a, world.country b where a.countryCode = b.code group by b.name order by 2 desc;",306725
88722,nan,316298
88568,"Which country has the maximum population (If population is taken as the population of the country table)? select Name, max(population) from country; is this code wrong because it gives me right population value but wrong Country name ?",318377
90522,nan,308495
92779,nan,301644
93258,what are ways to get previous record value or than using lag() or any window function,301108
92287,nan,300706
92136,nan,319846
92782,.dll file missing it was working well before suddenly stopped working tried reinstalling,317988
93455,nan,311046
93141,"after laoding sql , reached at my sql first assignmnet now the 1st question is count cities . where I hv to go now mostly keys are inactive ... can anyone suggest the steps for next process pl",319969
92816,find difference between two dates read from a table in mySQL,306734
84651,"It is mentioned by lecturer that in select clause we should write e.dno p.pno etc for retrieving data But I tried without . operator it still works Maybe this is because once tables are merged, the . operator is redundant",308437
92444,I am getting data truncated warning for fields I am inserting null value. The datatype is double. Why is this so?,317149
87266,nan,311119
91374,nan,307491
92388,nan,314612
88346,I got a notification that I need to make a submission for SQL. When i try to open the assignment it says to be launched on 30th Sept. Is there something still pending on my part ?,311312
84650,"In the example employee, workson and project tables are merged But only employee and project tables merging is sufficient for the question right ?",308437
84205,Tried without that ; in my SQL workbench it worked Then what is the necessity??,308437
84206,Example no 29 in sql Same thing can be done using having clause but didn't work? Reason?,308437
84244,This is not explained in the lecture can anybody explain the flow and braces structure?,312050
84249,nan,308634
88434,"select e1.fname,e1.lname,e1.salary,e1.dno from employee e1 where e1.salary &gt;=(select avg(salary) employee e2 where e2.dno= e1.dno);",305841
88476,"Select dno,Count(*) as &quot;Emp count&quot;, avg(salary) from employee group by dno Having salary&lt;=avg(salary);",313767
88645,nan,316132
88490,"When we run a query in SQL editor, the action output shows pass / fail of the query with the number of records returned. But I could not see the result grid at all, Though we can run the EXPLAIN command and go to result grid but this option is time consuming. Is there a way to pin the result grid as the output window.",314730
84819,nan,313826
85009,,316368
85327,How can these be done using the HAVING clause? can someone help me with these?,304815
93257,In real time join queries are faster or sub queries ?,301108
85328,What is the output of above question?,310419
85350,"I am not able to attend the last question of this video, The Question is not displayed at all, because of which, I am not able to proceed further. Any help would be appreciated.",311061
86441,the co-related sub query example in outer join is not very clear...example what does e1.DO=e2.DO signify? Can it be explained further?,310509
85345,What is the output of the above question?,310419
83295,"Hello all, can we use Join clauses instead of Nested queries, as the common thing in both is that both take a reference of different table? please share your views..",305129
81635,"I asked the below question in stackoverflow today, can someone look into it and see if they can explain the behavior? https://stackoverflow.com/questions/51856270/error-code-1055-expression-1-of-select-list-is-not-in-group-by-clause-and-con I know the fix is to add dno in the group by clause or put any_value(dno) in the select clause but I want to understand the above behavior. My question is simple, if select dname group by dno works, why select dno group by dname wont' work when both the combination of dname and dno for all the resulting groups is unique as shown below? Reference for group by explanation https://www.youtube.com/watch?v=bziXLSplVFc",310974
83312,"I have solved this using inner join, as below- select fname,ssn from employee e inner join works_on w on e.ssn=w.essn where hours &gt;=20; Can it be solved by using nested queries???",305129
91448,Please help as I am unable to proceed because of this.,305845
92362,I've used the SQL import wizard for importing the CSV file and this as we have seen in many questions is leaving a row giving the imported count as 888 against the original 889 rows. Will this impact the grades?,305845
92523,I am running the command alter table employee add hno int; and getting error -Failed to set SDI Companydb.works_on in tablespace 'companydb/works_on',300735
86976,This is what I&#39;m trying &quot;select * from employee where salary &gt; avg(salary) group by dno;&quot;,315250
86727,I have completed all the items in the sessions.,312376
90980,nan,308495
87693,"I am not able to see SQA session in Preparatory Program, Today morning also i have completed few topics-Basic SQL in that but now i am not able see that session, could you please help and guide .",320008
87721,"--show no. of male employees dept wise select dno, count(*) from employee where sex = &#39;M&#39; group by dno;",318335
87710,Getting as SQL Practice Questions available on 30th SEP? Pls share if any,320008
92289,nan,320606
91002,1. How to install MySQL/SQL on macbook? 2. Are there any basic requirements before installing MySQL 3. Is installing MySQL &amp; SQL different? ( i.e. Do I need to install SQL if I install MySQL?) Please help,308636
91580,nan,308437
91587,nan,307708
87647,What is wrong in the below query ? It is giving the maximum salary among males but the name of the person having the maximum salary is wrong. It should be james Borg and not John Smith. Same thing happens when applying where clause for sex = &#39;F&#39;. It returns the max salary for female which is 43000 but instead of fetching the name Jennifer Wallace it fetches the first female name in the table i.e. Joyce English.,318479
88138,"Hello all, When we try to do aggregate functions like Min, Max on &#39;name&#39;, It gives alphabetically first name and alphabetically last name as answer respectively..... Where as When we try to do statistical functions like avg, std dev etc.. It gives answer as &#39;Zero&#39; / &#39;0&#39;. Why not an error?",318347
91597,nan,319721
88256,nan,318159
92798,As I am new to stock marketing... Could some one please help me with some analysis points that we need to specifications.,301641
90498,,300690
90568,nan,308495
92117,nan,319846
91196,Im a bit concerned here because Im comfortable using MSSQL implementation of SQL whereas MySQL(as per installation instructions in SQL module) is bit different. Would that be a problem or might create problem during doing assignments/graded questions?,307176
92015,are the below same: 1. select year(bdate) from employee; 2. select extract(year from bdate) from employee;,312259
91117,For ex. I want to add a column 'sum' into employee table and want to populate this with the sum of column A and column B from same table.,320073
91131,"I am little confused of what constitues a facts table . In the image below , can we define a star schema around products i.e products will be the fact table and the sales and store suppy dimentional information about products ?",317149
91835,dense_rank() gives correct results but whereas rank(0 doesn't give correct results. What is the use of rank() function?,308442
91837,nan,308442
91842,nan,308442
91870,nan,307843
91871,nan,307843
91320,I didn't found matching answer in the option to the first question of the SQL graded question-II. Question: What is the sum of the house numbers of all employees? Did I miss something! Can anybody help me understand the question?,318585
91295,alter table employee drop foreign key fk_dno; Above code able to remove foreign key dno from employee table Again i wanted to add back alter table employee add constraint fk_dno foreign key(dno) references department(dnumber); This is not working. Error of already its foriegn for dept location. add constraint fk_dno foreign key(dno) references employee(ssn); if i give employy(dno or ssn) says error employee table not exits. How to set back the foriegn key here. Please help.,312019
91311,alter table works_on drop primary key; desc works_on; alter table works_on drop foreign key fk_essn; desc works_on; alter table works_on drop foreign key fk_pno; desc works_on; alter table works_on add constraint fk_pno foreign key (pno) references works_on(essn); Not working,312019
93082,how to store a return data set from a select statement into a column in mysql?,302739
92868,nan,318780
92769,After importing data in MySQL and create table.Adding the primary key field gives an error and table cannot be saved. What is the reason.,320648
93422,I see the moving average calculated using over clause doesn't match with the actual average value. I tried using both ROWS and RANGE for preceding 20 and preceding 50 MA. Am I missing something?,318080
93488,There have been multiple questions which are exactly the same. If you search for your questions before you post them your question can be answered within no time. It also helps all us to deal with different variety of questions.,319721
92622,"what should be the syntax for problem: Display the deptname, the project the department runs??",317558
92531,"select ssn, concat(fname, ' ', lname) as emp_name, dno, salary, sum(salary) over () as total_salary, sum(salary) over (partition by dno) as dep_salary from employee order by dno By executing the abovequery in my Mac Laptop i getting following error rror Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '() as total_salary, sum(salary) over (partition by dn' at line 6 0.00023 sec",318461
92745,"For the sql assignment, should the columns names given in the task be followed exactly or is it just for reference?",317149
92749,If I add deterministic it will tell that output is same for same input. But if I dont add? Is there some performance issue that arises?,308784
92723,"select pno, count(*) numemps from works_on group by pno having numemps = (select min(nemps) from ( select pno, count(*) nemps from works_on group by pno ) tempproj );",305804
92616,"In SQL multi-join statement a problem was explained by RC Sir which states: Display the ssn, their department, the project they work on and the name of the department which runs that project solution to which is given as select e.fname, ed.dname 'employee-department', p.pname, pd.dname from employee e inner join department ed on e.dno=ed.dnumber inner join works_on w on w.essn = e.ssn inner join project p on w.pno = p.pnumber inner join department pd on p.dnum = pd.dnumber ; So my doubt in this problem is that - in select statement don't we need to first define the alias before directly mentioning it? As it is directly mentioned select e.fname, ed.dname 'employee-department', p . pname , pd.dname and secondly, can we use multiple aliases for the same table as in the above example for the department - 2 alias has been used i.e. ed &amp; pd.",317558
93222,Can we use alias in where clause where alias is used for Lag Function,301108
93224,Do Inline queries are more efficient or sub queries are more effficent,301108
91874,"Please confirm if we need to perform this just for 1 stock? Also, for brief summary for TASK 5, do we need to write for bajaj or all the stocks?",305845
94707,Here's the link to understand everything about SQL Window Functions and its types. https://www.sqlshack.com/use-window-functions-sql-server/,312953
95375,"Would like to know the ideal answer for the question in advanced sql assignment for the last part - conclusions and summary. We were expected to write our key observations, conclusion, inference etc Though there is a feedback for this section asking us to improve our answer, it doesn't tell us what exactly is the ideal or expected answer for this question... Providing this answer helps us to correct ourselves for future such assignments TA pls answer",308437
93251,"In Advance SQL Graded questions 2, For calculation of avg distance of houses, i am facing an issue. I have calculated distances by using abs function.After calculating average my answer doesnt matches with any of the options available. Anyone faced this issue? Any help is highly appreciated.",318362
103619,How it is different from INNER join. When is it used?,318344
91375,nan,305804
78633,,304397
87131,nan,310385
84701,"Picture1: Tutor screen has one default shema SYS Pic2: When i created my frist MySQL Scema i was seeing 3 default schemas called &quot;SYS&quot;,&quot;WORLD&quot;&amp;&quot;SAKILA&quot;. can some one tell me what is this 3 schemas",315455
80584,"what is the order of execution of all the clauses in sql (WHERE, HAVING, ORDER BY, GROUP BY, SELECT, FROM, etc) ?",301655
82297,Hi I have dowloaded mysql on my mac. but it is showing not connected to server. how do i make it connected to server ?,311449
81437,1) There is no MySql installer for MacOs. I'm guessing I have to go with the community server: 2) The session says we have to install 32-bit for Window even though our machine supports it. That applies to Mac as well? I don't see a 32-bit option for MacOs community server: Upgrad should give installation instructions for Mac as well. I see they assumed everyone uses Windows. Duh?,310974
78954,nan,305838
77554,nan,300686
81607,"Hello All, I am trying to Install MYSQL but the installation is failing due to Visual Studio 2017, 2015 or 2013. I tried installing the same but still the issue does not resolve. Is someone else also facing the similar issue. Thanks Vikas Lalchandani",310210
87202,nan,310385
79199,This is what I am getting,305838
81690,"My MySQL workbench is acting weird. When I am running SQL queries, it suddenly stops displaying `Result Grid` but executes the query. I end up restarting my workbench to work normally. I am using Mac SQL version of ` macOS 10.13 (x86, 64-bit), DMG Archive ` and ` MySQL Workbench 8.0.12 `",307486
84133,"While installating MySQL as per the document I do not get the step which asks you create account, it directly shows the files to be downloaded Is anyone else getting something similar or am I missing any step",312892
87213,"Hi All, This is one of the example of companydb. Display the SSN of all employees who live in Houston. I am getting null value for ssn. select ssn from employee where address LIKE(&#39;%Houston&#39;); can someone correct me in above syntax.",320195
84931,nan,306726
88555,password for root ijn sql intallation,318723
88575,nan,318016
82028,"1. Should we download Mysql installer 32 bit or 64 bit? 2. Do we need to install the custom or the developer default? 3. Do we need to seperately install .NET and Visual C++ redistributable? Again which version 32 or 64 and which one - 2013 or 2015 or 2017? 4, Do we also need to install visual studio before installing Mysql installer? I have tried all permutations and combination. But some or the other setup fails. Pls guide.",304319
80253,"I was hoping to use Postgres. But, is that allowed? Thanks for the answer!",306733
82229,nan,300719
91898,As it will help in relating the query and logic,318791
90210,nan,317990
87420,,314313
83416,nan,307488
93261,If a table having Composite primary key with three columns . Can we use only one column as foreign key in other table,301108
91216,nan,314678
86413,"Hi guys, Autocomplete option in MYSQL work bench 8.0 not working for me. Could you please let me know if any option need to be enabled for this purpose ? PS: I have already enabled Automatically start code completion option in preferences. thanks, Prathap Seesala.",305652
85269,nan,312518
81165,"hi, Is there anyone facing the same prob. as mine ? I have installed My sql as instruction per instruction for mac. I can run the querry &amp; Action out out is valid but I am not able to see the result grid. please help me if anyone had already solved the issue. Screen shot attached Thank you",306012
85312,what is the solution of the above question?,310419
91250,I cannot login. Do i need to create new connection. Delete the existing connection and follow the steps mentioned in the mySQL pdf document. Please suggest.,305843
80974,nan,306245
88798,"While installing MySQL Installer, after proceeding with Check Requirements, I am getting only 3 products Connector/ODBC 8.0.12, Connector/J 8.0.12 and Connector/NET 8.0.12. Did anyone got similar issue!",303227
83632,,302742
83129,ERROR 1146 (42S02) at line 313: Table &#39;companydb.deparment&#39; doesn&#39;t exist Operation failed with exitcode 1,305655
86478,nan,314313
83282,,300690
81083,"Hi, In the 2nd video. Why is Pno not a foreign key and only Essn a foreign key??",308962
85472,,300733
82838,"Hello, can anyone please provide me the link of SQL download for mac.. i am not able to find it on mysql page..",305129
82841,"Hello, is the below sql version we have to download for mac? mysql-8.0.12-macos10.13-x86_64.dmg as in the pdf of download instructions its written- Mac OS X ver. 10.6 (x86, 64-bit),",305129
85490,nan,308782
85492,nan,312019
85624,nan,308432
87371,nan,312199
85715,nan,308432
88897,nan,305335
84052,"Is sql case sensitive? The lecturer said from,where,select etc all keywords can be either caps or small or mixed.... What abt the strings which we search from table? For eg. If I want to access reg no of madhu, if I type as MADHU will I still get same reg no?",308437
86946,,318723
84055,I have typed the command as mentioned in the video but the employee table not showing in the MySQL Workbench. I have even clicked on button &quot;Execute the command under keyboard cursor&quot; and the companydb schema is also selected. Also have run the companydb script which was successfully executed. Please help me out... Thank You,306010
93785,"Workbench has stopped working, the service cannot be restarted- gives error . No changes have been done to configuration. Has anyone else faced this? How can this be rectified without uninstalling and reinstalling ? PS: tried most of the links suggested by Google like trying to update password which all ends in error.",317149
85655,,315885
86523,The above two photos show the steps in creating the table and the error that appears. Also I do not get the screen shown in the video by Professor R C while the table is being created. Please help.,310505
86110,dont,314313
86121,nan,314162
86118,,314162
86114,nan,314313
86251,"Query completed successfully in work bench , but I could not find results grid. Could you please let me know if there is any option to restore the grid pane. Thanks in advance.",305652
86933,nan,315797
86261,nan,315661
86358,"As part of Data Analysis using SQL module - there are 3 sessions in which session 3 will be unlocked on 30th Sept. Even though I have completed session 1 and 2 its not showing as ""Completed"". Is that as expected?",320103
91468,,320251
88882,"I have Windows8 OS on my desktop. When I am trying to install MySQL I am getting Initializing Database error (Please refer attachment). Please note: I have installed .NET Framework 4.6 .NET Framework 4.7.2 is supported on Windows Server 2012. It is not supported on Windows 8. https://docs.microsoft.com/en-us/dotnet/framework/install/on-windows-8 Tried multiple times uninstalling, restarting, etc.",311870
91397,"I am constanstly facing the error 'Error Code: 1064. You have an error in your SQL syntax.' because I am using the syntax from SQL Server. Came across these links and found it useful. Sharing it , incase anyone else is facing the same. https://www.mssqltips.com/sqlservertutorial/2204/mysql-to-sql-server-coding-differences/ https://www.dbload.com/articles/mssql-and-mysql-comparison.htm",317149
87408,nan,318335
92625,"Can anyone help me solving connection error of mysql in mac? I am getting below error. server start log: 2018-10-24 17:29:13 - Starting server... 2018-10-24 17:29:13 - Executing 'launchctl load -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist' 2018-10-24 17:29:19 - Checking server status... 2018-10-24 17:29:19 - Trying to connect to MySQL... 2018-10-24 17:29:19 - Can't connect to MySQL server on '127.0.0.1' (61) (2003) 2018-10-24 17:29:19 - Assuming server is not running 2018-10-24 17:29:19 - Starting server... 2018-10-24 17:29:19 - Executing 'launchctl load -F /Library/LaunchDaemons/com.oracle.oss.mysql.mysqld.plist' 2018-10-24 17:29:19 - Start server: /Library/LaunchDaemons/com.oracle.oss.my 2018-10-24 17:29:19 - Start server: sql.mysqld.plist: service already loaded query execution error: 17:28:40 Could not connect, server may not be running. Can't connect to MySQL server on '127.0.0.1' (61)",318493
91927,nan,319846
93741,nan,307489
92734,nan,314329
87655,,316349
88219,Please find the screen shot.,308639
90073,sq,315121
92086,Currently I am facing issues with MySQL 8.0.12 installation.But I could install MySQL 5.7 without any issues and able to connect to databases. So is it fine if I contiue all our assignments using MySQL 5.7 instead of MySQL 8.0.12 ? Also let me know what are limitations for using MySQL 5.7 in comparision with MySQL 8.0.12?,312623
90281,nan,310008
89700,"While installing MySQL as per the installation guide, I cam across a screen saying &quot;The following products have failing requirements.....&quot; and listed &quot;MySQL for Visual Studio 1.2.8&quot; below. The requirement is that visual Studio version 2012, 2013, 2015 or 2017 must be installed. The installation guide just advises to click &quot;Next&quot;. Bt when I did that, I get a ;popup saying &quot;One or more product requirements have not been satisfied. Those products with missing requirements will not be installed or upgraded. Do you want to continue?&quot; What should I do here?",318762
90282,nan,308495
91630,,306996
91957,"I Have excel 2007 , VC+ 201, and .net framework 4.5.2 installed but when i trying installing MySql through installer , unable to see all the items in list. I am doing exactly as per doc file while running MySql installer ---https://cdn.upgrad.com/UpGrad/temp/89ddbb36-9d58-4db6-92b9-d0bebd4d6862/MySQL+Installation+Guide+(Windows).pdf",311386
91703,How to start mysql,318005
92481,In the given list of questions 3rd question - Retrieve details of projects that are based out Houston or Stafford. when I am writing code : select * from project where plocation = 'houston' or 'stafford'; its giving output only for houston not of stafford and if i write select * from project where plocation = 'houston' or plocation = 'stafford'; then its giving details for both the location. Do I have to mention plocation for both locations??,317558
91985,nan,319846
91992,nan,319846
95799,,303115
91783,,315560
92333,"Guys, we all might have faced various roadblocks on errors while coding. We finally might've ended up with a solution for our errors. If we can share these errors and solutions in the comments below it'd be great! As detailed in the title, this is not a question but a set of solutions to some of the errors you might face while trying to import files in MySQL 8. Note: Just make sure that the errors are not taken from graded questions and assignments. There can be conceptually similar errors but not functionally same.",306733
91167,"MySQL server is disconnected and when i am launching it again, it is not asking me for the password as well. How to fix this issue?",310509
91824,While MySQL installation below two products didn't got installed 1. MySql Router 8.0.12 2. Connector/ODBC 8.0.12 all other products got installed successfully . Can any one help what should I do or is it fine if I can move on with this installation ?,312623
91232,"This is not a question, my finding on the Basics of SQL --Prep Course. Link to the page as below https://learn.upgrad.com/v/course/208/session/15786/segment/79809 There are 3 videos in this page. Now, the Video 1 should be Video 2 and Video 2 should be Video 1 in this page to make sense for the continuation of course. Kindly check and correct it. PS: Issue has been reported.",314048
91241,I am not able to install mysql but I have oracle in my local can I use that for our study?,318869
91354,nan,306147
91841,nan,301890
91192,se find ty,318723
92595,For the question : Display the names of the project that are neither based out of houston nor out of stafford using AND/OR clause I wrote this code: select * from project where plocation = 'bellaire' and plocation = 'sugarland'; where am I wrong in this code as I am not able to execute?,317558
92601,select ssn from employee where address like &quot;%Houston%&quot;; All the ssn numbers are displayed when I run the code.,308638
92764,"Hi all, can you please help in figure out the following problem as I new to it. while trying to run my already created scheme, I am getting the above and unable to run my SQL script.",316399
127477,nan,300733
107032,I tried many ways at data understanding head but fail . most of the time it is giving error,319969
103689,"Given sample sollution mentioned that 60 months term loans default more than 36 months , but actual countplot showing different analysis. Sample solution bar plot: Count plot from analysis: Am I missing something here ? did sample sollution using different data ?",305652
102251,"I read in a few places that only charged-off rows should be considered for analysis (as said by TA - https://learn.upgrad.com/v/course/208/question/102031) I don't understand why we are not looking at relative ratios. So, totally made up example, let's say that out of defaulted loans grade B, C,D loans occur the most (absolute value). But when comparing it with non-defaulted loans we might see that the *ratio* (we have to use ratios as there are wayyy more.non-defaulted loans) for grade A and B loans is much higher for non-defaulted loans; and grade C to G ratios are more higher for defaulted loans. So taking only defaulted loans (ie. Absolute values) we would have hypothised that B,C,D loans lead to higher defaults; but taking ratios of defaulted vs non-defaulted loans would lead us to the more accurate hypothesis that grade C to G loans lead to more defaults.",300694
102043,"I wanted to collect data only less than 6% empty. dropping used below command ndf = df.loc[:, df.isin([' ','NULL','NaN', 0]).mean() &lt; .6] Its not working. any problem here ?",312019
99955,,311254
100421,Out of bounds nanosecond timestamp: 1-12-11 00:00:00 how to fix the above error using datetime function? i tried to use errors =&#39;corece&#39; but it made all the values as NaT.,310509
100427,nan,305839
100399,Dropping Attributes.,318435
102430,"Is anyone facing an issue while running the python code where even if the column exists, on the graphs it says it doesnt. Few of us in the group have this issue while few dont. Really confused. Tried clear cache and other miscellaneous techniques but no success. Is it owing to system issue or completely due to some kind of data issue?",303670
101070,nan,300717
102376,nan,318579
101154,In the data dictionary I see that every important attribute in the dataset is categorised into 2 like total_paymnt &amp; total_paymnt_inv. Can somebody explain to me how this impacts the loan process?,318381
101194,nan,300717
100561,"The Problem statement talks about doing independent research on the Variables and refering lending club&#39;s website business model. i tried searching the web, but unable to find anything useful. can anyone help with useful links to go through and prepare for the case study please?",316036
101208,"Whenever i run a plots, my note book hangs with the message of &quot;Kernel Busy&quot;! Notebook never comes out of this mode. Any suggetion to resolve this ?",311741
101236,nan,317418
100349,"Dropping more than 30 columns all of which having null values could be sometimes cumbersome, so apart from using dataframe.drop do we have other quick option?Also is it allowed in such analysis where we can take only the subset of dataframe because the original one has NA values?",301114
101223,"For types of decisions by the bank under loan accepted following is given: Charged-off : Applicant has not paid the instalments in due time for a long period of time, i.e. he/she has defaulted on the loan Please elaborate on this..",310508
101220,The dataset is realy huge in the Gramener Case Study Loan csv. Can we delete the columns with NA from top to bottom.? or are they required for analysis futher analysis?,310508
101930,Unclear about the meaning of these terms,305655
101264,Please help to understand the difference between loan_amnt and funded_amnt funded_amnt- The total amount committed to that loan at that point in time. Not clear on what is 'funded_amnt'.,309451
101249,How to decide upon which columns to use for analysis &amp; drawing inferences since there are so many coulmns in this data set for Univariate &amp; Bivariate analysis? Do we group/rank the important columns? TAs..Please help us understand the data set better. Thank you,310508
101931,nan,308638
101316,"In the Data_Dictionary excel sheet, there is a RejectStats worksheet providing details of the columns in RejectStats File. But, in the Loan Data Set zip file, I could see only one csv file &quot;loan.csv&quot; having only &quot;loan&quot; worksheet. Am I missing something here.",314730
101323,"There are so many variables/columns to do UV and BV analysis!! I am sure not more than 25% of originally provided columns in csv file, will be useful. How to go about this? TA: pls give some hint",308437
101938,nan,316255
101324,It is mentioned in assignment that we need to understand risk analytics and associated variables and their significance. Searched google on this but in vain. Any useful links to know what are they key variables to focus on in the input csv file?,308437
101325,What exactly are we supposed to know from this website? TA: pls guide on this.,308437
101336,Should it be based on row count or a particular percentage of row count is under practive in the industry?,300721
101351,The description is given as total amount funded by investor but who is the investor here and what is the context of this?,305656
101969,nan,301644
101974,nan,318517
102281,"Hi, Anyone able to create a stacked bar chart in seaborn? This is needed for the segmented univariate analysis. I'm able to do in Tableau but there is no easier way in seaborn.",310974
101978,nan,314313
101993,as per data disctionary data set contains data both loan approved and loan rejected cases where as when we check data there is no data on Rejected cases why?,315455
101999,"In the evaluation rubric it says ""All data quality issues are correctly identified and reported"". This needs to be done to only the columns we select for the analysis or all the columns?",310974
98697,Will we have to form group again for upcoming EDA group case study. ? The reason for asking is I can't see the previous group members names into next EDA group case study module. Has someone noticed this ? Can TA's clear this ?,317991
102021,It is given that funded_amnt- The total amount committed to that loan at that point in time. Funded_amnt_invThe total amount committed by investors for that loan at that point in time.What is the key difference?,308638
102031,"Hi, Should we take only the charged off data from the data set and perform the Univariate/Bi-Variate analysis or the entire data?",310974
102040,What do we mean by public record bankruptcy??,319869
102100,I have tried doing following from the anaconda prompt - conda update seaborn Everything is done seemingly successfully. Yet I am getting this error.,311857
102054,nan,319357
102102,how can we use earliest_cr_line ?,306244
102105,nan,308495
102106,nan,308495
102107,nan,308495
102108,nan,308495
102109,nan,308495
102112,nan,314818
102143,"Since it is asked in the case study to address the outliers, what is the range of values to be considered beyond which any value is considered as an outlier ?",313691
102151,"Also, please, paste a link after your answer if possible for further reading . Pasting answers if not confident waste a lot of time of others. Please, excuse me if I sound a little Harsh but my intention is good and for everyone's benefit.",315560
102184,Are we supposed to find atleast 5 important driver varaibles just from the Univariate and Segemented Univariate analysis as per the below evaluation rubric? Univariate and segmented univariate analysis is done correctly and appropriate realistic assumptions are made wherever required. The analyses successfully identify at least the 5 important driver variables (i.e. variables which are strong indicators of default).,310974
102218,"When i try to check data transformation with bar and histogram for some important columns, did not show normal distribution. Do we needs to normalize, not sure which data to remove, not able to found very few values causing this issue. Any generic help on this ?",312019
102248,Do we needs to do Hypothesis testing with python libraries scipy-.stats like sample t test paired test etc with python scipy lib ?,312019
102235,,311254
102220,Are we missing age or dob columns with this loan.cvs data set ? Or its not important ?,312019
102241,Do we needs to make new column with fill Accept or Reject status with the current data columns information of loan.,312019
102275,nan,315560
102271,The data given below contains the information about past loan applicants and whether they defaulted or not. I did not able to figure out the column related to defaulted or no with loan.csv files. could any one help on this ?,312019
102288,"Is it the case that, lets say, my max credit limit from all the credit lines is 10000 and revolving util of 98% means that, I&#39;m yet to pay 9800 and have 200 (installment amount) left for taking additional loan? Because, from the complete data set, for a fully paid loan and charged off, revolving util is &gt;90% for many of the cases. Does it make sense to check: Total credit = revolving balance/(revolving_util/100) --&gt; 9800/0.98 --&gt; 10000 Revolving_balance + installment should not exceed total credit. If the installment is 150 --&gt; 9800 + 150 &lt; Total credit --&gt; Acceptable for loan If the installment is 250 --&gt; 9800 + 250 &gt; Total credit --&gt; Cant be approved In the TA session, can this topic be answered pl?",314084
101348,The problem statement seems to suggest that we need to figure out metrics that lead to a loan being charged off. This will be used to figure out if we should deny or accept future loan applicants. We don't however know what data is taken from loan applicants. There is a lot of data in the dataset that quite obviously will not be taken from future loan applicants. What will be available from future loan applicants?,319357
102295,nan,315650
102297,"What is the meaning of "" The total number of credit lines currently in the borrower's credit file "" I believe that it means the borrowe has multiple loan amounts (credti lines) in his tenure. Please give comment on the same",306243
102298,"Please provide hint on this ""emp_title"" column",306243
102304,what should be the cut-off for removing outliers from annual_inc and int_rate?,300698
100746,nan,311254
102330,"The emp_title column contains lots of names which are similar, e.g. UPS, ups, U.P.S. Or US Army, US army, United States Army, United States Army, U.S. Army, us army, etc. - with or without punctuations or trailing space like 'US Army ' or 'US Army.' How to clean such data quickly?",318078
102334,"While converting the datatype for issue_d to datetime, i am getting the following error. Can anyone throw some light on how to handle this? &quot;Out of bounds nanosecond timestamp: 1-12-11 00:00:00&quot;",312096
102350,"SyntaxUsed: sns.catplot(x=""Status"", hue=""TimeSlots"", col=""Pickup point"", data=UBER) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-26-1999fc0ee2d0&gt; in &lt;module&gt;() 3 4 # Plotting the Categorical graph based on the pickup points of the trip and Time Slots derived ----&gt; 5 sns.catplot(x=""Status"", hue=""TimeSlots"", col=""Pickup point"", 6 data=UBER) AttributeError: module 'seaborn' has no attribute 'catplot'",318013
102355,nan,312096
102369,Do I need to divide variables into appropriate slots? confused as to how to begin the bivariate analysis.,314221
102312,Field Name: emp_length,312259
88648,nan,303674
88699,I think the names are similar but the approch is different. Is it like data model of a data warehouse is a base for data modelling in data analysis? Please help me understand if I have misunderstood.,316255
88703,nan,311857
89526,nan,311745
88608,Hi - I could see a need of bookmarking a page/session/topic in DS program. Is there any way we can get bookmark option on this course?,310518
88720,nan,306996
88894,nan,318846
89143,This article augments the content on CRISP-DM framework provided by course and also its relevance in projects focussing on deep learning. https://www.datasciencecentral.com/profiles/blogs/crisp-dm-a-standard-methodology-to-ensure-a-good-outcome,313691
89208,nan,305804
89206,nan,305804
89651,nan,318083
89833,nan,311169
89367,"While trying to deep dive into the nuances of CRISP-DM, I stumbled upon the below sites on alternative models and limitations with CRISP-DM. https://jenstirrup.com/2017/07/01/whats-wrong-with-crisp-dm-and-is-there-an-alternative/ This interesting read is on what is wrong with this framework and the alternative model called Microsoft's Team Data Science Process (TDSP). The article has well elaborated the CRISP-DM, TDSP, the 5 V's of Big Data. https://www.kdnuggets.com/2017/01/four-problems-crisp-dm-fix.html This is an excellent article on the 4 problems with CRISP-DM, which are A lack of clarity Mindless rework Blind hand-off to IT Failure to iterate",314730
91010,nan,304812
90914,nan,320257
90156,nan,302739
88764,# Read the input s = input() s1 =s.lower() def rev(s): str1=&quot; &quot; for i in s1 : str1 = i + str1 return(str1) #print (s1) #print(rev(s1)) s3 = (rev(s1)) if s1 == rev(s1): print(1) else: print(0),318017
90851,"For the Coding question related to Set: I understand that I dont have to hardcode anything into my code which might not pass with other Test cases. For question 4: I dont think we have a Set of students being called during execution, does this require me to hardcode a set comprising all the students and then compare it with other 3 sets ?",300699
88727,nan,307497
88693,nan,314197
89116,"Obj=slice(2:) for this getting SyntaxError: invalid syntax, could you please help me this is correct(slice(2:)) ? any wrong from my side? pls share.",320008
88853,nan,320689
90624,nan,307493
89441,My answer for Sports Set question is rejected even after the correct output. Does the code quality matters? and should it be matched it actual solution code?,301124
89458,,306996
89439,nan,300724
88783,Input 1: You love Python! Output 1: ouoeoY lv Pythn!,318017
89725,nan,308958
89497,"for the coding question on set operation, while verifying the code it says success, when I run the code it says success but while submitting the code it says rejected. any idea why this happens",312892
88792,"This my code which got rejected : Can I know why did it happend ? # Read the three input lists, i.e. 'C', 'F', and 'H'. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) C = input_list[0] F = input_list[1] H = input_list[2] C_S = set(C) F_S = set(F) H_S = set(H) A_S = set(range(1,21)) # Write your code here print(sorted(list(C_S &amp; F_S &amp; H_S))) print(sorted(list(C_S &amp; F_S - H_S ))) print(sorted(list((C_S &amp; H_S - F_S) | (C_S &amp; F_S - H_S)))) print(sorted(list(A_S - (C_S | F_S | H_S))))",311466
88809,"In this coding question there is a total of 20 numbers, but the details of who plays or does not play what is given only for 11 numbers( 2,3,5,8,9,12,13,14,15,16,20 ) and not given for (1,4,6,7,10,11,17,18,19). How are we supposed to code for this question? Should we or should we not include the numbers who&#39;s details have not been given?",304389
88835,"Write a code to sort a listof 0s, 1s, and 2s. Note that the time complexity should be O(n). Note: Do not use the built-in sorted() function. Example: Input 1: [0, 2, 0, 0, 1, 1, 2, 0, 1] Output 1: [0, 0, 0, 0, 1, 1, 1, 2, 2]",318017
88834,"input: [1, 3, 5, 7] [2, 4, 6, 8] Output 1: [1, 2, 3, 4, 5, 6, 7, 8] Read the input import ast,sys input_str = sys.stdin.read() input_lists = ast.literal_eval(input_str) list1 = input_lists[0] list2 = input_lists[1] #print(list1) #print(list2) # Write your code here def outp(list1,list2): p=0 q=0 l1=[ ] x=len(list1) y=len(list2) #print(list1[p]) #print(list2[q]) while p != x and q != y: if int(list1[p]) &lt; int(list2[q]): l1.append(list1[p]) p+=1 elif int(list2[q]) &lt; int(list1[p]): l1.append(list2[q]) q+=1 return(l1) print(outp(list1,list2))",318017
88837,I understand that comprehension equivalent will be just the logic and list comprehension will be [ logic ],315383
89445,Please understand that a discussion forum is a place where your learning is enhanced while you brainstorm concepts and understand the subject in a better way by peer to peer learning. There have been a lot of questions raised regarding the graded questions. And there have been many discussions regarding the solutions on the forum. Graded questions are there to test the knowledge you developed over the period of your course. So displaying answers for these will hinder you and your peers from being tested well as there is a possibility of solutions being copied. Please refrain from asking or posting direct solutions for these questions.,319721
89749,nan,305804
88869,"when I tried the code for the above question it says test case passed with the message, Description Testcase passed! The solution&#39;s output matches the expected output. but there is another message which says that the testcase is rejected if it is passed how can it be rejected.... any ideas",312892
90239,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str)",319860
90107,"This error is showing in my coding question &quot;An error occured while loading this embed .Please reload the page, or contact support if problem persists.&quot;",319969
90661,nan,307496
88916,"I would like to say this to all that while performing a coding problem dont be get impatient too early , give it a try if still not there take a break think again go back to your notes. It will make you more stronger getting help is not bad but too early is dangerous thx",319869
88936,nan,300687
88940,"Hey Guys, What is the shortcut to comment multiple lines in python - jupiter ?",305652
88950,nan,301107
88979,n=input(&quot;Enter the number &quot;) m=input(&quot;Enter the number &quot;) a=n+m print(a) When running the above code the cursor is moving to the next line without prompting me to enter a value. Kindly let me understand where am I going wrong,300688
90842,Why the index doesn't change for ipl17.set_index('Team') The output shows as below: Team Matches Won Lost Tied N/R Points NRR For Against 1 SRH 14 9 5 0 0 18 0.284 2230 2193 2 CSK 14 9 5 0 0 18 0.253 2488 2433 3 KKR 14 8 6 0 0 16 -0.070 2363 2425 4 RR 14 7 7 0 0 14 -0.250 2130 2141 5 MI 14 6 8 0 0 12 0.317 2380 2282 6 RCB 14 6 8 0 0 12 0.129 2322 2383 7 KXIP 14 6 8 0 0 12 -0.502 2210 2259 8 DD 14 5 9 0 0 10 -0.222 2297 2304,302739
88748,# Read the input n = int(input()) k=n+1 def rec(n): #print (k) if n &gt; 0: print(-n) if n &gt; 0: rec(n-1) elif n==0: print(n) rec(n-1) elif n &lt; 0: if n!=-k: (print(-n)) rec(n-1) print(rec(3)),318017
88986,"In the set question, I am getting answers of the 1st 3, but in the last question as I&#39;m trying to convert main input list into a set, it is giving error here saying &#39;unhashable list&#39;. The logic is ok as I got answers to all 4 in jupyter notebook.",318335
89029,"Will I be getting any partial marks if my code gets submitted and then rejected ? My code passed all the sample test cases, but sadly did not pass the hidden test cases. Is it either full marks or no marks?",304389
88893,"Is there a need to convert the set back to list and then sort? The sorted() function itself will do the job, as demonstrated in the code below: L = set([1,2,3,4,5]) print(sorted(L)) Output: [1, 2, 3, 4, 5] My solution without the use of list conversion got accepted too. I was wondering if there is some specific significance of converting it to list, as per the discussed solution.",317987
88765,"Intro toPython Graded questions: There are two sections in it 1. MCQs and 2. Coding. Is it possible to exit from it and resume after some time? Or once we start the test, we must finish it completely? Will there be more than one attempt allowed for that graded test?",304389
89087,nan,300719
89144,nan,301117
89147,My solution spanned several lines but I am truly amazed by the simplicity and elegance of the 4 print statements for the 4 questions. Power of Python programming versus conventional programming.,300717
89152,nan,320606
90817,,307005
90832,"out is (8,4) but is show incorrect .what should i do?",310008
90958,nan,311046
90959,nan,311046
89219,"Description Given an array, all the numbers appear twice &mdash; except one. Find the non-repeating number. (Hint: Use Bitwise Operators) Example: Input 1: [2, 2, 7, 7, 1, 1, 3, 9, 9] Output 1: 3",320195
89228,"Can anyone explain why the below statement is wrong? #return (reduce(lambda x: x*factorial(x-1), range(1,a+1))) I got th answer to the coding question 1, but still searching for better way with lot less variables involved .",302735
89230,"Testcase failed! The solution's output doesn't match the expected output. Solution output [2, 6, 13] [3, 5] [3, 5, 8, 16] [] Expected output [2, 6, 13] [3, 5] [3, 5, 8, 11, 12, 16] []",318454
89184,"C_Set=set(C) F_Set=set(F) H_Set=set(H) C_Set=set(C) F_Set=set(F) H_Set=set(H) print(sorted(list(C_Set.intersection(F_Set).intersection(H_Set)))) print(list(C_Set.intersection(F_Set).difference(H_Set))) print(sorted(list(C_Set.intersection(H_Set).difference(C_Set.intersection(F_Set)).union(C_Set.intersection(F_Set).difference(C_Set.intersection(H_Set)))))) U=[] for i in range(1,21): U.append(i) U_Set=set(U) print(sorted(list(U_Set.difference(C_Set.union(F_Set).union(H_Set))))) after execution i got output correct . where as its rejected after submission why?",315455
89330,nan,306009
90787,nan,306241
89340,"[3, 14, 20, 78] as 78 is not in the universal list",303082
89346,"Hi Guys, Just sharing the below info, where I had to implement in one of the questions outside UpGrad. To have the reverse range, below is the logic: range (10,1,-1). Just a normal mentioning like range (10,1) will not work in reverse order",312093
89846,nan,320690
90356,nan,310008
89114,"why does it takes larger memory size, as an example, here if you can see I have multidimensional array with total 8 elements, and when I click details after submission, it shows 16 MB of Memory was required for executing my program, can anybody please explain why is the compiler taking larger amount of memory while executing, any idea guys?",300734
90092,nan,316399
89576,This is from &#39;Interoduction to Programming&#39; the additional reading which right at the end of the solution of graded questions. the code was to reverse the digits of a number.. but when i tried it in Jupyter notebook it throws output as &#39;inf&#39; rather then giving the correct output.. can you pelase improvise the code? --------------------------------------------------- n = 4562 ; rev = 0 while (n &gt; 0 ): a = n % 10 rev = rev * 10 + a n = n / 10 print (rev) -----------------------------------,316349
90806,"Subtask 3.5: Find the best directors , for subtask 3.5 onwards should I use movies dataframe or top_250 datafram, someone has already asked this question but still I am not sure as some one in comment rightly mentioned that then whats the purpose of cleaning the data, thaks",317577
89959,nan,318083
90098,"C = input_list[0] F = input_list[1] H = input_list[2] C=set(input_list[0]) F=set(input_list[1]) H=set(input_list[2]) print(sorted(list(C&amp;F&amp;H))) print(sorted(list((C&amp;F)-(C&amp;F&amp;H)))) print(sorted(list(((C&amp;F)-(C&amp;F&amp;H))|((C&amp;H)-(C&amp;F&amp;H))|((F&amp;H)-(C&amp;F&amp;H))))) print(sorted(list(set(range(1,21))-(C|F|H)))))",300690
89400,Please read this article if you need understanding of set operations. https://docs.python.org/3/tutorial/datastructures.html#sets,318458
90099,nan,306243
89590,"There are two sets of test cases - sample and non-sample test cases Sample test cases are the ones which are visible and that get evaluated once you click on 'Verify' Non-sample test cases are the ones which are hidden and only get evaluated when you click on submit. So if your code is getting verified correctly but getting rejected on submission, the reasons could be: You're hardcoding the solution This happens if you're directly using the values provided to you in the description. Please note that all the inputs are already read and you need to work with only the variables that the inputs are read into. So always try and write a generic code that will pass all the test cases. And always make sure that you're reviewing your code correctly before you click on submit. Your code is incorrect There might be times when even though you've written a generic code, some of you test cases fail on submission. This is because you might not be handling your edge cases very well. An example of an edge case is, say, that 0! = 1. Now, this is a case that needs to be explicitly taken care of. So make sure that you handle all the edge cases if any before you submit your code.",306040
90493,"While solving Intro to Python:Graded Question , Code Verified Sucessfully. But when I submitted it showed that Test2 has failed. Is threre any way to check all Testcases before submitting code? So that code can be checked for all Testcases before submitting.",308636
89350,"The data type of integer in most languages will include negative numbers for which we need to test. Is it correct to give a status like -1 or &#39;NA&#39; for such values, or, should we handle these using exceptions? Which would lead to a better result for a data scientist?",308637
88917,nan,319869
90589,"In graded questions there is no special mention that this MCQ is single choice question or multiple choice, please if it will mention that in question itself that one or maore may correct then there is no confusion. I hope TA and Upgrad team will look into it.",318319
90736,nan,303085
91069,"Hi, I have wrongly submitted my assignment before even the file is autosaved. When I tried re-submitting it, the button is already disabled. Any help there please?",318080
91038,Why does Python for Data Science module compeltion show 93% when i have gone through the content &amp; graded Qs?,318827
90015,"In Jupitor notebook getting below error for line ""input_list = ast.literal_eval(input_str)"" Traceback (most recent call last): File ""C:\Users\ABHI\AppData\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 2961, in run_code exec(code_obj, self.user_global_ns, self.user_ns) File ""&lt;ipython-input-53-553f4e5df867&gt;"", line 3, in &lt;module&gt; input_list = ast.literal_eval(input_str) File ""C:\Users\ABHI\AppData\anaconda3\lib\ast.py"", line 46, in literal_eval node_or_string = parse(node_or_string, mode='eval') File ""C:\Users\ABHI\AppData\anaconda3\lib\ast.py"", line 35, in parse return compile(source, filename, mode, PyCF_ONLY_AST) File ""&lt;unknown&gt;"", line unknown ^ SyntaxError: unexpected EOF while parsing While searching found that this sytax error happens when some brackets etc. are not closed properly,in order to find out have commented everything else except below three line, and still getting the error while defining input_list : import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) The Upgrad interface is not throwing any error, anyone faced this kind of issue??",303240
95730,nan,312050
96310,Can someone please explain how can we approach this question.,320103
96550,Anyone explain the above question? I am taking all the data after the first submission date and time.Is it correct?,310419
96647,"How many students submitted the assignment after the first deadline (including the students who submitted after the second deadline) ? The deadline provided on the info is as below ""This was the Association Rule Mining assignment whose submission deadline was Jan 3, 2017 - 11:59:59 PM. The second deadline was Jan 9, 2017 - 11:59 PM. Submissions between the first and the second deadline attract a 30% penalty in marks."" However I am unable to find Jan month dates under submit_time column in the provided datset, all I can see is 6 rows of data all are from same day of Jan 2017 .PFB attachment for the same, I'm Is data missing from file or do I need to use different technique to solve this? Help me out on this.",312756
97077,"Why do we need to use a lambda function over the column, why can&#39;t we directly do column.day?",318344
96201,"For the above question, it is unclear what the column avg_rating is and how it needs to be calculated. From the expected output, it looks like it is the food rating. But otherwise, average will be the sum of the 3 ratings divided by 3. Please clarify what is required in the column.",310505
96226,What percentage of students submitted their solutions in .zip format? Should I include 3rd Jan to 9th jan submission count(30% penelty data),320689
97195,nan,305651
97157,nan,303228
98603,l,308638
98748,nan,303085
96411,where is this file to be downloaded,312892
95616,What&#39;s the license under which Awesome Datasets are available?,317984
96512,Can someone explain about their understanding for the given boxplot?,317984
96462,"In Quick way of Segmentation Professor shows What influence class VIII Marks analysis in an interactive page/portal. Does anyone know how to create such an interactive page using the analysis data? I see it was made in Gramener, but are there other ways of doing this? Thanks",312490
95781,nan,317993
97134,"Hi, when i use bar plot for &#39;literacy rate&#39; vs &#39;area&#39; i got below plot. I am not sure how to clear the x-axis values properly. Could some one look in to this.?",317410
97018,how to calculate the literacy rate for the graded question?,320606
96694,nan,308673
96711,Can anyone help me understand the quession?,307495
96716,percentage of females in the age group 20-24 are illiterate in India?for this I tried to find the fraction with total number of females in that age grop by number of illeterate females. Could some one explain this?,304693
96840,"So, the answer says that The average marks of students in Science in the all the labels are the same, i.e. around 36% marks . Can somebody please explain this to me? Because in my case the average of science marks segmented by Watch.TV is not even close.",318381
96849,We need to compare the literacy rates in each age group. What are the age-groups we need to consider?,304319
96476,nan,318481
96640,nan,318429
96513,Can someone add examples of the continuous variable which can be considered as the categorical variable?,317984
96377,"I&#39;m using &quot; plt.matshow(df.corr())&quot; for the correlation plot but unable to view the plot properly because of the size tried to increase the size but is of now use. Could anyone guide me how to deal with it to make it look visible. For increasing size i used &quot; plt.figure(figsize=(x,x) &quot;",306010
96970,How to resolve below error while creating pivot table in Python cannot copy sequence with size 41 to array axis with dimension 29,311868
96806,,306996
95878,nan,311160
97005,nan,313228
96783,do we need to groupby here for the 2008 while finding correlation between SilverPrice and GoldPrice,319444
96813,,306996
97090,I converted the month column to datetime format but wen i filter it for year 2008 i do not get any data..i even checked in the excel sheet..but did not get any date for 2008 there. Am i doing sumthing wrong or there are no 2008 dates.,320690
96437,"I have a column containing date jan-98 in object type, how i will convert it in date format.",310419
96138,Business Problems Involving Correlation 1st video has no sound.,320689
99043,nan,308964
96847,is there any guide available which shows how to build the corelation matrix as shown in the lecture by Anand sir?,310509
96535,How to find number of missing values in a row ?,314629
96559,"This question is based on programming question 1 in &#39;standardising values&#39;, so, please do not paste code in your answers. In places where there is a standard prefix to a column, there are many ways to filter the column. We can for example slice the string or do a lstrip. I prefer slicing as the operation is faster. However I am not sure if it works in all scenarios. Hence the question. Please share your experiences and examples of data so that I can learn better.",308637
96639,any hint to remove cust from cust_id?,320606
97642,nan,320685
96257,marks = pd.read_csv('https://query.data.world/s/HqjNNadqEnwSq1qnoV_JqyRJkc7o6O') not sure someting wrong in this. i did not see any of the row has 5 missing values. i wrote to csv and checked. below question is wrong ? Please clarify Remove all the rows in the dataset 'marks' having 5 missing values and then print the number of missing values in each column.,312019
96233,"Data+Cleaning+_+Checklist.xlsx To work on this sheet for Checklist for Fixing Rows and Columns to apply, we should do from python numpy and pandas or we can manage with excel. not sure everyting possible with python. some cleaning to be required in excel before going to python . pls clarify",312019
96414,,306996
96427,,306996
96456,"If I've a data set consisting of millions/billions of records. How would I be able to know whether a particular data in a column has a miss spelt or wrongly entered data and correcting it to proper data or removing it and making it as NaN. In Investment case study, there was a wrongly spelt data ""A0lytics"" and had to be corrected to ""Analytics"". But in broader case for millions of records, how would I be able to solve it?",312953
96492,"How these colors are coming and how to set them. i could able to practice about mean.median,std deviation. colors need help.",312019
95689,marks =marks[marks.isnull().sum(axis=1) != 5]--why it should be marks =marks[marks.isnull().sum(axis=1) == 5],320689
96698,nan,301110
95637,Kaggle: https://www.kaggle.com/dansbecker/handling-missing-values Towards Data Science: https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4 Analytics Vidhya: https://www.analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/ Hope you find them useful,317984
96519,"Hi , Remove all the rows in the dataset &#39;marks&#39; having 5 missing values and then print the number of missing values in each column. Can anyone explain this question ?",314629
97284,nan,318440
96846,nan,318077
96878,"customer[&#39;Cust_id&#39;]=customer[&#39;Cust_id&#39;].str.split(&#39;_&#39;,1)[0]",314629
96957,nan,310502
99126,nan,305842
97333,"The excel file popularity is read properly in python. When I am trying to plot histogram for column num_keywords ,am getting keyerror How to address this issue",308638
96317,Runs Scored by Tendulkar Plot a bar chart showing runs scored on the x-axis and frequency/count on the y-axis. In which bucket has he scored runs the most often?,318429
96406,From the video session it is clear on how to plot on log-log scale. It is also clear that it should / will be close to a straight line. How do I analyse such a plot. What conclusions can one draw from it and how?,318078
96412,nan,305652
96860,nan,301890
97361,nan,310472
96558,nan,308673
96965,"Hi, Though i have converted the column into &#39;int64&#39; and removed all blanks/null values in the row. I am getting above error, could some one help me on this. Thanks.",317410
96395,,300733
96968,nan,310624
96805,"I am trying to remove the spaces in the column header names across all columns using the below code, but it is not working. Can someone highlight what is the issue here? ( Column header is in 1st row, so index 0) df[:1, :].str.strip()",310509
97023,nan,301641
96153,nan,311117
96188,nan,311857
97089,"S Anand during 2min 42sec says Y=bx+c in linear is nothing but Y=cx^b in log-log scale. But I think it shouldve been the opposite and is wrongly written in the video. that is, Y=cx^b in linear scale translatest to Y=bx+c in log-log scale. Can someone please confirm?",318079
97581,nan,310179
96090,"75%tile value is referred as q3 25%tile value is referred as q1 50%tile value is referred as q2 in terms of percentile for upper quartile and lower quartile ,we have to calculate the percentile or we can use this upper quartile and lower quartile value as limits . or basically is their way to plot several intermediate quantiles in box plot for ease of analysis.",318005
96189,nan,311117
97681,nan,312892
95598,"After removing outliers using the method metioned on the page, remove outliers in Python the mean value i got is not in the list of options. Is there a problem or anyone else also facing this problem?",318329
98351,"we have to remove perform check of outlier only on shares column right, following is my sample code, popularity_df[&#39;shares&#39;].apply(lambda x: x &lt; popularity_df.shares.quantile(0.95)) But it always gives me result in true-false, in series, not dataset? Can anyone help?",315423
96202,nan,311117
96443,nan,300733
96674,"I am not able to understand where to do data cleaning &amp; univariate analysis using python or excel or Tableau. What ever thought in data cleaning session can be done using excel easily. Can someone please elaborate on it.,",311119
95728,nan,303674
96732,the stackoverflow answer on removing outliers was not very clear on how to replicate it for all columns...especially usage of quantiles and then using the x.name functionality in the code...can it be explained bit further?,310509
96740,What is the meaning of the &quot;What is the most common value of the variable X&quot;,313526
96920,import pandas as pd data = pd.read_csv(&#39;C:/Users/shruthi.hr/Downloads/popularity.csv&#39;) print(data[&#39;shares&#39;]) i get error for the above code,314629
101332,"Should there be Python questions in place of R questions, since, primarily we have been taught Python? Seems like this part, of course, is not yet updated. Can the Python questions also be added, please?",317987
126768,nan,300733
135743,I am not able to download data sets from https://shorturl.at/oDFTU,301643
104511,"Question to TA: It says that there are 4 sessions Linear Algebra (sessions 1-3) and Multivariate Calculus (session-4) in the additional resources module ""Math for Data Analysis"". But, I could see only 3 sessions as shown below:",310974
120924,nan,303085
138146,nan,311466
138147,nan,311466
134325,like when we want to reduce the no of features and get the best top features. In implementation we have to pass linear model object. Please clarify.,315028
116639,How to figure out the impact of  on RSS in ridge regression. How the value of  is impacting the RSS is there any function to understand this.,300735
118097,"In which import metrics, model parametrs are defined.",306734
117874,nan,301644
98345,"Not able to handle NaT while trying to extract time from a datetime Series. How can I make time() ignore NaT just like date() does automatically? Is there any parameter that I need to set ? Need help on this.Below is an example for the same. # creating datetime series dates = pd.date_range('2018-12-01 00:00', '2018-12-01 04:00', freq='H') dates = pd.Series(dates) dates # setting one record to NaT dates.iloc[2] = pd.NaT dates # converting to date dates.apply(lambda x: x.date()) # ignores the NaT and returns the date # converting to time dates.apply(lambda x: x.time) # return time object (and not the time which I need) # converting to time dates.apply(lambda x: x.time()) # gives error NaTType does not support time.",318479
97335,nan,318429
99579,nan,319357
97327,nan,310419
97329,nan,315633
97331,nan,304692
99386,"I am having confusion while plotting plot for this, can anyone explain what exactly needs to be done here?",315423
96306,i have made the graph but in that graph i want the count of supply as well as count of demand(sum of all the status ) .so can any one giveme a suggestion howtodo ?,303674
97211,nan,300687
97352,nan,310624
97350,"Can someone/TA confirm on below regarding Supply and Demand. Which one of below is correct:- Supply = Trip completed Demand = Trip completed + No Cars Available + Cancelled OR Supply = Trip completed Demand = No Cars Available + Cancelled In case of 1st scenarios - gap will be equivalent to (No Cars Available + Cancelled), so no need to consider &quot;Trip completed&quot; in that case. Please confirm.",320103
98303,nan,318579
97682,Getting below error while using catplot() module 'seaborn' has no attribute 'catplot',318479
97367,I have created a groupby object grouping by 2 cols. How do I plot a barchart? I am able to plot a bar chartwhen i groupby single column.,304319
97369,I separated date from time in both time stamps... Should I go ahead with deleting seconds portion from extracted times? As they wouldn't be necessary...I would b retaining hrs and mins,308437
97375,nan,313526
97376,"Hi I have uploaded the Uber Data to Pandas DataFrame and could see issues with Date-TimeStamp Fields with few of them having '-' and few with '/'. Apart from these, is there anything that has to be looked on w.r.t Data Cleansing.",311472
97379,Any specific date format required for analysis ? It can be yyyymmdd or ddmmyyyy or whatever...as long as all values in both date columns are in same format it should be fine right?,308437
97111,How to handle the extra detail of seconds in records? Either one should neglect those fileds where time is given till seconds or do the vice versa where the time is given till minutes precision?,318741
97381,"In the given data set whe i say pickup point is city, it meant as City to Airport cab request right?",311741
97118,"After downloading Uber request data I saw the dates in only one format i.e 11-07-2016 11:51:00. I did not see 2 types of date format(one with ""/"" and other with ""-"") as being discussed in forum. Can someone confirm this? <table style=""border-collapse: collapse;width:91pt"" width=""121"" cellspacing=""0"" cellpadding=""0"" border=""0""> <td class=""xl63"" style=""height:14.4pt; width:91pt"" width=""121"" height=""19"" align=""right"">",300698
97392,nan,318335
97395,nan,308639
98308,nan,318335
97131,are we suppose to do the date time conversion to a single format in python or can we do it in csv file itself and then import?,302738
97406,"as it is writtehn &quot;Note: For this assignment, only the trips to and from the airport are being considered.&quot;",318814
97415,How can I create a histogram for Status as it is a non-numeric field,301641
97416,nan,314197
97199,"Problematic type requests (city-&gt;airport,airport-&gt;city)should be filter with frequency of requests or irrespective of frequency.",320689
97130,After converting i get two formats 11/7/2016 11:51 (old format) --&gt; 2016-11-07 11:51:00 (New format) 13-07-2016 08:33:16 (old fomat) --&gt; 2016-07-13 08:33:16 (New Format) First one is treated as %Y - %d - %m Second one is treated as %Y - %m - %d Is this okay ? I am using to_datetime function for converstion and speciying the format as well.. but it is not consistent across the column. Did anyone encounterred this issue. Please suggest.,300727
97438,nan,304696
97445,"i see date and time are in the same column, i think we need to separet these 2 and create a separte column for another attribute, am i correct?",320606
97448,nan,308495
97457,I'm using 'For' loop to get the values from the column and then used if - elif - elif - else condition to check the values with the integers from the column to update a new column as 'Morning' or 'Evening' or 'Night' or 'Noon' but it doesn't appear to be working though the code is correct and it doesn't give any issues after running.. But the only problem is the results are not correct.. But rather is has no pattern in the results shown.. It is showing any scrap value in the newly created column in the dataset. Anyone facing the same problme? any coments on why it is not working?,316349
97459,demand = cancellation + no cars available ? Supply = completed trips ?,318358
97471,one of the issues i have noticed is that month field within date have a leading 0 which needs to be cleaned up. I am trying to use dt.strftime(%d%m%Y %H%M).lstrip(&#39;0&#39;) to remove the same but getting error. not able to understand how to fix the same.,310509
97493,nan,301641
97502,nan,308967
97515,"After extraction of time from request timestamp, the resulting new time column is not of datetime type... I am not able to extract hrs from this new column Y is it so??",308437
97528,Do we have to find the difference between the total and completed trips and then plot it? Or just the plot of total demand( completed + cancelled + no cars available) and each of the status is enough?,304319
97530,"How to group by particular variable and then plot a graph? Say column has 3 possible values x,y,z I wanna plot graph of only x vs time",308437
97354,nan,318455
97541,nan,318579
97546,0-5 morning 5-11 day time etc.,310419
97390,"In evaluation rubrics, one of the points mentioned was The demand and supply are defined properly and the numbers are correct. What kind of numbers for demand and supply could we possibly show?",318329
97552,df[&#39;hour&#39;]=df[&#39;timestamp&#39;].dt.hour. I want integer values in drop hour also while extarcting from drop timestamp?,318814
99401,What is the supply-demand gap exactly in our assignment? Is it the gap between cabs that are requested but not available or is It gap between cabs that are requested but get canceled or both? What do we need to find in this assignment?,315423
97569,I am getting the below error while trying to convert the datetime format: ValueError: time data '11/7/2016 11:51' does not match format '%d/%m/%Y %H:%M:%S.%f' Can some one suggest what this error mean?,319302
97591,nan,300687
97588,nan,317575
97594,nan,315464
98324,nan,319056
97592,nan,300687
96081,"Assignment says we need to identify time slots (early morning,late evening etc.) with highest problems. Any guideline on which hours to consider as early mornings, which as late evenings? Or we can have our own interpretations?",311686
97601,,300687
97632,after importing CSV file and check the column which has date data getting chaged day in place of month. how to handle this. Ex i have date values actually 10-07-2018 (DD-MM-YYYY) chaged to 07-10-2018 13-07-2018(DD-MM-YYYY) will be as it is this happens only for whare day and month between 1 to 12 Pls some one help how to handle.,315455
97637,nan,315464
97638,I have a column having mixture of both date and string type.Date is in &#39;12/3/2016&#39; and String is in 30/01/16.,320635
97644,Is there a Powerpoint presentation template to present the analysis done in the Uber Case Study?,318078
97651,"How to extract days from the format days HH:MM:SS example to extract a day from 09 days 5:50:5, here I want to extract the 9 because the days is 9",307843
97656,nan,308964
97658,nan,310419
97657,nan,318804
97661,nan,318329
97667,nan,318579
96094,The dataframe generated after reading the given csv contains values with two dfifferent formats in Request Timestamp (&amp; Drop Timestamp) column. Struggling with Converting them as &#39;DateTime&#39; in one go as formats are different in the same column. What I am missing?,311686
98337,nan,310611
98340,my jupyter notebook is not responding while using matplotlib after that. need to help. how to fix this issue,319444
97695,"""What do you think is the reason for this issue for the supply-demand gap? Write the answer in less than 100 words."" Where we have to write answer.? Is it jupyter notebook or any other word doc or in presentation to be submitted.? Can any TA's confirm.",317991
98346,nan,311466
98368,Import Used is &quot;import seaborn as sns&quot;,300721
98361,"I imported the dataset into Tableau, but in request time column, month is taken as date and Date is considred as Month. Can any suugetion here to resolve this issue please.",311741
98366,We can have any number of time slots that the data can be broken into. How many slots are required? What about the time ranges for each?,318366
98344,nan,315560
98372,,303674
98379,nan,303228
98385,Want to add the count number (height lables) on bar charts. What is best way to do this?,307495
98407,nan,318579
98406,"For analysing supply demand, which do you think is a more sensbile approach to aggregate the count of trips which did not have supply when there was demand - a) aggregate it over the 5 days and see how that varies across time slots b) aggregate it taking average count per day and seeing how it varies across time slots ?",318079
98398,nan,315464
98414,nan,308495
98421,Hi all in uber case study does the date contains only july data. so it does not make any sense to create month and day cols for analysis.. right?,301641
98418,nan,311169
97544,"after reading csv 'Request timestamp' data and applying pd.to_datetime, i can observe error in extracting correct day and month. example - before conversion data read as - 11/7/2016 11:51 after conversion with pd.to_datetime - '2016-11-07 11:51:00' Apply dt.month is returning as 11 and dt.date is returning 7 which is incorrect. it must be opposite/ how to fix it? Tried pd.to_datetime(df['Request timestamp'], format = %Y-%d-%m %H:%M) giving syntax error. please suggest thanks",306245
98440,How many time slots should we choose ? Should it one or more based on the magnitude of the gap ?,313691
98471,"alueError Traceback (most recent call last) &lt;ipython-input-476-aab8f1951ea9&gt; in &lt;module&gt;() ----&gt; 1 Uber.fillna(0) C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\frame.py in fillna(self, value, method, axis, inplace, limit, downcast, **kwargs) 3788 self).fillna(value=value, method=method, axis=axis, 3789 inplace=inplace, limit=limit, -&gt; 3790 downcast=downcast, **kwargs) 3791 3792 @Appender(_shared_docs['replace'] % _shared_doc_kwargs) C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\generic.py in fillna(self, value, method, axis, inplace, limit, downcast) 5425 new_data = self._data.fillna(value=value, limit=limit, 5426 inplace=inplace, -&gt; 5427 downcast=downcast) 5428 elif isinstance(value, DataFrame) and self.ndim == 2: 5429 new_data = self.where(self.notna(), value) C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals.py in fillna(self, **kwargs) 3706 3707 def fillna(self, **kwargs): -&gt; 3708 return self.apply('fillna', **kwargs) 3709 3710 def downcast(self, **kwargs): C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs) 3579 3580 kwargs['mgr'] = self -&gt; 3581 applied = getattr(b, f)(**kwargs) 3582 result_blocks = _extend_blocks(applied, result_blocks) 3583 C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\internals.py in fillna(self, value, limit, inplace, downcast, mgr) 2004 mgr=None): 2005 values = self.values if inplace else self.values.copy() -&gt; 2006 values = values.fillna(value=value, limit=limit) 2007 return [self.make_block_same_class(values=values, 2008 placement=self.mgr_locs, C:\ProgramData\Anaconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs) 176 else: 177 kwargs[new_arg_name] = new_arg_value --&gt; 178 return func(*args, **kwargs) 179 return wrapper 180 return _deprecate_kwarg C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\arrays\categorical.py in fillna(self, value, method, limit) 1754 elif is_hashable(value): 1755 if not isna(value) and value not in self.categories: -&gt; 1756 raise ValueError(""fill value must be in categories"") 1757 1758 mask = values == -1 ValueError: fill value must be in categories",318240
98442,"How to add data lables in Graph, e.g. In below graph, there is not data lables. I want to add data label on each bar, How to proceed it and what is the process.",306243
98447,nan,320603
98452,Any body was able to calculate wait time.. can someone please give me a hint to how to do that.I tried .shif() to compare previous and curent trip and then calculate watime using drop timestamp-request timestamp of previous and current row. but I am getting python errors,301641
98456,How to substract times in columns I am getting following error &lt;class &#39;datetime.time&#39;&gt; is not convertible to datetime,301641
97599,nan,308673
98460,Uber[Uber[Driver_ID]] = Uber[Uber[Driver_ID]].astype(int) But the message is --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-111-82350a29e139&gt; in &lt;module&gt;() ----&gt; 1 Uber[Uber[Driver_ID]] = Uber[Uber[Driver_ID]].astype(int) NameError: name 'Driver_ID' is not defined,318240
98462,"I have done group by on request hour to count request id as below: df_time=groupby('request hour')['Request_id'].count() but when trying to plot bar graph or plot using below command: plt.plot(df_time['request hour'],df_time['Request_id'], 'r*') getting error as : TypeError: an integer is required KeyError: 'request hour' any help here",317156
98472,"For Q2 -- gap between supply and demand and show the same using plots. It will be done only for trip is cancelled or No cars available, Right? . Why to do for Trip completed (No Demand-supply gap).?",311386
98477,nan,300706
98483,"Any idea,how can I plot subplots? Nothing seems to be working out.",314221
98465,"ValueError Traceback (most recent call last) &lt;ipython-input-234-c4ab9f78ee2e&gt; in &lt;module&gt;() ----&gt; 1 Pre_Morning = Uber.loc[(Uber.Request_Hour &gt;= 3) and (Uber.Request_Hour &lt;= 6)] C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\generic.py in __nonzero__(self) 1574 raise ValueError(""The truth value of a {0} is ambiguous. "" 1575 ""Use a.empty, a.bool(), a.item(), a.any() or a.all()."" -&gt; 1576 .format(self.__class__.__name__)) 1577 1578 __bool__ = __nonzero__ ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",318240
98484,"Hi, Can I submit RAR file instead of zip.?",317410
98497,"I have DF = groupby ( request day, request hour) and I have used groupby (status).value_count.unstack() on above DF to display complete information of request date, request hour followed by 3 column of status as completed, cancel &amp; not available.. I need help to plot all above info in one single plot. please suggest.. thanks.",306245
98518,"I had weird numbers showing us when calculating the trip time. Looks like the date format for few rows is DD/MM/YYYY and for few, it is MM/DD/YYYY. How do I make everything in one format? I tried to use strftime but this doesn't give me what I am trying",316253
98520,'str' object has no attribute 'time',314629
98535,nan,308641
99425,"when i try converting date and time into a common format, the ones with DD-MM-YYYY are being converted correctly to YYYY-MM-DD format, whereas the ones with DD/MM/YYYY are being converted to YYYY-DD-MM. even though i have mentioned dayfirst there, still it's returning the same format. how to resolve this",315831
98572,Can any suggest how can approch this question,305804
98576,In uber supply-demand gap. the columns that were to be changed to datetime format were &#39;Request timestamp&#39; and &#39;Drop timestamp&#39;.,315661
97287,AttributeError: module 'seaborn' has no attribute 'lineplot',318017
97218,What to do with NA values in Drop time stamp column? Can we leave them as it is or we need to remove ?,304693
97221,nan,307494
98669,"In Uber Dataset given, some columns are filled with NA but my thought is not to remove such data as They are NA because of some valid reasons like Drop Time is NA when Status = Cancelled or Driver Id is NA when No Cabs available. So is it the right approach?",315423
96748,"After conversion of the columns containing dates to datetime format, the NaN values are getting converted to NaT. Any implications of having the NaT values or should these be converted to NaN?",313826
98170,nan,311006
97258,nan,314313
97250,"I use matplotlib for plotting the data in this assignment. I wanted to increase my plot size. So i googled and got the below method for doing that. fig,ax =mpl.subplots(1,2, figsize=(15,4)); But whe i see the documentation on mpl.subplots() there is no argument present as &quot;figsize&quot;. But this is working for me. Can anyone help me undersntad where can i see more this hidden attribute &quot;figsize&quot;?",311741
99446,nan,318800
98203,Is there a minimum number of graphs expected for Uber Analysis or we can do it based on our understanding ? Like Status/Time Slot against counts etc,319876
98212,nan,319876
98213,nan,307708
97174,"The data column is non-null object and has Nan values.I was trying to use ~np.isnan to solve the Nan issue. But got an error that &quot;TypeError: ufunc &#39;isnan&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;",301114
99572,"I have some doubts regarding couple of feedback points though I believe I've answered them as summary in my assignment. Univariate and segmented analysis are done correctly and successfully identified the problems. Feedback: Results in numbers are not explicitly mentioned. The demand and supply are defined properly and the numbers are correct. Feedback: Results in numbers are not explicitly mentioned. Want to understand on what exactly needs to be put for above points before I apply for reval. The presentation has a clear structure, is not too long, and explains the most important results concisely. Feedback: Results in numbers are not explicitly mentioned. Not sure why the numbers are required for the presentation?",318329
100480,"In the python code, I see only simple bar graphs plotted. Is the stacked bar in the presentation plotted with python? if yes can some one share the code here. If no what tool is used (tableu?)",305842
99672,"I have taken very good care of putting the labels on the plots , putting percetage gap of supply at appropriate places and summarized the percentages of supply gap as a whole, at city, at airport and during early morning at city, late evening at airport. It's really dissapointing to receive feedback "" Results in numbers are not explicitly mentioned. "" after doing so much which shows that it wasn't been evaluated as it should have been. This is the second time I'm observing this (Raised reval for MySQL assignment as well). Regarding the feedback "" Hourly plot of data is missing. "" Is this not the hourly plot? How such a plot could be overlooked? I also do not understand the reason why numbers are required in presentation when it was mentioned to provide analysis in 100 words (Though I've put labels over the plots) The reason for me to post this on discussion forum is that there is no other way for me to communicate the concerns about Feedback and there is no 2 way communication except Reval which is only one way. The writeup for Reval, once submitted will not be shown anywhere.",318329
88479,"I have got basic understanding of tools (Python, R, Excel, SQL and Tableau) in prep course. I now want to get a hang of applying them by mimicking someone who has already applied the tools. I am not looking for examples, but a complete end to end application case study with an optimisation goal. Thank you.",308637
88550,,318723
88933,nan,317982
89013,Do you think we should have a session with students from previous cohort? How can we request this to Upgrad - IIIT?,318458
89650,nan,318732
90013,"Write a program that takes in two sorted vectors as input, and gives an output of a sorted vector  that is their union.Note: Use the two-pointer method only.(Any one got correct answer to this question)",301641
89307,"Reference material for python, and other important topics posted by the author in the above link.",317418
88821,You can keep this handy for your future references. (May be pin the post at top.) Encourage you all to add the links. Assessment - https://cdn.upgrad.com/UpGrad/temp/f39088ba-dbb2-453a-891a-188e22144cc5/Student+Assessment+And+Learning+Experience+Manual_DS+C8.pdf Community Guidelines - https://cdn.upgrad.com/UpGrad/temp/f7394d37-a28e-4455-b4a2-e451689f851f/Community%20Guidelines%20-%20DS.pdf Code of Conduct - https://cdn.upgrad.com/UpGrad/temp/ee897815-6e84-4226-815b-e8d0cf10c830/PGDDS%20Code%20of%20Conduct.pdf CRISP - https://cdn.upgrad.com/UpGrad/temp/3ecdc2f8-00d1-43e7-9058-4a92403bb812/Lecture%20Notes%20-%20CRISP%20DM.pdf,318458
92693,how and when we use Alter and Update Can alter be used in place of. update and vice versa ?,312357
124944,nan,310179
82268,"Just started my prep course starting from today, feels exciting. Hope to meet you soon on discussions board :)",312093
85179,"Hi everyone, I enrolled in the course just a few days back. I missed an opportunity to attend Bootcamp today at Bangalore :( Please give some insights on today's Bootcamp, anyone among you, who attended. Thanks in advance.",318364
77209,"Folks, while going through the preparatory content material, starting with R programming, I found that Python is also mandatory. I remember that we needed to go through and opt for either R or Python but the content seems to be designed for through both. Can you please validate this?",300717
82287,nan,310634
74870,"Hi All, Don&#39;t we have an option of learning both R and Python? If I had to choose one can someone really help in understanding which will be a better option? Thanks In Advance",300688
84779,nan,318319
81384,"R seems to be the tough one to learn and most of us are choosing python. Based on the pre education requirements, in the R track you have to read about python we well however in the Python track no need to read about R. Does that say something?",310617
81393,nan,310505
84820,nan,310508
81850,nan,312357
81859,It is one of the streams which required good knowledge and I am ready to dive into the ocean of knowledge so can any one has any hint about Data science blogs Thanxs In Advance Nikunj Sharma,312357
77024,I also have a question similar to Rohith Krishnamurthy. Can someone please let know if we choose anyone will the other topic be covered in detail once course starts!,303227
75903,"The first question is which one to pick? Though this question is already been raised, my concern is what is more important, shall we choose something becasue its easy right now! Or shall we look at from a broader perspective which can give us better advantage in the longer period of time. To give you a brief idea about the subject, i&#39;m completely alien to this field. I am an Architect by education and working as a professor right now. Pure curiosity has brought me into this. Can anyone pitch in with an answer looking at these points?",302734
77130,nan,303673
84932,nan,318576
84963,nan,318822
82502,"Just a brief introduction of mine, I am an MBA with 12 years of Food, GM and Apparel retail experience while working with top International and Indian retailers including Target, Reliance, Aditya Birla and retail planning for USPolo brand.",313767
83412,"This is my first day. In the opening session I chose python over R based on the info provided in the session. However, it mentioned that choosing python will need pre requisite in python only. Whereas, I could see that on choosing R will need pre requisite in python as well as R. Will I still be able to study the pre requisite of R even after choosing python? It would be great to have some basics about both.",315471
85283,"Started the Data Science journey, today! What do all of you think on the lanuage options for DS learning: Python or R? Gretaly appreciate varied feedbacks!",318827
85317,nan,315831
82639,"number&lt;-as.integer(readline(prompt=""Enter number"")) for(i in 1:number) {print(paste(number,""x"",i,""="",number*i))}",312063
83479,nan,315765
87005,friends... pls join Hyderabad WhatsApp group... send me a message @8125328608 and I will add you thanks,314162
85750,nan,320008
88711,,305847
87446,the same,317493
87761,nan,306996
90066,Anyone wants to create group by case study?,317990
90074,nan,318019
89850,"I tried looking for a students list where I could see the names and contact details(phone, email id, location) of everyone from our PGDDS Sep 2018 batch; but couldnt find it anywhere. Can the administrators please share it with everyone here? It will help us in knowing each other and forming groups for the group activities.",318762
88284,"Hi All, For SQL &amp; R, it shows only 67% completed, as the Practice questions in both the modules will be activated on 29th Sep. Is this why i dont get the ""Completed"" status on the course module.",316202
92611,nan,312357
123899,nan,318455
125095,nan,318846
85083,nan,318335
78757,nan,305843
82418,"I was trying to resolve the below problem from thnkpythonbook2 Suppose the cover price of a book is $24.95, but bookstores get a 40% discount. Shipping costs $3 for the first copy and 75 cents for each additional copy. What is the total wholesale cost for 60 copies? I tried to do it two ways but the second one is running succesfully. Here is the snapshot Can anyone tell me where is the issue?",302734
77935,nan,301124
82430,"If I leave my house at 6:52 am and run 1 mile at an easy pace (8:15 per mile), then 3 miles at tempo (7:12 per mile) and 1 mile at easy pace again, what time do I get home for breakfast? to solve this problem i have come this far to find out the total time. How to make the addition in hour minutes to find out the final time?",302734
77964,nan,305847
83548,After choosing the R or Python for Data Science there will be mathematics topics to learn. How can I get back to that page?,315661
83889,,315455
89376,nan,315121
88876,Youre given a positive number n as an input. Write a recursive program to print all the numbers from -n to +n (including 0),320073
111724,nan,318353
88691,http://thomas-cokelaer.info/tutorials/python/boolean.html,304338
90433,nan,318804
92637,"Hi , I am facing difficulties while using [ ] or ( ) in python coding. Anybody has any idea how to remember when to use [ ] or ( ) in syntax. Thanks, Nitin",300735
91426,In the Advanced SQL course company db is used. From where can I download company db?,301643
88678,check Testcase #1 (sample) Status Passed Execution time 0.34s CPU 0s Memory 6MB Description Testcase passed! The solution's output matches the expected output. Annotation Right answer. Input alpha Solution output YES Expected output YES keyboard_arrow_up close Testcase #2 (weight: 1) Status Failed Execution time 0.33s CPU 0s Memory 6MB Description Testcase failed! The solution's output doesn't match the expected output. Input beta Solution output YES Expected output NO,304338
89232,I am doing one project that deal with multi indexing. How can i create multi indexing in dataframes,318461
112491,"Hi Guys, I want to write some messages and results of data analysis in a text file. Is there a way to write a log file in Python?",301650
89267,"&gt; Here the data set given has march in the months field. But on sorting in ascending order, the data is ordered from April onward. Why is this so? Also for sort_values, its ascending order true by default? (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html)",317149
116442,In parts of this lecture - it was stated that if lambda = 0 - it results in overfitting. In ohter pat - its stated that lower values of alpha 1 and 2 results in underfitting. I am unable to comprehend.,319759
118062,nan,311466
118422,"Few queries on scaling process in car price example: 1) Why is scaling done before train/test data split? this impacts the test accuracy 2) what kind of scaler is &quot;scale&quot;? 3) Since, scaling is done on the entire X data, even dummy variables seem to be scaled...why is this?",310509
116596,In terms of formula how different are they and how the sum is equal to TSS ( in formula terms)?,318814
116597,In case of Lasso and Ridge regression . . we still needs to do RFE or feature elimination before performing Ridge and Lasso regression ? Although Lasso regression provide the significance of the feature.,308965
117995,nan,301890
118442,nan,303085
117642,nan,300733
118058,Im trying to understand whats written in the content but cant understand it to clearly Error and Regularization Contours Diagram,311466
118064,Car pricing example in lecture pd.DataFrame(model_cv.cv_results_) cv_results = cv_results[cv_results[&#39;param_alpha&#39;]&lt;=200] cv_results.head(),311803
118505,nan,317984
117308,nan,317575
115987,"Here we are adding either squares or absolute value of the coeffients to the minimisation equation. But don't understand the logic, how does complex equate to large. For example: 5.825^2 will be roughly equate to 6^2, so how are we ensuring that the model is not too complex",304022
115989,Professor mentioned adding lambda I to the X^T*X has an added advantage that an inverse will exist for that matrix. But wouldn't X*X^T create a square matrix which can be inverted.,304022
138769,nan,300721
117324,"Till now in the course we have been suggested to do scaling of features after train-test split. Then why in the example of car-price prediction using ridge and lasso, scaling is done before train-test split.",309451
117051,nan,310467
117075,Why cannot we apply the backward selection method for n&lt;p where n is the no. of data points and p is the no. of predictors.?,304319
117077,nan,314092
116126,"In regularization, we are trying to minimize the sum of squares of coefficients to avoid the model becoming complex. And in the video at one place the &#39;complexity&#39; is defined as no. of bits to store the coefficient i.e. 5.7785*x1 is more complex than 5*x1. But how minimizing the sum of squares of coefficients reducing this complexity? If I understood correctly it will stop coefficients from taking large values like 50 or 100 but not stop them from taking small values like 5.7785 as it&#39;ll try to minimize the absolute values of the coefficients. And from the above mentioned concept of complexity 5.7785*x1 will still be complex. I think I&#39;m really missing something here.",311686
117875,How do i know which regression model (Lasso or ridge) suffer with loss of data?,306734
118209,Can someone explain what are the limitations in crisp ? Not understanding the limitions of it from the video.,306735
137984,nan,310533
117135,TA - please explain How RSS varies with lambda??,316349
116211,nan,303674
117459,nan,317575
116233,"In the chapter ""ridge and lasso regression in Python"", professor mentioned that alpha is the hyperparamter. But in previous modules he mentioned alpha as regularization term and lambda as hyperparameter. Which one of them is the hyper parameter?",310467
122163,"I was going through the solution of advanced regression assignment and found that before performing Ridge and Lasso regression, scaling of vars have not been done. Can anybody explain why ?",300735
116241,"The Mean Absolute Error (or MAE) is the sum of the absolute differences between predictions and actual values. It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting). How can a &#39;Mean absolute error&#39; value be a negative value? Why is it called &quot;negative mean absolute error&quot;?",310467
117157,What is the relation between RSS and variance of the model ? As lambda increases RSS increases - what will happen to the variance?,319759
116270,nan,310419
117867,Have calculated Ridge and Lasso co efficients .. how to proceed from here ?,319759
116362,nan,308673
117510,"the co-efficients which are obtained by running ridge or lasso regression, are they co-efficents of the different features? the result is in the form of an array, so how to link it back to the features?",310509
117910,How to choose hyper parameter values? What are the basis to choose it ?,318427
117916,How do we determine an accurate value of alpha from the plots of scores (negative absolute mean error) vs alpha? Is it just by looking at the trends in the graph plotted? Does it require trying out different alpha &amp; then determining the best fit,316147
117879,"In one of the video of RFE, the prof says that one pipeline can be do Lasso Regression using RFE to get a pruned data set and then apply ridge regression on that. What does this mean?",304319
118034,What is the significance of number of observations in backward and forward stepwise selection ?,305650
118033,"Why Backward Stepwise Selection cannot be applied, as a full model when n&lt;p ?",305650
120861,nan,308964
126340,"it is mentioned that When we increase the value of &lambda;, the error term will increase and regularisation term will decrease and the opposite will happen when we decrease the value of &lambda;. However, it is also mentiioned that high value of lambda means highly regularised term and vice versa? are the above 2 not contradictory?",310509
116490,Can someone please explain the term 'function of the attributes' in this question ?,317149
116489,nan,300733
116518,nan,300733
118107,"# converting X to a two dimensional array, as required by the learning algorithm X_train = train.Year.reshape(-1,1) #Making X two dimensional y_train = train.Consumption X_test = test.Year.reshape(-1,1) #Making X two dimensional y_test = test.Consumption ~\Anaconda3\lib\site-packages\pandas\core\generic.py in __getattr__(self, name) 4374 if self._info_axis._can_hold_identifiers_and_holds_name(name): 4375 return self[name] -&gt; 4376 return object.__getattribute__(self, name) 4377 4378 def __setattr__(self, name, value): AttributeError: &#39;Series&#39; object has no attribute &#39;reshape&#39;",305804
118047,nan,304813
117279,"If no, why is it not linear? y=ax1+bex2+x3+csin(dx4) Can anyone explain the answer for this question?",318321
116021,How do we determine if an individual feature is linear or not? Functions can be determined to be linear or non linear,317996
116042,nan,310419
116054,"How can e&minus;x3.cos(x1) be used as one of the features for modeling y as a function of the attributes? My understanding is that in a linear regression problem, &#39;y is linearly related to the coefficients&#39; -- meaning that only two operations can be applied between the coefficients - 1) Multiplying them by constants (i.e. the features) such as a11(x),a22(x) and 2) Adding the terms with each other such asa11(x)+a22(x). What we cannot do is multiply them together, raise to one another&#39;s power, etc. That is, you cannot have terms such as a0.a1,a3log(a5),aa32 etc. Please clarify.",310511
116156,"If I have a data set containing ""days"" on the x axis and ""number of customers"" on y axix, and If I am looking to draw the best fit line, then which is the best method? Shall we use polynomial feature ( say of degree 3) to plot the curve, or shall I use the curve_fit function from scipy an pass Michaelis-Menten: Y=AX/(B+X) funtion to the curve_fit? What exactly is the use case of Polynomial and Curve_fit?",304814
125585,I also eliminated the VIF value . Kindly suggest as i believe the model is not so good to go .....,314612
116080,"Example : If we have raw attributes are x1, x2, x3 and x4...... and features are sin(x1) , e^x2,e^-x3 and e^x4. Can we say all raw attributes, features, multiplication of two or more features are used for modeling y as a function of the attributes?",310419
117832,What are loss functions ? what does this means ? ** minimise some loss function **,311466
116238,How to differentiate Cos/Sin wave from polynomial function curve ?,319759
116244,nan,312756
117158,Why can features can not be created from dependent / target variables?,318427
117852,"# converting X to a two dimensional array, as required by the learning algorithm X_train = train.Year.reshape(-1,1) #Making X two dimensional y_train = train.Consumption X_test = test.Year.reshape(-1,1) #Making X two dimensional y_test = test.Consumption AttributeError: &#39;Series&#39; object has no attribute &#39;reshape&#39;",308638
116288,nan,312093
116290,"y=ax1+bex2+x3+csin(dx4) .Why is the equation not considered linear, if it is not linear why is it on account of csin(dx4) and not because of bex2+x3",310629
117207,"In this module, it has been mentioned that multiplying raw attributes by a constant or adding them is a linear. I believe even subtracting raw attributes is also linear, can anyone please confirm on this.",314730
117225,nan,313526
108359,nan,312608
108404,nan,317073
107271,What is the app used in this session video to modify beta0 and beta1? Why is it that they have values from -15 to -8 and from 0.025 to 0.07. Is it preset or will we be allowed to set the two limits for beta0 and beta1?,304389
107493,nan,308774
109076,I am finding it little hard to understand the concept of gradient descent and when do we care about this?,301649
107123,If machine learning is broadly divided into three parts Regression Classification and clustering Then why logistic regression is called regression which solves the classification problem?,318585
107533,nan,305804
107560,"While taking the product to find the best fit, why are we taking P5 for maximum instead of 1-P5, but we are doing it another way around for P6? i.e. Multiplying P5 and but for P6 it is (1-P6) to calculate the maximum. Please clarify.",311115
108231,nan,314818
108249,,319869
108271,nan,308673
107936,nan,308495
108166,nan,314313
107937,P ( Diabetes) = 4 * P (No Diabetes) In the lecture it showed the Percentage of person having diabetes is = P(Diabetes) = 0.8 (80%),310179
106449,"in Linear Regression the cost Function was OLS and we Had to minimize that, but here we have to maximize the Cost Function (Likelihood Function ). What is likelihood? Is it likelihood probability of A variable being &#39;Yes&#39; or 1",317984
107926,nan,310179
107930,The curve is made from the data received for the diabetes patient right. But how do we get initial values of B0 and B1.,310179
107933,What are the observations found for the graphs (Sigmoid cuve) while changing the values of B0 and B1. Can you all please list it down here which helps to learn the curve more better.,310179
115651,nan,304025
107385,nan,311254
110087,nan,320251
107378,"From ""Finding the Best Fit Sigmoid Curve - II"" --- I was just playing with the sigmoid curve app and trying to understand how the curve is said to be a better fit ? Can I say the below curve is also a better fit for the same diabetic data ? ""This curve is alos a better fit. It has many big yellow bars, and even the small ones are reasonably large. Just by looking at this curve, can we tell that it will have a high likelihood value.... ??? "" If ""No"" ... why ?",312479
114607,nan,312448
108316,"I tried to do the Tenure_woe_data+.xlsx for compuation of woe and iv values are not matching for queation Questions:5/6 Information Value What is the total information value of both the variables? they gave values Contract = 1.24 , Tenure = 0.83 i am not getting proper. i am not sure something wrong. anyone got same . Please help .. Hope this is not graded question.",312019
107423,I've one-hot encoding which converts each category to a dummy variable (0 or 1) and now when the categories are more in number we will think before using it. (*more number means in my view more than 50). Now for example in the training data we have lets say 32 categories and we are fine with encoding it and in test data we are unaware of how many more categories it may have. Then is there any other way of encoding that we can use?,318328
112438,nan,301644
108409,"The demonstration used here shows that goods and bads are already fed in the excel file, but there is no formula behind it.",318335
108321,WOE pattern of the groups post binning - coarse classing Some explain how to do this what exactly Not able to get understand from material.,312019
108672,Variable distribution stability and Population Stability Index (PSI) sound similar. What is the difference between them?,320074
109672,nan,303085
115787,There is a section in Industry Applicaiton -II which talks about Model tracking and governance and different software packages used for this. Where can I find more details on : 1) Model Tracking and Governance 2) Can you provide names/links to any packages that are commonly used for this?,318438
108461,It will be helpful if somebody will give some useful link to understand this concept as I'm not able to understand it.,315423
108323,nan,307710
108296,"For the question What is the total information value of both the variables? Total value for Contract IV is given as Total IV for Contract = IVbucket-1(month-to-month) + IVbucket-2(One-year) + IVbucket-3(two-year) . I had thought that we just sum up IV of all buckets for total IV. What is the significance of -1(month to month) , -2(one-year) etc. I am missing something",317996
108451,Cost function in linear regression minimise the error term Sum(Actual(Y)-Predicted(Y))^2 but logistic regression uses maximum likelihood method for maximising probabilities.,310179
108457,nan,318751
108382,how do we interpret the IV? it says it should be high - how to define high? is it always in the range of 0-1? what is the best way to express the model and explain to business managers?,310509
107975,nan,318455
108200,"Since WOE is based on the target variable (Good/Bad Loans) and further coarse bucketing takes a step forward to see a linear relationship, we have already established this relationship between dependent and independent variable. So by using WOE aren&#39;t we manipulating the model and inducing the algorithm to learn the pattern we have established already?",304022
113485,nan,305847
114771,nan,301643
108437,nan,317073
108257,nan,320636
109432,"In statsmodels.api, there are couple of ways to implement Logistic Regression. 1. Logit 2. GLM Can someone please explain the difference between these two implementations?",318084
107437,"for eg. in the previous lesson, &quot;furnished&quot; was dropped and only two columns &quot;semi-furnished&quot; and &quot;unfurnished&quot; were used in creating the model. what if &quot;furnished&quot; is a strong predictor of &quot;price&quot;. How would an analyst interpret that from the created model?",317575
107826,"Here in the data preparation second video, we had used the dummy variables in two different methods. can I know why to use to two different methods and what basics can we know which methods can be suitable for the specific categorical columns",304692
105925,nan,308962
107081,In case of Outliers if we observe how we need to handle. In this example presenter said there is no outlier and moved further. but what / how to handle if it is available.,315455
107520,"Logistic+Regression+-+Telecom+Churn+Case+Study Dropping highly correlated dummy variables X_test = X_test.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No', 'StreamingTV_No','StreamingMovies_No'], 1) Could you please some explain how these are highlly correlated variables ? Because in heatmap i could not see more value.",312019
107545,In my previous question on linear regression case study i had asked that can we drop the co -related values. But most of the answers what i got was not to drop them and use them in model and decide to drop them or not using VIF? So does it mean that we can drop the highly co-related ones? Can anyone explain Link to previous discussion on linear regression : https://learn.upgrad.com/v/course/208/question/106099,301114
109399,"In first quiz we are asked to tick mark insignificant variables based on the summary statistics, how the p-value is calculated to 0.05",307496
107549,"num_telecom = telecom[[&#39;tenure&#39;,&#39;MonthlyCharges&#39;,&#39;SeniorCitizen&#39;,&#39;TotalCharges&#39;]] num_telecom.describe(percentiles=[.25, .5, .75, .90, .95, .99]) While reviewing telecom data if you find sudden jump between intervals, how are you expeected to handle these outliers? Do you delete those rows? Do you impute those rows with median or average values?",317514
108272,nan,301890
108129,nan,314612
108363,I thought family = sm.families.Binomial() would help directly bring out a yes/no or 1/0 answer but it brought the probabilities. We had to explicitly convert the probabilities into 1/0. I tried removing the family argument and the outcome appeared to be the same. Any thoughts on its significance especially in the model built?,318007
108142,Can someone please explain the rationale behind taking logarithm for calculating the woe? Can't we just work with %??,318084
108411,nan,311864
107611,RFE output variables/features changes when you run the code from a different system/os. Ensure you order the variables alphabetically before it is passed to RFE. Please let me know if anybody faced this issue. Please refer the link below. https://datascience.stackexchange.com/questions/27062/why-the-rfe-to-select-features-is-changing-when-the-os-or-computer-is-changed,310467
108339,https://learn.upgrad.com/v/course/208/session/25993/segment/133973 Why was the MultipleLines_Yes not removed though it had a high p value ?,312259
108011,"On what basis do we decide to use Standard scaler method for the Telecom Churn problem. Since all the other columns have values 0 and 1, won't the Min-Max scaler would be a better option?",304319
107897,Sometimes we use statsmodel as well as the RFE sometimes does not. What is the key for the same?,300721
108066,nan,304397
108201,"It is mentioned that it is advisable to maintain class balance (50/50 is ideal) but then is it not defined by the data itself? what if we have a dataset where a mjority of the data always yields a particular value , say &gt;90%, then why would we need to &quot;balance&quot; the same?",310509
107902,How is it that Total Charges showed no null values when we checked initially while using the code telecom.info() and it showed 11 values later?,304022
109019,"After merging all the daraframes when we check total charges column, there are no null values. But post creation dummy values, we see that Total charges 11 rows has null values. Am i missing anything here?",307495
107004,"In the video, Month-to-month has count of 3875, however gets dropped ; same with Bank Transfer which has a count of 1544 what was the basis why first value was dropped? In MultipleLines_No phone service, the drop was justifiable. in this case the counts are higher to be ignored. Why was it still ignored / dropped ? any thoughts? dummy1 = pd.get_dummies(telecom[['Contract', 'PaymentMethod', 'gender', 'InternetService']], drop_first=True) telecom.groupby(""Contract"").count()['customerID'] Contract Month-to-month 3875 One year 1473 Two year 1695 Name: customerID, dtype: int64 telecom.groupby(""PaymentMethod"").count()['customerID'] PaymentMethod Bank transfer (automatic) 1544 Credit card (automatic) 1522 Electronic check 2365 Mailed check 1612 Name: customerID, dtype: int64",309211
107905,"We know by common logic, that internet plan (how many gb data subscribed to) can be an important variable, but we don&#39;t see that in the Internet dataset. Do we assume that other variables such as Monthly Charges, Total Charges, Internet variables(as mentioned in the internet data-set) cover that bit and having another variable would be collinear anyway Or do we try compute that derived variable?",304022
107575,"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100) why do we set random_state=100?",310467
115634,nan,300733
107956,"os = pd.get_dummies(telecom[&#39;OnlineSecurity&#39;], prefix=&#39;OnlineSecurity&#39;)",318335
108342,How to develop a model in class-imbalance case. If my data set is baiased towards one catogory of data what should be the approch to building the model.,311741
113263,What could be the reason for this issue? and how to overcome this?,310522
107517,"In session 2, multivariate logistic regression telecom churn example, even though multiple lines yes feature had pvalue of 0.087 which is higher than 0.05 it was still not removed...in the next step VIF is checked and then Total charges is removed....there is no concrete rule in manual feature elimination I am observing.. Sometimes feature is eliminated based on only vif, sometimes only based on p-value...what exactly is the rule???",308437
108027,nan,305845
107445,"A lot of online discussions, forums and articles have the following definition of One-Hot and Dummy Endoding: One-Hot Encoding : Given a Categorical Variable with &#39; n &#39; categories -&gt; creates &#39;n&#39; columns. Dummy-Encoding : Given a Categorical Variable with &#39;n&#39; categories --&gt; creates &#39;n-1&#39; columns. However, in the Upgrad videos, One-Hot Encoding and Dummy-Encoding are mentioned as synonymous to each other meaning One-Hot Encoding is same as Dummy-Encoding where in a Categorical Variable with &#39;n&#39; categories creates &#39;n-1&#39; columns.. Hence the confusion. Is there a difference between One-Hot Encoding and Dummy-Encoding? If yes, what is the difference and when to use which type of encoding?",313826
114922,nan,310179
113181,"While doing manual model building for logistic regression , we tend to take care of VIF values evertime. What is afte removing some variable having high VIF value , there is a significant change in P value of some other variable. Do we need to drop the column with high p value then?",318427
114923,nan,310179
115437,"********Current Implementation - Using Logistic Regression ******************** - Requirement : To understand if a customer will churn or not. Hence a binary decision problem - Method: Create a logistic model based on a sigmoid curve using statmodel-GLM ; family = binomial - Output - Probability of churn - choice of threshold: based on accuracy/specificity/sensitivity intersection - evaluation: based on AUC *********Alternate approach - Log ODDS based implementation*************** - Method: Create a linear model, using RFE for feature selection. - Output : Log of Odds. Find Odds by doing e^(output value)(rounded to 2 places) - Threshold - Determine the churn rates using all thresholds from 0-99. Calculate the accuracy/specificity and sensitivity as in above and find the intersection - Accuracy - Adj R2 *********************************************************************************** Specifically I am trying to understand, if implementing the Telecom Churn using Linear Regression based on the above proposed manner give us equivalent results to logistics regression? The underlying reason is I am trying to judge whether the use Linear Regression replace Logistic Regression, practiacally, in binomial problems?",305839
108467,TN,311864
108441,Can anyone clearly explainsthe concept and also in business terms as well.,310179
108242,Model 1- TPR= 100 % FPR = 67% Model 2- TPR=50% FPR= 0.00% Please explain why?,300721
109632,nan,303085
107484,,303674
109090,nan,316211
107865,"def draw_roc ( actual, probs ): fpr, tpr, thresholds = metrics . roc_curve( actual, probs, drop_intermediate = False ) auc_score = metrics . roc_auc_score( actual, probs ) plt . figure(figsize = ( 5 , 5 )) plt . plot( fpr, tpr, label = &#39;ROC curve (area = %0.2f )&#39; % auc_score )",301643
108099,nan,310611
108252,if we consider Not Churn = False and Churn =True in Actuals colum Then red colored ones shoul be chane like False Negetives in 2st cell and below it True Negetive. Please help me to understnd it better.,301110
108160,nan,310467
108080,nan,304692
108041,"What is the use of writing ?precision_score in the code, I saw this in the last video of precision and recall section of Multivariate Logistic Regression.",301655
107945,"Is it always the case that accuracy, sensitivity and specificity will converge/intersect at a single point? Are there cases where only any two of them intersect at a single point?",308435
136385,In Logistic Regression what does the transformation of non-linear features into linear features to use them for logistic regression mean?,315028
137292,"Have come to a revision of logistic regression, looking at below statement: ""This is because the denominator of precision, i.e.(TP+FP) is not constant as these are the predicted values of 1s. And because the predicted values can swing wildly, you get a very jumpy curve. "" Aren't all the values TP, TN, FP, FN all dependent on predicted values? Why only jumpy curve for precision then?",318598
78415,,305847
87100,why this assigning and appending does not work in a single line in python it works in every other language,312050
90170,"Its not a question.. but the demonstartion by Prof. G.S for comparison of ""Computation Times in NumPy and Standard Python Lists"" by creating a code to calculate the actual time taken was pretty cool feature of Python to see..",310508
81314,"Though i installed anaconda 3.x version and completed the installation as mentioned in the document , still i am unable to get anaconda navigator app in the windows button. since how many times i tried as it is but unable to go the anaconda navigator icon. can anyone help me out please",305844
87280,,314313
81869,After watching the video the 1st question on the prep course was to remove the blank space from the string; &#39; This is my first code.&#39; But I didnot get the pass. This i swhat i see. I cant understand where it went wrong,302734
78218,&gt;&gt;&gt; a=&#39;12345678&#39; &gt;&gt;&gt; a[::-2] this was the o/p &#39;8642&#39; Please explain the execution of above scenario,300693
81336,When i am trying any python programme in the jupiter notebook i am geting the error &lt;built-in method lstrip of str object at 0x7f19a6ccb300&gt; do i need to import any package to run those inbuilt functions. As i am using the latest version of jupiter notebook but still i am facing this error. Is it related to error in loading the in built functions ?,305844
85021,"This is not a question but wanted to share something which is good to have, not sure how many of you already know this,are you gus aware of Copies Shallow in case of nested List? Here it goes as: A = [[1,2],[3,4]] B = A[:] By above you will get in Python A is B --&gt; False A == B --&gt;True You will also see A[0] = [1,2] and B[0] = [1,2] of you print them same for A[1] and B[1] Now if you change A[0] as A[0] = [8,9] then if you print A[0] it will show you [8,9] but B[0] will still point to [1,2] If you append in A[1] as A[1].append(5) THEN both A[1] and B[1] will print [3,4,5] This is called Copies Shallows in terms memory view point. Thought of sharing this useful information as someties it gets tricky while working in real scenarios. If you like it please upvote :) Thanks!",304813
84680,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1)#Type your answer here set_2 = set(list_2)#Type your answer here answer_1 =set_1.difference(set_2) #Type your answer here answer_2 =set_1.symmetric_difference(set_2)#Type your answer here print(answer_1) print(answer_2)",308639
81342,When I click on this jupyter notebook in my Anaconda navigator it opens up a local host server on my browser n then I had to upload a new file of code on which I have to work. Am I missing anything in terms of software installation?,300721
84174,In the session c = list(b) print(&quot;c is: {0}&quot;.format(c)) what does the second line specify,310385
84181,nan,310624
84717,"t = 12345, 54321, &#39;hello!&#39; # This is Tuple Packing and x, y, z = t # This is Sequence Unpacking. Does this unpacking will assign the tuple, t to all the 3 variables x,y,z ? Or does it mean something else.",317070
81892,"Hi all, the below code has worked and accepted but my query is ?is the logic right or its just a luck and go through? import ast,sys input_str = sys.stdin.read() first_name =input_str[6:10] #write your answer here second_name =input_str[0:5] #write your answer here customer_code =input_str[11:15] #write your answer here print(first_name) print(second_name) print(customer_code)",300687
77636,nan,304397
78790,Is it when it is used inside print?,301890
87151,So in the above snippets i run the code manually everytime to replace &#39;R&#39; with &#39;SAAS&#39;. Is there an alternative for the same,314313
80298,"check Testcase #1 (sample) Status Passed Execution time 0.30s CPU 0s Memory 1MB Description Testcase passed! The solution&#39;s output matches the expected output. Annotation Right answer Input {&#39;Jack Dorsey&#39; : &#39;Twitter&#39; , &#39;Tim Cook&#39; : &#39;Apple&#39;,&#39;Jeff Bezos&#39; : &#39;Amazon&#39; ,&#39;Mukesh Ambani&#39; : &#39;RJIO&#39;} Solution output [&#39;Amazon&#39;, &#39;Apple&#39;, &#39;RJIO&#39;, &#39;Twitter&#39;] Expected output [&#39;Amazon&#39;, &#39;Apple&#39;, &#39;RJIO&#39;, &#39;Twitter&#39;] keyboard_arrow_up close Testcase #2 (weight: 1) Status Failed Execution time 0.34s CPU 0s Memory 6MB Description Testcase failed! The solution&#39;s output doesn&#39;t match the expected output This is the code i am using input_dict = {&#39;Jack Dorsey&#39; : &#39;Twitter&#39; , &#39;Tim Cook&#39; : &#39;Apple&#39;,&#39;Jeff Bezos&#39; : &#39;Amazon&#39; ,&#39;Mukesh Ambani&#39; : &#39;RJIO&#39;} sorted_x = sorted(input_dict.values()) print(sorted_x)",305790
81449,nan,310611
84810,"Dear Alok sir, my code is executed succesfully but when I verify it showing the error. Kindly look the below code and advice where I have made the mistakes import ast,sys input_str = sys.stdin.read() input_dict = ast.literal_eval(input_str) value_list=list(input_dict.values()) #Type your answer here print(sorted(value_list))",308639
84280,nan,317070
90124,"for &#39;I love Data Science &amp; Python&#39;. Used input_str.split(&#39;&amp;&#39;) but it is not accepting answer, Please help",321850
78896,What is the significance of it?,305845
78895,What does DA_languages[:] do?,305845
88492,Why is the syntax in Join function different to syntax on Split function in Python?,310509
81536,"Hi, I'm unable to get the correct expected output for following question in Python module, even after trying multiple approaches. has anybody come across this? I've tried following approaches: tuple_1=list(input_tuple) tuple_1.append('python') tuple_2=tuple(tuple_1) print(tuple_2) tuple_2=list(input_tuple) tuple_2=insert(3,'python') tuple_2=tuple(tuple_2) print(tuple_2) tuple_2=list(input_tuple) tuple_2 += ['python'] tuple_2=tuple(tuple_2) print(tuple_2)",306011
75981,nan,301651
84869,"Code: import ast,sys input_str = sys.stdin.read() input_tuple = ast.literal_eval(input_str) # Write your code here in_list=list(input_tuple) out_list=in_list.append(&#39;Python&#39;) tuple_2 = tuple(out_list) # Make sure to name the final tuple &#39;tuple_2&#39; print(tuple_2) Error:",310505
81555,"Input [[&#39;SAS&#39;,&#39;R&#39;],[&#39;Tableau&#39;,&#39;SQL&#39;],[&#39;Python&#39;,&#39;Java&#39;]] Solution output Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 5, in &lt;module&gt; answer =input_str([2][0])#Type your answer here TypeError: &#39;str&#39; object is not callable Expected output Python import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) answer =input_str([2][0])#Type your answer here print(answer)",300687
80168,strip function,308437
80169,Strip function,308437
83649,nan,310585
88758,nan,314221
81671,nan,300721
88685,"Navigator Error An unexpected error occurred on Navigator start-up Report Please report this issue in the anaconda issue tracker Main Error expected str, bytes or os.PathLike object, not NoneType Traceback Traceback (most recent call last): File &quot;C:\Users\Admin1\Anaconda3\lib\site-packages\anaconda_navigator\exceptions.py&quot;, line 75, in exception_handler",317156
81677,"For eg- # This program shows various identities str1 = &quot;geek&quot; print ( id (str1)) str2 = &quot;geek&quot; print ( id (str2)) ouptut- 140252505691448 140252505691448 other eg of list is- print(DA_languages) print(id(DA_languages[2])) print(id(DA_languages[3])) output- [&#39;Scala&#39;, 42, &#39;R&#39;, &#39;R&#39;, &#39;Java&#39;] 545828562624 545828562624",300721
80668,"In Python, list comprehensions seemed tough to understand (atleast for me). Any good tutorials? Please suggest",301109
80523,nan,308634
80525,"I am not getting the desired output which is [&#39;Amazon&#39;, &#39;Apple&#39;, &#39;RJIO&#39;, &#39;Twitter&#39;]",303968
87210,"Extract Python from a nested list input_list = [[&#39;SAS&#39;,&#39;R&#39;],[&#39;Tableau&#39;,&#39;SQL&#39;],[&#39;Python&#39;,&#39;Java&#39;]] answer = nest(input_list[2][0]) print(answer)",306728
84783,"d = {&#39;vehicle&#39; : &#39;car&#39;, &#39;type&#39; : &#39;sedan&#39;, &#39;model&#39; : 2017} print(d) print(list(d.keys())) Error : {&#39;vehicle&#39;: &#39;car&#39;, &#39;type&#39;: &#39;sedan&#39;, &#39;model&#39;: 2017} --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-92-357a097a3965&gt; in &lt;module&gt;() 1 d = {&#39;vehicle&#39; : &#39;car&#39;, &#39;type&#39; : &#39;sedan&#39;, &#39;model&#39; : 2017} 2 print(d) ----&gt; 3 print(list(d.keys())) TypeError: &#39;list&#39; object is not callable",317070
81702,"pimport ast,sys input_str = sys.stdin.read() input_tuple = ast.literal_eval(input_str) # Write your code here list1=list(input_tuple) list1.append(&#39;Python&#39;) tuple_2=tuple(list1) # Make sure to name the final tuple &#39;tuple_2&#39; print(tuple_2)",300687
81705,"My code get executed but while submitting its rejected please advise import ast,sys input_str = sys.stdin.read() input_dict = ast.literal_eval(input_str) value_list =list(input_dict.keys()) #Type you answer here print(sorted(value_list))",300687
88770,"I was going through numpy in phyton, and when we are slicing the second row of a matrix, the command given in the study material is print(array_2d[1, :]). I tried using the command without using "":"" - print(array_2d[1, ] and it fetches the same result. Why are we using "":"" to slice the elements in 2 dimensional array.",307493
87233,"lets say Instead of Kumar_Ravi_003, the input is Jha_Gopal_01.",316132
79282,,305847
83528,,308639
85175,"In below example we are getting output as first 2 lists. I need to understand how the [0:2] works here. If my understanding is correct the list0= [1,2,3,4], list1=[5,6,7] and list 2=[8,9,10] so I was expecting the answer as [ [1,2,3,4], [5,6,7], [8,9,10] ]. I think I am not clear with indexing in this case. Please help",311870
80545,nan,308437
87666,nan,318347
83672,,310519
83717,"As per quiz 5 in question 2 in Pythin Lists topic, the output should be an empty list but when I ran it, I recieved a syntax error. Can anyone guide me the correct answer. The question is if word = [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;], what will be output of the below. word[:] = [] print(word)",314730
83803,nan,315560
85191,In the link &quot;https://learn.upgrad.com/v/course/208/session/15860/segment/80229&quot; it is written to download the video &quot;The file used in the video can be downloaded from below. We recommend that you download this file and follow the same along with the video&quot;. How can we do this? I&#39;m not able to locate download option in web browser.,318429
83804,nan,315560
80746,nan,304813
83184,nan,308432
83431,"Hi when i try to execute below its giving error lst= [[&#39;SAS&#39;,&#39;R&#39;],[&#39;Tableau&#39;,&#39;SQL&#39;],[&#39;Python&#39;,&#39;Java&#39;]] print(lst[2][0]) I am getting Python as output as expected, but in sample question its throwing an error",300716
83820,,308639
85781,"Please let me know the difference between single quote and double quote while defining Python strings and lists. For example: Both the below definitions return the string value in single quotes only. Why Python treats a string in single quotes and not in double quotes? String_1 = ""Python is a versatile programming language"" String_2 = `Python is a versatile programming language' Both the above string assignments return the output in single quotes as below. 'Python is a versatile programming language' Similarly, in lists as well. list_1 = [""Vinod"", ""Ravi""] list_1[0] 'Vinod' list_2 = ['Vinod', 'Ravi'] list_2[0] 'Vinod'",314730
80530,"i have converted list into sets and calling the function difference still it shows an error. set_1 = set(list_1) set_2 = set(list_2) answer_1 = set_1.difference(set_2()) answer_2 = set_1(setdiff(set_2)) print(answer_1) print(answer_2) Input [[1,2,3,4,5,6],[2,3,4,5,6,7,8,9]] Solution output Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 9, in &lt;module&gt; answer_1 = set_1.difference(set_2()) TypeError: &#39;set&#39; object is not callable Expected output {1} {1, 7, 8, 9}",303968
89468,nan,307708
85249,&quot;.Ipnyb&quot;,315277
83361,nan,312376
89940,How to print the columns of a dataframe?,316891
90635,"Given an even integer n, create an n*n checkerboard matrix with the values 0 and 1, using the tile function.",310008
79572,"While using append function : Error: ,Traceback (most recent call last): File ""/code/source.py3"", line 4, in &lt;module&gt; input_list.append('SPARK') AttributeError: 'str' object has no attribute 'append' why is the list here considered str or what exactly this error refers to",300684
83568,,310419
83569,nan,310585
85353,nan,308431
83419,"Why my codes first get submitted and after some time gets rejected? When i verify code, everything seems fine and when i run code same, fine. But when i submit my code, its gets accepted first and then after some time get rejected. Python gets changed to Penthon..",302735
86470,nan,320688
83252,"Hi, I was working on the test problem where we are expected to find the symmetric difference between two sets and compare it to the solution. The solution was i.e {a, b, c, d} whereas my output was something like {a,c,b,d} and that is why I was not able to verify the code. After trying a few times the real sequence showed up {a,b,c,d} and was able to verify the code. What could be the reason for this and how to tackle such issues? Thanks A",312490
83393,new_list=DA_languages DA_languages.append(&#39;Java&#39;) print(new_list) give result with &#39;Java&#39; appended,310598
88814,"Getting an error stating &#39;Nonetype&#39; object is not iterable. Here,I am converting list &#39;a&#39; back to tuple and it is displaying an error. tuple_2=tuple(a) TypeError: &#39;NoneType&#39; object is not iterable",314221
86985,"How to extract the specific word from nested list! Extract Python from a nested list input_list = [[&#39;SAS&#39;,&#39;R&#39;],[&#39;Tableau&#39;,&#39;SQL&#39;],[&#39;Python&#39;,&#39;Java&#39;]]",303227
83130,"Hello friends, this is nakul here can anyone help me , in python jupyter notebook how to do assign the mathematical expression. For e.g, addition, substraction, multiplication and assigning values and etc.",308639
85446,nan,317990
85455,How to solve the test case 2 in all the programs,316891
85449,nan,317980
85469,Can someone please help me with the correct code....,318446
87253,"You have a list of email ids. The username for each email consists of all characters occurring before the @ symbol. You will need to write a code that will return a True or False after accepting user's input on the email id and check if the usernames is in the list of usernames or not. If it is present print True, else print False.",316545
85489,"Tried using Shift+Tab in Mac, but its not giving any response.",318335
85462,"We are given DA_languages = [&#39;R&#39;,&#39;Python&#39;, &#39;SAS&#39;, &#39;Scala&#39;, 42] print(DA_languages[ : -3]) The output as per the lesson is: [&#39;R&#39;, &#39;Python&#39;] According to me it is wrong answer. The correct answer will be [&#39;R&#39;, &#39;Python&#39;, &#39;SAS&#39;] because -3 index is &#39;SAS&#39; Can someone point out the mistake in my thought process?",318077
85491,input_str = &#39; This is my first code&#39; print(input_str.lstrip()),318353
93767,How to get the difference of two dataframes without converting it to set ?,314092
84127,"In the 1st Python question, it is asking to remove the leading spaces from the input string. Below is the question: Python_1 Description Remove the leading spaces from the string input_str = &#39; This is my first code&#39; My code for this is below: input_str=&quot; This is my first code&quot; input_str.strip() final_str = &quot;This is my first code&quot; I had verified and ran the code. It passed the sample Test. However, it failed while submitting. The reason is due to Sample Test 2. In Sample Test 2, the input given is &quot; This to my first code &quot;. I have given my input as per the question. Can some one help me if i am missing something. Thanks in advance.",316202
85520,My DoSelect python IDE is not working ! It always shows ' Sorry! Please Try again' on 'Run Code' Option . Anyone facing the same issue? Please help ! Neither Running the code nor Verifying it is posible.,318557
80210,"Hi Friends, Please help me, every time i put the code and it scuccessfuly passed through validation but in submission it shows below error. check Testcase #1 (sample) Status Passed Execution time 0.30s CPU 0s Memory 3MB Description Testcase passed! The solution&#39;s output matches the expected output. Annotation Right answer Input (&#39;Monty Python&#39;, &#39;British&#39;, 1969) Solution output (&#39;Monty Python&#39;, &#39;British&#39;, 1969, &#39;Python&#39;) Expected output (&#39;Monty Python&#39;, &#39;British&#39;, 1969, &#39;Python&#39;) keyboard_arrow_up close Testcase #2 (weight: 1) Status Failed Execution time 0.31s CPU 0s Memory 3MB Description Testcase failed! The solution&#39;s output doesn&#39;t match the expected output. What is the issue, you can see the expected output is matching with solution output. Thanks!",305790
85685,"Description Split the string input_str = 'Kumar_Ravi_003' to the person's second name, first name and unique customer code. In this example, second_name= 'Kumar', first_name= 'Ravi', customer_code = '003'. A sample output of the input 'Kumar_Ravi_003' is: Output Ravi Kumar 003 Hi all, the below code has not worked and rejected but my query is ?is the logic right and if not then which logic will help here. Testcase2 is getting failed. import ast,sys input_str = sys.stdin.read() first_name = ""Ravi"" second_name = ""Kumar"" customer_code = ""003"" print(""{0}\n{1}\n{2}"".format(first_name,second_name,customer_code))",320195
86505,Jupyter notebook is not opening. Command prompt says that it is not recognised as internal or ecternal command. Any suggestions?,318404
85586,nan,318353
85613,,317980
87365,nan,315793
90443,nan,311046
87357,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1) set_2 = set(list_2) Line_1 = set_1.difference(set_2) Line_3 = list(Line_1) Line_2 = set_2.symmetric_difference(set_1) Line_4 = list(Line_2) answer_1 = format(Line_3) answer_2 = format(Line_4) print(answer_1) print(answer_2)",316132
87376,nan,312357
85634,In the input cell when i executed python code it added a sequence number starting from 1. PFA for the image as well. Thanks in advance,318370
88840,"I need some clarity on one of the practice question that appears under &quot;List&quot; section. &quot; Description Convert a string input_str = &#39;I love Data Science &amp; Python&#39; to a list by splitting it on &lsquo;&amp;&rsquo;. The sample output for this string will be: [&#39;I love Data Science &#39;, &#39; Python&#39;] &quot; Answer that I wrote for the question is as below but I am not getting the desired result. import ast,sys input_str = sys.stdin.read() output_list = input_str.split( &quot; &amp; &quot; ) print(output_list) The output of the above query is coming as below; Input I love Data Science &amp; Python Solution output [&#39;I love Data Science&#39;, &#39;Python&#39;] Expected output [&#39;I love Data Science &#39;, &#39; Python&#39;] Can someone help in correcting my answer? Thanks! PKR",317857
85747,nan,320008
86064,nan,318723
93773,How to check if a object is series or dataframe in python ?,301108
88903,nan,310611
86063,"Var=&quot;I love Python programming&quot; print(Var[-11:-1]) When I compile this, I am getting &quot;Programmin&quot; as output instead of &quot;Programming&quot;. Where am I going wrong?",310617
90370,nan,318083
89760,nan,320603
86159,"When the professor was giving the demo, if any of you had observed, why is that, input order is not retained in the output of the dictionary? Height element appeared in the last",314084
84087,nan,313228
86155,"Find the difference, using difference and symmetric_difference, between two given lists - list1 and list2. First, convert the lists into sets and store them as set_1 and set_2. Then store the difference and symmetric difference in answer_1 and answer_2 respectively. Print both the answers as sorted lists, i.e. convert the final sets to lists, sort it and then return it.",318429
88951,"Though autosave option is an important feature, I find it really annoying if it keeps on saving things I dont intend to every 30 secs or so. I tried checked few suggestions available online but no positive effect. Any workarounds or suggestions appreciated.",309451
86168,"Anyone using python jupyter note book for coding , Installed from Ananconda. Seems very slow almost non responsive . Anyone is facing the issue. Your early help will be highly appreciated. Please also suggest any other alternatives for python IDE ( good one) ...",311227
88957,,308966
86223,https://www.youtube.com/watch?v=q_BzsPxwLOE and simplified Jupyter notebooks : https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks,318741
85373,"Split the string input_str = &#39;Kumar_Ravi_003&#39; to the person&#39;s second name, first name and unique customer code. In this example, second_name= &#39;Kumar&#39;, first_name= &#39;Ravi&#39;, customer_code = &#39;003&#39;. I am using the below code : import ast,sys input_str = sys.stdin.read() first_name =&quot;Ravi&quot;; second_name =&quot;Kumar&quot;; customer_code =&#39;003&#39;; print(first_name) print(second_name) print(customer_code) When verifying and running the code it is giving me a successful message. However, when submitting the code it got rejected. Getting the below error when submitting the code Testcase failed! The solution&#39;s output doesn&#39;t match the expected output. Can someone help me with this, please",300688
88961,nan,318455
83864,"import ast,sys input_str = sys.stdin.read() input_dict = ast.literal_eval(input_str) answer =input_dict.get(&#39;Name&#39;:&#39;Monty&#39;, &#39;Profession&#39;:&#39;Singer&#39;,&#39;Label&#39;:&#39;NA&#39;)#Type your answer print(answer)",308639
86229,nan,314197
86235,Here is simple code I have written to lstrip. input_str = &#39; This is my first code&#39; print(input_str.lstrip()) testcase#1 is succeeded Testcase#2 after submit is not correct and failed. Please advice here ...,318846
84113,nan,312019
86352,nan,316926
90244,nan,303115
86342,I have install Anaconda post that I Clicked over it and select Anaconda Navigator to launch the Juypter Notebook. But when it opens up in browser it ask me token or password?How can i get rid of this and launch Juypter Notebook,312623
86279,"Hi, The query to using split function is not working in Jupyter notebook while it is working fine on upgrad&#39;s platform, do we need to first import any classes in the Jupyter notebook to make &quot;Split&quot; function work?",315277
86355,"When i am trying to run below code - its taking time to execute and finally showing error ""Sorry! Please try again!"" Code import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] set_1 = set(list_1) set_2 = set(list_2) answer_1 = sorted(set_1.difference(set_2)) answer_2 = sorted(set_1.symmetric_difference(set_2)) print(answer_1) print(answer_2) Anyone facing same issue and how to resolve same.",320103
89976,nan,315423
89969,"ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",317156
87194,"For example, a paragraph is: "" Hello Mr. Scott, today I am not well. So kindly inform Mr. Peter that I won't be able to come today.""",318851
85153,"Question is Description Split the string input_str = 'Kumar_Ravi_003' to the person's second name, first name and unique customer code. In this example, second_name= 'Kumar', first_name= 'Ravi', customer_code = '003'. A sample output of the input 'Kumar_Ravi_003' is: Ravi Kumar 003 Note that you need to print in the order first name, last name and customer code. ---------------------------------------------------------------- My Answer: import ast,sys input_str = sys.stdin.read() first_name = ""Ravi"" second_name = ""Kumar"" customer_code = ""003"" print(first_name) print(second_name) print(customer_code) ------------------------------------------------ I have only added names (ravi and kumar) and code (003) otherwise everything was ready made code.. not understading why the testcase2 failed here.. and the test case 2 appears weird with spelling mistakes.. please have a look at the snapshot..",316349
87425,"Extract Python from a nested list input_list = [[&#39;SAS&#39;,&#39;R&#39;],[&#39;Tableau&#39;,&#39;SQL&#39;],[&#39;Python&#39;,&#39;Java&#39;]] if i use print(input_list[2:]) i&#39;m getting python and java . how can remove java from there?",302743
87429,My net connection at home is very poor. Can i download the videos when i have a good connection and watch them later? Becasue of high nuber of students the net speed is really poor inside the campus and taking any other connection is not allowed. Hence I am stuck with a poor connetion that runs very slow. I need to know is there any way to download the videos from the webpage on my laptop so that i can see them later on. Thank you,302734
87428,I mean we are not specifying in the program that we are using x and y to refer elements of the list yet it is refering to that list. What happens when we rais an ambiguity like having N no of list and want a lambda function to perform on all the list. How will we specify x and y to refer which list at a time.,320688
89103,"I thought of sharing this rather interesting way the strip method works when we pass a set of characters. The strip method continues to remove the matching leading and trailing characters till it encounters a mismatch on both the leading and trailing side of a string. For example: str1 = ""I Love Python"" print(str1.strip(' ItPoynh')) Output: Love str1 = ""www.google.com"" print(str1.strip('wmoc.')) Output: google",314730
89107,"I could see both intersection and &amp; operator return the same result in SET operations. Similarly, union and | operator return the same result. Is there any difference between them. See the below example:",314730
87281,,314313
87418,"listo=(&#39;mom&#39;,&#39;dad&#39;,&#39;baby&#39;,&#39;baba&#39;,&#39;fun&#39;) listo.append(&#39;mama&#39;) it is giving following error:- --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-65-de04057113e6&gt; in &lt;module&gt;() 1 listo=(&#39;mom&#39;,&#39;dad&#39;,&#39;baby&#39;,&#39;baba&#39;,&#39;fun&#39;) ----&gt; 2 listo.append(&#39;mama&#39;) AttributeError: &#39;tuple&#39; object has no attribute &#39;append&#39;",318791
87493,In below snippet I have below lines of code - What is the meaning of function Id(ListName) ?,312623
87579,"As per https://docs.python.org/3/tutorial/datastructures.html. Refer- Using list as stack &amp; Using list as queue For both stack &amp; queue, append function adds the item in the end. For stack , the append should add the item on the left instead of right end . Even for pop function , stack should remove the elements from the left instead of right end .Please correct me if I am missing something.",311119
89788,nan,306242
89789,nan,306242
87648,,318347
87676,What does the second statement in this code suggests?,318751
89816,nan,305804
87743,nan,307496
87747,nan,315114
89268,"Hi Guys, I came across a Handy commonly used commands/functions "" Python for Data Science"".",300691
86101,While writing code for trimming space from the beginning of the string &quot;input_str.lstrip()&quot; I noticed that while on &quot;input_str.l&quot; a prompt opens up which showing &quot;keywords&quot; and &quot;locals&quot;. I tried running the code with some of these only to get errors. If it is not relevant to &quot;input_str&quot; why does it pop up anyway. Or am I getting ahead of myself?,319898
88236,"I am trying to covert a tuple to a list, add one element to the list and convert it back to a tuple. While doing this I am getting error as below. Please help me in removing this error. &quot; Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 8, in &lt;module&gt; list2 = tuple(list2) TypeError: &#39;NoneType&#39; object is not iterable &quot;",304816
88234,nan,307488
90799,nan,301120
89370,"When i try to execute word[-12:-18] it doesn&#39;t work, but word[-18:-12] works. Also if i run the word[-12:-18:-1] it only returns &quot;nohty&quot; and not &quot;nohtyp&quot;.",308440
89381,nan,319846
89397,nan,306244
87404,I am using coding console for the first time. Whenevr I verify/run the code it works but when I submit the code it gets rejected.,300699
88288,"can anyone explain the use of ID function DA_languages = ['R','Python', 'SAS', 'Scala', 42] print(id(DA_languages)) when and where is this function used ?",300687
90036,nan,318082
87311,"See below line of code and its output- print(""This is my first line of code in python"") print(""This is my second line of code in python"") print(""This is my third line of code in python"") print(""This is my fourthline of code in python"") print(""Hello\nThis is in the next line"" =============Output===================== This is my first line of code in python This is my second line of code in python This is my third line of code in python This is my fourthline of code in python Hello This is in the next line In the above even though I have not provided \n in any of the first four lines still my texts got printed on new lines for every print statement. Unless we provide \n it should not print text on the next line, right? Just wanted to know here how is the behaviour of the Print() statement in the python.? In other languages like java- Print() - function will print text in the same line Println() - function will print text in the next line",312623
89715,nan,317558
89425,nan,320633
91622,nan,311046
90420,having problem with out put. Solution output 0 1 1 4 2 9 3 16 dtype: int64 Expected output 1 1 2 4 3 9 4 16 dtype: int64 help me out,310008
94815,"val=dict(Name=&#39;pallu&#39;, Age=[25, 27])",314629
90459,"Given three data frames containing the number of gold, silver, and bronze Olympic medals won by some countries, determine the total number of medals won by each country. Note: All the three data frames dont have all the same countries. So, ensure you use the fill_value argument (set it to zero), to avoid getting NaN values. Also, ensure you sort the final dataframe, according to the total medal count in descending order.",310008
91743,do we have any plugin that can be used to auto-predict the method we are using in IDE like when we are typing str.split() it should recognise and autocomplete the method or give suggestion.,301108
90540,Description Group the data 'df' by 'month' and 'day' and find the mean value for column 'rain' and 'wind'.,310008
90590,Can you explain me with example where we need to use expand=True in split method and where we don't. I am bit confused in it.,318319
88923,"If we want to return the maximum string element in a list based on the length of the string, we need to use the syntax ""max(&lt;list&gt;, key = len)"". In the below example, the string elements ""Earth"" and ""Water"" have the same length which is 5. names = ['Earth','Air','Fire','Water'] In this case when I executed the below statement print(max(names, key=len)) The string element ""Earth"" is returned. Why ""Water"" is not returned? What additional criteria Python considered to return ""Earth"" as the max string element in this case?",314730
93918,"For code below, which works correctly in python - ls = ['a','b','c','a','d','t','s','g','s','y','q','l','n','d'] d=dict() for v in ls: if d.get(v): d[v]+=1 else: d[v]=1 I am trying to use comprehension as below - ls = ['a','b','c','a','d','t','s','g','s','y','q','l','n','d'] d=dict() d[v]+=1 if d.get(v) else d[v]=1 for v in ls But I am getting this error - File ""&lt;ipython-input-85-13228a183fae&gt;"", line 3 d[v]+=1 if d.get(v) else d[v]=1 for v in ls ^ SyntaxError: invalid syntax What is wrong in my code?",318458
95116,"As I uderstnad, raw strings can be defined by appending r before the sting as below. raw_stmt = r'\n This is a raw string' raw_stmt Output: '\\n This is a raw string' But, what is raw string and how is it different from regular strings?",314730
102729,"DA_Lang=[&#39;R&#39;,&#39;Python&#39;,&#39;SAS&#39;,&#39;Scala&#39;,42] 1) print(DA_Lang[:-3]) o/p [&#39;R&#39;, &#39;Python&#39;] 2)print(DA_Lang[::-3]) o/p [42, &#39;Python&#39;]",301114
142184,"Take any list, example below. list1 = [&#39;x&#39;, &#39;g&#39;, 1, &#39;good&#39;] try to append and print as below. print(list1.append(&#39;Y&#39;) Output: None Now, just do &quot;print(list1)&quot; Output [&#39;x&#39;, &#39;g&#39;, 1, &#39;good&#39;, &#39;Y&#39;] Can anybody explain the logic here.",314730
81422,--------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-4-072a80255adc&gt; in &lt;module&gt;() ----&gt; 1 print(DA_languages[-1]) NameError: name 'DA_languages' is not defined,306011
78093,nan,305847
80628,I had started with the tutorials of prep courses with R. And reading all the discussions in the group its clear that Python would be the mode of chosen language. So now is it necessary to complete R before going for the Python tutorials.,302734
79885,nan,308437
80591,nan,307709
83573,nan,310585
86433,"Hi Guys, Has anyone worked on projects like these? are there tools that can give these results? or is it pure coding???https://flowingdata.com/2015/12/15/a-day-in-the-life-of-americans/",307709
83572,nan,315560
87255,can we get input parameters and output parameters for non sample test cases?,317073
129603,A few of the learners get this error in the coding question when do-select gets blocked by their network. Please do the following steps to resolve this: 1. Go to Chrome settings by typing ` chrome://settings ` in the address bar. 2. Go to Content settings 3. Go to cookies 4. Check if 3rd party cookies are blocked 5. If blocked then add api.doselect.com and dosel ect.com in allowed and/or enable third-party cookies. Then refresh the page. 6. If not blocked then report the error.,301618
79317,nan,306038
87117,if input_str[0]==&#39;a&#39; or &#39;e&#39; or &#39;i&#39; or &#39;o&#39; or &#39;u&#39; or &#39;A&#39; or &#39;E&#39; or &#39;I&#39; or &#39;U&#39;: print (&quot;YES&quot;) else: print(&quot;NO&quot;),318335
90406,Please help me understand what&#39;s wrong in the attached code.,316084
81930,"d = {0, 1, 2} for x in d: print(d.add(x)) the output is nonenonenone as i excuted in jupyter but can anyone explain how it works",300687
81329,"Question Using the function Map, count the number of words that start with S in input_list. input_list = ['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] My Solution count = len([x for x in list(map(lambda x: x if x[0]=='S' else 0,input_list)) if x!=0]) print(count)",310974
83913,nan,315560
84705,nan,308432
84175,"How can the code take the n value dynamically.. one of test case passes but the pther doesn&#39;t .. can someone help. import ast, sys n = int(input()) # Write your code here (remember to print the list) list = [n**2 for n in range(1,5)] print(list)",300727
84264,nan,315028
84797,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) from functools import reduce answer =list(reduce(lambda x,y:x if x&gt;y else y,input_list)) print(answer) I am not getting error in the above code.",300690
81458,nan,308437
81477,"Unable to get desired output which is : Input [&#39;6&#39;,&#39;7&#39;] Expected output 279936",303968
84713,,310585
82111,for eg- if a==b if c==d print(&quot;wow&quot;) else: print(&quot;Try Again&quot;) else: ..... ......,300721
88506,"Code: check = list(map(lambda x : x if (x[0] == &#39;s&#39; and x[-1] == &#39;p&#39;) else pass,input_list)) I just want to get the value of x when &#39;if&#39; is true and do nothing in else part, so I used &#39;pass&#39; but this throws an error message. Can someone tell me how to go about this?",312160
84893,nan,310585
81652,Map function code,308437
81688,"Using the Reduce function, concatenate a list of words in input_list, and print the output as a string. If input_list = [&#39;I&#39;,&#39;Love&#39;,&#39;Python&#39;], the output should be the string &#39;I Love Python&#39;.",308962
82450,"Please share the code- i have also done, please tell what is wrong?",305129
79614,,305847
81961,new_num_list = [] 10 for x in num_list: ---&gt; 11 if (int(x[0])%2 == 0) and (int(x[1])%2 == 0) and (int(x[2])%2 == 0) and (int(x[3]%2) == 0) : 12 new_num_list.append(x) 13 print(new_num_list),308634
88541,"This is the part of the code snippets given in the handbook for Comprehensions : squares_list = [x**2 for x in range(1,10)] print(squares_list) when i do Esc and then Ctrl+Enter to run that cell it is not showing anything . Can somebody please guide me?",318801
87250,"# Write your code here (remember to print the list) #define a blank list that will contain the list of squares at the end list_squares = [] #start a loop that will square every number from 1 till n for x in range(1,n+1): list_squares.append(x**2) print(list_squares)",318335
79999,"Map Function Description: Using the function Map, count the number of words that start with &lsquo;S&rsquo; in input_list.",302738
88576,"Below mentioned code is in sample Functions notebook whichgives output as square of number passed as argument , need to know the use of print(&quot;sqr_4 is&quot;+str(sqr_4)) def sqr(num): out = num**2 print(out) sqr_4= sqr(4) print(&quot;sqr_4 is&quot;+str(sqr_4))",307496
83314,What&#39;s wrong with my following code: vowel = &#39;aeiou&#39; if input_str[0].lower() in vowel: print(&quot;YES&quot;) else: print(&quot;NO&quot;) It worked for the sample test case: &quot;alpha&quot; and also other test cases that I have tried. But it got rejected in the submission for the case &quot;beta&quot;. Could anyone please explain?,312507
80436,nan,302742
87640,"Filter Function Description You are given a list of strings such as input_list = [&#39;hdjk&#39;, &#39;salsap&#39;, &#39;sherpa&#39;]. Extract a list of names that start with an &lsquo;s&rsquo; and end with a &lsquo;p&rsquo; (both &#39;s&#39; and &#39;p&#39; are lowercase) in input_list. import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) sp=filter(lambda x,y: x[0]==&#39;s&#39; and y[-1]==&#39;p&#39;) print(sp,input_list) Input [&#39;soap&#39;,&#39;sharp&#39;,&#39;shy&#39;,&#39;silent&#39;,&#39;ship&#39;,&#39;summer&#39;,&#39;sheep&#39;] Solution output Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 4, in &lt;module&gt; sp=filter(lambda x,y: x[0]==&#39;s&#39; and y[-1]==&#39;p&#39;) TypeError: filter expected 2 arguments, got 1 Expected output [&#39;soap&#39;, &#39;sharp&#39;, &#39;ship&#39;, &#39;sheep&#39;]",304389
87595,"Write code to check if the string in input_st starts with a vowel or not. Print capital YES or NO.For example, if input_st = &#39;analyti&#39; , your output should print &#39;YES&#39; Below code and let me know where it is wrong the_vowel = &quot;aeiou&quot; if myword[0].lower() in the_vowel: print(&#39;YES&#39;) else: print(&#39;NO&#39;) I want to know why the code is given error",307494
80920,"list_city=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] def funct(): if word[0]=='S': return (word) count=len(list(map(funct,list_city))) print (count)",305655
89449,"* * * * * * * * * * * * * * * * * * * * * * * * * str1='' for i in range(0,9): if i&lt;5: str1 += '* ' print(str1) else : str1 = str1[:-2] print(str1)",318372
80922,"list_city=['San Jose', 'San Francisco', 'Santa Fe', 'Houston'] def funct(x): if x[0]=='S': return (x) count=list(map(funct,list_city)) print (count)",305655
80891,nan,300690
80876,"students_data = {1:['Shivam Bansal', 24] , 2:['Udit Bansal',25], 3:['Sonam Gupta', 26], 4:['Saif Ansari',24], 5:['Huzefa Calcuttawala',27]} names_dict ={} #iterate over each key, val pair for roll_num,details in students_data.items(): if roll_num%2==0: names_dict[roll_num]= details[0] print(names_dict)",305655
82367,Can anyone please share the code for this question pls.,305129
80894,nan,300690
80914,,300690
85246,"Can someone please explain why x[0] = &#39;S&#39;? Ideally it points to the complete item like &quot;San Jose&quot;. But both the below command works same count = sum(list(map(lambda x : x[0] == &#39;S&#39;, input_list))) count = sum(list(map(lambda x : x[0][0] == &#39;S&#39;, input_list)))",301650
87602,nan,315793
80780,,305847
80699,"What&#39;s wrong in this coding, showing invalid syntax in the first line my_word = input(&quot;Enter your word&quot; : &#39;&#39;) a = str(my_word).upper() b = a[0] if b in (&#39;A&#39;,&#39;E&#39;,&#39;O&#39;,&#39;U&#39;,&#39;I&#39;): print(&quot;Yes&quot;) else: print(&quot;No&quot;)",305790
86374,"I have used : sum(list(map(lambda x : 1 if x[0] == 'S' else 0, input_list))) Is there any other way",318554
85308,"str1=&#39;&#39; for i in range(0,9): if i&lt;5: str1 += &#39;* &#39; print(str1) else: str1 = str1[:-2] print(str1) in this in if part we can increasing counter str by 1 and print * but the str1 = str1[:-2] s not clear",310385
88786,"list_vowel =[] number_vowel = [&#39;a&#39;,&#39;e&#39;,&#39;i&#39;,&#39;o&#39;,&#39;u&#39;] input_list=[&#39;wood&#39;, &#39;old&#39;, &#39;apple&#39;, &#39;big&#39;, &#39;item&#39;, &#39;euphoria&#39;] for word in input_list: if word[0] in number_vowel: list_vowel.append(word) print(list_vowel) my code",315560
83423,"As Python is not a pure functional programming language, it will have side effects. As a programmer, where should I be careful with state when writing a program in Python? Can you please provide me some examples?",308637
83686,"greater = lambda x,y : x if (x&gt;y)==True else y print(greater(4,7)) will give result 7 but if my code was greater = lambda x,y : print(x) if (x&gt;y)==True else print(y) print(greater(4,7)) result will be 7 None what is the difference in using print()",310598
86439,"d = {0, 1, 2} for x in d: print(d.add(x)) I tried running above and got output as:- None None None Can someone explain why output is coming as None.",320103
83289,"input_list=[&quot;sort&quot;,&quot;order&quot;,&quot;some&quot;] starts_with = lambda x: x[0] == &#39;s&#39; rslt=filter(starts_with,input_list) print(list(rslt)) Above line of code creates a list of such words with given input_list but when I try to execute print(len(lrslt)) in the next line it give error, when I use list function on the filtered object &#39;rslt&#39; it has converted it into a list so now I should be able to use the len function as it operates on list but it failed why?What function I can use to check the count?",300721
83750,nan,310585
85364,nan,303084
85737,nan,317156
88830,I have written code on quiz screen its getting passesd on Verify but geting rejected on submission pls share the correct solution,307496
80804,"Hi can anyone help me with this import ast,sys input_str = sys.stdin.read() word = input(&quot;Enter string : &quot;) x = word[0] if x in (&#39;a&#39;,&#39;e&#39;,&#39;i&#39;,&#39;o&#39;,&#39;u&#39;,&#39;A&#39;,&#39;E&#39;,&#39;I&#39;,&#39;O&#39;,&#39;U&#39;): print(&quot;YES&quot;) else: print(&quot;NO&quot;) I am getting below error Enter string : Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 3, in &lt;module&gt; word = input(&quot;Enter string : &quot;) EOFError: EOF when reading a line",306738
87530,nan,306996
85471,"if n == 100: print(""Perfect"") elif 90 &lt;= n &lt; 100: print(""Distinction"") elif 65 &lt;= n &lt; 90: print(""First Class"") elif 40 &lt;= n &lt; 65: print(""Second Class"") else: print(""Failed"")",308639
81266,nan,308437
81212,"How to Create a lambda function 'greater', which takes two arguments x and y and return x if x&gt;y otherwise y. If x = 2 and y= 3, then the output should be 3.",307843
85677,nan,318353
85648,"n = int(input()) when we run this, it should prompt for an input. but, its not happening like that.",318353
82349,"Input [&#39;All&#39;,&#39;you&#39;,&#39;have&#39;,&#39;to&#39;,&#39;fear&#39;,&#39;is&#39;,&#39;fear&#39;,&#39;itself&#39;] Solution output [&#39;A&#39;, &#39;l&#39;, &#39;l&#39;, &#39; &#39;, &#39;y&#39;, &#39;o&#39;, &#39;u&#39;, &#39; &#39;, &#39;h&#39;, &#39;a&#39;, &#39;v&#39;, &#39;e&#39;, &#39; &#39;, &#39;t&#39;, &#39;o&#39;, &#39; &#39;, &#39;f&#39;, &#39;e&#39;, &#39;a&#39;, &#39;r&#39;, &#39; &#39;, &#39;i&#39;, &#39;s&#39;, &#39; &#39;, &#39;f&#39;, &#39;e&#39;, &#39;a&#39;, &#39;r&#39;, &#39; &#39;, &#39;i&#39;, &#39;t&#39;, &#39;s&#39;, &#39;e&#39;, &#39;l&#39;, &#39;f&#39;] Expected output All you have to fear is fear itself ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) from functools import reduce p=reduce(lambda x,y:&#39; &#39;.join(input_list),input_list) #write your code here # print() print(list(p)) please advise where am i ginh wrong? any better solution is appreciated.thanks",300687
86169,"Short ciruit and: If first expression before &quot;and&quot; is False, rest of the condition won&#39;t be evaluated. Short circuit or: If first expression before &quot;or&quot; is True, rest of the condtion after or wont be evaluated. Logical and/or: Full expression will be evaluated. Any idea, how python works?",320074
86185,"lis2=[11,22,33,44,55] funct=lambda x,y: x if x&gt;y else y print(list(reduce(funct,lis2)))",313767
96664,"I have used below code: f1 = lambda x:x[0].lower() == &#39;s&#39; f2 = lambda y:y[-1].lower() ==&#39;p&#39; sp=filter(f1,input_list) sp2=filter(f2,input_list) sp3=set(list(sp)) sp4=set(list(sp2)) sp5=sp4.intersection(sp3) sp6=print(list(sp5)) the output I am getting is [&#39;soap&#39;, &#39;sheep&#39;, &#39;ship&#39;, &#39;sharp&#39;] the expected output is [&#39;soap&#39;, &#39;sharp&#39;, &#39;ship&#39;, &#39;sheep&#39;] the order is different.",311046
86225,"I know the syntax of def and lambda and the use of lambda in map, filter and reduce functions. Apart from this, I understand that it is a temporary function created for a limited context without a name and does not exist until the lambda line of code is executed. But what exactly is the practical usage of lambda compared to def. I am not sure though why the founders of this construct did not use def itself if they want to use it in map, filter and reduce functions.",314730
86226,nan,317990
86192,"I see, &quot;if else&quot; needs to be written upfront for &quot;for&quot; loop. And if we have, only &quot;if&quot; and no else, we need to write &quot;for&quot; followed by &quot;if&quot;. I wonder, how to write &quot;if else&quot; in case of nested for loop? Will &quot;if else&quot; come before first &quot;for&quot; loop or inner &quot;for&quot; loop? Or should be mention it last? Assume, if else belongs to inner for loop.",320074
86728,"Websites, learning material, applying Python for data analysis, real-world examples etc.",312376
89936,def proper(some_text): some_text = some_text.strip() some_text = &quot; &quot;.join([word[0].upper() + word[1:] for word in some_text.split()]) return some_text captain = proper(&quot;mahendra singh dhoni&quot;) print(captain) can someone explain why the line some_text = some_text.strip() is written in above code. If we omit this line then also function returns same value.,316084
89766,,320603
86360,d={A:x*3 for x in 'abc' } print(d),318427
87277,,318335
87436,"n = 5 dictionary = (key:key**2 for key in range(1,n+1)) print(dictionary)",302741
89064,nan,320687
87385,,314313
89101,is there a particular scenario where i should use normal for loop and list comprehension. is there a harm if i use list comprehension in case of any for loops.,308784
87678,nan,318404
87730,,318347
89666,Getting syntax error for this code..... kindly assist in figuring out the error,315464
89311,"am not able to understand how to solve this problem ...please help Using the Reduce function, concatenate a list of words in input_list, and print the output as a string. If input_list = ['I','Love','Python'], the output should be the string 'I Love Python'. why cant' we simply use the join function?",310509
90781,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) mul5 = lambda x: x % 5 == 0 list_answer =filter(mul5, input_list) #Write your answer here print(list_answer)",303115
89812,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_vowel = [item for item in input_list if item[0] in &#39;aeiouAEIOU&#39;] print(list_vowel)",319876
88679,"While reading more about Python, I came across this function called ""XRange"". I found it to be quite useful and interesting from a Data Scientists' point of view, and thus thought should share it with all. Basically, Xrange does the same thing as what range does, but uses substantially less amount of memory and is faster than range. this is because xrange generates the numbers during runtime. So, while working on large sets of numbers (common for datascientists), this can save substantial memory and also make the program 10 folds more efficient. I shall play around with this function tomorrow and share the statistics of the difference it creates. Reference: https://code-maven.com/range-vs-xrange-in-python",317998
89342,nan,315560
88238,I am not able to understand how the value is 9996. The output should be 9999. Could anyone help me .,311119
86217,"Create a function squared(), which takes x and y as arguments and returns the x**y value. For e.g., if x = 2 and y = 3 , then the output is 8. My code: import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) x = int(input_list[0]) y = int(input_list[1]) def squared(x,y): return(x*y) print(squared(x,y)) error coming in upgrad editor: Testcase #1 Status Failed Execution time 0.30s CPU 0s Memory 1MB Description Testcase failed! The solution&#39;s output doesn&#39;t match the expected output. Input [&#39;6&#39;,&#39;7&#39;] Solution output 42 Expected output 279936",318429
89398,"For mentioned Quiz question i have used below mentioned code and got correct answer but need to is there any other precise code to this or not c = list(map( lambda x: x[0]== &quot;S&quot;,input_list)) #Type your code here count= sum(c)",307496
96566,nan,311046
88320,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) divid5 = lambda x: x % 5 == 0 list_answer = filter(divid5, input_list) print(list_answer)",305804
89433,"I am writing a code as mentioned below which is getting executed on quiz screen but on my jupyter notebook its showing:- TypeError : &#39;list&#39; object is not callable name =list(map(lambda x,y: x[0:]+ &quot; &quot;+ y[0:], first_name, last_name)) #code for joining names on two lists",307496
89304,"I am trying to solve the filter question in console with the following code input_list = ['soap','sharp','shy','silent','ship','summer','sheep'] operation = lambda x: True if x[0].lower()=='s' and x[-1].lower() == 'p' else False sp = list(filter(operation,input_list)) print(sp) while the test case is passing the solution is being rejected due to hard coding i guess? how to overcome this problem?",310509
89295,"input_list= [5,6,4,8,9] print(list(map(lambda x :x*x*x,input_list))) my sol works in the console but when i submit it , it shows error. can anyone explain why?",315560
91229,,317558
91239,,317558
90738,"Function Description Create a function squared(), which takes x and y as arguments and returns the x**y value. For e.g., if x = 2 and y = 3 , then the output is 8. Execution Time Limit 5 seconds Submit createAttempted0 of 2 CodeInputOutput CodeInputOutput Python 3 Run Code Verify more_vert close No sample testcases passed. (0/1) Testcase #1 Status Failed Execution time 0.31s CPU 0s Memory 5MB Description Testcase failed! The solution&#39;s output doesn&#39;t match the expected output. Input [&#39;6&#39;,&#39;7&#39;] Solution output 279936 None Expected output 279936",303115
95134,str=&#39;Abdul Kalam is a great scientist&#39; vowel=&quot;&quot; for alphabet in str: if aplhabet in &#39;aeiou&#39;: vowel+=&quot; &quot;+alpha print(vowel),314629
95135,",count=0 data={1:[&#39;shruthi&#39;,27], 2:{&#39;divya&#39;:20}} for key, val in data.items(): if val[1]&lt;25: count+=1 print(count)",314629
95197,"paragraph=[&quot;this a beautiful day&quot;, &quot;Thank god everyday for the things that you have&quot;, &quot;Indigo am good girls&quot;, &#39;bangalore weather is awesome&#39;] vowels=[&#39;a&#39;,&#39;e&#39;,&#39;i&#39;,&#39;o&#39;,&#39;u&#39;] words=[word for sentence in paragraph for word in sentence.split() if word[0].lower in vowels] print(words)",314629
112616,nan,318598
112413,"Hi Guys, Can anayone tell what is the process of steps in the cricket challenge Hackathon.",306243
112334,nan,303085
111211,nan,305847
111402,"Hi Guys, Can anyone tell , how to proceed this task. what steps.",306243
97057,"I am not able to remove space from the columns of popluarity Data set. I am using Popularity.columns.str.replace('',' ')",318532
95506,The professor while demonstrating said that plt.plot() will generate and display the plot object and to explicitly display the plot plt.show() has to be used. However in my case the plot was displayed along with the plot object. I did not had to use plt.show(). Why did this happened ? Is it due to some recent changes in the library or any other reason ?,318479
95522,"I executed the first subplot example given in the tutorial. I commented out initiating figure i.e. plt.figure(1) But still the 2 subplots were displayed. I did not specify the plt.figure(1), so is that python handles it even if it is not mentioned.",301114
96142,"In the question embedded in the lesson where we have to identify the right type of plot, we have just one variable - the credit card transaction value. To find outliers, I would tend to think that a histogram would be the right plot as we have just one quantitative variable. Why is the correct answer different?",318762
95548,"Hi, While labeling i am getting error plt.xlabel(&quot;Current&quot;) plt.ylabel(&quot;Voltage&quot;) plt.title(&quot;Ohm&#39;s Law&quot;) TypeError: &#39;str&#39; object is not callable",307018
95929,"Since question is asking for a blue line with circles as pointers I believe the correct code should be plt.plot(x,y,&#39;bo-&#39;) plt.show() opionions please.",313767
95569,nan,310179
95591,"How to use plt.axis mentioned in the below code for plotting graphs. # Define the range of labels of the axis # Arguments: plt.axis(xmin, xmax, ymin, ymax)",311119
95593,I am bit confused of box plot(distribution of a continous variable) and scatter plot(distribution of single variable). Please clarify where it can apply correctly.,312019
95599,"I have written following code on my own for subplot (2 rows, 2 columns). I ma getting error . Could you please let me know what is the issue. plt.figure(1) plt.subplot(2,2,1) plt.plot(x,y**2) x = np.linspace(0,10,20) plt.title(&quot;y- square&quot;) plt.subplot(2,2,2) plt.plot(x,y**3) x=np.linspace(0,10,20) plt.title(&quot;y-cube&quot;) plt.subplot(2,2,3) plt.plot(x,y**2) x=np.plot(0,10,20) plt.title(&quot;y-sqaure&quot;) plt.subplot(2,2,4) plt.plot(x,y**3) x=np.plot(0,10,20) plt.title(&quot;y-cube&quot;) ValueError: x and y must have same first dimension, but have shapes (30,) and (20,)",311119
95610,"In the jupyter notebook provided, the code for creation of subplot 3 seems to be incorrect. The code is as given below: # subplot 3 plt.figure(2) plt.subplot(2, 2, 1) plt.title(&quot;Log&quot;) plt.plot(x, np.log(x)) Shouldn&#39;t the highlighted line be plt.subplot(2, 2, 3) as the last argument indicates the position in the array?? Also, what is the significance of &quot;plt.figure(2)&quot; here??",313826
95639,"While making 4 subplots in a 2x2 matrix space they gets cluttered. Eg. the plot title overlaps the plot x axis, etc. How can we handle this ?",318479
95993,Are the commands used for plots in matplotlib similar to those used in MATLAB software?,318788
95661,"Arrays of Plots In &#39;plt.subplot(), the numbering starts from the top left element, and moves rightward along each row, and then goes to the next row. Out of the following subplot options, which one does NOT represent a plot which is part of the second column from the left in the array of sub-plots?",318429
96014,"As per video, plot is not visible unless we use plot.show() function. Why is graph visible in my jupyter notebook?",318458
95692,"when running the above command Iget the following File &quot;&lt;ipython-input-10-7dde0e831adf&gt;&quot;, line 1 plt.plot(x,y,&#39;bo&#39;)plt.show() ^ SyntaxError: invalid syntax",312892
95353,I am unable to view the plot after using plt.show at the beginning of lecture.Has anyone encountered this?,300698
96287,nan,305804
95469,"In the example, when creating a subplot of the normal scale and log scale, why is first a figure not created , as was the case in the previous example? is it not mandatory to create figure when creating subplots?",310509
95545,"As professor explained subplot(1,2,1) means 1 row 2 columns and figure 1 but where I am unable to differentiate the row columns in the graph. Yeah, the graph does vary by changing the values, however I am uanble to understand what actually happening.",312756
95940,installing packages for my environment i got error while instaling as given below saying to install few more packages which is required. how ever when i checked all others are already installed and available. how ever i tried to install by clicking apply. After completion of instalation when i checked my installed packages these 2 are not available. I repeated this almost 5+ times. Pls let me know the issue,315455
95499,nan,314678
95518,nan,301110
96991,Anyone sees a major advantages in using python over Tableau? Tableau is much more robust and has lot of variance. Any specific reason why we should consider python over Tableau for visualizing except the license cost aspect?,314084
95511,nan,300733
95294,Heat Map Displaying the Most Scoring Zones on the Field not available,303082
97256,"Hi Everyone, For Data visualization in Pandas I found the below link very useful as it shows most of the plot types : https://pandas.pydata.org/pandas-docs/stable/visualization.html",317460
96963,How to give figure a heading within box plot? I have two figures and I need to provide a heading. any idea how to do that?,309211
95776,I am not clear on how the answer is 30000-40000 here as the below is the rug plot i got and i am getting no value in &gt;200K.,310509
95778,"Why is a figure created for heatmap and what does the below mean # figure size plt.figure(figsize=(10,8)) What is the significance of figsize =(10,8) ?",310509
95787,"While using seaborn to plot the Sales column, say you want to specify the the number of bins as 50 while making the function call. What function call will you use? Can anyone explain this!!",318846
95791,"Couldn&#39;t able to understand easily? Always has to dependant on searching and learning. Please give the Basic concepts in detail. Need more subjective info in python and python visualization concepts. TA,Please respond on this.",318846
95796,"The warning message is the following : D:\Installed_app\Anaconda\lib\site-packages\scipy\stats\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval",320073
95823,"Can some one explain how to find correlation between different variables and if i have n variables of data, what is the best approach to find min to max values of correlations finding. in general pls explain.",312019
95827,nan,315560
95822,"When I plotted the box plot for 'Sales' column , the output boxplot was huge and not clear as shown in the above image. So I mentioned plt.yscale('log'). I got an error . Could you let me how to make the boxplot more clear . sb.boxplot(df['Sales']) plt.yscale('log') plt.show()",311119
95860,nan,314313
95889,,314313
97103,what is wrong in this code? Can anyone help?,317558
95626,nan,310501
95715,There are 2 different words 'pearsonr &amp; p values' seen in the seaborn joinplot. What is the significance of these two?,306248
96833,Distplot shows negative values for a histogram. I don't have negative values in my dataset. Is there a way to set limits on x axis ?,309211
95785,"When i run the below code provided with the python notebook: # simple density plot sns.distplot(df['Shipping_Cost']) plt.show() I get the below message - the plot does get displayed after the message: anaconda3/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6462: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg. warnings.warn(""The 'normed' kwarg is deprecated, and has been "" Anyone know what is leading to this message.",319302
102897,nan,318344
95492,Whats the effect of Technology category in Home segment? How can we explain that?,317984
95949,"In the code below: plt.figure(figsize=(10, 8)) # time series plot sns.tsplot(df_time) plt.xlabel(""Time"") plt.ylabel(""Sales"") plt.show() there is a warning: C:\Anaconda3\lib\site-packages\seaborn\timeseries.py:183: UserWarning: The `tsplot` function is deprecated and will be removed in a future release. Please update your code to use the new `lineplot` function. warnings.warn(msg, UserWarning) How to plot time series without using tsplot()?",304319
95800,nan,318846
95937,Time-series plots have x-axis as whole numbers rather than showing date or time. Should we add additional formatting to show the time value ?,316211
95944,"How to choose when to use log function and when to remove the extreme data to visualize the box plot correctly. In Plotting Categorical and Time-Series Data section , for Sales data log function being used to plot the box graph and for profit data , data has been shrunk (extreme data removed). I am confused when to use log function over extreme data removal. Any thoughts.",305652
95666,"In the video for plotting the barplots, the explanation provided by the Prof. for the blackline is not very clear. Can somebody please explain it with some examples?",313826
95674,nan,320197
95575,"Dear Friends, I have observed these warnings a couple of times. When I suppress the warnings this does not show; however when I remove the ""Suppress warnings"" , I see this warning almost often but it disappears at times and at times I see it [ meaning, it does not constantly show up]. I found this StackOverflow link googling out but it is not being too helpful: https://stackoverflow.com/questions/52594235/futurewarning-using-a-non-tuple-sequence-for-multidimensional-indexing-is-depre Command ezecuted: sns.barplot(x='Product_Category', y='Sales', data=df) plt.show() Warning seen: /anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result. return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval",309211
96007,"In the Aggregator Section, I see the label displayed in my notebook for Y--axis is simply &#39;Sales&#39; and NOT the mean(Sales). How do I know which aggregator is being used and what is the default estimator for barplot?",311115
96033,nan,314313
97166,"I am getting the error module 'seaborn' has no attribute 'catplot' Eg: sns.catplot(x=""sex"", y=""total_bill"", hue=""smoker"", col=""time"", data=tips, kind=""box"", height=4, aspect=.7); --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-32-9d1738587da4&gt; in &lt;module&gt;() ----&gt; 1 sns.catplot(x=""sex"", y=""total_bill"", hue=""smoker"", col=""time"", data=tips, kind=""box"", height=4, aspect=.7); AttributeError: module 'seaborn' has no attribute 'catplot' Has anyone faced the issue?",309451
88601,nan,305651
88623,nan,318017
89543,nan,312019
78546,nan,304692
75927,"&gt; mylist[[2]] #Accessing second elements [1] &quot;Wankhede&quot; &quot;Eden Gardens&quot; &quot;Firozshah Kotla&quot; &gt; mylist[[&quot;St&quot;]] #Alternate [1] &quot;Wankhede&quot; &quot;Eden Gardens&quot; &quot;Firozshah Kotla&quot; &gt; mylist[2] $`St` [1] &quot;Wankhede&quot; &quot;Eden Gardens&quot; &quot;Firozshah Kotla&quot; Here 2nd column of this list is a vector of stadium having alternate name st, is this the only logic?",300721
78628,"In the data frames video, if the structure of the data frame is called the output is different in one other way too. the values are arranged in alphabetical order or numerical order depending on the data type. for what other such cases does the output in str() differ in similar manner?",304389
76388,nan,302741
78207,&gt; team_from_text_file&lt;- read.table(&quot;myfile.txt&quot;) Warning message: In read.table(&quot;myfile.txt&quot;) : incomplete final line found by readTableHeader on &#39;myfile.txt&#39;,305790
78214,nan,306038
84160,nan,316084
75536,"Again, I repeat the error over here: &gt; complete_team &lt;- rbind(team_data, team_data2) Error in match.names(clabs, names(xi)) : names do not match previous names &gt; complete_team Error: object 'complete_team' not found",301655
75130,nan,300717
75136,nan,300721
75325,Does anyone else faced the issue of permission denied while downloading swirl package?,300688
76595,"I have executed in the below karan_dice &lt;- c(3,1,5,7,6,1) raj_dice &lt;- c(6,1,3,4,6,1) ajay_dice &lt;- c(1,1,4,2,2) dice_vector &lt;- c(karan_dice, raj_dice, ajay_dice) dice_matrix &lt;- matrix(dice_vector, nrow = 3, byrow = TRUE) dice_matrix And i got the the below output. dice_matrix [,1] [,2] [,3] [,4] [,5] [,6] [1,] 3 1 5 7 6 1 [2,] 6 1 3 4 6 1 [3,] 1 1 4 2 2 3 Could any one please let me know the logic. I purposefully used ajay_dice with 5 values where as others with 6 values. My intitution is that it should use recycle logic and the value should be populated as 1. However when executing it is showing as 3. Not able to understand why it is so.",302741
75181,Can anyhow let me know the difference between class and typeof function example : When I use class(2) is says numeric and when typeof(2) is used it says double. A basic doubt over here how will we know when to use which function.,300688
80286,"Operations in vectors Create vectors a = c(2 ,6, 3) and b = c(4, 3, 5, 7) on R console. What is the result of multiplication of a and b? The answer in the assigment is below with explanation 8 18 15 14 Feedback : You can note that vector multiplication also occurs element-by-element like vector addition. R expects you to operate in vectors of similar lengths, but if you dont, it uses the first element of vector a and multiplies it with the last remaining element (7 of vector b here). But as this is not proper recycling, R also issues a warning for this and not an error, so you may choose to ignore the warning and look at the output. However when I did practice in Rstudio it gives me warning",307843
75467,nan,301655
75559,"Create vectors a = c(2 ,6, 3) and b = c(4, 3, 5, 7) on R console.What is the result of multiplication of a and b? Answer from tutorial: 8 18 15 14 Feedback : You can note that vector multiplication also occurs element-by-element like vector addition. R expects you to operate in vectors of similar lengths, but if you don&rsquo;t, it uses the first element of vector a and multiplies it with the last remaining element (7 of vector b here). But as this is not proper recycling, R also issues a warning for this and not an error, so you may choose to ignore the warning and look at the output. Answer when i tried in Upgrad: In a * b : longer object length is not a multiple of shorter object length",301114
76643,nan,301122
79080,"In Data Frames chapter, our professor uses &lt;- for rbind() function, but he uses &lt;&lt;- operator when using cbind() function (At around 9:30 in the second video in Data Frames page). Any specific reasons? Edit: I used&lt;- operator when using cbind() and everything worked just fine. &lt;&lt;- is called the scoping assignment operator , but I didn't understand why or when to use it.",301652
80587,nan,307494
80643,nan,302744
79097,"Is that something irrespective of what language we chose, we need to learn the basics of R? Please correct me if I am wrong.",305843
79204,nan,301652
78288,nan,304812
77951,nan,305845
79246,"Need confirmation on the terminology that Vectors , Factors and Metrices can be categorized into. Thanks in advance.",305652
79646,"Create a vector , say myvector&lt;-c(1,2,3,4) c[4-3:4] , prints 3 2 1. Can anybody help me in understanding the logic here.",304398
78411,nan,304692
87422,nan,308437
80405,"My working directory is set as below setwd(""C:/My_Folder/Hadoop/Upgrad/R/Work"") However when running the command it is giving error for both .txt and and .csv file?Please let me know what is the reason In read.table(""my_file.txt"") : incomplete final line found by readTableHeader on 'my_file.txt' Error in team_from_text_file &lt; team_from_csv &lt;- read.csv(""myfile.csv"") : incorrect number of arguments to ""&lt;&lt;-""",307843
82560,nan,301890
86921,nan,312376
86962,"Guys, I just checked both the options 1 &amp; 2 and they are identical.. though i selcted only the second one for submission since we could select only one! Am i missing something? or did i do anything wrong? please correct..",316349
83151,,314313
86474,,310974
88824,"in given examples RBIND : Complete_team &lt;- rbind(team1,team2) CBIND : team1_info &lt;&lt;- cbind(team1,team3) why we are using &lt;- for RBIND and &lt;&lt;- for CBIND is there any significant difference bcose when i am using team1_info &lt;- cbind(team1,team3) i got same result",315455
87349,nan,308437
87351,nan,308437
83643,nan,314128
86051,I am getting 'stringi' package not found. And when i am trying to install stiringi package it is taking very long to install and some logs are getting printed.,320073
85724,nan,320073
85763,nan,310504
83765,"I tried this out my_vector1 &lt;- c(1,3,5,7,11) my_vector2 &lt;- c(2,4,6,8) added_vector &lt;- my_vector1 + my_vector2 added_vector This gave me the output with an error. Output -&gt; 3 7 11 15 13 Error -&gt; Warning message: In my_vector1 + my_vector2 : longer object length is not a multiple of shorter object length",308962
86116,"I have tried this on on R console created vectors a = c(2 ,6, 3) and b = c(4, 3, 5, 7) and on adding a+b It is giving, Warning message: In a + b : longer object length is not a multiple of shorter object length Whereas your answer is &quot;8 18 15 14&quot;. Could you please explain how this answer is correct?",307496
86241,In the assigned variable screen of R Studio by default they get stored alphabetically is there a way out to sort the created variables in first in first out manner so that it gets easy to access the variables which are in use for present program.,307496
84116,"in the Data frame example on Cricket, why is Strike rate showing as a strong instead of numerical?",310509
86336,"On using rbind cmd below mentioned warning is coming and that to for data frame Complete_team for two enteries NA is printed, why so in video there is no such kind of warning &gt; Complete_team &lt;- rbind(team1,team2) Warning message: In `[&lt;-.factor`(`*tmp*`, ri, value = c(1200, 1500, 5000)) : invalid factor level, NA generated",307496
87632,"In R Studio , Tools -&gt; Install Packages , types &quot;Swirl&quot; and pressed &quot;Install&quot;. It gives the following erorr in the console : Warning: unable to access index for repository https://cran.rstudio.com/src/contrib: cannot open URL &#39;https://cran.rstudio.com/src/contrib/PACKAGES&#39; How do i go about this",319759
87590,nan,318495
87724,nan,306996
87725,nan,306996
87744,nan,320689
87729,,306996
76634,"Here is an example &gt; vctrs&lt;-c(""A"",""B"",""C"") &gt; &gt; mtrx&lt;-matrix(1:6,2,3) &gt; &gt; dtfrm&lt;-read.table(""myfile.txt"") &gt; &gt; mylist&lt;-list(v = vctrs, m=mtrx, df =dtfrm) &gt; &gt; mylist[[2]] [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 &gt; &gt; mylist[2] $m [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 &gt; mylist1&lt;-list(vctrs, mtrx,dtfrm) &gt; mylist1[[2]] [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 &gt; mylist1[2] [[1]] - HOW DO I INTERPRET THIS VALUE? [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6",300708
83651,,314313
87137,nee some more explanantion in &quot; !is.na &quot; in the condition,305804
87148,"function_math &lt;- function(x, y){ z &lt;- x + y p &lt;- x * y q &lt;- z / p return(c(z, p, q)) } function_math(2, 3)",305804
82066,"Hi all, my question is about functions in Intermediate R after running the following #Calculating the row number of person with maximum age which.max(bank$age) I get [1] 33700, how to identify its exact location in terms of rows n columns in shortest way...",310508
88424,nan,320008
77355,"I have a dataset which is related to movies. There is a column in it &quot;production_companies&quot;. Basically more than one company can produce the movie. The dataset of that particular column is as below &gt;firstmovie &lt;- moviedataset[1,&quot;production_companies&quot;] &gt;firstmovie &gt;[{\&quot;name\&quot;: \&quot;Walt Disney Pictures\&quot;, \&quot;id\&quot;: 2}, {\&quot;name\&quot;: \&quot;Jerry Bruckheimer Films\&quot;, \&quot;id\&quot;: 130}, {\&quot;name\&quot;: \&quot;Second Mate Productions\&quot;, \&quot;id\&quot;: 19936}]&quot; Basically i want to get all movies which has &quot;Walt Disney Pictures&quot; in it. How can i get this ?",301114
82312,"When i execute a program the result that i get is 1e+05 which is 100,000. How can i get the right display",310629
80164,nan,304319
76886,"Like we did the credit card issuance example, where we opearted on some existing set of records. Suppose we have an application where we are capturing customers info (similar to bank.csv file) and every day my customer database increases and hence I want to generate a CSV file every EOD taking the same decision whether we should issue credit card to the newly added customer or not or any such process where we are adding a new column to the existiing database and sending these files to some other network as well. My entire process should be automated.",300721
79124,nan,304813
79318,"In R a matrix say player_matrix there are 2 columns-player_name &amp; player_age and 8 rows.How to pop a particular row for which ""player_age""=30.Command should be used is which(player_matrix[ ,""player_age""=30])?",300690
82199,,310508
81746,"I completed the swirl assignment , but at the end of the assignment I was asked about the token number and I don't have an idea of what it is. Someone please help me..",304389
84979,I followed the exact steps but Bank.csv not appearing on my R console. Please suggest. Thanks :),308962
80624,"How do you add new row to an existing data frame? Ex: df &lt;- data.frame(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;);",308442
80376,Can someone please explain the above with examples?,307843
80893,"I am unable to install Swirl on my Mac. I was able to successfully install and try R and R Studio via Homebrew. However, when I tried installing Swirl library on Mac I get the following error: ERROR: dependency RCurl is not available for package swirl * removing /usr/local/lib/R/3.5/site-library/swirl Warning in install.packages : installation of package swirl had non-zero exit status The downloaded source packages are in /private/var/folders/53/7hd8gxfs2b17_mmm73tvrbl8mcy1n1/T/Rtmp92P1Mv/downloaded_packages Is anyone else facing same problem?",309211
85237,nan,310509
82596,nan,310629
88283,"In the line of code below, I have a query regarding is.na . is.na will return the boolean output and person$salary has a numeric value. How does logical operator '&amp;' work in that case? I mean lets take 2 scenarios: if(person$housing == ""yes"" | (!is.na(person$salary) &amp; person$salary) &gt; 60000) 1. (!is.na) is TRUE and person$salary= 50000, what will be the output of '&amp;' operator? 2. (!is.na) is FALSE and person$salary= 50000, what will be the output of '&amp;' operator?",314361
83757,,314313
80627,"I was trying the swirl exercise on functions. There is a a question that asks for the the indices (row numbers) in the dataset when solar radiation is not recorded (NA). So my answer to that is which(is.na(airquality$Solar.R)) But the swirl env does'nt let me pass because it believes the correct ans should be ""which(is.na(airquality$Solar.R)== TRUE)"". Though both returns the same indices value. Now if I refer the docs -&gt; which(): Give the TRUE indices of a logical object, allowing for array indices. So looks like the check condition ""==TRUE"" is actually not needed. What am I missing here?? Why is my ans wrong?",309451
83514,"for(i in 1:nrow(bank)) { person&lt;-bank[i, ] person if(person$marital == &quot;married&quot;) { if(person$housing == &quot;yes&quot; | (!is.na(person$salary)) &amp; person$salary &gt; 60000) { bank[i, &quot;my_decision&quot;]&lt;- &quot;yes&quot; } else { bank[i, &quot;my_decision&quot;]&lt;- &quot;no&quot; } } else if(person$marital == &quot;single&quot;) { if(person$education == &quot;tertiary&quot; &amp; (!is.na(person$salary)) &amp; person$salary &gt; 40000) { bank[i, &quot;my_decison&quot;]&lt;- &quot;yes&quot; } else { bank[i, &quot;my_decision&quot;]&lt;- &quot;no&quot; } } }",310505
86987,"I troed the code for bank data on when to issue a credit card , but as in example the conditions are implemented to all rows of bank file by using condition &gt; for (i in 1:nrow(bank)) + { Need to know how can we check the condition just for particular number of rows let say from 5th to 10th row.",307496
87596,How does one access Swirl package in R,312892
88228,nan,308437
87187,"For an data frame airquality there are entries Ozone, Solar.R, Wind, Month, Temp and Day and below mention is question check the the indices (row numbers) in the dataset when solar radiation is not recorded (NA) I have tried the which and is.na() finction to answer it , but the swirl is telling it wrong... &gt; which(is.na(airquality$Solar.R)) Please sugest the mistake and solution",307496
90873,The Wordings of the options are very ambigous: option a is &quot;The total number of students who have taken course sections taught by the instructor ID 10101&quot; which in essence is similar to &quot;The total number of distinct students who have taken course sections taught by the instructor ID 10101&quot; since its a non-technical opinion based question: total number students and total distinct students are very similar sounding,319866
91085,"Table structure is very confusing ..... what the (a,b,c,d),(1,2,3,4) and roman columns are why they have specified....? instead of these team would have given an ERD...",300693
88701,First Attempt Marks Second Attempt Marks Question 10 5 with 2 Attempts Question 10 0 with 1 Attempt,313676
89008,nan,300687
92397,"create a User defined function to calculate the Average distance of an employee's house from the other employees. Take the input as the ssn and return the Average distance of that employee's house. Using the function, determine the average distance between the house of CEO (super_ssn is null) and the employees. unable to understand the question.please advise",300687
90797,"- ### Subtask 2.4: Drop unecessary rows Some of the rows might have greater than five NaN values. Such rows aren&#39;t of much use for the analysis and hence, should be removed. but my dataset after performing previous step ( removing rows for cols haivng more than 5% null) has not any rows with 5 null, can any one chime on this one or I am missing something, thanks",317577
93437,Here the primary key for the employee table is empID and foreign key is strID but strID is not the primary key of any table instead for store table the primary key is storeID . Similarly the case of rest 3 tables are same. So either the foreign key of all the table should be renamed or the answer should be (D)none of these,320257
134277,"Need some understanding of windowing concept and rank function in pyspark,how we can implement it in pyspark without writing sql queries if possible.",311861
133986,"After googling, I see only 77 precincts in Newyork out of which 4 are non numeric which would leave us with 73. Am I safe to remove the rest of the entries stating the same in the assumption? It accounts to 26.2% of the data!",311160
133913,"Summons Number is integer When i load the dtypes shows was string and checking null value df1.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show() nothing came but after changing datatype of int and doing null value chek as above, it wont show null values but when i do show() it shows few are null. How come this happen ?",312019
133344,"# Checking the percentage of missing values We do below way in pandas round(100*(telecom.isnull().sum()/len(telecom.index)), 2) How to find the missing values percentages over the spark df ?",312019
134212,nan,314612
134219,"park2.createOrReplaceTempView(&quot;p2table&quot;) spark.sql(&#39;SELECT Violation Code FROM p2table&#39;).show() Above simple code not running, throwing some very large error. What could be the reason?",308437
134234,"I used isnull, isnan, IS EMPTY (in sql) functions to detect null/missing values in Violation TIme column All of them returned 0 Am I missing something?",308437
134238,nan,318358
134059,"Do we need to do any optimization technique for this assignment like partitioning, bucketing, any ml algo to apply or only the analysis and as per the quetions mentioned ? For confirmation ?",312019
134254,Hi team I am getting OutOf Memory Exception while using pyspark . Please suggest on this,301108
134782,"There is a conflict on the problem statement page where it is written that we need to submit only one jupyter notebook and on the submission page, it is mentioned we need to submit pdf/doc file also containing all qus and ans. @TA please confirm",305650
134288,nan,318005
134367,Are there any inbuilt functions to convert Violation Time to time format? I have seen in live session the logic to convert data into understandable time format. I just want to know details on how to do it or what steps should I have to follow as I have to convert the time manually.,315423
134388,I have written a query to find the no of count for each violation code in data and I'm getting for each and every violation code. How do I take out only top 5 ? (By using what SQL command to use for it?,311466
134422,"How to convert ""Violation Time"" in the format of HH:MM and then divide into hours of the day to create bins",300735
134222,"While looking for null values using ISNULL clause of SQL (as given in the hint), no null values appear. So while converting the string values in Violation Time column to time format, some rows are appearing as null while others are getting converted properly. Any possible reasons as to why this is happening and is it ok to delete them because they are about 28K in number out of about 5.4 million data points. Please support on this issue. I&#39;ve looked at other discussions on null values, but they aren&#39;t helping on this issue. Below is a snapshot with the highlighted row under Violation Time column which is not null to begin with. Below is the snapshot of the conversion with the corresponding row highlighted that has become null.",310505
134677,"I&#39;m getting &#39;NoneType&#39; object has no attribute &#39;_jdf&#39; while Bucketizer the Violation hours Code: from pyspark.ml.feature import Bucketizer bucketizer = Bucketizer(splits=[ 0, 4, 8, 12, 16, 20, 24 ],inputCol=&quot;Violation_Time_hr&quot;, outputCol=&quot;Violation_Time_hr_bucket&quot;) bucketizer.setHandleInvalid(&quot;keep&quot;).transform(dfnyc_parking) dfnyc_parking.Violation_Time_hr is a derived column based on first 2 bytes of dfnyc_parking.Violation_Time from the dataset. Could you please share any insights.. I&#39;m reading blogs that it is related to null pointer exception, but no clue how to void it.",310518
134447,"I am trying to extract moth and year from date format by using this code : df.withColumn(""Mon"", month(to_date(df(""date_in_dateFormat""),""yyyy-mm-dd""))).show(5) buy getting the error NoneType' object has no attribute 'withColumn'",300735
134453,"I am executing code from pyspark.sql.functions import date_format,from_unixtime,month, unix_timestamp, year, to_date df2=df1.withColumn(""month"", date_format(to_date(""date_in_dateFormat"", ""yyyy-mm-dd""),""M"")).show(5) its creates month but after that from pyspark.sql.functions import col df2.cube(""month"").count().sort(col(""count"").desc()).show(100) gives error Getting some strange error NoneType' object has no attribute 'cube' Not able to understand why this error is reflecting",300735
134442,Values for the column Violation time are overlapping. E.g. value of 1235A == 0035A. Upon cleanup these should converge to 0035 hours. Is this understanding correct or am I missing anything here?,318078
134337,"i somehow got violation time into XXYYAM / XXYYPM format but when converting into timestamp, all values starting with 00 are getting converted to nulls. Y is this so?? Rest of the values are neatly being converted into XX:YY in 24 hr format",308437
133586,"Do we need to find out the frequency in terms of time or count? So, should the answer be like : - Violation Code A occurs once every 10 secs or - Violation code A occurs total xxx times ?",301116
134119,"If i am trying to group by two columns, can i just get a result which shows all the groups by column1, and just top 5 groups by column 2?",312096
133094,"After loading the data from the path mentioned in the problem statement, I&#39;m able to see only 10 columns. When I check the data dictionary in kaggle, I see there are around 51 columns there. Are we supposed to ignore kaggle and just work with the 10 columns dataset? Please confirm.",318084
134487,"I have been trying to complete the case study from last couple of hour, but struggling a lot with platform response. for a single commnd some time it is taking more than 5 minute to show the result and from last one hour, it is constantly showing error: Py4JJavaError: An error occurred while calling o89.collectToPython. : java.lang.OutOfMemoryError: GC overhead limit exceeded I have tried restarting jupyter, restart kernal but nothing helps. Did any one else faced such issue ?",306005
134225,"It was all working fine. Suddenly since 10-15 min, I&#39;m facing error as below : The currently active SparkContext was created at: (No active SparkContext.) Any suggestions ? thanks",316211
134744,"I know so far this question has been asked many times but confusion arises just now when I saw two different answer from TA regarding the file submission for this assignment. In below link for the same question, https://learn.upgrad.com/v/course/208/question/134238 And Now confusion arises again. TA please verify once and for all that how many files need to be submitted.",317991
134501,nan,300733
134093,How do I replace numeric entry 99 in the column with the desired value? I tried many functions of pyspark but it's giving me an error!,311466
134046,The assignment does not explicitly mention anything about data cleaning and null value handling. But I see many columns have blanks and junk characters (some of the entries in some columns appear to be junk but do not know how to classify between junk and clean entries) Should we clean all these? What is the guideline to clean these? Thanks in advance for the response.,308437
134069,"In the assignment, all hints r given only under sql commands, nothing related to pyspark... Does it also mean wherever hint like that is given we should follow only sql table approach and not direct pyspark??",308437
133837,"There&#39;s no data dictionary available for this case study, so I want to know what&#39;s unique in the dataset? is it summons number or plate Id?",315423
134089,"Analyses can be done in pyspark dataframes, spark.sql but which one to use and when !",311466
133517,"Mentioned in the assignment &quot;Here, you would have noticed that the dataframe has the&#39;Violating Precinct&#39; or &#39;Issuing Precinct&#39; as &#39;0&#39;. These are erroneous entries.&quot; Is 0 the only erroneous values or we have to check the actual precincts of New York and then remove the erroneous records accordingly. For Example : 933",311254
134096,"On the pyspark data frame while checking for null and nan values,its shows count as 0 (as no null or nan values present ) with below commands df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show() but when I checked manually there are few columns which has null and nan values. like ViolationTime there are few rows which has values as nan values. Why both of the above checks are not capturing it? TA please check it from your side !",311466
133538,"From question number:3 in aggregation tasks, erreneous entries for Precint should be deleted ? &quot; Here, you would have noticed that the dataframe has the&#39;Violating Precinct&#39; or &#39;Issuing Precinct&#39; as &#39;0&#39;. These are erroneous entries.&quot;",305652
134542,Can someone elaborate above question line ? missing values of all columns or missing values for any specific column ?,318770
134097,nan,311466
134556,"I have created the buckets based on hour. When I try any aggregation on it (cube, add, groupBy), I get the following error: &quot;An error occurred while calling o1376.showString&quot; If I replace the bucket column with any of the pre-existing columns (says Summons Number), the same code works. Can idea on what&#39;s going wrong?",305653
134559,nan,311952
134565,I am confused about the question 2 of aggregation tasks . Are there two separate questions or are we to find to find the count of vehicle make in relation to the vehicle body type ?,317149
134568,"The highlighted points 3 and 4 in the snapshot below ask for a text file which has subjective inferences, plots etc. for the case study. But the evaluation rubric doesn&#39;t specify anything like that explicitly. What exactly must we submit?? Isn&#39;t the Jupyter notebook enoough?",310505
133624,Do we need to convert it into Timestamp format? How can we do it?,317996
133897,Violation time is in the format - 2016-11-21 00:00:00. How do i extract year from this and form a new column &quot;Year&quot;?,319759
134111,"Created a dataframe from the csv input file. Now i wanted to take only records pertaining to year 2017. Hence tried converting the Issue Date column from string to datetime for further processing. But i get an error cannot convert column into bool: please use &#39;&amp;&#39; for &#39;and&#39;, &#39;|&#39; for &#39;or&#39;, &#39;~&#39; for &#39;not&#39; when building dataframe boolean expressions. when using pd.to_datetime. Is there any other way to do it. I dont want to load into sql view and do it.",301114
132888,Is this put in by mistake or are we supposed to code using RStudio as well along with Jupyter notebook? &quot;The following analysis should be performed on RStudio mounted on your CoreStack cluster using the SparkR library. Remember that you need to summarise the analysis with your insights along with the code.&quot;,309211
133985,"Thought about imputing year with 2017 but then upper Limit of year is also not mentioned as only of 2017 and In order to Discard, I need exact date up to which the data is recorded And any operation on this Date column will impact Seasonality Trend!",317984
134627,"After creating buckets for time column, how to merge this bucket column with main dataframe...",308437
134666,nan,314197
134380,"Initialised by spark session as below from pyspark.sql import SparkSession spark = SparkSession \ .builder \ .appName(&quot;nyc_parking&quot;) \ .getOrCreate() created a dataframe as below nyc_prk = spark.read.format(&quot;csv&quot;).option(&quot;header&quot;,&quot;true&quot;).load(&quot;/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv&quot;) nyc_prk When tried executing the below, it is throwing error nyc_prk.createOrReplaceTempView(&quot;nyc_prk_assmt&quot;) --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-31-472e576bf093&gt; in &lt;module&gt;() ----&gt; 1 nyc_prk.createOrReplaceTempView(&quot;nyc_prk_assmt&quot;) AttributeError: &#39;list&#39; object has no attribute &#39;createOrReplaceTempView&#39; Not sure where went wrong when tried using below df1 = spark.createDataFrame(nyc_prk) got an error the object is already a dataframe.",302741
133484,nan,301116
133711,"Assignment says tickets over 2017 only to be analysed. But as per Kaggle site, data is arranged as per Fiscal year (july1-June30) Hence the csv file stored in HDFS has data related to &#39;fiscal year&#39; 2017 Now, the doubt is whether we should consider entire data (fiscal year 2017) for analysis or ONLY 2017 data (actual year 2017) ? TA, [pls confirm",308437
133824,"Hi, after reading CSV file only, few columns are being able to run. Could you please help in solving this.",317410
134057,"when i calculated violator precinct and issuer precinct distinct values, both were showing different values? How is this possible? Because issuer and violators stations are the same, both the counts should match right?",308437
133708,Where is the data dictionary for this assignment? I didnt find this in Kaggle page as well,308437
134266,"I see Violation Precinct &amp; Issuer Precinct having 0 value for some rows, is 0 valid for this column? or we need to impute a value for such rows?",315423
134297,nyc_prk.createOrReplaceTempView(&quot;nyc_prk_assmt&quot;) got the below error --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-16-472e576bf093&gt; in &lt;module&gt;() ----&gt; 1 nyc_prk.createOrReplaceTempView(&quot;nyc_prk_assmt&quot;) AttributeError: &#39;list&#39; object has no attribute &#39;createOrReplaceTempView&#39; Could you please let me know how to resolve it,302741
134668,"Hi Team , In problem statement it was stated to submit the python file with all the observation .But in Submission it is showing to submit a zip file with python and PPT file . So, please clarify on this",301108
134071,It is a bit unclear if we need to do absolutely have SQL or not. The question says IF SQL is used but the evaluation rubric says that we need to use a SQL view or else we wiill be penalised &#39;SQL view is created wherever required.&#39; so just to explicitly ask - can we do the assignment and get full marks without using any sql? or must SQL be used?,300694
134044,"I observed following things in the dataset: Plate ID - has some entries as only nos, combination of letters+numbers, one junk value by name Aug etc Vehicle body type - has some entries as only nos, some blanks, only letters etc Vehicle Make - has junk characters, numbers, blanks etc Violation time - junk characters, blanks etc Should we ignore and go ahead? TAs pls clarify....",308437
134693,"I have formatted value of Violation time in proper time format but next challange is, it&#39;s in string format which will be difficult to group data into bins, so how to convert it back to date or timestamp format ?",315423
134694,"I have column in hh:mm AM/PM format (string format) Need to convert this into HHmm (24-hr time format, without date) Tried umpteen no of functions by googling, couldnt resolve. Is there any simple function/way of accomplishing this task?",308437
134324,"I am getting above error when using command park2 = park2.withColumn(&quot;Time&quot;, concat(col(&quot;Violation Time&quot;),lit(&quot;M&quot;))).show(5) What does this error mean?",308437
133686,"This case study problem statement is listed twice one using SparkR and one using PySpark. Given that we used PySpark in our modules earlier, will proceed with that. TA - please update the problem statement for this case study by removing SparkR reference.",308633
134780,"I want to bring some more attention on this topic of submitting how many &amp; what all files. This is really becoming an assignment in iteself to decide how many files are required to submit. The post which I saw was : https://learn.upgrad.com/v/course/208/question/134238 As seen below, this question was asked 3 days ago i.e. 21st June, 2019 And then one of the candidate answers the question on the same day &amp; request the TA to verify. To which the TA replies that we need to submit only .ipynb file as shown in the screenshot below: And then finally a TA answers this question today i.e. after 3 days &amp; rather on the day when assignment needs to submitted redirecting all of us in a new direction. The same can be seen in the screenshot below. There might be some sort of confustion but in the ideal case, as most of us including me have already submitted the one python notebook as mentioned by one of the TAs. How can one expect that now we should submit a doc file as well. We're working professionals &amp; its hard to find time from our daily routines especially into entirlely different domains. One cannot expect that now we'll create the doc file &amp; resubmit. I would not expect any clarifications regarding the number of files to be submitted on the day of submission of assignment.",318355
133541,"Hi, I am getting the error while attemting to connect to the kernel. Please help Connection failed A connection to the notebook server could not be established. The notebook will continue trying to reconnect. Check your network connection or notebook server configuration.",313676
133718,"Bcoz the problem statement says to analyse tickets over 2017 only, should I go ahead in deleting all 2016 data at once and then proceed. So that for further questions, I need not put a where/filter clause for year.?",308437
133909,"I see Violation Precinct and Issuer Precinct informations and havin same data, is this correct, or i have wrong data ?",312019
134795,What are you doing with records having violation time in 24 hours clock also where there is no A or P at the end. I am facing issues with handling conversion for 12 hour and 24 hour time in the same sql. using functions from_unixtime(unix_timestamp( +----+-------------+-------------------+ |test|ViolationTime| t1| +----+-------------+-------------------+ |null| 2150P|2017-06-15 21:50 PM| +----+-------------+-------------------+,314197
133577,"Hi all, I was going through the assignment content &amp; first its written that we need to submit Jupyter Notebook as shown below Moving on to the next section, its mentioned to do it in the RStudio as shown below: It seems this is a content error. I request the TA to please confirm that we need to do the analysis in PySpark &amp; submit the code in form of Jupyter notebook. Thank you!",318355
133975,"On the sprak data frame while checking null check, shows nothing with below commands select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show() df.dropna() Both above nothing changes, shows like no null values But when i check manually ViolationTime values few are there as nan values. Why both of above checks are not captures ?",312019
113932,nan,317073
112117,nan,308638
112780,"Using k means I was able to get the clusters, but other than checking which cluster has what countries with high mortality rates, is there any other way to sort the clusters based on a feature?",312491
112113,"The columns of the PCA output are 0,1,2...., but how do I know which are those columns?",316255
111868,Few columns in teh dataset are mentioned as percentage of GDP. Is there any modification being done for those columns ?,300698
112132,9. Do scatter-plots of the PCs post outliers - What is expected here?,313826
112019,nan,312093
112126,"-------------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-103-92df67ddd7d9&gt; in &lt;module&gt;() ----&gt; 1 hopkins(df_kmeans) &lt;ipython-input-102-53981c731d97&gt; in hopkins(X) 10 n = len(X) # rows 11 m = int(0.1 * n) ---&gt; 12 nbrs = NearestNeighbors(n_neighbors=1).fit(X.values) 13 14 rand_X = sample(range(0, n, 1), m) AttributeError: 'numpy.ndarray' object has no attribute 'values'",314197
112021,How to add the non PCA Columns back to the PCA dataset? I calculated the PCA and now need to add country name back to the dataset obtained.,314221
112262,nan,318797
112209,"NGO_final. InfoCan some &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 7 entries, 0 to 8 Data columns (total 16 columns): index 7 non-null int64 child_mort 7 non-null float64 exports 7 non-null float64 health 7 non-null float64 imports 7 non-null float64 income 7 non-null int64 inflation 7 non-null float64 life_expec 7 non-null float64 total_fer 7 non-null float64 gdpp 7 non-null float64 PC1 7 non-null float64 PC2 7 non-null float64 PC3 7 non-null float64 PC4 7 non-null float64 PC5 7 non-null float64 Feature 7 non-null object dtypes: float64(13), int64(2), object(1) memory usage: 952.0+ bytes .................... Can some one tell how to rectify it.",318814
112323,"Clusterid is varying when i run the K mean again for k= 5, e.g two times same code. Is it normal? or it should be fixed if i run the code multiple times.",306243
110724,"Do we need to perform outlier treatment for the PCA &amp; Clustering assignmnt: If we do it, we end up dropping about 25% of data. Meaning we might miss out on important countries. And if we dont do it, we get clusters with highly unequal no of countries in them. What do we do in general in such a scenario? Requesting TAs to pls respond asap.",311686
111296,nan,318446
111601,"After making clusters, I am trying to classify outlier data into one of the cluster. To project the outlier data into PCA space, we need to normalize the outlier data before i can user pca.transform(normalized_outlier_data) Not sure how to obtain outlier data in normalized form. Doing this looks incorrect to me normalized_df=(df-df.mean())/df.std() where df = outlier data frame.",307509
111608,nan,314313
111961,"I am doing outlier treatment after finding PC's. Let's say, i arrive at 5 number of principle components. Now outlier treatment needs to be done one by one for all PC's? Or we need to treat it on entire DataFrame in one go.",307495
112231,Can the predict function of KMeans be used to assign outliers to respective clusters ?,313691
112940,"i have dropped the column -( country)from country_data. after performing pca, i want to link the country column to pca dataframe which was dropped earlier. i am not able to merge, please help",320606
111230,nan,307494
111311,nan,317418
111820,"Statement in assignment: ""Basically, you need to perform PCA on the dataset in order to obtain the Principal Components, perform Clustering on the countries using a suitable number of components and show the clusters on those components as well as a few selected variables."" The highlited part makes me understand it as: "" Generate PCs, perform clustering using them and then explain the country grouping on the basis of PCs and a few selected variables"" All discussions in forum state: "" Do 4 independent cluster models"" Ref: https://learn.upgrad.com/v/course/208/question/109856 Which is the correct requirement? Request TA verification.",308637
110776,"Hi, Assignment page mentions two points : 1.You are also supposed to use dimensionality reduction using PCA to get the visualisations of the clusters in a 2-D form. 2. The clusters must be visualised on both the Principal Components and some of the original variables.Basically, you need to perform PCA on the dataset in order to obtain the Principal Components, perform Clustering on the countries using a suitable number of components and show the clusters on those components as well as a few selected variables. Does it outirghtly mean that we have to use only 2 PCs for clustering? If not and suppose I use 4 PCs then how to show the visualization in 2-d form which is also to be included in the PPT?",311686
110893,nan,314313
110906,nan,314313
110950,This is mentioned in Evaluation Rubric : K-means algorithm is utilised properly and at least two iterations are done on different K. What exactly does it mean? If we have to build multiple models with different values for K then visualizations also to be shown for all of them? Requesting some TA to respond asap.,311686
111287,"pred_probs_test = model_pca.predict_proba(df_test_pca)[:,1] ""{:2.2}"".format(metrics.roc_auc_score(y_test, pred_probs_test)) While running above line i am getting below error message:- AttributeError: 'KMeans' object has no attribute 'predict_proba' Anyidea on above.",320103
111514,"This is the code : pca.explained_variance_ output : array([6.70739440e+08, 3.68465803e+07, 1.25097869e+03, 9.81348399e+02, 1.35148254e+02, 7.34938583e+01, 1.28824224e+01, 4.23786889e+00, 6.12008307e-01]) %matplotlib inline fig = plt.figure(figsize = (12,8)) plt.plot(np.cumsum(pca.explained_variance_)) plt.xlabel(&#39;number of components&#39;) plt.ylabel(&#39; explained variance&#39;) plt.show()",319759
111107,"When we got the independent variables say X , we can check their co-relation using heatmap. Further in the housing case study example these are the following lines of code used. corrmat = np.corrcoef(X.transpose()) p=np.diagflat(corrmat.diagonal()) corrmat_diag_zero = corrmat - p print(&quot;max corr:&quot;,corrmat_diag_zero.max(), &quot;, min corr: &quot;, corrmat_diag_zero.min(),) This is basically just to find the max correlated and min correlated variable right using the python code which otherwise is the same thing we can get to know if we straight away visualize the heatmap. What inferences can be drawn from it? Does it provides some extra info apart from finding max and min corelated variables?",301114
111805,nan,306738
110933,nan,300718
110955,nan,318329
110944,nan,318329
109905,"Do we need to perform outlier treatment for the PCA &amp; Clustering assignmnt: If we do it, we end up dropping about 25% of data. Meaning we might miss out on important countries. And if we dont do it, we get clusters with highly unequal no of countries in them. What do we do in general in such a scenario? Any suggestion is appreciated.",310511
111902,Please suggest the the steps-- 2) What is the difference between Outlier Treatment using RFM and thru percentile?,318846
111835,nan,318455
110341,RFM generally is related to some invoice dates.,301114
112145,"Do we need to convert child_mort into percentage per 100 live births as well as other attributes like exports, health, imports are provided as % of total GDP.",314730
112041,I have peformed PCA and have identified the Principle Components. Now I have a dataframe with the principle components. How do i add the country name to these rows?,312096
111129,"For this assignment, does this mean we have to perform below steps 1. Reduce the dimensions using PCA 2. Once its done, we need to prepare clusters using K-Mean and Hierarchical methods 3. and finally plot the output of the clustering in 2D space using bar graphs? i.e. each dimesnion vs clusters?",304814
112096,"After PCA, I am getting from 9 columns to 5, but the indexs are 0,1,2,3,4. How do I know what are those columns ?",318780
111143,nan,317418
112046,nan,304319
112153,"Hi, After doing PCA we have got dataframe where columns are named 0-4 (for component=4). How to map them to original columns for outlier analysis and clustering. Confused. Need help.",317156
111151,nan,307494
112335,nan,300721
112047,AttributeError: &#39;numpy.ndarray&#39; object has no attribute &#39;column&#39; while executing the colnames = list(NGO_data_PCA.column.values) where NGO_data_PCA is the dataframe. Please can nay one tell me whats the error ?,318814
111205,"question says use k means and hierarchial methods..So steps are: first use PCA to bring down no of features, then use K means and hierarchial methods separately and compare the results? TAs please confirm",308437
111538,"Post PCA, we are supposed to add country back to PCA Output, in my case df_train_pca and then run regression plot to see which country occupies which cluster What syntax is used for Concatenate please append etc and how are we doing clustering post adding is it a Scatter diagram because videos in session taught have not done any clustering, it was based purely on RFM which is not applicable here",344598
111585,nan,302741
109856,"One of the results expected for the assignment is a well-commented Jupyter notebook containing the Clustering Models(both K-means and Hierarchical Clustering) and the final clusters of countries. The clusters must be visualised on both the Principal Components and some of the original variables. Basically, you need to perform PCA on the dataset in order to obtain the Principal Components, perform Clustering on the countries using a suitable number of components and show the clusters on those components as well as a few selected variables.+ Does this mean, we need to built 4 models in all: 1. by K-means Clustering based on Principal Components 2. by K-means Clustering based on some of the original variables 3. by Hierarchical Clustering based on Principal Components 4. by Hierarchical Clustering based on some of the original variables",310511
111236,"Are these 2 countries duplicates? - Congo, Dem. Rep. (row number 37) - Congo, Rep. (row number 38)",306248
113418,nan,318772
111935,What does randon_state=42 here signify .?,312033
112162,"After removal of outliers from the PCA dataset, clustering is performed. So how we will merge the PCA dataset with the original dataset as there will be mismatch with the country names",318804
111312,nan,320687
111300,"It was already mentioned in the &quot;Outlier&quot; analysis discussion, that we need to remove the Outliers, perform the Clustering and then we need to assign these Outliers back to the nearest cluster cenroids. My question is &quot;How do we do thta? This wasn&#39;t discussed in the session. Can anyone help me on the approach. Instead of this, I think, we can impute the Outliers with the mean value of its corresponding columns. Can you share your thoughts on this approach",316202
111321,"While I try to perform pca.fit(X) , I get the following error : ValueError: could not convert string to float: &#39;Zambia&#39; Can you please help me in resolving this.Thanks",319759
111318,"As part of PCA and Clustering Assignment, when we build the model do we drop country in X and retain all other variables and then have Country in Y. If we do this are we indirectly saying Country is independent variable.... because Clustering and PCA doesnt have concept of Independent variable",344598
111331,"Once PCA analysis is done, it reduces the number of dimensions. However, it results in columns heads name as 0,1,2, etc. reflecting thr reduced number of dimensions. What is the best way to add back the dimension name to this DF, as clustering analysis will need the name as well? Even in the sample examples, all the PC dimensions are shown as 0,1,2, etc. without converting them back to the names. Will the index of the reduced DF be the same as the original DF with original variables for direct mapping?",310509
111347,,318455
111361,nan,314313
111685,nan,318455
111691,I have 5 variables after PCA and total of 5 clusters. Now I want to check countries in their corresponding clusters. How to check that ? Attching screenshot for the clusters.,318780
111716,"Instead of removing the Outliers is it okay if we impute the outliers with Mean, first quartile or with third quartile values and perform PCA? And can we use the same data for Clustering as well? Will we have any impact on the final result?",300727
112213,nan,318797
113466,nan,318772
111738,"After we do PCA and fit(), where do we get the various matrices of SVD?",300698
111739,"If we remove the outliers, do we ignore them or assign clusterID to them? How do we do it in Python?",300698
111855,"Made PCA. Got required no of pca's example as per scree plot Now table has pca1,pca2,pca3,pca4,pca5,features,country. Now we need to proceed with this new data frame above further doing outlier treatment and k mean,heirarchical clustering. Plese correct me.",312019
111740,I am getting different plots for number of countries in cluster using KMeans and Hierarchical.. Is that okay?,300698
111416,Do we need to do df_pca.fit() or df_pca.fit_transform()? What is the difference between the two in this case ? As there are no seperate test and train data.,304319
111925,Above are my clusters This is my country column as a df Getting this error,318451
111946,"Few columns are given as percentage of total GDP, but we dont have Tatal GDP, we have given GDP per capita. How to calculate the actual value? Is it required?",301649
111421,"After PCA is done, we have PCs in hand. How do we do clustering on both PCs and original variables? Does it mean that we also need to do clustering without PCA?",304319
111965,nan,314678
111968,"After performing the PCA, i converted it to a dataframe which had all the columns except country. Then i performed outlier analysis using quantile and end result was just 3 columns. Is something wrong here? In case i give this outlier treated dataframe to hierarchical clustering i get error that could not convert string to float: &#39;total_fer&#39;. But when i check the dataframe.info() all are float. I am stuck here and unable to proceed.",301114
112050,"I am unable to convert the result of the K-means into a dataframe to view it and concatenate it. I get the error ""DataFrame constructor not properly called!""",316416
111408,nan,318797
111426,"Since this is a smaller dataset we use PCA, incremental PCA is not needed right? If incremental PCA is used what would be the outcome of it? How to decide between the two?",301114
112087,"I have performed the clustering on the dataframe with my principle components. How do i interpret these principle components? When i check the means of these principle components across the clusters, how do I know what these stand for?",312096
111749,Have done PCA . Not sure how to proceed to K-Means clustering. normalized_df has all columns without county with standarixed numerical variables. Original dataframe df has all columns How to i proceed from here to K means clustering..Thanks,319759
112378,nan,308495
112008,nan,314678
112081,I have my original dataframe with 167 rows and 10 columns and converted counrty as index. In my PCA dataframe i have 155 rows after removing outliers and 5 columns pc1 to pc5. Now i wanted to merge these 2 datasets . How to do that? i created PCA dataframe from array so definiytly it dosn't have country as index.,300735
111216,"after plotting heatmap for initial 9 variables, we find lot of variables correlated (&gt;0.6) should we drop them straight away, it doesnt make sense for me though since there are only 9 variables to do analysis. Should we continue with the analysis with such highly correlated variables? Should we let PCA do this job of eliminating highly correlated variables?",308437
111451,"As i understand we need to seperate Country variable and then perform PCA on the remaining data set. once PCA done we need to again rejoin country column with the PCA DF to perform Clustering and view results. Now i am using the concatenate function to join Country with PCA DF, is this the correct approach? post Concatenation i get quite a few NANs either in country column or PCA columns - what could i be doing wrong?",316036
111731,Initially i&#39;m trying to remove outliers from a column of a dataset. So once after removal of initial set of outliers i have plotted boxplot and seeing few more new outliers now. This is beacuse it is calculating the outliers with new mean and standard deviation happended after removal of outliers in the first iteration. How to avoid this and how to completely remove outliers in the first iteration itself?,318579
111733,"After doing PCA i appended the country column using np.hstack now I am not able to proceed further with the k means clustering step i.e. Hopkins statistics, assign the value of k etc",318451
111452,"Once PCA is performed and we get the PC components, should we add/concatenate few selected original variables and perform clustering? Or could we add the original variables in the end after clustering, for the purpose of visualizing the Clusters alone?",300697
111454,"Once PCA is done, i got 5 Principal components are enough than 10 there by reducing the dimensions and explaining the variance, now how do i relate this to clustering. I did clustering post this using same Country data set, and got using Silhouette Analysis that arounfd 2 to 5 clusters are enough. How do i related both and do a plotting",344598
111458,go through the link for better understanding of outliers: https://blog.socialcops.com/academy/resources/when-delete-outliers-dataset/,318017
111169,"Do we need to convert Export, Imports and Health; since they are expressed as %age of GDP??",316349
111819,"Following the steps given by TA's When I concat cluster with PCA, I am getting lot of NaN. I know it is coming because of index mismatch. One probable solution is to reset index before concat. But I want to retain the index as it is if poosible. How it can be achieved ?",317991
111482,It is mentioned that we need to do both K-means and hierarchial clustering in the assignment. Step 1: Do K-means clustering for PCs (obtained after PCA) for some values of K Step 2: D hierarchial clustering for PCs (obtained after PCA) Compare results of step 1 and step 2 . Is this what is required as part of assignment? Should we get same insights from both types of clustering?,308437
111506,nan,318455
111984,"Once you have done the Principal component Analysis and selected Pricipal components, Outlier Treatment to be done on the selected Principal components or original variables?",317514
111520,I am confused whether we need to standardise the values or not in clustering and pca assignment because we are getting quite a lot of outliers in it. Can someone please put some light on this and explain the same in much detailed manner!,301655
111828,"[Edit: Aplolgies for the font. I am unable to control it's size...] Outlier treatment before PCA is recommended as we need to ensure that the statistical spread of data after normalization is not affected. Removing outlier countries and manually adding the countries back to clusters seems to be counter productive. Scalers such as RobustScaler (https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) are built to implicitly dealwith outliers. If I use such a scaler, do I need to treat my data for outliers?",308637
112022,nan,310472
111524,"Please correct me if I'm wrong; We're expected to be treating the outliers which remvoed almost close to 20-25% of data from your main dataset. Now, with this remaining data i.e. 80%, we run the PCA and K-Means which gives us say 5 clusters. Now, do we need to assign the removed Outliers back to the nearest clusters so formed (5 clusters)? If yes, how do we do that?",316349
112168,"But do we need to perform outliers on PC1 , PC2...sepearetly or on the whole merged data ?",318814
111526,"For outliers, I belive we need to apply only transform ( not fit and transform) of the standardscale and PC. please verify.",310467
111546,"a . outlier treatment for each variable b .scaling the data (Not doing RFM, as i learnt from a discussion that it is done only on transactional datasets). Is it here that we fit() and fit_transform on country? c .k-means - Silhouette Analysis d .Heirarchical clustering e .PCA If I am totally going wrong can someone please outline me the steps or just a high level summary.",314313
111591,nan,318598
111582,nan,318579
111583,nan,318579
111555,"after obtaining clusters for Kmeans and hierarchial, it is asked in question to have visualization of clusters in 2D form? How to get these?",308437
112003,nan,310472
111587,If we remove outliers before starting the PCA and if we want to re assign them to their nearest clusters in the end. What is the best approach to do this as final cluster values are principal components but outliers will be original values. Also can we remove outliers after standardization of data?,318579
111596,nan,318797
111917,"DataError Traceback (most recent call last) &lt;ipython-input-653-260ff731d508&gt; in &lt;module&gt;() 6 km_clusters_life_expec = pd.DataFrame(pcs_df_km.groupby([&quot;ClusterID&quot;]).life_expec.mean()) 7 km_clusters_total_fer = pd.DataFrame(pcs_df_km.groupby([&quot;ClusterID&quot;]).total_fer.mean()) ----&gt; 8 km_clusters_gdpp = pd.DataFrame(pcs_df_km.groupby([&quot;ClusterID&quot;]).gdpp.mean()) ~\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py in mean(self, *args, **kwargs) 1304 nv.validate_groupby_func(&#39;mean&#39;, args, kwargs, [&#39;numeric_only&#39;]) 1305 try: -&gt; 1306 return self._cython_agg_general(&#39;mean&#39;, **kwargs) 1307 except GroupByError: 1308 raise ~\Anaconda3\lib\site-packages\pandas\core\groupby\groupby.py in _cython_agg_general(self, how, alt, numeric_only, min_count) 1054 1055 if len(output) == 0: -&gt; 1056 raise DataError(&#39;No numeric types to aggregate&#39;) 1057 1058 return self._wrap_aggregated_output(output, names) DataError: No numeric types to aggregate",301114
111567,"After PCA and fit when we check the pca.components_ we get the loadings aka eigen vectors. Similarly for the same data when we do IncrementalPCA and fit and then check pca.components_, the eigen vectors are slightly different with some of the signs reversed. Why is this so? And which one to use for analysis?",304319
111215,"In the example in lectures, we perform PCA on train data set (there we had response variable, so we split into x_train,y_train etc) Here for our example there is no response variable Y In such a case how do we split into train and test??",308437
111606,"This is the scree plot iam getting - and my y-axis values dont reach 95%. pca_explained_variance_ array([4.13565658, 1.54634631, 1.1703833 , 0.99478456, 0.66061903, 0.22358112, 0.11343874, 0.08831536, 0.06687501]) Not reacing 95",319759
111380,"it is said that we need to do outlier treatment if at all we found anything as such. In that case, it is fair to do outlier treatment for all columns/features right? Approximately 25% of rows are eliminated (which ofcourse we should later somehow need to out back by assigning them to individual clusters) Hope this is right approach. TA pls confirm",308437
111367,"Do we need to do outlier treatment before doing kmean or heirarchical clustering, which we did in course for RFM ?",312019
111313,nan,317418
111840,"Hi, One of the TA has mentioned that Outlier treatment should be done post PCA. Is it wrong if we go it before the PCA? I don't think there will be any issue if we do Outlier treatment first and then perform PCA. Please guide.",314361
110631,While doing clustering as part of assignment do we need to consider few variables and proceed or need to consider all the variables.,320103
111773,"How do we remove the outliers from original data (with outliers) after PCA analysis (as suggested in the webinar) - i.e., on PCA data frame before the k-mean /hierarchical clustering?. Please provide steps.",311115
112049,As mentioned in the steps given earlier we need to perform K-means and Hierarchical clustering. And it is also mentioned in step 17 to perform Hierarchical Clustering for both single and complete linkage. Since the no. of clusters coming for both are different so do we need to perform mean analysis for both i.e single linkage and complete linkage hierarchical clustering. TA's please help.,317991
111541,nan,301644
111624,"Need verification on below two steps for PCA 1. Do a fit and transform on the non outlier data 2. Used the same fit obtained from non outlier and just transform the outlier data(no fitting here). But please note that later when I join both the PCAs (outlier and non outlier) and check its correlation, it shows correlation (max 0.6) in corrmatrix, which looks wrong. Can I follow the above steps?",310467
110639,"In the data dictionary, inflation has been defined as &quot;the measurement of the annual growth rate of the Total GDP&quot;. This is certainly not the traditional definition of inflation and materially impact how we look at inflation. Can the TAs please confirm if this definition is in line with how we should treat inflation in the assignment",305653
111620,"i have posted this query before and tried all options, but still unable to fix this issue. 1) y_train_DF=y_train_DF.reset_index(drop=True) This gives me a 97 rows &times; 1 columns DF with Index starting from 0 to 96. Refer screenchot below: 2) df_train_pca_DF.reset_index(drop=True) This gives me a 97 rows &times; 4 columns DF with Index starting from 0 to 96. Refer Screenshot Below: 3) Country_Train_DF = pd.concat([y_train_DF, df_train_pca_DF], axis=1) This Gives me a 98 rows &times; 6 columns with Index starting from 0 - 97 Refer Screenshots below: Screenshot 1: Screenshot 2:",316036
111420,nan,310472
111417,nan,318797
111481,"it is little difficult to interpret the clustering results. (not explained clearly in lectures as well) We do PCA on original dataset. We obtain PCs. We do clustering on them. Then we finally assign cluster IDs to each row. After that we do bar graphs, where we get mean for each feature and under each feature for each cluster. What do we infer from these bar plots (with means)? How to get actionable insights from these? The x-labels of bar plots represent cluster ID - how do we get to know countries belonging to each cluster?",308437
111430,"I am not able to get enough understanding of this statement, Please suggest more clarity on it.",315423
111888,The topic of outlier has been discussed to death and we have been told to focus less on that; and that to drop outliers after doing PCA and choosing n number of PCAs. Dropping outliers makes me very uncomfortable considerring all these are real data and important data as it decides which country gets financial aid and helps the wellbeing of the country (as opposed to just say financial transactions). So say I drop Nigeria because it is an outlier (based on say dropping data that are more than 2 SDs away from the mean for each PC). Say It is outlier on 3 of the 4 chosen PCs whereby it is more than than minus 2 SDs from the mean. Does that not mean that I am possibly dropping a country (Nigeria) that needs financial aid the most just because it is an outlier in 1 or more PCs?? I find that disturbing. I personally would not want to drop any outlier countries as these are all real meaningful and high impact data. Please advise.,300694
112183,nan,318797
112180,Can anyone share a link of how to do outlier treatment?,315423
112206,"For presentation, can we recreate the plots in Tableau ? Or we need to add plots created on python.",317991
112208,nan,317073
111911,nan,300729
112176,"Python code by IIIT professor , doesn&#39;t explain much even in Video , unable to make out. I have K=3 but not able to make out , How to use in find clustering..",311386
112200,nan,307492
111633,"If you're using inliner as ""&gt;=Q1 - 1.5*IQR""; how do we extract the opposite of this (outlier) into another dataframe?",316349
111214,"Talking about outliers, i find that there are atleast 5 features which have clear outliers. Now, 1. Is it necessary to treat these outliers? this wasnt taught in class. How to proceed? 2. What happens if we ignore these and just go ahead? does it make a major impact on our analysis?",308437
113937,"I did not get the first step i.e. conversion of % values into absolute value in the solution of Clustering and PCA assignment. My understanding regarding the conversion of % variables into absolute value was like this As mentioned in the data dictionary GDP per capita (GDPP) was given as GDP / Total population. Now since we were not given Total population how could we have calculated GDP. And since we could not have calculated GDP then on what basis we should have converted Exports, Health and Imports into their absolute values. In the solution they have calculated absolute values based on GDPP. What is the point of converting exports variable into absolute values using GDPP despite of not having total population values. Can TA's clarify this doubt?",317991
130523,nan,301646
130524,The stripe footer contains a directory of stream locations. What does this means about the strip footer characteristics?,318372
130675,nan,318791
129752,"Error encountered is: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask Logs: 0, text: result.logs, logScroller: result.logs, logScrollerVisibilityEvent: showLogs, niceScroll"" tabindex=""4""> ERROR : Failed with exception Exception when loading 128 in table amazon_reviews_year_month_partition_orc with loadPath=hdfs://nameservice1/user/hive/warehouse/your_partition_folder_name_orc/.hive-staging_hive_2019-05-31_04-03-56_570_6847672996828128324-300/-ext-10000 org.apache.hadoop.hive.ql.metadata.HiveException: Exception when loading 128 in table amazon_reviews_year_month_partition_orc with loadPath=hdfs://nameservice1/user/hive/warehouse/your_partition_folder_name_orc/.hive-staging_hive_2019-05-31_04-03-56_570_6847672996828128324-300/-ext-10000 My Code: -- First, create ORC table create external table if not exists amazon_reviews_year_month_partition_orc (reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) partitioned by (yr int, mnth int) stored as orc location '/user/hive/warehouse/your_partition_folder_name_orc' tblproperties (""orc.compress""=""SNAPPY""); -- Then, write data from partition table into ORC table insert overwrite table amazon_reviews_year_month_partition_orc partition(yr , mnth) select * from amazon_reviews_year_month_partitioned; Please let me know where am I going wrong ?",301655
130265,I am not able to see the common folder - &quot; /common_folder/airlines/ &quot; in Hive? how to access the same?,310509
130459,"I keep getting this error on Hue in Corestack platform while running the query Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask Any inputs to resolve it?",310501
130268,"What is inside that jar file /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar, is it default for all other big default platform.",318319
129499,nan,304692
130270,"Hi, I am not getting output of bollywood_movies table output in correct format.",318319
131667,"this is my table creation code. create external table if not exists amazon_reviews_table(reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths' = '') location '/common_folder/amazon_reviews';",317073
129522,,311160
135626,nan,311466
130323,"Getting above error while executing the below code used in the video topic ' 'Understanding and Analysing the Data Stored in Hive Tables' create external table if not exists electronics_columns_s3(reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe' with serdeproperties ('paths' = '') location 's3://electronicsdata/amazonelectronics'; What does this error means ?",318479
129695,"Please someone help on getting rid of this error. As per lecturer, this can be cleared by cleaning the unwanted files on HDFS and then re running the JAR and then finally the code, it should work. But, did not tell how to delete the unwated files or where to see the unwanted files on HDFS. Please help..",316349
130151,All the data of the columns are coming in the first column itself.,318427
129958,"ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; create external table if not exists Electronics_5 (json_string string) location &#39;/common_folder/amazon_reviews/&#39;; select * from Electronics_5 limit 10; create external table if not exists amazon_review(reviewerID string,asin string,reviewerName string,helpful array&lt;int&gt;,reviewText string,overall double,summary string,unixReviewTime bigint) row format serde &#39;org.apache.hive.hcatalog.data.JsonSerDe&#39; with serdeproperties (&#39;paths&#39;=&quot;&quot;) location &#39;/common_folder/amazon_reviews/&#39;",300721
129482,"We are asked to create 4 columns namely: Movie, Lead_Actor, Release_Date, Total_Collection and Verdict. But the actual dataset contains more than 4 columns. So the mapping of Total_Collection happens with Ocollection present in the dataset. This gives us a wrong answer while calculating average collection of movies. Is there a way around this?",310505
130221,I tried the one received for Corestack URL login for not accepting.,301116
130443,"select avg(overall) from amazon_reviews_table where (size(split(&#39;reviewtext, &#39; &#39;)) &gt;= 206) and (year(from_unixtime(unixreviewtime)) = 2008); Error while compiling statement: FAILED: ParseException line 4:48 character &#39;&lt;EOF&gt;&#39; not supported here plz help me overcome this error in my above code",319759
130237,"even a simple query like select (count distinct) is taking lot of time to execute, without any results. What could be the reason?",308437
130967,Can someone help to resolve this issue.,310179
129767,Encountered this error while executing &quot; ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39; How to resolve this? Thanks,319759
129776,Got this error while running the select query. Plz help in resolving this.Thanks,319759
129806,Database name&quot;bin_muki&quot;. Table name: airline created But &quot;Table not found&quot; error throwsuo while execution,319759
129539,I have created a new database. Is it required to drop the table first and then create the partitioned table ?Anyways the table does not exist.,301114
130896,"Can anyone explain use of above property while creating a table. In case I want to name my columns different from that of available in json field, what will be syntax for SERDEPROPERTIES ?? In case I dont need any change from json format, even then is it mandatory to use SERDEPROPERTIES keyword in query ??",318770
134331,Spark session by specialist taken is in very detail but the course timing is not good instead of 1 weeks it should be given 2-3 weeks.,307843
129366,"When going through one of the videos on &#39;Introduction to Apache Hive&#39;, its stated that in the corestack environment we have RStudio to run Spark(R). Till now the whole course was in Python. Can you please confirm if we would be taught Spark in Python or R, so that we at least start with basics of R. Thank you!",318355
129369,nan,304692
130375,"Hello, I am trying to login to cloudenablers with correct ID and password. But while logging in I am getting error message &quot;Incorrect username or password&quot;. Did anyone faced similar issue! Please let me know.",303227
130376,Getting below error wehn running &quot;select * from bollywood_movies limit 10;&quot; statement on the bollywood table created using bollywood dataset:,303085
129082,"Hi, For how long access to the corestack lab environment is going to be available?",310974
129961,nan,318851
129964,,300713
129980,"If a user is present in the Hive , how he can know if a table is Managed table to External table . Is there a ways by doing some operation on table I can found this ? Or If i need to connect the External Table , i assume we need to give the external connection details of HDFS / S3 . But after the connection is established it is stored as metadata . Now is there any way to distinguis these as Internal or External Data ?",311861
129919,"If we have two tables say Employee Employee_copy where &#39;Employee&#39; table is an external table &amp; &#39;Employee_copy&#39; is an internal table. We know that if the user deletes &#39;Employee&#39; table, the data would still remain there as its an external table. But when the user drops the table &#39;Employee_copy&#39; which is an internal table. Does the data also get deleted or the data remains as there is still an external table pointing to the data? Thanks in advance!",318355
129265,How can we access different application present in my Labs. When clicking on it - its not navigating to required URL.,320103
130238,nan,308673
129327,"I am able to launch various aplication Cloudera Manger and then selecting any application from &quot;Clusters&quot; tab. But there is no provision to access WebConsole. Any idea to access the web console, as it was shown in second video link.",318778
129413,nan,312033
129448,Are the queried to be executed in the default or the new db?,311857
129483,Why we have to execute hive- hcatalog -core-1.1.0-cdh5.11.2.jar at the very begining ? Is it a pre-requisits in the production enviorment also ? What hive- hcatalog -core-1.1.0-cdh5.11.2.jar does internallly ? Thanks,312479
129631,could some one help why table is creating always string even i am going columns as int and double etc.,312019
129472,The page is not loading when I try opening the cloudenablers through the link provided in the https://upgrad.corestack.io/. Can someone help to resolve this section?,317857
129693,"HI, Allways am getting this error, can some one help me out",317410
129574,nan,300684
129553,"Why running the Jar file is necessary before running any query? Is this something specific to cloud based HIVE? because when i use HIVE in my day to day work, there is no requirement for such one liner execution",318159
130369,"Getting below error when creating external table and partitioninn it in the folder at the location mentioned in the query file. &quot;Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: hdfs://nameservice1./user/hive/warehouse/raveenakhan_amazon_reviews_yr_mnth_partition)&quot;",303085
129962,"CREATE EXTERNAL TABLE create external table if not exists amazon_reviews_table(reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) ROW FORMAT SERDE &#39;org.apache.hive.hcatalog.data.JsonSerDe&#39; with serdeproperties (&#39;paths&#39; = &#39;&#39;) location &#39;/common_folder/amazon_reviews&#39;;",305804
130048,,312376
130142,nan,320687
130052,html&gt; &lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;/title&gt;&lt;/head&gt; &lt;body bgcolor=&quot;white&quot;&gt; &lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;/h1&gt;&lt;/center&gt; &lt;/body&gt; &lt;/html&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt;,300721
129582,Anyone getting 1106.? I have used same query but different result. why different answer.?,312033
129586,"In Hue I created my own I created table. what location we should give here, its same as location &#39;/common_folder/amazon_reviews&#39;;. Where can i see this. How to add the data to my table ? Please clarify steps to create table with given data and whre to see the data. i could not found in file browser",312019
129294,"Hello All, I am trying to connect to sqoop db from WebConsole using below command. mysql -h sqoopdb.upg.cloudlab.com -u your_username -p your_password where your_username and your_password used as given in Sqoop setion of platform. I am getting the below error ERROR 1045 (28000): Access denied for user &#39;babita.parida_gmail&#39;@&#39;ip-30-0-41-204.ec2.internal&#39; (using password: YES) Please help.",307494
130114,nan,312093
131282,nan,318005
131554,as a non IT person can you give me the standered procedure for creating hue file and link to air flight link,319969
130150,nan,306248
132396,nan,318772
136810,nan,302742
130308,Did anyone get this message at the webconsole while entering the sqoop user id?,317575
132919,"I&#39;m getting the following exception while I&#39;m trying to export data, and I&#39;m not able to export any data 19/06/15 16:43:31 WARN security.UserGroupInformation: PriviledgedActionException as:ayyappa752_gmail (auth:SIMPLE) cause:org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern hdfs://nameservice1/user/ayyappa752_gmail/iris_data/* matches 0 files19/06/15 16:43:31 ERROR tool.ExportTool: Encountered IOException running export job: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern hdfs://nameservice1/user/ayyappa752_gmail/iris_data/* matches 0 files at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323) at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)",317073
130181,nan,320689
130568,I am getting the below error message while exporting 19/06/04 04:02:29 INFO mapreduce.Job: Job job_1558926375940_16899 failed with state FAILED due to: Task failed task_1558926375940_16899_m_000000Job failed as tasks failed. failedMaps:1 failedReduces:019/06/04 04:02:29 INFO mapreduce.Job: Counters: 12 Job Counters Failed map tasks=1 Killed map tasks=3 Launched map tasks=4 Rack-local map tasks=4 Total time spent by all maps in occupied slots (ms)=30620 Total time spent by all reduces in occupied slots (ms)=0 Total time spent by all map tasks (ms)=30620 Total vcore-milliseconds taken by all map tasks=30620 Total megabyte-milliseconds taken by all map tasks=62709760 Map-Reduce Framework CPU time spent (ms)=0 Physical memory (bytes) snapshot=0 Virtual memory (bytes) snapshot=019/06/04 04:02:29 WARN mapreduce.Counters: Group FileSystemCounters is deprecated. Use org.apache.hadoop.mapreduce.FileSystemCounter instead19/06/04 04:02:29 INFO mapreduce.ExportJobBase: Transferred 0 bytes in 24.1502 seconds (0 bytes/sec)19/06/04 04:02:29 INFO mapreduce.ExportJobBase: Exported 0 records.19/06/04 04:02:29 ERROR tool.ExportTool: Error during export: Export job failed! at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:439) at org.apache.sqoop.manager.SqlManager.exportTable(SqlManager.java:931) at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:80) at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252) This is the command I am using sqoop export -connect jdbc:mysql://sqoopdb.upg.cloudlab.com/mohanm0418_gmail -username mohanm0418_gmail -password xxxxxxx -table bollywood -export-dir /user/hive/warehouse/mohan_rao.db/bollywood/*,314092
130362,"I&#39;m getting the error: [pknayak0707.job_gmail@ip-30-0-41-204 ~]$ sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/pknayak0707_job_gmail --username pknayak0707.j ob_gmail --password pknayak0707.job_gmailuco7l --table iris --export-dir iris_data/* Warning: /opt/cloudera/parcels/CDH-5.11.2-1.cdh5.11.2.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. 19/06/02 18:40:15 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.11.2 19/06/02 18:40:15 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/02 18:40:15 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/02 18:40:15 INFO tool.CodeGenTool: Beginning code generation Sun Jun 02 18:40:15 UTC 2019 WARN: Establishing SSL connection without server&#39;s identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn&#39;t set. For compliance with existing application s not using SSL the verifyServerCertificate property is set to &#39;false&#39;. You need either to explicitly disable SSL by setting useSSL=false, or set use SSL=true and provide truststore for server certificate verification. 19/06/02 18:40:15 ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: Access denied for user &#39;pknayak0707.job_gmail&#39;@&#39;ip-30-0 -41-204.ec2.internal&#39; (using password: YES) java.sql.SQLException: Access denied for user &#39;pknayak0707.job_gmail&#39;@&#39;ip-30-0-41-204.ec2.internal&#39; (using password: YES) 19/06/02 18:40:15 ERROR tool.ExportTool: Encountered IOException running export job: java.io.IOException: No columns to generate for ClassWriter",318386
130212,nan,316211
130213,nan,316211
130246,How to rectify?,318474
130508,"I have created bollywood table in Mysql ran the following command for bollywood file. But that is failing. When I ran s similar command in hive, it was successful. sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/ruchitagup_gmail --username ruchitagup_gmail --password ruchitagup_gmailxvsyk --table bollywood --export-dir /common_folder/bollywood/* The error is",304319
129813,When i connect sqoop with webcosole it has showed database of nagalprasad_gmail and tables are empty. Actually i did not see the db for name nagalprasad_gmail. I created naga_db under Hive Db and tables created. how can i see this naga_db which has all tables created. I may be doing something wrong ?,312019
130826,"This Sqoop import tutorials are totally different. Could not able to execute these import and export. In Hive i have created naga_db and tables in it and queries. Fine. But i can't access this naga_db in sqoop to do sqoop import. And one default db called nagalprasad_gmail exits in sqoop db, this i can't access to make tables like hive. please find core stack people replay. Level-2 Support (CoreStack) Jun 5, 2:47 PM IST Hi Naga infact have created in hive naga_db which i cannot connect with webconsole. -- naga_db that you have created is available in hive, but you are trying to access in scoop which is not possible. -- nagalprasad_gmail is available in scoop which will not be access through hive. So kindly don't confuse with scoop and hive. The database which you have created in hive will be available only in hive not in scoop. @Sagar kinldy give him clerarification, he's having this issue from last friday. . Please help how to d sqoop import/export labs practice. big confusion we my need session on this sqoop with live creation.",312019
130668,"I am using the following command: sqoop import --connect jdbc:mysql://database.upg.cloudlab.com/sqoopdb -username your_username --password your_password --table table_name --m 1 --target-dir /user/labuser01/sqoopdbtest I am not able to find the values of username, password, tablename etc.,",313691
129942,,314678
130367,,308635
130364,"Hi All, I am getting the below error while importing data from mysql to HDFS [babita.parida_gmail@ip-30-0-41-204 ~]$ sqoop import --connect jdbc:mysql://database.upg.cloudlab.com/babita_parida_gmail --username babita_parida_gmail --password ***** --table iris --m 1 --target-dir /user/babita.parida_gmail/HDFS_dir Warning: /opt/cloudera/parcels/CDH-5.11.2-1.cdh5.11.2.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail. Please set $ACCUMULO_HOME to the root of your Accumulo installation. 19/06/02 18:52:26 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.11.2 19/06/02 18:52:26 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead. 19/06/02 18:52:26 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset. 19/06/02 18:52:26 INFO tool.CodeGenTool: Beginning code generation 19/06/02 18:52:26 ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: Access denied for user &#39;babita_parida_gmail&#39;@&#39;ip-30-0-41-204.ec2.internal&#39; (using password: YES) java.sql.SQLException: Access denied for user &#39;babita_parida_gmail&#39;@&#39;ip-30-0-41-204.ec2.internal&#39; (using password: YES) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:964) Please help.",307494
129765,I&#39;m getting the following error after trying to login to web console for Sqoop lab. Any way to debug this? I&#39;ve used the correct username and password for my login. Below is the snapshot for reference:,310505
130365,While running the command for exporting iris data from Sqoop to MySQL it gives below error: 19/06/02 18:52:31 ERROR tool.ExportTool: Encountered IOException running export job: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern hdfs://nameservice1/user/vipulshri102_gmail/iris_data/* matches 0 files What I can interpret from the error is file does not found ? But don't know which file is missing. How to execute this query ? TA please help.,317991
130160,I am trying to export the bollywood data to MySQL through Sqoop. I have created the schema in mysql. I am getting the below error on running the export command : [johnykrrish_gmail@ip-30-0-41-204 ~]$ sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/johnykrrish_gmail --username johnykrrish_gmail --password ****** --table bollywood_movies --export-dir /common_folder/bollywood/* 19/06/01 16:13:41 INFO impl.YarnClientImpl: Submitted application application_1558926375940_749519/06/01 16:13:41 INFO mapreduce.Job: The url to track the job: http://ip-30-0-21-241.ec2.internal:8088/proxy/application_1558926375940_7495/19/06/01 16:13:41 INFO mapreduce.Job: Running job: job_1558926375940_749519/06/01 16:13:46 INFO mapreduce.Job: Job job_1558926375940_7495 running in uber mode : false19/06/01 16:13:46 INFO mapreduce.Job: map 0% reduce 0%19/06/01 16:13:50 INFO mapreduce.Job: map 100% reduce 0%19/06/01 16:13:50 INFO mapreduce.Job: Job job_1558926375940_7495 failed with state FAILED due to: Task failed task_1558926375940_7495_m_000000Job failed as tasks failed. failedMaps:1 failedReduces:0 19/06/01 16:13:50 ERROR tool.ExportTool: Error during export: Export job failed! at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:439) at org.apache.sqoop.manager.SqlManager.exportTable(SqlManager.java:931) at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:80) at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:99) at org.apache.sqoop.Sqoop.run(Sqoop.java:147) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234) at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243) at org.apache.sqoop.Sqoop.main(Sqoop.java:252) I am unable to debug the error since in the console the error is not self-explanatory and also unable to check the status of the job.,318756
130664,"I am executing following command. I have created table named ""shalaka"" in my schema shalaka_mh_yahoo"" sqoop import --connect jdbc:mysql://database.upg.cloudlab.com/shalaka_mh_yahoo --username shalaka_mh_yahoo --password ***** --table shalaka --m 1 --target-dir /user/shalaka.mh_yahoo/sqoopdbtest I am getting following error Please Help. Error is - Warning: /opt/cloudera/parcels/CDH-5.11.2-1.cdh5.11.2.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.Please set $ACCUMULO_HOME to the root of your Accumulo installation.19/06/04 08:53:56 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.11.219/06/04 08:53:56 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.19/06/04 08:53:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.19/06/04 08:53:56 INFO tool.CodeGenTool: Beginning code generation19/06/04 08:53:56 ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: Access denied for user 'shalaka_mh_yahoo'@'ip-30-0-41-204.ec2.internal' (using password: YES)java.sql.SQLException: Access denied for user 'shalaka_mh_yahoo'@'ip-30-0-41-204.ec2.internal' (using password: YES) at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:964) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:873) at com.mysql.jdbc.MysqlIO.proceedHandshakeWithPluggableAuthentication(MysqlIO.java:1710) at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1226) at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2194) at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2225) at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2024) at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:779) at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at org.apache.sqoop.Sqoop.main(Sqoop.java:252)",308636
129473,"When I am running the below query i am getting following error : sqoop export --connect jdbc:mysql://sqoopdb.upg.cloudlab.com/johnykrrish_gmail -username johnykrrish_gmail --password ****** --table iris --export-dir iris_data/* 19/05/29 13:55:02 WARN security.UserGroupInformation: PriviledgedActionException as:johnykrrish_gmail (auth:SIMPLE) cause:org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern hdfs://nameservice1/user/johnykrrish_gmail/iris_data/* matches 0 files19/05/29 13:55:02 ERROR tool.ExportTool: Encountered IOException running export job: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern hdfs://nameservice1/user/johnykrrish_gmail/iris_data/* matches 0 files at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323) at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265) at org.apache.sqoop.mapreduce.ExportInputFormat.getJobSize(ExportInputFormat.java:51) Where does the iris data is supposed to be in hdfs ? Is it just for demonstration purpose ? Do we need to epxort only the data we have created in hdfs through hive tables like amazon_reviews,airlines etc ?",318756
132853,"I have a table which has a primary key of type UniqueIdentifier and other columns as strings. In this case, how mapping will be performed.",317073
130271,,304692
130128,"I m writing below query &amp; it seem this table is not found , can anyone help me here -- Then, write data from partition table into ORC table insert overwrite table amazon_reviews_year_month_partition_orc partition(yr , mnth) select * from amazon_reviews_year_month_partitioned; Getting below error Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 2:14 Table not found &#39;amazon_reviews_year_month_partitioned&#39;",311861
130306,"ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; SELECT explode( ngrams( sentences( lower(reviewtext) ), 3, 5)) FROM amazon_reviews_year_month_partition_orc where yr = 2013 and mnth = 1; I get error : Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 2:5 Table not found &#39;amazon_reviews_year_month_partition_orc&#39;",319759
130338,"This is the error- Could not connect to ip-30-0-31-114.ec2.internal:10000 (code THRIFTTRANSPORT): TTransportException(&#39;Could not connect to ip-30-0-31-114.ec2.internal:10000&#39;,)",301643
130292,"select size( split(reviewtext, &#39; &#39;) ),avg( size(split(reviewtext,&#39; &#39;)) ) as avg_lngth,variance( size(split(reviewtext,&#39; &#39;)) ) as vrnc from amazon_reviews_year_month_partition_orc where yr=2013 and mnth=1 limit 5; select avg( size(split(reviewtext, &#39; &#39;)) ) as avg_length, variance( size(split(reviewtext, &#39; &#39;)) ) as varinc from amazon_reviews_year_month_partition_orc where yr = 2013 and mnth = 1;",300721
131553,"insert overwrite table taxi_data_year_month_part_orc partition(mnth,yr) select * from taxi_data_year_month_part_orc WHERE (mnth=11 and yr=2017) or (mnth=12 and yr=2017) and (passenger_count &lt;&gt; 0 or trip_distance &lt;&gt; 0 or fare_amount &lt;&gt; 0 ); select * from taxi_data_year_month_part_orc where passenger_count=0 limit 10; Running this command but still not able to filter the data where these 3 vars are zero what is the issue with this query",300735
130310,"I am getting error while inserting data into the partitioned table as: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask Tried everything mentioned on Discussion Forum but the issue is not getting resolved. 0, text: result.logs, logScroller: result.logs, logScrollerVisibilityEvent: showLogs, niceScroll"" class=""logs logs-bigger logs-populated nicescrollified"" style=""overflow: hidden; outline: currentcolor none medium; height: 80px;"" tabindex=""4""> ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask INFO : MapReduce Jobs Launched: INFO : Stage-Stage-1: Map: 3 Cumulative CPU: 6.74 sec HDFS Read: 8267072 HDFS Write: 6702574 FAIL INFO : Total MapReduce CPU Time Spent: 6 seconds 740 msec INFO : Completed executing command(queryId=hive_20190602134141_a16fcf8a-6ef1-4f15-86f0-1922bef3834c); Time taken: 25.253 seconds",310210
130805,"core stack people says default db with your gmail name like nagalprasad_gmail. i could not see it so i have created naga_db in hive -&gt; Databses and tables etc But how to connect webcnsole webconsole not able to connect to naga_db which has tables webconslole shows db exits are nagalprasad_gmail, not sure where it is to add tables. pls clarify",312019
130244,"In the coursevideos,while typing auto-suggestion are provided.Is there shortcut key to get that?",318386
130254,"Eror while running hive query Could not connect to ip-30-0-31-114.ec2.internal:10000 (code THRIFTTRANSPORT): TTransportException(&#39;Could not connect to ip-30-0-31-114.ec2.internal:10000&#39;,)",313676
130377,"-- ORC FILE FORMAT -- This format improves query performance -- First, create ORC table create external table if not exists amazon_reviews_year_month_partition_orc (reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) partitioned by (yr int, mnth int) stored as orc location &#39;/user/hive/warehouse/your_partition_folder_name_orc&#39; tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;); -- Then, write data from partition table into ORC table insert overwrite table amazon_reviews_year_month_partition_orc partition(yr , mnth) select * from amazon_reviews_year_month_partitioned; -- Now you can query the ORC table -- First, let&#39;s find the length of some sample reviews RESULTING IN TABLE WITH EMPTY DATA. TOTALLY LOST WITH THIS STEP. NEED HELP!!!!!!",312199
130857,"While executing the below code in hive, I am getting null output, please help. select stddev_pop(size(split(reviewtext, &#39; &#39;))) from amazon_reviews where year(from_unixtime(unixreviewtime)) = 2004",316132
130327,What is the Command for finding correlation between 2 variables in a table?,319759
130494,,302742
130333,"Find the correlation between ""relative helpfulness"" (for example, in this case, 2/3 = 0.67) and ""length of the review"" for Jan 2008. How can we use helpful column in corr function",313770
130689,nan,318791
130391,I am getting above error as screen shot !! Anyone can sggest solution?,318770
131392,"&lt;html&gt; &lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;/title&gt;&lt;/head&gt; &lt;body bgcolor=""white""&gt; &lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;/h1&gt;&lt;/center&gt; &lt;/body&gt; &lt;/html&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt;",300735
131700,"select percentile(size(split(reviewtext, &#39; &#39;)), 75) from amazon_reviews_year_month_partition_orc where yr = 2008 and mnth = 10; This query is giving the following error : &quot;Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask&quot; log files have the same error : INFO : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1 INFO : 2019-06-09 12:04:59,665 Stage-1 map = 0%, reduce = 0% INFO : 2019-06-09 12:05:17,383 Stage-1 map = 100%, reduce = 100% ERROR : Ended Job = job_1558926375940_45205 with errors ERROR : FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask please note that below query is running fine : select percentile(size(split(reviewtext, &#39; &#39;)), 75) from amazon_reviews_year_month_partition_orc where yr = 2008 and mnth = 10;",317809
130007,"HI, For the error &#39;Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask&#39;. We should clean HDFS junk data, as per the earlier. But could you please let me know, how to resolve this error.",317410
130184,"I created a partitioned table however when inserting data i am getting below error Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask Please find below the statments create external table if not exists amazon_reviews_year_month_partition_samp (reviewerid string, asin string, reviewername string, helpful array&lt;int&gt;, reviewtext string, overall double, summary string, unixreviewtime bigint) partitioned by (yr int, mnth int) stored as orc location &#39;/user/hive/warehouse/your_partition_folder_name_orc&#39;; insert into amazon_reviews_year_month_partition_samp partition(yr,mnth) select reviewerid,asin,reviewername,helpful,reviewtext,overall,summary,unixreviewtime, year(from_unixtime(unixreviewtime)) as yr, month(from_unixtime(unixreviewtime)) as mnth from amazon_reviews_table; Could you please let me know what could be the reason and how can we rectify this error",302741
130050,Did anyone faced this issue while accessing or running the queries on Big Data Cloud Lab? I faced this error while running this command - ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar;,311502
130072,"Getting Error while inserting data from amazon_reviews_table t able to amazon_reviews_year_month_partitioned table: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask",300718
130134,I am trying to get airline123 data in airline123_orc but getting error. Pls suggest on how to proceed.,312033
136934,"In our course of Big Data Analytics , we have learned how to utilize &amp; do analysis on Structure data or Semi-Structured data . But till now we have not got any sample of handling the Un-structure data like PDF or Images . Can anyone TA or our Colleague give a sample code of doing the data analysis on Unstructure data like PDF if possible ?",311861
129643,folder s3 could not found where can i foud It says S3 filesystem exception. Access Denied,312019
131607,"insert overwrite table taxi_data_year_month_part_orc partition(mnth,yr) select * from taxi_data_year_month_part_orc WHERE (mnth=11 and yr=2017) or (mnth=12 and yr=2017) and (passenger_count &gt; 0 and trip_distance &gt; 0 and fare_amount &gt; 0 ); running this query and expecting and output where I should not get any rows with passenger cnt=0 or trip distance =0 or fare amnt =0 but this query is still providing the result with passenger cnt =0",300735
129504,"Hi, In the create table command, location specifies from which location the source data need to be taken from to create the Hive table right? But, in paritions module, SME says that the location specifies where the data will be stored. I'm confused. If location is from where the data is read, where is the table getting stored in?",310974
130155,"For eg. amazon_review table we created buckets with clauses like revieweres having first letter of their name should be in one bucket and so on, so there might be a scenario where we do not much reviewer with the name starting with let&#39;s say J so bucket having revieweres with letter J will be smaller in size.",300721
129416,"Getting &quot; Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask &quot; while trying to insert data into partitioned table. I am trying to create table from my database. I tried creating it from defailt databse too. Please refer to attached screenshot for more de tails.",312756
131743,nan,307710
130229,"I have created a folder /user/hive/warehouse/tezairlines and uploaded the data_2004-08.csv file into it using File Browser. I have run the following query successfully but do not see any results if I query select * from airlines_partitioned. Any suggestions are really appreciated. create external table if not exists airlines_partitioned ( `SNo` int, `Month` int, `DayofMonth` int, `DayOfWeek` int, `DepTime` int, `CRSDepTime` int, `ArrTime` int, `CRSArrTime` int, `UniqueCarrier` string, `FlightNum` int, `TailNum` string, `ActualElapsedTime` int, `CRSElapsedTime` int, `AirTime` int, `ArrDelay` int, `DepDelay` int, `Origin` string, `Dest` string, `Distance` int, `TaxiIn` int, `TaxiOut` int, `Cancelled` int, `CancellationCode` string, `Diverted` int, `CarrierDelay` int, `WeatherDelay` int, `NASDelay` int, `SecurityDelay` int, `LateAircraftDelay` int) partitioned by (`Year` int) location ' /user/hive/warehouse/tezairlines ';",318007
129726,,304692
130349,,305804
129928,"May be a trivial question, what is S3? I see the trainers keep using S3 / HDFS interchangeably. Are they both the same?",314730
130359,nan,314818
129922,"Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask",305655
129978,Can we load un-structure data like PDF in our HIVE platform ? Can someone tell the steps for it ?,311861
129611,nan,316211
132120,"""Error while compiling statement: FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table bharatidb.nytlcdatatable_orc that does not use an AcidOutputFormat or is not bucketed"" .",303228
130054,nan,311160
131549,Error while compiling statement: FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table How to delete the rows in the table,300735
131570,nan,308434
130447,"Hello, can you please help how to resolve this error which says access denied to load the data?",310509
125493,Please let me know if we need to drop mobile number column before PCA as it may cause lot of variance. Please correct me if my understanding is wrong.,310210
125781,"Since SMOTE() improves the metrics considerably, can we use SMOTE()? TA, please advise.",304319
124702,"Can somebody confirm, if we can use imblearn package to tackle Class Imbalance situation in our Telecom Churn Assignment ?",302740
124712,nan,317073
124716,nan,315277
124717,Does 0 stand for the name of the service scheme?,301643
125512,Can anyone plz clarify this?,301114
122726,"Regarding output variable churn out column data Is this missing. I could not found it in the csv file telecom_churn_data.csv. What is the basic procedure to do. We needs do multiple models for this classification problem ? Logistic, SVM, DecisionTree,Random Forest after PCA appying ?",312019
124739,nan,303228
124759,"Hi Paras/TA, Do we need to consider X is more than or equal to 70 percentile or only more than 70 percentile. With more than 70 percentile records count is 29954 and with more than or equal to 70 percentile records count 30002. Which should be considered please advise.",316215
124090,Which are the colmns consider for Topup Recharge and Data Recharge in months of June and July?,319759
125808,nan,314197
125222,"The &lsquo;churn&rsquo; phase : &quot;After tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase&quot; Please can someone elaborate this to get more clarity..",312518
125811,"'we have date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8', what we can do with it or maybe should I drop it as it's not letting me to scale the data features.",311466
122941,I wanted to list only non zero % .Is there any way (100*df.isnull().sum()/df.shape[0]).sort_values(ascending=False) When i use it list like below. arpu_3g_9 74.077741 total_rech_data_9 74.077741 av_rech_amt_data_9 74.077741 count_rech_3g_9 74.077741 count_rech_2g_9 74.077741 ... aug_vbc_3g 0.000000 jul_vbc_3g 0.000000 vol_2g_mb_9 0.000000 vol_2g_mb_7 0.000000 total_og_mou_7 0.000000 vol_2g_mb_6 0.000000 But i want the complete list and not to show 0.000 Can some one help on this.,312019
125841,nan,318455
125846,nan,318772
125834,nan,300706
126872,"for example If the root node has fbuser, and the next level has arpu_7, how do we get this information",311857
125499,"TA&#39;s, I know this might seem to be a repeat question but I am still asking because I havent seen a clear answer on this. In other questions of DF, TA suggested that to handle class imbalance, we do the following: For LOgistic Regression &gt;&gt; LogisticRegression(class_weight=&#39;balanced&#39;) Random Forest &gt;&gt; RandomForestClassifier(class_weight={0:0.1, 1: 0.9}) However, with this approach we are getting very low values of Sensitivity or Recall which is the parameter for model selection. Instead if we do an Over-sampling the minority class, by creating synthetic (not duplicate) samples of the minority class and hence making the minority class equal to the majority class, the parameter values are significantly improved. Please confirm if we can use this Over-sampling(or Under-sampling) procedure of handling class imbalance. If not, please let me know the reason why. TA&#39;s please help.",310511
125860,"As mentioned in various answer on discussion forum for handling class imbalance, we can use oversampling / under-sampling or any other technique to handle imbalance data. My question is when to perform this before train-test split or after train-test split. Or any other place ? TA's can help. Please share the reason also apart from the answer.",317991
125871,Since we are predicting the important dependant feature should such related columns be clubbed into one before proceeding with PCA?,301114
125872,nan,310009
125873,nan,310009
125874,nan,310009
125418,nan,318696
127035,Please share your views in the comment section.,322691
126118,"How to extract important features from the the high value custormer, Lasso + Random Forest Or RFE + Logistic (too lengthy) Is there any other approach? Need suggestions on this please.",306243
125495,Please let me know if we need to drop mobile number column before PCA as it may cause lot of variance. Please correct me if my understanding is wrong.,310210
125915,nan,318579
125777,"I have scaled the data before PCA, should I scale the data again after PCA and before modelling?",310009
125813,nan,314197
123194,"The definition of high value customers is defined as &quot;Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).&quot; So, we have total_rech_amt_6 and total_rech_amt_7. Finding the average of these two and taking 70%ile results in 29.9k rows as mentioned in the problem statement. total_rech_data has more than 75% missing values and I dont think is relevant from modelling perspective. If we include that to find the average and take 70%ile from that, the resultant dataframe contains only 5500 odd rows. So, kindly request you to validate and confirm, if it is ok to just take total_rech_amt_6 and 7.",318084
124174,nan,318804
125671,We are getting Logistic regression as predective power of 100% after the PCA?Is that has any meaning or taking the recall or confusion matrix help,307843
126003,"Apart Logistic Regression an Tree models, do we need to do SVM too?",311254
126344,as per problem statement for the assignment we are required to do PCA and then a model to predict. lets say we do PCA + LR When we perform PCA .. we split into Train and test (80/20) then we need to use the PCs as input into train/test for LR. Can we just use the training set from PCA (Which has been transformed) and input that into LR and then split that into train and test for LR? Other option would be to also transform the test DF for PCA. Then concat PCA Train and PCA test (lets call that PCA DF). Then split PCA DF into train and test for LR .. We would prefer option 1 as we would still have 80% of the high value customers and then we can again do a 80/20 split of this for LR.,300694
125287,nan,300687
125289,nan,311254
125291,"SMOTE treatment improves the precison/Recall and AUC ROC score drastically. However by using For LOgistic Regression &gt;&gt; LogisticRegression(class_weight=&#39;balanced&#39;) Random Forest &gt;&gt; RandomForestClassifier(class_weight={0:0.1, 1: 0.9}) the metrics remain quite poor. Please suggest.",304319
125206,Should we evaluate our model on the average recall/Precision or on Recall/Precision of positive class as we are more interested in Churners? I request the TAs not to give one line answer but also to explain the answer?,304319
125303,My doubt is regrading the filtering of high value customers. Should high value customers be filtered before data cleaning and EDA and the data cleaning and EDA to be performed only on filtered data ? or Should Data cleaning and EDA be performed on the entire dataset and then be filtered for high value customers ?,313691
124316,1. Is av_rech_amt_data_6 means total amount recharge for the data in that month divided by the no of recharge in that month? 2. is av_rech_amt_data_6 means total amount recharge for the data in that month?,318585
125319,nan,317845
124215,"Need guidance on handling the Date attributes for modeling and also how to manage the blank values for it? Dont think handling date fields was covered during any of the sessions or previous videos, hence need some guidance. is there a standard approach or recommendation for it?",316036
126008,getting the output as this? precision recall f1-score support 1 1.00 1.00 1.00 738 avg / total 1.00 1.00 1.00 738,301114
121405,"As per the given statement in the problem ""X is the 70th percentile of the average recharge amount in the first two months (the good phase)."" Can someone please explain this? Do we have to sum total_rech_amt_6 and total_rech_amt_7 and divide it by 2 to arrive at average recharge amount of the first 2 months?",306248
126010,nan,308638
126105,"After PCA, we are using SVM kernel to try and predict (we are using class_weight={0:0.1, 1: 0.9}). However due to the huge imbalance we are getting either very high sensitivity, almost 1.0 (if we choose scoring = recall) or low sensitivity, almost = 0 (if we use a differenct scoring metric). We ofcourse want a high sensitivity, but not at the cost of having specificity = 0. Considerring the churn rate is arnd 9% it seems the model is either just always predicting 1 or 0, depending on the scoring parameter. How do we change the threshold so that we can overcome this issue? Is there something else we are missing in our approach? For SVM, class weight parameter is having minimal benefit and choosing scoring= recall is giving horrible result. So looks like I will have to change approach to SMOTE and perhaps accuracy scoring.",300694
125004,Are the date related columns significant? Do we need them for analysis?,310974
125020,Which technique are you using for handling class imbalance ? As nothing regarding this was taught in any of the modules. TA please answer,305650
125014,"Need help in imputing null values in multiple column in Highvalue customers dataframe? Do we have to impute with the mean or impute with ""0"" in null values in columns? what is the logic behined it? Any suggestions",306243
125021,"As per case study, we need to do PCA and then use it further for classification model. Can someone help me to understand how can we choose top weighed variable from PCA ? If we draw graph of 1st principle component against 2nd, due to large number of column it's very clumsy..how to proceed?",320103
123329,"what is interpretation of these fields total_recharge_number(total_rech_num), max_recharge_amount(max_rech_amt) and max_recharge_data (max_rech_data) ? Does Average revenue per user for Jun mean average revenue over last x months and total_rech_amt_6 mean actual reachargable amount spend for the specific Jun month? Please clarify",317514
124411,1. How to find the average reacharge done by the customer? Is it avg = (total_rech_amt_6 + total_rech_amt_7).sum()/len(raw_data) If this is correct. What does it meant by 70 percentile of this value? is it 0.70*avg ?,311741
125546,"While building the logistic regression model using RFE, we are getting really low accuracy (about 52%). The other metrics are also bad (sensitivity = 0, specificity = 1). Similar situation occuring with default hyperparameters of random forest (only 47% accuracy). We have used PCA to get the top 25 components and also to make predictions. But since interpreting features using PCA is difficult, RFE has been used. Any solution to the low accuracy?? Is there anything missing that needs to be done??",310505
124425,Isn't total total_rech_amt inclusive of data and voice or not ?,311466
125053,"circle_id and the columns which are not showing variance could be removed , TA please confirm ?",311386
126217,"Are we required to do outlier detection and treatment ? If we do so, we may end losing high end value cutomers. Any suggestions?",311218
124154,"As per problem statement, high value customers are those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months. As per TA&#39;s explanation for some other question: calculate average recharge done by customer in June and July (Topup Recharge + Data Recharge ) and look at the 70th percentile recharge amount Regarding this, I have the following 2 queries: 1. I followed the below process: total_rech_6 = topup_6 + data_6 total_rech_7 = topup_7 + data_7 avg_rec_for_2 _months = (total_rec_6 + total_rec_7)/2 X = 70%ile of avg_rec_for_2 _months Let me know if I am on the right track. Also, as per this process, after the filtering, I am left with 30001 rows instead of 29.9 as per problem statement(101 more). 2. There is a huge percentage(~75%) of null value in some of the columns involved above in the calculations. Do I impute them with 0 and then do the calculations? TAs, please answer.",310511
126050,nan,305847
125127,as per the link https://learn.upgrad.com/v/course/208/question/124702 TA was suggesting to work on the orginal churn but the data set clearly shows that there exists some class imbalance and also the same has been mentioned in the problem statment. Request the TA's to please reverify/confirm on the same?,312259
126052,,305847
125142,Is this considered a fair Practice for modeling purpose? or we should use Class-weight Parameter in Logistic/RandomForest only?,317984
124433,"Explanation given for High value customers not clear. Didnt hear back regarding the followup questions also. So posting the same question again. TA confirmed answer: 1. calculate the total data recharge amount for June and July 2. calculate total recharge amount for June and July 3. calculate average recharge done by customer in June and July 4. Calculate the 70th percentile recharge amount Doubts: 1. In point no 2 above, by total recharge amount, do you mean total call recharge ONLY or total call recharge + total data recharge ? 2. Which are the columns to be considered for call recharge and data recharge? 3. In no 3 above, by average recharge done by customer in June and July, do you mean we have to find 2 averages separately for 2 months, or one average together for both? TAs, appreciate the help in advance.",310511
126055,I have done PCA to identify my principle components. The PCA data is not a dataframe. I have then run a log regression on this PCA dataset. How do i now get the predicted churn value?,312096
125148,nan,302750
126280,"## getting predicted values # Getting the predicted values on the train set y_train_pred = model_pca2.predict(df_train_pca2) y_train_pred[:10] array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])",300687
125724,Pls help to handle the missing date value date_of_last_rech_9 date_of_last_rech_8 date_of_last_rech_7 last_date_of_month_9 date_of_last_rech_6 last_date_of_month_8 last_date_of_month_7,312019
126066,nan,314197
126065,"As mentoned in the problem statement: ""Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase."" Does it mean ( total_ic_mou_9 == 0 | total_og_mou_9 == 0) &amp; ( vol_2g_mb_9 == 0 &amp; vol_3g_mb_9 == 0) leads to churn. ? Or any other condition to follow. Since it is the starting point of all model building confirmation is needed from TA. Is my understanding right ?",317991
124576,"total recharge data and total recharge number, what do they mean? Should we multiple these numbers with their respective amount columns?",316255
124477,in the problem statement it is mentioned that it is crucial to identify the high risk customers in the action phase. So does it mean that we only have to use the 3d month for prediction/modelling purposes and ignore the 1st 2 months completely? OR are we supposed to model based on first 3 months?,310509
124588,"I have performed the steps as per TA instructions. Still get data set as 30019, 220. I have tried removing the columns with 0 values in it and replaced the NaN values with 0 as well. Let me know if I am doing something wrong. Guide me.",314048
124594,"In what units is VBC measured. The dictionary mentions: Volume based cost - when no specific scheme is not purchased and paid as per usage So, I am assuming it in monetary units? If so, shouldn't this also be considered when calculating high value customer?",318438
124631,nan,312448
125196,Can we delete negative values in Revenue columns?,301643
123645,Does the dataset contain only prepaid customers? Or is there a variable/set of variables to distinguish between prepaid and postpaid customers?,305653
125759,nan,308638
126337,is tuning is required to do in telecom churn case study? if not why?and why we normally peforme tunning?,320606
124647,"After performing initial filtering based on recharge attributes, I am getting the count of high value customers as 30k rows. Is this a suitable number to proceed ahead ? PS: It would be really great if any TA puts light on this!",301655
125763,"Since we need to focus on the Churn customers, should we use the value &#39;Recall&#39; instead of &#39;accuracy&#39; for the scoring parameter in GridsearchCV? Or any other value?",304319
125766,"As the condition mentioned might yield 30k based on null value treatment and filtering, I wanted clarification of this aspect. The term &#39;about&#39; is ambiguous without range.",308637
140089,I feel the revolver interest which in the video was not correct. in the video it has been said tif we have not paid the complete statement balance or paid only minimum due or ony minimum due. the interest will be calculated from the date of the transaction. i feel the above one was not correct. the interest would be calcualted from the due date to till the statement balance is paid in full. and this interst will be reflect in the next month statment. and the interest will be reflected in 2 statements as teh date of pay will be ocurring in 2 billing cycles.,320606
140589,1) 400 x [(1+(.18/12))] = Rs 406 2) 6 x [(1+(.18/12))^2 - 1] = Rs 6.18.,310179
140091,"6 x [(1+(.18/12))^2 - 1] = Rs 6.18 i don&#39;t understand this my understandis 6*(0.18/12) = 0.09 total suplus amount is = 6.09 please make me understand, thank you",320606
140133,nan,317996
140153,nan,325779
142216,nan,304692
139482,nan,314511
138495,Upgrad has added Module1 for Introduction to e-commerce. Its of couple of hours. Is this is the only one for this week until 15th july ? Or some other modules will be added. Or i am missing something. pls clarify,312019
138416,nan,308962
138585,Is the introduction to e-commerce module just 1-hour module? Please confirm it or some other sessions or modules will be added? Do we have to start with the capstone project after this?,311466
139110,Got a mail that Module 2 of Domain elective to be completed by 22nd July. But it is not visible yet. Am I the only one facing this issue? Elective is E-commerce.,311686
140856,,316349
119208,nan,313826
118738,"In order to find out the classification boundary for deciding which kernel to use, can we perform PCA on the feature-set and use the first two components?",317984
120557,nan,310629
119544,nan,300988
120010,"I understand the solution works. But how does it arrive at the solution? Does it check for A and not A then B and not B till it reaches Z? worst case scenario, for each letter the model will run 24 times to arrive at a decision. Is this understanding true? if so, is it even a correct choice to use SVM for multi-class problem?",305839
119290,"best value from using GridsearchCv is C=1000 and gamma=0.01 ? but solution says &quot;It is changed from 92% to 94% at the value of hyperparameters i.e. c= 2 &amp; Sigma = &#39;0.05&#39;&quot; . Can someone clarify the question , please ?",316211
119600,"# creating a KFold object with 5 splits folds = KFold(n_splits = 5, shuffle = True, random_state = 101) # specify range of hyperparameters # Set the parameters by cross-validation hyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]}] # specify model model = SVC(kernel=""rbf"") # set up GridSearchCV() model_cv = GridSearchCV(estimator = model, param_grid = hyper_params, scoring= 'accuracy', cv = folds, verbose = 1, return_train_score=True) # fit the model model_cv.fit(X_train, y_train)",306736
119326,Getting the below plot when i fit the model and Run CV results plot. what could i be doing wrong?,316036
119333,In a dataset where there are multiple features how can i ascertain if the relationship is a linear or a non relation to decide on the conversion,310629
119305,Can somple give a simplified view on the difference between Tuning parameter C (Cost function) and Gamma ?,316036
119057,"I am bit confused of cv arguments passing sometimes cv=folds with forumulat sometines c=n_fold (n_fold 5) What for these and how to decide folds = KFold(n_splits = 5, shuffle = True, random_state = 4) model_cv = GridSearchCV(estimator = model, param_grid = params, scoring= 'accuracy', cv = folds, verbose = 1, return_train_score=True) n_folds, =1 model_cv = GridSearchCV(estimator = model, param_grid = params, scoring= 'accuracy', cv = n_folds, verbose = 1, return_train_score=True)",312019
119292,nan,316211
119323,nan,308673
119052,"I am getting the below error while running the code ValueError: &#39;c&#39; argument has 2 elements, which is not acceptable for use with &#39;x&#39; with size 200, &#39;y&#39; with size 200. Please help",307494
118662,How can one decide if dataset has linear classification or non-linear classification while deciding SVM classification method - SVC or Kernels?,318458
120276,nan,305656
120682,nan,301644
119643,nan,320687
119360,nan,308495
119361,nan,308495
118910,nan,308673
118551,"question 3/3 does not sync with the corresponding diagram. In the diagram class 1 is red and class2 is blue. which means equation-wise class 1 will be &lt;0 and class2 will be &gt;0 Now per the question, Y label is 1 and -1 for classes 1 and 2 respectively. This implies (W1X1 -X2 +W0)*Y will be negative for any point not on the line.",311857
118852,"Let&#39;s say that a straight line, L, as shown in the plot below, is given by x2=w1x1+w0, and divides the points belonging to two classes, C1 and C2, in a 2D space. If the points (a,b) and (p,q) belong to C1 and C2 respectively, then How is (b&minus;w1a&minus;w0)&lowast;(q&minus;w1p&minus;w0)&lt;0 true?",320687
119022,What&#39;s the formula to calculate the margin of closest point to hyperplane?,315423
118808,Why should the summation of coefficients square be equal to one?,306726
121042,,318319
118663,nan,300733
119096,"Hi, Given 10 points in a 2-dimensional space, how can we identify the coefficients of the separating line manually? Can anyone provide step-by-step calculations?",318438
118717,"Can anyone elaborate further and explain the below answer: Questions:1/2 Hyperplane Let&#39;s say that a straight line, L, as shown in the plot below, is given by x2=w1x1+w0, and divides the points belonging to two classes, C1 and C2, in a 2D space. If the points (a,b) and (p,q) belong to C1 and C2 respectively, then: (b&minus;w1a&minus;w0)&lowast;(q&minus;w1p&minus;w0)&gt;0 (b&minus;w1a&minus;w0)&lowast;(q&minus;w1p&minus;w0)&lt;0 Feedback : Since the two points belong to different classes, the terms (b&minus;w1a&minus;w0) and (q&minus;w1p&minus;w0)will have different signs for instance, if you observe the red points which lies below the line(hyperplane), give the negative value of expression x2&minus;w1x1&minus;w0. On the other hand, blue points which lies above the line(hyperplane), give the positive value of same expression x2&minus;w1x1&minus;w0. Thus, if you multiply both the expression&#39;s value, you will get a negative value. Correct!",300687
119239,Can someone please give an example of how the summation for squared coefficients becomes 1,320690
118897,"In the practice questions - question no. 3 - The Y labels for classes 1 and 2 are 1 and -1 respectively. If the optimal separating hyperplane is represented byX2=W1&lowast;X1+W0, then the classification rule to classify a general point (p, q) is given by: has given the feedback as - if any point lies below the line, the value of Y will be -1 and the value of expression is less than zero and the product is positive. But the product should have been negative. Is the feedback provided for question is an error or the feedback is right? The equation of the hyperplane is given as W1X1&minus;X2+W0=0 . If any new point lies below the line, the value of Y will be -1, and the value of the expression W1X1&minus;X2+W0 is less than zero; this means the product will be positive . Similarly, if the point lies above the line, the value of Y will be +1, and the expression W1X1 - X2 + W0 is greater than zero. Thus, the product is positive . Also, in the case of the point on the line, the value of the expression W1X1&minus;X2+W0 is equal to zero. Thus, the expression  ( W1 . p&minus;q+W0) . Y gives a positive value or zero.",316399
120879,"I am running the SVM code on kaggle, with k fold =3 for the hyperparameter tuning, still the execution is not cmpleting. I restarted the session few times still no use. Any best practices?",301644
118893,nan,311952
138141,nan,311466
119495,nan,318335
119077,nan,308673
119383,It is mentioned that slack variable takes value from 0 to infinity. Where does it attain the value infinity (in graph),316147
118978,How we come to know the first row corresponds to Ham and the second row corresponds to Spam,318372
118278,"&#39;if you increase the value of epsilon( &epsilon; ) from 0 to 1 the right side i.e M(1&minus;ei) of the equation liX(Wi.Yi)&gt;M(1&minus;ei) will be always positive or equal to zero. It means that the data point is correctly classified by the given hyperplane. But if the value of epsilon( &epsilon; ) is greater than 1, that means M(1&minus;ei) has a negative value, in which case the data point falls on the wrong side of the hyperplane.&#39; Hw does this concludes E&gt;1 ia valid for B as there are no misClassificatications in B according to Graph.",317984
119492,Can the solution of the diabetes coding problem be shared as well?,310509
118280,nan,317984
118855,nan,311404
118857,"Not sure how outliers can effecet SVC, as SVC depend of closest point to hyperplane. Kindly explain.",311404
119494,"While it is mentioned that SVM is relatively immune to outliers, however it can impact the classifier as was demonstrated in the video on cost. Hence, should outlier treatment be done for SVM models?",310509
118719,The image shows an error in the code of the SVM implementation. Any change in the syntax required?,310505
119176,nan,310508
119470,nan,313526
119069,All column values except Diabetes and id are float,311857
118871,"Coding question writing the predictions into csv file When we have below line exection in the live d = pd.DataFrame({'id': diabetes_test['id'], 'Diabetes_Predicted': predictions}) d.head() It writes the data first column as prediction and second as id ,but same thing we i execute in jupyter note book, it writes proper first column has id and second has predictions. confused how the live execution working differently. Any one please clarify this",312019
119606,"As we saw in the session of soft margin classifier, we choose the value of C to be low, moderate or high based upon the following: Keeping the value high would lead to high bias &amp; low variance Keeping it low would lead to low bias &amp; high variance. Thus, we should go for a moderate value. So, can we say that C is a hyperparameter which we&#39;re using to regularize our model? Happy Coding!",318355
119119,nan,310629
141333,Getting Following Error while calculating RMSE in the example shared.,315423
140328,"U1 = (5, 0, 3, 0, 2, 0, 0, 2, 0, 0) U2 = (3, 0, 2, 0, 1, 1, 0, 1, 0, 1) The cosine similarity = cos(U1, U2) = (U1 &sdot; U2) / ||U1|| ||U2||, In this problem how ||U1|| is 42^0.5? Couldn&#39;t understand . It should be sqrt(42) I believe.",311404
140341,nan,311466
140435,"As per screen shot, when I run print(scaler.fit(X)) , the code throws error.",318770
139491,regarding dataset ratings.csv from movielense many files are there. which zip file we have to take when i took zip file of 190 mb. it has ratings.csv file of size 520 mb while doing pivot it gives error says ValueError: negative dimensions are not allowed Let me know which file to use,312019
140377,I am using ratings_small.csv When pivoting my index values are getting reset to integers instead of UserID&#39;s. I have tried other portals but could not find the reason.,317984
141435,nan,311254
140451,,318335
140138,"We are calculating mean as per axis =1 and ignoring NaN. but the resultant array has a length equal to the number of rows i.e row-wise calculation of the mean I think. Also, why are we subtracting Correlation Matrix from 1",317984
140380,i am using ratings small.csv while pivoting my index values are getting resetted to integers instead of UserID. I have tried other portals but could not find the reason.,317984
141027,"In the recommender system example, it is shown that both train and test data sets have same set of userIds. How is this possible after doing train_test_split on the original data set with 70:30 ratio?",310974
140332,"In use cases such as movie recommendation systems, how are null values treated? As seen in the example, the users might not have seen the movie. Especially, if the system is on a very new website, this matrix will be sparse. Hence, I want to ask two questions in this regard: 1. How is the system built when the website/service is relatively new? Is there a preference for differnet recommendation systems based on maturity of the platform. 2. How are null values treated? should we impute with predictions? (Edit: I've seen the issues session after posting this question. However, I really want to know how these challenges are dealt with in detail.)",308637
141453,the link provided is this - https://grouplens.org/datasets/movielens/ I can't find the one used in the jupyter notebook,314536
140249,nan,300733
141180,Facing error with the pivot table with the big dataset. IndexError: index 1206342321 is out of bounds for axis 0 with size 1206323624,308962
140725,"How can we subtract two matrices with unequal dimensions here ? The mean along axis gives a column vector with (610,) rows and we have movies_features transposed to get (8536,610) matrix. How is the subtraction happening ?",315028
140704,I&#39;m facing the following error in the Python code of pivot table for recommendation systems. Any solution to this out of bounds exception error?,310505
140970,nan,311466
141654,nan,316432
140058,"in the Introduction for recommendation systems, it says we would implement one in R. i assume its a Typo and it should ideally be Python?",316036
141449,In the section on ' Recommendation System in Python - I' the second video shows how dummy test and dummy train works. According to what is being taught 'dummy_test' is the opposite of 'dummy_train' in terms of 0's and 1's. But in the lecture it is not reflecting the same. What does that imply? I am confused as to why it is not reflecting the same over there.,314536
141975,I understand that it is important to normalize the ratings given by each user because each user may have different tendencies but what is the point of normalizing ratings of movies against their mean??,319357
109235,"In this question. they say specifically say don't need to clean the data and do scaling. But when i do scaling, it fails as there are string characters in numeric variables like * in score denoting not out. How do i perform scale transform without cleaning the data ? Please advise.",310501
109327,nan,310629
109368,nan,310611
109237,https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a,301114
110006,nan,317418
109145,nan,310179
109420,nan,318802
109152,Why are we calculating Z here ?,310179
109449,nan,305804
108878,nan,305847
108490,"Suppose, I have mentioned K = 2 and no data points are assigned to cluster# 2, What should I do here ? Do I keep iterate the initial points again to make 2 clusters ? or shall I leave the second cluster and consider only 1 cluster ? In my opinion, we should leave the second cluster.",312479
109512,"In the first session of the Clustering module, Ujjyaini tells us about intra-segment homogeneity and inter-segment heterogeneity. Here, she explained that, ideally, the data points within a cluster should be tightly bound which is called as intra-segment homogeneity. The data points of different clusters should show very different properties to each other which is called as inter-segment heterogeneity. Now, in the second module, we have an open text question for which we are told to go through some additional reading. Here, I came across the following: Here, it is being said that the within-cluster distance should be maximum and between-cluster distance should be minimum. Which sort of contradicts what we studied earlier as I have mentioned above. In my opinion the earlier understanding makes sense. Attaching the link here: Link Please help me understand if my understanding is incorrect and these are two different things and what I am looking at is not related with each other. If not, then which one is correct.",315471
108587,"Although professor explained what Xi, k, &mu; i are, I am confused what they actually denote. Could anyone explain me in short what these terms are?",312756
109586,Is the cluster center picked from available data points.. or it can be any poin near other data points?,301641
109596,nan,320687
108641,nan,318846
108817,How to proceed with the initial step and also what is this Eucleadean center ? Wil it be the new center for the new cluster. Th way this topic is explain seems to be very confusing.,310179
108822,"While explaing the process K-map algorithm , prof. expalined the sigma for Xi points to (MU)i points, why do prof does not take in consideration Y points?",318427
108973,"Do we need to copy the data column C and D to column S and T , i am not able to understand , how to calculate the values in the excel sheet ?",318814
108669,"For k-means clustering, is there a limit for Number of iterations?",310508
109144,When should we stop clustering the data?,310179
108825,Attached is a screenshot - and also the formula for calculating distance.,312376
109124,If the link for SVM (Support Vector Machine is not working) Kindly use below link : http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.117.3731 And use this link to download it as pdf: http://luthuli.cs.uiuc.edu/~daf/courses/Optimization/Papers/SVM.pdf,318372
109600,"For k-mean clustering, say we are trying to create two clusters. (say there are two clusters as explained by Professor) During first &#39;assignment step&#39; in first iteration, if every point comes into first cluster and no points comes into second cluster, should we choose a different set of points or will optimization step be able to handle it?",320074
108837,Why is cost function is chosen to be sum of squared errors ? Is this the best cost-function that can be used for k-means alogirthm?,306736
109386,nan,303228
108431,"Both of these techniques sound similar, what are the differences between the two?",318078
109611,"I wanted to install kmodes. pip install kmodes via jypyter terminal when i did jupyter note book-&gt;New-&gt;Termnal is post as [CLOSED] message in terminal, i am not able enter above command to install. how to solve this. or any otherway i can install other libraries through anaconda .",312019
108808,"In the first step of Clustering Activity File - How does the center point X and Y have been chosen. If it is chosen randomly (as per the assignment step ) there are two sets of center points chosen. One set is 10.0 and 8.0 and the second set of 18.6 and 6.0. Also, those center data points are not of part of any of &#39;n&#39; data points listed. Can we take any two sets of data points randomly for the initial and subsequent assignments?",311115
108839,nan,310629
115642,nan,301646
115643,nan,301646
115645,nan,318019
109096,"Ok as the Professor described after choosing the 2nd cluster center we again compute the distance of each data point from the cluster centers, so is it that we compute distances of each data point with respect to both the data centers chosen thus far? Because the algorithm hasnt reached the assignment step as if now? Please help.",300734
108925,nan,308673
109687,nan,314629
109009,nan,314313
109147,The concept is explained in the below link but yet not clear.. Did anyone got the clear understanding of it? https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/,310179
110015,Also when to use it?,318448
108452,Not clear as to how to Input the distances of each point from both the centres in Dist_C1 and Dist_C2 columns. Are we suppose to mention the co-ordinates or the distance from the center ?,306011
113975,(i) is there a way where the K-mean can remain constant for each iteration (ii) With K mean changing with each iteration there could be different clusters formed each time. How do we assess which cluster combinition is better option.,310629
108650,,308635
110080,Elaborate this more please,311466
109153,nan,310179
111989,I created Data Frame having 5 PCA and one feature column. How to perform clustering on this. Do i need to do a transpose on this,300735
114777,"If we have given n fatures, is there a way to calculate total number of possible clusters?",301649
115647,nan,301646
108570,how can one explain the difference between EDA and Clustering? in both cases we are trying to look at data to find patterns...how are they diffterent?,310509
108369,nan,313228
108561,nan,308673
108580,Can please someone explain the explain problem in first session and steps on how to solve it?,301649
108903,nan,311466
108911,http://www.improvedoutcomes.com/docs/WebSiteDocs/Clustering/Clustering_Parameters/Distance_Metrics_Overview.htm,317982
113271,What is the difference in the implementation of the two? When do we use normalization and when to prefer other techniques? It would be great if someone can demonstrate with code examples.,317987
109378,nan,314678
110086,nan,318440
109474,nan,304697
109542,"&quot;You will be able to see the number of natural clusters from the dendrogram itself. If you want, you can change the scale as well. Which group of parameters give you the best result &quot; Can someone tell How do we compare the result? Question is unclear to me",315028
109388,"How can we create Dendogram if excel file is given? Is there any python code? if not, how to create manually?",320103
109148,The first video gets stuck at 2 mins 52 secs and doesnt proceed further. I request the technical team to please get this fixed as soon as possible.,318084
109077,Could be useful.. https://stackabuse.com/hierarchical-clustering-with-python-and-scikit-learn/ https://www.mathworks.com/help/stats/hierarchical-clustering.html,310508
109603,"Hi, Can anyone help in getting a clear plot in python or any alternative for plotting better dendrogram, Since with too many points it becomes difficult to get the points in the same cluster?",315028
108853,in excel sheet &quot;Clustering_activity&quot; for Agglomerative clustering in types of linkages session. Was that the Euclidean distacne which was calculated? could someone please shed some light on this?,312756
109666,nan,308673
109732,"when the K Means or Hierarchical clustering is run, then cluster IDs are generated numerically. However, what is the best way to identify what do these cluster IDs actually mean? for e.g. in the cricket example - with k =4, how to know what these 4 segment s actually represent? from the question asked it appears different levels of average and score are used for representation...however, how to know which is which?",310509
109729,"The below code is used where PD.series from 0-4 is concatanated to the difference RFM mean calculations. df = pd.concat([pd.Series([0,1,2,3,4]), km_clusters_amount, km_clusters_frequency, km_clusters_recency], axis=1) My question is - why does PD series required to be mentioned separately? shoulddn&#39;t Cluster ID, which is already calculated before, be used for the same? if the PD series is used to identify the relevant cluster id, how does the matching work?",310509
109668,"All the clustering topic is running around the distance between data points (x,y). But we may have more than 2 variables. How are these plotted on a 2-d graph to start with during the K-Mean or Hierarchial clustering?",318007
109702,nan,304693
109211,Do we have the Python Code book shared for this? i wasnt able to find one during the module...,316036
109216,nan,310419
112179,nan,303085
115388,"Understood the logic. But whats happens during the first linkage, since we begin with n different clusters in agglomerative clustering, to form the first cluster, do we take the maximum distance, that defeats the purpose isn&#39;t it",304022
109349,nan,310179
108830,"Why didn't we standardize the columns i.e Monetary , Frequency and Recency columns? Am i missing something?",311686
112264,"HI, I have country name and cluster_id as two columns. Could any one plz let me know which plot should I use to get similar one as attached .",317410
112435,"RFM.index = pd.RangeIndex(len(RFM.index)) If someone has not used this line while practising the code, certain Nan values are getting inserted while creating cluster visulaization. Someone can through some light how exactly this line of code ois preventing the creation of Nan rows.",318372
109324,"# Standardise all parameters RFM_normal = RFM.drop(&quot;CustomerID&quot;,axis=1)",310179
109322,# Outlier treatment for Amount plt.boxplot(RFM.Amount) Q1 = RFM.Amount.quantile(0.25) Q3 = RFM.Amount.Quantile(0.75) IQR = Q3-Q1 RFM = RFM[( RFM.Amount &gt;= Q1 - 1.5*IQR) &amp; (RFM.Amount&lt;= Q3 + 1.5 *IQR)] Not able to understand this line as why its multiplied by 1.5?,310179
109328,"classmodel_clus = KMeans(n_clusters = 3 , init = &#39;random&#39; , n_init = 10 , max_iter = 50 ) . fit(RFM_norm1)",310179
108872,"One way to do it is by eliminating all the data points which fall outside the 1.5 times the IQR of the 1st and the 3rd quartile. In the above statement what does IQR stand for, can anyone please explain in detail that what is IQR and why it is chosen as 1.5 times the IQR ?",301655
109696,I was not able to find the sample python code to be used to build the kmeans cluster and find the players that are part of each cluster. The python code provided only builds the cluster but not identify which customer is in which cluster. Can you please provide pointers on this.,312491
109351,nan,310179
109659,nan,310179
109329,"n_init: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia (explained below). classmodel_clus = KMeans(n_clusters = 3 , init = &#39;random&#39; , n_init = 10 , max_iter = 50 ) . fit(RFM_norm1)",310179
108395,nan,311254
109223,I am getting following erre while running the notebook given in K-Prototype in Python. This is the version I have,310508
111686,"After clustering, we plot a bar chart reflecting clusters. But i want to know what points are there in each cluster",306734
111440,nan,318780
108806,"1. We lost 15% data in outlier treatment. What is treated as acceptable? 2. Even after the outlier treatment, we find lot of outliers, when we boxplot it. If we try to do it recursively, we might endup with very minimal data. 3. At times have observed that when we treat mutiple variables, the original variables start showing outliers with the reduced set. 4. There might be some products/brand items which might be priced high and might not be outliers. Any guidelines on how we should be handling these?",317514
111857,nan,303085
115399,nan,318082
109455,Getting the following error even after installing kmodes pip install kmodes pip install --upgrade kmodes Error: ModuleNotFoundError: No module named &#39;kmodes&#39; Any suggestions?,317514
114455,"Silhouette score is used to determine optimal k values in K-means. But in K-modes or K-Prototype only elbow curve is used. Can we use Silhouette score as well in these cases? My understanding is it can be applied because conceptually the score signifies the similarity of the members. But I am not very sure of the mathematical feasibility of the same when K-mode/K-Prototype are used, where categorical values are considered.",305839
110727,I am unable to understand the calculated values. Step by step calculation is required for understanding.,311117
114860,"in the demo, as well as the assignments, we have used k means algorithm. In doing so, we had to create dummy variables for all categorical variables, which had lead to the increase in variables in great numbers. Using k prototype will reduce the inconvenience of dummy variable creation and feature space expansion. is there any particular reason for choosing k means over prototype?",305839
112456,nan,305847
115318,"When we used label encoder for categorical variables on EDA we required to know what each label represents, How can we get the mapping or how can we decode",318344
131744,nan,314818
131645,nan,318005
131705,"Hi all, I have entered this query: select min(fare_amount) as min_fare_amount, max(fare_amount) as max_fare_amount, avg(fare_amount) as average_fare_amount from NYC_table; I am getting different output for eg: min(fare_amount)=-10.0 but max(fare_ampount)=fare amount but not number What is wrong in query",308638
131754,nan,303228
131451,The instructions say a single .txt file for submission. Is there a way to download the hive notebook to a .txt file? Copy pasting the commands on a .txt makes it all clumsy. Is there a better way?,312096
131016,"Unexpected character (&#39;V&#39; (code 86)): expected a valid value (number, String, array, object, &#39;true&#39;, &#39;false&#39; or &#39;null&#39;)\n at [Source: java.io.ByteArrayInputStream@6d7fd1d9; line: 1, column: 2]:25:24&quot;,",304812
131436,"Permission denied: user=vivek.kamble1980_gmail, access=WRITE, inode=&quot;/common_folder&quot;:sanskar.agrawal_upgrad:trainers:drwxrwxr-x at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:279) at In which folder we have the access to write the data into file ?",304812
131739,all columns related to fare/price/tax have negative values...is this erroneous?,308437
131316,"After Creating the master table for TLC Assignment, table header shows as table_name.column_name and I get 1174570 instead of 1174569. Please advice. Please find the attached screenshot:",310210
130572,"Getting error while inserting rows in partitioned table partitioned (mnth,day) Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask However insert works perfectly fne on my local. I think it is read/write/load issue on platform while inserting heavy loads. Any idea?",300718
130676,"Hi, Is there any way to perform delete operation in the hive, I tried some online links but not able to do it successfully.",315423
130999,I find that sequence in data dictionary doesn't match the sequence in data file given for BIG Data Assignment. In case any of you faced it did you create your HIVE DDL as per corrected sequence?,306250
131399,"Which is effecting work. Is anything wrong with corestack lab? as it is weekend and so many users might have logged in into lab. Is anyone facing the same issue? I tried basic troubleshooting for 504 gateway error, however still facing the same error.",312756
131727,"delete from demo_p where (from_unixtime(unix_timestamp(tpep_pickup_datetime,&quot;yyyy-MM-dd HH:mm&quot;),&quot;yyyy&quot;)=2003); I am getting no error but rows are still not getting deleted. demo_p is already partioned, bucketed and orc used so now getting no ACID error.",312033
131638,nan,304812
131691,nan,303228
131029,Plz help me in resolving this error.Thanks,319759
131030,nan,312096
131039,nan,311868
131040,nan,311868
131644,Below is the screenshot of the error I&#39;m getting even while adding the Jar to the Hue platform. Does anyone know how to resolve this?,318355
131593,"Hi, I am getting below while performing percentile, am not sure why it is a syntax error. Could you please help out. Thanks in advance",317410
131740,column ratecodeid also has 99 as one of its values - this is not expected right?,308437
131068,"After deleting all the erroneous rows, count is getting reduced by around 50%. Is this as expected?",320103
131817,Do we have to create a new table with all the error rows or do we need to create a new column ??,302750
131387,"Am confused with this question. if we apply group by , we window in on &quot;passenger_count&quot; column, the value is NaN. If we just run Corr on passenger_count and tip amount, we get a single positive value. how are we supposed to interpret the value? is it that we have to run this query excluding solo passenger ; and run the query including solo passengers and then difference of it tells us whether + or - (if - then solo passengers don&#39;t tip more , and if + solo passengers tip less ... or is there a simpler interpretation ?",309211
130787,"Please refer to the snapshot above. Why is this happening? After creating the ORC partitioned table, I'm unable to delete any of the erroneous rows. What exactly is the mistake here? Please help on this. I've seen some other discussions on the same, but I'm unable to understand them. The table is in ORC format and is partitioned, but I'm still unable to delete the erroneous rows.",310505
131722,How to solve this error I have bucketed the table now trying to convert it into orc while insert query I am getting this error?,318451
131293,Do we need to create new columns for analysis 2 Q2 (segregation of tips) and analysis 2 Q3 (speed) ?,313691
131103,Do we need to import the data from the excel into one of the table for this assignment?,302741
131425,"For example if vendor 1 total count ==X vendor 2 total count ==Y vendor 1 count for erroneous rows==P vendor 2 count for erroneous rows==Q Then , whether we have to compare P/X with Q/Y",318005
131116,Error while executing a delete statment any suggestions,302741
131701,nan,314612
131723,nan,303228
130907,"When I create the table and do a query DOLocationId is null. -- CREATE EXTERNAL TABLE create external table if not exists yellow_taxi_trips(VendorID int, tpep_pickup_datetime Timestamp, tpep_dropoff_datetime Timestamp, Passenger_count int, Trip_distance double, PULocationID INT, DOLocationID INT, RateCodeID int, Store_and_fwd_flag varchar(1), Payment_type int, Fare_amount double, Extra double, MTA_tax double, Improvement_surcharge double, Tip_amount double, Tolls_amount double, Total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LOCATION &#39;/common_folder/nyc_taxi_data/&#39; tblproperties (&quot;skip.header.line.count&quot;=&quot;1&quot;);",311857
130501,"I m running below query , my table is getting created but with no Data . Can anyone tell me what I m misisng here ? create external table if not exists NYC_A(VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp,passenger_count int,trip_distance double,RatecodeID int, store_and_fwd_flag char(01),PULocationID int,DOLocationID int,payment_type int,fare_amount double, extra double,mta_tax double,tip_amount double,tolls_amount double,improvement_surcharge double, total_amount double) ROW FORMAT SERDE &#39;org.apache.hive.hcatalog.data.JsonSerDe&#39; with serdeproperties (&#39;paths&#39; = &#39;&#39;) location &#39;/common_folder/nyc_taxi_data/&#39;;",311861
130663,"In the question &quot;What is the correlation between the number of passengers on any given trip, and the tip paid per trip? Do multiple travellers tip more compared to solo travellers? &quot; what does &quot;Solo travellers&quot; mean? This is the issue: 1) If I choose just one category of travellers (say 1 ) then I get NaN as there is no variance 2) If I choose two categories ( say 1 and 2 passengers) , then I get a Pearson Correlation value; likewise, if multiple categories are chosen, there is no issue in deriving correlation coefficients. What does Solo travellers mean? Is it just &quot;1&quot; or &quot;1 and 2&quot; ?",309211
130505,"The assignment asks to create a clean, ORC partitioned table. On which columns do we need to partition?",314730
131885,"Trying a case statement in HIVE. it works fine on the non partitioned table (the initial table created from the data without partitioning and clustering) But when the same statement is run on the orc partitioned it throws the following error : Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask Anyone else face this? All help is greatly appreciated.",317149
130583,"Would negative amount values in columns such as fare amount, total amount etc. be considered data quality issues or erroneous data? It&#39;s possible that the customer didn&#39;t have to pay anything if they had offers or coupons on the ride. So do negative amount values have to be mentioned as erroneous data??",310505
131394,&lt;html&gt; &lt;head&gt;&lt;title&gt;504 Gateway Time-out&lt;/title&gt;&lt;/head&gt; &lt;body bgcolor=&quot;white&quot;&gt; &lt;center&gt;&lt;h1&gt;504 Gateway Time-out&lt;/h1&gt;&lt;/center&gt; &lt;/body&gt; &lt;/html&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt; &lt;!-- a padding to disable MSIE and Chrome friendly error page --&gt;,318455
131150,"create external table if not exists table_clustered(column inputs) partitioned by (yr int, mnth int) clustered by (column1) into 5 buckets location &#39;/user/hive/warehouse/location&#39;; SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000; -- Then insert the data in the table insert overwrite table table_clustered partition(yr, mnth) select column names,......, year(some date) as yr, month(somedates) as mnth from table_partition; SIMILAR way ORC is also created as decribed in the module.",318780
130851,I created the parttitioned table from the original one on condition that Extra takes either 1 or 0.50 as per data dictionary. Now this question is confusing as to I should run the analysis on the original table or the partitioned one.,310974
131161,"Error encountered when executed the below code: ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; STORED AS TEXTFILE LOCATION &#39;/common_folder/nyc_taxi_data/&#39; tblproperties (&quot;skip.header.line.count&quot;=&quot;2&quot;); How do i resoove this ? Thanks",319759
131305,nan,307710
131308,nan,305335
130885,Analysis-I Compare the overall average fare per trip for November and December. We shoudl use Fare_amount or total_amount columns ? Which column we should use here ?,312019
130888,"When I create the table and do a query DOLocationId is null. -- CREATE EXTERNAL TABLE create external table if not exists yellow_taxi_trips(VendorID int, tpep_pickup_datetime Timestamp, tpep_dropoff_datetime Timestamp, Passenger_count int, Trip_distance double, PULocationID INT, DOLocationID INT, RateCodeID int, Store_and_fwd_flag varchar(1), Payment_type int, Fare_amount double, Extra double, MTA_tax double, Improvement_surcharge double, Tip_amount double, Tolls_amount double, Total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; LOCATION &#39;/common_folder/nyc_taxi_data/&#39; tblproperties (&quot;skip.header.line.count&quot;=&quot;1&quot;);",311857
131317,,307492
130655,My count is not matching with the number of rows present in the excel file. Can anyone suggest on the count of records present.,307493
131199,Select * from table_name is running fine. But select count(*) is giving the error as above.,318585
130665,"While data cleaning step, I have some rows which are invalid and can not be used for analysis, So should we delete such rows?",315423
130688,"The problem statement of the assignment states that the data is provided for the months of november and december but in the csv file located at /common_folder/nyc_taxi_data/, the tpep_pickup_datetime and tpep_dropoff_datetime are for the months of october and november. I request the TA to please confirm if the months are october and november.",314730
130853,"Hi, I am trying to inset rows in my partitioned table from table created earlier. But it is throwing me &#39;Authorization error&#39;, am I mIssing some thing. Please help in doing this.",317410
130861,"Compare the average tip with the 25th, 50th and 75th percentiles and comment whether the &lsquo;average tip&rsquo; is a representative statistic (of the central tendency) of &lsquo;tip amount paid&rsquo;. ? What we have to do here exactly?",315423
130653,"As mentioned in the Assignment details: While creating the tables, it is mandatory to define the integers as int and floating points as double . Certain results may be affected if this is not followed and in that case, marks will not be awarded. There are some columns which have values in very low range &amp; can be perfectly covered by TINYINT or SMALLINT. Is it really mandatory to mark all the integer values as int? Can't we keep them as TINYINT or SMALLINT? I request the TA to answer this.",318355
130382,"I have created Table for taxi data provided, the specified date data type for tpep_pickup_datetime and tpep_dropoff_datetime Columns, but it&#39;s showing null in my table? Do I have to change the data type for a date ?",315423
130709,nan,308774
131504,"Cpommand1 : select distinct(PULocationID) from table_yellow_tripdata_2017 order by PULocationID ; Command 2 : select max(PULocationID) as max_PUL , min(PULocationID) as min_PUL from table_yellow_tripdata_2017 ;",318770
131681,The final rate code in effect at the end of the trip. 1= Standard rate 2=JFK 3=Newark 4=Nassau or Westchester 5=Negotiated fare 6=Group ride,300721
130679,"Is there a difference between ""=="" and ""=""? As it appears from what I tested out, both mean the same for a ""variable-to-variable"" or ""variable-to-value"" comparison. ""variable-to-value"" comparison."" : For example ""where A == 2 "" and ""where A = 2"" fetches me same answers that I was looking for (meaning it fetches the exact same number of rows ) ""variable-to-variable"" comparison."" For example ""where A == B "" and ""where A = B"" fetches me same answers that I was looking for (meaning it fetches the exact same number of rows when comparing one column to another in the same table",309211
131719,"Fare amount equals 0 or less than 0 ,should we drop these rows ?",314197
131752,"what is the procedure to check all queries at a go before submitting it in a txt file,",301115
130929,nan,314678
130932,nan,301890
131544,"this query is giving date but not time. insert into newyork_taxi_partitioned partition(year, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, from_unixtime(unix_timestamp(tpep_pickup_datetime),'yyyy-MM-dd') Pickup_date, from_unixtime(unix_timestamp(tpep_pickup_datetime),'HH:mm')as Pickup_Time, from_unixtime(unix_timestamp(tpep_dropoff_datetime),'yyyy-MM-dd') dropoff_date, from_unixtime(unix_timestamp(tpep_dropoff_datetime),'HH:mm')as dropoff_Time, year(tpep_pickup_datetime) as year, month(tpep_pickup_datetime) as mnth from newyork_taxi;",318802
131716,nan,303228
131874,query snapshot: exception snapshot Any help will be highly appreciated.,318362
131307,"execute the below statments ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000; create external table if not exists nyc_taxi_orc_suri (VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp,passenger_count int,trip_distance double, RatecodeID int, store_and_fwd_flag string, PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;,&#39; STORED AS TEXTFILE location &#39;/common_folder/nyc_taxi_data/&#39;; create external table if not exists nyc_taxi_part_orc_suri (VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp,passenger_count int,trip_distance double, RatecodeID int, store_and_fwd_flag string, PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) partitioned by (yr int, mnth int) stored as orc location &#39;/user/hive/warehouse/your_partition_folder_name_orc&#39; tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;); insert overwrite table nyc_taxi_part_orc_suri partition(yr, mnth)insert into nyc_taxi_part_orc_suri partition(yr, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, year(tpep_pickup_datetime) as yr, month(tpep_pickup_datetime) as mnth from nyc_taxi_orc_suri;",302741
131746,"&quot; Before answering the below questions, you need to create a clean, ORC partitioned table for analysis. Remove all the erroneous rows. &quot;",300721
131411,"create external table if not exists newyork_taxi(VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count int,trip_distance double, RatecodeID int, store_and_fwd_flag char(01), PULocationID int,DOLocationID int, payment_type int,fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE location '/common_folder/nyc_taxi_data/';",318802
131819,After import the first line in table is the header followed by actual data... How to remove header line from table data,308437
131312,"My insert is sucessfully getting executed when executed as below insert overwrite table nyc_taxi_part_orc_suri partition(yr, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, year(tpep_pickup_datetime) as yr, month(tpep_pickup_datetime) as mnth from nyc_taxi_orc_suri limit 10; however when trying to exeucte the below it is erroring out insert overwrite table nyc_taxi_part_orc_suri partition(yr, mnth) select VendorID, tpep_pickup_datetime, tpep_dropoff_datetime, passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, year(tpep_pickup_datetime) as yr, month(tpep_pickup_datetime) as mnth from nyc_taxi_orc_suri; please help",302741
131443,"I just successfully created the external table for the assignment and wanted to check the table's count. So I ran the ""select count(*) from table"" statement and I get an error 2 statement. Any idea what might be wrong? I also ran a simple ""select * from table"" statement and got a much longer error statement. This is before I can insert the partitioned data into the table. Do let me know if you have any pointers.",316416
130735,"In order to clean data, do we need to delete unwanted rows? If yes what is the command. Delete command throws error in Hive - Attempt to do update or delete on table that does not use an AcidOutputFormat or is not bucketed Please suggest",311868
130991,"Bad status for request TFetchResultsReq(fetchType=0, operationHandle=TOperationHandle(hasResultSet=True, modifiedRowCount=None, operationType=0, operationId=THandleIdentifier(secret=&#39;:ThMO\xb5Ny\xac\xb0Hh\x07\x85\xa9\x0b&#39;, guid=&#39;\xec\xf7\x99\xe0]\x1dK\x1e\xbe&amp;\xe2\xc9W\x89\xff\xc1&#39;)), orientation=4, maxRows=100): TFetchResultsResp(status=TStatus(errorCode=0, errorMessage=&quot;java.io.IOException: org.apache.hadoop.hive.serde2.SerDeException: org.codehaus.jackson.JsonParseException: Unexpected character (&#39;V&#39; (code 86)): expected a valid value (number, String, array, object, &#39;true&#39;, &#39;false&#39; or &#39;null&#39;)\n at [Source: java.io.ByteArrayInputStream@20177b09; line: 1, column: 2]&quot;,",318723
133049,"1366, &quot;Incorrect string value: &#39;\\xE2\\x80\\xACWec...&#39; for column &#39;search&#39; at row 1&quot;)",317982
133023,"Error while compiling statement: FAILED: RuntimeException Cannot create staging directory &#39;hdfs://nameservice1/common_folder/nyc_taxi_data/.hive-staging_hive_2019-06-16_09-47-03_295_209325844667245374-9&#39;: Permission denied: user=srishti125_gmail, access=WRITE, inode=&quot;/common_folder/nyc_taxi_data&quot;:sanskar.agrawal_upgrad:trainers:drwxrwxr-x at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:279) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:260) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:240) at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:162) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:152) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:3534) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:3517) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:3499) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6646) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4416) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4386) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4359) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:873) at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:323) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:618) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2220) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2214)",317982
132807,"Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask",308960
131459,"Getting error while executing DELETE statement. Created ORC table with paritions, clusters.",308636
131260,"Analyse the average speed of the most happening days of the year, i.e. 31st December (New years eve) and 25th December (Christmas) and compare it with the overall average. As part of above we need to compare with average of that particular month or with total average speed?",320103
131858,nan,303228
131478,nan,316211
131187,"--2.Segregate the data into five segments of tip paid: [0-5), [5-10), [10-15) , [15-20) and &gt;=20. --Calculate the percentage share of each bucket (i.e. the fraction of trips falling in each bucket). For above question do we need to bucking table from partition table. Because we have already done orc(which is binary) normally we do cluster by s clustered by (reviewerid) into 4 buckets how can we specify different range values to make bucketing",312019
131415,I tried to also look at the logs and found below description of error. Not sure how to resolve?,310509
131484,"like existing table name, column names or keywords like we get in pl-sql developer",300721
132303,"Segregate the data into five segments of &lsquo;tip paid&rsquo;: [0-5), [5-10), [10-15) , [15-20) and &gt;=20. Calculate the percentage share of each bucket (i.e. the fraction of trips falling in each bucket). Can someone please suggest how to proceed with this?",310952
131489,created a table like create external table ......partitioned by .... clustered by ...stored as orc location...tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;); I tried deleting the value from the orc partitioned bucketed table using where clause but got into error. Error while compiling statement: FAILED: SemanticException [Error 10122]: Bucketized tables do not support INSERT INTO: Table: nyc_taxi.taxi_partition_orc,301114
131491,"Hi, Can someone pls. explain exactly what needs to be done for this task? Didn&#39;t get clarity after going through related questions in the forum. Do we need to create another bucketed table on the column &#39;tip_amount&#39;? If yes then how to pass bin ranges? If no, then what is the expectation? Calculating &#39;percentage share&#39; in the query itself?",311686
131525,nan,318005
131608,"&quot; Miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges &quot; Does this mean that we need to only consider $0.50 and $1 or any values between $0.50 and $1?",312756
132314,nan,318791
131454,"In order to delete data from table we can use ACID properties.When we use ACID properties table properties should be set to tbleproperties(""transactional"" = ""true"") , But when we do that we can't enter data from other table like we can't use insert overwrite the table. I am aware that we can use "" insert into table partition(partition col) values(value_rows)"". But this is more of like manual entry. Can anyone help me out on how to DELETE data from table by using ACID properties.",312756
131753,nan,303228
131796,Trying to insert data from original table into ORC table . The table has been sucessfully created. The above error while inserting the records.,314313
131851,Is there a way to find difference between two timestamps. Ofcourse direct subtraction doesn&#39;t work.,311857
131831,nan,318005
131357,"I have created external table and have got some 1174570 entries. I need to proceed further on the orc partitioned tables. Had some doubts. It says: Before answering the below questions, you need to create a clean, ORC partitioned table for analysis. Remove all the erroneous rows. So we need to drop all errorneous rows in the current external table and this table will serve as input to the orc partiotioned table?",301114
130783,"In the data dictionary it is mentioned : MTA_tax - $0.50 MTA tax that is automatically triggered based on the metered rate in use. Improvement_surcharge -$0.30 improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015. In the dataset , there is data where MTA_tax =0 and Improvement_surcharge=0. Are these values valid or they are erroneous rows",311254
131361,"Regarding the Data Quality Issues, little confused as what needs to be done. Is it regarding the Distinct values in each columns, checking NULL vales in each column, Count of well formatted dates, numbers or strings, Min and max of numeric columns etc.",310210
131364,After creating external table i need to clean up the error rows. Hence i am overwriting the existing table by selecting the non erroroneous rows into the table. INSERT OVERWRITE TABLE taxi_tbl SELECT.... While performing the above i am getting exception that states canot create the directory with permission denied error.,301114
131286,nan,318005
131368,"I have used case when to group into buckets. However, do we then run a group by on that and how do we get the % of total for each bucket?",312096
131433,"As there are columns which have the value 0 as passenger_count, can we consider that as erroneous ? Also there are values more than 6 like 7/8/9, can we consider those also as erroneous?",318448
131559,nan,318005
130796,"while running the query INSERT OVERWRITE TABLE, getting the below error for the user Error while compiling statement: FAILED: RuntimeException Cannot create staging directory &#39;hdfs://nameservice1/common_folder/nyc_taxi_data/.hive-staging_hive_2019-06-05_07-54-25_095_7225248743357877055-1&#39;: Permission denied: user=prateeksinghs_XXXX, access=WRITE, please help",316036
130798,It has been mentioned in the data dictionary that $0.50 MTA tax is automatically triggerred based on the metered rate in use. Dose this mean that this column should contain only $0.50 values? Can anybody please confirm.,314730
130801,nan,312518
131622,nan,318802
131579,,305655
131580,"How to create partition table on timestamp column ( tpep_pickup_datetime timestamp,tpep_dropoff_datetime timestamp) as int and insert value from the external master table",307843
131597,"getting null in time wit below query- select *, substr(tpep_pickup_datetime,12)-substr(tpep_dropoff_datetime,12) as time from newyork_taxi_partitioned limit 10",318802
133347,Error while compiling statement: FAILED: SemanticException [Error 10297]: Attempt to do update or delete on table default.0_0_nyc_taxi_lovish that does not use an AcidOutputFormat or is not bucketed,318772
131482,"SET hive.exec.dynamic.partition.mode=non-strict; SET hive.enforce.bucketing =true; SET hive.exec.max.dynamic.partitions=100000; SET hive.exec.max.dynamic.partitions.pernode=100000; ADD JAR /opt/cloudera/parcels/CDH/lib/hive/lib/hive-hcatalog-core-1.1.0-cdh5.11.2.jar; -- CREATED TABLE create external table if not exists nyc_yellow_taxi_shalakam ( VendorId int, tpep_pickup_datetime timestamp,tpep_dropoff_datetime timestamp, passenger_count int, trip_distance double, RatecodeID int, store_and_fwd_flag string, PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE LOCATION '/common_folder/nyc_taxi_data/' TBLPROPERTIES (""skip.header.line.count""=""1""); create external table if not exists nyc_yellow_taxi_orc_shalakam ( VendorId int,tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp,passenger_count int, trip_distance double, RatecodeID int, store_and_fwd_flag string, PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) PARTITIONED BY ( yr INT, mn INT ) CLUSTERED BY (PULocationID) INTO 5 buckets STORED AS ORC LOCATION '/user/hive/warehouse/shalakaorc' TBLPROPERTIES (""orc.compress""=""SNAPPY""); insert overwrite table nyc_yellow_taxi_orc_shalakam partition(yr, mn) SELECT VendorId, tpep_pickup_datetime, tpep_dropoff_datetime,passenger_count, trip_distance,RatecodeID, store_and_fwd_flag, PULocationID, DOLocationID, payment_type, fare_amount, extra,mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, year(tpep_dropoff_datetime) as yr,month(tpep_dropoff_datetime) as mn FROM nyc_yellow_taxi_shalakam; delete from nyc_yellow_taxi_orc_shalakam where vendorid =0;",308636
131891,"Analysis-II What is the correlation between the number of passengers on any given trip, and the tip paid per trip? Do multiple travellers tip more compared to solo travellers? Hint: Use CORR(Col_1, Col_2) I am getting Nan when trying to find the correlation where passenger count is 1. Did anybody faced this issue ?",314197
131655,"1) MTa_tax have few records other than 0.5 value, should we keep those records? 2) Improvement_Surcharge have records other than 0.3 value, should we keep those records? 3) For few records Total Amount is not matching with addition of all columns(fare_amount + extra + mta_tax + tip_amount + tolls_amount + improvement_surcharge). Arent these error records &amp; shouldnt they be deleted?",312093
131859,"This means we need to consider records only for NOV,DEC -17 right from the begining? or It will be from Anlaysis - I? We need to consider all the records before that? like for the section basic data quality checks?",312756
130800,"HI, when i created my table, create external table if not exists nyc_yellow_taxi_theerthu -- (VendorID int, tpep_pickup_datetime timestamp, tpep_dropoff_datetime timestamp, passenger_count int, trip_distance double, -- RatecodeID int, store_and_fwd_flag string, PULocationID int, DOLocationID int, payment_type int, fare_amount double, extra double, -- mta_tax double, tip_amount double, tolls_amount double, improvement_surcharge double, total_amount double) -- location &#39;/common_folder/nyc_taxi_data/&#39;; and when select query I got all empty table. Can any one suggest me in solving this.",317410
131502,"Bad status for request TFetchResultsReq(fetchType=0, operationHandle=TOperationHandle(hasResultSet=True, modifiedRowCount=None, operationType=0, operationId=THandleIdentifier(secret=&#39;g]\xb6\xde\x87\xcbA\x95\x9aJ\x06\x9f\xc1\xfa@0&#39;, guid=&#39;;u\x8cSn\xe5KF\xb9Cb\x11\x107\xef\x80&#39;)), orientation=4, maxRows=100): TFetchResultsResp(status=TStatus(errorCode=0, errorMessage=&#39;java.io.IOException: org.apache.hadoop.hive.serde2.SerDeException: java.io.IOException: Start token not found where expected&#39;, sqlState=None, infoMessages=[&#39;*org.apache.hive.service.cli.HiveSQLException:java.io.IOException: org.apache.hadoop.hive.serde2.SerDeException: java.io.IOException: Start token not found where expected:25:24&#39;, &#39;org.apache.hive.service.cli.operation.SQLOperation:getNextRowSet:SQLOperation.java:437&#39;, &#39;org.apache.hive.service.cli.operation.OperationManager:getOperationNextRowSet:OperationManager.java:286&#39;, &#39;org.apache.hive.service.cli.session.HiveSessionImpl:fetchResults:HiveSessionImpl.java:768&#39;, &#39;sun.reflect.GeneratedMethodAccessor16:invoke::-1&#39;, &#39;sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43&#39;, &#39;java.lang.reflect.Method:invoke:Method.java:498&#39;, &#39;org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78&#39;, &#39;org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36&#39;, &#39;org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63&#39;, &#39;java.security.AccessController:doPrivileged:AccessController.java:-2&#39;, &#39;javax.security.auth.Subject:doAs:Subject.java:422&#39;, &#39;org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1920&#39;, &#39;org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59&#39;, &#39;com.sun.proxy.$Proxy22:fetchResults::-1&#39;, &#39;org.apache.hive.service.cli.CLIService:fetchResults:CLIService.java:462&#39;, &#39;org.apache.hive.service.cli.thrift.ThriftCLIService:FetchResults:ThriftCLIService.java:691&#39;, &#39;org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults:getResult:TCLIService.java:1553&#39;, &#39;org.apache.hive.service.cli.thrift.TCLIService$Processor$FetchResults:getResult:TCLIService.java:1538&#39;, &#39;org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39&#39;, &#39;org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39&#39;, &#39;org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56&#39;, &#39;org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286&#39;, &#39;java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1149&#39;, &#39;java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:624&#39;, &#39;java.lang.Thread:run:Thread.java:748&#39;, &#39;*java.io.IOException:org.apache.hadoop.hive.serde2.SerDeException: java.io.IOException: Start token not found where expected:29:4&#39;, &#39;org.apache.hadoop.hive.ql.exec.FetchOperator:getNextRow:FetchOperator.java:508&#39;, &#39;org.apache.hadoop.hive.ql.exec.FetchOperator:pushRow:FetchOperator.java:415&#39;, &#39;org.apache.hadoop.hive.ql.exec.FetchTask:fetch:FetchTask.java:140&#39;, &#39;org.apache.hadoop.hive.ql.Driver:getResults:Driver.java:1995&#39;, &#39;org.apache.hive.service.cli.operation.SQLOperation:getNextRowSet:SQLOperation.java:432&#39;, &#39;*org.apache.hadoop.hive.serde2.SerDeException:java.io.IOException: Start token not found where expected:30:1&#39;, &#39;org.apache.hive.hcatalog.data.JsonSerDe:deserialize:JsonSerDe.java:178&#39;, &#39;org.apache.hadoop.hive.ql.exec.FetchOperator:getNextRow:FetchOperator.java:489&#39;, &#39;*java.io.IOException:Start token not found where expected:30:0&#39;, &#39;org.apache.hive.hcatalog.data.JsonSerDe:deserialize:JsonSerDe.java:164&#39;], statusCode=3), results=None, hasMoreRows=None)",320687
140584,"Don&#39;t understand, the video on the last page doesn&#39;t show the standard time as mentioned in the question. How do we answer this question",304022
90899,"Extract the first two genres from the genres column and store them in two new columns: genre_1 and genre_2 . Some of the movies might have only one genre. In such cases, extract the single genre into both the columns, i.e. for such movies the genre_2 will be the same as genre_1 I am able to extract the data from genres: sp = movies['genres'].str.split('|') or split_me = list(map(lambda x: x.split(""|""), movies['genres'])) how do i now add to the 'movies'",302739
92455,Yesterday my marks were showing 19.4 and today it has reduced to 19.3. I am happy with both but just wondering why?,305845
90384,Did anyone successfully merge the IPL 17 &amp; 18 data. I'm getting wierd data set.,312199
90413,nan,300687
90180,nan,320689
90719,nan,314678
88943,"&lsquo; to the naked eye looks just as &#39; but gives an error when used in python code. For questions with checkboxes (multiple answers), What should we consider?",318329
90478,"Suppose in a MCQ Graded question there are multiple answeres for a specific question, it is necessary that the question should have some specific lines related to multiple solution. For example &quot;Multiple Solutions are possible.&quot; is written at the end of questions so that we will be careful to chose the multiple soultion OR we have to assume that it is possible that each or some of questions have multiple solutions without any demarcation at the end of the question line..",306243
91033,"temp = a[n,:] a[m,:] = a[n,:] a[m,:] = temp nth row is getting duplicate of mth row.Please suggest",310529
90592,nan,307708
92360,"I'm trying to update the table employee with a field 'hno' and trying to fill it with value 1. I was not able to fill the column with any value so just tried it with 1. Could anyone help me in solving this issue, i'm stuck with this?",318328
89782,"I answered to a question in graded questions and I verified and ran my code successfully and when I hit submit button it shows partially correct, what does it mean? can I take it as correct answer or Do I need to check and re-submit again. Please explain me.",312756
90975,Numpy graded question is not covered as part of lecture. I don't see it anywhere prof mentioned nor upgrad gave any comments reg the swapping of values.,306735
90985,nan,302877
90081,"Hello Everyone, For the first graded question in Pandas Graded Questions, we got a lot of concerns from students that their columns got sorted alphabetically when they executed the cell. It is a valid concern which is why we have decided that that particular graded question would be nullified, i.e. all the students would be scored equally. Meanwhile, don't worry too much as it is on us now and feel free to proceed with the rest of the course.",301619
90789,nan,311083
89410,nan,311952
90028,"In MCQ's graded questions, in some questions multiple choices are correct, but in that question it is not clearly mentioned that one or more choices are correct. I think Upgrad people should clearly mentioned if one or more choices are correct. Even in exams like GRE and CAT it is clearly mentioned",318429
90363,"There is some problem in the first question of Pandas Graded Quiz. Question is related to ipl18, top 4 teams. As per the below link, the question is nullify, so do we have to just submit it by choosing wrong answer. Can you guys please help? https://learn.upgrad.com/v/course/208/question/90081",306243
90857,"I am trying to create a zeroes array and getting the following error : Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 5, in &lt;module&gt; print(np.zeros(n,n), dtype=np.int) TypeError: data type not understood. What is wrong with the above sytax ?",319876
90930,nan,314756
91045,"Hi, For the final pandas graded question when I tried to execute the set_index code as below: ipl18.set_index(&#39;Team&#39;, inplace = True) ipl17.set_index(&#39;Team&#39;, inplace = True) ipl = ipl18.add(ipl17, fill_value = 0) It gives me the below error (see partial snapshot in attached): --------------------------------------------------------------------------- KeyError Traceback (most recent call last) ~\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance) 3062 try: -&gt; 3063 return self._engine.get_loc(key) 3064 except KeyError: pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() KeyError: &#39;Team&#39; During handling of the above exception, another exception occurred: KeyError Traceback (most recent call last) &lt;ipython-input-34-7210d0a844da&gt; in &lt;module&gt;() ----&gt; 1 ipl18.set_index(&#39;Team&#39;, inplace = True) 2 ipl17.set_index(&#39;Team&#39;, inplace = True) 3 ipl = ipl18.add(ipl17, fill_value = 0) 4 ipl ~\Anaconda3\lib\site-packages\pandas\core\frame.py in set_index(self, keys, drop, append, inplace, verify_integrity) 3904 names.append(None) 3905 else: -&gt; 3906 level = frame[col]._values 3907 names.append(col) 3908 if drop: ~\Anaconda3\lib\site-packages\pandas\core\frame.py in __getitem__(self, key) 2683 return self._getitem_multilevel(key) 2684 else: -&gt; 2685 return self._getitem_column(key) 2686 2687 def _getitem_column(self, key): ~\Anaconda3\lib\site-packages\pandas\core\frame.py in _getitem_column(self, key) 2690 # get column 2691 if self.columns.is_unique: -&gt; 2692 return self._get_item_cache(key) 2693 2694 # duplicate columns &amp; possible reduce dimensionality ~\Anaconda3\lib\site-packages\pandas\core\generic.py in _get_item_cache(self, item) 2484 res = cache.get(item) 2485 if res is None: -&gt; 2486 values = self._data.get(item) 2487 res = self._box_item_values(item, values) 2488 cache[item] = res ~\Anaconda3\lib\site-packages\pandas\core\internals.py in get(self, item, fastpath) 4113 4114 if not isna(item): -&gt; 4115 loc = self.items.get_loc(item) 4116 else: 4117 indexer = np.arange(len(self.items))[isna(self.items)] ~\Anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance) 3063 return self._engine.get_loc(key) 3064 except KeyError: -&gt; 3065 return self._engine.get_loc(self._maybe_cast_indexer(key)) 3066 3067 indexer = self.get_indexer([key], method=method, tolerance=tolerance) pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc() pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item() KeyError: &#39;Team&#39; I tried using the code from the solution sheet as well, assuming I had made some syntax error but I keep getting the same issue. Can someone help resolve this. TiA",312608
91570,"Hi all, My first question is that if we have 4 test cases and after submitting only 3 pass and 1 gets failed. Do we get marks for three test cases? If we do not get marks at all for partial correct answers then the following assertion must be wrong. But if we do get marks for partially correct answers the following must be true. I was attempting a graded coding question and on the start of the page what I see is the following: This means if we solve the question in first attempt, we get 100% based upon the number of testcases we clear. If we solve in second attempt then we get 50% of the total marks. I have a scenario and want to know whether is this the correct method to calculate the final score of the answer. Let's suppose if we have 4 testcases and after submitting following happens: In the first submit 3 out of 4 test cases gets passed. Thus we score 7.5 out of 10 based upon the image above In the second submit 4 test cases pass but as mentioned in the above image, even if I cleared all the testcases in the second attempt but my scores would 50% only i.e 5 out of 10. I request the TAs or someone who knows how scores are calculated answer this question so that we can answer the questions smartly and get maximum scores out of these.",318355
91368,"When I write a solution and try with Verify, am getting the following error Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 3, in &lt;module&gt; array_multipleof5 = np.arrange(5,55) AttributeError: &#39;module&#39; object has no attribute &#39;arrange&#39; What could be the error?",314329
90152,nan,320687
90212,nan,305804
90122,nan,305804
89440,nan,319302
89490,Is it not a simple solution than the sample solution provided? using negative index will reduce the lines of code,311061
90424,I am not pasting the code as many of you would be doing it . The solution code by upgrad is not working for odd integers as they have applied the concept of integer division using title(). Can anyone confirm that.,318372
88791,"Hi, The images included in the python notebook provided are not getting loaded. I have kept the image file in the same directory as the notebook. In edit mode I see that the command for the image file is : &lt;img src=&quot;numpy_axes.jpg&quot; style=&quot;width: 600px; height: 400px&quot;&gt; Can someone shed some light on this?",310511
89503,"For e.g: In Excel, if i use the function RAND() i know that the distribution of variables is uniform but What is the type of distribution (is it normal or uniform or something else) followed by the functions np.random.random() or np.random.randint() etc",318328
89522,nan,315757
88827,nan,307488
88841,nan,308495
88856,"what is wrong in code? import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] import numpy as np array_1 = np.array([list_1],[list_2]) print(array_1.shape) print(array_1.ndim)",315247
90376,"Code Exercises embedded between lessons have suddenly stopped working for me. The block loads properly in the page and I am able to type/edit the solution. But when I click &quot;Run Code&quot;, it hangs up . The status goes through the following phases - 1. Running 2. Hang On ! This seems to be taking time.. 3. Sorry ! Please try again! Interestingly, this is happening for all exercises - even the ones I had successfully completed earlier in preparatory modules. I tried switching network connections and also tried reinstalling browser; but it didnt help. Have tried it in Chrome as well as Chrome Canary. Does anyone have a clue?",318762
88862,nan,315247
89665,"Hi all, In Python we have tuples, lists, sets, dictionaries and now NumPy array. Please explain me how memory management is done in Python for it&#39;s Data Structures ? In C we know how memory management and array addrissing is done with indexes associated with ecah element. It is said in the first video here that NumPy arrays are faster and the operation are done on individual element.Well it can be visualised that way.But It is a little bit difficult to understand as how can we reduce the time from n**2 to 1 unless we know about how the array elements are stored in memory. Please explain me how we achieve the O(1) time for all operations in NumPy arrays",307490
90444,"In Numpy arrays, why do we not use array_2d[:, -1] to get the last column? Why do we need to calculate the number of rows and columns and why not directly use the &#39;-1th&#39; index, which will give the last? Similarly, for the last row, we can use array_2d[-1, :]. My solution is already accepted but I am willing to know any specific reason/ use-case to not use &#39;-1th&#39; index.",317987
88861,In coding question after submitting question it is showing submitted but not shown accepted or rejected. How will i know my answer is accepted or not.,310419
88868,nan,310585
88884,nan,310585
89531,,300690
89532,,300690
89533,nan,300690
89534,,300723
89536,nan,300723
90660,"Before starting this PGDDS I was not aware of string , char , etc word . Now I hv to work in Pyhton so for this I need some basic abcd ... where I hv to read and write pl. let me know .. I will work hard but Im stucked into this .",319969
89548,,306996
89908,nan,311169
89762,"I have given hard coded the values for this like below and exhausted my submission. is this graded MCQ? the verify button didn&#39;t throw any error. Now it says my code is rejected. col_first =array_2d[:,0] row_first = array_2d[0,:] col_last = array_2d[:,3] row_last = array_2d[2,:] print(col_first) print(row_first) print(col_last) print(row_last)",306735
89917,nan,320685
89948,"Hello Everyone. Need a clear explanation on one of FAQs in the Module 3 Session on Subset, Slicing and Indexing as mentioned below: Given a 2D NumPy array, &#39;a&#39;, what is the difference between the two commands: a[:, 1] and a[:, 1:2]? I am not able to clearly understand the explanation for the a[:, 1:2]. Thanks for your help.",310210
89960,"Can someone pls explain the working of the following code: x = np.array([[0, 1], [1, 0]]) check = np.tile(x, (n//2, n//2))",304815
88846,"As per Verify, the answer is correct. But at the time of submission, the below code got rejected. Please help. Input 1: [[11 12 13 14] [21 22 23 24] [31 32 33 34]] Output 1: [11 21 31] [11 12 13 14] [14 24 34] [31 32 33 34] # Read the input list import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) import numpy as np # Convert the input list to a NumPy array array_2d =np.array(input_list) # Extract the first column, first row, last column and last row respectively using # appropriate indexing col_first = array_2d[:,0] print(col_first) row_first = array_2d[0,:] print(row_first) col_last = array_2d[:,3] print(col_last) row_last = array_2d[2,:] print(row_last)",307494
89018,"Given an even integer n, create an n*n checkerboard matrix with the values 0 and 1. n may NOT be even number. # Read the variable from STDIN import numpy as np n = int(input()) # Type your code here.",318458
89021,"Have you noticed - import numpy as np array_1 = np.array([ [1,1,1], [1,1,1] ]) print(np.shape(array_1)) print(np.ndim(array_1)) is same as - import numpy as np array_1 = np.array([ [1,1,1], [1,1,1] ]) print(array_1.shape) print(array_1.ndim) ?",318458
89040,,300716
89738,"For below question,my test cases passed but its getting rejected during submission Description Given an integer &#39;x&#39;, create an array of size m*n having all integer values equal to &#39;x&#39;. Hint: Use dtype to specify integer. Format: Input: Line 1: A single integer &#39;x&#39; Line 2: A single integer &#39;m&#39; indicating the number of rows Line 3: A single integer &#39;n&#39; indicating the number of columns Output: An array of size &#39;m*n&#39; having all the values as &#39;x&#39; My Code : array_x = np.ones((rows_m,cols_n), dtype=np.int)",300699
89060,nan,313826
89110,nan,320689
89061,Solution output [ 0 5 10 15 20 25 30 35 40 45] Expected output [ 5 10 15 20 25 30 35 40 45 50] how to exclude the zero,300687
89135,nan,308437
89146,nan,320689
93866,nan,308495
89192,"Input: A single even integer &#39;n&#39;. Output: An &#39;n*n&#39; NumPy array in checkerboard format. check = np.tile(x, (n//2, n//2)) The above logic is used, cam someone explain me this??",300688
89198,"Prof says, that in a d-dimensional array we ask python to create an outer list, that consist of d-1 dimensional list? This does not make sense. for example : np.array([[1,2,3], [4,5,6], [7,8,9]]). Is this a 3 dimensional array? If yes, the individual list inside are three 1 dimensional arrays right? what d-1, i.e 2 dimensional object is inside this array?",317809
89199,"The question says that create the array using list which have been provided with input data in question, but it does not tells that create array using list with any input data . I think questions needs to be re-phrased as clarity is not there. Due to this other test cases fail",318814
89817,"I could see that np.ones &amp; np.zeros are creating a list as the result has the commas in between the values, understand the output data is homogeneous. But where as, arrays doesnt have the commas in between the values. So ideally np.ones &amp; np.zeros are creating a list &amp; not an array. Is my understanding true? Can any one please help.",312093
90067,Is there a better way to write code or syntax other than remembering it ? jupyter Nb gives hardly any hints to what why a particular syntax is wrong ?,315242
89276,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] list_3 = input_list[2] import numpy as np array_1 = np.array([[list_1],[list_2],[list_3]]) print(array_1) This is my code and the following is the output:",316368
90755,nan,310472
89391,,310508
89691,"How can i check syntax for any function ? is there a help in python..something similar to ? in R For example, i wish to understand the syntax of np.full(). I can go and check in google. However, is there an option similar to R?",312096
89395,"array_2d =np.array(input_list) # Extract the first column, first row, last column and last row respectively using # appropriate indexing col_first=array_2d[:,array_2d.shape[1]-4] row_first =array_2d[array_2d.shape[0]-3,:] col_last =array_2d[:,array_2d.shape[1]-1] row_last =array_2d[array_2d.shape[0]-1,:] #row_last = array_2d[array_2d.shape[0]-1 , :] print(col_first) print(row_first) print(col_last) print(row_last)",300687
89091,The above code works while verifying and doesn&#39;t shows any error while running the code as well. Still the above code is rejected. can anyone explain??,317240
89705,"For an array as below a = np.array([[1,2,3], [4,5,6]]) Can anyone explain me the output of ""np.reshape(a, (3,-1))"" which is as below array([[1, 2], [3, 4], [5, 6]])",314730
90275,The professor G. Srinivasaraghvan is saying that can't access the element of NumPy array using array_2d[2][2] but I'm able to access.,320687
89706,"I understand the basic operations of reshaping the original array using reshape method. However, I could not infer the optional parameters which can be passed to this merhod as below. numpy. reshape ( a , newshape , order='C' ) All that I understand is that the order values C, F or A will provide C-like or Fortran-like index order. But, how does this change the reshaping of an array. Any detailed elaboration of this method would be of great help.",314730
89424,nan,318732
89708,nan,311745
89719,Like in python programs I have seen we write some imports on the top then creating lists and then and then importing numpy library etc.,300721
89709,"# Read the input from stdin() import sys lines = sys.stdin.readlines() int_x = int(lines[0]) rows_m = int(lines[1]) cols_n = int(lines[2]) import numpy as np array_x = array_x = np.full ((lines[1], lines[2]), lines[0], dtype=int) # Write your code here print(array_x)",303083
90287,Can you help me understand where the code is going wrong?,316084
91664,"#Code submitted for the problem above import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[1,2,3] list_2 = input_list[4,5,6] list_3 = input_list[7,8,9] import numpy as np array_3D = [list_1,list_2,list_3]#Type your answer here print(array_3D) print (type(array_3D)",314329
90485,nan,308638
90488,nan,308638
90481,Can custom packages like NumPy be created and used as libraries ? If yes how? Can they be imported and used in the coding questions in the UpGrad consoles ?,310533
88887,"I almost always see &quot;Loading problem&quot; on some questions if there are say multiple questions on a page(say 5 or 10) , not all the questions load (and I click a couple of times and only then it loads). I am on superfast ACT broadband [ 4.18 Mbps download and 2.56 Mbps upload] which is the best speed in the market. not sure if website streaming has issues. any thoughts?",309211
88931,"I have tried changing the values of 2 with 1 and 10, I got an empty array and a 3X2 matrix respectively . It would be helpful if you answer why this happened as well Suppose you have the following NumPy array &#39;a&#39;: [[1 2 3] [4 5 6] [7 8 9]] the output for a[:, 1: 2 ] would be: [[2] [5] [8]]",310585
90577,"import NumPy as np arr = ([0, 1, 2]) np.tile(arr,3) Am getting error ike this: ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-7-08466b651a21&gt; in &lt;module&gt;() 1 ----&gt; 2 import NumPy as np 3 arr = ([0, 1, 2]) 4 np.tile(arr,3) ModuleNotFoundError: No module named &#39;NumPy&#39;",308638
89257,nan,303673
90999,nan,303085
89195,nan,308782
91353,nan,322683
89736,,306729
90876,"I installed Jupyter notebook when it opens I see a lot of folders, and a screen opens with the cursor blinking how do I start typing or working on it",312892
90511,,306012
95132,"Consider two examples and their output (1) f=np.linspace(0,1,6) f output: array([0. , 0.2, 0.4, 0.6, 0.8, 1. ]) (2) e=np.linspace(0,1,6,endpoint=False) e output:array([0. , 0.16666667, 0.33333333, 0.5 , 0.66666667, 0.83333333]) What is basically endpoint trying to do here?",301114
102498,"Whats the exact usage of np.arrays (apart from the fact that it is vectorized), in the context of data analysis? Any of you have used it in any of the assignments so far?",314084
103003,"In few of the articles on internet i have come across a feature called broadcasting. This has been termed as one of the advantages of using Numpy arrays over lists. What does this mean? Numpy array uses vectorized operation wherein looping,indexing happens behind the scenes and thus execution time is faster. How is broadcasting related to this?",301114
90151,"Hi all, I am unable to install PyMySQL and PyPDF2 in Jupyter notebook. pip install PyMySQL Can someone please highlight the exact steps?",320195
90135,"so in drop function axis 0 = rows . But otherwise, axis = 1 is doing calculations row-wise. ex - sum( axis = 1 ) &gt; 5 Am I correct in my interpretation?",308962
90863,nan,301890
96340,conda install -c anaconda pydataset Even doing above is not working.,312019
90924,"Find out the percentage of missing values in each column in the given dataset. import pandas as pd df = pd.read_csv('https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0') print(round (100 * df.isnull().sum()/len(df.index)), 2)#Round off percentage values to 2 decimial places.",300748
90897,,312376
93739,"movies= pd.read_csv(&quot;C:\IIIT B\Main Program\Case Study Investment\movies.txt&quot;, sep=&quot;\t&quot;, encoding = &quot;ISO-8859-1&quot;) rounds2 = pd.read_csv(&quot;C:/IIIT B/Main Program/Case Study Investment/rounds2.csv&quot;,encoding = &quot;ISO-8859-1&quot;) Difference of &quot;/&quot; and &quot;\&quot; why so?",300721
90887,In section 3.6 of Python Assignment,318778
90398,nan,312019
90416,nan,321435
90148,"Hi, it is confusing to see axis=1 in dropping column level data . can someone explain it # removing the three columns df = df.drop(&#39;BuildingArea&#39;, axis=1) df = df.drop(&#39;YearBuilt&#39;, axis=1) df = df.drop(&#39;CouncilArea&#39;, axis=1) round(100*(df.isnull().sum()/len(df.index)), 2)",300716
90739,"can numpy be used to maniulate pandas series , is the &quot;~ &quot; a is not function , why can&#39;t we mention is not nan instead.",316132
90674,"When i have my .csv file stored in my desktop. how do i call this in the jupyter. my intension here is not to understand the command, but what need to be put after pd.read_csv(&quot;how to you represent the location&quot;)",304024
90548,nan,300687
89502,"The expected output is not matching the solution output,the solution seems to look correct to me,whats surprising is the round off to 2 digits is not working, eventhough i give it in the command..This is the solution I have given. what is the problem here? import numpy as np import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0&#39;) #Type your code here for mean imputation df.loc[np.isnan(df[&#39;Product_Base_Margin&#39;])] = df[&#39;Product_Base_Margin&#39;].mean() print(round(100*(df.isnull().sum()/len(df.index)), 2))",304027
90304,"Hi All Actually while imputing ther missing values in column &#39;Product_Base_Margin&#39; isnan is used as you can see in code. I know why isnan use dbut why is it used here i cant get that.Could anyone please explain about this concept df.loc[np.isnan(df[&#39;Product_Base_Margin&#39;]), [&#39;Product_Base_Margin&#39;]] = df[&#39;Product_Base_Margin&#39;].mean()",310611
90452,nan,312448
91070,"Remove the missing values from the rows having greater than 5 missing values and then print the percentage of missing values in each colum. my code is import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0&#39;) df[df.isnull().sum(axis=1) &gt; 5] print(round(100*(df.isnull().sum()/len(df.index)),2)) but getting wrong answer",320687
89897,"# converting to type &#39;category&#39; df[&#39;Car&#39;] = df[&#39;Car&#39;].astype(&#39;category&#39;) # displaying frequencies of each category df[&#39;Car&#39;].value_counts() # imputing NaNs by 2.0 df.loc[pd.isnull(df[&#39;Car&#39;]), [&#39;Car&#39;]] = 2 round(100*(df.isnull().sum()/len(df.index)), 2)",314547
89750,,303674
89752,nan,300690
90237,"It seems that Google has changed the page definition, and there is no more a ""review-body"" class containing the reviews. What is the new class name to get the reviews, and also if I need to perform such sort of operations, then how would I get to know which class to use?",304814
90455,"I am having the below error when opening CSV file. I have tried all the suggestion on stackoverflow, such as addinfg r to string, changing \ to / but none of the solutions is working. In fact, it creates another error saying ""File not found"" even the file is there. Kindly help how to resolve the same.",310509
90559,"# removing the three columns df = df.drop(&#39;BuildingArea&#39;, axis=1) df = df.drop(&#39;YearBuilt&#39;, axis=1) df = df.drop(&#39;CouncilArea&#39;, axis=1)",300687
90234,"As per video, axis = 1 will act on rows but as far as i remember axis = 0 relates to row. Please throw some light on same For e.g df.isnull.any(axis=1) - It will check if any of the value in row is null and return output accordingly. But it should act on columns as axis=1 refers to column.",320103
90572,getting the abive error please advise,300687
90606,nan,308782
89915,nan,308432
89921,nan,308432
90453,"Solution output Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 4, in &lt;module&gt; df = df.loc[np.isnan(df[&#39;Product_Base_Margin&#39;]), [&#39;Product_Base_Margin&#39;]] = df[&#39;Product_Base_Margin&#39;].mean()#Type your code here for mean imputation AttributeError: &#39;numpy.float64&#39; object has no attribute &#39;loc&#39;",314678
90729,nan,306996
89945,nan,300723
89949,df=df[~np.isnan(df[&#39;Price&#39;])] can somebody please explain why &#39;~&#39; is used and why numpy(np) came into picture.,300723
89595,"In &#39;Getting data from Websites&#39; section in jupyter notebook exercise for getting data from facebook website review data, I am getting the error, in the code section where Beautifulsoup library &#39;SELECT&#39; returning a List, &#39;review-body&#39; giving an exception &#39;IndexError&#39; ( class &#39;.review-body&#39;). Hence, I am not able to proceed to check the rest of the code exercise for completeness. Please help.",311115
89360,"Hi, I am getting the error - &#39;No module named &#39;pymysql&#39;. I see that in FAQs, the solution is given as &quot;This happens probably because you have two or more versions of python. You are installing pymysql with a different Python version and the anaconda is using a different Python version. Try to install pymysql from anaconda distribution using the following command: &gt;&gt; conda install -c anaconda pymysql After installing pymysql this way, just update your conda distribution&quot; Can someone please highlight the exact steps? Also, I am new to Python architecture. So I am not sure how to write a code on the anaconda distribution or what &#39;update the conda distribution&#39; really means. Any help is appreciated.",310511
89613,"In bs4.BeautifulSoup(req.text, ""html5lib""), if we specify html5lib, what kind of errors can be avoided. Also what are the different types available apart from html5lib",318804
90499,"Hi , Am unable to revert the dropped column from dataframe...Need suggestions on how to get back the dropped column,as am unable to get any commands in google..need help on this",300693
90982,How to execute group of statements at once in jupyter notebook?,319006
92686,"I am running scrapy 1.4.0 using python 2.7, in scrapy shell environment, and trying to get the details from online shopping websites, I am able to load the basic data and get the html, but the issues comes while trying to run the below code : response.css("".title::text"").extract() rather than giving a list it is returning an empty set.. Kindly point out how to resolve this issue. Is there a better way to get product spec and reviews from online websites than using scrapy, please suggest. Thank you!",316132
89984,nan,306996
89638,nan,303674
89804,pip install pymysql in jupiter notebook shown as invalid syntax how to resolve this?,308437
89988,nan,320689
89809,nan,300733
90844,nan,310520
90979,I have installed the pyPDF in anaconda. still I am gettingt the error . &quot;No module named &#39;pyPDF2&#39;&quot; please solve my issue,308634
90977,How can we specify the path of the pdf files to be read in python. what if the pdf is not in the default directory,311032
89279,"Hi, Can anybody help me to understand the program for Rotating PDF pages. https://www.geeksforgeeks.org/working-with-pdf-files-in-python/ I want to understand the flow of the proram like how the main() is called. Thanks , Babita",307494
89837,"import requests import bs4 as bs url = ""https://play.google.com/store/apps/details?id=com.upgrad.student"" req = requests.get(url) soup = bs.BeautifulSoup('req.text',""html5lib"") reviews = soup.select('div') print(len(reviews)) Getting 0 length tried other tags and body as well. Thanks in advance.",315028
90774,Any one had this problem in windows 10?,303666
90828,"import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0&#39;) df = df[df.isnull().sum(axis=1) &lt; 5]#Type your code here. print(round(100*((df.isnull().sum(axis = 0))/len(df.index)),2))#Type the code for taking the percentage of missing values in each column))#Round off to 2 decimal places.",314313
90995,"i am geeting above typro error when rounding percentage round(100 * (df.isnull().sum() / len(df.index)), )",305804
89323,"I gave the below query: df = pd.DataFrame(list(all_rows), columns=[&#39;Employee_ID&#39;,&#39;Employee_Name&#39;,&#39;Company&#39;,&#39;City&#39;]) df.head() It gives the error: AssertionError: 4 columns passed, passed data had 9 columns",314547
90084,Where/How can we use library chardet to know above encoding scheme? Any steps would be helpful.,320103
89677,While installing conda install pymysql it was using miniconda3 even though i used the Anaconda3 Prompt...Please help on this Quckily,318846
89825,"Can anyone help with the practice problem amazon.in.I changed the shoe_prices to shoe_prices = soup.select(&#39;.a-size-base.a-color-price.s-price.a-text-bold&#39;) I am getting the data but post I didnt understand how to split the data so that we only get prices and not as &lt;span class=&quot;a-size-base a-color-price s-price a-text-bold&quot;.....&gt; Since the example given python file is not working for Website Review,I dont know how to approach the problem.",301651
90790,please suggest how it will work and what are the requiirements,310385
89233,"There is no proper word document that helps user understand how to install Pymysql &amp; beautiful soup, it just says install, which version , how to update nothing is mentioned. In previous modules a detailed document was shared on how to install mysql, python, anconda. which was helpful. have got no clue on how to proceed in this module, as the codes donot execute. Dont think with such content with lack of clarity new users can do justice to the program.",308495
89366,Duplicates rows removal?,317993
89797,,318335
89682,"I am trying do a assigment in the Module 3&gt; Session 4 &gt; Cleaning Datasets Solution needs to be to round the values to 2 decimal but when I try to do the same I am getting unexpected behaviour. Values before applying rounding. import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0&#39;) print((df.isnull().sum() /len(df.index))* 100) Ord_id 0.000000 Prod_id 0.000000 Ship_id 0.000000 Cust_id 0.000000 Sales 0.238124 Discount 0.654840 Order_Quantity 0.654840 Profit 0.654840 Shipping_Cost 0.654840 Product_Base_Margin 1.297774 Values after rounding. import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0&#39;) print(round((df.isnull().sum() / len(df.index)) * 100),2) Ord_id 0.0 Prod_id 0.0 Ship_id 0.0 Cust_id 0.0 Sales 0.0 Discount 1.0 Order_Quantity 1.0 Profit 1.0 Shipping_Cost 1.0 Product_Base_Margin 1.0",318368
89865,nan,318335
90361,While imputing the values for the columns Bathroom and Cars professor uses pd.isnull() whereas for imputing the missnig values for Lattitude and Longtitude columns he uses np.isnan(). Can someone please explain the logic behind the same ?,318479
89951,nan,320689
89422,"code is like this: import numpy as np import pandas as pd companies = pd.read_csv(&quot;C:\Eshwar\Eshwar personal\UpGrad IITB\Python\3_Getting_and_Cleaning_Data\3_Getting_and_Cleaning_Data/addresses.txt&quot;, sep = &quot;\t&quot;, encoding = &quot;ISO-8859-1&quot;) companies.head() error: IOError: File C:\Eshwar\Eshwar personal\UpGrad IITB\Python_x0003__Getting_and_Cleaning_Data_x0003__Getting_and_Cleaning_Data/addresses.txt does not exist",318723
90276,,310509
90362,nan,311868
90371,"Using encoding = &quot;ISO-8859-1&quot; import pandas as pd companies = pd.read_csv(&quot;companies.txt&quot;, sep=&quot;\t&quot;, encoding = &quot;ISO-8859-1&quot;) companies.head() It showing me the error",308639
89437,nan,300733
89722,nan,308437
90101,,300690
90483,Below is the output: Ord_id 0.000000 Prod_id 0.000000 Ship_id 0.000000 Cust_id 0.000000 Sales 0.238124 Discount 0.654840 Order_Quantity 0.654840 Profit 0.654840 Shipping_Cost 0.654840 Product_Base_Margin 1.297774 dtype: float64 Ord_id 0.0 Prod_id 0.0 Ship_id 0.0 Cust_id 0.0 Sales 0.0 Discount 1.0 Order_Quantity 1.0 Profit 1.0 Shipping_Cost 1.0 Product_Base_Margin 1.0,302739
90383,"Hi, When I run the query of fetching data from web for the example shown, I am getting empty list. Code: reviews = soup.select(&#39;.review-body&#39;) print(type(reviews)) print(len(reviews)) print(&quot;\n&quot;) # printing an element of the reviews list print(reviews[:]) Output &lt;class &#39;list&#39;&gt; 0 [] Can any one help me",317410
90496,nan,300687
90487,"import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0&#39;) df[df.isnull().sum(axis=1)&gt;5] print(round(100*(df.isnull().sum()/len(df.index)),2))",319860
90491,"Hi, Can any explain what df.info does ,when i execute df.info() it returns the information of dataframe but unable to understand the output of just df.info",300693
90765,"Remove the missing values from the rows having greater than 5 missing values and then print the percentage of missing values in each column. using the below, it seems I can only delete columns: df= df[~ny.isnan(df[&#39;Profit&#39;])] how do I delete NaN values in a row?",302739
90470,nan,314678
89368,"While executing the jupiter workbook code to read from data base I am getting the following error: OperationalError: (1045, &quot;Access denied for user &#39;root&#39;@&#39;localhost&#39; (using password: YES)&quot;) The code that I ran was: import pymysql # create a connection object &#39;conn&#39; conn = pymysql.connect(host=&quot;localhost&quot;, # your host, localhost for your local machine user=&quot;root&quot;, # your username, usually &quot;root&quot; for localhost passwd=&quot;yourpassword&quot;, # your password db=&quot;world&quot;) # name of the data base; world comes inbuilt with mysql # create a cursor object c c = conn.cursor() # execute a query using c.execute c.execute(&quot;select * from city;&quot;) # getting the first row of data as a tuple all_rows = c.fetchall() # to get only the first row, use c.fetchone() instead",310511
90619,In video 2 it is mentioned that isnull().any(axis=0) denotes null values in a column. Just wanted to confirm axis = 0 denotes - row and axis = 1 denotes column. Is my understanding correct?,310443
90553,"In the pandas document for read_csv while describing the sep paramater, there is a mention of two different engines (python and pandas). What these engines are and what is the role of these engines? Are there multiple engines running at the same time?",318576
90113,"import pymysql conn = pymysql.connect(host=&quot;127.0.0.1&quot;, # your host, localhost for your local machine user=&quot;root&quot;, passwd=&quot;root&quot;, db=&quot;world&quot;) Is giving me the below error. ImportError: DLL load failed: The operating system cannot run %1. NOTE - I have this database and password is also correct. I&#39;ve tried with both localhost and 127.0.0.1",308962
90642,"conn = pymysql.connect(host=&quot;localhost&quot;, # your host, localhost for your local machine user=&quot;root&quot;, # your username, usually &quot;root&quot; for localhost passwd=&quot;yourpassword&quot;, # your password db= &quot;information_schema&quot; ) # name of the data base; world comes inbuilt with mysql # create a cursor object c c = conn.cursor() # execute a query using c.execute c.execute(&quot;select * from engines;&quot;) There is no database named as &quot;information_schema&quot; in MySQL ,still it show the values.From where is it retrieving the info?",318386
90360,ModuleNotFoundError : No module named &#39;pymysql&#39;while import pymysql from my MAC Need urgent help,319319
90862,nan,301890
89877,"i have done the sql connection, but i m not able to see any result when i run the below coomand nor i am getting any error please help. import pymysql # create a connection object 'conn' conn = pymysql.connect(host=""localhost"", # your host, localhost for your local machine user=""root"", # your username, usually ""root"" for localhost passwd=""******"", # your password db=""world"") # name of the data base; world comes inbuilt with mysql # create a cursor object c c = conn.cursor() # execute a query using c.execute c.execute(""select * from city;"") # getting the first row of data as a tuple all_rows = c.fetchall()",307493
96468,Plot a bar chart showing runs scored on the x-axis and frequency/count on the y-axis. In which bucket has he scored runs the most often? could you pls help someone here. I have cleaned tendulkar_ODI.csv runs column which has * and DNB and TDNB and made Runs column has int type. Now i am bit confused how to make bar plot with runs and frequency. pls help here.,312019
102972,"Dear All, Little puzzled on this topic on how are large scale deployments of Python handled? For eg: Websites like Quora or applications like Dropbox have majority of the portion developed in Python. How are large scale &#39;production&#39; deployment planned for Python scripts? How can system architecture be carried out for such large scale deployments? Typically, if we have MAC or Linux, python comes i built and use this to test our scripts there. PS: Already searched over internet and couldnt find anything concrete.",314084
116036,"I have a data-frame which has 200 variables from var_0 to var_199. All these vars are an integer type. My objective is here to remove the outliers only to keep the data falling between 2nd to the 98th percentile. For this, I need a function which can perform this on all the variables. Pls, help me with the code.",300735
90858,nan,302739
90125,nan,311165
90149,,315560
90157,"df_1 = pd.merge(market_df, customer_df, how=&#39;inner&#39;, on=&#39;Cust_id&#39;) on both the tables Key column for merge is labeled as &#39;Cust_id&#39;. What if column labels are different, &#39;Cust_id&#39; on one table and &#39;Customer_id&#39; on other. Should we make it same and merge or there is any option?",318436
90886,nan,318005
90327,"df_1 = pd.merge(market_df, customer_df, product_df, how=&#39;inner&#39;, on=&#39;Cust_id&#39;",307708
90161,"import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;France&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [15, 13, 9]} ) silver = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;Germany&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [29, 20, 16]} ) bronze = pd.DataFrame({&#39;Country&#39;: [&#39;France&#39;, &#39;USA&#39;, &#39;UK&#39;], &#39;Medals&#39;: [40, 28, 27]} ) tot_medal = gold.add(silver, fill_value = 0).add(bronze, fill_value = 0) #k=tot_medal.set_index(&#39;Country&#39;,inplace=&#39;True&#39;) print(tot_medal.sort_values(by =[&#39;Medals&#39;],ascending=False))",300687
90418,nan,317992
89430,"I am getting the below error while trying to run the command to extract data from the csv files. FileNotFoundError: File b&#39;../global_sales_data/market_fact.csv&#39; does not exist Also, is there a way to download the data from this file or see the physical location of this file?",315764
90119,"import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF&#39;) df_2 =df.loc[[&#39;month&#39;,&#39;day&#39;,&#39;temp&#39;,&#39;area&#39;]] #Type your code for selecting columns &#39;month&#39;, &#39;day&#39; , &#39;temp&#39; , &#39;area&#39; print(df_2.head(20))",300687
89362,How does the system read the CSV with just the folder location (code below). Shouldn&#39;t I provide the whole directory? It gave me an error when I typed in the whole directory. # reading a CSV file as a dataframe market_df = pd.read_csv(&quot;global_sales_data/market_fact.csv&quot;),308962
89409,"This question is based on global_sales_data/market_fact.csv dataset. I want to select a single column (&#39;Sales&#39;) from the dataframe. sales = market_df[&#39;Sales&#39;] Here, I want to fetch first 5, mid 5 and finally last 5 records from the dataframe. After fetching the records, I want to combine all 15 records in a dataframe to visualize them. How can I acheive it ? Example : Sales 0 136.81 1 &lt;sales&gt; 2 &lt;sales&gt; . . . 15 &lt;sales&gt;",312479
90294,nan,311160
89349,"Although my solution has been submitted. I am not clear with the logic. For example UK has 27 bronze and it should be last. In general, as medals tally is counted, more weightage is given to gold, then to silver , followed by bronze. Going by this logic, Russia should be third. But anyways, if the intention here is just to count total, then why should medals be categorized into gold, silver, bronze. pls let me know, if someone has some explanation.",317269
89487,"Team, I have a setuation wherein i am running a below code. list = [1,2,3,4]; s_1 = pd.Series(list) s_3 = s_1.apply(lambda x:x**2); s2 = pd.Series(s_3, index=list) print(s2); Ans it output the s2 as 1 4.0 2 9.0 3 16.0 4 NaN dtype: float64 If i remove the manually assigned index the output would be 0 1 1 4 2 9 3 16 dtype: int64 Why this difference. Please give some inputs to understand this behaviour.",311741
89494,nan,308432
90226,"While going through the coding console, I have found that sometimes even though the code is correct and it is passing all the sample test cases, code is showing as submitted. Reason (analyzing certain pattern)- writing code in readable manner. eg. sample 1: df[[&#39;month&#39;,&#39;day&#39;,&#39;temp,&#39;area&#39;&#39;]] sample2: df[[&#39;month&#39;, &#39;day&#39;, &#39;temp&#39;, &#39;area&#39;]] Although both the codes are same, but sample 2 is more readable gets accepted, whereas sample1 is not. Conclusion: One should try to write the code in more readable manner. Give space where it is required. By Require I mean It is good to have.",301649
90225,"What is the error? Columns X and Y from the data frame have to be multiplied and stored in a new column XY. import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF&#39;) df[&#39;XY&#39;] = df[&#39;X&#39;,&#39;Y&#39;].apply(lambda x,y: x*y) #Type your code here. print(df.head(20))",310505
90552,"Hi, I wrote the following code for Manual indexing question: opseries = pd.Series(np.array(range(1,(n+1))**2),index=range(1,n+1)) print (opseries) I got the following error: Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 7, in &lt;module&gt; opseries = pd.Series(np.array(range(1,(n+1))**2),index=range(1,n+1)) TypeError: unsupported operand type(s) for ** or pow(): &#39;range&#39; and &#39;int&#39; While I figured out the solution code, I still haven&#39;t figured the error here. Any guidance so I can understand the same is much appreciated. Thanks in advance!",312608
89507,nan,308432
88850,Ascending on one attribute and at the same time descending order by second attribute?,317993
90132,"Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 21, in &lt;module&gt; t3 = t3.drop(columns=[&#39;Medals_x&#39;,&#39;Medals_y&#39;]) TypeError: drop() got an unexpected keyword argument &#39;columns&#39; Checked the python version is same and pandas doc had drop(columns=[&#39;&#39;,&#39;&#39;]). Any one please let know why I see this. Is there any issue with what upgrade tool uses to verify our code.",301118
90439,"Why is column one in float and two in integer? import pandas as pd d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) print df output: one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4",300748
90953,"Position based Indexing df.iloc[0:3] -&gt; gets the rows 0,1,2 ( 3 not included ) Position based Indexing df.loc[0:3] -&gt; gets the rows 0,1,2,3 ( 3 included ) Is my observation correct? why do we have such a variation ?",310533
90232,nan,304319
90450,"import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;France&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [15, 13, 9]} ) silver = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;Germany&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [29, 20, 16]} ) bronze = pd.DataFrame({&#39;Country&#39;: [&#39;France&#39;, &#39;USA&#39;, &#39;UK&#39;], &#39;Medals&#39;: [40, 28, 27]} ) df1=pd.merge(gold,silver,how=&#39;outer&#39;,on=&#39;Country&#39;) df=pd.merge(df1,bronze,how=&#39;outer&#39;,on=&#39;Country&#39;) df.fillna(0,inplace=True) df[&#39;Medals&#39;]=df[&#39;Medals_x&#39;]+df[&#39;Medals_y&#39;]+df[&#39;Medals_z&#39;] df.sort_values(by=&#39;Medals&#39;, ascending=False) df.set_index(&#39;Country&#39;,inplace=True) print(df[[&#39;Medals&#39;]]) Where as my output is Medals Country USA 72.0 France 53.0 Russia 25.0 Germany 20.0 UK 27.0",318347
89529,"For the below pandas series &#39;pd_series1&#39; i am unable to update the indexes to 1 2 3 4 pd_series1: 0 1 1 4 2 9 3 16 dtype: int64 Dont think pd.Series(pd_series1, index = [&lsquo;a&rsquo;, &lsquo;b&rsquo;, &lsquo;c&rsquo;]) is the correct way of doing this.",316036
88895,"Most of the coding problems have Accepted/Rejected status after you submit the solution. But, I have noticed a few problems have only Submitted status. Are those special question or some bug?",312490
89900,"To fetch rows from 2 to 20, syntax i believe should be : df_2 = df.loc[2:21,] [Meaning fetch from 2 to 20] - but then why it is fetching from 2 to 21st and not matching expected output?",320103
93775,Do you know what type of encoding the file uses while reading input file? How to find that for given file?,318458
88899,"Difference in output ""df.head"" and ""df.head()"" mean? Suprisingly I found that ""df.head"" and ""df.tail"" will show same output.try without ""()"".You wont get any error,infacr not even warning. Plus You will see the whole dataframe is printed twice!!!",317993
89901,Followed stackoverflow link but still didn&#39;t get clear picture between these 2 operators. It would be really helpful if someone could explain with example.,320103
90998,nan,313228
90668,"Like. Select top 5 EmpName, Salary.",312518
89559,,318335
89925,"For above question if I am applying &#39;and&#39; operator it is throwing error but when i applied &#39;&amp;&#39; operator it executed successfully. As per my understanding as &#39;&amp;&#39; is bitwise operator and work on numbers we should use logical &#39;and&#39; operator but in this case it is behaving opposite. Could someone please explain this. Error while using &quot;and&quot; Code - df.loc[(df[&#39;area&#39;]&gt;0) and (df[&#39;wind&#39;]&gt;1) and (df[&#39;temp&#39;]&gt;15),:]",320103
90728,nan,306996
88990,nan,312518
89955,"import pandas as pd df = pd.read_csv('https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF') df_2 = df.sort_index('X', inplace = False) print(df_2.head())",316416
89965,How do we know which functions have parenthesis and which doesn&#39;t. Is there any logic behind it? e.g. df.describe() &amp; df.column,318436
89961,nan,312491
89972,"import numpy as np import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF&#39;) df_1 = df.pivot_table(index=(&#39;month&#39;,&#39;day&#39;),columns=(&#39;wind&#39;,&#39;rain&#39;),aggfunc=&#39;mean&#39;) print(df_1.head(20))",305843
89042,Medals Country France 53.0 Germany 20.0 Russia 25.0 UK 27.0 USA 72.0,314183
89044,"market_df = pd.read_csv(""../global_sales_data/market_fact.csv"") customer_df = pd.read_csv(""../global_sales_data/cust_dimen.csv"") product_df = pd.read_csv(""../global_sales_data/prod_dimen.csv"") shipping_df = pd.read_csv(""../global_sales_data/shipping_dimen.csv"") orders_df = pd.read_csv(""../global_sales_data/orders_dimen.csv"")",308495
89046,"df_1 = pd.merge(market_df, customer_df, how='inner', on='Cust_id') df_2 = pd.merge(df_1, product_df, how='inner', on='Prod_id') df_3 = pd.merge(df_2, shipping_df, how='inner', on='Ship_id') master_df = pd.merge(df_3, orders_df, how='inner', on='Ord_id') master_df.head()",308495
89836,"In the pandas module, there is a section where we sort a dataframe like this: market_df = pd.read_csv(""global_sales_data/market_fact.csv"") market_df.set_index('Ord_id',inplace=True) market_df.sort_index(axis=0,ascending = True) So I am setting the index as the ord_id and then sort it by axis 0 i.e rows. But the output I get, has somewhere in the middle Ord_101 in between Ord_1009 and Ord_1010. Also some Ord_990 are also present below Ord_1010. That doesnt look sorted there. What am I missing here? Why is it like that? How does the sort_index work?",309451
89132,"Given three data frames containing the number of gold, silver, and bronze Olympic medals won by some countries, determine the total number of medals won by each country. Note: All the three data frames don&rsquo;t have all the same countries. So, ensure you use the &lsquo;fill_value&rsquo; argument (set it to zero), to avoid getting NaN values. Also, ensure you sort the final data frame, according to the total medal count in descending order.",304692
89025,nan,319759
89138,,300690
89140,nan,313676
89154,Group the data 'df' by 'month' and 'day' and find the mean value for column 'rain' and 'wind' using the pivot table command.,314547
89113,Group the data &#39;df&#39; by &#39;month&#39; and &#39;day&#39; and find the mean value for column &#39;rain&#39; and &#39;wind&#39;.,314547
89628,"df.sort_values(by=[&#39;month&#39;,&#39;day&#39;],ascending = True) In the above format,is there a provision to sort month is decending but day in ascending?",316036
89178,,303674
90836,"I find the Video of Professor very less informative , I am sure many have raise this concern. Upgrad please provide more details and information.",311386
89633,CountryTotal_Medals 0 USA 72.0 1 France 53.0 4 UK 27.0 2 Russia 25.0 3 Germany 20.0,300718
90372,nan,315757
90375,Create a new column &#39;XY&#39; which consist of values obtained from multiplying column &#39;X&#39; and column &#39;Y&#39;.,316891
89803,,306996
89218,nan,318344
91924,"while solving basics for pandas came across a question given that: Create a series using list = [6,7,8,9,2,3,4,5] and print the output series as the square of each number in the list. below is the code that I have written: and same is provided in the solution output as well, please find below: but still my code is getting rejected and showing error i.e. Can anyone help me with what mistake am i doing in this??",317558
89815,nan,317410
90749,the code when i execute in my console i got correct answer where as same when i run at upgrade console i got error why,315455
89656,nan,308437
89828,nan,306996
89829,nan,301111
89275,"I downloaded the &quot;Market_Fact&quot; csv file and have placed in my local drive. However, while accessing the csv file using pd.read_csv command, it throws an error. Do I need to give the complete file location. I tried that as well, but it shows the same error. Can anyone help me on this please.",316202
90778,nan,300748
90784,nan,306727
90022,"Hello I executed the following commands d1 = pd.DataFrame( ... {&#39;Name&#39;:[&#39;Aman&#39;,&#39;Joy&#39;,&#39;Rashmi&#39;,&#39;Saif&#39;], ... &#39;Age&#39;:[34,31,22,33], ... &#39;Gender&#39;:[&#39;M&#39;,&#39;M&#39;,&#39;F&#39;,&#39;M&#39;]} ... ) d2 = pd.DataFrame( ... {&#39;Name&#39;:[&#39;Akhil&#39;,&#39;Asha&#39;,&#39;Preeti&#39;], ... &#39;Age&#39;:[31,22,23], ... &#39;Gender&#39;:[&#39;M&#39;,&#39;F&#39;,&#39;F&#39;]} ... ) pd.concat([d1,d2],axis=1) # Name Age Gender Name Age Gender 0 Aman 34 M Akhil 31.0 M 1 Joy 31 M Asha 22.0 F 2 Rashmi 22 F Preeti 23.0 F 3 Saif 33 M NaN NaN NaN Have you observed NaN for newly appended Columns?",300708
89293,,306011
89310,nan,312756
90019,Any idea how we can get the csv files Professor is referencing in his examples? -- Rajesh,300708
89336,"df.sort_values(by=[&#39;month&#39;,&#39;date&#39;], ascending = True) What is the issue in the above code Sample eaxple applied to other colums working fine market.sort_values(by=[&#39;Sales&#39;,&#39;Ship_id&#39;],ascending=False)",318846
89844,"In all of the videos for introduction to pandas and the associated python notebooks, numpy is being imported with pandas. Do we always need to import numpy and pandas before we can use pandas library?",319302
90793,We have learnt merging on a common attribute/column. How do we merge if the data frames have a common column logically but the column names differs say(Prduct_Id in df1 and Prd_Id in df2),316147
90354,"import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF&#39;) df_2 = df_2 = df.set_index(&#39;X&#39; , inplace = True) df_2.head() print(df_2.head())",319860
90440,"import numpy as np import pandas as pd gold = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;France&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [15, 13, 9]} ) silver = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;Germany&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [29, 20, 16]} ) bronze = pd.DataFrame({&#39;Country&#39;: [&#39;France&#39;, &#39;USA&#39;, &#39;UK&#39;], &#39;Medals&#39;: [40, 28, 27]} ) df_add=pd.concat([gold,silver,bronze],).groupby([&#39;Country&#39;]).sum() df_final=df_add.sort_values(by = ([&#39;Medals&#39;]), ascending = False) print(df_final) I am getting output:",317845
91006,nan,316218
90795,nan,300687
93930,Hi Was looking to exporting csv from pandas. below code is working fine df.to_csv('df.csv') is there any other way out?,306244
89348,"Given three data frames containing the number of gold, silver, and bronze Olympic medals won by some countries, determine the total number of medals won by each country. Note: All the three data frames don&rsquo;t have all the same countries. So, ensure you use the &lsquo;fill_value&rsquo; argument (set it to zero), to avoid getting NaN values. Also, ensure you sort the final dataframe, according to the total medal count in descending order.",318017
90115,"Throwing error please help : n = int(input()) import numpy as np import pandas as pd list1 =range(1,n+1) s = pd.Series(pd.Series(list1).apply(lambda x : x**2 ),index = list1) print(s) My Output : 1 4.0 2 9.0 3 16.0 4 NaN dtype: float64",307490
89873,,306996
90117,nan,308782
89702,"Below is the Code import numpy as np import pandas as pd gold = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;France&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [15, 13, 9]} ) silver = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;Germany&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [29, 20, 16]} ) bronze = pd.DataFrame({&#39;Country&#39;: [&#39;France&#39;, &#39;USA&#39;, &#39;UK&#39;], &#39;Medals&#39;: [40, 28, 27]} ) gold.set_index(&#39;Country&#39;, inplace=True) silver.set_index(&#39;Country&#39;, inplace=True) bronze.set_index(&#39;Country&#39;, inplace=True) df_add1 = gold.add(silver, fill_value=0) df_add2 = df_add1.add(bronze,fill_value=0) df_sort = df_add2.sort([&#39;Medals&#39;],ascending=False) print (df_sort)",316036
90268,nan,314678
90369,What is the difference between df.describe and df.describe() ? For example :- import numpy as np import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF&#39;) print(df.describe()) print(df.describe) Here what will be the respective outputs and why ?,318427
89710,"# Using booleans # This selects the rows corresponding to True market_df.iloc[[True, True, False, True, True, False, True]]",318429
89883,"market_df.set_index(&#39;ord_id&#39;,inplace=True)",305655
89420,market_df[[&#39;Sales&#39;]] gives us a dataframe. market_df[&#39;Sales&#39;] and market_df.Sales give us a series. Can someone please give a real-world example where the data frame would be preferred over series and vice-versa??,308962
90367,nan,309210
89956,"My code gold.set_index(&#39;Country&#39;) silver.set_index(&#39;Country&#39;) bronze.set_index(&#39;Country&#39;) single_table=pd.concat([gold,silver,bronze], axis=0) Final=single_table.groupby(&#39;Country&#39;).sum() print(Final.sort_values(by=&#39;Medals&#39;, ascending=False)) output i got Medals Country USA 72 France 53 UK 27 Russia 25 Germany 20 Expected output Medals Country USA 72.0 France 53.0 UK 27.0 Russia 25.0 Germany 20.0",302744
89718,So do I have to produce NaNs to verify this particular code question. Something is wrong here!!!,313515
89721,,300690
89620,"FAQ mentioned in Introduction To Pandas- Panda Basics In the 'sort_values' function, the 'ascending' argument is already 'False' by default, so why is it mentioned explicitly in the notebook? Yes, the 'ascending' argument is already 'False' by default, but it is mentioned explicitly in the notebook to give the students the idea that the 'ascending' argument exists for the said function and can be used as per your requirement in the future. This is incorrect as by default the order by is ascending i.e. ascending=True and if needs decreasing then ascending=False needs to be explicitly mentioned .",317811
90063,"Columns of a Pandas DataFrame are supposed to be Pandas Series right? So, if i run the following code it should create a column called ""id"" and this column should be a Series with the values 1 to 9. instead it creates a column ""id"" having values 1,2,3,1,2,3,1,2,3. Why does that happen? However, if i do x['id']=[1,2,3,4,5,6,7,8,9] ie, just make a list instead of Series, then it correctly creates the column ""id"" with values 1,2,3,4,5,6,7,8,9. When DataFrame columns are supposed to be Series then why does it show the expected output with a list but shows different output with a Series? Can someone please explain this funny behaviour? import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({'Country': ['USA', 'France', 'Russia'], 'Medals': [15, 13, 9]} ) silver = pd.DataFrame({'Country': ['USA', 'Germany', 'Russia'], 'Medals': [29, 20, 16]} ) bronze = pd.DataFrame({'Country': ['France', 'USA', 'UK'], 'Medals': [40, 28, 27]} ) x=gold.append([silver,bronze]) x['id']=pd.Series([1,2,3,4,5,6,7,8,9]) print(x)",317998
90732,This code could never get executed in time due to time of execution. It said &#39;Hang on&#39; first and then sorry. What is the reason?,301121
90365,"df_1 = df.groupby ([&#39;month&#39;,&#39;day&#39;]) df_1[&#39;rain&#39;,&#39;wind&#39;].mean () print(df_1.head(20)) not sure why it is erroring out. any one please help",302741
90469,nan,320688
90501,"What is wrong in this code? import pandas as pd df = pd.read_csv(&#39;https://query.data.world/s/vBDCsoHCytUSLKkLvq851k2b8JOCkF&#39;) df_2 = df.sort_index(by=[&#39;month&#39;,&#39;day&#39;]) print(df_2.head(20)) Why is it not passing the testcase?",318495
90502,What is difference b/w df.sort_index() and df.sort_values() It&#39;d be great if anyone would be able to explain it using example,318495
90512,nan,318751
91756,What steps to follow for importing an csv file,307496
90576,nan,305804
90584,"Hi Team, There is a problem in this question. Using loc function print out all the columns and rows from 2 to 20 of the &#39;df&#39; dataset. Soluttion uses loc{2:20] but inmy view it should be loc[2:21] because if we have to print numbers 2 to 20, we have to keep end range to one number higher. Please help e.g # Selecting rows using a range of integer indices # Notice that 4 is included, 8 is not market_df.loc[4:8] in similar lines it should be loc[2:21]",305650
90492,Is there any special benefit/use of apply() function when we can directly get the desired output by applying operation directly on series? Eg: series.apply( lambda x: x**2) is equivalent to series**2,318495
90064,"In the below code, how can i change the datatype of the Medals column from Int to Float? Please if someone could help import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({'Country': ['USA', 'France', 'Russia'], 'Medals': [15, 13, 9]} ) silver = pd.DataFrame({'Country': ['USA', 'Germany', 'Russia'], 'Medals': [29, 20, 16]} ) bronze = pd.DataFrame({'Country': ['France', 'USA', 'UK'], 'Medals': [40, 28, 27]} ) x=gold.append([silver,bronze]) x['id']=[1,2,3,4,5,6,7,8,9] x.set_index(""id"",inplace=True) groupby_country=x.groupby('Country') groupby_country['Medals'].apply(pd.to_numeric, errors='ignore') k=groupby_country['Medals'].sum().sort_values(ascending=False) print(k)",317998
90791,Will 0th row not considered as an even row? we consider 0 as even number. But Upgrad solution saying otherwise,317073
90667,nan,318319
90520,"e.g. df.dropna(subset=[df.columns[df.isnull().mean() &gt; .95]]) using the above, i get a key error, but it works if i first execute the command, find the column names and then specify these column names in the subset. Trying to see how to use dropna wihtout explicity naming the columns.",312491
89924,"For above question if I am applying &#39;and&#39; operator it is throwing error but when i applied &#39;&amp;&#39; operator it executed successfully. As per my understanding as &#39;&amp;&#39; is bitwise operator and work on numbers we should use logical &#39;and&#39; operator but in this case it is behaving opposite. Could someone please explain this. Error while using &quot;and&quot; Code - df.loc[(df[&#39;area&#39;]&gt;0) and (df[&#39;wind&#39;]&gt;1) and (df[&#39;temp&#39;]&gt;15),:]",320103
89339,"I have completed NumPy and just started Pandas. Apart from data visualization, what else I can utilize from Pandas ? Both have functions to manipulate the data and both can be utlized for writinting ML algorithims. What are the features which make one better over the other ? Or, do we need to use both at the same time ?",312479
90870,CAn some one please explain the queston stem. Do we need to create a list an then proceed forward or we need to create a general list rule to evaluate ?,302735
89202,"Both the syntax gives the same result irrespective of argument ""by"" is used or not. market_df.sort_values(by=['Prod_id', 'Sales'], ascending = False) market_df.sort_values(['Prod_id', 'Sales'], ascending = False)",306248
89635,"Print all the columns and the rows where &#39;area&#39; is greater than 0, &#39;wind&#39; is greater than 1 and the &#39;temp&#39; is greater than 15.",320195
90497,"import numpy as np import pandas as pd # Defining the three dataframes indicating the gold, silver, and bronze medal counts # of different countries gold = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;France&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [15, 13, 9]} ) silver = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;Germany&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [29, 20, 16]} ) bronze = pd.DataFrame({&#39;Country&#39;: [&#39;France&#39;, &#39;USA&#39;, &#39;UK&#39;], &#39;Medals&#39;: [40, 28, 27]} ) gold.set_index(&#39;Country&#39;, inplace = True) silver.set_index(&#39;Country&#39;, inplace=True) bronze.set_index(&#39;Country&#39;, inplace=True) Total = gold.add(silver, fill_value = 0) Final=Total.add(bronze, fill_value=0) Final.sort_values(by=([&#39;Medals&#39;]), ascending=False)",318427
91139,"If you are familiar with SQL, here is an interesting side by side comparison of SQL with Pandas http://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html",314511
90779,"gold = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;France&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [15, 13, 9]} ) silver = pd.DataFrame({&#39;Country&#39;: [&#39;USA&#39;, &#39;Germany&#39;, &#39;Russia&#39;], &#39;Medals&#39;: [29, 20, 16]} ) bronze = pd.DataFrame({&#39;Country&#39;: [&#39;France&#39;, &#39;USA&#39;, &#39;UK&#39;], &#39;Medals&#39;: [40, 28, 27]} ) gold.set_index(&#39;country&#39;,inplace= True) silver.set_index(&#39;country&#39;,inplace= True) bronze.set_index(&#39;country&#39;,inplace= True) total= gold.add(silver,fill_value=0).add(bronze,fill_value=0) total= total.sort_values(by=&#39;medals&#39;, ascending = False) print(total)",305841
89716,"I have the following code for creating a pivot table: pd.DataFrame(round(100*(master_bank_data.pivot_table(values=&#39;response&#39;, index = &#39;age_group&#39;, columns = &#39;marital&#39;, aggfunc=&#39;mean&#39;, fill_value=&#39;0&#39;,margins=&#39;True&#39;, margins_name=&#39;Total&#39;))),dtype=int) This is how my pivot table looks like: How do I sort this pivot table based on the &#39;Total&#39; columns? Any help is appreciated.",310511
91211,"Whenever we want to apply filter based on certain conditions, is there an option that, a method is explicitly called. For eg: Retreive all the rows where the column values are above average or calculate using different column values and filter if matches?",314084
90253,Please help me where it went wrong?,306008
90718,"why df.iloc[[1,2,3]] is not returning the rows having indices 0,1, and 2 ?",310504
90118,"I&#39;m getting my output in Jupyter notebook same as expected output provided in question but when i verify my code it says test failed!. Given three data frames containing the number of gold, silver, and bronze Olympic medals won by some countries, determine the total number of medals won by each country. My Code: gold.set_index(&#39;Country&#39;, inplace = True) silver.set_index(&#39;Country&#39;, inplace = True) bronze.set_index(&#39;Country&#39;, inplace = True) Total = gold.add(silver, fill_value = 0).add(bronze, fill_value = 0) Total.sort_values(by = &#39;Medals&#39;, ascending = False)",318328
90915,"I am doing this but not finding it efficient . Is their an easy way to do this. data_df = {'employee_id': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'], 'employee_experience': ['1', '3', '2', '2', '4', '5', '6', '8','7', '1'], 'employee_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger'], 'manager_name': ['robert', 'charley', 'charley', 'donald', 'charley', 'robert', 'donald', 'charley', 'donald','ramesh'] } df = pd.DataFrame(data_df, columns = ['employee_id', 'employee_experience', 'employee_name', 'manager_name']) print(""\n dataframe"") df['employee_experience']=df['employee_experience'].astype(float) print(df) groupkey=['manager_name'] list_columns_to_analyze=list(set(df.columns.values)-set(groupkey)) print(""\n data grouped by manager_name"") column_data_combiner=lambda x: list(x) print(df.groupby(groupkey[list_columns_to_analyze].agg(column_data_combiner))",318005
90735,"Hi guys, in Intro to Data Management -&gt; Module -&gt; Session 3 i am facing pecular situation is that after VERIFY -&gt; RUN CODE -&gt; SUBMIT the test question i am getting below said error &quot;You have reached the maximum submissions limit for this problem&quot;. I am sure for this problem i submitted only onece. Any body has any clue. Also status shows &quot;SUBMITTED&quot; where as it shoud be &quot;ACCEPTED&quot;",315455
91104,"here is the piece of code that works in one and not another. def currency_millions(val): new_valstr = (str(val).replace(',','').replace('$', '')) new_value = float(new_valstr)/1000000 return ""million $ "" + str(new_value) movies.budget.apply(currency_millions) The below output is what i get when it works 0 million $ 760.505847 1 million $ 309.404152 2 million $ 200.074175 3 million $ 448.130642 5 million $ 73.058679 The error is ----&gt; 5 new_valstr = ( str ( val ) . replace ( ',' , '' ) . replace ( '$' , '' ) ) 6 new_value = float ( new_valstr ) / 1000000 7 return ""million $ "" + str ( new_value ) TypeError : 'Series' object is not callable",312491
94457,"df = pd.DataFrame(['A0lytics','X 2.0', 'Time Ma0gement'], columns=['P']) print(df) gives output as P 0 A0lytics 1 X 2.0 2 Time Ma0gement I need to use replace function to get output as P 0 Analytics 1 X 2.0 2 Time Management Please suggest how I can go about it.",306250
94741,,300735
90326,"Guys I am doing the following for stack array question: # Read the input import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] list_3 = input_list[2] # Import NumPy import numpy as np # Write your code here array_1=np.array(list_1) array_2=np.array(list_2) array_3=np.array(list_3) array_4=np.hstack(array_1,array_2) array_5= np.vstack(array_3,array_4) print(array_5) I am getting the below error: Traceback (most recent call last): File &quot;/code/source.py3&quot;, line 18, in &lt;module&gt; array_4=np.hstack(array_1,array_2) TypeError: hstack() takes 1 positional argument but 2 were given Any Idea??",301124
89724,function = np.vectorize(lambda x: x/5) and nomally '1//2',316349
88828,What is the reason that a vectorized function takes more time than a normal function? Or this is a special case?,312490
89602,"This question is in reference to the question asked by Abhijeet and when I was trying out the code to answer to it. Whie finding the computation time of a function, a vectorised function and a vectorised lambda function I had to run the code in seperate notebooks. If I ran the code in the same Jupyter notebook the computation time changed every time I ran the code. Why is this happening ? Please refer below screenshot for reference. First Second Third Fourth Fifth Sixth",318479
88851,"Given an even integer n, create an n*n checkerboard matrix with the values 0 and 1, using the tile function. - I tried this question and got the checkerboard array, however I am not getting the correct dimensions. Any pointers? My current code: import numpy as np a= np.array((0,1)) b= np.array((1,0)) cc = np.array((a,b)) dd = np.tile(cc, (2,2)) print(dd)",300748
90438,nan,312376
90441,"array_1 = np.arange(24).reshape(2, 3, 4) array_2 = np.arange(40).reshape(2, 5, 4) np.vstack((array_1, array_2)) This code throws and error. I am asuming the 2nd and 3rd dimension should be the same and the 1st dimension can differ.",312376
90437,P.S- Please let me know I shouldn&#39;t be comparing matrix to an array and why not?,312376
88890,I have tested list and arrays both worked,312050
89528,,300690
89527,,300690
89544,"difference between array(), ndarray(), asarray() and when to use which one.",309451
88953,"import ast,sys input_str = sys.stdin.read() input_list = ast.literal_eval(input_str) list_1 = input_list[0] list_2 = input_list[1] list_3 = input_list[2] # Import NumPy import numpy as np # Write your code here array_1=np.array([list_1]) array_2=np.array([list_2]) array_3=np.array([list_3]) array_4=np.hstack((array_1,array_2)) array_5=np.vstack((array_4,array_3)) print(array_5)",310419
89923,Why my submission is showing &quot;Submitted&quot; and not &quot;Accepted&quot; for the question in Operations onArrays session. The question was simple and my code and solution provided both are same. And I did not get any error.,318554
89926,nan,300729
90242,nan,313770
89015,nan,319912
89082,nan,300687
89771,"If A = [a i j] is an m &times; n matrix and B = [b i j] is an n &times; p matrix, the product AB is an m &times; p matrix. AB = [c i j], where c i j = a i1 b 1 j + a i 2 b 2 j+ &hellip; + a in bn j.",310562
89619,"for i in range(0, int(roww)): print(i) array_x = py.tile((1,0),int(roww)) array_y = py.tile((0,1),int(roww)) f = py.vstack((array_x,array_y)) #print (f) #print (fin) fin = py.append(f, fin, axis=0)",302739
89621,nan,306996
89777,"What is missing here? a = np.hstack((list_1, list_2)) print (a) np.vstack((a, list_3)) not able to get Solution output, ERROR Here is Solution output [[1 2 3 4] [5 6 7 8]] Expected output [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]]",306244
89389,,300690
89780,f = np.vectorize(lambda x: x/(x+1)) f(a) Why we dont have to define the above as f(n) = np.vectorize(lambda x: x/(x+1)) f(a),318814
89794,Some times i observed code will take without print (output) some times it insists for print (output) while validating code. which creating confusion since we are not aware the code is incorrect or missed print. as anyone observed this ? when i am answering Module 3 -&gt; session 2 -&gt; Nongraded question why i am raising this bcoz i spent more that 10 min just to check my code many times then randomly added print() and realised,315455
90827,"I tried this code: a = np.arange(1, 10).reshape(3, 3) z = np.linalg.inv(a) y = np.dot(a, z) print(y) I get the following output: [[ 0. 1. -0.5] [ 0. 2. -1. ] [ 0. 3. 2.5]] Shouldn&#39;t I have got the identity matrix? What am I missing here?",318762
89291,nan,308432
89333,"I observed when solving the practise problem, list1, list2 and list3 need not necessarily be converted to np.array . I directly used the list in hstack and vstack and got the correct solution. I am guessing numpy automatically converts list or tuple to ndarray when used directly. Here is the solution i submitted and accepted arr1 = np.vstack((np.hstack((L1,L2)),L3)) print(arr1)",300708
91581,how is it printing the output without any print statement? i am confused. f = np.vectorize(lambda x: x/(x+1)) f(a),322683
89427,"Do we always need to invoke a Lamba function with numpy.vectorize? Or we may have ways / situations to use np.vectorize without Lamba - if yes, then can explain with examples? I Found this session too brief and is there some good study material to understand why use np.vectorize and its advantages please?",316036
91553,"in a problem statement, it is stated that - Perform an element-wise multiplication using list_1 = [2,3,4,5] list_2 = [7,8,9,6] and obtain the output as a list. Code for above problem is given as below: my question here is where have we defined elements of List_1 and List_2?? As in code it is mentioned as List_1 = input_list[0] &amp; List_2 = input_list[1]. How is program automatically considering the elements of the list??",317558
89374,nan,300748
89343,"All, In my local python installation I tried following a = np.arange(1,10).reshape(3,3) np.linalg.inv(a) it threw following error Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;/home/mia/environment/venvpy36/lib/python3.6/site-packages/numpy/linalg/linalg.py&quot;, line 528, in inv ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj) File &quot;/home/mia/environment/venvpy36/lib/python3.6/site-packages/numpy/linalg/linalg.py&quot;, line 89, in _raise_linalgerror_singular raise LinAlgError(&quot;Singular matrix&quot;) numpy.linalg.linalg.LinAlgError: Singular matrix When I checked the determinant of above matrix, it was showing 0. np.linalg.det(a) #0.0 Some googling suggested that we cannot do inverse of a singular matrix. In that case how was the professor able to execute this function in his jupytor notebook. Can you please provide me more insights into basic matrix operations and more knowledge into the linear algebra on numpy Thanks in Advance. --Rajesh",300708
89778,"input_list= ([[1, 2], [5, 6]], [[3, 4], [7, 8]], [[9, 10, 11, 12]]) list_1 = input_list[0] list_2 = input_list[1] list_3 = input_list[2] # Import NumPy import numpy as np array_1=np.hstack((list_1,list_2)) print(array_1) list_4=list(array_1) print(np.vstack((list_4,list_2))) ---- [[1 2 3 4] [5 6 7 8]] --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-18-dcb3854e1ac7&gt; in &lt;module&gt;() 10 print(array_1) 11 list_4=list(array_1) ---&gt; 12 print(np.vstack((list_4,list_2))) 13 # (Write your code here 14 arr1 = np.vstack((np.hstack((list_1,list_2)),list_3)) ~\Anaconda3\lib\site-packages\numpy\core\shape_base.py in vstack(tup) 232 233 &quot;&quot;&quot; --&gt; 234 return _nx.concatenate([atleast_2d(_m) for _m in tup], 0) 235 236 def hstack(tup): ValueError: all the input array dimensions except for the concatenation axis must match exactly",318814
91550,"In Numpy introduction trying to solve question question which states - Create a 3*3 array using list_1 = [1,2,3] list_2 = [4,5,6] list_3 = [7,8,9] code i have written is - array_1 = np.array([[1, 2, 3], [4, 5, 6], [7 ,8 ,9]]) test case is passed expected output and solution output is also same but solution is rejected with error can anyone explain me what is happening here??",317558
103721,As test and Test1 is expected to give the same results say array_1 = np.arange(12) print(test(array_1)) print(Test1(array_1)) Ans:: [[ 0 2 4 6] [ 8 10 12 14] [16 18 20 22]] [[ 0 2 4 6] [ 8 10 12 14] [16 18 20 22]] both the above results have the same data type &lt;class 'numpy.ndarray'&gt;,312259
103575,,312448
102465,THis portal is not allow uploading resume and keep showing progress status. This is the status for past one week. Any solution from TA/admin?,318454
98659,nan,318429
98719,nan,318077
102624,nan,305839
103877,nan,300694
102681,nan,310502
106808,How to plot the best fitted line after model evaluation with the variables which we found significant during regression? Equation of our best fitted line-&gt; y=mx+c,308442
106897,"when I change RFE variable count it changes their model's variables and results alot..why? In RFE , n is changed to 10,15,20 etc and their model's final variable list changes a lot.. pls explain..also will it impact the r2 scores in test?",318454
105789,nan,310508
105795,"the evaluation rubric suggests that a reasonable number of models should be built - however, its not clear which approach to follow - manual or RFE? any suggestions here?",310509
105799,"These are the available Engine Type &#39;dohc&#39;, &#39;ohcv&#39;, &#39;ohc&#39;, &#39;l&#39;, &#39;rotor&#39;, &#39;ohcf&#39;, &#39;dohcv&#39;. What are they and are they valid? Googled most of it but partial outcome is coming.",318780
106913,Features VIF 27 SUBARU inf 23 PEUGEOT inf,320635
105806,,300719
106918,nan,320635
105817,"Many numerical variables for visualization with pariplot. What is the best procedure, do we needs to take selected numerical variables ?",312019
105814,reaplace and map usage for changing the column values based on some value cars1['x']=cars1['x'].replace({'abc':'def'}) //this worked cars1['x']=cars1['x'].map({'abc':'def'}) //this made nan value. did not work Any help on this,312019
106930,Suppose we have VIF is 150 and p value is 0 with respect to one feature variable. Should we drop by considering VIF irrespective of p value or should we look the next feature variable,308638
106933,nan,317575
106894,nan,300733
105823,"<span style=""font-size:10.0pt;mso-bidi-font-size:11.0pt;font-family:&quot;Arial&quot;,sans-serif; mso-fareast-font-family:Arial""> 1. <span style=""font-size:10.0pt;mso-bidi-font-size:11.0pt;font-family:&quot;Arial&quot;,sans-serif; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi"">Too many categorical variables to handle If I create dummy for each feature, then overall no of columns in the dataset will go beyond 50... Is this fine, can I proceed in this way?",308437
105820,"mapping of the categorical variable values We should map only which variables having yes or not. Or we can map to 0, 1 if the values of that variable is only two. Is the correct approach ?",312019
106932,"The duplicate could correspond to a different model or make but while plotting data, it would cause same set of values of independent variable(x) correpsond to different price (y). Should these be dropped?",317575
106938,nan,313770
106947,"While model building, I got R squared value of train data as 0.842 and while making predictions out of the test data, I got R squared value of test data set vs the predicted data set as 0.815. My question is, is it fine to get a difference of 0.03 between these two ?",301655
105873,"In MLR chapter we sw furnishingstatus (furnished,semi-furnished,unfurnished). We made dummy variables which gave 3 columns. But we removed frunished and furnishingstatus. Not sure same applicable for all dummy variabls, example fuel type like gas/diesel case do we need to keep only one dummy_variable column ? Bit confused here .",312019
114363,"Hi Guys, I have two data sets individual files 1. Train Data Set 2. Test Data Set Both the data sets have same columns. Now while applying linear regression on the Train data set, I am doing data cleaning, bin creation and dummy variable creation and apply linerar regression model and a final model is created based on train data set. (Please note that I have not split the data set into Train and Test) Now My question is that, do I have to repeat the steps such as data cleaning, bin creation and dummy variable creation on the test data set and then apply the train model on the test data set. ? Let us know if you want any more information.",306243
105875,"Is it ok always apply scaling feature. Any special conditions to see. Because its not affecting results. ? any suggestions. Things are in different units prices, leghth, count, Any special treatment required for symboling,carname,car_id. or we can directly proceed for modeling with these three or to be removed ?",312019
105879,"Hi, As per my understanding, Geely Auto wouldn't interested to know which Car company influences the price. Rather they would concentrate on other properties. So, thinking of dropping car name from model? Can TA answer this and hint if this is right thinking?",310974
106951,nan,300733
105886,"After making dummy variables, train data, applying scaling for train data. correlation finding and to see the heatmap and finding hieght correlation is very hard. Graph is clumsy as vairbles are too many. Suggestions please.",312019
105894,"While calculating the VIF values, I am getting inf values in the VIF. RuntimeWarning: divide by zero encountered in double_scalars vif = 1. / (1. - r_squared_i) What does this mean? Can I just simple drop the variables which are appreading as inf?",304814
105905,"We have been taught to create dummy variables out of categorical variables. However in case of cylinders, which is multilevel caltegorical value, do we really have to create multiple varibales out of this categorical variable or we can simply convert the text into numbers? For e.g. cylindernumber contains four,five etc, can't we just treat these values as 4,5 instead of creating multiple (n-1) dummy varibales? What impact it would have on the model?",304814
105924,nan,311952
107063,"Making predictions using the fourth model --Getting issue at this step in Making Predictions Using the Final Model issue details are here------- ValueError Traceback (most recent call last) &lt;ipython-input-540-644afb480634&gt; in &lt;module&gt;() 1 # Making predictions using the fourth model 2 #y_train_price = lr_53.predict(X_train_lm) ----&gt; 3 y_pred_m5 = lr_53.predict(X_test_m4) ~\Anaconda3\lib\site-packages\statsmodels\base\model.py in predict(self, exog, transform, *args, **kwargs) 852 853 predict_results = self.model.predict(self.params, exog, *args, --&gt; 854 **kwargs) 855 856 if exog_index is not None and not hasattr(predict_results, ~\Anaconda3\lib\site-packages\statsmodels\regression\linear_model.py in predict(self, params, exog) 343 exog = self.exog 344 --&gt; 345 return np.dot(exog, params) 346 347 def get_distribution(self, params, scale, exog=None, dist_class=None): ValueError: shapes (62,15) and (14,) not aligned: 15 (dim 1) != 14 (dim 0)",318846
107006,Are the values for FuelSystem and Enginetype correct? or are these any data issues to be corrected? How can validate if these unique values which are coming are infact proper?,312096
107011,"I got the final set of features with low p values and low VIF. Decent R square value. When i am fitting the model in the test data i am again getting decent R square value. However , for one or two features the p value is nan and the following warning &quot;The smallest eigenvalue is 0. This might indicate that there are strong multicollinearity problems or that the design matrix is singular.&quot;. The r2_score between ytest and ypred looks good.",311254
107012,"For Car Name, there are 22 unique values.. which means 5 dummy variables should be enough. (2 power 5 = 32 which is &gt;22 . Hence, 5 binary columns are sufficient to capture 22 values) However, when i use get_dummies, it creates 22 columns. How do i convert this to only 5 columns?",312096
107015,"Suppose while initially visualising data we found that there is Multicollinearity amongst variables and as per domain knowledge also it gets proved that there is multicollinearity, So when we deal with it.? Should we deal there itself (during visualization) or should we wait till model building and VIF calculation ? TA's please help.",317991
107024,"ValueError: shapes (143,3) and (4,) not aligned: 3 (dim 1) != 4 (dim 0)",311952
107029,Can we ignore them just by dropping them out or is there any other way to solve this? please suggest on this step.,312756
107031,Bootstrapping Vs Train-test spilt as the data is not huge,302877
107035,nan,314197
107036,"I have used RFE and manual approach in dropping the variables which are not significant. In this process, we get a number of models. Is this what is expected?",312376
106015,nan,314361
107051,nan,317980
107042,What to do when the variables have high VIF(&gt;&gt;5) but are significant(p|t|&lt;0.05) Should they be dropped?If so then how and in which order?,318374
107047,"When i run multicollinearity test using ViF, i get &#39;Inf&#39; as values for few columns. What does it mean? is there something wrong with my model which has given these values? or should i just keep removing these columns one by one",312096
107048,"What should be done when I have a model where all the variables are significant p|t|&lt;0.05 but some of them have high VIF(&gt;5). More over if I drop the variables with high VIF, The adjusted r square gets reduced. What should be done? Should I retain those variables or remove them?",318374
107021,nan,314197
106961,While calculating lr2 after dropping one column and then using OLS on the current dataset its throwing me an error. ValueError: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).,314313
106572,am getting this error when i am running the command for vif unsupported operand type(s) for -: &#39;str&#39; and &#39;str&#39; what can be the problem ,314678
107003,I am trying to map the categorical variable &quot;aspiration&quot; to 0 and 1. Values are instead getting replaced by NaN values. What am I doing wrong here?,317141
106062,Totally there are 11 categ variables and 55 dummy variables created!!! Only after performing dis step we need to go to train test split right?? Any shortcut or dis is the preferred way? Bcoz at dis point I really don't feel like dropping any variable,308437
107059,"Reading thru the problem statement, Geely Auto is interested in understanding factors on which car pricing depends. Brand loyalty cannot be assessed with the data provided, as we do not have market share or sales (number of units). Leaving the car name, rest of the factors like fueltype, carbody, doornumber etc will have a bearing as these indicate the price range that might influence a buyers interest in the car. So far I have not seen anyone raise this question, if we eliminate CarName from the equation you are cutting of 22 columns from the analysis, this definetely sounds like white noise that can be reduced for the model to be more indicative of the factors influencing price",312199
106068,"Extracted the first part of car name , and drew pair plots. Also plotted box plots for categorical varibales - fueltype, aspiration,doornumber,carbody, drivewheel and enginelocation. There are too many plots and Iam at loss on how to proceed from here. Would be grateful if someone provide steps in how do i proceed from here. Thanks",319759
106069,"How to i categorise variables like carbody,enginetype,drivewheel,cylindernumber,fuelsystem? pd.get_dummies() is of no help i guess in these cases. Please help!!",314313
107061,"Lets consider a scenario. 1) Final LM created after removing the features with p&gt;0.05 one by one. 2) now VIF is checked. 3) Few features have high VIF (&gt;10) 4) these are removed one by one 5) But it is observed in the lm.summary() results that few of the remaining features p-value becomes &gt;0.05 So, do we again start removing these features based on p-value ? Or am I doing something wrong ?",318479
106076,The correlation map is very congested - how to make it readable?,319759
106080,"in the assignment there a lot of categ variables using label encoding, one can convert the values in the column to numbers then this entire column can be converted into numeric features. Is this allowed? In that way, handling categ variables with multilevels becomes easier TA pls answer",308437
106099,For few variables we see they are highly co-related. So should we still continue with model building or can we drop them before we proceed.,301114
106081,"label encoder reduces/retains no of columns but with many numbers in the column get_dummies on the other hand increases no of columns but fills the columns with only 0s and 1s First method is preferref, but am afraid if it changes the result since there the categ variables will be treated as numeric variables. TA pls guide",308437
107070,,317980
107065,nan,317980
106108,"My thought is Data of car company will not require, as we are calculating factors that affect car price. Anyone can please give more insight on this?",315423
107073,"In http://docs.w3cub.com/scikit_learn/modules/generated/sklearn.metrics.r2_score/ its mentioned that 1.0 is a good score, i am getting aroung 0.85 so what can i assume based on this?",301114
107080,nan,311861
106137,nan,315560
106138,nan,315560
107085,"My P value for a variable is less than 0.05 but the VIF is 5.38, if I remove this variable the R square is decreased by 72. Should i Remove this variable or not.",313770
106155,nan,311952
106181,"If i do model with one y and one x. plot we can draw plt.plot(x_train_lm.iloc[:, 1], -0.072888 + 1.210676*x_train_lm.iloc[:, 1], 'r') If i do model with one y with more than one x. how to draw the plot here, to see the fit line. i have one c and multiple slope. How best to do plot here. pls help",312019
106174,nan,310419
106175,nan,319056
106179,"Instead of getting n-1 Columns by Dummy Encoding, If I assign Numerical Values(0:n) for each n, it will affect my regression and thus the outcome.",317984
107103,nan,311254
106199,nan,300733
107106,nan,311254
107111,i am getting the error while creating rfe,310598
107113,nan,310179
106219,"I am manually dropping features one by one only by looking at the VIF values.... Once all VIF <5, then I use p-values to drop them... Is this correct methodology??",308437
106218,nan,310419
107138,Linear Regression - Carname,314612
107131,nan,317073
106252,nan,318576
106318,The Evaluation metric mentions one criteria as &#39;The results are on par with the best possible model on the dataset&#39;. How to think of/find such a model. Intution? Or hit &amp; trial?,311686
107165,Is it possible that r2_score for test data &gt; adjusted R-square of train data set ?,317991
107168,nan,310179
106263,nan,317460
107585,Why everytime we need to remove constants for VIF calculation ? Can anybody please explain,305650
107586,"In my case I am getting r2 score for my my model is 0.88 and when i train the model using test data, I got r2_score for my test data as 0.78? Is this the right one I m getting or there is a problem with my model",305650
107174,"Some columns like enginesize have p-value 0, but a very high VIF, 392. Some have a VIF inf for same p-value 0. Should these be deleted or kept in the final model?",318078
107175,Does RFE take into consideration multicollinearity while shortlisting the variables?,317514
107169,Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).,310179
108357,do we have to submit it in zip format or how to submit this assignment,312892
107181,"The material does not clearly explain the approach on the priority: 1. Say I have the following pvalues 0.1, 0.2, 0.3 and 0.01 for four variables and respective VIF are 50,40,30 and 100. In such case which variable should be dropped first? 2. Say I have the following pvalues 0.01, 0.02, 0.03 for three variables and respective VIF as 100, 90 and 80. In such case which variable should be dropped first?",317514
106269,"I am getting five INF values after calculating the VIF using price as the dependent column and 15 as the number of variables selected during RFE , while some other people are getting three INF values when they are calculating their VIF using 15 as the number of variables selected during RFE and price as the dependent column. Can anyone please tell me, that why is this happening and what is the reason behind this ?",301655
106280,There are many categorical variables in Dataset. If we convery each of them into Dummy variables there will be lot number of columns. Do we need to convert all categorical variables into Dummy variables?,308636
106279,"After removing values which are greater than zero in column P&gt;|t|, I have T values as negative for some features. In the video it is told that T values should be positive and higher value. So now should I remove features with T value as negative or go with VIF. I tried removing them(features with negative T value) but my F -statistic and r - square are impacting. What the right approach for better model?",301118
106285,nan,314197
106303,nan,311952
106295,nan,314197
107196,nan,308495
106292,nan,311254
107217,nan,300733
106305,With so many variables even rfe can become quite involved.,311857
106314,"if the regression equation has been determined after scaling, then what is the easiest way to practically implement it with real world data ( non scaled data ) esp. on the categorical variables?",310509
106307,"Getting the above error - when trying to build a linear regression model taking all the columns import statsmodels.api as sm X_train_lm = sm.add_constant(X_train) lr_1 = sm.OLS(y_train, X_train_lm).fit() lr_1.params",319759
106341,"The Symbolic coumn contain negative values from -3 to +3,Can it be left like that or changed to positive values starting from 0? Would the model be able to handle negative values effectively?",300698
106386,Constant in the regression model has a high vif (very high at d beginning) from the first iteration...at the end of model finalization it still has high vif. But if I delete all other variables' VIF goes high...should I retain this constant even though it has high vif??? TA pls clarify,308437
106385,nan,318579
106228,"In housing dataset example using rfe technique, const (added to line equation) had high p value...it wasn't deleted at first go...bedrooms was deleted... Later on second attempt, it was deleted and model wasn't rebuilt, instead vif was calculated and solved.... How is this??? Can anyone explain? In this assignment, whenever I delete constant (bcoz of high p value) should I rebuild model or go ahead with vif?",308437
106403,"enginelocation has only 2 values - front and rear. There are very few occurences of rear but those are very highly priced. Should we consider this variable for analysis? As in the train data, there are only 1 or 2 occurences of rear. RFE selects this variable.",304319
106415,"I have installed all python dependencies, sklearn, scipy, scikit-learn, statsmodels But whenever I am trying to do import statsmodels.api as sm I am getting error as No module name patsy found. I have installed patsy also separately.",318780
106423,I believe to use .replace() we will have to manually check the csv file for spelling errors and the use .replace() over them. However is there any other possible way which does not require manual searching. I read about fuzzy_replace() but guess it&#39;s too advanced at this point.,320690
107149,"y_train = df_train.pop(&#39;price&#39;) x_train = df_train Can anyone resolve this issue. Though same thing working fine if plot scatter plot. plt.figure(figsize = (6,6)) plt.scatter(df_train.enginesize, df_train.price)",310179
107277,"While coming up with the regression model, we are scaling the values using minmax. Now, having predicted the values on test data, how can i again get my actual price numbers?",312096
107269,"Below is the peice of code df_train,df_test=train_test_split(carprices,train_size=0.7,test_size=0.3,random_state=100) After some time, if we re-run the code then it shows like this",301115
106443,nan,314197
106414,Example: a column of country names in dataframe has similar multiple entries but some of them have spelling mistakes. How do you match these for the analysis?,320690
106458,Can there be Correlation between categorical variables? How to find them. Is it required?,318436
106461,"What is the best way to visualize multiple regression model AS tuotorial showed x_train and y_train with x_train, y_pred With scatter and .pot fine. For multiple independent variables we have multiple slopes. How to visualize these x_train(multiple) and y_train and x_train(multiple) with y_predict applying y=mx+c ggplot somthing ?? . What is the best approach",312019
105487,Can any body suggest how to handle variables which have many categorical values? Will creating so many dummy columns be a good idea? I don&#39;t think so.,311686
106499,Should i do the exploratory data analysis before creating dummy variables or the other way around ? Is it necessary to perform univariate analysis for all the variables ?,313691
106501,nan,304812
106530,"While Calculating VIF, Im getting inf as VIF value for some column? what does this mean? Should I drop such columns too ?",315423
106528,As we can't do this in case of p-values but Can we drop multiple High VIF columns at once ?,315423
106537,nan,310467
106535,nan,310467
106541,"While building a linear model , i used all the variables to start with wherein i had 69 variables and basis VIF i am eliminating varibles. The variable i am eliminating keeps coming up in the next model i build and everytime it stays 68, not sure what is the issue",344598
106511,Is outlier treatment required before model building?,304319
106519,nan,304812
106573,nan,314678
106340,"When i build the heatmap. Taking higher correlation variables with price Noted those variables. But when we do RFE , not getting many of those higher correlation variables for x_train_rfe.columns[rfe.support_] Is this correct. Not sure why VIF comes for some variables 50, 60 like. Something really wrong in modeling ? When we add those and build and making VIF , VIF value comes very high. If VIF is very high &gt;10. We should stright way drop ? eventhough correlation with prices was very high ? Please clarify .",312019
106765,"in my model I have P - value slightly higher than 0.05 P value. P value of variable is 0.058, so should I drop this variable as It's slightly higher than significant level of 0.05 ?",315423
106766,"If 3 dummy variable represents 4 categories and if we drop one during feature selection, there will be data loss..because now what 0 ,0 represents is not sure. Want to understand this.",318436
106767,"In my final model i am getting good values for p-values as well as VIF, but for const the VIF is high. If i drop the const variable the VIF goes up for the other variables. Is it normal that on dropping const the vif of other variables might go up ? I was going through the forum and saw posts where it has been said that it is not required to drop the constant variable.",318756
106768,"a. If the given categorical variables are say color of car and the available colors are red, green, yellow, blue, brown. When we convert to dummy variables, is it mandatory to assume the color can be identified by 4 colors and delete one of the column or we can keep all 5 color columns for clarity? b. In the housing example the equation of best fit line was: price=0.236area+0.202bathrooms+0.11stories+0.05mainroad+0.04guestroom+0.0876hotwaterheating+0.0682airconditioning+0.0629parking+0.0637prefarea0.0337unfurnished Let us focus on furnishing status. Does the above equation mean semifurnished and furnished are not significant parameter to determine price of house? c. If we had assumed 00 for unfurnished, would the equation have changed to include either furnished or unfurnished as part of the equation? d. Given the category set, only one of the category variable can be part of the equation or is it possible to have multiple variables within the same category set(furnished and semifurnished) be part of the equation?",317514
106777,nan,318579
106474,nan,318579
106801,What does it mean by How well those variables describe the price of a car What we have to perform in this step? Can someone please give insights on this?,315423
106803,"Hi, If constant variable is negative, do we need to drop the independent variable. When I calculated lr.parms i found it. Thanks",317410
106542,This might indicate that there are strong multicollinearity problems or that the design matrix is singular. I am getting this message at the end after OLS. What does it mean?,308435
106814,nan,300706
106819,I have labelencoder to encode few categorical variables instead of dummy variables I have ended with five predictors with permissible range of both VIF and p value How to validate the model obtained Even checked with plot between y test n y pres but not satisfied and among five predictors two of them are not so correlated as we see in heat map,308638
106827,"For the final submission, should I include all the steps that went into choosing the final set of variables for building the model? In my case, I have selected around 50+ variables and then eliminated them one by one and finally was able to build a model with 10 variables. In this case, am I expected to share the entire process of eliminating one variable after another in my final submission. TA's please confirm the expectation.",318084
106829,"In the lectures, we saw that the model that was built had AIC and BIC values were negative. Is it necessary that it has to be negative only? What is the industry standard for that? Can someone explain the significance of this variable in model evaluation?",318084
106831,There are many statistics in the model summary. But we just use a very few of them. Can someone explain what does the other stats represent and how to interpret them?,318084
106861,"Just like it was done in Linear regression python notebook, they have removed furnished column in addition to furnishing status column. Is there any particular reason why they have removed furnished column?",318429
106863,nan,318429
106440,Hi enginetype dohc 12 dohcv 1 l 12 ohc 148 ohcf 15 ohcv 13 rotor 4 dohcv has only one entry and rotor has only 4.They are less than 1% and should not affect the outcome. Should entries with dohcv and rotor be removed or kept as it is?,300698
105824,"<span style=""font-size:10.0pt;mso-bidi-font-size:11.0pt;font-family:&quot;Arial&quot;,sans-serif; mso-fareast-font-family:Arial""> 1. Question says consider only car company under the variable CARNAME. <span style=""font-size:10.0pt;mso-bidi-font-size:11.0pt;font-family:&quot;Arial&quot;,sans-serif; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi"">ShShould we consider only car name as independent variable and ignore car model (not consider it indep variable?) Can we delete car model details from dataset? <span style=""font-size:10.0pt;mso-bidi-font-size:11.0pt;font-family:&quot;Arial&quot;,sans-serif; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi"">C <span style=""font-size:10.0pt;mso-bidi-font-size:11.0pt;font-family:&quot;Arial&quot;,sans-serif; mso-bidi-font-family:&quot;Times New Roman&quot;;mso-bidi-theme-font:minor-bidi"">",308437
106828,"I came across some of the posts in discussion forum, where people are saying that if the constant term has very high VIF, then it must be dropped for sure. Here is my question: VIF is a term that indicates the level of collinearity between the variables that are used in building the model. Constant is just a number and not a variable. Not able to understand how constant can have multicollinearity with other variables. On the other hand, when I remove the constant from the model, I can see that the values of the model change completely. I can see VIF and p-value for some of the variables start increasing. Can somebody explain this contradiction?",318084
106866,"If we plot the feature value on x-axis and the associated target value on y-axis, then we get a circle. Assume that we fit a linear regression to the data, we can get infinite regression lines, passing through origin. Can we derive the formula for error? Will the error be same for every regression line passing through origin?",317070
106425,I sense that something went wrong here. RFE gives only 10 variables to support.After calculating VIF for these data almost 5 have high VIF values. How do i know what went wrong?,301114
106876,should I convert all the company names into dummy variables or can I drop the column ?,313691
106045,"CarPrice.dropna(inplace = True) new = CarPrice[&quot;CarName&quot;].str.split(&quot; &quot;, n = 1, expand = True) CarPrice[&quot;FirstName&quot;]= new[0] CarPrice[&quot;LastName&quot;]= new[1] CarPrice.drop(columns =[&quot;CarName&quot;], inplace = True) new KeyError: &#39;CarName&#39; During handling of the above exception, another exception occurred: KeyError Traceback (most recent call last) &lt;ipython-input-37-6d92840dfe12&gt; in &lt;module&gt;() 1 CarPrice.dropna(inplace = True) 2 ----&gt; 3 new = CarPrice[&quot;CarName&quot;].str.split(&quot; &quot;, n = 1, expand = True) 4 5 CarPrice[&quot;FirstName&quot;]= new[0] ~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __getitem__(self, key) 2686 return self._getitem_multilevel(key) 2687 else: -&gt; 2688 return self._getitem_column(key) 2689 2690 def _getitem_column(self, key):",303228
106881,nan,312376
107134,,318240
107129,"--------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-787-89272a28f56f&gt; in &lt;module&gt;() 1 # Making predictions ----&gt; 2 y_pred = lm.predict(X_test_new) C:\ProgramData\Anaconda3\lib\site-packages\statsmodels\base\model.py in predict(self, exog, transform, *args, **kwargs) 852 853 predict_results = self.model.predict(self.params, exog, *args, --&gt; 854 **kwargs) 855 856 if exog_index is not None and not hasattr(predict_results, C:\ProgramData\Anaconda3\lib\site-packages\statsmodels\regression\linear_model.py in predict(self, params, exog) 343 exog = self.exog 344 --&gt; 345 return np.dot(exog, params) 346 347 def get_distribution(self, params, scale, exog=None, dist_class=None): ValueError: shapes (62,6) and (5,) not aligned: 6 (dim 1) != 5 (dim 0)",318240
107176,"Hi All, can somebody quickly help here 1) i removed all the columns having p values more then .050 and the removed all the columna with VIF higher then 5 When regenrated the models again column have appeared with High P value which was earlier not appearing is it safe to remove them.",311386
110716,nan,301114
128992,nan,312376
128360,nan,308967
128414,nan,308638
127816,I've tried to understand how big data tools work in the past. All the talk informs me about how clusters can churn results that can be merged. I am eqally interested in knowing how data is sourced. A python scraper on a website will not be able to handle the velocity. What tools and techniques are used here? Specifically in the context of the analyst trying to gather the data. IoT is the opposite and easier to gather data.,308637
127966,Can someone explain what Veracity is (in Big Data Context) and how it cannot be handled by traditional Relation Databases,308962
128359,nan,308967
132767,"IllegalArgumentException: 'Data type string of column TV is not supported.\nData type string of column Radio is not supported.\nData type string of column Newspaper is not supported.\nData type string of column Sales is not supported.' I get this error while running the below code: from pyspark.ml .feature import VectorAssembler vectorAssembler = VectorAssembler(inputCols = ['TV' , 'Radio' , 'Newspaper' , 'Sales'], outputCol = 'features') vhouse_df = vectorAssembler.transform(df) vhouse_df = vhouse_df.select(['features', 'MV']) vhouse_df.show(3) Please guide",319759
99634,"In this example, we are using 150 as win amount(favorable case) and 10 is a loosing amount(unfavorable case). As stated in video, house makes profit when EV becomes negative value. Could you please share any possible combination which will help house to make money( in place of 150 and 10 rs.)",318778
97447,"It is mentioned that P(X = -150) = P(getting 4 blue balls) = 0.027 ; where is thias represented within video? or how is this computed? Throughout, the representation is P(getting 4 red balls) which is 0.133",309211
98317,"0.16 is not the probability of getting red ball once, it is the probability distribution which is based on the 75 sample records. Probability of getting one red ball is probability of such occurances/total scenarios which is nothing but 4/16 = 0.25 What am I missing here?",304814
97288,"In the video it was shown that the probabilty of X = 2 as 26/75 beacuse 26 people got 2 red balls out of 75 people. But why it was not 6/16, because there can be 6 possible combinations/permu for 2 red balls out of all 16 possible combinations as attached image below: My question will be like if we go by the shown logic, what if there were 100000 people and all got 2 red balls then would be probablity be 100000/100000 = 1 then???? please help me understand this one! much appriaciated.",304813
101244,nan,301112
98315,"well I am little confused here. There are 5 balls, and occurance of getting 0/1/2/3/4 red balls is below 0 red, 4 Blue : 1 1 red, 3 Blue : 4 2 red, 2 Blue : 6 3 red, 1 Blue : 4 4 red, 0 Blue : 1 So, the total scenarios are 16, and we are looking for the 4 red balls scenario and Probability of this is 1/16 = 6% But, as per probability distribution, for sample population of 75, probability of getting 4 red balls is 10/75 = 13% Probability distribution would vary depending upon the sample size. It could reduce or increase based on the size, but I dont think there would be too much variation. So, the question is, for this red ball game, is it profitable or loss maker?? If you consider probablity then it is profit making, but from probabilty distribution perspective, it is a Loss making",304814
98660,"As per the link provided in the same page ""Expected loss for the bank calculation formula is: EL = PD * LGD * EAD LGD = (1-Recovery Rate) Hence the calculation will yeild the results as; EL = 0.007*(1-0.2)*11.5 = 0.0644 lacks But the answer though logical in options is ""9.2 lakh""!!",316349
99097,nan,308967
98706,There are three red balls and two blue balls put in the bag then how can someone get 4 red balls !!!!,318479
98734,"IN the game , where in 4 consecutive red balls wins, Professor manually noted down all 16 possible outcomes. Is there a easy way to find total number of outcomes? Manually writing down all outcomes may not be possible for a larger sample",317996
99148,nan,301890
99359,Getting No Red Ball and No Blue Ball should be same as per probability theory ? where as in this example which is .027 : 0.133 is this depends on sampling when you made every time. If yes how to find exact probebility of getting No Red : No Blue,315455
100950,Please explain type-1 and type-2 errors with examples..,300735
99201,Asking this question because I can hardly allocate any time to this because of office commitments. Suggestions would help ease my mind.,318335
103925,"As discussed in inferential Statistics, a sample size of 30 or more can approximate to a normal behaviour for the population. So, what I conclude is a sample size of 30 is sufficient for any statistical study using CLT and Z scores to predict the behaviour of the population (no matter how big). Can someone please help me to understand the concept correctly, as I believe my understanding is wrong.",305839
106919,"while running y_train_price = lm.predict(X_train_lm) i am getting below error: shapes (10,9) and (24,) not aligned: 9 (dim 1) != 24 (dim 0) how to handel this ...",300735
99248,Please expplain how values in Z table are calculated,308636
99241,A bit confusion about Z* score finding,306734
99471,"Why we considered 1-P(Z&lt;=p) for calculating P(Z&gt;p). Why we can't directly check for P(Z&gt;p) from Z table ? If we directly check for P(Z.&gt;p), answer is nearly same as calculated for 1-P(Z&lt;=p). Please help",305650
99428,nan,308635
99566,"An event in a sample space is called independent if occurence of event 1 is not affected by occurence of event 2. Assuming there are only two events in the sample space. What will happen if we roll a non biased coin. There are two events H or T. One event does not affect other. Getting head does not affect getting tail. Therefore, H is an independent event and T is an independent event. Therfore, H or T/ H &amp; T events will also be independent. Event- Head in two successive flips. Is this event mutually exclusice?? I think the event is H&amp;H. And this can happen therefore NOT MUTUALLY EXCLUSIVE. However, getting H or getting T in one roll of a coin is MUTUALLY EXCLUSIVE. Because in one flip H &amp; T cannot occur simulataneously. Please help me with this concept.",301643
127292,Why is the probability of Green Ball Not just 5/8?,317984
99835,"In the answer, they are taking probability as 0.05? I don&#39;t understand how they are getting probability as 0.05. Can anybody please explain?",315423
96663,What is the probability that exactly 4 students are able to solve all the 3 questions? Hint : First you need to calculate the probability that a randomly selected student is able to solve all the 3 questions. (Use this nCr calculator .),303674
97558,nan,311117
97655,nan,311952
98928,nan,308673
95373,"The Picture for the Question here is not shown. So, please include it.",318435
98967,"2nd video of Prof Tricha - started with 2 blue balls &amp; 3 red balls with probability of (2/5) &amp; (3/5) for one ball each respectively. However, at 2.36 in the video, it is mentioned, probably of 1 red ball is 0.6 &amp; for 3 blue balls each is 0.4 . How is that possible ? When we had only 2 blue balls out of 5 , now how can we have 3 blue balls with probability of 0.4 each ? Can someone explain ? Am I missing something or there is a discrepancy in the video ?",319876
99301,"Hi, I understood the concept of EV, RV, PD etc but the example of getting 4 red balls does seems to be matching. Initially you have taken 3 red balls and 2 blue balls. Then how can some get 4 red balls.",301124
99052,"After all the 10 students attempted the test, she found that 7 students were able to solve the normal level question, 5 students were able to solve the intermediate level question and only 2 students were able to solve the high-level question. What was the theoretical probability that such a situation would have occurred?",319876
99339,nan,312199
99073,ANy tips to solve this one ? I am not able to crack this...The given hint in question has not helped me ... Binomial Distribution What is the probability that exactly 4 students are able to solve all the 3 questions?,310501
98700,"Can we also infer from the fact that since there is a 5% probablity of finding a defective product, therefore the expected value should simply be 5%*10=0.5 ( rather than finding the probablity of every possible combination and then finding the expected value )?",310509
103863,We have concepts like CDF - cumulative distribution function which is used for continuous variables. Then we have PDF - probability distribution function. Is PDF used for for discrete variables or can it be used for both?,301114
100100,nan,318355
98972,"While using ay type of sampling method, how can we test whether the data collected is valid or not? Faulty data can lead to faulty results.",318344
99192,nan,312518
99205,"Any tips in solving this one ? Rainfall Data It was calculated that the floods were caused by a rainfall of 2200 mm. From the given information, what would be the probability that the state receives more than 2200 mm of rainfall in this period?",310501
99204,"Any tips in cracking this one ? Rainfall Data It was calculated that the floods were caused by a rainfall of 2200 mm. From the given information, what would be the probability that the state receives more than 2200 mm of rainfall in this period?",310501
97677,nan,311117
99237,"How the Probability Density Functions graph is drawn , as in example of daily commute of the employees ? can some one explain it how the values are mapped ?",318814
99249,"Question 2 - What is the probability that the tablet selected by QC has a paracetamol level above 450 mg? In the solution , it is given that "" P(Z&gt;-3) = 1 - P(Z&lt;-3)"" Why "" P(Z&gt;-3) "" is equal to ""1 - P(Z&lt;-3)"" ?",308636
99244,"Daily commute time is continuous variable and table is as - 20-25, 25-30,30-35,35-40 and 40-45. But, how can we say that prob between 0 and 45 is 1? I think it should be 20-45 is 1.",301643
99251,"Question 3 is - What is the probability that the tablet selected by QC has a paracetamol level between 450 mg and 550 mg? In the answer, P(Z &lt; 1.8) - P(Z &gt; -2.2) = 0.9641 - 0.0139 From where values of P(Z &lt; 1.8) = 0.9641 and P(Z &gt; -2.2) = 0.0139 are taken?",308636
99258,"Hence, what is the probability that the astronomer under- or over-estimates the distance by less than 500 km?",318429
99261,nan,318429
99001,"It was calculated that the floods were caused by a rainfall of 2200 mm. From the given information, what would be the probability that the state receives more than 2200 mm of rainfall in this period?",312756
99027,Can we take the average rainfall received in August as Mean value ?,300727
96776,in this question what do you mean by underestimate or overestimate by 500 ?,303674
99099,"In the video, while calculating Z value, it was taken &micro; = 35 and &sigma; = 5. Why?",320103
98743,why cannot the error be on the negative side ( that the astronomer under estimates the distance by 2330 kms? so should nt the prbability be 1- (P(-2.33&lt;Z&lt;2.33))?,310509
99159,At what cutoff rainfall value should the current infrastructure be redesigned so that there is only a 3% chance that either similar or heavier rains are observed in the future?,314183
99405,"Like 36.6, if I get a x = 76.6, would I continue following same process? How can x = 36.6 be used as  in the 95.4% equation? And how can 2 be replaced by 2 while SE = 1, S = 10 and that SE is not . This video is just driving me crazy. https://learn.upgrad.com/v/course/208/session/18005/segment/91591",318007
99408,"During the calculation of SE for the sampling distribution, Standard Deviation of the original population is approximated as the standard deviation of the sample. Is this approach right?",310533
98436,nan,311117
98923,nan,311952
99260,How to decide the SD for 2 BHK house rents in powai? Any inputs,300727
99395,Z* is the Z score associated with 99% confidence level = 2.58. Z* is the Z score associated with 95% confidence level = 1.96.,320195
99424,"How to come to a conclusion that which confidence level(90%,95%,99%) we need to take for analysis",301641
98573,"In this video, it is shown that P( -2&lt;36.6&lt; +2)= 95.4%. In this 95.4%, how .4 arrived?",311117
99037,"Mean estimation using CLT 95% interval how it can be answer (50.11%, 50.89%) (1.96*0.2)/SQRT(10000)= 0.00392 this will be range I believe 50.49608 50.50392",311404
99331,In the video professor is taking 100 random samples of 5 from the population itself. But in actual senario we might not have the population data. So do we create the sampling distribution from the sample itself ?,318479
99063,Please explain me the output.,311004
99441,If the samle data is choosen wrong then all the work we will do is wrong. so how we can choose data points for a sample?,317073
99106,nan,315471
98181,How is X-X(bar) calculated?,309211
99284,"Since many people have some doubts regarding how the Z* is calculated for confidence intervals and how they're different, here is a short summary that explains both the points. First, let's see the difference between the two. We'll be using the same confidence level of 95.4% as explained in the video. If you find the Z-value and Z* for 95.4% probability, both will yield different results as shown in the figure. As you can see in the above image, even though the area covered by both amounts to a 95.4% probability, the calculated Z-values/Z* don't match since the intervals are different. Z-value or Z-score uses the cumulative probability and finds z such that P(Z&lt;z)=95.4% which in this case comes out to be 1.685. Z* is calculated a bit differently that P[-Z*&lt;X&lt;Z*] = 95.4% and in this case Z* comes out to be about 2. Thus there is a clear difference between the two. So you just can't go and check the confidence level % in the Z-table and report back the value as it would give the wrong answer of 1.685 instead of 2 in this case. And this is where you need to find the cumulative probability associated with the given confidence level and find that probability's corresponding Z-score which would be this confidence level's Z*. For example, let's say the confidence level is y%. Now check the image below. By using the symmetry of the normal distribution, we see that the Z* associated with a y% confidence level is equal to the Z-score associated with the cumulative probability of ((100-y)/2 +y )%. Hence you need to just calculate the previously mentioned value and report back its Z-score. It would be the same as the Z* for the confidence level being asked. We can verify this by taking y=95.4 Thus the corresponding cumulative probability would be (100-95.4)/2 + 95.4 = 97.7%. If you check the tables for Z-value corresponding to 0.977 probability you'll get the value of z as 2. Thus 2 is the Z* for the original confidence level",313517
99169,"in the lecture video it was explained that the 95.4% probability is also the confidence level. Whereas in the FAQ section, while explaining Z*, it was explained that first the confidence level needs to be converted to probability. Example shared was that y=95% confidence level, probability is 0.975 or 97.5%. Thus the Conf. level is not equal to probability and here seems to be a conflict beltween these 2 statements...what&#39;s the correct interppretation?",310509
99170,"In the example, it was asked to find the probability of average being within 2 mins. However, in the actual calculation, this 2 mins was interpreted as 2 sigma or 2 Std Deviations which seems confusing. If the question is in mins. then the Z score should be first calculated using a gap of 2 mins on both sides resulting in -0.2 and +0.2 Z which then converted to probability gives approx 16%. Is my interpretation correct?",310509
99179,if one party 95% confidence interval is 50% -60% and other party 95% confidence interval is 40% - 50%.,310419
99088,"In the video at 1:42, the Prof. gives the formula for Sampling Distribution&#39;s standard deviation(also known as Sampling Error S.E) as S.E = &sigma; / &radic;n i.e., sigma/Sqrt(n) where &sigma; --&gt; Std. Dev. of Population and n --&gt; Sample Size She further goes on to tell that the value of &sigma; is 0.99. I have 2 questions regarding this: 1. From where did the value of &sigma; = 0.99 come from? Is it from the previous module(already known)? Or am I missing something?? 2. In real world scenario, if we are doing the calculation by using a sample, then we will not be knowing the standard deviation of the population mean. Then, how can we apply this formula to find the Standard Error?? Link for page containng the video: https://learn.upgrad.com/v/course/208/session/18005/segment/91596",313826
99340,"Let&rsquo;s say you actually took some more samples, each of size 100, and made a sampling distribution for X, the proportion of people that voted for BJP. The mean of this sampling distribution is &mu; &macr; X = 0.50 and the standard error = 0.052. So, for the whole survey population, i.e. all the voters in ward 75N, the proportion of people that voted for BJP is approximately equal to: Can someone please explain what is being asked in this question, not able to comprehend from the language.",310210
105880,nan,311803
107046,I am trying to use scaler.transform for the test data set and I'm applying it on all the variables that I applied it for the train data set however it is throwing this error. Does anyone know what's going wrong?,316416
106102,nan,305842
106104,"It is mentioned that, ""So in the case of multiple linear regression, you should always look at the adjusted R-squared value in order to keep redundant variables out from your regression model."". The formulae for Adjusted Rsquare just have N- no of samples, and p- No predictor variable. How is this formulae able to determine which parameters are redundant and not.???( As per formulae , irrespective of the redundancy/multicolleniarity p and N will be same for all varaiables)",305842
106114,"Let assume the following regression model developed- Y = 0.5 *X1+2*X2+50.In developing the model we have scaled down the variable X1.Sensitivity calculated based on regression regarding X1 is based on the scaled down value. When we want to predict the value of Y based on the above equation , how do we input the value of X1 in the model - Do we scale down the value of X1 or input the value of X1 as it is in the model .If we want to scale down the value of X1 how do we do the same",310629
106115,Is it needed to scale down the values of the variable. Will the model not be correct if we donot scale down the values and built the model based on the data format or size available. What is the risk of not scaling down the variables.,310629
106142,"The session is on linear regression, I don&#39;t get the term Overfitting in this context. Even if there are many variables explaining the output (Y label - Sales), they are considered in linear equation i.e in Y = c + mX format only. I don&#39;t see we use the equation with different orders of X (squared, cubed etc..) to compensate the variation (curve fitting).",318404
106184,"In the course content of modeling, there is very limited univariate/bivariate analysis done and has been mentioned that we do it to get a ""fee"" of data. How is this useful to modeling? How are we validating the model using the analysis? Or it doesn't help at all?",310974
104985,Is it just the number of points plotted or increased data points for training ? how are we generating the polynomial graph?,304027
105010,Why use VIF when R-Squared pretty much conveys the same. For example if R-squared is 0.9 that effectively means VIF is 10. Why have another metric?,304022
107142,nan,317073
106284,,314197
105205,Calculating Adjusted R-Squared https://learn.upgrad.com/v/course/208/session/24636/segment/126228 Seems answers not correctly listed .is tihs true ?,312019
106402,Can all the variables with VIF value as infinity be dropped?,319759
105226,"In the example explained for gender male and female, we need n variables i.e 2 in this case.Here n-1 doesnt hold good because we have 2 variables male and female.",301114
105235,nan,310179
105236,nan,310179
105301,"Example- 3 categorical variable of 3,2,4 levels respectively. then how many dummy variable should use for building the model?",310419
105353,"As per Multiple reg Question 1 , it says the if we wil add two or more vaiables then R Sq will either increase or remain same . My point is it will depend on what R Sq value vaiable u will add . In this case If we will add newspaper R sq value then it will reduce total R sq value . It seems Question is wrong",319969
106452,Based in vif and p vaues - when should we stop dropping the variables?,319759
106460,"Unable to drop numerical varibales like '1' , '3' etc. It throws up error - "" '1' not found int axis."" Able to do the same for other variables",319759
106483,Multicollinearity does not affect goodness-of-fit statistics. This means R square? What are other statistics ?,301643
107896,"First run the regression on the train data(say 70% of available data).say u have 10 indepedent variables(x(i) for test data---i-&gt;1 to 10) and one dependent variable(y_train).after running the linear regression model(say finalized model), you will be geting the 10 co-eefients related to independent varaibles and one constant.(say LM_1) Using these values you will calculate the y_train_predicted. Basically u will compare the y_train ans y_train_predicted values for our model fit.(say accuary or R-square) My question is that why cant we run a simple linear regression such that my y_train_predicted will become my independent variabale and y_train will become my dependent variable. If i perform this I will get two co-efficienst (one for y_train_predicted and one for constant)(say this model as LM_2) So, now i will run LM_1 on my x_test data and will be getting the y_test_predicted_lm_1. Now, i run the LM_2 to get the y_predicted_lm_2. So for my models accuary i wil get the r-sqare bw the y_test and y_test_predicted_lm_2.",317993
105385,nan,312093
105516,Can we do use logerthemic values for all variables... as this gives a better scaling for values? What are th other scaling methods which we can sue.,301113
105557,nan,319951
105582,"I don't completely understand the logic behind taking n-1 dummy variables. In the specific case of furnishing status, we dropped the dummy variable furnished. By looking at the values, we can tell if both are 0,0 it is furnished. But how does the model know and are we not losing valuable information here? Furnished column could be a very good indicator right? How is this derived from having the other two?",310974
105613,At which stage ideally VIF should be used? Can it be used even before creating dummy variables? Or before fitting the first model? Or after fitting first model and looking its statistics as is done in session videos/codes ?_x007f_,311686
105674,nan,310533
106802,"In the step - making predictions using final model - i assign all the final varibables to num_vars and do df_test[num_vars] = scaler.transform(df_test[num_vars]) I get the error - ValueError: operands could not be broadcast together with shapes (62,10) (14,) (62,10) Please let me know how to proceed",319759
106810,nan,314629
106822,what should be the approach here for handling variables which looks like numerical variables but mentioned in data as categorical variables.,313767
115908,"Hi, I need to learn how to write my own algorithm to build model. So for starting step I need to learn how can we proceed from it. Could anyone help me with some useful links to make progress. Thanks,",318319
122877,nan,312623
114072,"Hi, If 2 vriables have high VIF, can we create new variable using these two. If not what type of variables need to choosen for creating new variable .",317410
122876,nan,312623
122878,nan,312623
115622,nan,304025
114921,nan,310179
115339,nan,308495
134141,"Please see the attached snippet, where the correlation between the dependent variable Price and sqft_living is high. Do we need to normalize this? Unable to understand the comment, how Normalizing using log will help in modeling. What absolute no of the regression model it is talking about?",315028
105313,"There appears an error when python program of Simple Linear Regression is executed which has reshape (-1,1) X_train_lm = X_train_lm.reshape(-1,1) X_test_lm = X_test_lm.reshape(-1,1) AttributeError Traceback (most recent call last) &lt;ipython-input-93-62797947e7f2&gt; in &lt;module&gt;() ----&gt; 1 X_train_lm = X_train_lm.reshape(-1,1) 2 X_test_lm = X_test_lm.reshape(-1,1) ~\Anaconda3\lib\site-packages\pandas\core\generic.py in __getattr__(self, name) 4370 if self._info_axis._can_hold_identifiers_and_holds_name(name): 4371 return self[name] -&gt; 4372 return object.__getattribute__(self, name) 4373 4374 def __setattr__(self, name, value): AttributeError: 'Series' object has no attribute 'reshape'",301121
105796,Can someone give some example on calculating the t-score based on simple linear equation? How we will get the standard error in this case. e t-score,315028
105464,nan,308774
106038,nan,305655
104828,How does &beta;1 = 0 signify that &beta;1 is not significant? Isnt &beta;1 the slope of the line? &beta;1 = 0 would mean the slope is 0.ie the line is horizontal.,310511
104830,"How is the mean point coming to zero, according to the Null hypothesis? Null hypothesis says Null Hypothesis (H0) :&beta;1=0. How is &beta;1 linked to the mean point or population mean?",310511
104854,Error terms are independent of each other- what does this mean? We just said that all the error terms are same as the variance is same across x values,304027
104878,nan,301114
105493,"No google answers please, I'd appreciate the response only from someone who has really worked and know the significance of the parameter ""random_state"" within train_test_split. [ or leave it to TA's who have worked on exploring this parameter ] I see that training video in https://learn.upgrad.com/v/course/208/session/24635/segment/126217 has random_state value of ""100"" 1) what does the parameter signify ? 2) does this variable accept any value? or what is the signficance of 100 there? https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html shows this parameter as 42",309211
104911,"In https://learn.upgrad.com/v/course/208/session/24635/segment/126217, value of R-Squared is 0.153 however the Prob(F-statistic) is 5.93e-56 (as shown in the table) which is a practically zero value [ This means the model&#39;s overall fit is significant] How is that possible? Usually the model whose fit is significant will have higher value of R-Squared.",309211
104983,"Both values indicate m, for linearly related variables. slope 1 indicates high correlation and coefficent 1. is there any other derivatiofor this? How is this related to scaling? I think it is irresepective of scaling PS: This is discussed under &quot;Additional Links - Linear Regression&quot; here: https://learn.upgrad.com/v/course/208/session/25297/segment/126220, video 2",304027
105000,"Here the residual analysis showed us that X had a little influence on error values. But considering that the model is fitted automatically using statsmodel or sklearn how do we fit a better model. Use other techniques Multiple regression? try different Train, Test sets?",304022
105015,Didn't understand how this t-value came up: The value should be 27.25 = (0.0545-0)/0.002 right?,310974
105025,"Please help to solve this error AttributeError: 'Series' object has no attribute 'reshape' from sklearn.model_selection import train_test_split x_train_lm,x_test_lm,y_train_lm,y_test_lm=train_test_split(x,y,train_size=0.7,test_size=0.3,random_state=100) # linear regression from sklearn expects 2d array x_train_lm=x_train_lm.reshape(-1,1) x_test_lm=x_test_lm.reshape(-1,1) AttributeError: 'Series' object has no attribute 'reshape'",312019
105036,============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ const 6.9487 0.385 18.068 0.000 6.188 7.709 TV 0.0545 0.002 24.722 0.000 0.050 0.059,317987
105049,nan,301110
105032,"While running the command for reshaping the train data set into a 2-D array in the Python notebook provided for the session on simple linear regression, I&#39;m getting the following error: Any specific reason this is occuring, because there&#39;s no such problem shown to occur in the lecture video. All the previous commands have been run properly.",310505
105092,"In Video 2 while explaining the summary statistics . It stated that T value = coeff/ Standard Error But Example shown in that video has Tv Coeff = 0.0545 , Standard Error = 0.002 T value = 0.0545/0.002 = 27.25 But in summary statistic it has : 24.722 Please explain.",301108
105137,nan,310419
105171,nan,300733
105150,"Hi , As per the given screenshot of Summary Statistics. It states that t values calculates is 24.722. With this T score i am not able to find in P(t) in T table. Can you guys help me on how to explain &amp; calculate below three fields p &gt; |t| , [0.025 , 0.975]",301108
105038,"It is theoretically explained throughout the videos and certain decisions are being upon observation of plots, however, is there any numerical quantity that will say that okay, this model is x% good fit? Do we have any chart to see that if x% good fit is enough or we can do better?",317987
105229,nan,310179
105161,I didn't understand what really train_test_split fucntion is. could anyone give me some information what actually is it?,312756
105228,nan,310179
105231,hello all. it is unclear to me as to what are tue error terms of x and y as i did not get any clear explanation for this. kindly advise.,310217
105308,nan,312093
105302,I am not clear what exactly is the dataset generated by running the x_train.head() and y_trail.head() function...what are the 2 columns?,310509
105305,"In the example it is shown that while the model is significant because of high F-statistic, but still since the R-squared value is low, it does not explain much of the variance. Intuititively, am not able to understand this - how can a &quot;significant&quot; model not explain the variance? what is the best way to interpret this?",310509
105319,"I understood that the statsmodel library fits a line that passes through the origin. But in order to have an intercept, we need to manually use the add_constant attribute. Can someone explain this practically. Like, why does it need to have an intercept? If not, how will it affect the result?",318004
105346,"Since we talk only about error between predicted and actual values, from where did we get the error distribution ?",313691
105381,nan,304397
105377,nan,308434
105412,"I could not understand how a normal distribution curve is drawn at each error term e65, e90. If X is fixed, are we changing b0 and b1 to get different error terms? Also how can mean be 0 in each of these normal distributions curves?",318007
105396,"&quot;By default, the statsmodels library fits a line on the dataset which passes through the origin. But in order to have an intercept, you need to manually use the add_constant attribute of statsmodels .&quot; Now if we want to add the intercept then why we are adding constant to the X_train?We should be adding it to the y_train by mathematical logic,right? Or it&#39;s just that&#39;s how the function in the statsmodels is written and we have to adhere to that?",318386
106490,nan,320687
105542,nan,308673
105531,I can't find the Associated T-Table. The one Given in sheet is with respect to Degree of Freedom. And there is no mention of Degree of Freedom or number of Samples while testing of Hypothesis of beta1.,317984
105568,"This sentence after at the end &quot;if the p-value turns out to be less than 0.05, you can reject the null hypothesis and state that &beta;1 is indeed significant&quot; shouldn&#39;t it be you fail to reject the null hypothesis since the p-value is less than 0.05(almost close to 0). What will happen if the Pvalue is greater than 0.05?",315436
105585,nan,305804
105604,What proportion of the data should be taken as test data?,300721
105607,nan,300721
105622,R-squared is about 0.816 which means that the model is able to explain 81.6% of the variance which is pretty good. How exactly can this be thought of? I am unable to understand what this actually means. I do understand that higer R-squared is desirable as this means low RSS but variance is something I do not understand.,318079
105643,How is the plot between X and Error terms relevant to the question of model having predictive power or it is just that fit was by chance? Can someone please explain the concept behind it.,318079
111126,nan,318082
126734,"In Linear regression, how does t statastics help in determining if the coefficient is significant..",315028
115366,nan,300733
126755,One of the Interview question I encountered.,300721
114966,nan,308495
115365,so here variance means how well actual values are predicted correctly? is it correct?,300733
114468,nan,317073
114900,nan,310179
126754,Can anyone help me with one of the interview question I encountered .As pe my knowledge Error terms are always normally distributed around its zero mean ( Histogram like graph ) Which was not the satisfactory for the panel.,300721
105884,nan,300735
107014,"What is the problem if we dont drop first value while creating dummy variables, sometimes there can be more dummy variables like 20 or 30 and while performing analysis some of them might be removed because of dependency , and because of this the basic use of drop_first will be removed.",317073
107016,nan,305655
105885,nan,303666
107109,nan,320683
107137,nan,310179
107173,nan,314629
107152,"(I hope I am posting in the right section - can;t see the whole title of the category I am posting to) This is a thought based on our steps in the assignment for regression analysis. The basic flow I am find is: (1) perform RFE for n variables (2) go through OLS first and get rid of all insignificant features - each time a feature is dropped, re-generate the model (3) once all features are significant, check for multi-collinearity using VIF - drop features until VIF &lt; 5; each time a feature is dropped go back to step (2) above (4) do residual analysis and check that the model meets the three required conditions And ofcourse we can have LOTs of different models for the same data based on the choices we take along Steps 1-4 above. E.g with one set of choices I had 5 variables with a r-score of 0.81 with test data; with another set of choices I had 9 variable with r-score of 0.88. Is there not a python method/package whereby we can automate steps 1-4 and perform them on all possible models to come up with the best model?",300694
107203,"Getting the following error when i try to use transform on df_test[num_vars] = scaler.transform(df_test[num_vars]) alueError Traceback (most recent call last) &lt;ipython-input-472-358a27d8d27b&gt; in &lt;module&gt; () 1 num_vars = [ 'horsepower' , 'fwd' ] 2 ----&gt; 3 df_test [ num_vars ] = scaler . transform ( df_test [ num_vars ] ) ~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py in transform (self, X) 367 X = check_array ( X , copy = self . copy , dtype = FLOAT_DTYPES ) 368 --&gt; 369 X *= self . scale_ 370 X += self . min_ 371 return X ValueError : operands could not be broadcast together with shapes (62,2) (16,) (62,2)",312491
107229,nan,310179
106364,"I built a model using rfe module with 19 significant variables. All variables having p value much lesser than 0.05. After this while I started checking multicollinearity amongst variables and removed first variable basis high VIF, some of the variables from earlier 19 variables have became now insignificant. How this can be possible and what is the solution for this.",313767
107237,"Plots with continuous variables give an indication of the direction of correlation. But with categorical variables, sometimes, the data is lined-up for few categories or is dispersed to give no idea. How to analyze such data?",318078
106807,"Iam tring to eliminate varibales using RFE - and I encounter the following error- please help : # First extract the target variable which is our House prices Y = df.price.values # Drop price from the house dataframe and create a matrix out of the house data df = df.drop(['price'], axis=1) X = df.as_matrix() # Store the column/feature names into a list ""colnames"" colnames = df.columns # Construct our Linear Regression model lr = LinearRegression(normalize=True) lr.fit(X,Y) #stop the search when only the last feature is left rfe = RFE(lr, n_features_to_select=1, verbose =3 ) ValueError: could not convert string to float: 'mpfi'",319759
106429,nan,310179
106430,nan,310179
106432,nan,310179
105318,nan,318455
105359,nan,310419
105389,nan,308962
105436,"More number of entries (rows in a dataset) would presumably improve the predictive power of a model. But is there a gold standard, as in how many data points are considered ideal?",304022
105431,Can someone suggest why my tests are failing ? when both solution output and expected outputs are matiching,306011
105405,"&quot;Adding two more variables has improved the model. From an adjusted R-squared value of 28%, it has moved to 50%.&quot; I did not get how the percentage (28 % and 50% ) is calculated ?",312479
105476,"In our case while creating dummy variable, we had eliminated &#39;Furnished&#39; category. As a result it doesn&#39;t feature in the Heat Map. How do we see the correlation of this variable on the target variable?",304022
105499,"the example of encoding of dummy variables in the blood group example is not clear - what is the bsis of taking those specific dummy codes A-10, B-01, AB-11, O -00. are these being used as binary codes within the dummy codes?",310509
108666,,320636
105537,Question is not getting clear. get_dummies will get us the 3 dummy column and drop_first will remove the base column. Why dropping 2 levels? Is there any parameter we can add to consider particular column value? Or we have to drop these two values?,315028
105372,"When using heatmap we get many co-related values for different variables taken in the heatmap. Is there a cutoff like if we have some co-related value as 0.6 and above we take it as highly co-related values? Because on this we can interpret few things when we try to build the model. And not considering some of the values may lead to in accuracy. In the video on heatmap even the values like 0.31,0.45 are considered as having strong co-relation.",301114
105656,In lectures it is advised to remove one feature during creartion of a model. Why we need to do so? Dosent it inmpact analysis.. What if the deleted feature was an important feature?. How we decide which feature to delete?,301641
105664,I can&#39;t post the whole code but the below code works: df[[&#39;Response&#39;]] = df[[&#39;Response&#39;]].apply(binary_map) while df[&#39;Response&#39;] = df[&#39;Response&#39;].apply(binary_map) gives error &#39;str does not have a attribute map,308440
105670,Since constant is having higher p Value thus can&#39;t we say contsant is also insignificant.,313767
106786,nan,317980
105592,nan,318455
106514,Can we drop variables which are negatively or leaser correlated with price based on coorelation map ?,319759
106811,nan,303666
106824,it has been mentioned in many books that keep you model simple and try yo keep number of variables lesser. I understand we should skip variables which are not significant but still couldn't understand why need to lessen the number of variables and remove variables having multicollinearity problem. anyhow variables having high VIF are also explaining the variables to some extent anf contributing to adjusted r square.,313767
107163,nan,310179
118945,"Here is the scenario - scalling affects the coefficients but leaves the metrics unchanged. Now, the coefficients are values that determine the affect of a certain predictor on the output variable. So my question is - if scaling the dataset will somehow skew the coefficient then is it not incorrect to report the morphed coefficients as the affect of the feature? Eg&gt; Please see the original (x,y) and min-max scaled (x`,y') x y x` y` 0 3 0.00 0.00 10 23 0.01 0.01 20 43 0.02 0.02 50 103 0.05 0.05 100 203 0.10 0.10 500 1003 0.50 0.50 1000 2003 1.00 1.00 The x,y equation is y=2x+3, where as the x`,y` eqn is y`=x` So in the model, the coefficient of x' is 1 which is reported to the end user, but in fact in the original data, x has a coefficient of 2. So, hence I ask, is ""unscaling"" of the coefficient required?",305839
104698,"I need some insights on it, from what I know if a relation between variable is strong then it&#39;s correlation should be positive then why it says, it must be negative.",315423
104693,Why professor is calling there is one independent variable and one dependent variable in simple linear regression? as both are dependent on each other and straight line just proves there linear dependency.,320685
104692,There are lot of machine learning algorithm but what is the use case when linear regression is best.,320685
104709,how to add the &#39;y=mx+c&#39; equation on the trend line on the scatter plot in Excel,308962
104718,nan,317984
104729,nan,317984
104820,"The independent variable X from a linear regression is measured in miles. If you convert it to kilometres (keeping the unit of the dependent Y variable same), how would the slope coefficient change? (1 mile = 1.6 km)",316036
104821,nan,318353
104866,nan,312448
104872,"I hv current yr ,and last 4 yr sales data, then how to get RSS, next year predicted sales",319969
104908,nan,310503
104909,nan,310503
104903,I am getting confused on the statement made in the video referring RSS as absolute and TSS as relative. Can someone throw light on this?,318372
104952,"What is the use of RSE, After calculating RSE what we have to do with it. This is not clear for me. Request some one to clarify it.",301113
106129,nan,318404
104949,nan,312376
104989,nan,308774
105031,nan,305847
105048,"Got a doubt when looking into simple regression error calculation. Why do we have to square the errors. I know that without the squares the error numbers may cancel each other, but is that the reason behind squaring the errors.",301114
105152,nan,305650
105155,nan,305650
105259,"Without knowing what data is present in a dataset, how will you be able to cluster data and later identify the field that the cluster represents? .How to differentiate between the two? How to identify when to use which?If it is similar to classification, doesn&#39;t it make both classification and clustering the same? I&#39;m confused. Someone please help me out.",304389
105272,nan,308673
105290,nan,308673
108713,nan,311466
105485,nan,305804
105588,nan,305842
105504,(A classification problem can be Solved using Logistic Regression ),317984
106405,"# Deriving &quot;days since the show started&quot; from datetime import datetime d0 = date(2017, 2, 28) d1 = media.Date.datetime.now().date() delta = d1 - d0 media[&#39;day&#39;]= delta Error unsupported operand type(s) for -: &#39;DatetimeIndex&#39; and &#39;datetime.date&#39;",305804
105921,"Below error sometime comes in the OLS Regression Result ( has also appeared in the Media case study shared by expert Ms Mitra) The condition number is large, 9.26e+03. This might indicate that there are strong multicollinearity or other numerical problems. Can we ignore such errors? According to me NO because it means that there is strong multi collinearity in the model which needs to be looked into. If this is the case then why this was ignored in the presented Medica case study? If yes, then why?",304814
105973,"Can any help me choose between these two? When to use these Scaling techniques. The definition is clear, just need some light on when to choose one over the other. Scenarios. Thanks",315028
106121,nan,318404
106182,"1. Is linear regression performed on sample data or population data ? 2. If it is performed on sample data, then should the population data remain with in the extreme values of the sample data for good prediction (and also to avoid the problem of extrapolation) ?",313691
108909,"In case i have the following sample data Independent1 Independent2 Independent3 Dependent1 Dependent2 Dependent3 100 102 104 50 55 66 101 104 105 60 65 86 103 105 106 70 75 46 104 107 107 80 85 36 105 108 108 90 95 76 Based on Independent1,Independent2, Independent3 we get some dependent value Dependent1. Similarly based on Independent1,Independent2, Independent3 we get some dependent value Dependent2 and then Dependent3.In this case how do we use linear regression? To start off i can delete Dependent2 and Dependent3 and do multiple linear regression analysis for only Dependent1. But what if i need to analyze all the Dependent variables at one go can it be done?",301114
115615,nan,310533
105523,"I hv one problem . While making Plastic sheet , The sheet breaks now there are n no. of backend end issues e.g. approx different more than 8 no. Raw material ingredients , extrusions system , melting of plastic sysetm , cooling of plastic , stretching of plastic sheet , thickness uniformity period sheet can break , machine break down etc . these are some broader leavel of system . Not only these there are small parameteric level variation also involved . Now how can I use this regression tools or is there any other tools to make one standered model to check which system has to check first during any plastic sheet breakage",319969
105336,Needs probing and explanation on the abstract terms in this lecture,304027
105383,"d0 = date(2017, 2, 28) d1 = media.Date delta = d1 - d0 error is TypeError: unsupported operand type(s) for -: &#39;DatetimeIndex&#39; and &#39;datetime.date&#39;",317156
105380,"media = media.drop(&#39;Unnamed: 7&#39;,axis = 1)",317156
105565,"In the media MLR example, SME is refereing R-Squared along with Adjusted R-Squared. If I understand correctly, R-Squared is not much useful when we use MLR ? Am I wrong ? Or do we need to consider both in MLR always ?",312479
105639,nan,311117
105678,"Hi All, Can you please suggest the basis of the below function used in the media case study to create weekend variable. # create Weekend variable, with value 1 at weekends and 0 at weekdays def cond(i): if i % 7 == 5: return 1 elif i % 7 == 4: return 1 else :return 0 return i Either the logic looks incorrect or I am not able to interpret the same. Kindly suggest. With warm regards, Amit",311729
105696,It says &quot;Character A&rsquo;s absence and presence create a significant change in show viewership. Character A&rsquo;s presence brings viewers to the show.&quot; But the coefficient of Character A variable is negative. Doesnt it mean Character A adversely affects the viewership?,310511
105692,"In the section on Interpolation, while fitting a regression line on data from 0 to P2, the curve is non-linear.(Blue line- D2P2).",318334
114990,Is election poll (prediction) a linear regression problem or classification problem?,317987
136584,nan,318598
116475,nan,310419
116494,e is a normally distributed noise with mean zero and variance 1.,311466
116025,nan,313526
116469,nan,300687
115820,Option 1 : How to choose models so as to achieve the highest level of accuracy Accuracy of what? Accuracy of predcistions or accuracy of given data!. If it is accuracy of prediction which obviously apply on test data then it might be correct one. Option3 : How to extrapolate learnings from a finite amount of data to explain or predict all possible inputs of the same kind What it means by &quot; all possible inputs of the same kind &quot; statement ? I think it should be--&gt; &quot;predict outcomes from all possible inputs of the same kind&quot;. Kindly advise !!,318770
116188,What is the difference between parameters and hyperparameters?,319759
116427,There is a mention of &quot;Class of models&quot; in this session. I am a bit confused upon how a learning algorithm can produce a class of models. My understanding is that it produces one model depending on the algorithm we implement. Does they mean implementing different algorithms and choosing the best model from this class.,314730
116271,nan,310419
123574,It is being told in the lectures that only Logistic Regression is able to provide the probabilities of a label being part of the class. But random forest also has a predict_proba method which gives the probability of the classes. Is it something different compared to the probability offered by Logistic Regression ?,318756
116448,"Is it possible to fit a polynomial to the data with more than one variable? If not then what is the reason behind it? If yes, how is the curve is represented in expression ?",316147
116454,Why is the VIF part ignored in the python code demonstration of the module Model Selection? They have used RFE to select best variables but Ignored VIF? Any Ideas for why so?,311032
116669,nan,306731
116457,"As going through, the content, suddenly came across this statement that ""Number of features is the hyperparameter in linear regression"". Checked on Data Science stackExchange as well as shown below, the things seems a bit different. Can someone put more light on this ? Thanks in advance!",318355
115855,Is there any thumb rule to select k for K-fold cross-validation. Does the value of optimal features depend on value of K?,318344
115935,nan,314313
115915,,318732
115947,"In ""Cross-Validation: Motivation"" python exercise, why didn't we use ""transform"" method on test data as we had been doing earlier. Why ""fit_transform"" is used with the test data here?",311729
115941,Why is the k-fold cross validation done on the training sets only in the python notebook example? Isnt it supposed tobe done on the entire dataset. My understanding was the dynamic test-train split is implicit in the k-fold process.,310511
125798,nan,318455
116020,nan,320687
116027,"Ratio for Train,Validation and Test data?",310503
116031,nan,314313
116053,nan,304692
116087,"in PCA, we are trying to reduce dimensions. Similarly, in hyperparameter tuning, we are also trying to identify the optimal number of dimensions..so, are these 2 approaches trying to achieve the same goal of dimensionality reduction? how are they different?",310509
116089,"in the Grid Search CV model, why is the k fold cross validation done on 1 parameter at a time? why cant the same be done using all the 13 parameters at one shot?",310509
116033,"In the GridSearch Cross-validation, what does the param_grid refer to. In this example, it takes a value from 1 to 13. My understanding is it refers to the number of features to be taken for running the algorithm. If there are a total of 13 features, they can be selected in 8191 different combinations (nCr - 13C1, 13C2..... 13C13) and multiply that with 5 for 5 fold cross validation, the total number of times the algorithm runs should be 40955. I&#39;m not able to understand why it runs only for 65 times (13*5). Can someone please explain this?",318084
116106,nan,310443
116138,Unable to understand this step.where we have to give the range under &#39;n_features_to_select&#39;. do we just need to give the total Variable / column count as the range?,316036
116158,"In the example python sheet, the author has selected optimal features as 10 and then built a final model. Please refer to attached screenshot for reference: In last module, we used PCA to figure out the optimal numbers of features for the model ? Could anyone explain the difference between two execution techinques ?",302735
116301,"a response on StackOverflow says, "" The decision threshold is not a hyper-parameter in the sense of model tuning because it doesn't change the flexibility of the model. "" Link: https://stats.stackexchange.com/questions/390186/is-decision-threshold-a-hyperparameter-in-logistic-regression",316349
116303,"As shown in the screenshot above, the explanation given is if reduce Lambda, the error term would decrease but how's that happening? On reducing Lambda, the overall term is being reduced but how can we say that Lambda is influencing the error term? Thanks in advance!",318355
116307,The answer to this question is actually opposite to the question. Should the question be &quot;Which of the following is NOT an example of hyperparameter? &quot;,306725
116319,nan,318403
116343,nan,319912
116346,nan,310179
116286,Do we use K fold when we have less train data or when we want to select optimum number of features or to check if model overfits?,301643
118632,"Hi Everyone, Since there was an ambiguity in the graded question on hyperparameters, the same has been deleted and won't count in your MCQ scores.",301619
118412,In the example shown for Linear Regrssion Model along with the training dataset the test dataset is also scaled. the method Fit_transform is being used for scaling both the dataset.My doubt is that for the purpose of scaling the test datset transform method should be used and not fit_transform as first the scaler needs to be fitted to the traning dataset and the corresponding parameters obtained will be used to tranform the test dataset.usinf Fit_transform again recalculates these parametrs and differ them from training set. Please explain if this understanding is wrong and how refitting the scaler for test datset is helpful as we are recalculating the parameters which will keep on changing with a new test dataset.,303115
116368,nan,301890
116046,"Each fold works on separate data set. Hence each fold gives a separate model with different co-efficients of the predictors. The presentation mentions ""average score"" of the model to be found from the scores array. What is the significance of this average score? How can average score of separate independent models be of any significance to the business? My point for max score is - why take average of all models created for each hyperparameter? Why not take the best model instead by choosing the max score and compare the same instead?",305839
119458,"folds = KFold(n_splits=5,shuffle=true,random_state=100) What is the purpose/extra benefits of this code as we can specify the no. of folds/divisions in the cross_val_score method itself? This method introduces extra verbose in the code and in subsequent section the same code is being used hence wanted to know added advantage of the code. What will be the difference in results if shuffle is made true or false?",303115
95417,"Using the data set, find out the country against which Virat Kohli has the maximum batting average. Here, the batting average is the (number of runs scored)/(the number of innings) and not the average runs Virat Kohli scored before getting out.",320606
95471,"For Not Out cases, the runs value is showing with &#39;*&#39; in the trailing value. To perform SUM operation on Tableau, do we need to change the Runs Column value. Is there any alternative or workaround not to perform Spilt/Trim to remove &#39;*&#39; in Tableau ?",310518
95489,"My interval range is correct w.r.t the maximum number of sixes. But, the value provided for the number of 4s against that interval in the options isn't matching with what I'm getting. Did anyone face this?",310974
95285,nan,318426
95515,nan,303674
95530,Do we need to compute the runs using the SR and Mins where ever we have null values in Runs?,300717
95845,nan,308434
95854,nan,314730
95852,nan,314730
95866,In Histogram while defining the range of 10; then it considering 0-10 where the value range between 0&gt;= and &lt;10 .But how to define like 0&gt;= and &lt;=10.,312518
95882,"calculating batting average = total(runs)/total(inns) after converting runs to whole number shows as invalid. any idea, why?",318827
95536,nan,305847
95550,,300687
95564,nan,308495
95582,"Here, the batting average is the (number of runs scored)/(the number of innings) and not the average runs Virat Kohli scored before getting out.",305845
95956,"The question asks to find average strike rate when Kohli has scored between 90-100 (excluding 100) using Histogram. However, the bins created automatically by Tableau are not in the range of 10s. The bins are of variable ranges. Is there any way of manuaaly setting the bin range?",315471
95608,nan,312518
95636,In the above question best average means avg of run that we calculated in question 1 or avg of run included the run attach with * sign?,310419
95643,nan,306996
95284,"1. Do we have to consider only Q2 and Q4 or Q2, Q3 and Q4. 2. When we say Kohli&#39;s Runs kept improving, do we mean his average runs, batting average or total runs. 3. Not sure what exactly is meant by &#39;played atleast one match&#39; in case no match was played in a period, we would not have a data point for the quarter anyways, so what exacly needs to be filtered.",306725
95288,What does this question mean? Does this mean that we need to find the longest time spent and then pick the average SR of maximum time?,301643
95331,"Hi, I am trying to plot the histogram using the runs scored. But At the starting point of histogram there is a clear speperation of data as 0-10, 10-20 etc.. But the at the end of chart data points appear in the center position of the bar. What is the reason for this?",311741
95337,nan,315560
95733,Batting Performance In which years have Kohli&rsquo;s runs kept improving in the Q2-Q4 period given that he played at least one match in that period?,305845
95374,nan,318335
96128,While creating the bin size from 0-10. How 90-99 is being categorized in one bin size. Any logic or idea behind it?,318372
96131,"In order to create Pie Chart by percentile, I took percetile of inning, but there are options like 5%,10%,90% etc? What to choose among these ?",315423
100265,"how to do question 3. Whenever Virat Kohli has scored 90-100 runs (exclude 100), what has been the average strike rate?",311046
95463,https://towardsdatascience.com/the-ultimate-cheat-sheet-on-tableau-charts-642bca94dde5,304026
95482,nan,300687
96047,"If you want to know the spread of duration of loan application over the entire year, which of the following visualisation method should be used?",310179
95171,nan,311117
95173,nan,318344
95186,nan,304813
95187,nan,304813
95987,nan,314799
95264,For the first question in &quot;Assignment-I&quot; what is the reason Blend is not an aswer for given senario! As we have 3 tables (May be 3 different excel) Asper my understnading Join is used when we got the data from same datasource and Bled to use when we have a data from different data source! So with this said the answer could have been blend for the given question!,311741
95281,,318335
95348,"can you please explain the question for ""What is the percent difference in the average salary for the month of July?"" what are the 2 parameters should we consider to calucalte percent difference.",320606
95354,"Using a pie-chart report the average salary of customers who were last contacted in the month of May. which column tells when was last contact ? I see contact column as cellular, telephone and unknown ..but which column tells when the customer was contacted last ??",310501
95368,nan,304026
128741,nan,318732
128272,"Suppose if name node goes down and standby node takes over, then my doubt is will it be having snapshot of metadata already like secondary node or not? If not then how will this be useful when it doesn&#39;t have metadata?",318495
130511,does the name node store the address of each block as well as address of the data node within which block exists? and how does it identify which data is being stored in blocks.. I get that it has address of block but how does it know what data exists in that address?,318079
128839,"If the name node fails; the standby node becomes active and take care of the failure. but of a data node failes, how will the data be restored ?",312259
127985,nan,306248
128051,nan,308962
129100,nan,303085
129505,nan,308774
128624,Name node only contains the address of data node and block and data type,301643
128623,If a name node fails then does secondary node restarts the name node or stand by node takes the place of name node?,301643
128900,"in AIRLINE DATA example, data distributed into three parts and stored in the different blocks in three different data nodes, if the stored data in the blocks corrupted. name node (cluster) works properly.",307491
128146,"Because the data nodes stores information, incase one of the data nodes fails what happens with the existing information stored in the failed data node?",301114
128648,nan,312093
128405,"I did not understand the need for a secondary node if a standby node is already there. As Shakun explained, the secondary node takes an impage or snapshot of the name node regularly. But, what does it do with it if the standby node comes into action when the name node fails.",314730
128869,"If both Secondary and Standby nodes checkpoint the data from name node at regular intervals, thenwhy need secondary node at all? Is this created as some kind of second back up just to replacate the data ( but with downtime)?",310509
128726,The answer says that the job doesnt go to the cluster as there is no standby node. But previiously it was said that the secondary node comes in to play if the name node and the standby node have failed. Explanation please,314313
128313,nan,308962
128332,nan,308962
128370,,320251
128513,"From above line, I understood that for each job submitted by client, Application Manager will get a separate Application Master launched dynamically. So, Application Master get its existence inside a container after allocation by Node Manager and when job is done, that container which is working as Application Master is released and remains mere a container. Correct if it is wrong !!",318770
129061,"1) If resources are not available on a node. how is the next node identified by the Scheduler? is it that node which has the data block replica, and the next best alternative for processing efficiently? 2) Why does the application master need to negotiate with application manager for resources? why it cannot do the same with the nodel manager, as its the node manager which is launching it?",310509
128145,nan,300718
128646,"If we know that we have 500TB, is it stored already somewhere? How do I know that it is 500TB? If I am capable to storing 500TB somewhere, will I use hadoop just because I fail processing on a normal machine? If it is streamed data, how would i know how much of data will be formed? My basic confusion is, all through the module, we have been told, huge data. How did I know that I have huge data if it is not already on one machine?",318007
135386,"Hi In Spark do we have any Build-in UDF in spark which can be used to updated value in Column like map , apply function in python.",301108
139028,"First Query select s.student_name,avg(marks) from student s left join marks m on s.student_id=m.student_id group by s.student_id order by avg(marks) desc Second Query select student.student_name , avg(marks.marks) from student,marks group by student.student_id order by avg(marks.marks) desc",315028
92351,nan,314621
90243,"For me while solving this question, I am getting ""No sample testcases passed. (0/2)"" while clicking on Verify but when I click on ""Run code"" it says ""Code execution successful!"".. what to do?",310508
91168,"I have completed all python graded questions &amp; assignments as part of module 2, 3 &amp; 4 before deadline of 14th October. However due to some constrain, earlier I had not attempted some 3/4 ungraded questions in coding console of ''Python for Data Science - module 3. As a result the completion tick mark did not appear on top of module  3 with 97% completion mark till 14 th October deadline. Today I have completed the pending few ungraded question and now I can see the completion tick mark on module. Will there be any penalty for missing deadline of module? Wil it attract penalty towards the graded question inside the module -3 even though its numpy/panda graded questions were submitted before 14 th October time line ?",306245
88646,nan,310502
90467,"Guys, My program expected Input and Output details are as below. Input 1: You love Python! Output 1: ouoeoY lv Pythn! And My Code is s = input() vowels = set(""aeiou""); index = 0; non_vowels_word = """"; vowels_word = """" for letter in s: index += 1; if letter in vowels: vowels_word = vowels_word + letter; else: non_vowels_word = non_vowels_word + letter; print(vowels_word + non_vowels_word) Please let me know if any improvements can be suggested to this code as per the Python paradigm",311741
88625,Did anyone try this using Tableau? Multiple names of a city causing issues. Is there a way to group these city names based on pattern matching in tableau?,318397
88683,"Hi All, Today when I was going through the start module , I got to know that deadlines for completing the modules or assignments are in IST. There are many students who are taking the course from outside India, so deadlines in IST might not be difficult assuming 10.5 hours difference for guys in EST timezone. Also assuming IST deadlines , will the modules get unlocked exactly at 12:00 AM IST ?",317460
88590,"Hello, I have used pivot tables with City as rows,count of gender and finally conditional formatting to solve. I wanted to know the different approaches used to solve the question.",315383
88715,"Guys, To calculate the percentage of Women in Cohort, as per the solution, the correct answer is 19% which is calculated with 152 count of women out of 806 total count. According to me, it should be 152/654. 152 women and 654 being total count. I am considering 654 since there are 152 missing values for dataset for gender column. so 806-152 = 654 total count. So my answer is 23.24%. please correct me if I am wrong.",301124
89303,nan,315661
88642,nan,312019
92662,Does Anyone have a detailed copy of full syllabus for this course ?,315242
123967,where exactly cart and chaid trees applied?,320606
124006,"Logistic regression can help us identify prominient features which can be used to build decision tree. Question - If you know, what are prominient features using logistic regression, how to use it for Decision tree while model buildig to get better results.",307495
122868,"1) Ujjayni speaks about importance score in logisitc regression. How to get the same from the model? 2) Also, how to obtain the log-odd scores from the logistic regression model?",310509
122909,how to build a CHAID decision tree in Python..can the code be shared?,310509
123085,Please share your views in comment section.,322691
122236,"Can TA provide pointers to additional materials for Chi square statistics/ mining KPI&#39;s using Python . I did go through Chi square statistic from Khan Academy, Python examples for CHAID appears to be missing from the lecture notes",309211
123422,"Hi alla, CART and CHAID types of trees are taught in our previous lectures already (thinking only that its a new name to our python codes) OR we need to learn it separately. Thanks.",317410
123462,nan,318770
133015,not able to view it says unauthorized and not allowed PGDDS C7| Live Session on Big Data Analytics This session was conducted on 17th Feb with Sandeep Diddi on Big Data Analytics,312019
99599,nan,310508
99934,nan,305655
99964,The null hypothesis is always formulated by either = or &le; or &ge; whereas the alternate hypothesis is formulated by &ne; or &gt; or &lt;. ... This is what we learn here from tutorial . But my point is if &ge; is for null than again how &gt; will be used in alternate,319969
99967,some of the points you refered take approximiate population&#39;s stnd deviation for samples&#39;s stnd deviation . Cudnt understand ... population and samples will will be always same .. can you elaborate with some examples,319969
99968,nan,306734
99034,"Goodyear claims that each of its tyres can travel more than 7500 miles on average before they need any replacement. Assuming that the average travel distance is given by &mu;, what would be the null and the alternate hypothesis in this case? As good year claims tyres can travel more than 7500 miles, they must be stating it based on their prior performance. So the status quo should be more than 7500 miles. Hence H: &mu; &ge; 7500 miles and H: &mu; &lt; 7500 miles. Answer is opposite, please clarify.",318344
99814,nan,312448
99820,"For the graded question in the first module of hypothesis testing Its mentioned that The sample deviation, as calculated from the sample, is 10.7 g. To calculate the Critical value do we use the sample deviation or the population deviation because as per the answer explanation they have used the sample deviation.",300688
99679,Any idea where do we see it? Is it in the other sessions of the same module or a different module itself?,318079
99821,In which case Zc comes out to be negative? Trying to solve Kent RO graded question in Session1 of Module 7?,308636
99682,nan,318846
101064,nan,314612
100179,nan,314629
100183,What is the basis of setting a scenario as the Null hypothesis? Is there any specific thing we should keep in mind?,314431
100182,"I have understood the process and arrived at the solution but am unable to get required meaning from it. If a claim statement says average is less than 400, we change the null hypothesis to average greater than or equal to 400. Is not this quite opposite of what te statement said. I am missing some basics here. Could someone help me understand the meaning behind this changing of operator from &lt; to &gt;=.",318007
100196,nan,310533
100199,"A house owner claims that the current market value of his house is at least Rs.40,00,000. 60 real estate agents are asked independently to estimate the house's value. The hypothesis test that is conducted ends with the decision of ""reject H"". Which of the following statements accurately states the conclusion? How to approach this problem?",306736
99707,"In the video it says &quot;Failure to reject Null hypotheses is not equal to Accepting Null hypotheses&quot;, but in the below quiz 1st question states that &#39;if you fail to reject the null hypothesis , what can you conclude from this statement?&#39; Answer was : &#39;Maggi Noodles do not contain excess lead&#39; Isn&#39;t the answer accepts Null hypotheses then, which is contradicting statement to the above?",312093
98707,"In this problem, Goodyear claims that each of its tyres can travel more than 7500 miles on average before they need any replacement. Assuming that the average travel distance is given by , what would be the null and the alternate hypothesis in this case?........ as per answer to this problem, ""H0 :Mu &lt;= 7500 and H1 = Mu &gt; 7500 i.e. Whatever Goodyear claims has been chosen as ""Alternate hypothesis"" in this problem I can also make an assumption on whatever Goodyear claims as Null hypothesis instead of Alternate hypothesis that is H1 : Mu &gt;= 7500 and H1: Mu &lt; 7500 . why can't I ? The same question also in the following situation. per Video, Problem statement starts with ""Defendent is innocent"" being chosen as ""null hypothesis"" [H0: Defendent is innocent H1: Defendent is not innocent] Why can't the problem statement start with ""Defendent is not innocent"" being chosen as ""null hypothesis"" ? [H0 : Defendent is not innocent H1 : Defendent is innocent]",309211
99259,"For the below question, H0 should be Mu &gt;= 7500 and H1 &lt; 7500. However, the answer shows H0 = Mu &lt;=7500 miles and H1 &gt; 7500 miles. Can someone confirm why this is correct. Goodyear claims that each of its tyres can travel more than 7500 miles on average before they need any replacement. Assuming that the average travel distance is given by &mu;, what would be the null and the alternate hypothesis in this case?",316202
99715,nan,301890
99461,"In the case of Apura having an archery score of 70, the null hypothesis is  = 70 which is clear. But since the idea is that we think she is lying,then shouldn't alternate hypothesis be  &lt; 70. H:   70 could mean that that it is greater or lesser than 70. Does that really cover what we are trying to prove?",317149
99464,"Shouldt the alternative hypothesis be mean AC sales &gt; 350 units rather than AC not equal to 350 units, as we are trying to prove that the demand will be higher than average and not lower?",310509
99522,Any inputs on getting 2.17 looking at the z table it shows a different value,300727
99524,"It has been established that Apurva lies and we do not believe her claims, which is why we test it out by playing 5 games of archery with her. Therefore, the Null Hypothesis should Ho&lt;=70 which is the Status quo IMO (option 3). Can someone please shed some light on this.",308962
100144,nan,314818
99590,"Hi, Can any one explain what is the difference b/w these two values. Thanks.",317410
99731,"In every scenario we&#39;re taking the claim to be Null hypothesis, however, in this scenario the Null Hypothesis is taken as opposite to claim and claim is considered alternate hypothesis? Why&#39;s that?",307176
99924,"It is already mentioned that population standerd deviation is 0.6, according to formula cv=M+(Zc*(sample standerd deviation)). So critical value will be 2.5+1.88*0.6 =3.62 but in the answer it is said as 2.61 Kindly some one help me understanding this",320636
99462,nan,306996
99940,nan,316211
99962,How is the Zc table and related table is generated. Theoretical explaination will be enough if mathematical calculation is out of course?,311386
99903,The calculation in the table is not clear. Can excel be shared with the formulas. n(j)*((x(j)-bar)-(x-bar)) = 11 * (8.045 - 7.6206) = 4.627 Not sure how 1.98 was derived?,317514
100925,nan,304320
99953,"the last sentence of 2 sample proportion test; how do we calculate that?? ""The risk to reject the null hypothesis H0 while it is true is 30.91%.""",316349
99952,"What Null hypothesis to be used? '=' '&lt;=' OR '&gt;=' Mathematically it is cleat but not able to understand very well in correlation with real world scenarios.. Like; Peppy panner pizza why did we considered the insignificant difference between 'Control' and 'Experimental' population?? Also, some practical example would really help for each of the three cases '=' '&lt;=' OR '&gt;=' of null hypothesis..",316349
100097,Which formula is used to find the value of Zc? How is degrees of freedom calculated?,308442
99806,"Please some once explain me how below statement is ok As the computed p-value is greater than the significance level alpha=0.05, one cannot reject the null hypothesis H0. does question says p value should be greater than 5% so accept the null h0. if so where its asked.",312019
100417,p-value &gt; alpha in the Two-Sample Proportion test while p-value &lt; alpha in the Two-Sample Unpaired test. So for the same data we have two different conclusions depending on the method used.,311857
99699,,307494
101125,nan,301890
100189,"In this example, conversions are more in new feature than in old feature Still we calculated p and alpha and concluded old feature is better bcoz p &gt;alpha... Why is this so? Even though conversions r more in new feature, why the proporion test results gave opposite result? Then what is the use of conversion ratio??",308437
100195,nan,317412
100219,"In the solution for 2-sample mean test, given on the page of 2-sample proportion test, the df is 56. But When I am tring to solve this. I have df = 62 and p value as 0.88",318532
99452,Does it become a Z test because sample size is beyond 30?,301114
99641,"in the lecture notes, on page 16, in the calculation of p-value example, the shaded areas in grey is shown as ~18%. However should it not be ~9% , considering its a 2 tail test and the 18% critical values should be split between both lower and upper tails?",310509
99514,nan,310467
99954,Please help on what is 'Control' and 'Experimental/Test' population; not able to connect this to real life scenarios..,316349
99908,nan,318791
100526,What can you conclude from this test about the two unpaired samples if you take a significance level of 5%? I could not find the values as specified by them to be present in the table. Can any one help me with this!,318427
100171,There is no explanation in the video on the one sample mean test. Can anyone let me know how to do it in excel.,314730
100176,"May be a question to TA, is it possible for us to get a licensed version of rhe xlstat tool as it would expire after trail period. Otherwise, if anyone can guide on a similar alternative free tools, that would be great for future practice.",314730
100173,There is no explanation on the output received after performing two-sample mean test (both paired and unpaired). Though I am aware of some of these output parameters. It would have been good if they are elaborated in the lecture. Can anybody provide details of the below parameters. Variance Pearson Correlation t Stat t Critical one-tail t Critical two-tail,314730
100310,nan,311466
99978,T test,314612
100948,"I'll just sum up my understanding of alpha and beta. Does the values for Alpha and Beta refer to the probabilty of making those errors? Kindly request TAs to validate my understanding. Alpha --&gt; Refers to the probability of making the type I error. Since we assume 5% to be value for most of the cases, can we say that if we repeat the test 100 times/sample, we are bound to make type I error for 5 times? Beta --&gt; Refers to the probability of making the type II error. If beta value for a test is 10%, can we say that out of 100 times/sample, we are bound to make the type II error 10 times?",318084
100029,nan,301118
99957,"Just want to relate the concepts better in normal language, what does the standard deviation means ? Example: Upon sampling 900 tablets, you get an average paracetemol content of 510 mg with a standard deviation of 110. What does 110 mean here in normal language? Tablets are manufactured with 400 mg (510-110) paracetemol content?",312093
99965,As far as I understand when alpha increase beta decrease and vice versa. But will there be any scenario in which alpha and beta have the same vaue?,310467
99966,if p vlaue is border line then null hypo will be accepeted ?,319969
99974,"In this though good year claims that, tyres can run atleast 7500 miles, hypothesis stated in the correct answer (specially the alternate hypothesis) seems bit confusing.",314084
99775,nan,311466
99564,See the explanation.. it doesn't result into 2.61 but rather 2.71.. Am i missing anything or this is really an error in calculation??,316349
99803,why its not possible to have low values for alpha and beta at the same time? what stops them from having such values? What are the constraints? Pls explain with a practical example,308437
99697,can anyone tell exact difference between Mu(X-bar) and X-bar. In AC problem X-bar = 370.16 and Mu(bar) 370. My understanding is that Mu is Population mean and given 370.,315679
101090,"TA please explain how to proceed with Q3 b You know that two types of errors can occur during hypothesis testing  namely Type-I and Type-II errors  whose probabilities are denoted by  and  respectively. For the current hypothesis test conditions (sample size, mean, and standard deviation), the value of  and  come out to 0.05 and 0.45 respectively. Now, a different sampling procedure is proposed so that when the same hypothesis test is conducted, the values of  and  are controlled at 0.15 each. Explain under what conditions would either method be more preferred than the other",311386
100124,nan,312448
99714,nan,305847
99719,Type 1 and Type 2 errors explanation is very limited. Its not clear. Providing more example would help.,317618
99648,nan,300733
100156,How to decided when to take 2 tailed and when to take 1 tailed test?,301641
99173,If we have universal truth as Ho. Can we say that the significance level is 0%? For example Ho = Sun rises in the east. H1 = Sun does not rise in the east. Can we say with 100% probability to accept the null hypothesis (fail to reject)? Practically it&#39;s not possible to have 0% significance level for normal example!!,318344
98739,"How can you know which problem is a two tailed test v/s which problem is a single tailed test? More specfically, for the paracetamol problem, how was it arrived that it is a two tailed test?",309211
99736,"Let me clarify a bit about the hypothesis testing method before I get into the errors part. This will explain the idea of how errors can exist in a hypothesis test. First of all, a hypothesis test doesn't prove whether the statement given by the null hypothesis or the alternate hypothesis is true or not. I think Prof. Tricha has also said this in one of the videos, that you don't prove anything here, you only say that it is a statistically better choice to either reject or fail to reject the null hypothesis based on the sample conditions. I think the term the Prof used was having "" sufficient evidence from the sample"". The reason why you're not proving anything is that unless and until you check for the entire population, you can never say the population mean is equal to something or not. Let me give an example to make it clear. Let's say that you are conducting a hypothesis test on 10000 cars and you claim that the average mileage of the cars is at least 30 kmpl. Now unless you go and test each and every car's mileage, compute their average and then come to the conclusion that whatever is claimed is true or not, you are not exactly proving the claim statement. Now what hypothesis testing allows us to do is that we can take a small sample of cars, carry out tests on it, calculate its average mileage and then infer whether or not it is a statistically smart move to reject or fail to reject the null hypothesis. By a statistically smart move, what I mean is that you calculate the values like Zc or Z-score and see if it falls in the critical region or not(if you are using critical value method) or whether its p-value is large enough(if you're using the p-value approach)or not. On the basis of that, you either reject or fail to reject the null hypothesis. [The Central Limit Theorem that you learnt in Inferential Statistics Module is again another ""statistically smart move"" to infer in what interval the population mean might lie based on the sample mean. You are never sure of whether it is true or not, but you do have a ""confidence level"" to justify it. Whatever you've learnt in Statistics course invariably always boils down to this: "" to use limited information about the sample to predict how the population might behave with a reasonable amount of confidence by using some statistical techniques ""] (Continued below)",313517
99667,"Why p-value must be multiplied with 2 in two tailed test , once the p-value probability is calculated is itn&#39;t the final value in both two tailed and one tailed tests ?",305652
91369,nan,318433
92470,nan,311952
90911,"But, for the other tables, i get 888. I cross checked the csv file and it has 890 rows in it.",310974
92488,nan,316889
92447,,318322
92457,nan,306996
92456,nan,311952
92354,When i try to update the calculated MA values to MA column i get the below error.pls advise 0 251 17:14:48 Error Code: 3593. You cannot use the window function 'avg' in this context.' 0.000 sec,300687
92502,nan,319444
92505,In SQL assignment can row50 (first row which has both MA20 &amp; MA50) be considered as cross over..? Or should that be Hold,311472
90176,In the statement : &#39;Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal.&#39; What is meant by &#39;Moving Averages Cross each other&#39;? Does it mean that there is a change in relationship b/w short-term and long-term moving averages in terms of bigger/smaller? Can&#39;t this change happen on daily basis?,311686
93677,"while working on the assignment, to change the date which was by default of 'text' data type, I wanted to change it to 'date' data type. tried using str_to_date function. I read the documentation for the function and used it accordingly. but Everytime, result was coming as null. There were different people who faced the same issue but I couldn't resolve it even after going through forum answers. Is there anything that anyone figured out? Above method is it not deterministic the way we wrote our UDF.",301649
92511,nan,314678
92458,"Error Code: 1148?? LOAD DATA LOCAL INFILE &#39;C:\ProgramData\MySQL\MySQL Server 8.0\Uploads\Bajaj.csv&#39; INTO TABLE bajaj FIELDS TERMINATED BY &#39;,&#39; ENCLOSED BY &#39;&quot;&#39; LINES TERMINATED BY &#39;\n&#39; IGNORE 1 ROWS Error Code: 1148. The used command is not allowed with this MySQL version 0.000 sec",318322
92514,nan,307176
92417,"Hi All, can some one explain how much difference is required between 20ma and 50ma to generate signal as mearly above and below is not sufficient to generate signal as mentioned in assignment.",301641
92464,can someone please help in telling if the signal on 50th row should be HOLD? or based on the 20&gt;50 or 50&gt;20 it should be buy or sell?,316349
92476,nan,318723
93558,"SELECT bajaj_1.date as stock_date, bajaj_1.Close_Price as bajaj, eicher_1.Close_Price as eicher, hero_1.Close_Price as hero, tcs_1.Close_Price as tcs, tvs_1.Close_Price as tvs FROM bajaj_1 INNER JOIN eicher_1 inner join hero_1 inner join infosys_1 inner join tcs_1 inner join tvs_1 ON bajaj_1.date = eicher_1.date = hero_1.date = infosys_1.date = tcs_1.date = tvs_1.date",303230
92431,"in my laptop , MY SQL got loaded in program files. C:\Program Files\MySQL\MySQL Server 8.0 where as my sql is giving me output to store as below. 'secure_file_priv', 'C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\' i have created uploads folder as per above path but error persists.how to manage..pl suggest. im stuck",300723
92515,"For SQL assignment ""3. Use the table created in Part(1) to generate buy and sell signal. Store this in another table named 'bajaj2'. Perform this operation for all stocks."" What is table from Part(1), is it from Question 1 or from question 2.",311472
92186,Can anyone guide how to make first 19 rows null. Any idea how to compare MA 20 and MA40 to add signal?,306243
92187,"What is the purpose of creating UDF mentioned in step-4 ( return buy,sell,hold based on date). This information is already present in bajaj2 table. I mean if someone does &quot;select signal from bajaj2 where date=xx&quot; would return the signal.",304814
92978,"After creating the UDF, when trying to test it out and enter a date, I entered a date in 2014 to check. But the error code says ""Incorrect Date Value 2002 for column @X at row 1"" even though the date I entered is 2014. X is my date input declared in the first line of the UDF which was created successfully. Does anone have any pointers as to why this might be?",316416
92767,Can we ignore 1 row..will it make any difference for the final analysis ?,318814
92771,nan,320197
93205,nan,301648
93252,"Do we consider yesterday's, todays and Tomorrow's values and then generate the signals or is it just yesterdays and today Close Price?",312259
93255,"I am using this script: UPDATE Table1, Table2 SET Table1.Date = function(), Table2.Date = function(); I am getting Column Date is ambiguous error.",310522
92773,"When the signal is neither buy nor sell, it is classified as hold. If you already own the stock, keep it and if you don't then don't buy it now. What in case if i already own a stock and i see for two days i am getting signlas to sell based on Short term &amp; long term moving averages and after two days again their is signal to buy. (i think here we can hold) what all other check to perform apart from moving avg for holding a stock. if hold concept is something diffrent then this please provide input.",306244
93256,please explain what kind of report is expected as summary. A report from a data analyst perspective or from a technical solution perspective?,300725
90737,20 Day MA and 50 Day MA is to be calculated on which table column? Also what is the column WAP in the stock tables provided?,310511
93259,When i define the following UDF: CREATE FUNCTION GEN_SIG(dd Date) returns varchar(4) deterministic BEGIN SELECT sig from bajaj2 where MarketDate=dd; return (sig); END Giving error code : 1415. Not allowed to return a result set from a function.,304319
91377,nan,308495
93262,nan,320688
93211,"I have used lag function on 20 Day MA and 50 Day MA with the cases for generating signal, Is it correct approach??",311004
93264,nan,307710
91383,"For generating the signal, is it just expected to calculate just Buy and Sell? Do we have to compute the logic for hold also? Can someone confirm please?",318084
91331,Like Auto_Expo to Auto Expo How to change?,310419
93210,Does it matter how we load the CSV file in table using wizard or manually or using load file. Would it be graded on how the file has been loaded.,306725
93610,SQL assignment,315558
93209,"Each of Stock data csv file(s) Bajaj, Eicher, Hero, etc. has 1 record with NULL value for 2 columns (1) Deliverable Quantity (2)% Deli. Qty to Traded Qty), should we keep them throughout the assignment steps and calculations as we are using mainly ""Close Price"" for calculating MA.",306731
91882,"Do we need to take care of null value rows before loading the csv to mysql ? when I am trying to load the csv with load data infile , it is throwing error with empty data in Deliverable Quanitity column. Any suggestions.",305652
93267,I created some columns in the table that helped me in doing what is asked. After the desired result is obtained I dropped those columns. Is this an optimised way or should I use temporary tables ?,318479
93274,nan,311117
93275,GEtting row 1 was truncated; it contained more data than there were input columns error while loading file in DB using Load,301641
92223,"As suggested by many, I changed the type of Data for the Date column to either Text or Char(30). While it helped me to import the data successfully (using either Import table wizard or Load infile command), I am not able to change back the date type to Date using the STR_TO_DATE function. I tried using both Alter table and update table command but its not working. i continue to get errors on the 1st row ( Error Code: 1411. Incorrect datetime value: &#39;2018-07-31&#39; for function str_to_date) or (Error Code: 1292. Incorrect date value: &#39;31-Jul-18&#39; for column &#39;Date&#39; at row 1). I am not sure how to solve this? I used the below commands: update `assignment`.`tcs` set `tcs`.`date` = str_to_date(`tcs`.`date`,&#39;%d-%M-%Y&#39;); OR update table tcs set date = str_to_date(date,&#39;%e-%M-%Y&#39;); OR alter table tcs modify Date Date; OR alter table tcs modify Date str_to_date(Date, &#39;%d-%M-%Y&#39;) None of the above is working....I am not sure what else to do?",310509
91043,nan,300690
90634,"Hi, In the Submission guideline, it is mentioned 'Make sure you have not made any changes to the original dataset provided to you.' Can't we make changes in the column names of the CSV files provided? For example first column name is 'Date' in the CSV file. It is conflicting with the 'Date' keyword while creating table. Also, colmn name '% Deli. Qty to Traded Qty' is also creating issues.",311686
92371,"Hi All LOAD DATA infile 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Bajaj Auto.csv' INTO TABLE Ass_bajaj FIELDS terminated by ',' enclosed by '""' LINES terminated by'\r\n' IGNORE 1 ROWS; While importing the csv file i am facing the error as Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'Set Date=str_to_date_(@date, '%e-%m-%y')' at line 8",310611
92986,nan,300718
92373,"Are we expected to solve Question 3 with the lessons taught in the lectures? I am not able to think of any solution by using ""insert into bajaj2 select"" and then figuring out when 20MA and 50MA cross each other. I wonder if this is possible just with one insert into table select statement. Any clues?",318007
91447,"alter table abc add xyz varchar(20) select name, age, date, if (expression_1) then set xyz=&#39;man&#39; elseif (expression_2) then set xyz=&#39;women&#39; else set xyz=&#39;child&#39; from abc; Error coming at select part.",310419
91449,"It&#39;s not clear what price (opening price or closing price) should I calculate the moving average on. Also, it would be helpful if someone can provide more details on the dataset columns and their meaning/significance (e.g. what does spread high-low, spread close-open mean).",318438
91453,"In the given csv file i cannot find any columns with name Moving Average . So ,Can you please let me know what exactly is MA mean related to CSV files ?",301108
92791,nan,303673
92353,"Deriving signals through SQL, what was taught in the Sessions is not sufficient to get the answer of the question. I am struggling from past 3 days to get the solution for this. is it only me or there are people who supports this? TA. Need intervention please. Thanks.",313676
91071,"How do we import the provided CSV files into tables Also, do we have to use temporary tables as the number of columns in CSV files are more than asked for the tasks.",318329
92465,What is the best procedure to make table from different tables with selection of required columns from other tables.,312019
92987,"I am having a post installation issue with MySQL and not able to carry on with the SQL assignment. Could I have the assignment done on another open source client pgAdmin using postgres SQL. Please advice. Regards, Prashanth",320243
91083,I am getting error Truncated incorrect DOUBLE value:'31-July-2018' How to remove this error?,310419
93089,if statement not taking window functions inside if()..why? window functions do not work in where clause why? if() doesnt return value returned by a function like for eg if(sign()) is wrong... why? is it possible to have a list of all limitations wrt each important function in sql?,308437
89007,"In the explanation illustrated, for the first five days the 5 Day MA is empty (Short Tem MA) and the first ten days the 10 Day MA is empty (Long Term MA) So, first ten days we cannot evaluate any signals. Should the null values be replaced by something? What signal should be given in the first ten days or in our case for first 50 days in case of the assignment.",318381
92790,"There are some ambiguities in the MySQL assignment questions. It'll be good if a TA can clarify them on what is required from evaluation perspective. Below are the questions: 1. For Question3 on calculation of Signal, is it ok to only populate data from 50th row in the table? Or If we have to include the first 49 rows, what should be the value of Signal for those rows? 2. What should be value of Signal for the 50th row? HOLD or buy/sell depending on whether 20DMA &gt; or &lt; 50DMA 3. What should be signal value for weekends/holidays? 4. For Question 5 regarding the write-up, should be a technical write-up on how we have created the tables and instructions on how to evaluate our assignment or should it be a functional write-up about the data comparing different stocks.",318084
93057,"Q1. Do I need to complete data set of all stocks, because all the questions are based on `date` and `close price` so importing `date`,`close price` is enough?? Q2. For signal table do I need maintain signal as &lsquo;hold&rsquo; once after signal &lsquo;buy&rsquo; generated?? Or for every iteration of &lsquo;date&rsquo; and that respective short-term, long-term results need to generate signal?? Need more clarity about this question. if possible share some sample results like you provided in the introduction about &lsquo;moving averages&rsquo;. Q3. UDF based on the signal table we need generate or need to write the code for that particular `date` based on that day respective `moving averages&rsquo;?? Q4. In the last we need write summary about the processing or programming and about the results ?? or difference between 6 stocks and their comparison analysis??",318322
92395,"While importing data from bajaj.csv file, getting the above mentioned error",318448
91485,I am trying to order the data by date column (order by date) in ascending order but it is getting ordered in random fashion. Also I am not able to fill blank data for 20 ma for 1st previous 19 rows as my data is not getting sorted properly. please let me know what I m doing wrong,317982
92679,nan,314678
92680,nan,300687
91923,"Hi, I am getting moving average for all the row for calculating 20MA. How to make first 19 rows as blank. Please guide.",307494
92682,i am using case statement and getting this error .please advise,300687
91492,"As per the brief, we need to create a new schema named 'Assignment' Do we need to capitalize the schema name as the default settings in mysql doesn't permit it?",318085
92077,I am unable to execute the LOAD command while importing Data from CSV to MySQL Table. Error: The used command is not allowed with this MySQL version. Any leads?,314547
91498,"For 20 Day MA and 50 day MA, the values would be null for the first 19 and 49 days respectively. In this case, do we have to calculate the signal or they should be HOLD by default",318085
91493,nan,308782
91511,"1. Should the tables names like Bajaj1 and others be in caps or bajaj1 will be ok? 2. Do we need to include sql query for creation of all the tables like bajaj1 , eicher1 or single query will work as only table name is different in all other queries?",320073
91001,"Hi All, As per the problem statement, I wanted to create the initial tables from the stock files with space in their names. I do not feel if this can be done via Table Import Wizard. So i wrote the following query: create table `Bajaj Auto` ( `Date` text, `Open Price` double, `High Price` double, `Low Price` double, `Close Price` double, WAP double, `No.of Shares` int, `No. of Trades` int, `Total Turnover (Rs.)` double, `Deliverable Quantity` int, `% Deli. Qty to Traded Qty` double, `Spread High-Low` double, `Spread Close-Open` double ); This created the table with space in the name. However when I am trying to load the table with records from csv, I have been getting the below error: Query: LOAD DATA INFILE &#39;C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Uploads\\Bajaj Auto.csv&#39; INTO TABLE `Bajaj Auto` fields terminated BY &#39;,&#39; lines terminated BY &#39;LF&#39;; Error : Error Code: 1265. Data truncated for column &#39;Open Price&#39; at row 1 0.047 sec I made sure the column datatypes and lengths are all similar as to when you create it via the wizard. Can some put some light on this?",310511
90810,nan,310419
90835,"Hi, I am finding the business logic for calculating the Buy/Sell/Hold signal from Moving averages, a little hard to understand. It says in the problem statement - &quot;Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal.&quot; What does that exactly mean? On a particular day if the short term MA is greater than the long term MA, doesnt it mean the signal has to be &#39;BUY&#39;? How does the &#39;cross over/below&#39; concept come into play here? Any response is appreciated.",310511
92685,"I&#39;ve populated the master table, but I&#39;m unable to display it correctly with the date in proper format using my code. Could anyone help with this?",310505
92619,"While submitting the solution, do we need to just show solution from 1 to 5 or from starting i.e Create a new schema named &#39;Assignment&#39; and Import the CSV files in MySQL, naming the tables as the name of the stocks.",320103
92532,nan,312518
92999,nan,317990
93597,"The way SQL assignment was presented required lot of rework. Considering the simplicity of the assignment, lot of hours were wasted in rework. Below are some of my concerns:- 1) It was not mention to have exact column and table names. I used underscore in table and column names, but had to redo entire code. One of the TA mentioned to have same column names and table names only couple of days before 2) Usage of function is also not very clear. We are asked to identify signal in Q3, then what is the point of creating same UDF in Q4? This added lot of confusion. 3) For all other questions, we were asked to create 6 tables one for each stock, but UDF was specific to bajaj. I created a generic function then had to redo to keep it specific for Bajaj. ( This was done once one of TA couple of days before set the expectation to have this UDF only for bajaj ) I would request the professors to look into the quality of questions and specially the way they are presented.",304814
92535,Is the price for a specific date the opening price or the closing price? Also do we need to take in account the inconisstency of the dates in the data. for eg. There is no 28-July-2018 but there is 27-July-2018 and 29-July-2018,304391
91875,"Can some TA please share the expectations on the brief summary? Should we share our analysis on the stock performance in functional language (I mean as a Stock Market Trader)? If so, should we write about all 6 stocks or choose any one as per our wish? Or Should the analysis be more focussed on the SQL learnings that we got from this exercise? Like how we approached each question and so on? It would be good if some TA can provide some insights on this. It will help us. Please answer this question as soon as possible",318084
92539,What is the format for input date? Standard MySQL format(yyyy-mm-dd) or it can be any format? not specified in ques.,318436
92540,I got this error when creating UDF. Need help in fixing this issue.,300708
92683,"I&#39;ve populated the master table with the close prices of each stock and I&#39;m getting the following output: But now the problem is displaying the date in the correct format i.e., YYYY-MM-DD. I&#39;m using a certain code and getting the following output: The dates are displaying correctly, but all the close prices have become 0. How is this solved? I have imported the CSV files using the import wizard and created all tables using the wizad as well. Please support on this issue. I&#39;m really stuck and unable to figure out how to resolve this. If needed, code snippet can be shared later.",310505
90932,While creating master table i found the above error. Anyone help me for removing the error.,310419
92688,why is my UDF returning no value when i select the signal from the required table using where condition where n = date?,310509
90752,"In the explanation illustrated, for the first five days the 5 Day MA is empty (Short Tem MA) and for the first ten days the 10 Day MA is empty (Long Term MA) . However when we calculate the MAs using sql, the values dont come as empty. Am I mission something here?",310511
90754,The date column in the input stock data files is in text. Do we need to convert this to date datatype for sorting while calculating moving averages?,310511
90110,Should I create the table and import?,317993
91997,"Third question is; 3. Use the table created in Part(1) to generate buy and sell signal. Store this in another table named 'bajaj2'. Perform this operation for all stocks. &gt;&gt; are we expected only to update the signals as 'BUY' and 'SELL' and not 'HOLD'?? &gt;&gt; If yes, do we need to be worrying about the 'Golden cross' and 'Death cross' AS IN; we just have to fill the signals based on a condition like; if 20 MA &gt; 50MA -- BUY if 20MA &lt; 50MA -- SELL other wise (only = left :) ) -- HOLD ?? or something else??",316349
90991,What exactly is expected in executive summary report for Advance SQL Assignment? An analysis on Bajaj stock as we defined a function for it. Or overall analysis seeing the Master table? Or even more generic analysis?,311686
92405,"When the shorter-term moving average crosses above the longer-term moving average, it is a signal to BUY , as it indicates that the trend is shifting up. This is known as a Golden Cross. On the opposite when the shorter term moving average crosses below the longer term moving average, it is a signal to SELL , as it indicates the trend is shifting down. It is sometimes referred to as the Death Cross. Does it mean 20MA&gt;50MA, it is signal to BUY and 20MA&lt;50MA it is signal to SELL?",308442
97365,nan,311046
89305,Do we need to stick to any specific version of MySQL ?,318381
91560,i can see we have same dates for various rows as the open price n etc are different. This needs to be combined while alculating MA? or treated as separate,317982
93087,"sum() over(rows between 1 preceding and current row) should return sum of previous and current row values but this is not happening why?? also sum() not adding positive and negative values, returning NULL values..why?",308437
91005,nan,300690
92543,Not able to load the csv file in the mysql workbench. I did add the below in the C:\ProgramData\MySQL\MySQL Server 8.0\my [client] loose-local-infile=1,301114
91935,nan,300733
92547,,315797
92549,"for master table i need to use join for all table sperately, is there any cocise way??",320197
92551,"I see many are suggesting to use the LAG function.. Can anyone sugest on how to apply LAG function on 2 different columns with CASE statements as well ? Seriously, I am stuck here ;(",300727
91600,nan,319866
92550,"I have created a table named Bajaj with Date in Date datatype and others in varchar.I am getting error while importing data. LOAD DATA LOCAL INFILE 'C:\\Desktop\\Assignment\\Bajaj_Auto.csv' INTO TABLE bajaj FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\r\n' IGNORE 1 LINES; error code 1148(used command is not allowed in this version) can someone please help?",300698
91615,Do we need to round off the 20 MA and 50 MA fields to 2 decimals in the assignment as part of the acceptance criteria? Currently they have lot of significant digits in the result,318085
91945,"Hello, Can anyone please elaborate this question? means what is the concept.. when to consider the stocks to buy and when to sell, hold respectively? it would be great help..",305129
92193,Is the &quot;input date&quot; has to be take from bajaj2 table? or any date value? And also if we consider any date then input date format conversion has to be taken care? Please clarify.,318846
92100,nan,318802
91949,"In a real world, a BUY or SELL would be determined based on the the Close Price of the past days 20, 50 days but not today's date right? We are yet to get a Close Date for today right? I was just confused with the idea of calculating moving average including today's Closing Price in this assignment. Any thoughts on this? Wanted to check if I am deviated?",318007
92765,nan,314197
93055,What shall the UDF return for the dates which are not present in bajaj table ? Is it supposed to return NULL or HOLD ? Since if we only return the select query result from UDF it will return NULL when the query will fetch no rows.,318756
92085,"CREATE TABLE bajaj (`Date` date, LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Bajaj Auto.csv' INTO TABLE bajaj FIELDS TERMINATED BY ',' ENCLOSED BY '""' LINES TERMINATED BY '\n' IGNORE 1 ROWS; Any error here?",314547
94034,nan,308495
91650,nan,300687
91958,,300733
92106,nan,318005
92498,nan,306996
92499,nan,306996
94032,Date column of type text is not getting modified to date type... code : alter table `bajaj auto` modify `date` date; error: 1292. Incorrect date value: &#39;31-July-2018&#39; for column &#39;date&#39; at row 1 plz help,316133
91676,"I'm using window function to calculate the 20 Day MA.. but now struggling with the right code to add it in the Bajaj1 table. MySQL thows an error saying ""You cannot use the window function 'avg' in this context"" Tried with 'Update' code; Alter table code and also Insert code (which is adding data randomly)..",316349
91692,nan,300687
91681,In Question 1 what does it mean - (This has to be done for all 6 stocks meants ) for Columns 20 day MA and 50 day MA,301108
92098,nan,318802
93085,I am trying to CREATE FUNCTION with BEGIN and END but it is not reflecting under schemas after execution.,307495
92099,nan,318802
91969,,311004
91702,"My system was fine all this while. But when i imported the tables for sql assignment, it got very slow. My laptops disk usage is 100% now, and i am not able to do any tasks. Did anyone faced this issue?",316202
92112,What should the UDF return if the date passed is a holiday? Should it return NULL or some message or the signal for the last working date?,313826
91966,"It is instructed to skip the rows where moving average cannot be calculated. Now in this case We skip first 19 rows for moving average of 20 days. There are 30 rows wherein 20 Day Moving Average won&#39;t be null, but 50 Day MA will be null for those 30 rows. Do we then skip first 49 rows altogether?",319866
92119,"I am getting error 1262 - Row 1 was truncated...i have checked the length of all the fields and compared with the set up in the tabnle and no where does it look to be short....so, how to fix this issue is there any way to idenitfy which particular field is being truncated?",310509
91971,"The column deliverable quantity is having null. But, we will be calculating moving average on another column. But, keeping this row can impact the result set. Are we supposed to delete the row or keep it?",301643
94072,Can someone tell me where am i going wrong.,319846
91973,,311004
93062,"In General question what is the best moving average days to be used to find signal , to conclude buying and selling the stocks.",312019
92122,"I have tried convert, cast and str_to_date and get the following error: Error Code: 1411. Incorrect datetime value: '31-July-2018' for function str_to_date 0.297 sec",300748
93075,nan,311117
92492,"Is it ok to create schema and tables and also import the CSV files using the wizard in MySQL workbench or is it mandatory to use only commands to do these operations? I&#39;ve used the wizard, but I&#39;m wondering if the commands are now mandatory. Please support on this.",310505
91736,,303674
91983,nan,318451
92144,How to change the working directory in MySQL workbench in order to import CSV file into it,311727
91733,"For Example we need to get Moving average 20 for all records If i am at 21st records then i would be taking moving average of last 20 records i.e from 2 to 21st records But If i am 3rd row so , it can take only average of first three records . So, Can that be treated as moving average 20 for that row. ? Appreciate your reply in advance",301108
91750,Anyone using &quot;SQL Workbench 8&quot; ? I am getting beloe error when using &quot;LOAD DATA LOCAL INFILE&quot; method to load data into mysql table. &quot;Error Code: 1148. The used command is not allowed with this MySQL version&quot; SHOW VARIABLES LIKE &#39;local_infile&#39; is &quot;ON&quot;. Any suggestion to resolve this error ?,304020
92146,nan,300733
92036,What is the issue with the below code when creating table?,310509
92214,nan,318455
92210,"According to me, there is not much difference b/w Q3 and Q4. Q3 asks to create signal for each date using query. Whereas Q4 asks to create a function to get the signal value for particular date. If you create a table in Q3 containing signals, then a simple select query would return the signal ( I dont see a point of Q4 to create a function) Or I have misunderstood it completely?",304814
91759,"Is it right to assume that only on the day of occurance of Golden Cross the signal will be ""Buy"" and similarly only on the day of occurance of Death Cross the signal will be sell? Can anyone confirm if this understanding of business problem statement is correct?",306250
92899,nan,306995
91593,"I&#39;ve been trying hard for a solution for the 3rd assignment question but finding it tough to proceed further. At a broader level, I feel signal could be derived using a stored procedure. I got the logic to derive the signal but not very clear on how to pass the table to the stored procedure and read the columns of the table. Kindly request someone to provide some pointers to proceed further.",318084
90585,"Hi, In grading criteria page the 1st criteria is mentioned as &#39;Creating Table Wipro1 and six others with Date, Close Price, 20 Day MA and 50 Day MA&#39;. But there is no Wipro data set. Am I missing something of it is some kind of typo error?",311686
92149,Problem introduction states following: &quot;Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal.&quot; What does this mean?,301641
91760,nan,318451
91771,nan,310419
92007,nan,319866
91780,,300733
91781,"Question is specifically about the type of analyis done using bajaj2 table (Calculations of 20 days and 50 days MA, calculation of Signal values et al)... Is that analysis supposed to be repeated for all 6 stocks?",306250
92339,Bajaj1 table will have first 19 rows for MA20 as null. Should Bajaj1 start with 2015 date or 2018?,314547
92166,Here it seems all records are 889 and seems all r having same dates but what if data related to some dates are missing from a stock.,318005
93080,"While getting the moving average for 20 and 50 days, the Close price is being used twice, is this causing the error, or something else.",316132
94204,,320687
91346,installation problem someone help me out,310008
91237,"For a 20 Day Moving Average column, is it expected that the first 19 rows has to be empty? Similarly for 50 Day Moving Average, is it expected that the first 49 rows has to be empty? Can someone please clarify this?",318084
91345,"The stock csv files have the date values in descending order ie latest date at the top. Do we proceed the calculation of Moving averages and finally the Signal based on this ordering or we change this ordering to ascending, ie earliest date at the top. Because, based on this ordering, the final results and no of buy/sell indicators will change.",310511
91132,"Hi, I understand to load data from CSV to 6 stocks tables. But how the table bajaj1 and master table will be populated. I am not very clear . Please help.",307494
91122,"Please note that for the days where it is not possible to calculate the required Moving Averages, it is better to ignore these rows rather than trying to deal with NULL by filling it with average value as that would make no practical sense.",311952
91130,nan,311952
91133,nan,311952
92167,nan,318802
92016,,318846
91852,nan,300697
92285,"I can observe missing value at row 230 in Deliverable Quantity, How to add this row too?",318436
92283,"Getting the below error pls advise 0 121 10:06:15 UPDATE bajaj1 SET Date = STR_TO_DATE(Date, '%d %m %Y') Error Code: 1411. Incorrect datetime value: '31-July-2018' for function str_to_date 0.000 sec",300687
92173,My over command is showing me an error like this: Opening Parenthesis not allowed at this position Please help me know that how can I solve this issue PS: Error is shown in the snippet above,301655
91590,"Hi all, In this does 20 day MA and 50 Day MA should be a permanent column in the table? or is it ok to write a query so that when we run it will show the 20 Day MA and 50 Day MA with the values but doesnt add permanently to the table?",305129
92328,While updating the signals in Bajaj2; I'm facing this error. How to get rid of this? -- Please help,316349
92042,"Hi, Please help to explain. I am having difficulty in understanding Cross concept. Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal. Thanks, Babita",307494
92052,nan,314612
92046,nan,318455
91324,Write a brief summary of the results obtained and what inferences you can draw from the analysis performed. (Less than 250 words to be submitted in a pdf file),317689
91325,Write a brief summary of the results obtained and what inferences you can draw from the analysis performed. (Less than 250 words to be submitted in a pdf file),317689
92045,"Even after remove secure-prev settings from my.ini ,it throws error. restarted my server ,my workbench ,system but still throws error.",318005
93081,is it possible to create a function / procedure to update 20dayma and 50dayma by passing the table name?,302739
93084,nan,306738
92059,"I am getting below error when loading table...also, please advise if the code is correct as well?",310509
93086,"Hi, I ran this code - &quot;INSERT INTO `master` (Hero) SELECT `Close Price` FROM heromotocorp;&quot; And I am getting the following error - Error Code: 1364. Field &#39;mDate&#39; doesn&#39;t have a default value The issue is that mDate field has already been updated with its values in a previous insert statement from another table. Can someone please help with this issue?",312608
93098,"&quot;4. Create a User defined function, that takes the date as input and returns the signal for that particular day (Buy/Sell/Hold) for the Bajaj stock.&quot; We dont need to write UDF for all others?",300717
97386,"Hi, for the User defned function that takes date as input and gives signal of the perticular date, I have written the function query as DELIMITER $$ CREATE FUNCTION SIGNAL_INFO(DATE DATE) RETURNS VARCHAR(5) DETERMINISTIC BEGIN DECLARE SIGNAL_INFORMATION VARCHAR(5); SELECT `SIGNAL` AS SIGNAL_INFORMATION FROM ASSIGNMENT.BAJAJ2 WHERE DATE = DATE; RETURN(SIGNAL_INFORMATION); END $$ But I am getting an error : Error code 1415. Not allowed to return a result set from a function.",311046
91312,"While loading csv file into MySQL table I am getting the following error - ""Error Code: 1290. The MySQL server is running with the --secure-file-priv option so it cannot execute this statement"" ...any idea what needs to be done?",306250
93099,,310522
93212,Is it okey of the name of the table created is 'bajaj' instead of 'Bajaj Auto'?,313826
93213,"Hi, Do we need to create Row Number for calculating 20 Day MA and 50 day MA, let me know how i can proceed without creating a Row number column",318822
93214,Can I add a row number column in the bajaj2 table for the ease of calculating signal? Need TA help.,311745
93216,nan,305334
94312,nan,320687
93217,"Assignment talks about creating columns : date, close price, 20 Day MA and 50 Day MA. Do the column names need to be exactly these (containing spaces and keywords) ? Or can we opt for naming convention with underscores like rec_date, close_price, 20_day_ma, 50_day_ma? I know that it is possible to have these exact column names; but it does not align with the usual naming convention that was covered in the videos and is practiced in the industry; hence checking.",318762
93203,"I am using the IIF case, and in that I am using the ROWS as parameter. I am getting below error You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;ROWS &gt; 19, `20 Day MA`, NULL) `20 Day MA`, IIF(ROWS &gt; 49, `50 Day MA`, NU&#39; at line 3 0.000 sec How to resolve it",318814
93218,while I'm running this query. The output would be printed all ssn employees who contain the Houston in the address. I'm getting output who doesn't contain Houston.,320687
92735,nan,318579
93105,"Do we need to create all 6 different tables? as an example baja1,TCS1 etc?",308964
92737,nan,318579
93106,"It is given &quot;Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal.&quot; In the example shared in the Problem Introduction page, by the above statement, the first cross over is on day 12, is this correct?",300717
93389,FOr the SQL assignment is it necessary to upload the csv files provided in the zip file or just the .sql file and summary in .pdf file will do?,304389
93330,As in question they mentioned to pull exact column name.,318802
93114,nan,318788
92938,when i am using the query the average is not macthing with the average in excel,316215
94360,I am getting this problem when I am trying to copy data from one table to another. How to resolve this?,310472
92454,"Are there are any null values in the close prices in any of the datasets(Bajaj, TVS etc.) in the Stock Market Assignment? Because after uploading the dataset about 30% of the values in the close prices are missing in Bajaj.",311727
92500,nan,306996
93148,nan,318579
92588,nan,316041
93122,"HI, Can we change the date format in given CSV file and import it to MYSQL and work on it.?",317410
93484,nan,318772
93388,nan,308640
92561,"LOAD DATA INFILE is not allowing in a stored procedure/UDF. Since, loading data is common for 6 stocks, i want to avoid duplication. Is there a way to do this apart from having this code in an executable and running through native exec.",318329
92562,"When I am trying to use LAG method for a particular date, it is returning NULL. This is how I am using SELECT LAG(20maccolumn, 1) OVER (ORDER BY Date) INTO previousMa20 FROM Bajaj1 WHERE Date = somedate;",318329
93139,FYI.. Just used join,314313
93140,nan,312746
92564,nan,311857
93450,nan,312758
93142,"I have got the moving averages without using the lag function. I am still not very clear on how to use this lag function without example. Also, in a lot of places I see a &quot;with&quot; clause what is this and how is it used",311857
93145,nan,311857
92565,"while Converting String to Date, UPDATE assignment.bajaj1 SET Bajaj1.Date = STR_TO_DATE(Bajaj1.Date, &#39;%d-%m-%y&#39;); Getting error message Error Code: 1411. Incorrect datetime value: &#39;01-Apr-15&#39; for function str_to_date",320648
93088,1. using insert statement we cannot insert column values 1 by 1...it asks for entire collumn to be updated... but not all the times...why is this anomaly? 2. what is d best way to update single/multiple column values?,308437
93128,Are we using the Update statement to insert the values from bajaj for different values or we using one Insert statement to insert the values from bajaj to bajaj1,318814
93144,I have done this by both join and where clauses (separately) I find the where clause neater assuming both take the same time/optimisation. Is there a reason you should use one over the other,311857
92567,Can we give the column names a little different than whatever mentioned in the assignment? I mean instead of column 'date' can i make it 'market_dt' or instead of column '20 day MA' can i keep it '20_day_ma'? Similarly for column 'signal' can we make it 'stock_signal' ? Or do we need to adhere to the field names provided ? Will there be any deduction in marks for the same ?,318756
92592,"I have tried everything but couldnt get LOAD DATA INFILE working on my Mac, I have tried setting load_infile to true, I have even Provide a secure path on my machine, but nothing seems to be working. I am stuck here.",300734
92438,For the SqL assignment- Should I use table Import wizard to import the csv files? While doing so I saw that one row is missing that was with null values? Can I ignore that? Can anyone share the correct way to do so?,300698
92571,"How is the "" 20 day MA "" column being updated in bajaj1 table?Tried using windowing function but that can not be used to update a column.",300698
93154,"The deliverable quantity variable was showing error due to null values, so I imported them as strings. But unable to convert it back to numeric values.",318397
92449,nan,311952
92602,nan,314678
93565,"Hi, I am trying to import csv data into table Bajaj_Motors. I am getting error &quot; The MySQL server is running with the --secure-file-priv option so it cannot execute this statement&quot; , I am using below command to import it : LOAD DATA INFILE &#39;C:\Users\Naren\Downloads\Assignment\Assignment\Dump\Bajaj_Auto.csv&#39; INTO TABLE assignment.Bajaj_Auto FIELDS TERMINATED BY &#39;,&#39;;",311046
93567,nan,308495
93161,nan,308637
92895,error while loading the file pls advise,300687
93466,"Here in Problem introduction we were told that compare current ROW MA of shorter term with Current ROW MA of longer term, and if current shorter term value is above longer term value then give BUY signal otherwise SELL. Now, there are many people in discussion forum discussing that we need to calculate signal based on current as well as previous data ? So which is proper approach to follow ?",315423
93468,nan,318429
92577,nan,311952
93472,nan,318772
94442,nan,310472
93163,"what is meaning of below error and how to solve this ERROR: ASCII '\0' appeared in the statement, but this is not allowed unless option --binary-mode is enabled and mysql is run in non-interactive mode. Set --binary-mode to 1 if ASCII '\0' is expected. Query: 'PK_x0003__x0004__x0014_'.",319969
93474,nan,315650
93164,You will be required to submit the commented a zip file consisting of a SQL file and pdf file describing the key observations and summary in under 250 words that you can make from the results.,317156
92368,"I am facing issue with loading data file in MySQL. I am getting the below error - Error Code: 1290. The MySQL server is running with the --secure-file-priv option so it cannot execute this statement I have followed the instructions to create my.cnf file with the below settings - [mysqld] secure-file-priv=/Users/anujgarg also tried with secure-file-priv='/Users/anujgarg' but command is still failing. The command SHOW VARIABLES LIKE ""secure_file_priv""; gives the below output - 'secure_file_priv', 'NULL' How to solve this issue on Mac ? Any help please ? Thanks, Edit : &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Also, whenever I start mysql from terminal, I am getting the below error - anujgarg-mac:bin anujgarg$ sudo mysqld start Password: 2018-10-22T17:33:37.080876Z 0 [Warning] [MY-010091] [Server] Can't create test file /data/anujgarg-mac.lower-test 2018-10-22T17:33:37.080918Z 0 [Warning] [MY-010101] [Server] Insecure configuration for --secure-file-priv: Location is accessible to all OS users. Consider choosing a different directory. 2018-10-22T17:33:37.080966Z 0 [System] [MY-010116] [Server] mysqld (mysqld 8.0.12) starting as process 28350 2018-10-22T17:33:37.081002Z 0 [ERROR] [MY-010338] [Server] Can't find error-message file '/share/errmsg.sys'. Check error-message file location and 'lc-messages-dir' configuration directive. 2018-10-22T17:33:37.081946Z 0 [Warning] [MY-010091] [Server] Can't create test file /data/anujgarg-mac.lower-test 2018-10-22T17:33:37.081960Z 0 [Warning] [MY-010159] [Server] Setting lower_case_table_names=2 because file system for /data/ is case insensitive 2018-10-22T17:33:37.082041Z 0 [ERROR] [MY-010123] [Server] Fatal error: Please read ""Security"" section of the manual to find out how to run mysqld as root! 2018-10-22T17:33:37.082117Z 0 [ERROR] [MY-010119] [Server] Aborting 2018-10-22T17:33:37.082381Z 0 [System] [MY-010910] [Server] mysqld: Shutdown complete (mysqld 8.0.12) MySQL Community Server - GPL.",312479
93165,I am struck with population very first table bajaj1. It is trowing error 1265 while population very first 20 DAY MA. It work fine till null values for 20DAY MA. Any idea why?,319319
93489,nan,310504
92610,"Select `Date`, `Close Price`, (CASE when `20_DAY_MA` -`50_DAY_MA` &gt;0 and lag(`20_DAY_MA` -`50_DAY_MA`,1) &lt;0 then &quot;BUY&quot; else &quot;SELL&quot; END) as `Signal` from Bajaj1; please dont go by the logic of the code, just help me with the syntax.",310509
93531,CASE WHEN `20 DAY MA` &gt; `50 DAY MA` THEN &#39;Buy&#39; WHEN `20 DAY MA` &lt; `20 DAY MA` THEN &#39;Sell&#39; ELSE NULL END as Signal,318807
93529,nan,320603
92618,nan,312518
92920,nan,308495
92627,"select Date, `Close Price`,avg(`Close Price`) over (order by Date rows 19 preceding) as MA from bajaj1;",314678
92621,,306996
93530,"what this means? can any one explain with example? Please note that it is important that the Moving Averages Cross each other in order to generate a signal. Merely being above or below is not sufficient to generate a signal. When the signal is neither buy nor sell, it is classified as hold. If you already own the stock, keep it and if you don't then don't buy it now.",318461
92905,"While calculating moving average, which price should we need to consider? is it closing price or open price?",317073
92815,Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;%d-%m-%y)&#39; at line 2 0.000 sec,303673
92649,"Is it required / necessary to keep the order of the bajaj1 table from 2015 to 2018? ie, in ascending order? or, do we keep it as it is given in the dataset ? ie, from 2018 to 2015 ?? Please clarify. thank you.",317998
92911,nan,303673
95595,Executive Summary(should be less than 250 words),312259
92909,Throwing this error.. all of a sudden .. i have uninstalled mySQL Workbench and reinstalled it again still facing the same issue. Please help,314313
92918,nan,308495
92921,nan,308495
92917,When i am checking for the row number with ... WHEN 'Row' &gt;= 50 I am getting the error Error Code 1292 : Truncated Incorrect DOUBLE value : 'Row'. Can't understand.,304319
93537,"In the assignment problem for importing CSV file to sql workbench steps that i have followed are: Created a new schema Inserted new table and specified column names(date, close_price, MA20, and MA50) and data types then on right click, there is an option to import data from the external source and selected to which columns data needs to be imported then I can see table I have created with name bajaj1 under assignment schema is this the method to import file data to SQL??? As I am not able to find the table on using the command select* from bajaj1; Can anyone please help me with this activity",317558
92819,I am trying to update the signal column. using update table set case in this when command with two conidtions check &gt; and &lt;. when i try to get value of preivous row with lag and over. it says error like Error Code: 3593. You cannot use the window function 'lag' in this context.' end,312019
92742,i got my data uploaded to table easily by putting the csv file in the upload location of mysql..Will it be acceptable when assignment will be executed finally after submission. or i need to change system variable path i.e secure_file_priv,319869
92744,How to resolve this issue?,310522
92756,"Create a new schema named 'Assignment' Import the CSV files in MySQL, naming the tables as the name of the stocks. Above 2 statements are not part of grading.. I assume, before executing our SQL queries, these tables will be present for the TA? Please clear.",313676
92748,"While loading csv to table i got all rows loaded but with warning as i only fetch specific column Date and Closing Price and in return i got 889 rows affected and rows truncated also as it contain more data than specific column.. Is it ok??? # Date, Close Price, 20 Day MA, 50 Day MA &#39;2018-07-31&#39;, &#39;521&#39;, NULL, NULL &#39;2018-07-30&#39;, &#39;527&#39;, NULL, NULL &#39;2018-07-27&#39;, &#39;530&#39;, NULL, NULL &#39;2018-07-26&#39;, &#39;542&#39;, NULL, NULL &#39;2018-07-25&#39;, &#39;551&#39;, NULL, NULL &#39;2018-07-24&#39;, &#39;540&#39;, NULL, NULL &#39;2018-07-23&#39;, &#39;564&#39;, NULL, NULL &#39;2018-07-20&#39;, &#39;562&#39;, NULL, NULL &#39;2018-07-19&#39;, &#39;568&#39;, NULL, NULL &#39;2018-07-18&#39;, &#39;575&#39;, NULL, NULL",319869
92598,nan,311952
92189,"Hi, Is it mandatory to import the data with LOAD DATA INFILE process or can we do it directly with right clicking on table defined and go through import process as mysql enginee handles...?",300693
92578,nan,303674
92753,"While calculating the signal using 20DMA and 50DMA can we consider future dates, eg if i have to calclulate the signal for 13th Jan, can i have the comparison with 14th Jan as well or should we limit our comparison to 12th Jan values only. I am asking this because in actual practive we would not have future data available.",306725
92948,nan,306738
92951,"In order to load the data , i have created the table with field as varchar which contain value '31-Jul-18' i.e . Now I haves used the str_to_date function to convert the date format to 'YYYY-MM-DD' e.g '2018-07-31'. Now I want to convert this field which is varchar type to Date type . I have tried Cast &amp; Convert function but still not able to convert the field type of the table .",311861
92955,"the tables are populated like it shows on the left and the schema is selected before querying,but error 1146:",300684
93243,"I have two queries - In &lsquo;bajaj1&#39; under &lsquo;20 Day MA&rsquo; &amp; &lsquo;50 Day MA&rsquo; column, what will be the value for initial 19 rows and 49 rows respectively? I understand the initial 19 rows and 49 rows under above column will have incorrect moving average value. So do we need to mark it as null or keep the wrong value as it is ? Based on above query, in &lsquo;bajaj2&rsquo; , what will be the &lsquo;signal&rsquo; column value for initial 49 days ? I understand we need to calculate signal from 50 th rows onwards but in overall query on table, it is coming as &lsquo;&rsquo;HOLD&rsquo;&rsquo; . so do I need to update it as &lsquo;&rsquo;NA&rsquo;&rsquo; or leave it as it is ? Please suggest. thanks",306245
92931,Getting error while importing csv file. Please help. using below command. LOAD DATA INFILE 'C:/ProgramData/MySQL/MySQL Server 8.0/Uploads/Hero Motocorp.csv' INTO TABLE `eicher motors`;,313676
92755,"I have been trying look for more information about the golden and death cross and came across a few explainations. This picture was helpful for me for the same, hence sharing. As this picture shows, during the period of 7 years, there were only 6 golden and 5 death crosses. Hope this is helpful for all.",319302
92469,"While inserting required data into the bajaj1 table, I'm encountering the error even though I have declared 4 columns. The select function is working fine and is calculating the MAs. The problem is the insert. Any pointers? insert into bajaj1 (`Date`,`Close Price`,`20 Day MA`,`50 Day MA`)",316416
92822,nan,307708
92825,nan,318013
92889,select AVG(`Close Price`) OVER(ORDER BY `Close Price` ROWS 19 PRECEDING ) AS 20DayMA from `bajaj auto`; what is error in this query . its giving some 1064 error?,315757
93595,,318772
93168,reference below link https://www.investopedia.com/terms/s/stop-lossorder.asp,306244
92898,"Hi, while importing the data set I am getting the mentioned error. The column is fo type float. Why is it truncating while importing ?",318479
92900,"MYSQL not working. the software was working fine until today evening, when it got very slow and when I checked thisPC on my laptop my C drive was full and the SQL folder in ProgramData was occupying 91.2 Gb of space. I tried unisntalling the software but the data is still there. I tried Shift+ Delete, but it is showing some error .Somebody please help me. I had completed half of my assignment and now this has happened.",304389
93170,How to insert data ? I have used import wizard. Do we need to use insert command ? then we have to insert 889 line for each table? During submission do we need to submit insert command related file also? or I can use import wizard only?,311404
92678,"I have populated the master table with the date and close price of each stock. I&#39;m getting the following output: But now, the prolem is display of the date in proper format i.e., YYYY-MM-DD. With the code that I&#39;m using, I&#39;m getting the following output: The date is displaying correctly, but all the stock prices have become 0. I have imported the CSV files using the import wizard and also created all the tables using the wizard in MySQL Workbench. Please support on this. I&#39;m really stuck and unable to figure out what to do. If needed I can share the code snippet for displaying the table later.",310505
93171,"How to import the assignment files and fix the few error i.e 1064 etc , hot fix those . if we need to imort 4 or 6 file together then which keys requires . I need one time simple process know how from importing to answering the solution",319969
93174,Can we use temp tables or directly from bajaj to bajaj1 to bajaj2? i Ask this whether using temp tables is considered as an optimized query?,317269
93175,nan,308442
93176,what do we need write in the summary. does this need to be technical details on how we performed the analysis or we need to analyse the data. in case we need to analyse the data what kind of analysis is required as we are not familure with the domain.,306725
93177,"For signal calculation, what should be the value of the first row? Depending on whether 20DMA is greater or lesser than 50DMA, can we calculate it as Buy/Sell?",318084
93178,Should I give datatype to date as text or any other solution is possible?,313767
93179,I could create window function to find MA but not able to add this as a new column to bajaj1 table. How to do that?,311061
92958,,310504
92969,I get this error when creating the master table using inner join for the 6 tables. Any suggestions: Error Code: 2013. Lost connection to MySQL server during query 30.000 sec,318844
93187,Should Moving Averages be built using SQL Analytical functions like RANK or could be done using PL/SQL Procedures.,320243
92974,UDF rerlated My select Query in the return should fetch the signal call based on the Date i pass. Not sure why it throws a 1242 and says i am fetching more than one row.. Any inputs.......,300727
92959,nan,306996
93182,"How to create User defined function, that takes the date as input and returns the signal for that particular day (Buy/Sell/Hold) for the Bajaj stock?",313200
93183,how to convert varchar to date ?,318427
93184,Do we need to add below conditional check in the function ? What is the acceptance criteria ? I have added (a) only in the function. a) If date is out of range b) If date is on weekend or holidays c) If input date is not in correct format d) if table does not exist,312479
93185,"hi, when I use the command update bajaj set date=str_to_date(date,'%Y,%M,%D'); I am getting below error. Error 1411 Incorrect datetime value: 31-july-2018.",317410
93220,nan,317514
93221,Update bajaj1 set a.20DayMA = select AVG(a.ClosePrice) OVER(Order BY a.Date AS ROWS 19 PRECEDING) FROM bajaj1 AS a where ROWNUM &gt;19;,320648
92980,nan,312376
92396,"I am using the below command to import LOAD DATA INFILE &#39;/Users/avinash/Bajaj Auto.csv&#39; INTO TABLE bajaj1 FIELDS TERMINATED BY &#39;,&#39; LINES TERMINATED BY &#39;\n&#39; IGNORE 1 ROWS;",318448
92983,"The lost connection has caused all kinds of pause with my assignment. I have tried lots of suggestions available on the discussion forum, StackOverflow and some other forums. Changing settings, reinstall, etc. It has still been a hit and a miss kind of a thing. Out of 15-20 tries, it works once and then the error is back. I am trying to create the master table but been stuck at the step for 3-4 days now because of this error.",319302
93339,In the question they asked about create Table bajaj1 that contains 20 days MA and 50 days MA data. I don&#39;t really understand on which value we need to do computation for 20 days or 50 days MA. Can someday please share a good intuition on how we are calculating these values,315423
92893,"As there is discrepency witht the data types for several columns in the CSV , as a general practice which is advisable : 1. To convert the data types while importing ? 2. Update the tables after loading in all values as either VARCHAR/TEXT ?",308673
93396,nan,314431
93190,"AS we are creating table bajaj2, we need to give a column name as Signal which is already a keyword present in sql, due to that we are not able to put any condition on that column, so is it fine to change the name of the column.",318448
93180,"In Bajaj1 table I have created the columns for 20MA as well as 50 MA but while updating those new columns with acdual values o moving averages, it does not accept AVG function. Can you help?",301121
93191,How to pull null values in top19 MA and top49 MA,306243
93283,nan,305650
93225,Can we keep the first 19 rows for 20MA and 49 rows for 50MA respectively or should keep them empty?,318427
93192,For example I want to update a column with statement as ROW_NUMBER() over(order by b.date) as `row number` Can anyone share me a method or alternative?,318013
93228,nan,320689
93404,nan,318329
93284,"I am using the below concept, with this no Hold is coming. Whats wrong in my logic. IF n &gt; m THEN SET s = &#39;BUY&#39;; ELSEIF n &lt; m THEN SET s = &#39;SELL&#39;; ELSE SET s = &#39;HOLD&#39;;",318814
93193,nan,319770
93227,select 'Close Price' from bajaj; with this command am getting below data which is not correct. Close Price Close Price Close Price Close Price Close Price Close Price Close Price Close Price can any one help out with this error,317410
93195,"Hi TA, Since we have to create 6 separate tables for each of the stocks, is that allowed as part of an assignment to use two generic Stored Procedures to dynamically create and load the respective tables for both question 1 ( Generating Moving Average tables ) and question 3 ( Generating Signal Tables). Otherwise, do we allowed to write ONLY separate six individual DDL scripts for creation and loading data? please advise.",311115
93229,"because for these rows there wont be a previous day value for 50dayMA to calculate the signal. so, it should be kept Null or Hold ??",317998
93196,nan,318397
93231,nan,318013
93238,nan,312518
93427,"In my understanding, we either need to keep the 19 days MA and 49 days MA either blank or NULL. How do we update a certain column for specific rows in SQL? Please do not share the answer for the question in the assignment, but if someone can point me towards the logic/method, i ll try to pick from there. Thanks.",319302
93619,While I am trying to run this SQL avg(`Close price`) over (order by `Date` rows between 49 preceding and current row) as &#39;50_Day_MA&#39; i am getting error Error Code: 1264. Out of range value for column &#39;50_Day_MA&#39; at row 51 Could some MySQL expert provide help,311386
93198,"During the . CSV file load, How to handle . the NULL values which is available in Deliverable Quantity` and `% Deli. Qty to Traded Qty` attribute. #(@`Deliverable Quantity`,@`% Deli. Qty to Traded Qty`) #SET `Deliverable Quantity` = IF(@`Deliverable Quantity` = NULL, 0, @`Deliverable Quantity`),",313526
93199,Ca you elaborate more on key observations and summary and what all to include in this from the tables that we generate and how to submit this file??,318741
93233,Update table with set does not accept AVG function What is the best method to go about?,301121
93200,"For signal calculation, what should be the value of the 50th row? Depending on whether 20DMA is greater or lesser than 50DMA, can we calculate it as Buy/Sell?",318084
93230,Are we expected write a function for creating these 6 tables ? or we can create them individually ?,306011
93350,"In SQL assigment, will I lose marks if i don't round off close price to 2 decimal places ? Please advise TA's.",306735
93136,"If we pass rhe date inside udf (for problem 4), how does it access the signal? If we use select query, then it will give a set of values .",304319
93201,Even after doing required changes changing variable init value to 1 still getting the same error.,318741
93235,"I have created table for different stocks, made the deliverable_quantity datatype as varchar so that I can import 889 rows ! now do we have to update the datatype after the import is done or its okay ?",311466
93202,How can i validate the signals i am generating is correct. shoudl we counts the holds as well? in questions 3 is there any mark deduction if my buy and sell signal matches but my hold signal mismatches by one count or so?,306244
93306,nan,318329
93242,can the imported tables have names with underscore? like bajaj_auto or does it have to be with a space like in the columns given,300748
93104,,312746
93156,"First of all, please ensure that you have MySQL version of 8.0 or above. There are a few functionalities which are used in the assignment which are supported only in these versions. Then for importing the csv files: You can follow any process you like to import the data into MySQL. It doesn't have to be a code specifically, you can use the import wizard to do the same. But please make sure you're not skipping any data from the files given, while you're importing them. And remember that, it is enough to do whatever is specified you don't need to import extra data. 1. Creating new tables: Make sure you are creating 6 different tables as mentioned. And the column names should be exactly the same as given(if given) in the problem statement throughout the assignment. 2. Creating the master table: The stocks are taken from the same NSE website for the same time period. So, all the dates should be the same for all the 6 stocks. To create a master table, there have been timeout errors faced by a few of the folks. Make sure you're optimising the process of joining tables here by detailing the code with keys on which the joining has to be done. 3. Buy/Sell/Hold Please understand the concept of Moving Average(50) right. It is important to understand it perfectly to carry out this task. Here, we need to compare MA50 and MA20. Many of you have asked about the first 49/50 rows. Think about it, if we need 50 data points to find an average, can an MA50 be generated for these intital rows? 4. UDF It is mentioned that this process needs to be carried out only for the Bajaj stock. 5. Summary You're expected to answer this according to the analysis you have performed and the results you obtained. Make sure that the conclusion you make regarding the stock data should be backed by the facts obtained from the analysis you carried out. PS: I hope this helps you guys. Good look for the assignment. And of course, I'll be happy to help if you have any other questions apart from the mentioned. :) PPS: Please do not post your questions as comments, you need to ask them on the Discussion Forum as separate questions. If not, it's difficult for me to track them. Thank you!",319721
93245,nan,318352
93246,"On the SQL assignment, while calculating the cross, if the previous days 20 day to 50 day difference is zero; then do we need to keep looking at the day before that (and so forth) until we make a determination its a cross? Thanks,",317144
93247,I'm explicitly writing code to not generate first 19 and 49 rows for 20 ma and 50 ma respectively. also have written explicit code to not generate signals for the first 50 signals. I hope this is OK and I won't lose marks over this.,300748
93249,When do we get this kind of error?,310522
93357,"I am finding it confusing as to what columns we have to import inside the table bajaj1 when we will create it. as mentioned in question only 4 columns are there, so as per my understanding does all 6 tables will have only 4 columns as mentioned in the question. thanks in advance.",305847
93239,"Hi, Please tell when to use single quote and double quote in sql strings. We are getting the same results when u used single or double quote i.e select revised_date as 'date' from employee; or select revised_date as ""date"" from employee; Please tell as there is a lot of confusion in this",305650
93204,"is it required to load all columns of the csv into a table and then create a bajaj1 table from those values, OR, if we directly load only the required columns into baja1 from csv, wont be an issue right?? please clarify",317998
93367,nan,308432
93365,"0 6 Error Code: 1064. You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ', when (MA20 &lt; MA50 ) and (MA20_prev &gt; MA50_prev ) Then 'Sell' end as ' at line 5 0.000 sec",300687
93348,"WITH UPD1 AS ( SELECT date,ROW_NUMBER() OVER (ORDER BY DATE ASC) as ROWNUM from bajaj1 ) UPDATE bajaj1 as a SET ROWNUM = a.ROWNUM from upd1 as b where b.date = a.date;",320648
93096,"The given data doesn&#39;t consistes for weekends, so for task 4 if we give the date of a sunday, which signal we need to give?",317073
94511,nan,310472
93372,nan,315423
93317,nan,318579
93322,nan,318579
93250,nan,300706
93379,You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;=select `Signal` from bajaj2 In function I have declared a varchar variable like below DECLARE S VARCHAR(20); S=select `Signal` from Table2 where.... What could be the error ? How to resolve it,318814
94657,"I&#39;m using the MySQL for Excel import wizard and this error pops up - Exception message: Value was either too large or too small for an Int32. I tried changing the datatype for data to text, it does not work. I still get the same error.",305657
94659,nan,315831
95309,"The assignment asked us to import the data from excel to SQL. This task was not graded.Also, no where the data type was mentioned. I imported the data in column with data type as date.The solution video imported the data as text and then converted the datatype as date time. My marks was deducted saying that I did not convert the data type. Since, I already imported the data into a column whose data type was date. I formatted the column in the wizard - %y%m%d.There was no need to convert. Also, all my results are correct. Only because of this step, I lost points.",301643
93382,nan,301648
93361,"Hi, while creating tables names of stock is to be taken as table name so we have to take BABAJ AUTO or BAJAJ or Bajaj format to do it. how this will be done. can anyone clarify.",305847
93207,"If I only create a user defined function with only date as input, It is kind of impossible to generate signal asI dont have access to Moving Averages. Or if I access moving averages inside function using a select into from clause it makes no sense since I have to explicitly mention a table name bajaj1 which defeats the purpose of creating a UDF which should work for all tables and all dates. Is there any other way that I completely missed or we simply give UDF the date and moving averages? Also how do I compare previous row moving average inside the UDF for generating signale for current row?",313515
92053,nan,307843
93384,nan,318579
92556,"I have gone through multiple discussion questions regarding this topic and every answer states that Signal should be BUY or SELL only when the 20DaysMA and 50Days MA crosses each other rest of the days should be HOLD. If we consider practicle scenario, we are saying to do market transactions only on the days when there is golder cross or death cross. Rest of the days we should HOLD from doing any market trasactions. Per my understanding, the signal should be BUY at golden cross and remain BUY till the next death cross, similarly it should be SELL at death cross and remain SELL till next golden cross and should be HOLD when the 2 averages are close to each other as the future trend is very difficult to predict. Consider a new investor joins the market in an upwords trend, should we signal him to HOLD or BUY, Similarly if he/she joins the market during downward trend, should we signal to HOLD or SELL. Here is a visual representation of my understanding: The dots represent the signal on 20Days moveing average line and Orange line is the 50Days MA. Red means SELL, Green means BUY and Blue means HOLD. Please share your thought.",306725
93412,"Please note that for the days where it is not possible to calculate the required Moving Averages, it is better to ignore these rows rather than trying to deal with NULL by filling it with average value as that would make no practical sense. Should we remove first 19 rows from 20 Day MA and 49 rows from &#39;50 Day MA&#39; as we cannot do both operation together ?",302877
93409,nan,310507
93398,nan,308640
93414,"While submitting assignment, do we have to include ""create table"" queries which we wrote before importing the csv file.",317991
93186,"First 49 rows of 50 Day MA column doesn&#39;t have exact 50 day MA. Similary, first 19 rows doesn&#39;t have accurate 20 day MA. Their moving average is of lesser rows. What should be the signal in these cases? Buy or sell or hold or empty?",320074
93402,nan,314678
93416,As for first 19 roiws of the column 20Day MA and first 49 rows of the column 50Day MA is going to null. Order of Price Date ascending or Decending?,300721
93439,nan,315423
93419,"I want to know the logic for buy, sell and Hold. I tried couple following logic using Lag, but SELL is not coming for Bajaj1 table WHEN `20 Day MA` &gt; `50 Day MA` AND &#39;Lag_20_MA&#39; &lt; &#39;Lag_50_MA&#39; THEN &#39;BUY&#39; WHEN `20 Day MA` &lt; `50 Day MA` AND &#39;Lag_20_MA&#39; &gt; &#39;Lag_50_MA&#39; THEN &#39;SELL&#39; ELSE &#39;HOLD&#39;",315423
93436,"Hi All, Is there any specific folder to point to data file during the load operation. If not then during verification, they will have to change the path in each sql file for every student and then load. Better if they publish standard folder for data files.",315679
93448,"It is mentioned that "" You will be required to submit the commented a zip file consisting "". What does commented a Zip file means?",304814
93544,nan,318827
93490,nan,318772
93481,nan,318807
93546,nan,311046
93542,"when doing inner join for question 2 in sql assignment, I am getting 'Date' column as duplicate error for the second join Error 1060.",300685
93573,should there be null values in the question 1 sql assignment.,313770
93550,Please check above screenshot,311046
93463,"select Str_to_Date(Date, &#39;%y/%m/%d&#39;) as &#39;Date&#39; from bajaj_auto I&#39;m getting null values.",311169
93483,,318493
92770,nan,314197
93581,Do we have to send the six SQL tables with 20/50 DMA calculated and shown. Do we have to send the mastertable also in addition to what is mentioned in the submission page.,312892
93589,nan,311857
93560,nan,308495
93588,I have used import wizard for importing csv file instead of writing any code is that a problem,312892
93600,nan,311046
93586,Error Code: 1292. Truncated incorrect DOUBLE value: &#39;Close Price&#39; Getting this error while creating bajaj1 table. Also after unchecking the Safre mode option in Preferences its not switching off Safe mode. Can someone help.,303228
93591,Are they going to import the data ? How are they going to create tables,300687
95295,"The points 3.2 and 3.4 seems to be same for me and I'm not clear about the hint given for 3.4. Is it about calculating signal using difference in MA's? If that's the case, I used comparison of previous and current MA's and comparing with the differences doesn't seem to be a simpler logic for me. Is it?",318329
112639,nan,304692
112618,nan,301644
112625,Last Activity Last Notable Activity These two has similar options . I am confused here do we need to make dummy variables for both. If i want to create dummy variables for only selected options and igonore rest. is this possible ? Others after creation we needs to drop whichever is not important ?,312019
112775,"""The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%"" Does this have any significance to the way we choose parameters in our models?",310974
112785,nan,312376
112802,Both are being used to predict probabbilities of our target variable after fitting the model. Then what is the difference between them?,304319
112820,"While doing EDA , I came across certain variables which had no variation throughout the dataset i.e. only a single value was in those variable. Can we delete those variables (clumns) as they are not going to make any difference to our model",318741
112821,"Hi while merging PCs and original datasets I am getting following error. Any help what does it mean and how to solve it. &quot; first argument must be an iterable of pandas objects, you passed an object of type &quot;DataFrame&quot;",313767
112830,Please someone guide me how to deal with large number of Categorical NAN values. As many columns in leading score case study have large categorical NAN values.,316041
112747,nan,304692
112843,nan,301114
114450,nan,318772
112857,nan,318319
112859,nan,315560
114452,There a lot of null data in both the columns but they do contain important data too. can anyone suggest how to impute the missing values,318772
112881,nan,318579
112889,nan,313526
112897,nan,306738
112898,,315560
112900,nan,306738
112926,We can remove Lead Number and Prospect Id . I think 'Converted' is out target variables. That means out y_train will be 'Converted' and rest will be x_train ? Please confirm on this. Do we need another Lead Score metric column ?,312019
112924,nan,306738
112947,"The problem statement says ""The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%."". Does it mean we need to have the accuracy/sensitivity of our model at a min of 80% or we need to keep the probability threshhold/cutoff at .8 to decide if the Converted column is 1 or 0.",310511
112923,"When should standardisation of the data be done? before splitting or after splitting? or, does it NOT matter at all when it is done? In some places i read it is more prudent to split first and then Standardize it. But, in the telecom Churn case study (the one mentioned in course module) the standardisation is done first and then the splitting is done. So, please can anyone explain when should it be done? thank you.",317998
112957,"In this assignment, it is mentioned that &#39;Select&#39; values in columns are to be treated as nulls. But if I replace these &#39;Select&#39; with &#39;NaN&#39; and recalculate null%, null % values remain same. Y is this so?",308437
112958,asking for top 3 variables is question 1 (original variables only? I don't have 3 in my model); asking top 3 categorical/derived variables is question 2. they seem the same/similar. can you please clarify what is the difference in what is expected?,300694
112965,"Here is it related to ""Potential Leads""data in the column ""Lead Profile"". If it is, we have 1970 Nulls which is more than 30%. Thumb rule is to drop columns which has more than 30% Null Values. If it is not related to this data of this column then what is it referring to and how do we approach this question ?",310501
112967,Do we need to use RFE and PCA both in this case study and then decide which one is better for creating final model?,300698
112994,nan,305804
112991,"in prediction model ,probability of every prospect to convert is calculated ,then a cutoff prob is found out ,then prediction is calculated ,,what to do with lead conversion rate ,it is not clear",318005
113012,"While performing transform method on test data set on selected incremental pca variable, I am receiving below error. ValueError: operands could not be broadcast together with shapes (2519,12) (111,) Can anyone help in resolving this error? Below is the code which I am using pca_final = IncrementalPCA(n_components=18) df_test_pca = pca_final.transform(X_test) df_test_pca.shape",304814
113018,Do we need to remove the varible having P &gt; |z| is greater then 0.05 like in linear regression by considering it as insignificant or not? Please suggest.,314361
113019,"After performing model building, I got an accuracy of 73% and got the ROC curve as follows: So, I would like to know that, are both of these things fair enough to be consider? If any of the TAs can throw some light on this, then it would be great!",301655
112945,"Built the model using PCA and LR. The sensitivity(0,81) and specificity(0.80) is quite good. The model looks quite robust. But how to we know which actual features contribute most towards the probability of the lead getting converted?",304319
112546,nan,320687
113034,"Shall we use Sensitivity, Specificity and ROC curve or Precision, recall, AUC curve to find Optimcal Cutoff Probability? Or we need to use both and have two different predictions?",314361
112838,How do we go about cleaning categorical variable with more types of data? Is there any better approach to clean categorical variable?,301649
113072,"In the Q3 and Q4 of the word document, do we only need to justify the approach only in the word document or we are supposed to implement the same in Python notebook as well. TAs please comfirm",314361
113073,We can do scale on the data scale and split train and test. This should be ok? Earlier examples i saw we apply scale on train data and build model. For evaluation stage we apply scale tranform on test data and predict. If ok if we do scale on data and then do test and train split ?,312019
113259,"For example if a column has 100 rows with 70 missing values. The 30 values for example are (Not null values count 30) apple - 10 (or 10/30 = 33.33%) banana - 10 (or 10/30 = 33.33%) pineapple - 5 (or 5/30 = 16.67%) custord apple - 5 (or 5/30 = 16.67%) The above percentages are based on the 'NOT NULL' values. Now my question is can we fill the missing values with n Apples, or m bananas, etc such that the percentage remains same for each, ie. (Null value rows count 70) apple = 70*0.3333 = 23.33 banana = 70*0.3333 - 23.33 pineapple = 70*16.67 = 11.67 custord apple = 70*16.67 = 11.67 The sum counts to 70. Of course the values will be assigned randomly to the calculated number of rows. Is this a right approach ?",318479
113100,"what does the ROC curve representing , model is over fitting ?",305652
113129,"TA varified answer: https://learn.upgrad.com/v/course/208/question/112947 In the above link, TA mentioned that we need to calculate precision to find the target conversion rate. Precision in this case study = leads that we predict as converted / leads who got actually converted. Should the ""Conversion rate of the model"" be as follows? Conversion rate of the model = leads that we predict as converted/ total number of leads. TA, please confirm which one of the above formulas is the correct one ?",310467
113106,nan,318335
113115,After treating columns having 30 % of rows with missing values I am left with 31 column which has 15% of the rows having more than 5 columns as NaN shall remove those columns?,300721
113114,Can we replace by the most occuring value or by mean value or median .Will it affect?Or can we continue without replacing Nan in these columns,308638
113116,nan,318335
113119,nan,305847
113134,nan,300718
113154,"All the below columns have high null values Asymmetrique Activity Index, Asymmetrique Profile Index, Asymmetrique Activity Score, Asymmetrique Profile Score Can we drop all 4 of them or need some kind of imputing? Any hint.",306243
113169,"In my model, when I try to increse the Precision to 0.8, the Sensitivity falls to 0.7. With sensitivity as 0.8, precision is 0.73. Which should be a chosen?",304319
113175,"in the telecon churn example, variables were dropped only based on VIF. Once the VIFs are less than 5, model was freezed. But in this assignment, i have a situation wherein all VIFs &lt; 4 but p-values of 2-3 variables are high - should i drop these variables based on p-value? or go ahead with model?",308437
113190,nan,300721
112439,I actually imported the data from case study and checked out for the null values in each of the columns and I found that some of the columns have 4000 null values which is a very large value when compared with the total size of our dataset which is 9000. So my question is as to how we are supposed to approach this kind of a problem ?,301655
113201,I am trying to find out the count of &#39;Select&#39; in all columns with a single command. Similar to how we check for null values. Is there any such command?,312096
113210,"After performing logistic regression on the dataset and printing out it's summary we get a list of our RFE selected column names with their respective p-values and every other things. But, my question is that after doing this step we predict the values on our train dataset. So, can we consider the values of prediction on the train dataset as our lead score values for each lead ? I would love if any of the TAs answers this in brief!",301655
112613,Can we apply PCA and then do the Logistic Regression ? Because we have many features and once we do dummy variables and mapping yes/no may lead to still more features.,312019
113216,"Since in this study, we are getting large number of variables after dummy creation, is we need to start with higher number of selected features for performing RFE ? Plz help",305650
112919,"Problem statement says Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value. Now if we consider 'select' value as Null for every column where this value exist, then the Null % of those columns increases significantly. And eventually we have to drop that column. By this way we are losing information unnecessarily. The reason for 'Select' value in column could be that customer has not selected any value or forgot to select values. Treating all these 'select' values as Null values might not be a fair idea. And also by doing this we are forced to drop those columns also which might be important for analysis. Need clarity on this So my question for TA is should we treat all these 'select' value as NULL only ? or we can try to impute the 'select' values based on the other values of column ?",317991
112520,This question is regarding sequence of steps: drop unnecessary columns -&gt; treat NAs -&gt; dummy var creation -&gt; outlier treatment this sequence is correct?,308437
113217,nan,318005
113219,Can some one from TA group help us understand the difference between Q1 and Q2 in the word document? I really don't find any distinction.,310974
113221,nan,317073
113223,nan,319951
113231,"When i am trying to run RFE on train data that have 69 columns and continuous variables are sclaed, Jupyter notebook is getting hanged. Only i can see execution in progress but no result. I waited for 5-10 mins. Input statement is: from sklearn.feature_selection import RFE rfe = RFE(logreg, 69) I tried with less features as well but same execution. Any help here .",317156
113234,nan,319951
113237,There is confusion around the expected answer for Q3 and Q4 in the word document. The expected answer is how to tweak the properties of the model to output more potential leads or to suggest how to increase the convert rate of the potential leads given by the model which is completely a business strategy and nothing to do with changing the model in place?,310974
113123,"The CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%. What is the meaning of ""target lead conversion rate to be around 80%"". Which metrics is related to this ? Accuracy / Precision / Specificity ? TAs can you please give insight about this ?",317991
113249,"In the final step of calculating the predicted probabilities, there are some probabilities that are exceeding 1 and some that are below 0 although most are between 0 and 1. Is this a mistake on my end or this to be expected. If it is to be expected, why is it happening?",316416
113262,Once the final model is created after that how to assign a rank to these leads...,300735
113267,"I am getting the below error when trying to build the model : Perfect separation detected, results not available",308673
112525,"Lead case study few columns has more than 45% null values, so should we drop vars or should we drop the rows which having null values??",311004
113275,nan,300716
113293,I have done logisitc regression for the given datset. But after rfe model it has given accuracy score of 79.2 % . The Cut off point calibrate from roc curve and precision &amp; recall curve isn&#39;t giving any change in accuracy score. Is their any way to increase in accuracy score. Currently while performing RFE analysis on the dataset i have select only top 15 columns . Is increase column in RFE model going to improve accuracy score ?,301108
113302,nan,314197
113304,After prediction sensitivity and specificity are coming almost equal ? What does this means? What we studied that they are inversly related. Is my model wrong then ? What could be happening here ?,318479
113308,How de we interpret the answers for the subjective questions in the assignment especially 3 &amp; 4 from our model,316147
113321,For ex: Should we replace NaN by &quot;Unemployed&quot; or something else?,305335
113313,"We are taught to perform the linear regression on train and test data set ( after dropping the result variable, in this case Lead Converted). So train data set gets all the records irrespective of lead was converted or not, and model gives us some important variables to consider. Now, should not we consider only the converted lead records ( i.e. converted=1), and then perform regression to identify the important variables? That should be the true indicator of the variables required to convert a lead.... In short, take the records where leads are converted. perform RFE to identify important varibales, and then predict the output on whole data set. This is just a general question which is applicable for all LR case studies.",304814
112527,There are multiple columns such as current occupation and &#39;how did you hear about x education&#39; with as much as 30-50% null data. Should we straight away drop those columns or do we need to imput their values?,318374
113326,My understanding of problem statement is that we need to write code for lead score after final model. But just need confirmation Do we need write code to calculate lead score ? If yes then on which data set train dataset or test dataset / both ? TA's please confirm,317991
113355,nan,318335
113354,"1. should the cut-off be considered based on accuracy, sensitivity, specitivity or 2. Using the precision_recall_curve function calculated based on the Converted column and its probability ?",312259
113359,nan,300721
113311,"I&#39;m trying to impute the null values or values of &#39;select&#39; basing on the &#39;converted&#39; column having &#39;1&#39; (so that I&#39;ll trace a potential lead better). If I impute the values like that, am I deliberately fitting the curve or making the model better? I don&#39;t have supporting knowledge to justify my approach, can somebody correct me if I&#39;m wrong ?",318013
112519,In the data set most of the variables are categorical. Is PCA applicable for one hot encoded dummy variables ?,318479
113402,nan,320683
113474,nan,310504
113495,We are getting accuracy for test data around 73% and for train data around 50% Cant figure it out where we are going wrong? What is difference allowed,308638
113502,nan,310533
113515,nan,314197
112433,nan,311119
112697,The different values in tag column are not explained un the dictionary.. has about 35% null values . The column looks important. How to handle this column?,300698
112837,"Eg: i would like to impute Click2call in place of NaN values. leads.loc[np.isnan(leads[&#39;Lead Source&#39;]), [&#39;Lead Source&#39;]]=&#39;Click2call&#39; it is throwing me below error:ufunc &#39;isnan&#39; not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule &#39;&#39;safe&#39;&#39;",300723
112932,nan,312093
112981,nan,304695
113121,nan,320690
113215,nan,316891
113292,nan,314197
113394,Do we need to add Lead score to main data (original data) or on the train/test data?.,306243
113224,"If i choose the cutoff value from the intersection of accuracy,sensitivity and specificity curve i am getting a precision of 78%, whereas if i increase the cutoff value i am getting good precision above 0.8. Since it has been said to have a precision of 80% in the assignment, can we take any arbitrary cutoff value which will make my precision fall in the desired range ?",318756
113335,"I notice that in the telecom churn example, different cut-offs have been used for train and test data. I can't seem to wrap my head around this. I understand that we are testing the model and not the cut-offs, but in that case, we should eventually run the model (after testing) on the full set and decide the final cutoff on the entire dataset",305653
113343,"Is it possible to check if my logit model is over fitting? My AUC &gt;95%, and for the predicted values ( test data set) the accuracy, sensitivity and specificity are also &gt;90%. How can I cautiously convince myself that my model is not over fitted?",318479
113164,"Hi, The total time spent by the customer on the website has no unit attached to it. I assume its seconds. Can anyone please confirm?",306736
113509,nan,310504
112778,"How do we handle case of Categorical variables where you have large number of attributes, some attributes frequency are high while frequency of the remaining attributes is very low (each 1% or less) Eg in Lead Source has 21 attributes, the frequency of 5 attributes are above 5% constituting 95% frequency and the occurrence of the remaining attributes are less than 1% welearn, welearnblog_Home, Payperclick Ads, testone, blog, etc It might not make sense to create dummy variables for each of these attributes? They might also not show correlation. Do we delete such rows or replace these attributes to something generic? In that case need to careful that it does not exceed to become significant attribute. Any advice please.",317514
113318,"I am getting an accuracy of 80%, precision and recall to be 75% when the cutoff is chosen based on intersection of precision/recall curve. For the precision to be close to 80%, do I need to decrease the cutoff or do something else about the mode?",318329
112544,How to handle Prospect ID column in the given dataset? Its alphanumeric and I am not sure on how to use it for analysis. Should it be dropped? Any inputs?,310508
114309,"There are some entries in which country is USA/ Canada and city is Mumbai. Is it valid ? if so, then how to deal with null values in country columns",318772
113171,"The column Lead source has few NaN values. I would like to impute values based on the leads['Lead Source'].value_counts() result, which has the highest count. Is this a right approach ?",302750
113315,nan,315560
112576,"After replacing 'select' with NaN, we get many columns with more than 45% of null/blank values, so do we need to drop all those columns or is there any particular criteria to choose the columns which needs to be dropped?",318448
112582,"'Asymmetrique Activity Index' and 'Asymmetrique Profile Index' are related to 'Asymmetrique Activity Score', and 'Asymmetrique Profile Score',. So can we delete 'Asymmetrique Activity Index' and 'Asymmetrique Profile Index' ?",318497
111457,"The target variable, in this case, is the column &lsquo;Converted&rsquo; which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn&rsquo;t converted. Since we have to buid a Logistic regression model here, I understand that here we have to predict the dependable variable &#39;Converted&#39; for the test set. If thats the case, can you please explain the following statement in the light of this context: &quot;Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads.&quot; Are we predicting the value of &#39;Converted&#39; or &#39;Lead Score&#39;? If its the latter that we are predicting &#39;Converted&#39;, what will be the business logic for populating &#39;Lead Score&#39; since the dependable variable &#39;Converted&#39; will be having either &#39;0&#39; or &#39;1&#39; as values? Any help is appreciated.",310511
112626,"In the data set, ""Page Views Per Visit"" column has decimal values which doesn't make sense. This need to considered as data cleaning?",310974
113310,"logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial()) logm1.fit().summary().....Any one can help on this ?",318814
112665,"When we combine the select and NaN values of the column 'Lead Profile' , the resulting number is pretty big (6855). So should we drop the whole column of 'Lead Profile' or is there any other way out someone has tried and succeeded ?",301655
112663,"For &quot;Lead Scoring&quot; Case Study: 1. Will it be logical to impute &quot;Other&quot; value for missing values in columns like &quot;Specialization&quot; , and &quot;What is your current occupation&quot; ? 2. Does it make sense to create dummy variables for such columns and drop the missing value dummy variable ?",308440
112675,"While trying to imput Null values and data cleaning, it was noticed that we have the data value as &#39;Select&#39; under many columns like&#39;City&#39;, &#39;Lead Profile&#39;, etc... can anyone help me understand the signifiance of this value? it may give some clues on imputing the NULL values, hence thought would check",316036
112678,"Generally ""Select"" is an option on a web form / survey form and could depict that the user has not selected any value(s). This may be considered equal to Null. Another perspective is that ""Select"" is an odd one out in all such columns. e.g. ""Select"" in column ""City"" means that the user has not selected any City from the given options, which may translate to a Null value. Thoughts?",318078
112676,"There are some columns in the data that have very less variations e.g. in columns ""Do Not Call"" almost all values are ""No"" (except 2). Likewise ther are other similar columns as well in the dataset. I hope that we can easily drop these variables as actually they don't appear to be variable but a constant. Please suggest",311729
112743,"As per the data dictionary: Last Notable Activity: Last notable acitivity performed by the student. Last Activity: Last activity performed by the customer. While checking the data, the main difference that I found are There is an aditional entry in Last Notable Activity, &quot;Modified&quot; . For 5k customers, both these fields have same values. For 3k customers, Last Notable Activity is &quot;Modified&quot;. Can we get some more details of these 2 fields? What is the significance of the value &quot;Modified&quot; in &quot;Last Notable Activity&quot; field?",310467
112930,"I tried this way for scaling normalized_df=(RFM-RFM.mean())/RFM.std() This made changed the few of the dummy variables, which is not suppose to happen, because dummy variablces have values 0,1 after creation. Not what happed here ?",312019
113294,nan,317992
113554,nan,312448
112944,"Do we needs to do the heat map with correlation including output variables converted ? With respect to Converted we shoudl check the correlation value, if its high that variables can be dropped ? If is correct ? or directly use RFE or PCA to reduce the variables ? Because too many variables using heatmap correlation difficult to decide.",312019
113158,nan,302741
112537,May I please request the TA&#39;s to kindly suggest a detailed approach for the Lead Scoring Case Study as early as possible. This is to ensure that we are following the right path. Since its a group study co-ordiantion and updating the changes becomes time consuming. Approach from TAs will really help in getting all of us on same page. Thank you!,310508
114252,Country Specialization How did you hear about X Education What is your current occupation What matters most to you in choosing a course Tags Lead Quality Lead Profile City Asymmetrique Activity Index Asymmetrique Profile Index,318772
114302,such as Do Not Call Search MAgazine Newspaper Article X Education Forums Newspaper Digital Advertisement Through Recommendations Receive More Updates About Our Courses Update me on Supply Chain Content Get updates on DM Content I agree to pay the amount through cheque these column contain 99.5% of NO,318772
113225,"Please suggest me. When I enter leads[&#39;Converted&#39;], I get the above error.",310522
121784,"The list wnt on and ended like this. So, I updated all the packages using 'conda update --all' After thi I again tried installing graphviz but in vane. Due to above not able to run the notebook. But I see that anaconda has this package ...so why ?",318479
120700,"Let&#39;s say we have 4 categorical variables in the data set with each having 5 different categories. So to determine the condition at the root of the tree, the number of iterations it takes is 20 with in each iteration calculating the gini index of the split? If you combine this with the hyper parameter tuning which let&#39;s say has 100 split based on different combinations of hyper parameters, to determine root condition it would take 100 * 20 = 200 iterations just to determine the root condition. This essentially repeats with a decreasing number of iterations but happens at every stage of tree split. This is computationally very intensive. Is my understanding right?",310974
122781,"# plotting tree with max_depth=3 dot_data = StringIO() export_graphviz(dt_default, out_file=dot_data, feature_names=features, filled=True,rounded=True) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_pdf(""dt_heartdisease.pdf"")",320685
122786,Facing the below error even after installing graphviz.,314313
122394,While plotting the tree it s showing the below error. Please help InvocationException: GraphViz's executables not found Enivironment variables Even the PDF is getting generated but unable to open the document. Please help,314313
126103,nan,315464
115929,"I have added environmental variable, how to install pydotplus and graphviz? do I need to use cmd prompt. Install python graphviz package- pip install graphviz Install pydotplus - pip install pydotplus",310463
120509,Can someone share the picutre please,319759
122154,nan,308673
121966,I have tried with conda and pip install. I get the message - Successfully installed pydotplus-2.0.2 but I still get the error - No module named 'pydotplus' when I run import in the notebook. Any one else face the same issue? Any help / tip is greatly appreciated.,317149
121248,,308495
119674,How many leaves does the tree have? how it has 8 nodes..there are 7 nodes only which we can&#39;t split futher!!,318322
121275,nan,319951
120440,"! pip install graphviz  ! pip install pydotplus Requirement already satisfied: graphviz in e:\anaconda\lib\site-packages (0.10.1) twisted 18.7.0 requires PyHamcrest&gt;=1.9.0, which is not installed. You are using pip version 10.0.1, however version 19.0.3 is available. You should consider upgrading via the &#39;python -m pip install --upgrade pip&#39; command. Requirement already satisfied: pydotplus in e:\anaconda\lib\site-packages (2.0.2) Requirement already satisfied: pyparsing&gt;=2.0.1 in e:\anaconda\lib\site-packages (from pydotplus) (2.2.0) twisted 18.7.0 requires PyHamcrest&gt;=1.9.0, which is not installed. You are using pip version 10.0.1, however version 19.0.3 is available. You should consider upgrading via the &#39;python -m pip install --upgrade pip&#39; command.",314678
121481,nan,301890
121515,nan,320687
121536,nan,301643
120981,I&#39;m unable to understand one test /node.,320687
123241,"the error receiving in command prompt is &#39;conda&#39; is not recognized as an internal or external command, operable program or batch file. Please help and environmental path file is sucesfully done.",320606
121642,i am getting above error while running command &quot; graph.write_pdf(&#39;dt_heartdisease.pdf&#39;) &quot; I do get a pdf file (0kb) in location but ...when I open it... prompt ...file corrupted i have installed graphviz and pydotplus and added both the environment varibles,318377
123248,,320606
121471,"In Question 5: If the algorithm predicts that a person does not have heart disease, and it is known that she has &lsquo;Thal&rsquo; &gt; 4.5, then which of the following conditions has to be TRUE? More than one option may be correct. As it is asked which conditions has to be TRUE that means they are true for all branches of the three: As we go right from the root node, first test can give 2 results, so Flouroscopy.coloured can be less than equal to or greater than 0.5. In the first case when it is less than equal to 0.5 Exercise.angina is tested but not BP In the second case when it is greater than 0.5 on BP is tested and not Exercise.angina As both the branches are giving +ve and -ve results, how can we be sure which conditions must be TRUE. Considering 2nd case for a +ve hear disease, we know BP must be &gt;109 but we are not sure what is the value for Exercise.angina. Not sure if i am able to explain my doubts :(",306725
123415,"# plotting tree with max_depth=3 dot_data = StringIO() export_graphviz(dt_default, out_file=dot_data, feature_names=features, filled=True,rounded=True) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_pdf(""dt_heartdisease.pdf"") Geeting error in this last line. Can anybody help me with this error? All the modules are installed and i can see them using 'pip list' command. Thanks!",317981
121362,"I unzipped files and copied to program files directory. Then added environment variables. Then, I installed pydotplus first and then graphviz next using the follwoing commands: 1. pip install pydotplus - installation was successful 2. conda install graphviz -installation was successful Now i am running decision tree example. I am getting the above stated error. Why is it coming? If i want to uninstall what is the command?",306734
121608,All the basic instructions given are already followed. C:\Windows\system32&gt;pip install graphviz Unknown or unsupported command 'install' C:\Windows\system32&gt;where pip C:\perl\perl\bin\pip C:\perl\perl\bin\pip.bat This turns out to be perl pip but not python pip . Searched for pip.exe on the system and found one in D:\ProgramData\Anaconda3\pkgs\pip-10.0.1-py36_0\Scripts. Added this to the Path and got some other error. C:\Users\j1012496&gt;where pip D:\ProgramData\Anaconda3\pkgs\pip-10.0.1-py36_0\Scripts\pip.exe C:\perl\perl\bin\pip C:\perl\perl\bin\pip.bat C:\Users\j1012496&gt;pip install graphviz Unable to create process using 'D:\ProgramData\Anaconda3\pkgs\pip-10.0.1-py36_0\ python.exe D:\ProgramData\Anaconda3\pkgs\pip-10.0.1-py36_0\Scripts\pip-script.py install graphviz' The 'New Terminal' option in the jupyter notebook brings up a terminal on web page but has message [CLOSED] on it. I am not able to type any commands there. I have also installed both packages using anaconda navigator and that is done without errors. This did not help anyways. Please help as I am stuck here and not able to proceed.,318007
138453,nan,311466
122292,"Why do we check the performance of tree after pruning only on ""Validation set""??",316349
122705,"We have two parameters min_samples_split and min_samples_leaf . Basic idea behind them looks similar, how to differentiate between them ?",318770
123026,nan,300733
122141,Can we use label encoding for solving linear and logistic regression problem ? Or it is used specifically for decision trees ? Plz tell,305650
121986,"I am trying to use import pydotplus, graphviz but getting below error: ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-31-3150bf65b2fb&gt; in &lt;module&gt;() 3 from sklearn.externals.six import StringIO 4 from sklearn.tree import export_graphviz ----&gt; 5 import pydotplus, graphviz 6 7 # Putting features ModuleNotFoundError: No module named 'pydotplus'",300735
122410,nan,318429
119784,Every node in generated image has 4 following things. 1. Attribute name 2. gini 3. samples 4. values Can someone please explain what `value` signifies or how is it generated,306248
121967,Is there difference between labelencoder and astype(&#39;category&#39;)? if they are same can we use one inplace of other,301113
121730,"The snapshot below shows error in plotting the decision tree. I have followed all the steps for installing graphviz and pydotplus. The heart disease Python notebook did not have an issue, but the income prediction notebook has. The path has been specified and all environment variables are present. Any solution to this? Note: As mentioned above, there was no issue in running the heart disease Python notebook provided where the decision tree was generated. So all the required packages have been installed and the required paths have been specified in the environment variables.",310505
119882,what does this mean?,317984
121304,nan,301114
122188,trunc,308495
121438,In te graded questions - how do we construct a tree without coding?,319759
121451,Time exceeded error comes up while executing coding question 2 while the same gets executed in Jupyter notebook,319759
123123,nan,312093
121520,,304692
123418,nan,308673
122118,"One of the advantage of decision tree is, normalization not required; can someone explain the reason why it is not required?",316349
122510,in this node i have 12 data points.. i can split them on criteria like a1&lt;= 6 and get 6 samples in each leaf.. thereby satisfying condition min_samples_leaf = 5 but this is showing wrong answer .. why?,318791
126526,nan,318455
123666,"I'm doing this small project for my client where I've been asked to predict the right time to reach out to a customer and I'm planning to keep the date time variable (shaded in yellow) as one of the features for my RF model. However the challenge that Ive over here is how do I treat this date time variable. Should I convert it to numeric and if at all numeric then what should be the methodology or should I divide it into 3 or 4 separate variables such as day of the year, month, year etc. Any suggestions, advice, help would be much appreciated. Thanks in advance.",318455
120715,How is aggregation in random forests a mean when it should be mode as it takes the majority score of all the predictions.,310974
122714,"I haven&#39;t changed anything in the notebook, just run the given commands. What could be the reason?",318335
122264,"In the calculation of OOB error, all the observations in the training dataset is used. This is however, based on the assumption that every training data set is not part of atleast 1 tree which did not include it as part of the sample. Is this assumption always correct esp. if the number o trees in the ensemble increase?",310509
122740,nan,304692
123983,"In one of the lectures, I remember prof mentioning it as an advantage that, there is no need for test train split for Random forests and this is one of the major advantages. But, in the Python notebook, can still see that, there is test train split done, Am i missing something?",314084
122289,"Couple of clarifications: 1) Why was the data split in training and test when creating random forest in the credit card example, when in lecutre its mentioned its not required? 2) how to calculate the OOB error from python for the model?",310509
124858,Please explain the above line.,320687
122261,can the concept of ensembles be extended to other regression models? e.g. create random linear or logistic regression models and pick those which give best R2 or accuracy figures?,310509
123567,"When I plot max_Depth and accuracy, I get a blank chart.Any thoughts?",301643
123286,nan,318770
123552,nan,300721
123570,nan,300721
123577,nan,308673
123473,nan,300721
123571,"eg - param_grid = { &#39;max_depth&#39;: [4,8,10], &#39;min_samples_leaf&#39;: range(100, 400, 200), &#39;min_samples_split&#39;: range(200, 500, 200), &#39;n_estimators&#39;: [100,200, 300], &#39;max_features&#39;: [5, 10] }",300721
127468,nan,300733
138148,nan,311466
122885,I could not understand the entropy and information gain concepts clearly. Are there any online material which could walkthrough this concept with example so that I can relate how entropy / information gain effects the homogenity measure.,314730
122666,in the question asked in the video as explained by Professor Raghavan. How did we get 0.7 and 0.3 in the equation,301124
121768,nan,313526
121773,What is the ideal Value of threshold for homogeniety in data points?,316349
121412,nan,304692
123411,nan,312623
121926,nan,318433
138152,"A dataset of 100 data points in it with 4 binary attributes are = (A, B, C, D) Now, the dataset has a binary class label with 20% of the datapoint taking value 1 for the label and the rest take value 0. The fraction of datapoint with a given label that has value 1 for attribute A is shown in below (table) The gain in Gini index if the dataset is split on A",311466
138151,"A decision tree of depth = 1 and the binary label has two leaves of equal partition size during training with each partition having an entropy of -0.2864 So, The expected accuracy of decision tree would be approximate? TA please provide a proper solution to this question with all possible comments (On paper)",311466
110743,Given the number of unique variables(n) how do you find the number of Principal Components (k) chosen in the final model? Why k is rational number and not integer?,317514
110993,"This not clear for me, what we have to explain here, is it road map for PCA?",301113
110774,nan,303228
110775,nan,303228
110772,No loop matching the specified signature and casting was found for ufunc svd_n_f,317410
110363,"if the vector is 3i + 4j and the new i is (0,1) and new j is (-1,0) then the co-ordinates of the new point P should be (3*0+4*-1, 3*1+4*0) =(-4,3). so , how come answer shown is (4,-3)?",310509
109953,nan,311117
110720,Can we have a online session on SVD &amp; it's mathematical formula understanding?,318827
110838,nan,320251
110722,"How the matrices U,s,V are obtained?",318448
110726,nan,308434
110817,a detailed theory of PCA https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial,318017
110646,"In the videos it was shown that a matrix A(m*n) can be decomposed into U (m * k), S(k *K) and V(k*n) matrices. But when we run the code np.linalg.svd(data, full_matrices = True) the matriX A (12 * 9) get decomposed to U (12 * 12), S(,9) and V (9*9) which is quite different from the theory. Why is it so??",304319
110834,nan,320251
110770,"Getting errotrwhile executing below command - U, s, VT = np.linalg.svd(MFR, full_matrices=True) TypeError Traceback (most recent call last) &lt;ipython-input-20-68b66ab40115&gt; in &lt;module&gt;() ----&gt; 1 U, s, VT = np.linalg.svd(MFR, full_matrices=True) ~\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\linalg\linalg.py in svd(a, full_matrices, compute_uv) 1442 1443 signature = &#39;D-&gt;DdD&#39; if isComplexType(t) else &#39;d-&gt;ddd&#39; -&gt; 1444 u, s, vh = gufunc(a, signature=signature, extobj=extobj) 1445 u = u.astype(result_t, copy=False) 1446 s = s.astype(_realType(result_t), copy=False) TypeError: No loop matching the specified signature and casting was found for ufunc svd_n_f",304812
110301,nan,311857
110843,nan,303228
112996,"For a dataset that has large number of binary features (0 and 1) along with continuous variables, is PCA the right tool to reduce dimensionality for further model building? If yes, then shuold the binary features be standardized (mean 0, Std. dev 1)? If No, how can we achieve dimensionality reduciton?",318438
110700,"The point (3,4) in a 2-D space can be represented as the vector 3i + 4j where i and j are basis vectors with the coordinates of (1,0) and (0,1). If we rotate the basis vector 90 degrees anticlockwise, the new position of i becomes (0,1) and that of the new j becomes (-1,0). What are the coordinates of point P in the new vector space? Per my understanding, the new coordinates should be (-4,3) both if we visualize and matrix transformation as below: |0 -1| |3| |1 0| |4| The above matrix transformation would also give us (-4,3) coordinates. Can someone help understand what am i doing wrong.",306725
112716,using pca we can reduce the number of variables required to explain the variance. for example in the class video we had 30 variables and using pca we came to know 14 variables is only required to explain 90+% variance.. so the question is how to identify which are the 14 variables form the original 30 variables that are important? hope that you guys got my question.,311032
110953,"In the recommender system using SVD in our module, how we derive various themes and how disfferent matrices (USV) are calculated ? Can anybody plz explain ?",305650
110648,nan,311006
109891,nan,304692
110141,"Query about PCA &amp; dimensionality reduction.. ex: We begin with p= number of variables in the dataset&gt;Perform PCA&gt;now choose k components which is less than p&gt; thus achieve dimensionality reduction.. In a way,does this mean we drop p-k =x number of variables from the dataset..? Just wanted a clarty on this..",310508
110689,nan,318804
110426,"It is mentioned in the video that PC is the line which captures the maximum variation and closest to the points. Is it not the same as finding the line of best fit in a 2D space? if yes, can it act as the starting point for PC1?",310509
110024,"Why is this option not applicable - "" If one of the basis vectors passes through the point, then there are four possible coordinates for this point"".",311857
110438,How to find the first diagonal entry in the s matrix?,318846
109864,nan,312942
109900,How to find if 2 vectors are linearly independent?,319759
110637,I saw many of us are facing difficulty to understand PCA completely. I request to TA's that can a live session be arranged on PCA with some more examples and detailed explanation of code provided in the python implementation ?,317991
111982,gdpp is given w/o population figures. how to get absolute export value?,318827
117354,I did not understand how to infer the new themes derived from SVD. The users to themes and books to themes explained in this module is not clear. Can anybody provide any references or explanation around how we can infer which theme referring to which variable / feature and what it signifies.,314730
109849,Any idea for below error? TypeError : No loop matching the specified signature and casting was found for ufunc svd_n_f,300718
110441,"Matrix A is the new basis vector and point P = (5,0) is in this basis space. So, in order to find Point P in conventional basis ( 1 0, 0 1) , shouldn't we multiply ( 5,0) with (1 0,0 1)",317996
111013,"Can anyone tell me what&#39;s the problem with below code? import numpy as np import pandas as pd data=pd.read_csv(&quot;Downloads/MyFoodRatings.csv&quot;) u, s, vt = np.linalg.svd(data, full_matrices=True)",314629
141903,"%matplotlib inline fig = plt.figure(figsize = (8,8)) plt.scatter(pcs_df.PC1, pcs_df.PC2) plt.xlabel(&#39;Principal Component 1&#39;) plt.ylabel(&#39;Principal Component 2&#39;) for i, txt in enumerate(pcs_df.Feature): plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i])) plt.tight_layout() plt.show()",310179
110736,nan,311952
110661,"What is the value of the highest correlation amongst the explanatory variables? I didn't get this question, what is exactly the meaning of explanatory variables, can anyone please help and elaborate on the same ?",301655
110288,What is exactly themes as Rahim has talked about the same in the video on SVD. Can anyone please elaborate regarding the same!,301655
112027,When we do PCA implementation on a dataframe . Based on cummulative explained variance we decide number of PCA required and do component analysis . But after doing analysis how do we decide which columns need to be taken for further clustering and which need to be avoided?,301108
110491,video and audio are not at all syncing in the session: PCA in Python videos.. Its getting irritating..Why these issues are not addressed before uploading? Lecturer is explaining something and the something else is appearing in the video - maybe there is a LAG..,308437
110829,what is the significance of random state command in the model? sometimes it is selected as 100 andf sometimes as 42...how to interpret this and what&#39;s the best way to select the same?,310509
110835,"Since, Scaling and Centering is a pre-requisite for using PCA, does it mean that only standardization method of scaling and centering is allowed? can MinMax scaler also work with PCA?",310509
110621,Need clarity if PCA can be applied on both Categorical and Numerical Variables?,316036
110876,nan,308434
110926,what is the purpose of making the value of diagnoal 0 when finding co-relation?,310509
111219,"from sklearn.decomposition import PCA pca = PCA(svd_solver=&#39;randomized&#39;, random_state=42) pca.fit(normalised_df) Getting this error after using these commands to do pca.",305655
111000,"in PCA Example, where he gave a 3d PLOT i got an empty space, any one know what to do for this?",317073
110542,None of the options provided for the graded question 4 matched the answer that I got after executing the correct code. Did anybody else also encounter the same??,313826
111041,Need some review comments on SVM model post PCA (without dimension reduction). Problem Statement - Accuracy of model is 9% Snippest of code Any review comments are welcome :),307495
111051,nan,300721
111082,"Hi Team, What is the significance of using random state in every python code while performing any performing any predictive analytics ?",305650
110697,Can PCA feature selection technique be used in Linear Regression like Logistic ?,305652
113244,"I have dropped the column in pca assignment, at last of the assignment it is difficult to extract the country column and concat again. so, i want to pick the column in initial data frame, say df1 and store in that. if will use this df1 at the end to concat it with the cluster df..how do i do it?",320606
110558,We have Label Column missing in the test data provided. how to handle that?,316036
111908,Perfomred PCA and got the pripncipal components. How do we extract the feature names?,303666
111108,nan,311803
112070,nan,314612
110619,Can anybody share the link for the highlited part?,310508
111419,The model with X_train[ Columns] is commented,317984
111745,"Ask of the assignment is to get a combination of Principal Components and important variables. How to get this combination, it seems like people are using only principal components for clustering",300735
111617,"Did anyone complete the MNIST assignment? I started. But, my system always hangs up.",301643
112093,"The output of the matrix having 16 columns or 14 columns when fit with PCA, I am confused about how the number of variables/features is told as number of components by instructor. Component is a linear equation of features with weights right? 14 columns/features/reduced-features does not mean 14 principal components right?",318007
141905,corrmat = np.corrcoef(df_train_pca.transpose()),310179
140144,Generally clustering is done using 1 atttribute at a time. but what if a product is to be launched where clusters involving multuple attributes make more sense...how can that be perfomed?,310509
140289,nan,300706
141058,What does the cumlift column signify?,304319
141082,"The net profit, i am getting is 1.23 million after subtracting the cost on campaign. But there is no matching option. I have taken 10% of the total approvals i.e 2717.",304319
141358,"Data dictionary specifies below variables are monthly or quarterly based which means it has to be a fixed value throughout a particular month Employment variation rate - quarterly indicator (numeric) Consumer price index - monthly indicator (numeric) Consumer confidence index - monthly indicator (numeric) If that so, when we look at data for a specific month, can see more than one value for these variables. So does that mean this data is from different years ?",316147
141174,nan,302742
140532,"in the lecture, it was mentioned that the standardization on the x_test data should be done using X_train means, as the model is built on x_train. However, the codes showes x_test means being used. so, not clear about the approach here?",310509
140714,"In video 2 of EDA-II, at 8.12 mins, the prof explains the campaign overlap insight. How did he infer that there was an overlap in the campaign? Couldn&#39;t understand it.",304319
140806,What is the use of cumulative lift shown in the python lab,316147
140776,nan,304692
141727,nan,311952
140151,"I am not sure how the macro economic data ( such as consumer confidence index, euribor, etc.) be mapped at an individual level and also varying across individuals? what does it signifiy?",310509
141665,nan,302877
141667,nan,302877
141651,nan,301646
141348,nan,312093
141378,nan,317575
141136,How do we calculate the cost of each call,318804
141397,nan,300721
141160,"i did not understand the question below. can some explain me where do we get the prospect id of a varable. Create a data frame with the variables prospect ID, actual response, predicted response, predicted probability of response, duration of the call in seconds and cost of the call",302741
141390,,302742
141340,,304319
141440,In the logistic regression we will only derive the probablity of response. The duarion column is dropped. How to calcualte the cost?,302741
141339,"Also, set the business objective to achieving 80% of total responders at the minimum possible cost. The total number of responders is the total number of prospects who responded, from the available data of about 45,000 prospects. This should be on the test set or train set?",304319
140722,"Sort the prospects in order of decreasing probability of response (predicted by the best model) and target the top X% (or top Y deciles), where X would be determined by your business objective (e.g., maximising the overall response rate/number of responders at a fixed marketing cost).",301108
140838,nan,310419
140604,"Hi, Given that DURATION variable can be removed, so along with that can we remove other varibles such as &#39;DAY_of week&#39;, &#39;month&#39;. etc.. OR it is compleltly our decision. thanks.",317410
140884,What are the usual methods they are refering to here?,302741
141273,Do we need to find out the optimal cut-off point or go ahead with 0.5?,304319
141279,"Can we use PCA for dimensionality reduction and build logistic regression on top of that ? or is it mandatory to do RFE for feature selection and eliminate feature variables one by one checking the p-value , since it will be a lengthy process then. Anyone please confirm.",318756
141816,emp.var.rate: Employment Variation Rate - Quarterly Indicator cons.price.idx: Consumer Price Index - Monthly Indicator cons.conf.idx: Consumer Confidence Index - Monthly Indicator euribor3m: Euribor 3 Month Rate - Daily Indicator nr.employed: Number of Employees - Quarterly Indicato,310179
141502,where exactly we use the sought function and drop the duration attribute?,320606
141503,The cost of acquisition is 1 * (no. of people contacted) as given in the problem statement. What is the significance of the Cost of the call in the analysis?,304319
141825,emp.var.rate: Employment Variation Rate - Quarterly Indicator cons.price.idx: Consumer Price Index - Monthly Indicator cons.conf.idx: Consumer Confidence Index - Monthly Indicator euribor3m: Euribor 3 Month Rate - Daily Indicator nr.employed: Number of Employees - Quarterly Indicator,310179
141507,Which dataset we have to use for assignment in BFSI domain elective.,310598
141514,"For checkpoint 3 and checkpoint 4, which dataset to consider ? Do we need to consider one of train or test data ? Or we need to consider whole dataset ? TA can please clarify on this.",317991
141526,"&quot;Also, set the business objective to achieving 80% of total responders at the minimum possible cost. The total number of responders is the total number of prospects who responded, from the available data of about 45,000 prospects. Based on this information, calculate the X in the top X%, i.e., how many prospects should be called to meet the business objective.&quot; In the above statement what exactly does the X imply?",312942
141527,nan,318802
141642,nan,305334
141853,Even on converting categorical data to dummy variable for the &quot; loan &quot; DataFrame still the X_Train contains the same Categorical data and also on printing the X_Train.columns print the old same list of columns and not dummy variable columns. Although for X_Train . head prints the dummy variables. Can anyone explain it.,310179
141571,from imblearn.metrics import sensitivity_specificity_support,300721
141882,nan,314612
141884,What is the significance of Cost of acquisition as we already have Cost of call calculated in step 3? And although it is mentioned that Consider cost = 1*number of contacts made in the current campaign; But it is still not clear how to calculate cost of acquisition? Can TA please clarify on this.,317991
141660,nan,308432
141890,,320687
141590,nan,300721
141529,similar process to create decile df and use this for checkpoints.?,318802
141536,nan,308432
141604,nan,318429
141915,"ValueError: operands could not be broadcast together with shapes (12357,51) (50,)",310179
141835,"Do we need to perform standardization on both X-Train, X- Test. In assignment why had they performed only on X-test.",310179
141887,nan,320687
141912,"ValueError: operands could not be broadcast together with shapes (12357,51) (50,) df_test_pca = pca_final.transform(X_test) df_test_pca.shape",310179
93753,nan,306244
93731,"Hi, Based on which column we need to merge two dataframes if nothing we can use concat function",317410
93756,"Hi Team, How we will find companies in the rounds2 file which are not present in companies table ? What is the approach you are using ?",305650
93628,nan,315679
93629,"During the Investment Group Case Study ...one question came up regarding the stage at which you should cleanse your data? Since here we are working with companies and rounds2 dataframes...there could be 2 approaches... 1. Clean the component dataframes (companies and rounds2) and then merge 2. Don't cleanse the component dataframes, direct cleanse the merged dataframe (master_frame). Based on the adopted approach the answers to some questions may vary. What is your suggested approach here?",306250
93630,Can anyone suggest what&#39;s going wrong?,312756
93636,"@ Basavesh Hiremath, Bishnu Agrawal, Deepak Padhan - we need to connect to discuss. I dont have your contact details. Please connect with me on 9986033095 over WhatsApp. @UpGrad, please help to connect.",313228
91393,After forming the Group for case study-How to communicate each with group members i mean through whatsapp /Discussion forum/Mails or else any other way? And how can you come to know that we are discussing each other and who is he active participant all? Pls confirm,320008
93634,Merge the two data frames so that all variables (columns) in the companies frame are added to the rounds2 data frame. Name the merged frame master_frame . How many observations are present in master_frame?,318846
92461,Are we supposed to load the pandas output to mysql or extract excel/ csv output to access through Tableau?,306250
93764,nan,318814
93631,can anyone please suggest how you people are driving the task in spark fund case study?,306244
93685,please confirm if this is a genuine mistake or is it purposely added for us to take care ?,305845
93687,For filtering out the top investment countries do we need to consider latest investment dates or just the investment amount.,301641
93692,"As per above question in section2.1 is it stating that Spark Funds want to invest between 5 to 15 million based on respective average investion of angel, seed &amp; private equity . Please correct me if i am wrong.",301108
93742,nan,318579
93696,"Guys, when I am trying to get the average of the funding value of different types of funding I ma getting the result in Scientific format. How can I conver the same into decimal format in Python. Values like ""1.687046e+08"" to be needed to convert into decimal format. Please help.",313676
92425,nan,310009
93708,nan,313826
93743,"After creation of master data frame, do we need to drop the rows having NaN in raised_amount_usd or replace it with 0?",320103
93709,"The description of the 3rd Tableau plot is &quot;A plot showing the number of investments in the top 3 sectors of the top 3 countries on one chart (for the chosen investment type FT). &quot; Apart from considering the chosen investment, do we also assume our results based on the fact that the top 3 countries should be english speaking and funding amount should be between 5mil and 15mil USD? These are not mentioned explicitly anywhere in checkpoint 6.",310511
93710,"There are some primary sectors in master dataframe which doesnt have corresponding main sector value in the mapping dataframe, even after doing all possible data cleaning of mapping dataframe. Can we impute null values of main sector (after merging to primary sector) to &#39;Others&#39; ?",310511
93752,"In attachment Investments.xlsx of section 4 Excel File (mandatory submission) , Table 3.1 questions are related to English-Speaking Countries. I don&#39;t see any column/data related to languages So, How do we determine the spoken language of a country?",315101
93726,Use the mapping file &#39;mapping.csv&#39; to map each primary sector to one of the eight main sectors (Note that &lsquo;Others&rsquo; is also considered one of the main sectors) Expected Results: Code for a merged data frame with each primary sector mapped to its main sector (the primary sector should be present in a separate column). i didnt understand this part?,303674
93727,Do we need to calculate the check point 1 table after inserting the csv or after cleaning the data (missing values etc) ? Thank you.,305652
93763,How can i establish which code type is present in data and what encoding i should use.,306244
93612,"1. Checkpoint3, we have to create a datafram of english speaking country and compare with master_frame and find out the top 9 companies by usd funding right? 2. Checkpoit 4 onwards we have to filter out the English-speaking countries and start the analyses on the filter out dataframe. Right?",306243
93606,,310522
93271,nan,317990
92787,what do you mean by observations?,301113
93768,nan,315633
93778,nan,318579
93784,"On the null values in a few columns, do we have to replace the null with ""more occurring value"" or remove the rows completely? If either of the approaches are fine and can be used with a proper justification, then it is fine otherwise If we know the business expectations(like we knew during IMDB Movies assignment) , then it is easier to choose the right approach.",312758
91084,"For example consider, Analytics, It does not exist in the mappings.csv as &#39;category_list&#39; but itself it&#39;s defined as a primary sector. Now if you see the company data, H2O.ai is mainly an analytics firm but if you do a join it might raise a NaN vale. So, should we fallback to sectors or use the category as a primary sector if no match is found?",318381
92990,"Below is the command being used and for both cases, companies = pd.read_csv('companies.txt', sep ='\t', header = None).permalink.str.encode('utf-8').str.decode('ascii','ignore') rounds2 = pd.read_csv('rounds2.csv').company_permalink.str.encode('utf-8').str.decode('ascii','ignore') i am getting error in both cases still. UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 25: invalid start byte Please suggest how to resolve",310509
93827,do we need to create first df top9 which have all speaking language countries. then find out top 3 english speaking countries from the top9. or top9 should be created with top 9 english speaking countries only.,311952
93828,nan,303673
93832,nan,314197
93834,nan,311952
93835,nan,315633
94249,"but it would hv been better if they would hv been dropped before country analysis because in the end ,,the analysis is on whole factor country,raised amt ,sectors ... i hv slight confusion regarding this now look at data ,,,which student is having highest total.....it should be b if we drop second row containing blanks subject ,,,but it will be a if we ignore it during highest marks calculation student subject marks 0 a x 100 1 a 50 2 a y 50 3 b x 180 4 c r 90 #################",318005
93838,nan,318455
96473,"Got feedback for Country and Sector analysis as shown in screenshot. Clarification required is for ""Also the category_list column values need to be converted to either uppercase/lower case format in both the files. You have missed this part. "" Was this a required step.? Without doing this step also we got all the findings same as sample solution provided. So should we apply for re-evaluation ? TA's please clarify.",317991
93840,"I want to add three dataframe df1,df2,df3.All the three dataframe have one common column. how to inner join all three dataframe?",310419
93843,nan,311169
91499,"In case of group case study submission, do we have to submit the body of work individually or one person in the group can submit on behalf of everyone?",318085
93848,,300687
93851,nan,315633
93863,UnicodeDecodeError: &#39;utf-8&#39; codec can&#39;t decode byte 0xa0 in position 25: invalid start byte How to resolve this guys?,318427
93865,nan,310504
93860,Please suggest the best possible way for each column,318344
93004,"Do we have to add a new column as 'Main sector' next to 'Primary sector'? telling what are the main sectors for that particular sector? OR just merge two dataframes and keep the main sectors as column names only; identification would be based on 1,0 value?",316349
93012,"Upon merging companies and rounds2, i am not getting any records. It is showing as 0. I have crossed checked the data types and they are either object or float. What could be the issue here?",310509
93728,"Clarification required on Investment Group Case Study We require clarificationon the below points: 1. When it is required to apply the country filter, is it before check point 1 or after check poin 1 and before check poin 2 2. Should top9 data frame should contain non english speaking countries or not 3. Is required to clean up unwanted columns which are not used, if yes this contradicts the check point 5. Also if yes, before what checkpoint we need to remove them. 4. Shold we drop rows where country_code is null or raised_amount_usd is null if yes at what check point? 5. Once master_frame is created, is it required dorp rows where main_sector is null and primary_sector is null? 6. Is it required use only D1,D2 &amp; D3 data frames for Tableau reporting or for tableau report do we need to consider master fame after applying below filters - funding_type = venture, seed, and private equity - countries with official language as English or all countries to considered - Delete the unwanted rows where country_code is null or raised_amount_usd is null - apply the filter 5 to 15 on raised_amount_usd or 8. What do you mean by fraction, do we need to consider only 3 sectors or all 9 sectors for it",318476
93877,Do we need to clean all the below data before answering the question in Chekpoint -1 1. Cleaning Funding Type 2. Funding amount 3. Mapping of sectors 4. removig the non English speaking rows.,318804
93878,Can anyone please tell me what to do with mapping.csv after category operation.,319869
93880,"The &#39;country_code&#39; and &#39;raised_amount_usd columns obtained after merging to a master is having a huge percentage of Nan values. So, Do we need to remove the rows having the above two columns having &#39;Nan&#39; Values?. If so, as it is being a critical columns for the analysis will it not impact the results ?. Any suggestions?",311115
90070,"Hi Guys, Tanay here from Halasuru, Bangalore. Currently working in Nielsen in Advance Analytics division. Education background : MBA - General Management. Skills : Finance Knowledge, Analytics. Wish to collaborate with peers staying in MG Road, Halasuru, Indiranagar, Domlur,Koramangala. Reply to this if interested to collab. Whatsapp no. 9987354187",318451
93885,Should we clean the data required for checkpoints 3 and 4 in step 1? Or that can be done in the respective checkpoints?,304319
90068,nan,318005
93889,"Now, the aim is to find out the most heavily invested main sectors in each of the three countries (for funding type FT and investments range of 5-15 M USD). is the investments range in Sector Analysis is also same funding type analysis i.e per investment round ?",301115
93893,,306738
93760,nan,300723
93892,"Can anyone tell me what's the issue with this code? It keeps giving me an error: TypeError: 'float' object is not subscriptable master_frame[""primary_sector""]=master_frame[""category_list""].str.split(""|"").apply(lambda x: x[0])",318397
93871,"Point 1 in Checkpoint 5: Sector Analysis 2 asks to create the following: ""The total number (or count) of investments for each main sector in a separate column. The total amount invested in each main sector in a separate column"" Firstly, is it necessary to create these separate columns for each of D1, D2, D3? If YES, then what exactly is the need for it? because these columns would just have repetitive and redundant data, which wouldnt be used at all! So, are these columns required and necessary to be created ? Please if any TA could clarify. thank you.",317998
93891,Is it expected to write a code for identification of the top3 English language countries ? Or simply take from the TOP9 dataframe manually?,310509
93896,"In one of the input files, observed a column with high null % (>15) and this is one of the required key columns for the assignment to proceed... Cannot drop Can we replace the null values with the mean? This makes sense Because there is no general guideline or thumb rule when dealing with null values - it could be subjective some times... TA pls answer on priority thanks",308437
93867,"Can we/Should we just DROP the blanks column when we read the mapping.csv file?? It is mentioned in the question statement that there are exactly 8 main sectors . however, if blanks column is also counted then the total main sectors sums to 9. So, should we just drop it? or, is it required to keep it? If some TA could please clarify. thank you.",317998
93900,"Hi, when give master_frame.isnull().sum() i have got category_list 3407 do I need to remove these rows and merge it with mapping?",317410
93901,nan,310624
93899,"Checkpoint 2 clearly states that we need to select the finding type based on average of investment amount in range 5-15M USD. Checkpoint 5 states: &quot;Create three separate data frames D1, D2 and D3 for each of the three countries containing the observations of funding type FT falling within the 5-15 million USD range . &quot; So, I understand that checkpoint 2 needs average funding to be in range of 5-15 M USD. while checkpoint 5 requires us to select only those rows ( obeservations ) for DataFrames D1-D3 which have investment amount between 5-15M. This way, we can predict companies that can be considered for investment by our CEO. Otherwise, the top company may currently require 50M USD. Am I correct in my understanding?",308637
93907,nan,308434
93908,nan,310009
93912,nan,314197
93920,Do we need to consider the company name which got the highest investment in the top sector selected in point 3 or the top company name irespective of the sector.,306725
93929,"How to deal with the zeros and and amount in four digits or lesser, this sounds awkward if some one is funding $1000 No celar instructions is here. what should be the approach. any suggestions pls.",306244
93915,"HI, For meging of two data frames master_frame and mappping.Do we need to create new column in mapping file as &#39;main_sector&#39; having the column name of sector that has value 1. ?",317410
93936,"Do we need to consider the status of companies while performing the analysis? If so, do we need to remove comapanies in closed status?",317149
93919,"When creating dual axis bar chart in Tableau, it results in a stacked bar chart.. how to create dual axis bar chart with adjacent bars for different measures",306251
93939,"In Spark Investment case study, do we need to remove columns with Null values more than 5%? Because this limit was set for Movies Assignment only. I am not sure If we should use the same for this dataset also. Any opinions?",318532
93941,nan,300687
93982,"While creating dataframe D1, D2 and D3 do we need to consider below filter. Please confirm:- D1 = country1 + FT + rows where funding amount is between 5-15 million usd D2 = country 2 + FT + rows where funding amount is between 5-15 million usd D3 = country3 + FT + rows where funding amount is between 5-15 million usd",320103
93985,nan,318344
91569,There are many values in primary_sector for which no corresponding value exists in mapping df even after clearing mapping df. Ex. Swimming. In some cases there are near-similar values (at times even more than 1) in mapping df. Ex. Games What to do with these cases?,311686
93988,nan,318082
93991,nan,318082
93999,nan,307491
94009,nan,315560
94010,How does one acces the mapping file,312892
94018,"I used same way as we did code while leraning. make column name both as permalink and values to all capital master=pd.merge(df1,df1,how='inner', on='permalink') its throwing error MemoryError I am stuck here. Please help",312019
93914,"I know this question has been asked before, but I want to bring it into TA's attention. Question clearly states that "" Considering that Spark Funds wants to invest between 5 to 15 million USD per investment round , which investment type is the most suitable for it?"" If we take the average investement of four investment type, and identify 5-15 mil range manually, then it does not show how many investment rounds have been made for an organization/investment_type. I think number of rows per venture within 5 to 15 million USD needs to be captured here. TA, please verify",304814
94015,"Hi Dow we need to handling missing values for case study or we have to keep records as it is by clening only special charecters. I am asking this question because we have lot of missing values in fund_raising_amount,category_list,country_code &amp; sector. Overall analysis result may not inpact how ever this imacts checkpoints which was asked to update in Excel.",315455
94023,How to identify English speaking country in Top9,315679
94024,nan,313526
93265,"There are several clarifications needed from TA as I believe in the real world the business team would provide definitive answers to them. All questions below are about data mangling in Python and not editing the input data files. 1. Several categories (44?) have no match in mapping spreadsheet, what should be done for these? Should they be mapped to 'Others' main sector or just effectively ignored 2. One exception to the above is nanotechnology category which abt 250 companies have and has no mapping. Can we map this to semiconductor main sector? Or to also map to Others or ignore these companies? 3. Many companies have no category, should they be mapped to Others main sector? Or again ignored/deleted?",300694
94038,do we needs to remove modify the data frames also if any unique comany names ? And for companies we should use permalink and rounds company_permalink ? Needs help here,312019
94037,"Hi, What is funding type FT in the master_Frame? Is it from column &#39;funding_round_type&#39;.",317410
94026,"Hello everyone! I need to confirm one thing. In this check point while reviewing the mapping file it appears that at some sector names, word na is changed to 0 like Analytics is written as A0lytics in mapping file. There are some others as well. 1. Want to confirm if you all are seeing this. 2. If yes, wouldn't it affect our logic when we have to compare our primary sector with mapping file and punch the main sector. 3. Do we have to make changes ourselves in mapping file for this before writing the logic to punch the main sector. I have shared the screenshot as well.",311855
94030,"In companies dataframe, for counting the number of unique companies do we have to consider the name column ? There is a NULL value in the name column, do we need to consider it ? Also I guess for unique companies if we take the name column then we should find the unqiuenes by converting the cases first. In rounds2 dataframe , for counting the number of unique companies do we have to consider the company_permalink column ?",318756
94042,"will we get more, less, same marks for dynamically using values as we progress through the analysis and checkpoints? Some example: 1. say 'private equity' is the best funding type; can we just hardcode 'private equity' for our queries in checkpoints 3.1 and onwards? 2. similarly lets say we find that Australia, Malta, and Sudan are the top3 english speaking countries for the funding type in 1. above - can we hard code Australia, Malta, and Sudan when deriving D1,D2,D3 OR do we have to use lets say something like Top3Eng[0], Top3Eng[1], and then Top3Eng[2] to keep our code dynamic Again, I would like to know if we will get exra, less, same marks for dynamic approach. Thx.",300694
94045,"HI, Please correct me if I am wrong, is FT a value in funding_round_type or a calculated value?",317410
94044,"Am trying groupby,sum and sort in dataframe in this order It's showing following error : Sort_values() got an unexpected keyword 'by' Why???",308437
94048,nan,308495
94054,nan,315560
94064,nan,300723
94049,nan,308495
94050,"newdata_frame=df1.merge(df2,left_on=&#39;Col1DF1&#39;,right_on=&#39;columnDF2&#39;,how =&#39;inner&#39;)",312016
94052,nan,300723
93931,"When I am trying to split category_list column of companies file using lambda function to get primary sector (separated by |), its throwing error . But when manually splitting instead of lambda function, its working correctly, I am stuck in this stage. Kindly help. companies=pd.read_csv(""companies.txt"", sep='\t',engine='python') companies['category_list'].str.encode('utf-8').str.decode('ascii', 'ignore') Is there any issue with encoding. def convert(x): x1=x.split(""|"") return x1[0] companies['category_list2'] = companies['category_list'].apply(lambda x:convert(x)) AttributeError: 'float' object has no attribute 'split' (this error I am getting) inside function If I use x1=x.str.split(""|"" ), then getting below error. AttributeError: 'str' object has no attribute 'str'",311404
94058,nan,318433
94068,Instead I am getting 1 row and that row has permalink value and rest are NAN.is that fine can i drop that row?,300687
94051,nan,308495
94065,"After merging the mapping file with the master_frame...some of the main_sectors had NaN since there was no corresponding primary_sector for that. So, how should we treat these values ? --&gt; impute with value &#39;Others&#39; or delete these rows having null values ? Please confirm so that we can move ahead.",318479
94060,nan,308495
94053,"When i am applying the filter on top 4 i have getting the correct values (Graph 1 below for IND) But when I reduce the filter to top 3, i get incorrect 3rd value (Graph 2 for IND). Notice the graph for IND country, ideally it should give 52 as the 3rd,, but i get 20 as the 3rd entry in graph 2. Any idea what could be wrong?",316036
94062,nan,315560
91334,nan,317993
94066,nan,300723
94617,nan,318579
94124,nan,315633
93725,"There are some primary sectors in master dataframe which doesnt have corresponding main sector value in the mapping dataframe, even after doing all possible data cleaning of mapping dataframe. I know this question was discussed in the past and it was suggested that using inner join is the solution to this because it will take care of null values. But I am rethinking this and feel that using inner join may not necessarily be the only right option to make. Inner join will basically drop rows from master_frame that doesn&#39;t have matching values from the mapping dataframe. Instead if we impute null values to &#39;Others&#39; that will mean we are making a bold design decision but saving important rows. So its sort of a trade off. The case study wants us to make appropriate realistic assumptions wherever required. In light of this, can we not go with the latter option? TAs, please clarify.",310511
94129,Are these top 3 countries or top 3 english speaking countries. TA please confirm urgently.,300748
94132,"There is a column called ""Blanks"" in the mapping file which maps an empty cell to the Blanks main sector. Should we include it or drop it from mapping dataframe before merging it with the master_frame ?",313691
94131,"To create D1,D2 &amp; D3 we have to filter values based on Country+FT+rased _amt(5-15)M. then we can add total number of investment and total amt invested to the respective Dataframes .Is my approach correct.",319444
94133,"I am getting the below error while trying to import comparines.txt usind pd.read_csv() &quot;ParserError: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2&quot; On adding pd.read_csv(&#39;D:/DScns/GroupPrj/companies.txt&#39;, encoding = &#39;latin-1&#39;,error_bad_lines=False ) to identify the row having the issue, all rows were returned with the same error. Any pointers?",315764
93500,"I tried using some encoding format but could&#39;nt reach to it. While loading the txt file i m getting the result but the result is appearing in txt format ,all jumbled up.What to with that.? How to make it a look in normal format",310611
94135,Do we need to find below based on the top 3 english speaking countries and then on the main sector or should it be on the complete master table irrespective of country. The total number (or count) of investments for each main sector in a separate column The total amount invested in each main sector in a separate column,318804
94139,"For example as above, if we drop funding_round_code column, we may get duplicate rows. so it will be a problem if we drop that column right?",300733
94558,nan,308495
94119,As I have got nearby 82% in current scenario...Is this value is correct? And also while imputing the value to country_code it is correct having nearby 5k are null&#39;s. ? for string&#39;s we can&#39;t do the mean() so getting the top vallue from value_counts(). Can we consider value to replace?,318846
94151,nan,314678
94154,nan,315633
94149,"Hi TA, We have tri ed to extract the PDF document provided in the case study for English speaking countries using PyPDF2 . However, it is not working correctly as there seems to be some inherent problem in handling the &#39;White Spaces&#39; in the PDF document with PyPDF2 . It is generating output but the result are scrambled and unstructured without whitespaces. Instead, we are planning to use PDFMINER package (`pdfminer.six`). Kindly please confirm we can go ahead and use the PDFMINER to extract country names and use for the case study.",311115
94157,nan,315633
94156,nan,314629
94485,"In Section - Checkpoint 5: Sector Analysis 2 Create three separate data frame D1, D2, and D3 for each of the three countries containing the observations of funding type FT falling within the 5-15 million USD range. The three data frames should contain: 1. All the columns of the master_frame along with the primary sector and the main sector 2. The total number (or count) of investments for each main sector in a separate column 3. The total amount invested in each main sector in a separate column3 The points 2 and 3 are NOT very clear. Are we expecting two new columns in D1, D2 and D3 one for the total number of investment and one for the total amount invested in main_sector?. or, these aggregated values into a new data frame? please confirm.",311115
94172,"How to present a plot with all the three Data files (D1, D2 and D3) in one sheet?",304319
94174,nan,305847
94173,"For Table 1.1 , finding unique companies count, should we need to do data cleaning first and tehn find the unique number of companies or first unique number of companies and then data clean up?",317073
94178,nan,314818
93980,"Once we have a merged datafarme from master_frame &amp; mapping using inner joins . There are few rows with primary_sector=NaN &amp; main_sector=Blanks Though &#39;Blanks&#39; do not represents any valid main sector but the fact is that there were investments to such companies irrespective of the sector. The question asks for &quot;Total number of investments (count)&quot; &amp; &quot;Total amount of investment (USD)&quot; It has not been asked explicitly in questions to consider or ignore such records, that is, it has not been asked explicitly if the Total number &amp; Total amount should be with or without respect to the valid sectors. There is a differnce in values when we include such records or exclude such records. Other questions are specific to best performing sectors and are clear on what is needed but these 2 questions are not. These are two different scenarios and inclusion &amp; exclusion of &#39;Blanks&#39; and results differ. What is expected in this question? TAs - Please clarify the expectations.",306248
94180,"After dropping and imputing as much I could, there is about 6-7% of null values in country and region column. What should be the best way of handling this part? Or would it be a good idea to leave them null as we dont have any way we can determine the country code and region?",307176
94183,nan,308434
94184,Investment excel changes color. It became red when I entered some date. Does color coding has any significance?,301643
94185,"struck in to check point 3 . get NaN data in country_code with respect to &#39;raised_amount_usd&#39;.I am using below merge command did not get NaN data in country_code with respect to &#39;raised_amount_usd&#39;.I am using below merge command master_frame=round2.merge(companies, left_on=&#39;company_permalink&#39;, right_on=&#39;permalink&#39;, how=&#39;outer&#39;) pls help",320689
94187,Does the empty value represent funding information not being available or does it represent 0 funding? TA&#39;s Please respond.,318576
94195,nan,300721
94199,Should we perform sector analysis 1 and 2 on master_frame or data frame which is filtered for specific funding type and the top 3 countries ?,313691
94218,investment case study,314818
91115,"If we look closely at the mapping.csv file, a lot of categories having the term &#39;na&#39; in it which seems to have been replaced by 0. If you search by the expression: [A-Za-z]{1,}0[A-Za-z]{1,} you&#39;ll see it. example: Waste Ma0gement, Task Ma0gement",318381
92163,"The number of unique companies does not match numbers doing it through Python code vs. Checking through excel using drop duplicates, anybody seeing the same issue? is it expected ?",304027
92168,A significant percentage of NaN values exist in 'raised_amount_usd' column in the rounds2 dataset. I assume that is expected given the buisness scenario described here. A company may get investment through venture vs seed vs others. Would you suggest imputing some default value for nulls or leave the nulls as they exist?,306250
94233,nan,314197
94235,nan,318579
93933,The query is regarding how to submit the Tableau plots requested as part of the case study. Wanted to confirm if we just need to provide the Tableau plot screenshot under the investment case study Power Point presentation or need to also submit the tableau workbok files as part of the submission?,316036
91264,I know I can use the free version but that has limitations we were told we would be provided with the Tableau license key - we need that ASAP as not having it is holding us up completing the assignment,300694
94237,,308782
93090,1. Is it required to remove unused columns from the master_dataframe? 2. Is it required to remove unused rows from master_dataframe base on the column raised_amount_usd? 3. Do we need to do the cleansing of the data before the check point 1 or before checkpoint 2? Request you to be specific on the queires?,301113
91288,nan,317993
96538,We have received the following feedback. We have converted to lower case after taking the substring. Wouldn't that be sufficient to carry out the comparison? Will converting before taking the substring have an implication on the comparison? (Partial code screen shots below),317149
94264,nan,308495
94246,"1) I am able to do the data cleaning from mapping 2) Able to create the primary_sector too, in master frame. 3) After step-2 joined the 2 dataframe to get one wide data frame. Stuck at creation of master sector? Trying to use melt for identifying 8 main sector but got stuck. Any clue/help is appriciated.",307495
94273,nan,315560
94278,nan,314197
94272,How should we filter the constraint of only English speaking countries? And at what point?,318350
94258,nan,318344
94294,1) To resolve data quality issues when we replace '0' with 'na' there are some correct categories which would also get affected. How do we handle such scenarios - like - Enterprise 2.0 getting converted to Enterprise 2.na? 2) After replacing '0' with 'na' - some categories will become 'navigation' instead of 'Navigation' so they need to be handled too ? Please confirm,319876
94259,nan,308434
94297,"Also, you know the top three English speaking countries and the most suitable funding type for Spark Funds. Let&rsquo;s call the three countries &#39;Country 1&#39;, &#39;Country 2&#39; and &#39;Country 3&#39; and the funding type &#39;FT&#39;. ?",301108
94275,nan,315560
94303,nan,313691
94307,nan,315560
94260,"Can someone guide on how multi column sort can be performed on a group by function. For Example, i have done a group by on two columns and i am calculating the sum of third column for this group by. I want to sort the values now within the group by columns. First level of sort should run on column 1, then 2nd level of sort on column 2",312096
94277,nan,300706
94287,"Hi, After i merge the mapping file dateframe with the master_dateframe ,I am getting many rows with NaN values ....should i drop them or we arent suppose to get NaN values after merging ? P.S - i cleaned the mapping data ,replace the values and used inner join for merging",319860
94309,"I am getting the warning while creating a new column for D1 A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead Though I am using D1.loc[:, &#39;columnname&#39;] = # Series",318329
94311,"Hi, At checkpoint 5. I'm trying to add the sum and sum of investments to the whole new columns. I'm not sure how to reach that solution. Regards, Abhinay",318370
94316,"We have used an additional file in addition to the ones provided in case study. Should we submit our own data file in addition to the 3 files (python main file, pdf, excel) asked?",306250
94320,Will that affect my output of top 9 country? @nd query:To find top 3 english speaking country I understand from TA and discussion forum it should be done manually.So do we need to explain how we got the result or just put the names. I should the find the official of which country is english and then the investment amount and list them according,300687
94332,nan,310624
94322,"A plot showing the top 9 countries against the total amount of investments of funding type FT. This should make the top 3 countries (Country 1, Country 2, and Country 3) very clear. -- Are these (Country 1, Country 2, and Country 3) the countries found in checkpoint 3 country analysis?? A plot showing the number of investments in the top 3 sectors of the top 3 countries on one chart (for the chosen investment type FT). -- Here also the top 3 countries are the countries found in checkpoint 3 country analysis?? And the top 3 sectors here has to be calculated newly based on master_frame?",318328
94337,how to Extract the primary sector of each category list from the category_list column as there is no pipe sign?,320689
94335,This is when merging the mapping and master data frame.,312376
94339,Checkpoint 6 asks for plotting the dataframes. How can the dataframes be exported from jupyter file and in what format do they need to be for tableau import?,301644
94348,Or does the cleaned master_frame which is created after merge with mapping without any further filters or it should be used for tableau plots?,318328
94346,"Quuestion : &quot;Considering that Spark Funds wants to invest between 5 to 15 million USD per investment round, which investment type is the most suitable for it?&quot;",304695
94349,"Using an inner join for master and mapping data frames on &quot;category_list&quot; will result in joining both data frames on their respective &quot;category_list&quot; columns, however, we need to use the &quot;Primary Sector&quot; column of master data frame for this join. I used the left_on and right_on parameters as well to explicitly mention the columns to be used for the join. But that results in the merged data frame retaining the category_list columns from both the data frames. Is my understanding correct and what am I missing here?",317141
94347,nan,318426
94352,nan,317982
93111,nan,317990
94351,nan,315560
94350,nan,315560
93108,While trying to load the companies.txt and rounds2.csv into a data frame. I am getting &quot;&#39;utf-8&#39; codec can&#39;t decode byte 0xa0 in position 25: invalid start byte&quot; error even if I am using different ecoding format like &#39;iso-8859-1&#39; and not &#39;utf-8&#39;. Can someone please help on this so that I can move forward with the assignment.,318374
93334,"The 3rd task is : &#39;A plot showing the number of investments in the top 3 sectors of the top 3 countries on one chart (for the chosen investment type FT).&#39; Is it correct to assume it is to be done for 5mn-15mn records only. i.e. on D1,D2 and D3?",311686
94357,"I am using below format for merge mf - pd.merge(df1,df2, left_on=df1col, right_on=df2col, how=inner) when I use inner, unable to populate df. for concat, right,left and outer join, I am able to populate value. Need help",300685
94356,nan,318579
93117,nan,317990
93118,"There are certain primary_sectors like 'Natural Gas Uses','Self Development' which are missing from the category_list in mapping.csv file. As a result, the mapping to the Main_Sector is not possible. How should we treat such missing primary_sectors?",313826
94367,nan,315560
94368,"we are reqd to ploy fraction of the 3 given Fund types. When we filter on these 3 FTs, it finds fraction with respect to 3 only and not total. so it shows incorrect values.",304319
94373,"&quot;Checkpoint 4: Sector Analysis 1&quot; says 8 main sectors including &quot;Others&quot;, but when i check the Mapping file, it has 9 sectors including &quot;Others&quot; ?",308440
94369,nan,315560
94383,nan,311117
94372,Need some clarification on below statements:- The total number (or count) of investments for each main sector in a separate column The total amount invested in each main sector in a separate column,301108
94385,"Are these both counts? If yes, then what is the difference?",317514
94001,"For the given conditions of Funding type, Raised amount range of 5M to 15M and English speaking country and sector, if multiple companies are level in terms of the raised_amount, is there any guideline on how we should choose the top company? I also found the below statement in the case study. This makes me believe that we can make our own assumption for choosing the top company. ""Table 5.1: Based on the analysis of the sectors, which main sectors and countries would you recommend Spark Funds to invest in? Present your conclusions in the presentation. The conclusions are subjective (i.e. there may be no one right answer), but it should be based on the basic strategy  invest in sectors where most investments are occurring."" Can some TA confirm this?",318084
94387,We need to provide the analysis which we have performed. Apart from that what should be the format?,308442
94396,nan,305804
94380,Unable to read csv file,318240
94401,"now check point 6 ,,again to get data for avg amt per funding type ,this data will be different (slightly for this analysis)from checkpoint 2 due to dropping of blanks at check point 4.which data for plot.i think the last one.Confused",318005
94423,"While taking the sum of raised_amount_usd grouped by country code - I am geting ALB 2.767440e+05 like this amount , how to change it in correct format",318814
94430,Spart fund investment - group project,318497
94413,nan,300687
94416,while reading pansad dataframe to _csv getting this error i have written df.to_csv(r&quot;path_where_tore\filename.csv&quot;),319444
94420,nan,302741
94421,In PPT in problem solving methodology they have mentioned a flow chart. So do we need to represent it in diagramatic representation of flow chart or it&#39;s just the step by step procedure. please clarify.,312756
93564,nan,304813
92605,nan,310419
92812,nan,311169
94440,nan,314511
94443,nan,302741
92691,The data in rounds has same company_permalink but in different fonts. So this has to be considered as one company right? the other columns obviously varies Eg: /ORGANIZATION/0NDINE-BIOMEDICAL-INC /organization/0ndine-biomedical-inc the above is the same company?,305845
94449,nan,303673
93533,"The infomation given is all about the company permalink column, but seems like the &quot;name&quot; column also has issues with the encoding. However, when I tried to correct the encoding using the method in the stack overflow answer, using the lambda function, it gives the below error: AttributeError: (&quot;&#39;float&#39; object has no attribute &#39;encode&#39;&quot;, &#39;occurred at index name&#39;) Now, the point is do we need to correct the encoding of the &quot;name&quot; column because I feel it is not going to be used anywhere.",318397
94452,nan,310997
92866,"What are the deliverables for the Case Study? Also, how are we submitting the table worksheets - twb or twbx file?",310511
94460,There is no header row when you export the Top9countires out of pandas for usage in Tableau. How do you add the header row?,317514
94470,Are dataframes expected to be converted to Excel or dump into SQL Tables? and then the exports are expected to be imported into Tableau? Please suggest. If so this is expected without being taught correct? Or Am I in the wrong direction?,318007
92946,When i click on download button. It is opening new page. But not download started,311952
94483,For the presentation document what are the things that need to be submitted.,312892
94471,"MainSector column hold values such as Automotive &amp; Sports','Cleantech / Semiconductors',,'News, Search and Messaging','Others','Social,Finance,Analytics,Advertising' and Primary_sector has just 1 sector after splitting on '|' how will i map",311386
94462,Since there is no option provided the range of funding i.e 5 to 15 m$ for the funding to select in tabuela so the answer will vary? is my assumption correct for this tabuela 3 question. A plot showing the number of investments in the top 3 sectors of the top 3 countries on one chart (for the chosen investment type FT). TA please confirm it is question from our group.,307843
94481,nan,307176
94473,Checkpoint 5: Specific FT Top 3 English Countries Observations between 5MN to 15MN Checkpoint 6: for charts Specific FT Top 3 English Countries This will lead to different answers for python file and presentations. Kindly suggest.,318436
94477,There are two types of formats in Tableau one is twb and another is twbx. The format twbx will have the data source attached along with the plots whereas twb will only have the plots. Which format is required to be submitted?,312758
94493,do we need to use transform in order to get the count and sum with group by primary category? Transform throwing error.,300734
94487,nan,318579
94497,"How should I extract the count of investment and total investment from the respective columns, given that the numbers in those coluimns are redundant because of multiple main_sectors?",317141
94474,nan,318426
94496,nan,306038
94478,mapping.csv file cleaning How to clean and correct the mapping.csv? It should be done before the merging with master_frame or after merging . How can we do the cleaning? Any suggestion,318814
94484,Version 1: Just after creating master data-frame. Version 2: Cleaning the master data-frame just before mapping dataframe,318429
94500,"is it possible that in top main sector, there would be multiple companies received highest fund. e.g in money sector there would be three companies received same amount ?",306243
94501,"Additionally, if top 3 english speaking countires are changing from checkpoint 2 results to 5-15M range funding analysis, do we need to change the countries?",313767
94479,How to get 'Top N' records in a grouped dataframe object by grouped column .loc method throws an error in such cases,306250
94517,How to add the determined number of investments in each main sector as a column? Is this concisely possible in a single statement?,318007
94489,"Does others qualify as a main sector for the top 3 countries? If yes, then how can it be communicated via the plot what 'Others' is?",300748
94475,"We find 2 kind of expressions in a single dataframe column 'A0lytics', 'Time Ma0gement' , '0notechnology' 'X 2.0', 'X.301' In my search and replace action I want to change '0' to 'na' in the first case ( e.g. 'A0lytics' to 'Analytics') but not in the 2nd case (X 2.0 should not become X 2.na). How do I achieve it using regex?",306250
94495,nan,306008
94510,"Team, I am getting multiple companies which received the highest investment for D1 frame (USA)....is my analysis correct ? I am getting multiple companies which receives the same maximum amount of funding. If yes, then on what criteria, I have to take top two companies since all of then are receiving same max funding. Please help",305650
94502,"x['pr'] = x.c_l.str.split('|',expand = True)[0] when i tried this,getting below error. A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead why this function is not working?",310598
94491,"There are several null values in columns like city, founded at, etc not relevant to the analysis, should they be ignored, imputed or removed?",319357
94520,"In Checkpoint 5: Sector Analysis 2 It is mentioned that the D1, D2 and D3 frames should contain: All the columns of the master_frame along with the primary sector and the main sector. Does this mean that while cleaning we should not drop even a single column or does this mean we can have D1 without the deleted columns in master_frame after cleaning?",318576
98911,how to filter records with values =1 in dataframe?,311046
94503,"Since The total number (or count) of investments for each main sector in a separate column is a summary information, why this should be added to a separate column?",314084
94521,nan,318082
94524,nan,308964
94523,nan,300721
94526,"What does ""Number of investments in top sector (3)"" in excel sheet mean? Number of investments in 3rd top sector based on number of invesments or based on amount invested?",318007
94522,nan,315856
94516,nan,318017
94504,nan,315560
94499,nan,315560
94494,basically wanted to know what is the expectation for flowchart and analysis slides in the ppt,301114
94527,What kind of analysis should be included in ppt? Should we include inferences based on each checkpoint ? Suggestions please,308638
94505,How to extract column name from mapping file after merging for each row based on an unknown column value in master frame?,318007
94482,"Hi TA, We have tried to extract the PDF document provided in the case study for English speaking countries using PyPDF2. However, it is not working correctly as there seems to be some inherent problem in handling the &#39;White Spaces&#39; in the PDF document with PyPDF2. It is generating output but the result are scrambled and unstructured without whitespaces. I have raised this question before in the discussion forum Link https://learn.upgrad.com/v/course/208/question/94149 but have not received a reply. Instead, we have used PDFMINER package (` PDFMINER.six` ). Kindly please confirm it is acceptable to use PDFMINER to extract country names and use for the case study.",311115
94134,"Hi all! I&#39;m on checkpiont4 i.e. merging the main_sector with the master_frame. I&#39;m getting 4000+ null values and the explanation of the same is as follows: So what should be done? Should i first change spelligns in the excel sheet itself? Here you can see : for eg corrrespong to Analytics we have NAN where it is present in mapping.csv but spelling in the .csv file are wrong. Similarly for Finance, Financial Services, etc.. I am attaching the scrennshot for the same below.",318427
94506,Can we modify the order of PPT slides or should we retain them as provided and fill in only the content?,303670
94532,nan,315856
94498,nan,300721
94488,"In last python assignment, we first did Data Cleaning for data frames provided. So in this case study also,before starting checkpoints do we have to first find null percentage, remove null columns , reduce null percentage for data set provided that is, Companies and round2 dataset. I mean all the data cleaning has to be done before starting the assignment or we can ignore it ?",315423
94508,nan,300721
92781,"1. Companies.txt don&#39;t have language column how can we filter it? 2. You have given list of countries in a PDF file who speaks English, but there is no country_code, how can we relate this data to companies.txt, do we have to build this file and then later apply the filter to identify coutries? 3. When do we have to apply the filter for English speaking countries, whether before check point 1 or check point 2?",301113
92932,How do we identify the country based on its code?,305845
92970,"Guys, I have been trying to import rounds2.csv file, but getting error as unicode error import pandas as pd rounds2 = pd.read_csv(&#39;C:\Users\Kja technical1\Desktop\rounds2.csv&#39;) rounds2 I tried infuse &#39;r&#39; before &#39;C&#39; still getting as unicode decode error. why? can anybody suggest how do I go about it.",316399
94528,nan,319384
94537,nan,318329
93599,"print(len(rounds2.loc[~rounds2[&#39;company_permalink&#39;].isin(companies[&#39;permalink&#39;]), :])). Instead I am getting 114949. I have used sep=&quot;\t&quot;, encoding =&quot;latin-1&quot; in importing .txt and encoding =&quot;latin-1&quot; in importing .csv",318335
94539,nan,315560
94515,How can I get the column name based on a value of the row in pandas for a dataframe?,300748
92607,I assume that the suitable funding type is the one for which mean() of investment amounts is in the range USD 5-15 mn - Is that a correct assumption? (Though at the same time I don't understand why Spark Funds can't choose funding type where mean() is below 5 mn -&gt; they can invest in multiple companies in a single investment round??),306250
93617,nan,303673
93208,"Hi, 1st task is mentioned as : &#39;A plot showing the fraction of total investments (globally) in venture, seed, and private equity, and the average amount of investment in each funding type. This chart should make it clear that a certain funding type (FT) is best suited for Spark Funds.&#39; But if want to show this fraction against total investments then other funding types will also come in picture. Does total here mean total of venture, seed, and private equity?",311686
94514,nan,318017
94563,"i performed, say sum() while doing the group by. now i would like to sum the result of sum() aggregation as well. should i treat first level groupby sum() as new column and then perform sum of sum, or there is any other way possible",306005
94583,"Why do ""Checkpoint 6: Plots"" ask for plotting in Tableau when it is yet to be come up as next course, and not all the person have earlier hands on it.",306005
94574,What needs to be written in Abstract? Is it the objective of this case study?,308442
94586,Do we need to submit the case investment individully ? or Group creater can submit and considering for all.,307843
93137,nan,318585
94576,"For plot 1 in Tableau, can we show all funding types in the graph and highlight avg funding and fraction of investment for private equity, seed and venture?? Or should we show avg funding and investment fraction for only venture, seed and private equity? TA pls clarify dis thanks...",308437
94587,nan,318240
94589,nan,315560
94593,nan,303673
94612,nan,319846
94615,nan,315633
94604,nan,308634
94597,Should we consider angel funding in tableau plot 1?,301644
94607,nan,315560
94624,,300687
94625,nan,300721
94609,nan,318579
94626,nan,319846
94550,"I am using transform function and doing groupp by on &#39;main_sector&#39; for table D1, D2, D3 . Result for sum on column &#39;raised_amount_usd&#39; something like , is it expected or have i done something worng",311386
94628,nan,310504
94634,nan,318579
94581,"Should be we consider plotting for angel as it is not mentioned in point 1 to plot for that. It says: A plot showing the fraction of total investments (globally) in venture, seed, and private equity , and the average amount of investment in each funding type.",318350
93366,"1. For check point 1 sub-point 5 , do we have to clean the data and then calculate the observation e.g. only for english countries? 2. For checkpoint 2, do we have to consider only english speaking country or calculate data on all the countries? 3. For check point 3, we have to create a dataframe of country in which we have to create three column english official, first english language, second english language. we need to extract all the information to put in three column using google and then create a dataframe. I persume this is the approach we have to follow or can somebody guide the process. Thanks in advance",306243
94643,nan,310502
93399,"Do we need to create separate data frames to hold the count and sum of investment, becuase, it don&#39;t make any sense to add these columns to data frames D1, D2 &amp; D3",301113
93571,I am not getting how to divide the task among team members. what is the logic behind the group project if every tema member is doing all task them selves.,312746
93387,"hi, one of the questions I had was should the data be filtered pretty much from checkpoint 2 to ignore non-English speaking countries. This answer from TA says even up to and including checkpoint 2 we should not filter out non-English speaking countries: https://learn.upgrad.com/v/course/208/question/93366 That is great that we have this clarification, but I believe that advice conflicts with the requirements of the case study. On the requirements page of checkpoint 2 https://learn.upgrad.com/v/course/208/session/19904/segment/101324 it clearly says "" Considering the constraints of Spark Funds ... "" and ofourse on page 1 of the assignment https://learn.upgrad.com/v/course/208/session/19904/segment/101226 states that English speaking country is one of the constraints So now I am confused. Note: including or excluding non-english speaking countries will change the amount of averages, but the recommendation of 'funding type' does not change. My personal interpretation is that we should exclude non-English speaking countries in checkpoint 2 as we are making a recommendation for Spark and it has clearly said it only wants to focus on English speaking countries",300694
93471,Is something wrong with outcome I am getting?,305843
93582,"to find out english speaking countries, we are provided country codes (in companies file) and english speaking nations(PDF) But we need country names first. How to get? Can we use external data source to get country names? Pls answer TA, thanks.",308437
93580,"Though it is not specifically asked for, do we need to analyze rows/columns in terms of NULLs and drop columns (or delete rows) accordingly.. or update nulls with means etc.?",301116
93604,&#39;permalink&#39; column in rounds2 data frame and &#39;company_permalink&#39; column in companies data frame has same number of distinct values if we ignore the lower/upper case of entries.Shall we ignore the case of values and then merge the data frames keeping these two columns as index.?,304696
93621,nan,310585
98590,"I am getting exponential values when I sum ""fund raised "" column, 4.659255e+09. how to get this into normal value I am using below code to calculate sum based on the funding type. master_frame.pivot_table(values = 'raised_amount_usd',index = 'funding_round_type',aggfunc = 'sum')",311046
97285,Related to Analytics spelling where na was n0,318440
98829,"HI Spark Funds wants to see the top nine countries which have received the highest total funding (across ALL sectors for the chosen investment type) In this , Did not understand the word &quot;chosen invetment type&quot;. what is the chosen investment type here? does this mean we do not have to find out top9 countries for all investment types , we have to find top 9 countries for only one investment type? which investment type we have to choose here?",311046
99443,"Hi , I am using below code to create D2 dataframe, I am getting error for the bolded line, can any one help. D_USA = master_frame.loc[master_frame['country_code']=='USA'] D_USA1 = pd.merge(D_USA,merged_sector_category,how = 'left', on = 'primary_sector') D_USA1.FT= D_GB1.groupby(['primary_sector'])['raised_amount_usd'].sum().astype('str') D_USA1 = D_USA1[(D_USA1['FT']&gt;='5000000') &amp; (D_USA1['FT'] &lt;= '15000000')] D_USA1.NO_OF_INVESTMENTS= D_GB1.groupby(['primary_sector'])['name'].count().astype('str') D2 = D_USA1 D2",311046
98599,"HI , I am using below code to calculate sum of raised_amount_usd. output of this gives funding_round_type and country_code. but not getting column name for the summed column raised_amount_usd. How to name the column? by_cntry_type = master_frame.groupby([&#39;funding_round_type&#39;,&#39;country_code&#39;]) master_df2 = by_cntry_type.raised_amount_usd.sum().astype(&#39;int64&#39;)",311046
106770,"I get very high VIF almost most of the variables are having VIF more than 10, how to tackle this problem",300735
106234,"In the assignment, we have multiple categorical variables with more than 2 values so how to convert them into a numeric format to build the model",300735
116351,"How do we choose, what can or what can not be hyper parameters?",313676
141342,nan,316349
141069,"I am facing some issue for extraction of data frame with condition i have df with shape (474462, 3) i did filter df = df.loc[df['review_overall'] &gt; 2.0] or df=df[df['review_overall']&gt;2.0] I am getting more rows. I should get same or lesser df.shape (455776, 3) Please clarify some one",312019
141368,"In this Question, we have to find the N - minimum Frequency of reviews for any beer right?",315423
141144,"While creating a pivot for review profilename, returning this error: ValueError: Index contains duplicate entries, cannot reshape Kindly help as to how to proceed.",314221
141475,"While inspecting the beer data csv file, I could notice some values in review_profilename as Mar-77 (1/03/1977) and there are few values which are purely integers 972, 1099. Do we need to remove them?",314730
141302,Does above values are equal to &quot; number of ratings given to each beer &quot; and &quot; number of ratings given by each user &quot; respectively !! If not kindly elaborate the difference.,318770
141434,@TA: Please provide how to solve qus 3 and 4 ? What is the approach we need to follow ? Determine how similar the first 10 users are to each other and visualise it. Compute and visualise the similarity between the first 10 beers.,305650
141436,nan,311254
141172,"df = df.drop_duplicates([&#39;beer_beerid&#39;,&#39;review_profilename&#39;]) df_movie_features = train.pivot( index=&#39;beer_beerid&#39;, columns=&#39;review_profilename&#39;, values=&#39;review_overall&#39; ).fillna(0) This gives value error - ValueError: Index contains duplicate entries, cannot reshape",319759
140445,Under Data Exploration there is an ask to Visualize the rating values. Is it required to plot a graph?,308438
140507,"In the Data exploration part, we need to take only data that we got from Data Preparation (Only those beeers that have atleast N number of reviews) ? or need to consider full dataset?",320103
141454,One of the steps that is mentioned in the assignment - &quot;Choose only those beers that have at least N number of reviews.&quot; Similarly as part of data preparation &quot;Should we choose only those users that have given at least N number of reviews.?,310467
141463,nan,303228
140376,"Could some one clarify exactly what is expected graphs for below questions, bit confused. The average ratings for beer, users. The average no of ratings givien to beers and users.",312019
140658,"Do we needs to convert review_profilename to dummy columns , to make int values. For doing analysis we should make it as user_id etc ? Otherwise algorithms wont work. needs numerical values.",312019
140711,"Code:- df_beer_features = train.pivot( index='review_profilename', columns='beer_beerid', values='review_overall' ).fillna(0) Error ValueError: Index contains duplicate entries, cannot reshape Is it because user name is of type String rather Int. How to resolve this?",320103
140824,"Trying to make pivot ratings for the beermat features pivot ratings into beermat features df__features = train.pivot( index='id', columns='bi', values='score' ).fillna(0) raise ValueError('Index contains duplicate entries, ' 170 'cannot reshape') ValueError: Index contains duplicate entries, cannot reshape Becaues profile_name(profile_id) has duplicate . How to go about this ?",312019
141335,"Can some one please explain Real Rating Matrix? How to use this matrix for the ""review_profilename"" so that we can convert this column into integer value.",306243
140937,"Hi, I am getting excalty same RMSE values in both the models (i.e. user-based and item-based). Up to 10th decimal place. Is it possible? Anybody else also facing same situation?",311686
140946,"The problem statement wants us to find: The average beer ratings The average user ratings Regarding this, I have the following question: Does average beer/user rating mean avg rating per beer/user? Visualizing that for each individual beer/user wont be easy or very sensible I feel. Please help.",310511
140959,,316368
141257,I'm getting NaN as the first index after the pivot: Any idea why what is causing this? Note: I've taken care of the duplicates combinations of profilename and beerid:,310974
141272,"For one of the question in the forum, TA told us to convert &quot;real rating matrix&quot; to convert string values in review_profilename column to int values. Request TA to let us know how to perform that ?",305650
141322,nan,300733
141312,"while doing user_correlation it hangs forever. not sure what to be done records are about 4lakhs after removal of duplicates etc can some one help to do better way. from sklearn.metrics.pairwise import pairwise_distances # User Similarity Matrix user_correlation = 1 - pairwise_distances(df_beer_features, metric='cosine') user_correlation[np.isnan(user_correlation)] = 0 print(user_correlation)",312019
141491,"test_user_final_rating.fillna(0) ____________ from sklearn.preprocessing import MinMaxScaler from numpy import * from sklearn.preprocessing import MinMaxScaler from numpy import * X = test_user_final_rating.copy() X = X[X&gt;0] scaler = MinMaxScaler(feature_range=(1, 5)) print(scaler.fit(X)) y = (scaler.transform(X)) print(y) _____ Still getting ""Input contains NaN, infinity or a value too large for dtype('float64')."" Any help",306243
141811,"can any body help, how to solve this??? I have created pivot table using belwo code: -------- # pivot ratings into movie features df_beer_features = train.pivot_table( index=&#39;review_profilename&#39;, columns=&#39;beer_beerid&#39;, values=&#39;review_overall_x&#39; ).fillna(0) #in question 2 , 1st &amp; 3rd sentence is same and 2nd &amp; 4th question are same.TA please confirm???if any differnece between them?",305847
141505,"Getting the below error for scaler.fit(X) ValueError: Input contains NaN, infinity or a value too large for dtype(&#39;float64&#39;). can someone help with the resolution?",316036
141518,nan,306248
141525,nan,311254
141572,Memory error while creating cosine matrix,300719
141574,....how do you determine first 10?? Take the first 10 rows in the train set??,311857
141941,nan,300725
141933,,316891
141935,Recommendation System All your code has to be submitted in one main .R file. SUBMISSION DEADLINE 22 July 2019 11:59 PM IST We strongly recommend that you submit at least 30 minutes before the deadline. I hope its Python only.,317987
141649,"In Visualization, most of the time I'm facing the issue that dense plot is generated, which is not helpful as Not able to infer anything from it? What should we do in such situations?",315423
141785,nan,311466
141666,"After setting the Threshold for Minimum Count of Reviews to 400, I m getting 58000 rows with 106 Unique Beers and 11,900 UserID&#39;s. Now, How can I proceed with plotting especially the UserID-based Calculation. Shall I plot only first 20 rows or something? Or shall I change my threshold.",317984
141587,,310624
141601,nan,318598
141706,"I had built both of the models and selected one model say Users based model Now I want to predict items to particular users? Do we need to deploy the model or not? If yes, how do we deploy the IBCF model? further confused about what we should use user_final_rating or test_user_final_rating to predict items to users ? or do we need to create new for the whole dataset?",311466
141263,Can anyone explain this?,311254
141538,"can any body help, how to solve this. I have created pivot table using belwo code: -------- # pivot ratings into movie features df_beer_features = train.pivot_table( index='review_profilename', columns='beer_beerid', values='review_overall_x' ).fillna(0) ------- the data is huge and when i apply correlation matrix after creating user_correlation. it hangs. Can someone plz let me know the steps to solve the 3.3 questions ""Determine how similar the first 10 users are to each other and visualise it."" Thanks in advance",306243
141359,nan,318005
141829,nan,303228
141791,"Is it necessary to scale the values for RMSE calculation in the assignment? In the lecture notebook of the movie dataset, the ratings were on different scale. But in the assignment for the beer dataset we have been given that the ratings are already between 1 and 5 as below. So is it necessary to scale for calculating RMSE?? &quot;Description: Each record includes a beer&#39;s name and the user&#39;s name, along with the ratings he/she has given to the beer. All ratings are on a scale from 1 to 5, with 5 being the best rating.&quot;",310505
141681,Can you please help me understanding what is meant by first 10 beers. Is it by number of ratings they got or the average ratings they got. Also how is it related to the recommendation model. I am thinking this is part of data exploration.,318554
141877,In the movie recommendation system example the test data with its ratings is used to predict the same test data. Isn't this wrong. The target variable should never be used to predict itself.,319357
